<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: https://leo-editor.github.io/leo-editor/leo_toc.html -->
<leo_file xmlns:leo="http://leo-editor.github.io/leo-editor/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20230613022630.1"><vh>Startup</vh>
<v t="ekr.20230613022138.1"><vh>@settings</vh>
<v t="ekr.20230613022224.1"><vh>@bool run-flake8-on-write = False</vh></v>
<v t="ekr.20230828050824.1"><vh>@button backup</vh></v>
<v t="ekr.20230901114501.1"><vh>@data history-list</vh></v>
</v>
</v>
<v t="ekr.20230614124254.1"><vh> Recursive import script</vh></v>
<v t="ekr.20230902050406.1"><vh>mypy sources...</vh>
<v t="ekr.20230902050227.1"><vh>--- Others</vh>
<v t="ekr.20230831070358.1"><vh>--- dmypy</vh>
<v t="ekr.20230831011819.905"><vh>@clean dmypy_os.py</vh>
<v t="ekr.20230831011819.906"><vh>&lt;&lt; dmypy_os.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.907"><vh>function: alive</vh></v>
<v t="ekr.20230831011819.908"><vh>function: kill</vh></v>
</v>
<v t="ekr.20230831011819.909"><vh>@clean dmypy_server.py</vh>
<v t="ekr.20230831011819.911"><vh>&lt;&lt; dmypy_server.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.912"><vh>function: daemonize</vh></v>
<v t="ekr.20230831011819.913"><vh>function: _daemonize_cb</vh></v>
<v t="ekr.20230831011819.914"><vh>function: daemonize</vh></v>
<v t="ekr.20230831011819.915"><vh>function: process_start_options</vh></v>
<v t="ekr.20230831011819.916"><vh>function: ignore_suppressed_imports</vh></v>
<v t="ekr.20230831011819.917"><vh>class Server</vh>
<v t="ekr.20230831011819.918"><vh>Server.__init__</vh></v>
<v t="ekr.20230831011819.919"><vh>Server._response_metadata</vh></v>
<v t="ekr.20230831011819.920"><vh>Server.serve</vh></v>
<v t="ekr.20230831011819.921"><vh>Server.run_command</vh></v>
<v t="ekr.20230831011819.922"><vh>Server.cmd_status</vh></v>
<v t="ekr.20230831011819.923"><vh>Server.cmd_stop</vh></v>
<v t="ekr.20230831011819.924"><vh>Server.cmd_run</vh></v>
<v t="ekr.20230831011819.925"><vh>Server.cmd_check</vh></v>
<v t="ekr.20230831011819.926"><vh>Server.cmd_recheck</vh></v>
<v t="ekr.20230831011819.927"><vh>Server.check</vh></v>
<v t="ekr.20230831011819.928"><vh>Server.flush_caches</vh></v>
<v t="ekr.20230831011819.929"><vh>Server.update_stats</vh></v>
<v t="ekr.20230831011819.930"><vh>Server.following_imports</vh></v>
<v t="ekr.20230831011819.931"><vh>Server.initialize_fine_grained</vh></v>
<v t="ekr.20230831011819.932"><vh>Server.fine_grained_increment</vh></v>
<v t="ekr.20230831011819.933"><vh>Server.fine_grained_increment_follow_imports</vh></v>
<v t="ekr.20230831011819.934"><vh>Server.find_reachable_changed_modules</vh></v>
<v t="ekr.20230831011819.935"><vh>Server.direct_imports</vh></v>
<v t="ekr.20230831011819.936"><vh>Server.find_added_suppressed</vh></v>
<v t="ekr.20230831011819.937"><vh>Server.increment_output</vh></v>
<v t="ekr.20230831011819.938"><vh>Server.pretty_messages</vh></v>
<v t="ekr.20230831011819.939"><vh>Server.update_sources</vh></v>
<v t="ekr.20230831011819.940"><vh>Server.update_changed</vh></v>
<v t="ekr.20230831011819.941"><vh>Server.find_changed</vh></v>
<v t="ekr.20230831011819.942"><vh>Server._find_changed</vh></v>
<v t="ekr.20230831011819.943"><vh>Server.cmd_inspect</vh></v>
<v t="ekr.20230831011819.944"><vh>Server.cmd_suggest</vh></v>
<v t="ekr.20230831011819.945"><vh>Server.cmd_hang</vh></v>
</v>
<v t="ekr.20230831011819.946"><vh>function: get_meminfo</vh></v>
<v t="ekr.20230831011819.947"><vh>function: find_all_sources_in_build</vh></v>
<v t="ekr.20230831011819.948"><vh>function: fix_module_deps</vh></v>
<v t="ekr.20230831011819.949"><vh>function: filter_out_missing_top_level_packages</vh></v>
</v>
<v t="ekr.20230831011819.950"><vh>@clean dmypy_util.py</vh>
<v t="ekr.20230831011819.952"><vh>&lt;&lt; dmypy_util.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.953"><vh>function: receive</vh></v>
</v>
</v>
<v t="ekr.20230831071452.1"><vh>--- Errors &amp; messages</vh>
<v t="ekr.20230831011819.992"><vh>@clean errorcodes.py</vh>
<v t="ekr.20230831011819.994"><vh>&lt;&lt; errorcodes.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.995"><vh>class ErrorCode</vh>
<v t="ekr.20230831011819.996"><vh>ErrorCode.__init__</vh></v>
<v t="ekr.20230831011819.997"><vh>ErrorCode.__str__</vh></v>
<v t="ekr.20230831011819.998"><vh>ErrorCode.__eq__</vh></v>
<v t="ekr.20230831011819.999"><vh>ErrorCode.__hash__</vh></v>
</v>
</v>
<v t="ekr.20230831011819.1000"><vh>@clean errors.py</vh>
<v t="ekr.20230831011819.1001"><vh>&lt;&lt; errors.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1002"><vh>class ErrorInfo</vh>
<v t="ekr.20230831011819.1003"><vh>ErrorInfo.__init__</vh></v>
</v>
<v t="ekr.20230831011819.1004"><vh>class ErrorWatcher</vh>
<v t="ekr.20230831011819.1005"><vh>ErrorWatcher.__init__</vh></v>
<v t="ekr.20230831011819.1006"><vh>ErrorWatcher.__enter__</vh></v>
<v t="ekr.20230831011819.1007"><vh>ErrorWatcher.__exit__</vh></v>
<v t="ekr.20230831011819.1008"><vh>ErrorWatcher.on_error</vh></v>
<v t="ekr.20230831011819.1009"><vh>ErrorWatcher.has_new_errors</vh></v>
<v t="ekr.20230831011819.1010"><vh>ErrorWatcher.filtered_errors</vh></v>
</v>
<v t="ekr.20230831011819.1011"><vh>class Errors</vh>
<v t="ekr.20230831011819.1012"><vh>Errors.__init__</vh></v>
<v t="ekr.20230831011819.1013"><vh>Errors.initialize</vh></v>
<v t="ekr.20230831011819.1014"><vh>Errors.reset</vh></v>
<v t="ekr.20230831011819.1015"><vh>Errors.set_ignore_prefix</vh></v>
<v t="ekr.20230831011819.1016"><vh>Errors.simplify_path</vh></v>
<v t="ekr.20230831011819.1017"><vh>Errors.set_file</vh></v>
<v t="ekr.20230831011819.1018"><vh>Errors.set_file_ignored_lines</vh></v>
<v t="ekr.20230831011819.1019"><vh>Errors.set_skipped_lines</vh></v>
<v t="ekr.20230831011819.1020"><vh>Errors.current_target</vh></v>
<v t="ekr.20230831011819.1021"><vh>Errors.current_module</vh></v>
<v t="ekr.20230831011819.1022"><vh>Errors.import_context</vh></v>
<v t="ekr.20230831011819.1023"><vh>Errors.set_import_context</vh></v>
<v t="ekr.20230831011819.1024"><vh>Errors.report</vh></v>
<v t="ekr.20230831011819.1025"><vh>Errors._add_error_info</vh></v>
<v t="ekr.20230831011819.1026"><vh>Errors._filter_error</vh></v>
<v t="ekr.20230831011819.1027"><vh>Errors.add_error_info</vh></v>
<v t="ekr.20230831011819.1028"><vh>Errors.has_many_errors</vh></v>
<v t="ekr.20230831011819.1029"><vh>Errors.report_hidden_errors</vh></v>
<v t="ekr.20230831011819.1030"><vh>Errors.is_ignored_error</vh></v>
<v t="ekr.20230831011819.1031"><vh>Errors.is_error_code_enabled</vh></v>
<v t="ekr.20230831011819.1032"><vh>Errors.clear_errors_in_targets</vh></v>
<v t="ekr.20230831011819.1033"><vh>Errors.generate_unused_ignore_errors</vh></v>
<v t="ekr.20230831011819.1034"><vh>Errors.generate_ignore_without_code_errors</vh></v>
<v t="ekr.20230831011819.1035"><vh>Errors.num_messages</vh></v>
<v t="ekr.20230831011819.1036"><vh>Errors.is_errors</vh></v>
<v t="ekr.20230831011819.1037"><vh>Errors.is_blockers</vh></v>
<v t="ekr.20230831011819.1038"><vh>Errors.blocker_module</vh></v>
<v t="ekr.20230831011819.1039"><vh>Errors.is_errors_for_file</vh></v>
<v t="ekr.20230831011819.1040"><vh>Errors.prefer_simple_messages</vh></v>
<v t="ekr.20230831011819.1041"><vh>Errors.raise_error</vh></v>
<v t="ekr.20230831011819.1042"><vh>Errors.format_messages</vh></v>
<v t="ekr.20230831011819.1043"><vh>Errors.file_messages</vh></v>
<v t="ekr.20230831011819.1044"><vh>Errors.new_messages</vh></v>
<v t="ekr.20230831011819.1045"><vh>Errors.targets</vh></v>
<v t="ekr.20230831011819.1046"><vh>Errors.render_messages</vh></v>
<v t="ekr.20230831011819.1047"><vh>Errors.sort_messages</vh></v>
<v t="ekr.20230831011819.1048"><vh>Errors.sort_within_context</vh></v>
<v t="ekr.20230831011819.1049"><vh>Errors.remove_duplicates</vh></v>
</v>
<v t="ekr.20230831011819.1050"><vh>class CompileError</vh>
<v t="ekr.20230831011819.1051"><vh>CompileError.__init__</vh></v>
</v>
<v t="ekr.20230831011819.1052"><vh>function: remove_path_prefix</vh></v>
<v t="ekr.20230831011819.1053"><vh>function: report_internal_error</vh></v>
</v>
<v t="ekr.20230831011820.62"><vh>@clean message_registry.py</vh>
<v t="ekr.20230831011820.65"><vh>class ErrorMessage</vh>
<v t="ekr.20230831011820.66"><vh>ErrorMessage.format</vh></v>
<v t="ekr.20230831011820.67"><vh>ErrorMessage.with_additional_msg</vh></v>
</v>
</v>
<v t="ekr.20230831011820.68"><vh>@clean messages.py</vh>
<v t="ekr.20230831011820.70"><vh>&lt;&lt; messages.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.71"><vh>class MessageBuilder</vh>
<v t="ekr.20230831011820.72"><vh>MessageBuilder.__init__</vh></v>
<v t="ekr.20230831011820.73"><vh>MessageBuilder.filter_errors</vh></v>
<v t="ekr.20230831011820.74"><vh>MessageBuilder.add_errors</vh></v>
<v t="ekr.20230831011820.75"><vh>MessageBuilder.disable_type_names</vh></v>
<v t="ekr.20230831011820.76"><vh>MessageBuilder.are_type_names_disabled</vh></v>
<v t="ekr.20230831011820.77"><vh>MessageBuilder.prefer_simple_messages</vh></v>
<v t="ekr.20230831011820.78"><vh>MessageBuilder.report</vh></v>
<v t="ekr.20230831011820.79"><vh>MessageBuilder.fail</vh></v>
<v t="ekr.20230831011820.80"><vh>MessageBuilder.note</vh></v>
<v t="ekr.20230831011820.81"><vh>MessageBuilder.note_multiline</vh></v>
<v t="ekr.20230831011820.82"><vh>MessageBuilder.has_no_attr</vh></v>
<v t="ekr.20230831011820.83"><vh>MessageBuilder.unsupported_operand_types</vh></v>
<v t="ekr.20230831011820.84"><vh>MessageBuilder.unsupported_left_operand</vh></v>
<v t="ekr.20230831011820.85"><vh>MessageBuilder.not_callable</vh></v>
<v t="ekr.20230831011820.86"><vh>MessageBuilder.untyped_function_call</vh></v>
<v t="ekr.20230831011820.87"><vh>MessageBuilder.incompatible_argument</vh></v>
<v t="ekr.20230831011820.88"><vh>MessageBuilder.incompatible_argument_note</vh></v>
<v t="ekr.20230831011820.89"><vh>MessageBuilder.maybe_note_concatenate_pos_args</vh></v>
<v t="ekr.20230831011820.90"><vh>MessageBuilder.invalid_index_type</vh></v>
<v t="ekr.20230831011820.91"><vh>MessageBuilder.too_few_arguments</vh></v>
<v t="ekr.20230831011820.92"><vh>MessageBuilder.missing_named_argument</vh></v>
<v t="ekr.20230831011820.93"><vh>MessageBuilder.too_many_arguments</vh></v>
<v t="ekr.20230831011820.94"><vh>MessageBuilder.too_many_arguments_from_typed_dict</vh></v>
<v t="ekr.20230831011820.95"><vh>MessageBuilder.too_many_positional_arguments</vh></v>
<v t="ekr.20230831011820.96"><vh>MessageBuilder.maybe_note_about_special_args</vh></v>
<v t="ekr.20230831011820.97"><vh>MessageBuilder.unexpected_keyword_argument</vh></v>
<v t="ekr.20230831011820.98"><vh>MessageBuilder.duplicate_argument_value</vh></v>
<v t="ekr.20230831011820.99"><vh>MessageBuilder.does_not_return_value</vh></v>
<v t="ekr.20230831011820.100"><vh>MessageBuilder.deleted_as_rvalue</vh></v>
<v t="ekr.20230831011820.101"><vh>MessageBuilder.deleted_as_lvalue</vh></v>
<v t="ekr.20230831011820.102"><vh>MessageBuilder.no_variant_matches_arguments</vh></v>
<v t="ekr.20230831011820.103"><vh>MessageBuilder.wrong_number_values_to_unpack</vh></v>
<v t="ekr.20230831011820.104"><vh>MessageBuilder.unpacking_strings_disallowed</vh></v>
<v t="ekr.20230831011820.105"><vh>MessageBuilder.type_not_iterable</vh></v>
<v t="ekr.20230831011820.106"><vh>MessageBuilder.possible_missing_await</vh></v>
<v t="ekr.20230831011820.107"><vh>MessageBuilder.incompatible_operator_assignment</vh></v>
<v t="ekr.20230831011820.108"><vh>MessageBuilder.overload_signature_incompatible_with_supertype</vh></v>
<v t="ekr.20230831011820.109"><vh>MessageBuilder.signature_incompatible_with_supertype</vh></v>
<v t="ekr.20230831011820.110"><vh>MessageBuilder.pretty_callable_or_overload</vh></v>
<v t="ekr.20230831011820.111"><vh>MessageBuilder.argument_incompatible_with_supertype</vh></v>
<v t="ekr.20230831011820.112"><vh>MessageBuilder.comparison_method_example_msg</vh></v>
</v>
<v t="ekr.20230831011820.113"><vh>function: return_type_incompatible_with_supertype</vh></v>
<v t="ekr.20230831011820.114"><vh>function: override_target</vh></v>
<v t="ekr.20230831011820.115"><vh>function: incompatible_type_application</vh></v>
<v t="ekr.20230831011820.116"><vh>function: could_not_infer_type_arguments</vh></v>
<v t="ekr.20230831011820.117"><vh>function: invalid_var_arg</vh></v>
<v t="ekr.20230831011820.118"><vh>function: invalid_keyword_var_arg</vh></v>
<v t="ekr.20230831011820.119"><vh>function: undefined_in_superclass</vh></v>
<v t="ekr.20230831011820.120"><vh>function: variable_may_be_undefined</vh></v>
<v t="ekr.20230831011820.121"><vh>function: var_used_before_def</vh></v>
<v t="ekr.20230831011820.122"><vh>function: first_argument_for_super_must_be_type</vh></v>
<v t="ekr.20230831011820.123"><vh>function: unsafe_super</vh></v>
<v t="ekr.20230831011820.124"><vh>function: too_few_string_formatting_arguments</vh></v>
<v t="ekr.20230831011820.125"><vh>function: too_many_string_formatting_arguments</vh></v>
<v t="ekr.20230831011820.126"><vh>function: unsupported_placeholder</vh></v>
<v t="ekr.20230831011820.127"><vh>function: string_interpolation_with_star_and_key</vh></v>
<v t="ekr.20230831011820.128"><vh>function: requires_int_or_single_byte</vh></v>
<v t="ekr.20230831011820.129"><vh>function: requires_int_or_char</vh></v>
<v t="ekr.20230831011820.130"><vh>function: key_not_in_mapping</vh></v>
<v t="ekr.20230831011820.131"><vh>function: string_interpolation_mixing_key_and_non_keys</vh></v>
<v t="ekr.20230831011820.132"><vh>function: cannot_determine_type</vh></v>
<v t="ekr.20230831011820.133"><vh>function: cannot_determine_type_in_base</vh></v>
<v t="ekr.20230831011820.134"><vh>function: no_formal_self</vh></v>
<v t="ekr.20230831011820.135"><vh>function: incompatible_self_argument</vh></v>
<v t="ekr.20230831011820.136"><vh>function: incompatible_conditional_function_def</vh></v>
<v t="ekr.20230831011820.137"><vh>function: cannot_instantiate_abstract_class</vh></v>
<v t="ekr.20230831011820.138"><vh>function: base_class_definitions_incompatible</vh></v>
<v t="ekr.20230831011820.139"><vh>function: cant_assign_to_method</vh></v>
<v t="ekr.20230831011820.140"><vh>function: cant_assign_to_classvar</vh></v>
<v t="ekr.20230831011820.141"><vh>function: no_overridable_method</vh></v>
<v t="ekr.20230831011820.142"><vh>function: explicit_override_decorator_missing</vh></v>
<v t="ekr.20230831011820.143"><vh>function: final_cant_override_writable</vh></v>
<v t="ekr.20230831011820.144"><vh>function: cant_override_final</vh></v>
<v t="ekr.20230831011820.145"><vh>function: cant_assign_to_final</vh></v>
<v t="ekr.20230831011820.146"><vh>function: protocol_members_cant_be_final</vh></v>
<v t="ekr.20230831011820.147"><vh>function: final_without_value</vh></v>
<v t="ekr.20230831011820.148"><vh>function: read_only_property</vh></v>
<v t="ekr.20230831011820.149"><vh>function: incompatible_typevar_value</vh></v>
<v t="ekr.20230831011820.150"><vh>function: dangerous_comparison</vh></v>
<v t="ekr.20230831011820.151"><vh>function: overload_inconsistently_applies_decorator</vh></v>
<v t="ekr.20230831011820.152"><vh>function: overloaded_signatures_overlap</vh></v>
<v t="ekr.20230831011820.153"><vh>function: overloaded_signature_will_never_match</vh></v>
<v t="ekr.20230831011820.154"><vh>function: overloaded_signatures_typevar_specific</vh></v>
<v t="ekr.20230831011820.155"><vh>function: overloaded_signatures_arg_specific</vh></v>
<v t="ekr.20230831011820.156"><vh>function: overloaded_signatures_ret_specific</vh></v>
<v t="ekr.20230831011820.157"><vh>function: warn_both_operands_are_from_unions</vh></v>
<v t="ekr.20230831011820.158"><vh>function: warn_operand_was_from_union</vh></v>
<v t="ekr.20230831011820.159"><vh>function: operator_method_signatures_overlap</vh></v>
<v t="ekr.20230831011820.160"><vh>function: forward_operator_not_callable</vh></v>
<v t="ekr.20230831011820.161"><vh>function: signatures_incompatible</vh></v>
<v t="ekr.20230831011820.162"><vh>function: yield_from_invalid_operand_type</vh></v>
<v t="ekr.20230831011820.163"><vh>function: invalid_signature</vh></v>
<v t="ekr.20230831011820.164"><vh>function: invalid_signature_for_special_method</vh></v>
<v t="ekr.20230831011820.165"><vh>function: reveal_type</vh></v>
<v t="ekr.20230831011820.166"><vh>function: reveal_locals</vh></v>
<v t="ekr.20230831011820.167"><vh>function: unsupported_type_type</vh></v>
<v t="ekr.20230831011820.168"><vh>function: redundant_cast</vh></v>
<v t="ekr.20230831011820.169"><vh>function: assert_type_fail</vh></v>
<v t="ekr.20230831011820.170"><vh>function: unimported_type_becomes_any</vh></v>
<v t="ekr.20230831011820.171"><vh>function: need_annotation_for_var</vh></v>
<v t="ekr.20230831011820.172"><vh>function: explicit_any</vh></v>
<v t="ekr.20230831011820.173"><vh>function: unsupported_target_for_star_typeddict</vh></v>
<v t="ekr.20230831011820.174"><vh>function: non_required_keys_absent_with_star</vh></v>
<v t="ekr.20230831011820.175"><vh>function: unexpected_typeddict_keys</vh></v>
<v t="ekr.20230831011820.176"><vh>function: typeddict_key_must_be_string_literal</vh></v>
<v t="ekr.20230831011820.177"><vh>function: typeddict_key_not_found</vh></v>
<v t="ekr.20230831011820.178"><vh>function: typeddict_context_ambiguous</vh></v>
<v t="ekr.20230831011820.179"><vh>function: typeddict_key_cannot_be_deleted</vh></v>
<v t="ekr.20230831011820.180"><vh>function: typeddict_setdefault_arguments_inconsistent</vh></v>
<v t="ekr.20230831011820.181"><vh>function: type_arguments_not_allowed</vh></v>
<v t="ekr.20230831011820.182"><vh>function: disallowed_any_type</vh></v>
<v t="ekr.20230831011820.183"><vh>function: incorrectly_returning_any</vh></v>
<v t="ekr.20230831011820.184"><vh>function: incorrect__exit__return</vh></v>
<v t="ekr.20230831011820.185"><vh>function: untyped_decorated_function</vh></v>
<v t="ekr.20230831011820.186"><vh>function: typed_function_untyped_decorator</vh></v>
<v t="ekr.20230831011820.187"><vh>function: bad_proto_variance</vh></v>
<v t="ekr.20230831011820.188"><vh>function: concrete_only_assign</vh></v>
<v t="ekr.20230831011820.189"><vh>function: concrete_only_call</vh></v>
<v t="ekr.20230831011820.190"><vh>function: cannot_use_function_with_type</vh></v>
<v t="ekr.20230831011820.191"><vh>function: report_non_method_protocol</vh></v>
<v t="ekr.20230831011820.192"><vh>function: note_call</vh></v>
<v t="ekr.20230831011820.193"><vh>function: unreachable_statement</vh></v>
<v t="ekr.20230831011820.194"><vh>function: redundant_left_operand</vh></v>
<v t="ekr.20230831011820.195"><vh>function: unreachable_right_operand</vh></v>
<v t="ekr.20230831011820.196"><vh>function: redundant_condition_in_comprehension</vh></v>
<v t="ekr.20230831011820.197"><vh>function: redundant_condition_in_if</vh></v>
<v t="ekr.20230831011820.198"><vh>function: redundant_expr</vh></v>
<v t="ekr.20230831011820.199"><vh>function: impossible_intersection</vh></v>
<v t="ekr.20230831011820.200"><vh>function: report_protocol_problems</vh></v>
<v t="ekr.20230831011820.201"><vh>function: pretty_overload</vh></v>
<v t="ekr.20230831011820.202"><vh>function: print_more</vh></v>
<v t="ekr.20230831011820.203"><vh>function: try_report_long_tuple_assignment_error</vh></v>
<v t="ekr.20230831011820.204"><vh>function: format_long_tuple_type</vh></v>
<v t="ekr.20230831011820.205"><vh>function: generate_incompatible_tuple_error</vh></v>
<v t="ekr.20230831011820.206"><vh>function: add_fixture_note</vh></v>
<v t="ekr.20230831011820.207"><vh>function: annotation_in_unchecked_function</vh></v>
<v t="ekr.20230831011820.208"><vh>function: quote_type_string</vh></v>
<v t="ekr.20230831011820.209"><vh>function: format_callable_args</vh></v>
<v t="ekr.20230831011820.210"><vh>function: format_type_inner</vh></v>
<v t="ekr.20230831011820.211"><vh>function: collect_all_instances</vh></v>
<v t="ekr.20230831011820.212"><vh>class CollectAllInstancesQuery</vh>
<v t="ekr.20230831011820.213"><vh>CollectAllInstancesQuery.__init__</vh></v>
<v t="ekr.20230831011820.214"><vh>CollectAllInstancesQuery.visit_instance</vh></v>
<v t="ekr.20230831011820.215"><vh>CollectAllInstancesQuery.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011820.216"><vh>function: find_type_overlaps</vh></v>
<v t="ekr.20230831011820.217"><vh>function: format_type</vh></v>
<v t="ekr.20230831011820.218"><vh>function: format_type_bare</vh></v>
<v t="ekr.20230831011820.219"><vh>function: format_type_distinctly</vh></v>
<v t="ekr.20230831011820.220"><vh>function: pretty_class_or_static_decorator</vh></v>
<v t="ekr.20230831011820.221"><vh>function: pretty_callable</vh></v>
<v t="ekr.20230831011820.222"><vh>function: variance_string</vh></v>
<v t="ekr.20230831011820.223"><vh>function: get_missing_protocol_members</vh></v>
<v t="ekr.20230831011820.224"><vh>function: get_conflict_protocol_types</vh></v>
<v t="ekr.20230831011820.225"><vh>function: get_bad_protocol_flags</vh></v>
<v t="ekr.20230831011820.226"><vh>function: capitalize</vh></v>
<v t="ekr.20230831011820.227"><vh>function: extract_type</vh></v>
<v t="ekr.20230831011820.228"><vh>function: strip_quotes</vh></v>
<v t="ekr.20230831011820.229"><vh>function: format_string_list</vh></v>
<v t="ekr.20230831011820.230"><vh>function: format_item_name_list</vh></v>
<v t="ekr.20230831011820.231"><vh>function: callable_name</vh></v>
<v t="ekr.20230831011820.232"><vh>function: for_function</vh></v>
<v t="ekr.20230831011820.233"><vh>function: wrong_type_arg_count</vh></v>
<v t="ekr.20230831011820.234"><vh>function: find_defining_module</vh></v>
<v t="ekr.20230831011820.235"><vh>function: _real_quick_ratio</vh></v>
<v t="ekr.20230831011820.236"><vh>function: best_matches</vh></v>
<v t="ekr.20230831011820.237"><vh>function: pretty_seq</vh></v>
<v t="ekr.20230831011820.238"><vh>function: append_invariance_notes</vh></v>
<v t="ekr.20230831011820.239"><vh>function: append_numbers_notes</vh></v>
<v t="ekr.20230831011820.240"><vh>function: make_inferred_type_note</vh></v>
<v t="ekr.20230831011820.241"><vh>function: format_key_list</vh></v>
</v>
<v t="ekr.20230831011820.1016"><vh>@clean report.py</vh>
<v t="ekr.20230831011820.1017"><vh>&lt;&lt; report.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1018"><vh>class Reports</vh>
<v t="ekr.20230831011820.1019"><vh>Reports.__init__</vh></v>
<v t="ekr.20230831011820.1020"><vh>Reports.add_report</vh></v>
<v t="ekr.20230831011820.1021"><vh>Reports.file</vh></v>
<v t="ekr.20230831011820.1022"><vh>Reports.finish</vh></v>
</v>
<v t="ekr.20230831011820.1023"><vh>class AbstractReporter</vh>
<v t="ekr.20230831011820.1024"><vh>AbstractReporter.__init__</vh></v>
<v t="ekr.20230831011820.1025"><vh>AbstractReporter.on_file</vh></v>
<v t="ekr.20230831011820.1026"><vh>AbstractReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1027"><vh>function: register_reporter</vh></v>
<v t="ekr.20230831011820.1028"><vh>function: alias_reporter</vh></v>
<v t="ekr.20230831011820.1029"><vh>function: should_skip_path</vh></v>
<v t="ekr.20230831011820.1030"><vh>function: iterate_python_lines</vh></v>
<v t="ekr.20230831011820.1031"><vh>class FuncCounterVisitor</vh>
<v t="ekr.20230831011820.1032"><vh>FuncCounterVisitor.__init__</vh></v>
<v t="ekr.20230831011820.1033"><vh>FuncCounterVisitor.visit_func_def</vh></v>
</v>
<v t="ekr.20230831011820.1034"><vh>class LineCountReporter</vh>
<v t="ekr.20230831011820.1035"><vh>LineCountReporter.__init__</vh></v>
<v t="ekr.20230831011820.1036"><vh>LineCountReporter.on_file</vh></v>
<v t="ekr.20230831011820.1037"><vh>LineCountReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1038"><vh>class AnyExpressionsReporter</vh>
<v t="ekr.20230831011820.1039"><vh>AnyExpressionsReporter.__init__</vh></v>
<v t="ekr.20230831011820.1040"><vh>AnyExpressionsReporter.on_file</vh></v>
<v t="ekr.20230831011820.1041"><vh>AnyExpressionsReporter.on_finish</vh></v>
<v t="ekr.20230831011820.1042"><vh>AnyExpressionsReporter._write_out_report</vh></v>
<v t="ekr.20230831011820.1043"><vh>AnyExpressionsReporter._report_any_exprs</vh></v>
<v t="ekr.20230831011820.1044"><vh>AnyExpressionsReporter._report_types_of_anys</vh></v>
</v>
<v t="ekr.20230831011820.1045"><vh>class LineCoverageVisitor</vh>
<v t="ekr.20230831011820.1046"><vh>LineCoverageVisitor.__init__</vh></v>
<v t="ekr.20230831011820.1047"><vh>LineCoverageVisitor.indentation_level</vh></v>
<v t="ekr.20230831011820.1048"><vh>LineCoverageVisitor.visit_func_def</vh></v>
</v>
<v t="ekr.20230831011820.1049"><vh>class LineCoverageReporter</vh>
<v t="ekr.20230831011820.1050"><vh>LineCoverageReporter.__init__</vh></v>
<v t="ekr.20230831011820.1051"><vh>LineCoverageReporter.on_file</vh></v>
<v t="ekr.20230831011820.1052"><vh>LineCoverageReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1053"><vh>class FileInfo</vh>
<v t="ekr.20230831011820.1054"><vh>FileInfo.__init__</vh></v>
<v t="ekr.20230831011820.1055"><vh>FileInfo.total</vh></v>
<v t="ekr.20230831011820.1056"><vh>FileInfo.attrib</vh></v>
</v>
<v t="ekr.20230831011820.1057"><vh>class MemoryXmlReporter</vh>
<v t="ekr.20230831011820.1058"><vh>MemoryXmlReporter.__init__</vh></v>
<v t="ekr.20230831011820.1059"><vh>MemoryXmlReporter.on_file</vh></v>
<v t="ekr.20230831011820.1060"><vh>MemoryXmlReporter._get_any_info_for_line</vh></v>
<v t="ekr.20230831011820.1061"><vh>MemoryXmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1062"><vh>function: get_line_rate</vh></v>
<v t="ekr.20230831011820.1063"><vh>class CoberturaPackage</vh>
<v t="ekr.20230831011820.1064"><vh>CoberturaPackage.__init__</vh></v>
<v t="ekr.20230831011820.1065"><vh>CoberturaPackage.as_xml</vh></v>
<v t="ekr.20230831011820.1066"><vh>CoberturaPackage.add_packages</vh></v>
</v>
<v t="ekr.20230831011820.1067"><vh>class CoberturaXmlReporter</vh>
<v t="ekr.20230831011820.1068"><vh>CoberturaXmlReporter.__init__</vh></v>
<v t="ekr.20230831011820.1069"><vh>CoberturaXmlReporter.on_file</vh></v>
<v t="ekr.20230831011820.1070"><vh>CoberturaXmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1071"><vh>class AbstractXmlReporter</vh>
<v t="ekr.20230831011820.1072"><vh>AbstractXmlReporter.__init__</vh></v>
</v>
<v t="ekr.20230831011820.1073"><vh>class XmlReporter</vh>
<v t="ekr.20230831011820.1074"><vh>XmlReporter.on_file</vh></v>
<v t="ekr.20230831011820.1075"><vh>XmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1076"><vh>class XsltHtmlReporter</vh>
<v t="ekr.20230831011820.1077"><vh>XsltHtmlReporter.__init__</vh></v>
<v t="ekr.20230831011820.1078"><vh>XsltHtmlReporter.on_file</vh></v>
<v t="ekr.20230831011820.1079"><vh>XsltHtmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1080"><vh>class XsltTxtReporter</vh>
<v t="ekr.20230831011820.1081"><vh>XsltTxtReporter.__init__</vh></v>
<v t="ekr.20230831011820.1082"><vh>XsltTxtReporter.on_file</vh></v>
<v t="ekr.20230831011820.1083"><vh>XsltTxtReporter.on_finish</vh></v>
</v>
<v t="ekr.20230831011820.1084"><vh>class LinePrecisionReporter</vh>
<v t="ekr.20230831011820.1085"><vh>LinePrecisionReporter.__init__</vh></v>
<v t="ekr.20230831011820.1086"><vh>LinePrecisionReporter.on_file</vh></v>
<v t="ekr.20230831011820.1087"><vh>LinePrecisionReporter.on_finish</vh></v>
</v>
</v>
</v>
<v t="ekr.20230831070204.1"><vh>--- Stubgen</vh>
<v t="ekr.20230831011820.1746"><vh>@clean stubdoc.py</vh>
<v t="ekr.20230831011820.1748"><vh>&lt;&lt; stubdoc.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1749"><vh>function: is_valid_type</vh></v>
<v t="ekr.20230831011820.1750"><vh>class ArgSig</vh>
<v t="ekr.20230831011820.1751"><vh>ArgSig.__init__</vh></v>
<v t="ekr.20230831011820.1752"><vh>ArgSig.__repr__</vh></v>
<v t="ekr.20230831011820.1753"><vh>ArgSig.__eq__</vh></v>
</v>
<v t="ekr.20230831011820.1754"><vh>class FunctionSig</vh></v>
<v t="ekr.20230831011820.1755"><vh>class DocStringParser</vh>
<v t="ekr.20230831011820.1756"><vh>DocStringParser.__init__</vh></v>
<v t="ekr.20230831011820.1757"><vh>DocStringParser.add_token</vh></v>
<v t="ekr.20230831011820.1758"><vh>DocStringParser.reset</vh></v>
<v t="ekr.20230831011820.1759"><vh>DocStringParser.get_signatures</vh></v>
</v>
<v t="ekr.20230831011820.1760"><vh>function: infer_sig_from_docstring</vh></v>
<v t="ekr.20230831011820.1761"><vh>function: infer_arg_sig_from_anon_docstring</vh></v>
<v t="ekr.20230831011820.1762"><vh>function: infer_ret_type_sig_from_docstring</vh></v>
<v t="ekr.20230831011820.1763"><vh>function: infer_ret_type_sig_from_anon_docstring</vh></v>
<v t="ekr.20230831011820.1764"><vh>function: parse_signature</vh></v>
<v t="ekr.20230831011820.1765"><vh>function: build_signature</vh></v>
<v t="ekr.20230831011820.1766"><vh>function: parse_all_signatures</vh></v>
<v t="ekr.20230831011820.1767"><vh>function: find_unique_signatures</vh></v>
<v t="ekr.20230831011820.1768"><vh>function: infer_prop_type_from_docstring</vh></v>
</v>
<v t="ekr.20230831011820.1769"><vh>@clean stubgen.py</vh>
<v t="ekr.20230831011820.1770"><vh>&lt;&lt; stubgen.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1771"><vh>&lt;&lt; stubgen.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1772"><vh>class Options</vh>
<v t="ekr.20230831011820.1773"><vh>Options.__init__</vh></v>
</v>
<v t="ekr.20230831011820.1774"><vh>class StubSource</vh>
<v t="ekr.20230831011820.1775"><vh>StubSource.__init__</vh></v>
<v t="ekr.20230831011820.1776"><vh>StubSource.module</vh></v>
<v t="ekr.20230831011820.1777"><vh>StubSource.path</vh></v>
</v>
<v t="ekr.20230831011820.1778"><vh>class AnnotationPrinter</vh>
<v t="ekr.20230831011820.1779"><vh>AnnotationPrinter.__init__</vh></v>
<v t="ekr.20230831011820.1780"><vh>AnnotationPrinter.visit_any</vh></v>
<v t="ekr.20230831011820.1781"><vh>AnnotationPrinter.visit_unbound_type</vh></v>
<v t="ekr.20230831011820.1782"><vh>AnnotationPrinter.visit_none_type</vh></v>
<v t="ekr.20230831011820.1783"><vh>AnnotationPrinter.visit_type_list</vh></v>
<v t="ekr.20230831011820.1784"><vh>AnnotationPrinter.visit_union_type</vh></v>
<v t="ekr.20230831011820.1785"><vh>AnnotationPrinter.args_str</vh></v>
</v>
<v t="ekr.20230831011820.1786"><vh>class AliasPrinter</vh>
<v t="ekr.20230831011820.1787"><vh>AliasPrinter.__init__</vh></v>
<v t="ekr.20230831011820.1788"><vh>AliasPrinter.visit_call_expr</vh></v>
<v t="ekr.20230831011820.1789"><vh>AliasPrinter.visit_name_expr</vh></v>
<v t="ekr.20230831011820.1790"><vh>AliasPrinter.visit_member_expr</vh></v>
<v t="ekr.20230831011820.1791"><vh>AliasPrinter.visit_str_expr</vh></v>
<v t="ekr.20230831011820.1792"><vh>AliasPrinter.visit_index_expr</vh></v>
<v t="ekr.20230831011820.1793"><vh>AliasPrinter.visit_tuple_expr</vh></v>
<v t="ekr.20230831011820.1794"><vh>AliasPrinter.visit_list_expr</vh></v>
<v t="ekr.20230831011820.1795"><vh>AliasPrinter.visit_dict_expr</vh></v>
<v t="ekr.20230831011820.1796"><vh>AliasPrinter.visit_ellipsis</vh></v>
<v t="ekr.20230831011820.1797"><vh>AliasPrinter.visit_op_expr</vh></v>
</v>
<v t="ekr.20230831011820.1798"><vh>class ImportTracker</vh>
<v t="ekr.20230831011820.1799"><vh>ImportTracker.__init__</vh></v>
<v t="ekr.20230831011820.1800"><vh>ImportTracker.add_import_from</vh></v>
<v t="ekr.20230831011820.1801"><vh>ImportTracker.add_import</vh></v>
<v t="ekr.20230831011820.1802"><vh>ImportTracker.require_name</vh></v>
<v t="ekr.20230831011820.1803"><vh>ImportTracker.reexport</vh></v>
<v t="ekr.20230831011820.1804"><vh>ImportTracker.import_lines</vh></v>
</v>
<v t="ekr.20230831011820.1805"><vh>function: find_defined_names</vh></v>
<v t="ekr.20230831011820.1806"><vh>class DefinitionFinder</vh>
<v t="ekr.20230831011820.1807"><vh>DefinitionFinder.__init__</vh></v>
<v t="ekr.20230831011820.1808"><vh>DefinitionFinder.visit_class_def</vh></v>
<v t="ekr.20230831011820.1809"><vh>DefinitionFinder.visit_func_def</vh></v>
</v>
<v t="ekr.20230831011820.1810"><vh>function: find_referenced_names</vh></v>
<v t="ekr.20230831011820.1811"><vh>class ReferenceFinder</vh>
<v t="ekr.20230831011820.1812"><vh>ReferenceFinder.__init__</vh></v>
<v t="ekr.20230831011820.1813"><vh>ReferenceFinder.visit_block</vh></v>
<v t="ekr.20230831011820.1814"><vh>ReferenceFinder.visit_name_expr</vh></v>
<v t="ekr.20230831011820.1815"><vh>ReferenceFinder.visit_instance</vh></v>
<v t="ekr.20230831011820.1816"><vh>ReferenceFinder.visit_unbound_type</vh></v>
<v t="ekr.20230831011820.1817"><vh>ReferenceFinder.visit_tuple_type</vh></v>
<v t="ekr.20230831011820.1818"><vh>ReferenceFinder.visit_callable_type</vh></v>
<v t="ekr.20230831011820.1819"><vh>ReferenceFinder.add_ref</vh></v>
</v>
<v t="ekr.20230831011820.1820"><vh>class StubGenerator</vh>
<v t="ekr.20230831011820.1821"><vh>StubGenerator.__init__</vh></v>
<v t="ekr.20230831011820.1822"><vh>StubGenerator.visit_mypy_file</vh></v>
<v t="ekr.20230831011820.1823"><vh>StubGenerator.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011820.1824"><vh>StubGenerator.visit_func_def</vh></v>
<v t="ekr.20230831011820.1825"><vh>StubGenerator.is_none_expr</vh></v>
<v t="ekr.20230831011820.1826"><vh>StubGenerator.visit_decorator</vh></v>
<v t="ekr.20230831011820.1827"><vh>StubGenerator.process_decorator</vh></v>
<v t="ekr.20230831011820.1828"><vh>StubGenerator.get_fullname</vh></v>
<v t="ekr.20230831011820.1829"><vh>StubGenerator.visit_class_def</vh></v>
<v t="ekr.20230831011820.1830"><vh>StubGenerator.get_base_types</vh></v>
<v t="ekr.20230831011820.1831"><vh>StubGenerator.visit_block</vh></v>
<v t="ekr.20230831011820.1832"><vh>StubGenerator.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.1833"><vh>StubGenerator.is_namedtuple</vh></v>
<v t="ekr.20230831011820.1834"><vh>StubGenerator.is_typed_namedtuple</vh></v>
<v t="ekr.20230831011820.1835"><vh>StubGenerator._get_namedtuple_fields</vh></v>
<v t="ekr.20230831011820.1836"><vh>StubGenerator.process_namedtuple</vh></v>
<v t="ekr.20230831011820.1837"><vh>StubGenerator.is_typeddict</vh></v>
<v t="ekr.20230831011820.1838"><vh>StubGenerator.process_typeddict</vh></v>
<v t="ekr.20230831011820.1839"><vh>StubGenerator.annotate_as_incomplete</vh></v>
<v t="ekr.20230831011820.1840"><vh>StubGenerator.is_alias_expression</vh></v>
<v t="ekr.20230831011820.1841"><vh>StubGenerator.process_typealias</vh></v>
<v t="ekr.20230831011820.1842"><vh>StubGenerator.visit_if_stmt</vh></v>
<v t="ekr.20230831011820.1843"><vh>StubGenerator.visit_import_all</vh></v>
<v t="ekr.20230831011820.1844"><vh>StubGenerator.visit_import_from</vh></v>
<v t="ekr.20230831011820.1845"><vh>StubGenerator.visit_import</vh></v>
<v t="ekr.20230831011820.1846"><vh>StubGenerator.get_init</vh></v>
<v t="ekr.20230831011820.1847"><vh>StubGenerator.get_assign_initializer</vh></v>
<v t="ekr.20230831011820.1848"><vh>StubGenerator.add</vh></v>
<v t="ekr.20230831011820.1849"><vh>StubGenerator.add_decorator</vh></v>
<v t="ekr.20230831011820.1850"><vh>StubGenerator.clear_decorators</vh></v>
<v t="ekr.20230831011820.1851"><vh>StubGenerator.typing_name</vh></v>
<v t="ekr.20230831011820.1852"><vh>StubGenerator.add_typing_import</vh></v>
<v t="ekr.20230831011820.1853"><vh>StubGenerator.add_import_line</vh></v>
<v t="ekr.20230831011820.1854"><vh>StubGenerator.output</vh></v>
<v t="ekr.20230831011820.1855"><vh>StubGenerator.is_not_in_all</vh></v>
<v t="ekr.20230831011820.1856"><vh>StubGenerator.is_private_name</vh></v>
<v t="ekr.20230831011820.1857"><vh>StubGenerator.is_private_member</vh></v>
<v t="ekr.20230831011820.1858"><vh>StubGenerator.get_str_type_of_node</vh></v>
<v t="ekr.20230831011820.1859"><vh>StubGenerator.maybe_unwrap_unary_expr</vh></v>
<v t="ekr.20230831011820.1860"><vh>StubGenerator.print_annotation</vh></v>
<v t="ekr.20230831011820.1861"><vh>StubGenerator.is_top_level</vh></v>
<v t="ekr.20230831011820.1862"><vh>StubGenerator.record_name</vh></v>
<v t="ekr.20230831011820.1863"><vh>StubGenerator.is_recorded_name</vh></v>
</v>
<v t="ekr.20230831011820.1864"><vh>function: find_method_names</vh></v>
<v t="ekr.20230831011820.1865"><vh>class SelfTraverser</vh>
<v t="ekr.20230831011820.1866"><vh>SelfTraverser.__init__</vh></v>
<v t="ekr.20230831011820.1867"><vh>SelfTraverser.visit_assignment_stmt</vh></v>
</v>
<v t="ekr.20230831011820.1868"><vh>function: find_self_initializers</vh></v>
<v t="ekr.20230831011820.1869"><vh>function: get_qualified_name</vh></v>
<v t="ekr.20230831011820.1870"><vh>function: remove_blacklisted_modules</vh></v>
<v t="ekr.20230831011820.1871"><vh>function: is_blacklisted_path</vh></v>
<v t="ekr.20230831011820.1872"><vh>function: normalize_path_separators</vh></v>
<v t="ekr.20230831011820.1873"><vh>function: collect_build_targets</vh></v>
<v t="ekr.20230831011820.1874"><vh>function: find_module_paths_using_imports</vh></v>
<v t="ekr.20230831011820.1875"><vh>function: is_non_library_module</vh></v>
<v t="ekr.20230831011820.1876"><vh>function: translate_module_name</vh></v>
<v t="ekr.20230831011820.1877"><vh>function: find_module_paths_using_search</vh></v>
<v t="ekr.20230831011820.1878"><vh>function: mypy_options</vh></v>
<v t="ekr.20230831011820.1879"><vh>function: parse_source_file</vh></v>
<v t="ekr.20230831011820.1880"><vh>function: generate_asts_for_modules</vh></v>
<v t="ekr.20230831011820.1881"><vh>function: generate_stub_from_ast</vh></v>
<v t="ekr.20230831011820.1882"><vh>function: get_sig_generators</vh></v>
<v t="ekr.20230831011820.1883"><vh>function: collect_docs_signatures</vh></v>
<v t="ekr.20230831011820.1884"><vh>function: generate_stubs</vh></v>
<v t="ekr.20230831011820.1885"><vh>function: parse_options</vh></v>
<v t="ekr.20230831011820.1886"><vh>function: main</vh></v>
</v>
<v t="ekr.20230831011820.1887"><vh>@clean stubgenc.py</vh>
<v t="ekr.20230831011820.1888"><vh>&lt;&lt; stubgenc.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1889"><vh>&lt;&lt; stubgenc.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1890"><vh>class SignatureGenerator</vh>
<v t="ekr.20230831011820.1891"><vh>SignatureGenerator.remove_self_type</vh></v>
<v t="ekr.20230831011820.1892"><vh>SignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20230831011820.1893"><vh>SignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20230831011820.1894"><vh>class ExternalSignatureGenerator</vh>
<v t="ekr.20230831011820.1895"><vh>ExternalSignatureGenerator.__init__</vh></v>
<v t="ekr.20230831011820.1896"><vh>ExternalSignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20230831011820.1897"><vh>ExternalSignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20230831011820.1898"><vh>class DocstringSignatureGenerator</vh>
<v t="ekr.20230831011820.1899"><vh>DocstringSignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20230831011820.1900"><vh>DocstringSignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20230831011820.1901"><vh>class FallbackSignatureGenerator</vh>
<v t="ekr.20230831011820.1902"><vh>FallbackSignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20230831011820.1903"><vh>FallbackSignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20230831011820.1904"><vh>function: generate_stub_for_c_module</vh></v>
<v t="ekr.20230831011820.1905"><vh>function: add_typing_import</vh></v>
<v t="ekr.20230831011820.1906"><vh>function: get_members</vh></v>
<v t="ekr.20230831011820.1907"><vh>function: is_c_function</vh></v>
<v t="ekr.20230831011820.1908"><vh>function: is_c_method</vh></v>
<v t="ekr.20230831011820.1909"><vh>function: is_c_classmethod</vh></v>
<v t="ekr.20230831011820.1910"><vh>function: is_c_property</vh></v>
<v t="ekr.20230831011820.1911"><vh>function: is_c_property_readonly</vh></v>
<v t="ekr.20230831011820.1912"><vh>function: is_c_type</vh></v>
<v t="ekr.20230831011820.1913"><vh>function: is_pybind11_overloaded_function_docstring</vh></v>
<v t="ekr.20230831011820.1914"><vh>function: generate_c_function_stub</vh></v>
<v t="ekr.20230831011820.1915"><vh>function: strip_or_import</vh></v>
<v t="ekr.20230831011820.1916"><vh>function: is_static_property</vh></v>
<v t="ekr.20230831011820.1917"><vh>function: generate_c_property_stub</vh></v>
<v t="ekr.20230831011820.1918"><vh>function: generate_c_type_stub</vh></v>
<v t="ekr.20230831011820.1919"><vh>function: get_type_fullname</vh></v>
<v t="ekr.20230831011820.1920"><vh>function: method_name_sort_key</vh></v>
<v t="ekr.20230831011820.1921"><vh>function: is_pybind_skipped_attribute</vh></v>
<v t="ekr.20230831011820.1922"><vh>function: is_skipped_attribute</vh></v>
<v t="ekr.20230831011820.1923"><vh>function: infer_method_args</vh></v>
<v t="ekr.20230831011820.1924"><vh>function: infer_method_ret_type</vh></v>
</v>
<v t="ekr.20230831011820.1925"><vh>@clean stubinfo.py</vh>
<v t="ekr.20230831011820.1926"><vh>&lt;&lt; stubinfo.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1927"><vh>function: is_legacy_bundled_package</vh></v>
<v t="ekr.20230831011820.1928"><vh>function: approved_stub_package_exists</vh></v>
<v t="ekr.20230831011820.1929"><vh>function: stub_distribution_name</vh></v>
</v>
<v t="ekr.20230831011820.1930"><vh>@clean stubtest.py</vh>
<v t="ekr.20230831011820.1932"><vh>&lt;&lt; stubtest.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1933"><vh>class Missing</vh>
<v t="ekr.20230831011820.1934"><vh>Missing.__repr__</vh></v>
</v>
<v t="ekr.20230831011820.1935"><vh>function: _style</vh></v>
<v t="ekr.20230831011820.1936"><vh>function: _truncate</vh></v>
<v t="ekr.20230831011820.1937"><vh>class StubtestFailure</vh></v>
<v t="ekr.20230831011820.1938"><vh>class Error</vh>
<v t="ekr.20230831011820.1939"><vh>Error.__init__</vh></v>
<v t="ekr.20230831011820.1940"><vh>Error.is_missing_stub</vh></v>
<v t="ekr.20230831011820.1941"><vh>Error.is_positional_only_related</vh></v>
<v t="ekr.20230831011820.1942"><vh>Error.get_description</vh></v>
</v>
<v t="ekr.20230831011820.1943"><vh>function: silent_import_module</vh></v>
<v t="ekr.20230831011820.1944"><vh>function: test_module</vh></v>
<v t="ekr.20230831011820.1945"><vh>function: verify</vh></v>
<v t="ekr.20230831011820.1946"><vh>function: _verify_exported_names</vh></v>
<v t="ekr.20230831011820.1947"><vh>function: _get_imported_symbol_names</vh></v>
<v t="ekr.20230831011820.1948"><vh>function: verify_mypyfile</vh></v>
<v t="ekr.20230831011820.1949"><vh>function: _verify_final</vh>
<v t="ekr.20230831011820.1950"><vh>class SubClass</vh></v>
</v>
<v t="ekr.20230831011820.1951"><vh>function: _verify_metaclass</vh></v>
<v t="ekr.20230831011820.1952"><vh>function: verify_typeinfo</vh></v>
<v t="ekr.20230831011820.1953"><vh>function: _static_lookup_runtime</vh></v>
<v t="ekr.20230831011820.1954"><vh>function: _verify_static_class_methods</vh></v>
<v t="ekr.20230831011820.1955"><vh>function: _verify_arg_name</vh></v>
<v t="ekr.20230831011820.1956"><vh>function: _verify_arg_default_value</vh></v>
<v t="ekr.20230831011820.1957"><vh>function: maybe_strip_cls</vh></v>
<v t="ekr.20230831011820.1958"><vh>class Signature</vh>
<v t="ekr.20230831011820.1959"><vh>Signature.__init__</vh></v>
<v t="ekr.20230831011820.1960"><vh>Signature.__str__</vh></v>
<v t="ekr.20230831011820.1961"><vh>Signature.from_funcitem</vh></v>
<v t="ekr.20230831011820.1962"><vh>Signature.from_inspect_signature</vh></v>
<v t="ekr.20230831011820.1963"><vh>Signature.from_overloadedfuncdef</vh></v>
</v>
<v t="ekr.20230831011820.1964"><vh>function: _verify_signature</vh></v>
<v t="ekr.20230831011820.1965"><vh>function: verify_funcitem</vh></v>
<v t="ekr.20230831011820.1966"><vh>function: verify_none</vh></v>
<v t="ekr.20230831011820.1967"><vh>function: verify_var</vh></v>
<v t="ekr.20230831011820.1968"><vh>function: verify_overloadedfuncdef</vh></v>
<v t="ekr.20230831011820.1969"><vh>function: verify_typevarexpr</vh></v>
<v t="ekr.20230831011820.1970"><vh>function: verify_paramspecexpr</vh></v>
<v t="ekr.20230831011820.1971"><vh>function: _verify_readonly_property</vh></v>
<v t="ekr.20230831011820.1972"><vh>function: _verify_abstract_status</vh></v>
<v t="ekr.20230831011820.1973"><vh>function: _verify_final_method</vh></v>
<v t="ekr.20230831011820.1974"><vh>function: _resolve_funcitem_from_decorator</vh></v>
<v t="ekr.20230831011820.1975"><vh>function: verify_decorator</vh></v>
<v t="ekr.20230831011820.1976"><vh>function: verify_typealias</vh></v>
<v t="ekr.20230831011820.1977"><vh>function: is_probably_private</vh></v>
<v t="ekr.20230831011820.1978"><vh>function: is_probably_a_function</vh></v>
<v t="ekr.20230831011820.1979"><vh>function: is_read_only_property</vh></v>
<v t="ekr.20230831011820.1980"><vh>function: safe_inspect_signature</vh></v>
<v t="ekr.20230831011820.1981"><vh>function: is_subtype_helper</vh></v>
<v t="ekr.20230831011820.1982"><vh>function: get_mypy_type_of_runtime_value</vh></v>
<v t="ekr.20230831011820.1983"><vh>function: build_stubs</vh></v>
<v t="ekr.20230831011820.1984"><vh>function: get_stub</vh></v>
<v t="ekr.20230831011820.1985"><vh>function: get_typeshed_stdlib_modules</vh></v>
<v t="ekr.20230831011820.1986"><vh>function: get_importable_stdlib_modules</vh></v>
<v t="ekr.20230831011820.1987"><vh>function: get_allowlist_entries</vh></v>
<v t="ekr.20230831011820.1988"><vh>class _Arguments</vh></v>
<v t="ekr.20230831011820.1989"><vh>function: test_stubs</vh></v>
<v t="ekr.20230831011820.1990"><vh>function: parse_options</vh></v>
<v t="ekr.20230831011820.1991"><vh>function: main</vh></v>
</v>
<v t="ekr.20230831011820.1992"><vh>@clean stubutil.py</vh>
<v t="ekr.20230831011820.1993"><vh>&lt;&lt; stubutil.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1994"><vh>class CantImport</vh>
<v t="ekr.20230831011820.1995"><vh>CantImport.__init__</vh></v>
</v>
<v t="ekr.20230831011820.1996"><vh>function: walk_packages</vh></v>
<v t="ekr.20230831011820.1997"><vh>function: find_module_path_using_sys_path</vh></v>
<v t="ekr.20230831011820.1998"><vh>function: find_module_path_and_all_py3</vh></v>
<v t="ekr.20230831011820.1999"><vh>function: generate_guarded</vh></v>
<v t="ekr.20230831011820.2000"><vh>function: report_missing</vh></v>
<v t="ekr.20230831011820.2001"><vh>function: fail_missing</vh></v>
<v t="ekr.20230831011820.2002"><vh>function: remove_misplaced_type_comments</vh></v>
<v t="ekr.20230831011820.2003"><vh>function: remove_misplaced_type_comments</vh></v>
<v t="ekr.20230831011820.2004"><vh>function: remove_misplaced_type_comments</vh></v>
<v t="ekr.20230831011820.2005"><vh>function: common_dir_prefix</vh></v>
</v>
</v>
<v t="ekr.20230831071740.1"><vh>--- Utils</vh>
<v t="ekr.20230831011819.1316"><vh>@clean fixup.py</vh>
<v t="ekr.20230831011819.1317"><vh>&lt;&lt; fixup.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1318"><vh>function: fixup_module</vh></v>
<v t="ekr.20230831011819.1319"><vh>class NodeFixer</vh>
<v t="ekr.20230831011819.1320"><vh>NodeFixer.__init__</vh></v>
<v t="ekr.20230831011819.1321"><vh>NodeFixer.visit_type_info</vh></v>
<v t="ekr.20230831011819.1322"><vh>NodeFixer.visit_symbol_table</vh></v>
<v t="ekr.20230831011819.1323"><vh>NodeFixer.visit_func_def</vh></v>
<v t="ekr.20230831011819.1324"><vh>NodeFixer.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011819.1325"><vh>NodeFixer.visit_decorator</vh></v>
<v t="ekr.20230831011819.1326"><vh>NodeFixer.visit_class_def</vh></v>
<v t="ekr.20230831011819.1327"><vh>NodeFixer.visit_type_var_expr</vh></v>
<v t="ekr.20230831011819.1328"><vh>NodeFixer.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011819.1329"><vh>NodeFixer.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011819.1330"><vh>NodeFixer.visit_var</vh></v>
<v t="ekr.20230831011819.1331"><vh>NodeFixer.visit_type_alias</vh></v>
</v>
<v t="ekr.20230831011819.1332"><vh>class TypeFixer</vh>
<v t="ekr.20230831011819.1333"><vh>TypeFixer.__init__</vh></v>
<v t="ekr.20230831011819.1334"><vh>TypeFixer.visit_instance</vh></v>
<v t="ekr.20230831011819.1335"><vh>TypeFixer.visit_type_alias_type</vh></v>
<v t="ekr.20230831011819.1336"><vh>TypeFixer.visit_any</vh></v>
<v t="ekr.20230831011819.1337"><vh>TypeFixer.visit_callable_type</vh></v>
<v t="ekr.20230831011819.1338"><vh>TypeFixer.visit_overloaded</vh></v>
<v t="ekr.20230831011819.1339"><vh>TypeFixer.visit_erased_type</vh></v>
<v t="ekr.20230831011819.1340"><vh>TypeFixer.visit_deleted_type</vh></v>
<v t="ekr.20230831011819.1341"><vh>TypeFixer.visit_none_type</vh></v>
<v t="ekr.20230831011819.1342"><vh>TypeFixer.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.1343"><vh>TypeFixer.visit_partial_type</vh></v>
<v t="ekr.20230831011819.1344"><vh>TypeFixer.visit_tuple_type</vh></v>
<v t="ekr.20230831011819.1345"><vh>TypeFixer.visit_typeddict_type</vh></v>
<v t="ekr.20230831011819.1346"><vh>TypeFixer.visit_literal_type</vh></v>
<v t="ekr.20230831011819.1347"><vh>TypeFixer.visit_type_var</vh></v>
<v t="ekr.20230831011819.1348"><vh>TypeFixer.visit_param_spec</vh></v>
<v t="ekr.20230831011819.1349"><vh>TypeFixer.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.1350"><vh>TypeFixer.visit_unpack_type</vh></v>
<v t="ekr.20230831011819.1351"><vh>TypeFixer.visit_parameters</vh></v>
<v t="ekr.20230831011819.1352"><vh>TypeFixer.visit_unbound_type</vh></v>
<v t="ekr.20230831011819.1353"><vh>TypeFixer.visit_union_type</vh></v>
<v t="ekr.20230831011819.1354"><vh>TypeFixer.visit_void</vh></v>
<v t="ekr.20230831011819.1355"><vh>TypeFixer.visit_type_type</vh></v>
</v>
<v t="ekr.20230831011819.1356"><vh>function: lookup_fully_qualified_typeinfo</vh></v>
<v t="ekr.20230831011819.1357"><vh>function: lookup_fully_qualified_alias</vh></v>
<v t="ekr.20230831011819.1358"><vh>function: missing_info</vh></v>
<v t="ekr.20230831011819.1359"><vh>function: missing_alias</vh></v>
</v>
<v t="ekr.20230831011819.1398"><vh>@clean gclogger.py</vh>
<v t="ekr.20230831011819.1399"><vh>&lt;&lt; gclogger.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1400"><vh>class GcLogger</vh>
<v t="ekr.20230831011819.1401"><vh>GcLogger.__enter__</vh></v>
<v t="ekr.20230831011819.1402"><vh>GcLogger.gc_callback</vh></v>
<v t="ekr.20230831011819.1403"><vh>GcLogger.__exit__</vh></v>
<v t="ekr.20230831011819.1404"><vh>GcLogger.get_stats</vh></v>
</v>
</v>
<v t="ekr.20230831011819.1405"><vh>@clean git.py</vh>
<v t="ekr.20230831011819.1406"><vh>&lt;&lt; git.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1407"><vh>function: is_git_repo</vh></v>
<v t="ekr.20230831011819.1408"><vh>function: have_git</vh></v>
<v t="ekr.20230831011819.1409"><vh>function: git_revision</vh></v>
<v t="ekr.20230831011819.1410"><vh>function: is_dirty</vh></v>
</v>
<v t="ekr.20230831011819.1411"><vh>@clean graph_utils.py</vh>
<v t="ekr.20230831011819.1412"><vh>&lt;&lt; graph_utils.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1413"><vh>function: strongly_connected_components</vh></v>
<v t="ekr.20230831011819.1414"><vh>function: prepare_sccs</vh></v>
<v t="ekr.20230831011819.1415"><vh>function: topsort</vh></v>
</v>
<v t="ekr.20230831011819.1488"><vh>@clean ipc.py</vh>
<v t="ekr.20230831011819.1490"><vh>&lt;&lt; ipc.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1491"><vh>class IPCException</vh></v>
<v t="ekr.20230831011819.1492"><vh>class IPCBase</vh>
<v t="ekr.20230831011819.1493"><vh>IPCBase.__init__</vh></v>
<v t="ekr.20230831011819.1494"><vh>IPCBase.read</vh></v>
<v t="ekr.20230831011819.1495"><vh>IPCBase.write</vh></v>
<v t="ekr.20230831011819.1496"><vh>IPCBase.close</vh></v>
</v>
<v t="ekr.20230831011819.1497"><vh>class IPCClient</vh>
<v t="ekr.20230831011819.1498"><vh>IPCClient.__init__</vh></v>
<v t="ekr.20230831011819.1499"><vh>IPCClient.__enter__</vh></v>
<v t="ekr.20230831011819.1500"><vh>IPCClient.__exit__</vh></v>
</v>
<v t="ekr.20230831011819.1501"><vh>class IPCServer</vh>
<v t="ekr.20230831011819.1502"><vh>IPCServer.__init__</vh></v>
<v t="ekr.20230831011819.1503"><vh>IPCServer.__enter__</vh></v>
<v t="ekr.20230831011819.1504"><vh>IPCServer.__exit__</vh></v>
<v t="ekr.20230831011819.1505"><vh>IPCServer.cleanup</vh></v>
<v t="ekr.20230831011819.1506"><vh>IPCServer.connection_name</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1598"><vh>@clean stats.py</vh>
<v t="ekr.20230831011820.1599"><vh>&lt;&lt; stats.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1600"><vh>class StatisticsVisitor</vh>
<v t="ekr.20230831011820.1601"><vh>StatisticsVisitor.__init__</vh></v>
<v t="ekr.20230831011820.1602"><vh>StatisticsVisitor.visit_mypy_file</vh></v>
<v t="ekr.20230831011820.1603"><vh>StatisticsVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011820.1604"><vh>StatisticsVisitor.visit_import_all</vh></v>
<v t="ekr.20230831011820.1605"><vh>StatisticsVisitor.process_import</vh></v>
<v t="ekr.20230831011820.1606"><vh>StatisticsVisitor.visit_import</vh></v>
<v t="ekr.20230831011820.1607"><vh>StatisticsVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011820.1608"><vh>StatisticsVisitor.enter_scope</vh></v>
<v t="ekr.20230831011820.1609"><vh>StatisticsVisitor.is_checked_scope</vh></v>
<v t="ekr.20230831011820.1610"><vh>StatisticsVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011820.1611"><vh>StatisticsVisitor.visit_type_application</vh></v>
<v t="ekr.20230831011820.1612"><vh>StatisticsVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.1613"><vh>StatisticsVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20230831011820.1614"><vh>StatisticsVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20230831011820.1615"><vh>StatisticsVisitor.visit_break_stmt</vh></v>
<v t="ekr.20230831011820.1616"><vh>StatisticsVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20230831011820.1617"><vh>StatisticsVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011820.1618"><vh>StatisticsVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011820.1619"><vh>StatisticsVisitor.visit_call_expr</vh></v>
<v t="ekr.20230831011820.1620"><vh>StatisticsVisitor.record_call_target_precision</vh></v>
<v t="ekr.20230831011820.1621"><vh>StatisticsVisitor.record_callable_target_precision</vh></v>
<v t="ekr.20230831011820.1622"><vh>StatisticsVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011820.1623"><vh>StatisticsVisitor.visit_op_expr</vh></v>
<v t="ekr.20230831011820.1624"><vh>StatisticsVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20230831011820.1625"><vh>StatisticsVisitor.visit_index_expr</vh></v>
<v t="ekr.20230831011820.1626"><vh>StatisticsVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011820.1627"><vh>StatisticsVisitor.visit_unary_expr</vh></v>
<v t="ekr.20230831011820.1628"><vh>StatisticsVisitor.visit_str_expr</vh></v>
<v t="ekr.20230831011820.1629"><vh>StatisticsVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20230831011820.1630"><vh>StatisticsVisitor.visit_int_expr</vh></v>
<v t="ekr.20230831011820.1631"><vh>StatisticsVisitor.visit_float_expr</vh></v>
<v t="ekr.20230831011820.1632"><vh>StatisticsVisitor.visit_complex_expr</vh></v>
<v t="ekr.20230831011820.1633"><vh>StatisticsVisitor.visit_ellipsis</vh></v>
<v t="ekr.20230831011820.1634"><vh>StatisticsVisitor.process_node</vh></v>
<v t="ekr.20230831011820.1635"><vh>StatisticsVisitor.record_precise_if_checked_scope</vh></v>
<v t="ekr.20230831011820.1636"><vh>StatisticsVisitor.type</vh></v>
<v t="ekr.20230831011820.1637"><vh>StatisticsVisitor.log</vh></v>
<v t="ekr.20230831011820.1638"><vh>StatisticsVisitor.record_line</vh></v>
</v>
<v t="ekr.20230831011820.1639"><vh>function: dump_type_stats</vh></v>
<v t="ekr.20230831011820.1640"><vh>function: is_special_module</vh></v>
<v t="ekr.20230831011820.1641"><vh>function: is_imprecise</vh></v>
<v t="ekr.20230831011820.1642"><vh>class HasAnyQuery</vh>
<v t="ekr.20230831011820.1643"><vh>HasAnyQuery.__init__</vh></v>
<v t="ekr.20230831011820.1644"><vh>HasAnyQuery.visit_any</vh></v>
</v>
<v t="ekr.20230831011820.1645"><vh>function: is_imprecise2</vh></v>
<v t="ekr.20230831011820.1646"><vh>class HasAnyQuery2</vh>
<v t="ekr.20230831011820.1647"><vh>HasAnyQuery2.visit_callable_type</vh></v>
</v>
<v t="ekr.20230831011820.1648"><vh>function: is_generic</vh></v>
<v t="ekr.20230831011820.1649"><vh>function: is_complex</vh></v>
<v t="ekr.20230831011820.1650"><vh>function: ensure_dir_exists</vh></v>
<v t="ekr.20230831011820.1651"><vh>function: is_special_form_any</vh></v>
<v t="ekr.20230831011820.1652"><vh>function: get_original_any</vh></v>
</v>
<v t="ekr.20230831011820.1653"><vh>@clean strconv.py</vh>
<v t="ekr.20230831011820.1654"><vh>&lt;&lt; strconv.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1655"><vh>class StrConv</vh>
<v t="ekr.20230831011820.1656"><vh>StrConv.__init__</vh></v>
<v t="ekr.20230831011820.1657"><vh>StrConv.stringify_type</vh></v>
<v t="ekr.20230831011820.1658"><vh>StrConv.get_id</vh></v>
<v t="ekr.20230831011820.1659"><vh>StrConv.format_id</vh></v>
<v t="ekr.20230831011820.1660"><vh>StrConv.dump</vh></v>
<v t="ekr.20230831011820.1661"><vh>StrConv.func_helper</vh></v>
<v t="ekr.20230831011820.1662"><vh>StrConv.visit_mypy_file</vh></v>
<v t="ekr.20230831011820.1663"><vh>StrConv.visit_import</vh></v>
<v t="ekr.20230831011820.1664"><vh>StrConv.visit_import_from</vh></v>
<v t="ekr.20230831011820.1665"><vh>StrConv.visit_import_all</vh></v>
<v t="ekr.20230831011820.1666"><vh>StrConv.visit_func_def</vh></v>
<v t="ekr.20230831011820.1667"><vh>StrConv.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011820.1668"><vh>StrConv.visit_class_def</vh></v>
<v t="ekr.20230831011820.1669"><vh>StrConv.visit_var</vh></v>
<v t="ekr.20230831011820.1670"><vh>StrConv.visit_global_decl</vh></v>
<v t="ekr.20230831011820.1671"><vh>StrConv.visit_nonlocal_decl</vh></v>
<v t="ekr.20230831011820.1672"><vh>StrConv.visit_decorator</vh></v>
<v t="ekr.20230831011820.1673"><vh>StrConv.visit_block</vh></v>
<v t="ekr.20230831011820.1674"><vh>StrConv.visit_expression_stmt</vh></v>
<v t="ekr.20230831011820.1675"><vh>StrConv.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.1676"><vh>StrConv.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011820.1677"><vh>StrConv.visit_while_stmt</vh></v>
<v t="ekr.20230831011820.1678"><vh>StrConv.visit_for_stmt</vh></v>
<v t="ekr.20230831011820.1679"><vh>StrConv.visit_return_stmt</vh></v>
<v t="ekr.20230831011820.1680"><vh>StrConv.visit_if_stmt</vh></v>
<v t="ekr.20230831011820.1681"><vh>StrConv.visit_break_stmt</vh></v>
<v t="ekr.20230831011820.1682"><vh>StrConv.visit_continue_stmt</vh></v>
<v t="ekr.20230831011820.1683"><vh>StrConv.visit_pass_stmt</vh></v>
<v t="ekr.20230831011820.1684"><vh>StrConv.visit_raise_stmt</vh></v>
<v t="ekr.20230831011820.1685"><vh>StrConv.visit_assert_stmt</vh></v>
<v t="ekr.20230831011820.1686"><vh>StrConv.visit_await_expr</vh></v>
<v t="ekr.20230831011820.1687"><vh>StrConv.visit_del_stmt</vh></v>
<v t="ekr.20230831011820.1688"><vh>StrConv.visit_try_stmt</vh></v>
<v t="ekr.20230831011820.1689"><vh>StrConv.visit_with_stmt</vh></v>
<v t="ekr.20230831011820.1690"><vh>StrConv.visit_match_stmt</vh></v>
<v t="ekr.20230831011820.1691"><vh>StrConv.visit_int_expr</vh></v>
<v t="ekr.20230831011820.1692"><vh>StrConv.visit_str_expr</vh></v>
<v t="ekr.20230831011820.1693"><vh>StrConv.visit_bytes_expr</vh></v>
<v t="ekr.20230831011820.1694"><vh>StrConv.str_repr</vh></v>
<v t="ekr.20230831011820.1695"><vh>StrConv.visit_float_expr</vh></v>
<v t="ekr.20230831011820.1696"><vh>StrConv.visit_complex_expr</vh></v>
<v t="ekr.20230831011820.1697"><vh>StrConv.visit_ellipsis</vh></v>
<v t="ekr.20230831011820.1698"><vh>StrConv.visit_star_expr</vh></v>
<v t="ekr.20230831011820.1699"><vh>StrConv.visit_name_expr</vh></v>
<v t="ekr.20230831011820.1700"><vh>StrConv.pretty_name</vh></v>
<v t="ekr.20230831011820.1701"><vh>StrConv.visit_member_expr</vh></v>
<v t="ekr.20230831011820.1702"><vh>StrConv.visit_yield_expr</vh></v>
<v t="ekr.20230831011820.1703"><vh>StrConv.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011820.1704"><vh>StrConv.visit_call_expr</vh></v>
<v t="ekr.20230831011820.1705"><vh>StrConv.visit_op_expr</vh></v>
<v t="ekr.20230831011820.1706"><vh>StrConv.visit_comparison_expr</vh></v>
<v t="ekr.20230831011820.1707"><vh>StrConv.visit_cast_expr</vh></v>
<v t="ekr.20230831011820.1708"><vh>StrConv.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011820.1709"><vh>StrConv.visit_reveal_expr</vh></v>
<v t="ekr.20230831011820.1710"><vh>StrConv.visit_assignment_expr</vh></v>
<v t="ekr.20230831011820.1711"><vh>StrConv.visit_unary_expr</vh></v>
<v t="ekr.20230831011820.1712"><vh>StrConv.visit_list_expr</vh></v>
<v t="ekr.20230831011820.1713"><vh>StrConv.visit_dict_expr</vh></v>
<v t="ekr.20230831011820.1714"><vh>StrConv.visit_set_expr</vh></v>
<v t="ekr.20230831011820.1715"><vh>StrConv.visit_tuple_expr</vh></v>
<v t="ekr.20230831011820.1716"><vh>StrConv.visit_index_expr</vh></v>
<v t="ekr.20230831011820.1717"><vh>StrConv.visit_super_expr</vh></v>
<v t="ekr.20230831011820.1718"><vh>StrConv.visit_type_application</vh></v>
<v t="ekr.20230831011820.1719"><vh>StrConv.visit_type_var_expr</vh></v>
<v t="ekr.20230831011820.1720"><vh>StrConv.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011820.1721"><vh>StrConv.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011820.1722"><vh>StrConv.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011820.1723"><vh>StrConv.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011820.1724"><vh>StrConv.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011820.1725"><vh>StrConv.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011820.1726"><vh>StrConv.visit__promote_expr</vh></v>
<v t="ekr.20230831011820.1727"><vh>StrConv.visit_newtype_expr</vh></v>
<v t="ekr.20230831011820.1728"><vh>StrConv.visit_lambda_expr</vh></v>
<v t="ekr.20230831011820.1729"><vh>StrConv.visit_generator_expr</vh></v>
<v t="ekr.20230831011820.1730"><vh>StrConv.visit_list_comprehension</vh></v>
<v t="ekr.20230831011820.1731"><vh>StrConv.visit_set_comprehension</vh></v>
<v t="ekr.20230831011820.1732"><vh>StrConv.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011820.1733"><vh>StrConv.visit_conditional_expr</vh></v>
<v t="ekr.20230831011820.1734"><vh>StrConv.visit_slice_expr</vh></v>
<v t="ekr.20230831011820.1735"><vh>StrConv.visit_temp_node</vh></v>
<v t="ekr.20230831011820.1736"><vh>StrConv.visit_as_pattern</vh></v>
<v t="ekr.20230831011820.1737"><vh>StrConv.visit_or_pattern</vh></v>
<v t="ekr.20230831011820.1738"><vh>StrConv.visit_value_pattern</vh></v>
<v t="ekr.20230831011820.1739"><vh>StrConv.visit_singleton_pattern</vh></v>
<v t="ekr.20230831011820.1740"><vh>StrConv.visit_sequence_pattern</vh></v>
<v t="ekr.20230831011820.1741"><vh>StrConv.visit_starred_pattern</vh></v>
<v t="ekr.20230831011820.1742"><vh>StrConv.visit_mapping_pattern</vh></v>
<v t="ekr.20230831011820.1743"><vh>StrConv.visit_class_pattern</vh></v>
</v>
<v t="ekr.20230831011820.1744"><vh>function: dump_tagged</vh></v>
<v t="ekr.20230831011820.1745"><vh>function: indent</vh></v>
</v>
<v t="ekr.20230831011821.1014"><vh>@clean util.py</vh>
<v t="ekr.20230831011821.1015"><vh>&lt;&lt; util.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.1016"><vh>function: is_dunder</vh></v>
<v t="ekr.20230831011821.1017"><vh>function: is_sunder</vh></v>
<v t="ekr.20230831011821.1018"><vh>function: split_module_names</vh></v>
<v t="ekr.20230831011821.1019"><vh>function: module_prefix</vh></v>
<v t="ekr.20230831011821.1020"><vh>function: split_target</vh></v>
<v t="ekr.20230831011821.1021"><vh>function: short_type</vh></v>
<v t="ekr.20230831011821.1022"><vh>function: find_python_encoding</vh></v>
<v t="ekr.20230831011821.1023"><vh>function: bytes_to_human_readable_repr</vh></v>
<v t="ekr.20230831011821.1024"><vh>class DecodeError</vh></v>
<v t="ekr.20230831011821.1025"><vh>function: decode_python_encoding</vh></v>
<v t="ekr.20230831011821.1026"><vh>function: read_py_file</vh></v>
<v t="ekr.20230831011821.1027"><vh>function: trim_source_line</vh></v>
<v t="ekr.20230831011821.1028"><vh>function: get_mypy_comments</vh></v>
<v t="ekr.20230831011821.1029"><vh>function: write_junit_xml</vh></v>
<v t="ekr.20230831011821.1030"><vh>class IdMapper</vh>
<v t="ekr.20230831011821.1031"><vh>IdMapper.__init__</vh></v>
<v t="ekr.20230831011821.1032"><vh>IdMapper.id</vh></v>
</v>
<v t="ekr.20230831011821.1033"><vh>function: get_prefix</vh></v>
<v t="ekr.20230831011821.1034"><vh>function: correct_relative_import</vh></v>
<v t="ekr.20230831011821.1035"><vh>function: get_class_descriptors</vh></v>
<v t="ekr.20230831011821.1036"><vh>function: replace_object_state</vh></v>
<v t="ekr.20230831011821.1037"><vh>function: is_sub_path</vh></v>
<v t="ekr.20230831011821.1038"><vh>function: hard_exit</vh></v>
<v t="ekr.20230831011821.1039"><vh>function: unmangle</vh></v>
<v t="ekr.20230831011821.1040"><vh>function: get_unique_redefinition_name</vh></v>
<v t="ekr.20230831011821.1041"><vh>function: check_python_version</vh></v>
<v t="ekr.20230831011821.1042"><vh>function: count_stats</vh></v>
<v t="ekr.20230831011821.1043"><vh>function: split_words</vh></v>
<v t="ekr.20230831011821.1044"><vh>function: get_terminal_width</vh></v>
<v t="ekr.20230831011821.1045"><vh>function: soft_wrap</vh></v>
<v t="ekr.20230831011821.1046"><vh>function: hash_digest</vh></v>
<v t="ekr.20230831011821.1047"><vh>function: parse_gray_color</vh></v>
<v t="ekr.20230831011821.1048"><vh>function: should_force_color</vh></v>
<v t="ekr.20230831011821.1049"><vh>class FancyFormatter</vh>
<v t="ekr.20230831011821.1050"><vh>FancyFormatter.__init__</vh></v>
<v t="ekr.20230831011821.1051"><vh>FancyFormatter.initialize_vt100_colors</vh></v>
<v t="ekr.20230831011821.1052"><vh>FancyFormatter.initialize_win_colors</vh></v>
<v t="ekr.20230831011821.1053"><vh>FancyFormatter.initialize_unix_colors</vh></v>
<v t="ekr.20230831011821.1054"><vh>FancyFormatter.style</vh></v>
<v t="ekr.20230831011821.1055"><vh>FancyFormatter.fit_in_terminal</vh></v>
<v t="ekr.20230831011821.1056"><vh>FancyFormatter.colorize</vh></v>
<v t="ekr.20230831011821.1057"><vh>FancyFormatter.highlight_quote_groups</vh></v>
<v t="ekr.20230831011821.1058"><vh>FancyFormatter.underline_link</vh></v>
<v t="ekr.20230831011821.1059"><vh>FancyFormatter.format_success</vh></v>
<v t="ekr.20230831011821.1060"><vh>FancyFormatter.format_error</vh></v>
</v>
<v t="ekr.20230831011821.1061"><vh>function: is_typeshed_file</vh></v>
<v t="ekr.20230831011821.1062"><vh>function: is_stub_package_file</vh></v>
<v t="ekr.20230831011821.1063"><vh>function: unnamed_function</vh></v>
<v t="ekr.20230831011821.1064"><vh>function: time_spent_us</vh></v>
<v t="ekr.20230831011821.1065"><vh>function: plural_s</vh></v>
<v t="ekr.20230831011821.1066"><vh>function: quote_docstring</vh></v>
</v>
</v>
</v>
<v t="ekr.20230831070114.1"><vh>--- Semantic analysis</vh>
<v t="ekr.20230831011820.1106"><vh>@clean semanal.py</vh>
<v t="ekr.20230902050513.1"><vh>&lt;&lt; semanal.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1108"><vh>&lt;&lt; semanal.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1109"><vh>class SemanticAnalyzer</vh></v>
<v t="ekr.20230831011820.1110"><vh>function: __init__</vh></v>
<v t="ekr.20230831011820.1111"><vh>function: type</vh></v>
<v t="ekr.20230831011820.1112"><vh>function: is_stub_file</vh></v>
<v t="ekr.20230831011820.1113"><vh>function: is_typeshed_stub_file</vh></v>
<v t="ekr.20230831011820.1114"><vh>function: final_iteration</vh></v>
<v t="ekr.20230831011820.1115"><vh>function: allow_unbound_tvars_set</vh></v>
<v t="ekr.20230831011820.1116"><vh>function: prepare_file</vh></v>
<v t="ekr.20230831011820.1117"><vh>function: prepare_typing_namespace</vh></v>
<v t="ekr.20230831011820.1118"><vh>function: prepare_builtins_namespace</vh></v>
<v t="ekr.20230831011820.1119"><vh>function: refresh_partial</vh></v>
<v t="ekr.20230831011820.1120"><vh>function: refresh_top_level</vh></v>
<v t="ekr.20230831011820.1121"><vh>function: add_implicit_module_attrs</vh></v>
<v t="ekr.20230831011820.1122"><vh>function: add_builtin_aliases</vh></v>
<v t="ekr.20230831011820.1123"><vh>function: add_typing_extension_aliases</vh></v>
<v t="ekr.20230831011820.1124"><vh>function: create_alias</vh></v>
<v t="ekr.20230831011820.1125"><vh>function: adjust_public_exports</vh></v>
<v t="ekr.20230831011820.1126"><vh>function: file_context</vh></v>
<v t="ekr.20230831011820.1127"><vh>function: visit_func_def</vh></v>
<v t="ekr.20230831011820.1128"><vh>function: analyze_func_def</vh></v>
<v t="ekr.20230831011820.1129"><vh>function: remove_unpack_kwargs</vh></v>
<v t="ekr.20230831011820.1130"><vh>function: prepare_method_signature</vh></v>
<v t="ekr.20230831011820.1131"><vh>function: is_expected_self_type</vh></v>
<v t="ekr.20230831011820.1132"><vh>function: set_original_def</vh></v>
<v t="ekr.20230831011820.1133"><vh>function: update_function_type_variables</vh></v>
<v t="ekr.20230831011820.1134"><vh>function: setup_self_type</vh></v>
<v t="ekr.20230831011820.1135"><vh>function: visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011820.1136"><vh>function: analyze_overloaded_func_def</vh></v>
<v t="ekr.20230831011820.1137"><vh>function: process_overload_impl</vh></v>
<v t="ekr.20230831011820.1138"><vh>function: analyze_overload_sigs_and_impl</vh></v>
<v t="ekr.20230831011820.1139"><vh>function: handle_missing_overload_decorators</vh></v>
<v t="ekr.20230831011820.1140"><vh>function: handle_missing_overload_implementation</vh></v>
<v t="ekr.20230831011820.1141"><vh>function: process_final_in_overload</vh></v>
<v t="ekr.20230831011820.1142"><vh>function: process_static_or_class_method_in_overload</vh></v>
<v t="ekr.20230831011820.1143"><vh>function: analyze_property_with_multi_part_definition</vh></v>
<v t="ekr.20230831011820.1144"><vh>function: add_function_to_symbol_table</vh></v>
<v t="ekr.20230831011820.1145"><vh>function: analyze_arg_initializers</vh></v>
<v t="ekr.20230831011820.1146"><vh>function: analyze_function_body</vh></v>
<v t="ekr.20230831011820.1147"><vh>function: check_classvar_in_signature</vh></v>
<v t="ekr.20230831011820.1148"><vh>function: check_function_signature</vh></v>
<v t="ekr.20230831011820.1149"><vh>function: check_paramspec_definition</vh></v>
<v t="ekr.20230831011820.1150"><vh>function: visit_decorator</vh></v>
<v t="ekr.20230831011820.1151"><vh>function: check_decorated_function_is_method</vh></v>
<v t="ekr.20230831011820.1152"><vh>function: visit_class_def</vh></v>
<v t="ekr.20230831011820.1153"><vh>function: analyze_class</vh></v>
<v t="ekr.20230831011820.1154"><vh>function: setup_type_vars</vh></v>
<v t="ekr.20230831011820.1155"><vh>function: setup_alias_type_vars</vh></v>
<v t="ekr.20230831011820.1156"><vh>function: is_core_builtin_class</vh></v>
<v t="ekr.20230831011820.1157"><vh>function: analyze_class_body_common</vh></v>
<v t="ekr.20230831011820.1158"><vh>function: analyze_typeddict_classdef</vh></v>
<v t="ekr.20230831011820.1159"><vh>function: analyze_namedtuple_classdef</vh></v>
<v t="ekr.20230831011820.1160"><vh>function: apply_class_plugin_hooks</vh></v>
<v t="ekr.20230831011820.1161"><vh>function: get_fullname_for_hook</vh></v>
<v t="ekr.20230831011820.1162"><vh>function: analyze_class_keywords</vh></v>
<v t="ekr.20230831011820.1163"><vh>function: enter_class</vh></v>
<v t="ekr.20230831011820.1164"><vh>function: leave_class</vh></v>
<v t="ekr.20230831011820.1165"><vh>function: analyze_class_decorator</vh></v>
<v t="ekr.20230831011820.1166"><vh>function: clean_up_bases_and_infer_type_variables</vh></v>
<v t="ekr.20230831011820.1167"><vh>function: analyze_class_typevar_declaration</vh></v>
<v t="ekr.20230831011820.1168"><vh>function: analyze_unbound_tvar</vh></v>
<v t="ekr.20230831011820.1169"><vh>function: get_all_bases_tvars</vh></v>
<v t="ekr.20230831011820.1170"><vh>function: get_and_bind_all_tvars</vh></v>
<v t="ekr.20230831011820.1171"><vh>function: prepare_class_def</vh></v>
<v t="ekr.20230831011820.1172"><vh>function: make_empty_type_info</vh></v>
<v t="ekr.20230831011820.1173"><vh>function: get_name_repr_of_expr</vh></v>
<v t="ekr.20230831011820.1174"><vh>function: analyze_base_classes</vh></v>
<v t="ekr.20230831011820.1175"><vh>function: configure_base_classes</vh></v>
<v t="ekr.20230831011820.1176"><vh>function: configure_tuple_base_class</vh></v>
<v t="ekr.20230831011820.1177"><vh>function: set_dummy_mro</vh></v>
<v t="ekr.20230831011820.1178"><vh>function: set_any_mro</vh></v>
<v t="ekr.20230831011820.1179"><vh>function: calculate_class_mro</vh></v>
<v t="ekr.20230831011820.1180"><vh>function: infer_metaclass_and_bases_from_compat_helpers</vh></v>
<v t="ekr.20230831011820.1181"><vh>function: verify_base_classes</vh></v>
<v t="ekr.20230831011820.1182"><vh>function: verify_duplicate_base_classes</vh></v>
<v t="ekr.20230831011820.1183"><vh>function: is_base_class</vh></v>
<v t="ekr.20230831011820.1184"><vh>function: get_declared_metaclass</vh></v>
<v t="ekr.20230831011820.1185"><vh>function: recalculate_metaclass</vh></v>
<v t="ekr.20230831011820.1186"><vh>function: visit_import</vh></v>
<v t="ekr.20230831011820.1187"><vh>function: visit_import_from</vh></v>
<v t="ekr.20230831011820.1188"><vh>function: process_imported_symbol</vh></v>
<v t="ekr.20230831011820.1189"><vh>function: report_missing_module_attribute</vh></v>
<v t="ekr.20230831011820.1190"><vh>function: process_import_over_existing_name</vh></v>
<v t="ekr.20230831011820.1191"><vh>function: correct_relative_import</vh></v>
<v t="ekr.20230831011820.1192"><vh>function: visit_import_all</vh></v>
<v t="ekr.20230831011820.1193"><vh>function: visit_assignment_expr</vh></v>
<v t="ekr.20230831011820.1194"><vh>function: check_valid_comprehension</vh></v>
<v t="ekr.20230831011820.1195"><vh>function: visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.1196"><vh>function: analyze_identity_global_assignment</vh></v>
<v t="ekr.20230831011820.1197"><vh>function: should_wait_rhs</vh></v>
<v t="ekr.20230831011820.1198"><vh>function: can_be_type_alias</vh></v>
<v t="ekr.20230831011820.1199"><vh>function: can_possibly_be_type_form</vh></v>
<v t="ekr.20230831011820.1200"><vh>function: is_type_ref</vh></v>
<v t="ekr.20230831011820.1201"><vh>function: is_none_alias</vh></v>
<v t="ekr.20230831011820.1202"><vh>function: record_special_form_lvalue</vh></v>
<v t="ekr.20230831011820.1203"><vh>function: analyze_enum_assign</vh></v>
<v t="ekr.20230831011820.1204"><vh>function: analyze_namedtuple_assign</vh></v>
<v t="ekr.20230831011820.1205"><vh>function: analyze_typeddict_assign</vh></v>
<v t="ekr.20230831011820.1206"><vh>function: analyze_lvalues</vh></v>
<v t="ekr.20230831011820.1207"><vh>function: apply_dynamic_class_hook</vh></v>
<v t="ekr.20230831011820.1208"><vh>function: unwrap_final</vh></v>
<v t="ekr.20230831011820.1209"><vh>function: check_final_implicit_def</vh></v>
<v t="ekr.20230831011820.1210"><vh>function: store_final_status</vh></v>
<v t="ekr.20230831011820.1211"><vh>function: flatten_lvalues</vh></v>
<v t="ekr.20230831011820.1212"><vh>function: process_type_annotation</vh></v>
<v t="ekr.20230831011820.1213"><vh>function: is_annotated_protocol_member</vh></v>
<v t="ekr.20230831011820.1214"><vh>function: analyze_simple_literal_type</vh></v>
<v t="ekr.20230831011820.1215"><vh>function: analyze_alias</vh></v>
<v t="ekr.20230831011820.1216"><vh>function: is_pep_613</vh></v>
<v t="ekr.20230831011820.1217"><vh>function: check_and_set_up_type_alias</vh></v>
<v t="ekr.20230831011820.1218"><vh>function: disable_invalid_recursive_aliases</vh></v>
<v t="ekr.20230831011820.1219"><vh>function: analyze_lvalue</vh></v>
<v t="ekr.20230831011820.1220"><vh>function: analyze_name_lvalue</vh></v>
<v t="ekr.20230831011820.1221"><vh>function: is_final_redefinition</vh></v>
<v t="ekr.20230831011820.1222"><vh>function: is_alias_for_final_name</vh></v>
<v t="ekr.20230831011820.1223"><vh>function: make_name_lvalue_var</vh></v>
<v t="ekr.20230831011820.1224"><vh>function: make_name_lvalue_point_to_existing_def</vh></v>
<v t="ekr.20230831011820.1225"><vh>function: analyze_tuple_or_list_lvalue</vh></v>
<v t="ekr.20230831011820.1226"><vh>function: analyze_member_lvalue</vh></v>
<v t="ekr.20230831011820.1227"><vh>function: is_self_member_ref</vh></v>
<v t="ekr.20230831011820.1228"><vh>function: check_lvalue_validity</vh></v>
<v t="ekr.20230831011820.1229"><vh>function: store_declared_types</vh></v>
<v t="ekr.20230831011820.1230"><vh>function: process_typevar_declaration</vh></v>
<v t="ekr.20230831011820.1231"><vh>function: check_typevarlike_name</vh></v>
<v t="ekr.20230831011820.1232"><vh>function: get_typevarlike_declaration</vh></v>
<v t="ekr.20230831011820.1233"><vh>function: process_typevar_parameters</vh></v>
<v t="ekr.20230831011820.1234"><vh>function: get_typevarlike_argument</vh></v>
<v t="ekr.20230831011820.1235"><vh>function: extract_typevarlike_name</vh></v>
<v t="ekr.20230831011820.1236"><vh>function: process_paramspec_declaration</vh></v>
<v t="ekr.20230831011820.1237"><vh>function: process_typevartuple_declaration</vh></v>
<v t="ekr.20230831011820.1238"><vh>function: basic_new_typeinfo</vh></v>
<v t="ekr.20230831011820.1239"><vh>function: analyze_value_types</vh></v>
<v t="ekr.20230831011820.1240"><vh>function: check_classvar</vh></v>
<v t="ekr.20230831011820.1241"><vh>function: is_classvar</vh></v>
<v t="ekr.20230831011820.1242"><vh>function: is_final_type</vh></v>
<v t="ekr.20230831011820.1243"><vh>function: fail_invalid_classvar</vh></v>
<v t="ekr.20230831011820.1244"><vh>function: process_module_assignment</vh></v>
<v t="ekr.20230831011820.1245"><vh>function: process__all__</vh></v>
<v t="ekr.20230831011820.1246"><vh>function: process__deletable__</vh></v>
<v t="ekr.20230831011820.1247"><vh>function: process__slots__</vh></v>
<v t="ekr.20230831011820.1248"><vh>function: visit_block</vh></v>
<v t="ekr.20230831011820.1249"><vh>function: visit_block_maybe</vh></v>
<v t="ekr.20230831011820.1250"><vh>function: visit_expression_stmt</vh></v>
<v t="ekr.20230831011820.1251"><vh>function: visit_return_stmt</vh></v>
<v t="ekr.20230831011820.1252"><vh>function: visit_raise_stmt</vh></v>
<v t="ekr.20230831011820.1253"><vh>function: visit_assert_stmt</vh></v>
<v t="ekr.20230831011820.1254"><vh>function: visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011820.1255"><vh>function: visit_while_stmt</vh></v>
<v t="ekr.20230831011820.1256"><vh>function: visit_for_stmt</vh></v>
<v t="ekr.20230831011820.1257"><vh>function: visit_break_stmt</vh></v>
<v t="ekr.20230831011820.1258"><vh>function: visit_continue_stmt</vh></v>
<v t="ekr.20230831011820.1259"><vh>function: visit_if_stmt</vh></v>
<v t="ekr.20230831011820.1260"><vh>function: visit_try_stmt</vh></v>
<v t="ekr.20230831011820.1261"><vh>function: analyze_try_stmt</vh></v>
<v t="ekr.20230831011820.1262"><vh>function: visit_with_stmt</vh></v>
<v t="ekr.20230831011820.1263"><vh>function: visit_del_stmt</vh></v>
<v t="ekr.20230831011820.1264"><vh>function: is_valid_del_target</vh></v>
<v t="ekr.20230831011820.1265"><vh>function: visit_global_decl</vh></v>
<v t="ekr.20230831011820.1266"><vh>function: visit_nonlocal_decl</vh></v>
<v t="ekr.20230831011820.1267"><vh>function: visit_match_stmt</vh></v>
<v t="ekr.20230831011820.1268"><vh>function: visit_name_expr</vh></v>
<v t="ekr.20230831011820.1269"><vh>function: bind_name_expr</vh></v>
<v t="ekr.20230831011820.1270"><vh>function: visit_super_expr</vh></v>
<v t="ekr.20230831011820.1271"><vh>function: visit_tuple_expr</vh></v>
<v t="ekr.20230831011820.1272"><vh>function: visit_list_expr</vh></v>
<v t="ekr.20230831011820.1273"><vh>function: visit_set_expr</vh></v>
<v t="ekr.20230831011820.1274"><vh>function: visit_dict_expr</vh></v>
<v t="ekr.20230831011820.1275"><vh>function: visit_star_expr</vh></v>
<v t="ekr.20230831011820.1276"><vh>function: visit_yield_from_expr</vh></v>
<v t="ekr.20230831011820.1277"><vh>function: visit_call_expr</vh></v>
<v t="ekr.20230831011820.1278"><vh>function: translate_dict_call</vh></v>
<v t="ekr.20230831011820.1279"><vh>function: check_fixed_args</vh></v>
<v t="ekr.20230831011820.1280"><vh>function: visit_member_expr</vh></v>
<v t="ekr.20230831011820.1281"><vh>function: visit_op_expr</vh></v>
<v t="ekr.20230831011820.1282"><vh>function: visit_comparison_expr</vh></v>
<v t="ekr.20230831011820.1283"><vh>function: visit_unary_expr</vh></v>
<v t="ekr.20230831011820.1284"><vh>function: visit_index_expr</vh></v>
<v t="ekr.20230831011820.1285"><vh>function: analyze_type_application</vh></v>
<v t="ekr.20230831011820.1286"><vh>function: analyze_type_application_args</vh></v>
<v t="ekr.20230831011820.1287"><vh>function: visit_slice_expr</vh></v>
<v t="ekr.20230831011820.1288"><vh>function: visit_cast_expr</vh></v>
<v t="ekr.20230831011820.1289"><vh>function: visit_assert_type_expr</vh></v>
<v t="ekr.20230831011820.1290"><vh>function: visit_reveal_expr</vh></v>
<v t="ekr.20230831011820.1291"><vh>function: visit_type_application</vh></v>
<v t="ekr.20230831011820.1292"><vh>function: visit_list_comprehension</vh></v>
<v t="ekr.20230831011820.1293"><vh>function: visit_set_comprehension</vh></v>
<v t="ekr.20230831011820.1294"><vh>function: visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011820.1295"><vh>function: visit_generator_expr</vh></v>
<v t="ekr.20230831011820.1296"><vh>function: analyze_comp_for</vh></v>
<v t="ekr.20230831011820.1297"><vh>function: analyze_comp_for_2</vh></v>
<v t="ekr.20230831011820.1298"><vh>function: visit_lambda_expr</vh></v>
<v t="ekr.20230831011820.1299"><vh>function: visit_conditional_expr</vh></v>
<v t="ekr.20230831011820.1300"><vh>function: visit__promote_expr</vh></v>
<v t="ekr.20230831011820.1301"><vh>function: visit_yield_expr</vh></v>
<v t="ekr.20230831011820.1302"><vh>function: visit_await_expr</vh></v>
<v t="ekr.20230831011820.1303"><vh>function: visit_as_pattern</vh></v>
<v t="ekr.20230831011820.1304"><vh>function: visit_or_pattern</vh></v>
<v t="ekr.20230831011820.1305"><vh>function: visit_value_pattern</vh></v>
<v t="ekr.20230831011820.1306"><vh>function: visit_sequence_pattern</vh></v>
<v t="ekr.20230831011820.1307"><vh>function: visit_starred_pattern</vh></v>
<v t="ekr.20230831011820.1308"><vh>function: visit_mapping_pattern</vh></v>
<v t="ekr.20230831011820.1309"><vh>function: visit_class_pattern</vh></v>
<v t="ekr.20230831011820.1310"><vh>function: lookup</vh></v>
<v t="ekr.20230831011820.1311"><vh>function: is_active_symbol_in_class_body</vh></v>
<v t="ekr.20230831011820.1312"><vh>function: is_textually_before_statement</vh></v>
<v t="ekr.20230831011820.1313"><vh>function: is_overloaded_item</vh></v>
<v t="ekr.20230831011820.1314"><vh>function: is_defined_in_current_module</vh></v>
<v t="ekr.20230831011820.1315"><vh>function: lookup_qualified</vh></v>
<v t="ekr.20230831011820.1316"><vh>function: lookup_type_node</vh></v>
<v t="ekr.20230831011820.1317"><vh>function: get_module_symbol</vh></v>
<v t="ekr.20230831011820.1318"><vh>function: is_missing_module</vh></v>
<v t="ekr.20230831011820.1319"><vh>function: implicit_symbol</vh></v>
<v t="ekr.20230831011820.1320"><vh>function: create_getattr_var</vh></v>
<v t="ekr.20230831011820.1321"><vh>function: lookup_fully_qualified</vh></v>
<v t="ekr.20230831011820.1322"><vh>function: lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20230831011820.1323"><vh>function: object_type</vh></v>
<v t="ekr.20230831011820.1324"><vh>function: str_type</vh></v>
<v t="ekr.20230831011820.1325"><vh>function: named_type</vh></v>
<v t="ekr.20230831011820.1326"><vh>function: named_type_or_none</vh></v>
<v t="ekr.20230831011820.1327"><vh>function: builtin_type</vh></v>
<v t="ekr.20230831011820.1328"><vh>function: lookup_current_scope</vh></v>
<v t="ekr.20230831011820.1329"><vh>function: add_symbol</vh></v>
<v t="ekr.20230831011820.1330"><vh>function: add_symbol_skip_local</vh></v>
<v t="ekr.20230831011820.1331"><vh>function: add_symbol_table_node</vh></v>
<v t="ekr.20230831011820.1332"><vh>function: add_redefinition</vh></v>
<v t="ekr.20230831011820.1333"><vh>function: add_local</vh></v>
<v t="ekr.20230831011820.1334"><vh>function: _get_node_for_class_scoped_import</vh></v>
<v t="ekr.20230831011820.1335"><vh>function: add_imported_symbol</vh></v>
<v t="ekr.20230831011820.1336"><vh>function: add_unknown_imported_symbol</vh></v>
<v t="ekr.20230831011820.1337"><vh>function: tvar_scope_frame</vh></v>
<v t="ekr.20230831011820.1338"><vh>function: defer</vh></v>
<v t="ekr.20230831011820.1339"><vh>function: track_incomplete_refs</vh></v>
<v t="ekr.20230831011820.1340"><vh>function: found_incomplete_ref</vh></v>
<v t="ekr.20230831011820.1341"><vh>function: record_incomplete_ref</vh></v>
<v t="ekr.20230831011820.1342"><vh>function: mark_incomplete</vh></v>
<v t="ekr.20230831011820.1343"><vh>function: is_incomplete_namespace</vh></v>
<v t="ekr.20230831011820.1344"><vh>function: process_placeholder</vh></v>
<v t="ekr.20230831011820.1345"><vh>function: cannot_resolve_name</vh></v>
<v t="ekr.20230831011820.1346"><vh>function: qualified_name</vh></v>
<v t="ekr.20230831011820.1347"><vh>function: enter</vh></v>
<v t="ekr.20230831011820.1348"><vh>function: is_func_scope</vh></v>
<v t="ekr.20230831011820.1349"><vh>function: is_nested_within_func_scope</vh></v>
<v t="ekr.20230831011820.1350"><vh>function: is_class_scope</vh></v>
<v t="ekr.20230831011820.1351"><vh>function: is_module_scope</vh></v>
<v t="ekr.20230831011820.1352"><vh>function: current_symbol_kind</vh></v>
<v t="ekr.20230831011820.1353"><vh>function: current_symbol_table</vh></v>
<v t="ekr.20230831011820.1354"><vh>function: is_global_or_nonlocal</vh></v>
<v t="ekr.20230831011820.1355"><vh>function: add_exports</vh></v>
<v t="ekr.20230831011820.1356"><vh>function: name_not_defined</vh></v>
<v t="ekr.20230831011820.1357"><vh>function: already_defined</vh></v>
<v t="ekr.20230831011820.1358"><vh>function: name_already_defined</vh></v>
<v t="ekr.20230831011820.1359"><vh>function: attribute_already_defined</vh></v>
<v t="ekr.20230831011820.1360"><vh>function: is_local_name</vh></v>
<v t="ekr.20230831011820.1361"><vh>function: in_checked_function</vh></v>
<v t="ekr.20230831011820.1362"><vh>function: fail</vh></v>
<v t="ekr.20230831011820.1363"><vh>function: note</vh></v>
<v t="ekr.20230831011820.1364"><vh>function: incomplete_feature_enabled</vh></v>
<v t="ekr.20230831011820.1365"><vh>function: accept</vh></v>
<v t="ekr.20230831011820.1366"><vh>function: expr_to_analyzed_type</vh></v>
<v t="ekr.20230831011820.1367"><vh>function: analyze_type_expr</vh></v>
<v t="ekr.20230831011820.1368"><vh>function: type_analyzer</vh></v>
<v t="ekr.20230831011820.1369"><vh>function: expr_to_unanalyzed_type</vh></v>
<v t="ekr.20230831011820.1370"><vh>function: anal_type</vh></v>
<v t="ekr.20230831011820.1371"><vh>function: class_type</vh></v>
<v t="ekr.20230831011820.1372"><vh>function: schedule_patch</vh></v>
<v t="ekr.20230831011820.1373"><vh>function: report_hang</vh></v>
<v t="ekr.20230831011820.1374"><vh>function: add_plugin_dependency</vh></v>
<v t="ekr.20230831011820.1375"><vh>function: add_type_alias_deps</vh></v>
<v t="ekr.20230831011820.1376"><vh>function: is_mangled_global</vh></v>
<v t="ekr.20230831011820.1377"><vh>function: is_initial_mangled_global</vh></v>
<v t="ekr.20230831011820.1378"><vh>function: parse_bool</vh></v>
<v t="ekr.20230831011820.1379"><vh>function: parse_str_literal</vh></v>
<v t="ekr.20230831011820.1380"><vh>function: set_future_import_flags</vh></v>
<v t="ekr.20230831011820.1381"><vh>function: is_future_flag_set</vh></v>
<v t="ekr.20230831011820.1382"><vh>function: parse_dataclass_transform_spec</vh></v>
<v t="ekr.20230831011820.1383"><vh>function: parse_dataclass_transform_field_specifiers</vh></v>
<v t="ekr.20230831011820.1384"><vh>function: replace_implicit_first_type</vh></v>
<v t="ekr.20230831011820.1385"><vh>function: refers_to_fullname</vh></v>
<v t="ekr.20230831011820.1386"><vh>function: refers_to_class_or_function</vh></v>
<v t="ekr.20230831011820.1387"><vh>function: find_duplicate</vh></v>
<v t="ekr.20230831011820.1388"><vh>function: remove_imported_names_from_symtable</vh></v>
<v t="ekr.20230831011820.1389"><vh>function: make_any_non_explicit</vh></v>
<v t="ekr.20230831011820.1390"><vh>class MakeAnyNonExplicit</vh>
<v t="ekr.20230831011820.1391"><vh>MakeAnyNonExplicit.visit_any</vh></v>
<v t="ekr.20230831011820.1392"><vh>MakeAnyNonExplicit.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011820.1393"><vh>function: apply_semantic_analyzer_patches</vh></v>
<v t="ekr.20230831011820.1394"><vh>function: names_modified_by_assignment</vh></v>
<v t="ekr.20230831011820.1395"><vh>function: names_modified_in_lvalue</vh></v>
<v t="ekr.20230831011820.1396"><vh>function: is_same_var_from_getattr</vh></v>
<v t="ekr.20230831011820.1397"><vh>function: dummy_context</vh></v>
<v t="ekr.20230831011820.1398"><vh>function: is_valid_replacement</vh></v>
<v t="ekr.20230831011820.1399"><vh>function: is_same_symbol</vh></v>
<v t="ekr.20230831011820.1400"><vh>function: is_trivial_body</vh></v>
</v>
<v t="ekr.20230831011820.1401"><vh>@clean semanal_classprop.py</vh>
<v t="ekr.20230831011820.1403"><vh>&lt;&lt; semanal_classprop.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1404"><vh>function: calculate_class_abstract_status</vh></v>
<v t="ekr.20230831011820.1405"><vh>function: check_protocol_status</vh></v>
<v t="ekr.20230831011820.1406"><vh>function: calculate_class_vars</vh></v>
<v t="ekr.20230831011820.1407"><vh>function: add_type_promotion</vh></v>
</v>
<v t="ekr.20230831011820.1408"><vh>@clean semanal_enum.py</vh>
<v t="ekr.20230831011820.1410"><vh>&lt;&lt; semanal_enum.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1411"><vh>class EnumCallAnalyzer</vh>
<v t="ekr.20230831011820.1412"><vh>EnumCallAnalyzer.__init__</vh></v>
<v t="ekr.20230831011820.1413"><vh>EnumCallAnalyzer.process_enum_call</vh></v>
<v t="ekr.20230831011820.1414"><vh>EnumCallAnalyzer.check_enum_call</vh></v>
<v t="ekr.20230831011820.1415"><vh>EnumCallAnalyzer.build_enum_call_typeinfo</vh></v>
<v t="ekr.20230831011820.1416"><vh>EnumCallAnalyzer.parse_enum_call_args</vh></v>
<v t="ekr.20230831011820.1417"><vh>EnumCallAnalyzer.fail_enum_call_arg</vh></v>
<v t="ekr.20230831011820.1418"><vh>EnumCallAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1419"><vh>@clean semanal_infer.py</vh>
<v t="ekr.20230831011820.1420"><vh>&lt;&lt; semanal_infer.py: imports &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1421"><vh>function: infer_decorator_signature_if_simple</vh></v>
<v t="ekr.20230831011820.1422"><vh>function: is_identity_signature</vh></v>
<v t="ekr.20230831011820.1423"><vh>function: calculate_return_type</vh></v>
<v t="ekr.20230831011820.1424"><vh>function: find_fixed_callable_return</vh></v>
</v>
<v t="ekr.20230831011820.1425"><vh>@clean semanal_main.py</vh>
<v t="ekr.20230902062700.1"><vh>&lt;&lt; semanal_main.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1427"><vh>&lt;&lt; semanal_main.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1428"><vh>function: semantic_analysis_for_scc</vh></v>
<v t="ekr.20230831011820.1429"><vh>function: cleanup_builtin_scc</vh></v>
<v t="ekr.20230831011820.1430"><vh>function: semantic_analysis_for_targets</vh></v>
<v t="ekr.20230831011820.1431"><vh>function: restore_saved_attrs</vh></v>
<v t="ekr.20230831011820.1432"><vh>function: process_top_levels</vh></v>
<v t="ekr.20230831011820.1433"><vh>function: process_functions</vh></v>
<v t="ekr.20230831011820.1434"><vh>function: process_top_level_function</vh></v>
<v t="ekr.20230831011820.1435"><vh>function: get_all_leaf_targets</vh></v>
<v t="ekr.20230831011820.1436"><vh>function: semantic_analyze_target</vh></v>
<v t="ekr.20230831011820.1437"><vh>function: check_type_arguments</vh></v>
<v t="ekr.20230831011820.1438"><vh>function: check_type_arguments_in_targets</vh></v>
<v t="ekr.20230831011820.1439"><vh>function: apply_class_plugin_hooks</vh></v>
<v t="ekr.20230831011820.1440"><vh>function: apply_hooks_to_class</vh></v>
<v t="ekr.20230831011820.1441"><vh>function: calculate_class_properties</vh></v>
<v t="ekr.20230831011820.1442"><vh>function: check_blockers</vh></v>
</v>
<v t="ekr.20230831011820.1443"><vh>@clean semanal_namedtuple.py</vh>
<v t="ekr.20230831011820.1445"><vh>&lt;&lt; semanal_namedtuple.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1446"><vh>class NamedTupleAnalyzer</vh>
<v t="ekr.20230831011820.1447"><vh>NamedTupleAnalyzer.__init__</vh></v>
<v t="ekr.20230831011820.1448"><vh>NamedTupleAnalyzer.analyze_namedtuple_classdef</vh></v>
<v t="ekr.20230831011820.1449"><vh>NamedTupleAnalyzer.check_namedtuple_classdef</vh></v>
<v t="ekr.20230831011820.1450"><vh>NamedTupleAnalyzer.check_namedtuple</vh></v>
<v t="ekr.20230831011820.1451"><vh>NamedTupleAnalyzer.store_namedtuple_info</vh></v>
<v t="ekr.20230831011820.1452"><vh>NamedTupleAnalyzer.parse_namedtuple_args</vh></v>
<v t="ekr.20230831011820.1453"><vh>NamedTupleAnalyzer.parse_namedtuple_fields_with_types</vh></v>
<v t="ekr.20230831011820.1454"><vh>NamedTupleAnalyzer.build_namedtuple_typeinfo</vh></v>
<v t="ekr.20230831011820.1455"><vh>NamedTupleAnalyzer.save_namedtuple_body</vh></v>
<v t="ekr.20230831011820.1456"><vh>NamedTupleAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1457"><vh>@clean semanal_newtype.py</vh>
<v t="ekr.20230831011820.1459"><vh>&lt;&lt; semanal_newtype.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1460"><vh>class NewTypeAnalyzer</vh>
<v t="ekr.20230831011820.1461"><vh>NewTypeAnalyzer.__init__</vh></v>
<v t="ekr.20230831011820.1462"><vh>NewTypeAnalyzer.process_newtype_declaration</vh></v>
<v t="ekr.20230831011820.1463"><vh>NewTypeAnalyzer.analyze_newtype_declaration</vh></v>
<v t="ekr.20230831011820.1464"><vh>NewTypeAnalyzer.check_newtype_args</vh></v>
<v t="ekr.20230831011820.1465"><vh>NewTypeAnalyzer.build_newtype_typeinfo</vh></v>
<v t="ekr.20230831011820.1466"><vh>NewTypeAnalyzer.make_argument</vh></v>
<v t="ekr.20230831011820.1467"><vh>NewTypeAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1468"><vh>@clean semanal_pass1.py</vh>
<v t="ekr.20230831011820.1469"><vh>&lt;&lt; semanal_pass1.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1470"><vh>class SemanticAnalyzerPreAnalysis</vh>
<v t="ekr.20230831011820.1471"><vh>SemanticAnalyzerPreAnalysis.visit_file</vh></v>
<v t="ekr.20230831011820.1472"><vh>SemanticAnalyzerPreAnalysis.visit_func_def</vh></v>
<v t="ekr.20230831011820.1473"><vh>SemanticAnalyzerPreAnalysis.visit_class_def</vh></v>
<v t="ekr.20230831011820.1474"><vh>SemanticAnalyzerPreAnalysis.visit_import_from</vh></v>
<v t="ekr.20230831011820.1475"><vh>SemanticAnalyzerPreAnalysis.visit_import_all</vh></v>
<v t="ekr.20230831011820.1476"><vh>SemanticAnalyzerPreAnalysis.visit_import</vh></v>
<v t="ekr.20230831011820.1477"><vh>SemanticAnalyzerPreAnalysis.visit_if_stmt</vh></v>
<v t="ekr.20230831011820.1478"><vh>SemanticAnalyzerPreAnalysis.visit_block</vh></v>
<v t="ekr.20230831011820.1479"><vh>SemanticAnalyzerPreAnalysis.visit_match_stmt</vh></v>
<v t="ekr.20230831011820.1480"><vh>SemanticAnalyzerPreAnalysis.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.1481"><vh>SemanticAnalyzerPreAnalysis.visit_expression_stmt</vh></v>
<v t="ekr.20230831011820.1482"><vh>SemanticAnalyzerPreAnalysis.visit_return_stmt</vh></v>
<v t="ekr.20230831011820.1483"><vh>SemanticAnalyzerPreAnalysis.visit_for_stmt</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1484"><vh>@clean semanal_shared.py</vh>
<v t="ekr.20230831011820.1485"><vh>&lt;&lt; semanal_shared.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1486"><vh>class SemanticAnalyzerCoreInterface</vh>
<v t="ekr.20230831011820.1487"><vh>SemanticAnalyzerCoreInterface.lookup_qualified</vh></v>
<v t="ekr.20230831011820.1488"><vh>SemanticAnalyzerCoreInterface.lookup_fully_qualified</vh></v>
<v t="ekr.20230831011820.1489"><vh>SemanticAnalyzerCoreInterface.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20230831011820.1490"><vh>SemanticAnalyzerCoreInterface.fail</vh></v>
<v t="ekr.20230831011820.1491"><vh>SemanticAnalyzerCoreInterface.note</vh></v>
<v t="ekr.20230831011820.1492"><vh>SemanticAnalyzerCoreInterface.incomplete_feature_enabled</vh></v>
<v t="ekr.20230831011820.1493"><vh>SemanticAnalyzerCoreInterface.record_incomplete_ref</vh></v>
<v t="ekr.20230831011820.1494"><vh>SemanticAnalyzerCoreInterface.defer</vh></v>
<v t="ekr.20230831011820.1495"><vh>SemanticAnalyzerCoreInterface.is_incomplete_namespace</vh></v>
<v t="ekr.20230831011820.1496"><vh>SemanticAnalyzerCoreInterface.final_iteration</vh></v>
<v t="ekr.20230831011820.1497"><vh>SemanticAnalyzerCoreInterface.is_future_flag_set</vh></v>
<v t="ekr.20230831011820.1498"><vh>SemanticAnalyzerCoreInterface.is_stub_file</vh></v>
<v t="ekr.20230831011820.1499"><vh>SemanticAnalyzerCoreInterface.is_func_scope</vh></v>
<v t="ekr.20230831011820.1500"><vh>SemanticAnalyzerCoreInterface.type</vh></v>
</v>
<v t="ekr.20230831011820.1501"><vh>class SemanticAnalyzerInterface</vh>
<v t="ekr.20230831011820.1502"><vh>SemanticAnalyzerInterface.lookup</vh></v>
<v t="ekr.20230831011820.1503"><vh>SemanticAnalyzerInterface.named_type</vh></v>
<v t="ekr.20230831011820.1504"><vh>SemanticAnalyzerInterface.named_type_or_none</vh></v>
<v t="ekr.20230831011820.1505"><vh>SemanticAnalyzerInterface.accept</vh></v>
<v t="ekr.20230831011820.1506"><vh>SemanticAnalyzerInterface.anal_type</vh></v>
<v t="ekr.20230831011820.1507"><vh>SemanticAnalyzerInterface.get_and_bind_all_tvars</vh></v>
<v t="ekr.20230831011820.1508"><vh>SemanticAnalyzerInterface.basic_new_typeinfo</vh></v>
<v t="ekr.20230831011820.1509"><vh>SemanticAnalyzerInterface.schedule_patch</vh></v>
<v t="ekr.20230831011820.1510"><vh>SemanticAnalyzerInterface.add_symbol_table_node</vh></v>
<v t="ekr.20230831011820.1511"><vh>SemanticAnalyzerInterface.current_symbol_table</vh></v>
<v t="ekr.20230831011820.1512"><vh>SemanticAnalyzerInterface.add_symbol</vh></v>
<v t="ekr.20230831011820.1513"><vh>SemanticAnalyzerInterface.add_symbol_skip_local</vh></v>
<v t="ekr.20230831011820.1514"><vh>SemanticAnalyzerInterface.parse_bool</vh></v>
<v t="ekr.20230831011820.1515"><vh>SemanticAnalyzerInterface.qualified_name</vh></v>
<v t="ekr.20230831011820.1516"><vh>SemanticAnalyzerInterface.is_typeshed_stub_file</vh></v>
<v t="ekr.20230831011820.1517"><vh>SemanticAnalyzerInterface.process_placeholder</vh></v>
</v>
<v t="ekr.20230831011820.1518"><vh>function: set_callable_name</vh></v>
<v t="ekr.20230831011820.1519"><vh>function: calculate_tuple_fallback</vh></v>
<v t="ekr.20230831011820.1520"><vh>class _NamedTypeCallback</vh>
<v t="ekr.20230831011820.1521"><vh>_NamedTypeCallback.__call__</vh></v>
</v>
<v t="ekr.20230831011820.1522"><vh>function: paramspec_args</vh></v>
<v t="ekr.20230831011820.1523"><vh>function: paramspec_kwargs</vh></v>
<v t="ekr.20230831011820.1524"><vh>class HasPlaceholders</vh>
<v t="ekr.20230831011820.1525"><vh>HasPlaceholders.__init__</vh></v>
<v t="ekr.20230831011820.1526"><vh>HasPlaceholders.visit_placeholder_type</vh></v>
</v>
<v t="ekr.20230831011820.1527"><vh>function: has_placeholder</vh></v>
<v t="ekr.20230831011820.1528"><vh>function: find_dataclass_transform_spec</vh></v>
<v t="ekr.20230831011820.1529"><vh>function: require_bool_literal_argument</vh></v>
<v t="ekr.20230831011820.1530"><vh>function: require_bool_literal_argument</vh></v>
<v t="ekr.20230831011820.1531"><vh>function: require_bool_literal_argument</vh></v>
<v t="ekr.20230831011820.1532"><vh>function: parse_bool</vh></v>
</v>
<v t="ekr.20230831011820.1533"><vh>@clean semanal_typeargs.py</vh>
<v t="ekr.20230831011820.1535"><vh>&lt;&lt; semanal_typeargs.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1536"><vh>class TypeArgumentAnalyzer</vh>
<v t="ekr.20230831011820.1537"><vh>TypeArgumentAnalyzer.__init__</vh></v>
<v t="ekr.20230831011820.1538"><vh>TypeArgumentAnalyzer.visit_mypy_file</vh></v>
<v t="ekr.20230831011820.1539"><vh>TypeArgumentAnalyzer.visit_func</vh></v>
<v t="ekr.20230831011820.1540"><vh>TypeArgumentAnalyzer.visit_class_def</vh></v>
<v t="ekr.20230831011820.1541"><vh>TypeArgumentAnalyzer.visit_block</vh></v>
<v t="ekr.20230831011820.1542"><vh>TypeArgumentAnalyzer.visit_type_alias_type</vh></v>
<v t="ekr.20230831011820.1543"><vh>TypeArgumentAnalyzer.visit_tuple_type</vh></v>
<v t="ekr.20230831011820.1544"><vh>TypeArgumentAnalyzer.visit_callable_type</vh></v>
<v t="ekr.20230831011820.1545"><vh>TypeArgumentAnalyzer.visit_instance</vh></v>
<v t="ekr.20230831011820.1546"><vh>TypeArgumentAnalyzer.validate_args</vh></v>
<v t="ekr.20230831011820.1547"><vh>TypeArgumentAnalyzer.visit_unpack_type</vh></v>
<v t="ekr.20230831011820.1548"><vh>TypeArgumentAnalyzer.check_type_var_values</vh></v>
<v t="ekr.20230831011820.1549"><vh>TypeArgumentAnalyzer.fail</vh></v>
<v t="ekr.20230831011820.1550"><vh>TypeArgumentAnalyzer.note</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1551"><vh>@clean semanal_typeddict.py</vh>
<v t="ekr.20230831011820.1552"><vh>&lt;&lt; semanal_typeddict.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1553"><vh>class TypedDictAnalyzer</vh>
<v t="ekr.20230831011820.1554"><vh>TypedDictAnalyzer.__init__</vh></v>
<v t="ekr.20230831011820.1555"><vh>TypedDictAnalyzer.analyze_typeddict_classdef</vh></v>
<v t="ekr.20230831011820.1556"><vh>TypedDictAnalyzer.add_keys_and_types_from_base</vh></v>
<v t="ekr.20230831011820.1557"><vh>TypedDictAnalyzer.analyze_base_args</vh></v>
<v t="ekr.20230831011820.1558"><vh>TypedDictAnalyzer.map_items_to_base</vh></v>
<v t="ekr.20230831011820.1559"><vh>TypedDictAnalyzer.analyze_typeddict_classdef_fields</vh></v>
<v t="ekr.20230831011820.1560"><vh>TypedDictAnalyzer.check_typeddict</vh></v>
<v t="ekr.20230831011820.1561"><vh>TypedDictAnalyzer.parse_typeddict_args</vh></v>
<v t="ekr.20230831011820.1562"><vh>TypedDictAnalyzer.parse_typeddict_fields_with_types</vh></v>
<v t="ekr.20230831011820.1563"><vh>TypedDictAnalyzer.fail_typeddict_arg</vh></v>
<v t="ekr.20230831011820.1564"><vh>TypedDictAnalyzer.build_typeddict_typeinfo</vh></v>
<v t="ekr.20230831011820.1565"><vh>TypedDictAnalyzer.is_typeddict</vh></v>
<v t="ekr.20230831011820.1566"><vh>TypedDictAnalyzer.fail</vh></v>
<v t="ekr.20230831011820.1567"><vh>TypedDictAnalyzer.note</vh></v>
</v>
</v>
</v>
<v t="ekr.20230831071046.1"><vh>--- Top-level, build, parse</vh>
<v t="ekr.20230831011819.3"><vh>@clean __init__.py</vh></v>
<v t="ekr.20230831011819.4"><vh>@clean __main__.py</vh>
<v t="ekr.20230831011819.5"><vh>&lt;&lt; __main__.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.6"><vh>function: console_entry</vh></v>
</v>
<v t="ekr.20230831011819.7"><vh>@clean api.py</vh>
<v t="ekr.20230902063100.1"><vh>&lt;&lt; api.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011819.9"><vh>&lt;&lt; api.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.10"><vh>function: _run</vh></v>
<v t="ekr.20230831011819.11"><vh>function: run</vh></v>
<v t="ekr.20230831011819.12"><vh>function: run_dmypy</vh></v>
</v>
<v t="ekr.20230831011819.57"><vh>@clean build.py</vh>
<v t="ekr.20230831011819.59"><vh>&lt;&lt; build.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.60"><vh>class BuildResult</vh>
<v t="ekr.20230831011819.61"><vh>BuildResult.__init__</vh></v>
</v>
<v t="ekr.20230831011819.62"><vh>function: build</vh></v>
<v t="ekr.20230831011819.63"><vh>function: _build</vh></v>
<v t="ekr.20230831011819.64"><vh>function: default_data_dir</vh></v>
<v t="ekr.20230831011819.65"><vh>function: normpath</vh></v>
<v t="ekr.20230831011819.66"><vh>class CacheMeta</vh></v>
<v t="ekr.20230831011819.67"><vh>class FgDepMeta</vh></v>
<v t="ekr.20230831011819.68"><vh>function: cache_meta_from_dict</vh></v>
<v t="ekr.20230831011819.69"><vh>function: import_priority</vh></v>
<v t="ekr.20230831011819.70"><vh>function: load_plugins_from_config</vh></v>
<v t="ekr.20230831011819.71"><vh>function: load_plugins</vh></v>
<v t="ekr.20230831011819.72"><vh>function: take_module_snapshot</vh></v>
<v t="ekr.20230831011819.73"><vh>function: find_config_file_line_number</vh></v>
<v t="ekr.20230831011819.74"><vh>class BuildManager</vh>
<v t="ekr.20230831011819.75"><vh>BuildManager.__init__</vh></v>
<v t="ekr.20230831011819.76"><vh>BuildManager.dump_stats</vh></v>
<v t="ekr.20230831011819.77"><vh>BuildManager.use_fine_grained_cache</vh></v>
<v t="ekr.20230831011819.78"><vh>BuildManager.maybe_swap_for_shadow_path</vh></v>
<v t="ekr.20230831011819.79"><vh>BuildManager.get_stat</vh></v>
<v t="ekr.20230831011819.80"><vh>BuildManager.getmtime</vh></v>
<v t="ekr.20230831011819.81"><vh>BuildManager.all_imported_modules_in_file</vh></v>
<v t="ekr.20230831011819.82"><vh>BuildManager.is_module</vh></v>
<v t="ekr.20230831011819.83"><vh>BuildManager.parse_file</vh></v>
<v t="ekr.20230831011819.84"><vh>BuildManager.load_fine_grained_deps</vh></v>
<v t="ekr.20230831011819.85"><vh>BuildManager.report_file</vh></v>
<v t="ekr.20230831011819.86"><vh>BuildManager.verbosity</vh></v>
<v t="ekr.20230831011819.87"><vh>BuildManager.log</vh></v>
<v t="ekr.20230831011819.88"><vh>BuildManager.log_fine_grained</vh></v>
<v t="ekr.20230831011819.89"><vh>BuildManager.trace</vh></v>
<v t="ekr.20230831011819.90"><vh>BuildManager.add_stats</vh></v>
<v t="ekr.20230831011819.91"><vh>BuildManager.stats_summary</vh></v>
</v>
<v t="ekr.20230831011819.92"><vh>function: deps_to_json</vh></v>
<v t="ekr.20230831011819.93"><vh>function: write_deps_cache</vh></v>
<v t="ekr.20230831011819.94"><vh>function: invert_deps</vh></v>
<v t="ekr.20230831011819.95"><vh>function: generate_deps_for_cache</vh></v>
<v t="ekr.20230831011819.96"><vh>function: write_plugins_snapshot</vh></v>
<v t="ekr.20230831011819.97"><vh>function: read_plugins_snapshot</vh></v>
<v t="ekr.20230831011819.98"><vh>function: read_quickstart_file</vh></v>
<v t="ekr.20230831011819.99"><vh>function: read_deps_cache</vh></v>
<v t="ekr.20230831011819.100"><vh>function: _load_json_file</vh></v>
<v t="ekr.20230831011819.101"><vh>function: _cache_dir_prefix</vh></v>
<v t="ekr.20230831011819.102"><vh>function: add_catch_all_gitignore</vh></v>
<v t="ekr.20230831011819.103"><vh>function: exclude_from_backups</vh></v>
<v t="ekr.20230831011819.104"><vh>function: create_metastore</vh></v>
<v t="ekr.20230831011819.105"><vh>function: get_cache_names</vh></v>
<v t="ekr.20230831011819.106"><vh>function: find_cache_meta</vh></v>
<v t="ekr.20230831011819.107"><vh>function: validate_meta</vh></v>
<v t="ekr.20230831011819.108"><vh>function: compute_hash</vh></v>
<v t="ekr.20230831011819.109"><vh>function: json_dumps</vh></v>
<v t="ekr.20230831011819.110"><vh>function: write_cache</vh></v>
<v t="ekr.20230831011819.111"><vh>function: delete_cache</vh></v>
<v t="ekr.20230831011819.112"><vh>class ModuleNotFound</vh></v>
<v t="ekr.20230831011819.113"><vh>class State</vh>
<v t="ekr.20230831011819.114"><vh>State.__init__</vh></v>
<v t="ekr.20230831011819.115"><vh>State.xmeta</vh></v>
<v t="ekr.20230831011819.116"><vh>State.add_ancestors</vh></v>
<v t="ekr.20230831011819.117"><vh>State.is_fresh</vh></v>
<v t="ekr.20230831011819.118"><vh>State.is_interface_fresh</vh></v>
<v t="ekr.20230831011819.119"><vh>State.mark_as_rechecked</vh></v>
<v t="ekr.20230831011819.120"><vh>State.mark_interface_stale</vh></v>
<v t="ekr.20230831011819.121"><vh>State.check_blockers</vh></v>
<v t="ekr.20230831011819.122"><vh>State.wrap_context</vh></v>
<v t="ekr.20230831011819.123"><vh>State.load_fine_grained_deps</vh></v>
<v t="ekr.20230831011819.124"><vh>State.load_tree</vh></v>
<v t="ekr.20230831011819.125"><vh>State.fix_cross_refs</vh></v>
<v t="ekr.20230831011819.126"><vh>State.parse_file</vh></v>
<v t="ekr.20230831011819.127"><vh>State.parse_inline_configuration</vh></v>
<v t="ekr.20230831011819.128"><vh>State.semantic_analysis_pass1</vh></v>
<v t="ekr.20230831011819.129"><vh>State.add_dependency</vh></v>
<v t="ekr.20230831011819.130"><vh>State.suppress_dependency</vh></v>
<v t="ekr.20230831011819.131"><vh>State.compute_dependencies</vh></v>
<v t="ekr.20230831011819.132"><vh>State.type_check_first_pass</vh></v>
<v t="ekr.20230831011819.133"><vh>State.type_checker</vh></v>
<v t="ekr.20230831011819.134"><vh>State.type_map</vh></v>
<v t="ekr.20230831011819.135"><vh>State.type_check_second_pass</vh></v>
<v t="ekr.20230831011819.136"><vh>State.detect_possibly_undefined_vars</vh></v>
<v t="ekr.20230831011819.137"><vh>State.finish_passes</vh></v>
<v t="ekr.20230831011819.138"><vh>State.free_state</vh></v>
<v t="ekr.20230831011819.139"><vh>State._patch_indirect_dependencies</vh></v>
<v t="ekr.20230831011819.140"><vh>State.compute_fine_grained_deps</vh></v>
<v t="ekr.20230831011819.141"><vh>State.update_fine_grained_deps</vh></v>
<v t="ekr.20230831011819.142"><vh>State.valid_references</vh></v>
<v t="ekr.20230831011819.143"><vh>State.write_cache</vh></v>
<v t="ekr.20230831011819.144"><vh>State.verify_dependencies</vh></v>
<v t="ekr.20230831011819.145"><vh>State.dependency_priorities</vh></v>
<v t="ekr.20230831011819.146"><vh>State.dependency_lines</vh></v>
<v t="ekr.20230831011819.147"><vh>State.generate_unused_ignore_notes</vh></v>
<v t="ekr.20230831011819.148"><vh>State.generate_ignore_without_code_notes</vh></v>
</v>
<v t="ekr.20230831011819.149"><vh>function: find_module_and_diagnose</vh></v>
<v t="ekr.20230831011819.150"><vh>function: exist_added_packages</vh></v>
<v t="ekr.20230831011819.151"><vh>function: find_module_simple</vh></v>
<v t="ekr.20230831011819.152"><vh>function: find_module_with_reason</vh></v>
<v t="ekr.20230831011819.153"><vh>function: in_partial_package</vh></v>
<v t="ekr.20230831011819.154"><vh>function: module_not_found</vh></v>
<v t="ekr.20230831011819.155"><vh>function: skipping_module</vh></v>
<v t="ekr.20230831011819.156"><vh>function: skipping_ancestor</vh></v>
<v t="ekr.20230831011819.157"><vh>function: log_configuration</vh></v>
<v t="ekr.20230831011819.158"><vh>function: dispatch</vh></v>
<v t="ekr.20230831011819.159"><vh>class NodeInfo</vh>
<v t="ekr.20230831011819.160"><vh>NodeInfo.__init__</vh></v>
<v t="ekr.20230831011819.161"><vh>NodeInfo.dumps</vh></v>
</v>
<v t="ekr.20230831011819.162"><vh>function: dump_timing_stats</vh></v>
<v t="ekr.20230831011819.163"><vh>function: dump_line_checking_stats</vh></v>
<v t="ekr.20230831011819.164"><vh>function: dump_graph</vh></v>
<v t="ekr.20230831011819.165"><vh>function: load_graph</vh></v>
<v t="ekr.20230831011819.166"><vh>function: process_graph</vh></v>
<v t="ekr.20230831011819.167"><vh>function: order_ascc</vh></v>
<v t="ekr.20230831011819.168"><vh>function: process_fresh_modules</vh></v>
<v t="ekr.20230831011819.169"><vh>function: process_stale_scc</vh></v>
<v t="ekr.20230831011819.170"><vh>function: sorted_components</vh></v>
<v t="ekr.20230831011819.171"><vh>function: deps_filtered</vh></v>
<v t="ekr.20230831011819.172"><vh>function: missing_stubs_file</vh></v>
<v t="ekr.20230831011819.173"><vh>function: record_missing_stub_packages</vh></v>
<v t="ekr.20230831011819.174"><vh>function: is_silent_import_module</vh></v>
<v t="ekr.20230831011819.175"><vh>function: write_undocumented_ref_info</vh></v>
</v>
<v t="ekr.20230831011819.787"><vh>@clean config_parser.py</vh>
<v t="ekr.20230831011819.788"><vh>&lt;&lt; config_parser.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.789"><vh>function: parse_version</vh></v>
<v t="ekr.20230831011819.790"><vh>function: try_split</vh></v>
<v t="ekr.20230831011819.791"><vh>function: validate_codes</vh></v>
<v t="ekr.20230831011819.792"><vh>function: validate_package_allow_list</vh></v>
<v t="ekr.20230831011819.793"><vh>function: expand_path</vh></v>
<v t="ekr.20230831011819.794"><vh>function: str_or_array_as_list</vh></v>
<v t="ekr.20230831011819.795"><vh>function: split_and_match_files_list</vh></v>
<v t="ekr.20230831011819.796"><vh>function: split_and_match_files</vh></v>
<v t="ekr.20230831011819.797"><vh>function: check_follow_imports</vh></v>
<v t="ekr.20230831011819.798"><vh>function: split_commas</vh></v>
<v t="ekr.20230831011819.799"><vh>function: parse_config_file</vh></v>
<v t="ekr.20230831011819.800"><vh>function: get_prefix</vh></v>
<v t="ekr.20230831011819.801"><vh>function: is_toml</vh></v>
<v t="ekr.20230831011819.802"><vh>function: destructure_overrides</vh></v>
<v t="ekr.20230831011819.803"><vh>function: parse_section</vh></v>
<v t="ekr.20230831011819.804"><vh>function: convert_to_boolean</vh></v>
<v t="ekr.20230831011819.805"><vh>function: split_directive</vh></v>
<v t="ekr.20230831011819.806"><vh>function: mypy_comments_to_config_map</vh></v>
<v t="ekr.20230831011819.807"><vh>function: parse_mypy_comments</vh></v>
<v t="ekr.20230831011819.808"><vh>function: get_config_module_names</vh></v>
<v t="ekr.20230831011819.809"><vh>class ConfigTOMLValueError</vh></v>
</v>
<v t="ekr.20230831011819.904"><vh>@clean defaults.py</vh></v>
<v t="ekr.20230831011819.1157"><vh>@clean fastparse.py</vh>
<v t="ekr.20230831011819.1158"><vh>&lt;&lt; fastparse.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1159"><vh>function: ast3_parse</vh></v>
<v t="ekr.20230831011819.1160"><vh>function: parse</vh></v>
<v t="ekr.20230831011819.1161"><vh>function: parse_type_ignore_tag</vh></v>
<v t="ekr.20230831011819.1162"><vh>function: parse_type_comment</vh></v>
<v t="ekr.20230831011819.1163"><vh>function: parse_type_string</vh></v>
<v t="ekr.20230831011819.1164"><vh>function: is_no_type_check_decorator</vh></v>
<v t="ekr.20230831011819.1165"><vh>class ASTConverter</vh>
<v t="ekr.20230831011819.1166"><vh>ASTConverter.__init__</vh></v>
<v t="ekr.20230831011819.1167"><vh>ASTConverter.note</vh></v>
<v t="ekr.20230831011819.1168"><vh>ASTConverter.fail</vh></v>
<v t="ekr.20230831011819.1169"><vh>ASTConverter.fail_merge_overload</vh></v>
<v t="ekr.20230831011819.1170"><vh>ASTConverter.visit</vh></v>
<v t="ekr.20230831011819.1171"><vh>ASTConverter.set_line</vh></v>
<v t="ekr.20230831011819.1172"><vh>ASTConverter.translate_opt_expr_list</vh></v>
<v t="ekr.20230831011819.1173"><vh>ASTConverter.translate_expr_list</vh></v>
<v t="ekr.20230831011819.1174"><vh>ASTConverter.get_lineno</vh></v>
<v t="ekr.20230831011819.1175"><vh>ASTConverter.translate_stmt_list</vh></v>
<v t="ekr.20230831011819.1176"><vh>ASTConverter.translate_type_comment</vh></v>
<v t="ekr.20230831011819.1177"><vh>ASTConverter.from_operator</vh></v>
<v t="ekr.20230831011819.1178"><vh>ASTConverter.from_comp_operator</vh></v>
<v t="ekr.20230831011819.1179"><vh>ASTConverter.set_block_lines</vh></v>
<v t="ekr.20230831011819.1180"><vh>ASTConverter.as_block</vh></v>
<v t="ekr.20230831011819.1181"><vh>ASTConverter.as_required_block</vh></v>
<v t="ekr.20230831011819.1182"><vh>ASTConverter.fix_function_overloads</vh></v>
<v t="ekr.20230831011819.1183"><vh>ASTConverter._check_ifstmt_for_overloads</vh></v>
<v t="ekr.20230831011819.1184"><vh>ASTConverter._get_executable_if_block_with_overloads</vh></v>
<v t="ekr.20230831011819.1185"><vh>ASTConverter._strip_contents_from_if_stmt</vh></v>
<v t="ekr.20230831011819.1186"><vh>ASTConverter._is_stripped_if_stmt</vh></v>
<v t="ekr.20230831011819.1187"><vh>ASTConverter.translate_module_id</vh></v>
<v t="ekr.20230831011819.1188"><vh>ASTConverter.visit_Module</vh></v>
<v t="ekr.20230831011819.1189"><vh>ASTConverter.visit_FunctionDef</vh></v>
<v t="ekr.20230831011819.1190"><vh>ASTConverter.visit_AsyncFunctionDef</vh></v>
<v t="ekr.20230831011819.1191"><vh>ASTConverter.do_func_def</vh></v>
<v t="ekr.20230831011819.1192"><vh>ASTConverter.set_type_optional</vh></v>
<v t="ekr.20230831011819.1193"><vh>ASTConverter.transform_args</vh></v>
<v t="ekr.20230831011819.1194"><vh>ASTConverter.make_argument</vh></v>
<v t="ekr.20230831011819.1195"><vh>ASTConverter.fail_arg</vh></v>
<v t="ekr.20230831011819.1196"><vh>ASTConverter.visit_ClassDef</vh></v>
<v t="ekr.20230831011819.1197"><vh>ASTConverter.visit_Return</vh></v>
<v t="ekr.20230831011819.1198"><vh>ASTConverter.visit_Delete</vh></v>
<v t="ekr.20230831011819.1199"><vh>ASTConverter.visit_Assign</vh></v>
<v t="ekr.20230831011819.1200"><vh>ASTConverter.visit_AnnAssign</vh></v>
<v t="ekr.20230831011819.1201"><vh>ASTConverter.visit_AugAssign</vh></v>
<v t="ekr.20230831011819.1202"><vh>ASTConverter.visit_For</vh></v>
<v t="ekr.20230831011819.1203"><vh>ASTConverter.visit_AsyncFor</vh></v>
<v t="ekr.20230831011819.1204"><vh>ASTConverter.visit_While</vh></v>
<v t="ekr.20230831011819.1205"><vh>ASTConverter.visit_If</vh></v>
<v t="ekr.20230831011819.1206"><vh>ASTConverter.visit_With</vh></v>
<v t="ekr.20230831011819.1207"><vh>ASTConverter.visit_AsyncWith</vh></v>
<v t="ekr.20230831011819.1208"><vh>ASTConverter.visit_Raise</vh></v>
<v t="ekr.20230831011819.1209"><vh>ASTConverter.visit_Try</vh></v>
<v t="ekr.20230831011819.1210"><vh>ASTConverter.visit_TryStar</vh></v>
<v t="ekr.20230831011819.1211"><vh>ASTConverter.visit_Assert</vh></v>
<v t="ekr.20230831011819.1212"><vh>ASTConverter.visit_Import</vh></v>
<v t="ekr.20230831011819.1213"><vh>ASTConverter.visit_ImportFrom</vh></v>
<v t="ekr.20230831011819.1214"><vh>ASTConverter.visit_Global</vh></v>
<v t="ekr.20230831011819.1215"><vh>ASTConverter.visit_Nonlocal</vh></v>
<v t="ekr.20230831011819.1216"><vh>ASTConverter.visit_Expr</vh></v>
<v t="ekr.20230831011819.1217"><vh>ASTConverter.visit_Pass</vh></v>
<v t="ekr.20230831011819.1218"><vh>ASTConverter.visit_Break</vh></v>
<v t="ekr.20230831011819.1219"><vh>ASTConverter.visit_Continue</vh></v>
<v t="ekr.20230831011819.1220"><vh>ASTConverter.visit_NamedExpr</vh></v>
<v t="ekr.20230831011819.1221"><vh>ASTConverter.visit_BoolOp</vh></v>
<v t="ekr.20230831011819.1222"><vh>ASTConverter.group</vh></v>
<v t="ekr.20230831011819.1223"><vh>ASTConverter.visit_BinOp</vh></v>
<v t="ekr.20230831011819.1224"><vh>ASTConverter.visit_UnaryOp</vh></v>
<v t="ekr.20230831011819.1225"><vh>ASTConverter.visit_Lambda</vh></v>
<v t="ekr.20230831011819.1226"><vh>ASTConverter.visit_IfExp</vh></v>
<v t="ekr.20230831011819.1227"><vh>ASTConverter.visit_Dict</vh></v>
<v t="ekr.20230831011819.1228"><vh>ASTConverter.visit_Set</vh></v>
<v t="ekr.20230831011819.1229"><vh>ASTConverter.visit_ListComp</vh></v>
<v t="ekr.20230831011819.1230"><vh>ASTConverter.visit_SetComp</vh></v>
<v t="ekr.20230831011819.1231"><vh>ASTConverter.visit_DictComp</vh></v>
<v t="ekr.20230831011819.1232"><vh>ASTConverter.visit_GeneratorExp</vh></v>
<v t="ekr.20230831011819.1233"><vh>ASTConverter.visit_Await</vh></v>
<v t="ekr.20230831011819.1234"><vh>ASTConverter.visit_Yield</vh></v>
<v t="ekr.20230831011819.1235"><vh>ASTConverter.visit_YieldFrom</vh></v>
<v t="ekr.20230831011819.1236"><vh>ASTConverter.visit_Compare</vh></v>
<v t="ekr.20230831011819.1237"><vh>ASTConverter.visit_Call</vh></v>
<v t="ekr.20230831011819.1238"><vh>ASTConverter.visit_Constant</vh></v>
<v t="ekr.20230831011819.1239"><vh>ASTConverter.visit_JoinedStr</vh></v>
<v t="ekr.20230831011819.1240"><vh>ASTConverter.visit_FormattedValue</vh></v>
<v t="ekr.20230831011819.1241"><vh>ASTConverter.visit_Attribute</vh></v>
<v t="ekr.20230831011819.1242"><vh>ASTConverter.visit_Subscript</vh></v>
<v t="ekr.20230831011819.1243"><vh>ASTConverter.visit_Starred</vh></v>
<v t="ekr.20230831011819.1244"><vh>ASTConverter.visit_Name</vh></v>
<v t="ekr.20230831011819.1245"><vh>ASTConverter.visit_List</vh></v>
<v t="ekr.20230831011819.1246"><vh>ASTConverter.visit_Tuple</vh></v>
<v t="ekr.20230831011819.1247"><vh>ASTConverter.visit_Slice</vh></v>
<v t="ekr.20230831011819.1248"><vh>ASTConverter.visit_ExtSlice</vh></v>
<v t="ekr.20230831011819.1249"><vh>ASTConverter.visit_Index</vh></v>
<v t="ekr.20230831011819.1250"><vh>ASTConverter.visit_Match</vh></v>
<v t="ekr.20230831011819.1251"><vh>ASTConverter.visit_MatchValue</vh></v>
<v t="ekr.20230831011819.1252"><vh>ASTConverter.visit_MatchSingleton</vh></v>
<v t="ekr.20230831011819.1253"><vh>ASTConverter.visit_MatchSequence</vh></v>
<v t="ekr.20230831011819.1254"><vh>ASTConverter.visit_MatchStar</vh></v>
<v t="ekr.20230831011819.1255"><vh>ASTConverter.visit_MatchMapping</vh></v>
<v t="ekr.20230831011819.1256"><vh>ASTConverter.visit_MatchClass</vh></v>
<v t="ekr.20230831011819.1257"><vh>ASTConverter.visit_MatchAs</vh></v>
<v t="ekr.20230831011819.1258"><vh>ASTConverter.visit_MatchOr</vh></v>
</v>
<v t="ekr.20230831011819.1259"><vh>class TypeConverter</vh>
<v t="ekr.20230831011819.1260"><vh>TypeConverter.__init__</vh></v>
<v t="ekr.20230831011819.1261"><vh>TypeConverter.convert_column</vh></v>
<v t="ekr.20230831011819.1262"><vh>TypeConverter.invalid_type</vh></v>
<v t="ekr.20230831011819.1263"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20230831011819.1264"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20230831011819.1265"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20230831011819.1266"><vh>TypeConverter.parent</vh></v>
<v t="ekr.20230831011819.1267"><vh>TypeConverter.fail</vh></v>
<v t="ekr.20230831011819.1268"><vh>TypeConverter.note</vh></v>
<v t="ekr.20230831011819.1269"><vh>TypeConverter.translate_expr_list</vh></v>
<v t="ekr.20230831011819.1270"><vh>TypeConverter.visit_Call</vh></v>
<v t="ekr.20230831011819.1271"><vh>TypeConverter.translate_argument_list</vh></v>
<v t="ekr.20230831011819.1272"><vh>TypeConverter._extract_argument_name</vh></v>
<v t="ekr.20230831011819.1273"><vh>TypeConverter.visit_Name</vh></v>
<v t="ekr.20230831011819.1274"><vh>TypeConverter.visit_BinOp</vh></v>
<v t="ekr.20230831011819.1275"><vh>TypeConverter.visit_Constant</vh></v>
<v t="ekr.20230831011819.1276"><vh>TypeConverter.visit_UnaryOp</vh></v>
<v t="ekr.20230831011819.1277"><vh>TypeConverter.numeric_type</vh></v>
<v t="ekr.20230831011819.1278"><vh>TypeConverter.visit_Index</vh></v>
<v t="ekr.20230831011819.1279"><vh>TypeConverter.visit_Slice</vh></v>
<v t="ekr.20230831011819.1280"><vh>TypeConverter.visit_Subscript</vh></v>
<v t="ekr.20230831011819.1281"><vh>TypeConverter.visit_Tuple</vh></v>
<v t="ekr.20230831011819.1282"><vh>TypeConverter.visit_Attribute</vh></v>
<v t="ekr.20230831011819.1283"><vh>TypeConverter.visit_List</vh></v>
</v>
<v t="ekr.20230831011819.1284"><vh>function: stringify_name</vh></v>
<v t="ekr.20230831011819.1285"><vh>class FindAttributeAssign</vh>
<v t="ekr.20230831011819.1286"><vh>FindAttributeAssign.__init__</vh></v>
<v t="ekr.20230831011819.1287"><vh>FindAttributeAssign.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011819.1288"><vh>FindAttributeAssign.visit_with_stmt</vh></v>
<v t="ekr.20230831011819.1289"><vh>FindAttributeAssign.visit_for_stmt</vh></v>
<v t="ekr.20230831011819.1290"><vh>FindAttributeAssign.visit_expression_stmt</vh></v>
<v t="ekr.20230831011819.1291"><vh>FindAttributeAssign.visit_call_expr</vh></v>
<v t="ekr.20230831011819.1292"><vh>FindAttributeAssign.visit_index_expr</vh></v>
<v t="ekr.20230831011819.1293"><vh>FindAttributeAssign.visit_member_expr</vh></v>
</v>
<v t="ekr.20230831011819.1294"><vh>class FindYield</vh>
<v t="ekr.20230831011819.1295"><vh>FindYield.__init__</vh></v>
<v t="ekr.20230831011819.1296"><vh>FindYield.visit_yield_expr</vh></v>
<v t="ekr.20230831011819.1297"><vh>FindYield.visit_yield_from_expr</vh></v>
</v>
<v t="ekr.20230831011819.1298"><vh>function: is_possible_trivial_body</vh></v>
</v>
<v t="ekr.20230831011819.1299"><vh>@clean find_sources.py</vh>
<v t="ekr.20230831011819.1300"><vh>&lt;&lt; find_sources.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1301"><vh>class InvalidSourceList</vh></v>
<v t="ekr.20230831011819.1302"><vh>function: create_source_list</vh></v>
<v t="ekr.20230831011819.1303"><vh>function: keyfunc</vh></v>
<v t="ekr.20230831011819.1304"><vh>function: normalise_package_base</vh></v>
<v t="ekr.20230831011819.1305"><vh>function: get_explicit_package_bases</vh></v>
<v t="ekr.20230831011819.1306"><vh>class SourceFinder</vh>
<v t="ekr.20230831011819.1307"><vh>SourceFinder.__init__</vh></v>
<v t="ekr.20230831011819.1308"><vh>SourceFinder.is_explicit_package_base</vh></v>
<v t="ekr.20230831011819.1309"><vh>SourceFinder.find_sources_in_dir</vh></v>
<v t="ekr.20230831011819.1310"><vh>SourceFinder.crawl_up</vh></v>
<v t="ekr.20230831011819.1311"><vh>SourceFinder.crawl_up_dir</vh></v>
<v t="ekr.20230831011819.1312"><vh>SourceFinder._crawl_up_helper</vh></v>
<v t="ekr.20230831011819.1313"><vh>SourceFinder.get_init_file</vh></v>
</v>
<v t="ekr.20230831011819.1314"><vh>function: module_join</vh></v>
<v t="ekr.20230831011819.1315"><vh>function: strip_py</vh></v>
</v>
<v t="ekr.20230831011819.1365"><vh>@clean fscache.py</vh>
<v t="ekr.20230902063201.1"><vh>&lt;&lt; fscache.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1367"><vh>&lt;&lt; fscache.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1368"><vh>class FileSystemCache</vh>
<v t="ekr.20230831011819.1369"><vh>FileSystemCache.__init__</vh></v>
<v t="ekr.20230831011819.1370"><vh>FileSystemCache.set_package_root</vh></v>
<v t="ekr.20230831011819.1371"><vh>FileSystemCache.flush</vh></v>
<v t="ekr.20230831011819.1372"><vh>FileSystemCache.stat</vh></v>
<v t="ekr.20230831011819.1373"><vh>FileSystemCache.init_under_package_root</vh></v>
<v t="ekr.20230831011819.1374"><vh>FileSystemCache._fake_init</vh></v>
<v t="ekr.20230831011819.1375"><vh>FileSystemCache.listdir</vh></v>
<v t="ekr.20230831011819.1376"><vh>FileSystemCache.isfile</vh></v>
<v t="ekr.20230831011819.1377"><vh>FileSystemCache.isfile_case</vh></v>
<v t="ekr.20230831011819.1378"><vh>FileSystemCache.exists_case</vh></v>
<v t="ekr.20230831011819.1379"><vh>FileSystemCache.isdir</vh></v>
<v t="ekr.20230831011819.1380"><vh>FileSystemCache.exists</vh></v>
<v t="ekr.20230831011819.1381"><vh>FileSystemCache.read</vh></v>
<v t="ekr.20230831011819.1382"><vh>FileSystemCache.hash_digest</vh></v>
<v t="ekr.20230831011819.1383"><vh>FileSystemCache.samefile</vh></v>
</v>
<v t="ekr.20230831011819.1384"><vh>function: copy_os_error</vh></v>
</v>
<v t="ekr.20230831011819.1385"><vh>@clean fswatcher.py</vh>
<v t="ekr.20230831011819.1386"><vh>&lt;&lt; fswatcher.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1387"><vh>class FileData</vh></v>
<v t="ekr.20230831011819.1388"><vh>class FileSystemWatcher</vh>
<v t="ekr.20230831011819.1389"><vh>FileSystemWatcher.__init__</vh></v>
<v t="ekr.20230831011819.1390"><vh>FileSystemWatcher.dump_file_data</vh></v>
<v t="ekr.20230831011819.1391"><vh>FileSystemWatcher.set_file_data</vh></v>
<v t="ekr.20230831011819.1392"><vh>FileSystemWatcher.add_watched_paths</vh></v>
<v t="ekr.20230831011819.1393"><vh>FileSystemWatcher.remove_watched_paths</vh></v>
<v t="ekr.20230831011819.1394"><vh>FileSystemWatcher._update</vh></v>
<v t="ekr.20230831011819.1395"><vh>FileSystemWatcher._find_changed</vh></v>
<v t="ekr.20230831011819.1396"><vh>FileSystemWatcher.find_changed</vh></v>
<v t="ekr.20230831011819.1397"><vh>FileSystemWatcher.update_changed</vh></v>
</v>
</v>
<v t="ekr.20230831011819.1416"><vh>@clean indirection.py</vh>
<v t="ekr.20230831011819.1417"><vh>&lt;&lt; indirection.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1418"><vh>function: extract_module_names</vh></v>
<v t="ekr.20230831011819.1419"><vh>class TypeIndirectionVisitor</vh>
<v t="ekr.20230831011819.1420"><vh>TypeIndirectionVisitor.__init__</vh></v>
<v t="ekr.20230831011819.1421"><vh>TypeIndirectionVisitor.find_modules</vh></v>
<v t="ekr.20230831011819.1422"><vh>TypeIndirectionVisitor._visit</vh></v>
<v t="ekr.20230831011819.1423"><vh>TypeIndirectionVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011819.1424"><vh>TypeIndirectionVisitor.visit_any</vh></v>
<v t="ekr.20230831011819.1425"><vh>TypeIndirectionVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011819.1426"><vh>TypeIndirectionVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.1427"><vh>TypeIndirectionVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011819.1428"><vh>TypeIndirectionVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011819.1429"><vh>TypeIndirectionVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011819.1430"><vh>TypeIndirectionVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011819.1431"><vh>TypeIndirectionVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.1432"><vh>TypeIndirectionVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011819.1433"><vh>TypeIndirectionVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011819.1434"><vh>TypeIndirectionVisitor.visit_instance</vh></v>
<v t="ekr.20230831011819.1435"><vh>TypeIndirectionVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011819.1436"><vh>TypeIndirectionVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011819.1437"><vh>TypeIndirectionVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011819.1438"><vh>TypeIndirectionVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011819.1439"><vh>TypeIndirectionVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011819.1440"><vh>TypeIndirectionVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011819.1441"><vh>TypeIndirectionVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011819.1442"><vh>TypeIndirectionVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011819.1443"><vh>TypeIndirectionVisitor.visit_type_alias_type</vh></v>
</v>
</v>
<v t="ekr.20230831011819.1608"><vh>@clean main.py</vh>
<v t="ekr.20230831011819.1609"><vh>&lt;&lt; main.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1610"><vh>function: stat_proxy</vh></v>
<v t="ekr.20230831011819.1611"><vh>function: main</vh></v>
<v t="ekr.20230831011819.1612"><vh>function: run_build</vh></v>
<v t="ekr.20230831011819.1613"><vh>function: show_messages</vh></v>
<v t="ekr.20230831011819.1614"><vh>class AugmentedHelpFormatter</vh>
<v t="ekr.20230831011819.1615"><vh>AugmentedHelpFormatter.__init__</vh></v>
<v t="ekr.20230831011819.1616"><vh>AugmentedHelpFormatter._fill_text</vh></v>
</v>
<v t="ekr.20230831011819.1617"><vh>function: invert_flag_name</vh></v>
<v t="ekr.20230831011819.1618"><vh>class PythonExecutableInferenceError</vh></v>
<v t="ekr.20230831011819.1619"><vh>function: python_executable_prefix</vh></v>
<v t="ekr.20230831011819.1620"><vh>function: _python_executable_from_version</vh></v>
<v t="ekr.20230831011819.1621"><vh>function: infer_python_executable</vh></v>
<v t="ekr.20230831011819.1622"><vh>class CapturableArgumentParser</vh>
<v t="ekr.20230831011819.1623"><vh>CapturableArgumentParser.__init__</vh></v>
<v t="ekr.20230831011819.1624"><vh>CapturableArgumentParser.print_usage</vh></v>
<v t="ekr.20230831011819.1625"><vh>CapturableArgumentParser.print_help</vh></v>
<v t="ekr.20230831011819.1626"><vh>CapturableArgumentParser._print_message</vh></v>
<v t="ekr.20230831011819.1627"><vh>CapturableArgumentParser.exit</vh></v>
<v t="ekr.20230831011819.1628"><vh>CapturableArgumentParser.error</vh></v>
</v>
<v t="ekr.20230831011819.1629"><vh>class CapturableVersionAction</vh>
<v t="ekr.20230831011819.1630"><vh>CapturableVersionAction.__init__</vh></v>
<v t="ekr.20230831011819.1631"><vh>CapturableVersionAction.__call__</vh></v>
</v>
<v t="ekr.20230831011819.1632"><vh>function: process_options</vh></v>
<v t="ekr.20230831011819.1633"><vh>function: process_package_roots</vh></v>
<v t="ekr.20230831011820.1"><vh>function: process_cache_map</vh></v>
<v t="ekr.20230831011820.2"><vh>function: maybe_write_junit_xml</vh></v>
<v t="ekr.20230831011820.3"><vh>function: fail</vh></v>
<v t="ekr.20230831011820.4"><vh>function: read_types_packages_to_install</vh></v>
<v t="ekr.20230831011820.5"><vh>function: install_types</vh></v>
</v>
<v t="ekr.20230831011820.56"><vh>@clean memprofile.py</vh>
<v t="ekr.20230831011820.58"><vh>&lt;&lt; memprofile.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.59"><vh>function: collect_memory_stats</vh></v>
<v t="ekr.20230831011820.60"><vh>function: print_memory_profile</vh></v>
<v t="ekr.20230831011820.61"><vh>function: find_recursive_objects</vh></v>
</v>
<v t="ekr.20230831011820.242"><vh>@clean metastore.py</vh>
<v t="ekr.20230831011820.244"><vh>&lt;&lt; metastore.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.245"><vh>class MetadataStore</vh>
<v t="ekr.20230831011820.246"><vh>MetadataStore.getmtime</vh></v>
<v t="ekr.20230831011820.247"><vh>MetadataStore.read</vh></v>
<v t="ekr.20230831011820.248"><vh>MetadataStore.write</vh></v>
<v t="ekr.20230831011820.249"><vh>MetadataStore.remove</vh></v>
<v t="ekr.20230831011820.250"><vh>MetadataStore.commit</vh></v>
<v t="ekr.20230831011820.251"><vh>MetadataStore.list_all</vh></v>
</v>
<v t="ekr.20230831011820.252"><vh>function: random_string</vh></v>
<v t="ekr.20230831011820.253"><vh>class FilesystemMetadataStore</vh>
<v t="ekr.20230831011820.254"><vh>FilesystemMetadataStore.__init__</vh></v>
<v t="ekr.20230831011820.255"><vh>FilesystemMetadataStore.getmtime</vh></v>
<v t="ekr.20230831011820.256"><vh>FilesystemMetadataStore.read</vh></v>
<v t="ekr.20230831011820.257"><vh>FilesystemMetadataStore.write</vh></v>
<v t="ekr.20230831011820.258"><vh>FilesystemMetadataStore.remove</vh></v>
<v t="ekr.20230831011820.259"><vh>FilesystemMetadataStore.commit</vh></v>
<v t="ekr.20230831011820.260"><vh>FilesystemMetadataStore.list_all</vh></v>
</v>
<v t="ekr.20230831011820.261"><vh>function: connect_db</vh></v>
<v t="ekr.20230831011820.262"><vh>class SqliteMetadataStore</vh>
<v t="ekr.20230831011820.263"><vh>SqliteMetadataStore.__init__</vh></v>
<v t="ekr.20230831011820.264"><vh>SqliteMetadataStore._query</vh></v>
<v t="ekr.20230831011820.265"><vh>SqliteMetadataStore.getmtime</vh></v>
<v t="ekr.20230831011820.266"><vh>SqliteMetadataStore.read</vh></v>
<v t="ekr.20230831011820.267"><vh>SqliteMetadataStore.write</vh></v>
<v t="ekr.20230831011820.268"><vh>SqliteMetadataStore.remove</vh></v>
<v t="ekr.20230831011820.269"><vh>SqliteMetadataStore.commit</vh></v>
<v t="ekr.20230831011820.270"><vh>SqliteMetadataStore.list_all</vh></v>
</v>
</v>
<v t="ekr.20230831011820.291"><vh>@clean modulefinder.py</vh>
<v t="ekr.20230831011820.293"><vh>&lt;&lt; modulefinder.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.294"><vh>class SearchPaths</vh></v>
<v t="ekr.20230831011820.295"><vh>class ModuleNotFoundReason</vh>
<v t="ekr.20230831011820.296"><vh>ModuleNotFoundReason.error_message_templates</vh></v>
</v>
<v t="ekr.20230831011820.297"><vh>class BuildSource</vh>
<v t="ekr.20230831011820.298"><vh>BuildSource.__init__</vh></v>
<v t="ekr.20230831011820.299"><vh>BuildSource.__repr__</vh></v>
</v>
<v t="ekr.20230831011820.300"><vh>class BuildSourceSet</vh>
<v t="ekr.20230831011820.301"><vh>BuildSourceSet.__init__</vh></v>
<v t="ekr.20230831011820.302"><vh>BuildSourceSet.is_source</vh></v>
</v>
<v t="ekr.20230831011820.303"><vh>class FindModuleCache</vh>
<v t="ekr.20230831011820.304"><vh>FindModuleCache.__init__</vh></v>
<v t="ekr.20230831011820.305"><vh>FindModuleCache.clear</vh></v>
<v t="ekr.20230831011820.306"><vh>FindModuleCache.find_module_via_source_set</vh></v>
<v t="ekr.20230831011820.307"><vh>FindModuleCache.find_lib_path_dirs</vh></v>
<v t="ekr.20230831011820.308"><vh>FindModuleCache.get_toplevel_possibilities</vh></v>
<v t="ekr.20230831011820.309"><vh>FindModuleCache.find_module</vh></v>
<v t="ekr.20230831011820.310"><vh>FindModuleCache._typeshed_has_version</vh></v>
<v t="ekr.20230831011820.311"><vh>FindModuleCache._find_module_non_stub_helper</vh></v>
<v t="ekr.20230831011820.312"><vh>FindModuleCache._update_ns_ancestors</vh></v>
<v t="ekr.20230831011820.313"><vh>FindModuleCache._can_find_module_in_parent_dir</vh></v>
<v t="ekr.20230831011820.314"><vh>FindModuleCache._find_module</vh></v>
<v t="ekr.20230831011820.315"><vh>FindModuleCache._is_compatible_stub_package</vh></v>
<v t="ekr.20230831011820.316"><vh>FindModuleCache.find_modules_recursive</vh></v>
</v>
<v t="ekr.20230831011820.317"><vh>function: matches_exclude</vh></v>
<v t="ekr.20230831011820.318"><vh>function: is_init_file</vh></v>
<v t="ekr.20230831011820.319"><vh>function: verify_module</vh></v>
<v t="ekr.20230831011820.320"><vh>function: highest_init_level</vh></v>
<v t="ekr.20230831011820.321"><vh>function: mypy_path</vh></v>
<v t="ekr.20230831011820.322"><vh>function: default_lib_path</vh></v>
<v t="ekr.20230831011820.323"><vh>function: get_search_dirs</vh></v>
<v t="ekr.20230831011820.324"><vh>function: compute_search_paths</vh></v>
<v t="ekr.20230831011820.325"><vh>function: load_stdlib_py_versions</vh></v>
<v t="ekr.20230831011820.326"><vh>function: parse_version</vh></v>
<v t="ekr.20230831011820.327"><vh>function: typeshed_py_version</vh></v>
</v>
<v t="ekr.20230831011820.328"><vh>@clean moduleinspect.py</vh>
<v t="ekr.20230831011820.329"><vh>&lt;&lt; moduleinspect.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.330"><vh>class ModuleProperties</vh>
<v t="ekr.20230831011820.331"><vh>ModuleProperties.__init__</vh></v>
</v>
<v t="ekr.20230831011820.332"><vh>function: is_c_module</vh></v>
<v t="ekr.20230831011820.333"><vh>class InspectError</vh></v>
<v t="ekr.20230831011820.334"><vh>function: get_package_properties</vh></v>
<v t="ekr.20230831011820.335"><vh>function: worker</vh></v>
<v t="ekr.20230831011820.336"><vh>class ModuleInspect</vh>
<v t="ekr.20230831011820.337"><vh>ModuleInspect.__init__</vh></v>
<v t="ekr.20230831011820.338"><vh>ModuleInspect._start</vh></v>
<v t="ekr.20230831011820.339"><vh>ModuleInspect.close</vh></v>
<v t="ekr.20230831011820.340"><vh>ModuleInspect.get_package_properties</vh></v>
<v t="ekr.20230831011820.341"><vh>ModuleInspect._get_from_queue</vh></v>
<v t="ekr.20230831011820.342"><vh>ModuleInspect.__enter__</vh></v>
<v t="ekr.20230831011820.343"><vh>ModuleInspect.__exit__</vh></v>
</v>
</v>
<v t="ekr.20230831011820.718"><vh>@clean options.py</vh>
<v t="ekr.20230831011820.719"><vh>&lt;&lt; options.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.720"><vh>class BuildType</vh></v>
<v t="ekr.20230831011820.721"><vh>class Options</vh>
<v t="ekr.20230831011820.722"><vh>Options.__init__</vh></v>
<v t="ekr.20230831011820.723"><vh>Options.use_lowercase_names</vh></v>
<v t="ekr.20230831011820.724"><vh>Options.use_or_syntax</vh></v>
<v t="ekr.20230831011820.725"><vh>Options.new_semantic_analyzer</vh></v>
<v t="ekr.20230831011820.726"><vh>Options.snapshot</vh></v>
<v t="ekr.20230831011820.727"><vh>Options.__repr__</vh></v>
<v t="ekr.20230831011820.728"><vh>Options.apply_changes</vh></v>
<v t="ekr.20230831011820.729"><vh>Options.compare_stable</vh></v>
<v t="ekr.20230831011820.730"><vh>Options.build_per_module_cache</vh></v>
<v t="ekr.20230831011820.731"><vh>Options.clone_for_module</vh></v>
<v t="ekr.20230831011820.732"><vh>Options.compile_glob</vh></v>
<v t="ekr.20230831011820.733"><vh>Options.select_options_affecting_cache</vh></v>
</v>
</v>
<v t="ekr.20230831011820.734"><vh>@clean parse.py</vh>
<v t="ekr.20230831011820.735"><vh>&lt;&lt; parse.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.736"><vh>function: parse</vh></v>
</v>
<v t="ekr.20230831011820.842"><vh>@clean plugin.py</vh>
<v t="ekr.20230831011820.844"><vh>&lt;&lt; plugin.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.845"><vh>class TypeAnalyzerPluginInterface</vh>
<v t="ekr.20230831011820.846"><vh>TypeAnalyzerPluginInterface.fail</vh></v>
<v t="ekr.20230831011820.847"><vh>TypeAnalyzerPluginInterface.named_type</vh></v>
<v t="ekr.20230831011820.848"><vh>TypeAnalyzerPluginInterface.analyze_type</vh></v>
<v t="ekr.20230831011820.849"><vh>TypeAnalyzerPluginInterface.analyze_callable_args</vh></v>
</v>
<v t="ekr.20230831011820.850"><vh>class AnalyzeTypeContext</vh></v>
<v t="ekr.20230831011820.851"><vh>class CommonPluginApi</vh>
<v t="ekr.20230831011820.852"><vh>CommonPluginApi.lookup_fully_qualified</vh></v>
</v>
<v t="ekr.20230831011820.853"><vh>class CheckerPluginInterface</vh>
<v t="ekr.20230831011820.854"><vh>CheckerPluginInterface.type_context</vh></v>
<v t="ekr.20230831011820.855"><vh>CheckerPluginInterface.fail</vh></v>
<v t="ekr.20230831011820.856"><vh>CheckerPluginInterface.named_generic_type</vh></v>
<v t="ekr.20230831011820.857"><vh>CheckerPluginInterface.get_expression_type</vh></v>
</v>
<v t="ekr.20230831011820.858"><vh>class SemanticAnalyzerPluginInterface</vh>
<v t="ekr.20230831011820.859"><vh>SemanticAnalyzerPluginInterface.named_type</vh></v>
<v t="ekr.20230831011820.860"><vh>SemanticAnalyzerPluginInterface.builtin_type</vh></v>
<v t="ekr.20230831011820.861"><vh>SemanticAnalyzerPluginInterface.named_type_or_none</vh></v>
<v t="ekr.20230831011820.862"><vh>SemanticAnalyzerPluginInterface.basic_new_typeinfo</vh></v>
<v t="ekr.20230831011820.863"><vh>SemanticAnalyzerPluginInterface.parse_bool</vh></v>
<v t="ekr.20230831011820.864"><vh>SemanticAnalyzerPluginInterface.parse_str_literal</vh></v>
<v t="ekr.20230831011820.865"><vh>SemanticAnalyzerPluginInterface.fail</vh></v>
<v t="ekr.20230831011820.866"><vh>SemanticAnalyzerPluginInterface.anal_type</vh></v>
<v t="ekr.20230831011820.867"><vh>SemanticAnalyzerPluginInterface.class_type</vh></v>
<v t="ekr.20230831011820.868"><vh>SemanticAnalyzerPluginInterface.lookup_fully_qualified</vh></v>
<v t="ekr.20230831011820.869"><vh>SemanticAnalyzerPluginInterface.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20230831011820.870"><vh>SemanticAnalyzerPluginInterface.lookup_qualified</vh></v>
<v t="ekr.20230831011820.871"><vh>SemanticAnalyzerPluginInterface.add_plugin_dependency</vh></v>
<v t="ekr.20230831011820.872"><vh>SemanticAnalyzerPluginInterface.add_symbol_table_node</vh></v>
<v t="ekr.20230831011820.873"><vh>SemanticAnalyzerPluginInterface.qualified_name</vh></v>
<v t="ekr.20230831011820.874"><vh>SemanticAnalyzerPluginInterface.defer</vh></v>
<v t="ekr.20230831011820.875"><vh>SemanticAnalyzerPluginInterface.final_iteration</vh></v>
<v t="ekr.20230831011820.876"><vh>SemanticAnalyzerPluginInterface.is_stub_file</vh></v>
<v t="ekr.20230831011820.877"><vh>SemanticAnalyzerPluginInterface.analyze_simple_literal_type</vh></v>
</v>
<v t="ekr.20230831011820.878"><vh>class ReportConfigContext</vh></v>
<v t="ekr.20230831011820.879"><vh>class FunctionSigContext</vh></v>
<v t="ekr.20230831011820.880"><vh>class FunctionContext</vh></v>
<v t="ekr.20230831011820.881"><vh>class MethodSigContext</vh></v>
<v t="ekr.20230831011820.882"><vh>class MethodContext</vh></v>
<v t="ekr.20230831011820.883"><vh>class AttributeContext</vh></v>
<v t="ekr.20230831011820.884"><vh>class ClassDefContext</vh></v>
<v t="ekr.20230831011820.885"><vh>class DynamicClassDefContext</vh></v>
<v t="ekr.20230831011820.886"><vh>class Plugin</vh>
<v t="ekr.20230831011820.887"><vh>Plugin.__init__</vh></v>
<v t="ekr.20230831011820.888"><vh>Plugin.set_modules</vh></v>
<v t="ekr.20230831011820.889"><vh>Plugin.lookup_fully_qualified</vh></v>
<v t="ekr.20230831011820.890"><vh>Plugin.report_config_data</vh></v>
<v t="ekr.20230831011820.891"><vh>Plugin.get_additional_deps</vh></v>
<v t="ekr.20230831011820.892"><vh>Plugin.get_type_analyze_hook</vh></v>
<v t="ekr.20230831011820.893"><vh>Plugin.get_function_signature_hook</vh></v>
<v t="ekr.20230831011820.894"><vh>Plugin.get_function_hook</vh></v>
<v t="ekr.20230831011820.895"><vh>Plugin.get_method_signature_hook</vh></v>
<v t="ekr.20230831011820.896"><vh>Plugin.get_method_hook</vh></v>
<v t="ekr.20230831011820.897"><vh>Plugin.get_attribute_hook</vh></v>
<v t="ekr.20230831011820.898"><vh>Plugin.get_class_attribute_hook</vh></v>
<v t="ekr.20230831011820.899"><vh>Plugin.get_class_decorator_hook</vh></v>
<v t="ekr.20230831011820.900"><vh>Plugin.get_class_decorator_hook_2</vh></v>
<v t="ekr.20230831011820.901"><vh>Plugin.get_metaclass_hook</vh></v>
<v t="ekr.20230831011820.902"><vh>Plugin.get_base_class_hook</vh></v>
<v t="ekr.20230831011820.903"><vh>Plugin.get_customize_class_mro_hook</vh></v>
<v t="ekr.20230831011820.904"><vh>Plugin.get_dynamic_class_hook</vh></v>
</v>
<v t="ekr.20230831011820.905"><vh>class ChainedPlugin</vh>
<v t="ekr.20230831011820.906"><vh>ChainedPlugin.__init__</vh></v>
<v t="ekr.20230831011820.907"><vh>ChainedPlugin.set_modules</vh></v>
<v t="ekr.20230831011820.908"><vh>ChainedPlugin.report_config_data</vh></v>
<v t="ekr.20230831011820.909"><vh>ChainedPlugin.get_additional_deps</vh></v>
<v t="ekr.20230831011820.910"><vh>ChainedPlugin.get_type_analyze_hook</vh></v>
<v t="ekr.20230831011820.911"><vh>ChainedPlugin.get_function_signature_hook</vh></v>
<v t="ekr.20230831011820.912"><vh>ChainedPlugin.get_function_hook</vh></v>
<v t="ekr.20230831011820.913"><vh>ChainedPlugin.get_method_signature_hook</vh></v>
<v t="ekr.20230831011820.914"><vh>ChainedPlugin.get_method_hook</vh></v>
<v t="ekr.20230831011820.915"><vh>ChainedPlugin.get_attribute_hook</vh></v>
<v t="ekr.20230831011820.916"><vh>ChainedPlugin.get_class_attribute_hook</vh></v>
<v t="ekr.20230831011820.917"><vh>ChainedPlugin.get_class_decorator_hook</vh></v>
<v t="ekr.20230831011820.918"><vh>ChainedPlugin.get_class_decorator_hook_2</vh></v>
<v t="ekr.20230831011820.919"><vh>ChainedPlugin.get_metaclass_hook</vh></v>
<v t="ekr.20230831011820.920"><vh>ChainedPlugin.get_base_class_hook</vh></v>
<v t="ekr.20230831011820.921"><vh>ChainedPlugin.get_customize_class_mro_hook</vh></v>
<v t="ekr.20230831011820.922"><vh>ChainedPlugin.get_dynamic_class_hook</vh></v>
<v t="ekr.20230831011820.923"><vh>ChainedPlugin._find_hook</vh></v>
</v>
</v>
<v t="ekr.20230831011820.924"><vh>@clean pyinfo.py</vh>
<v t="ekr.20230831011820.925"><vh>&lt;&lt; pyinfo.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011820.926"><vh>&lt;&lt; pyinfo.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.927"><vh>function: getsitepackages</vh></v>
<v t="ekr.20230831011820.928"><vh>function: getsyspath</vh></v>
<v t="ekr.20230831011820.929"><vh>function: getsearchdirs</vh></v>
</v>
<v t="ekr.20230831011820.1568"><vh>@clean sharedparse.py</vh>
<v t="ekr.20230831011820.1569"><vh>&lt;&lt; sharedparse.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1570"><vh>function: special_function_elide_names</vh></v>
<v t="ekr.20230831011820.1571"><vh>function: argument_elide_name</vh></v>
</v>
<v t="ekr.20230831011821.1067"><vh>@clean version.py</vh></v>
</v>
<v t="ekr.20230831071850.1"><vh>--- Traversers &amp; tree utils</vh>
<v t="ekr.20230831011819.1360"><vh>@clean freetree.py</vh>
<v t="ekr.20230831011819.1362"><vh>class TreeFreer</vh>
<v t="ekr.20230831011819.1363"><vh>TreeFreer.visit_block</vh></v>
</v>
<v t="ekr.20230831011819.1364"><vh>function: free_tree</vh></v>
</v>
<v t="ekr.20230831011819.1449"><vh>@clean inspections.py</vh>
<v t="ekr.20230831011819.1450"><vh>&lt;&lt; inspections.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1451"><vh>function: node_starts_after</vh></v>
<v t="ekr.20230831011819.1452"><vh>function: node_ends_before</vh></v>
<v t="ekr.20230831011819.1453"><vh>function: expr_span</vh></v>
<v t="ekr.20230831011819.1454"><vh>function: get_instance_fallback</vh></v>
<v t="ekr.20230831011819.1455"><vh>function: find_node</vh></v>
<v t="ekr.20230831011819.1456"><vh>function: find_module_by_fullname</vh></v>
<v t="ekr.20230831011819.1457"><vh>class SearchVisitor</vh>
<v t="ekr.20230831011819.1458"><vh>SearchVisitor.__init__</vh></v>
<v t="ekr.20230831011819.1459"><vh>SearchVisitor.visit</vh></v>
</v>
<v t="ekr.20230831011819.1460"><vh>function: find_by_location</vh></v>
<v t="ekr.20230831011819.1461"><vh>class SearchAllVisitor</vh>
<v t="ekr.20230831011819.1462"><vh>SearchAllVisitor.__init__</vh></v>
<v t="ekr.20230831011819.1463"><vh>SearchAllVisitor.visit</vh></v>
</v>
<v t="ekr.20230831011819.1464"><vh>function: find_all_by_location</vh></v>
<v t="ekr.20230831011819.1465"><vh>class InspectionEngine</vh>
<v t="ekr.20230831011819.1466"><vh>InspectionEngine.__init__</vh></v>
<v t="ekr.20230831011819.1467"><vh>InspectionEngine.parse_location</vh></v>
<v t="ekr.20230831011819.1468"><vh>InspectionEngine.reload_module</vh></v>
<v t="ekr.20230831011819.1469"><vh>InspectionEngine.expr_type</vh></v>
<v t="ekr.20230831011819.1470"><vh>InspectionEngine.object_type</vh></v>
<v t="ekr.20230831011819.1471"><vh>InspectionEngine.collect_attrs</vh></v>
<v t="ekr.20230831011819.1472"><vh>InspectionEngine._fill_from_dict</vh></v>
<v t="ekr.20230831011819.1473"><vh>InspectionEngine.expr_attrs</vh></v>
<v t="ekr.20230831011819.1474"><vh>InspectionEngine.format_node</vh></v>
<v t="ekr.20230831011819.1475"><vh>InspectionEngine.collect_nodes</vh></v>
<v t="ekr.20230831011819.1476"><vh>InspectionEngine.modules_for_nodes</vh></v>
<v t="ekr.20230831011819.1477"><vh>InspectionEngine.expression_def</vh></v>
<v t="ekr.20230831011819.1478"><vh>InspectionEngine.missing_type</vh></v>
<v t="ekr.20230831011819.1479"><vh>InspectionEngine.missing_node</vh></v>
<v t="ekr.20230831011819.1480"><vh>InspectionEngine.add_prefixes</vh></v>
<v t="ekr.20230831011819.1481"><vh>InspectionEngine.run_inspection_by_exact_location</vh></v>
<v t="ekr.20230831011819.1482"><vh>InspectionEngine.run_inspection_by_position</vh></v>
<v t="ekr.20230831011819.1483"><vh>InspectionEngine.find_module</vh></v>
<v t="ekr.20230831011819.1484"><vh>InspectionEngine.run_inspection</vh></v>
<v t="ekr.20230831011819.1485"><vh>InspectionEngine.get_type</vh></v>
<v t="ekr.20230831011819.1486"><vh>InspectionEngine.get_attrs</vh></v>
<v t="ekr.20230831011819.1487"><vh>InspectionEngine.get_definition</vh></v>
</v>
</v>
<v t="ekr.20230831011820.271"><vh>@clean mixedtraverser.py</vh>
<v t="ekr.20230831011820.272"><vh>&lt;&lt; mixedtraverser.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.273"><vh>class MixedTraverserVisitor</vh>
<v t="ekr.20230831011820.274"><vh>MixedTraverserVisitor.__init__</vh></v>
<v t="ekr.20230831011820.275"><vh>MixedTraverserVisitor.visit_var</vh></v>
<v t="ekr.20230831011820.276"><vh>MixedTraverserVisitor.visit_func</vh></v>
<v t="ekr.20230831011820.277"><vh>MixedTraverserVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011820.278"><vh>MixedTraverserVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011820.279"><vh>MixedTraverserVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20230831011820.280"><vh>MixedTraverserVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011820.281"><vh>MixedTraverserVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011820.282"><vh>MixedTraverserVisitor.visit__promote_expr</vh></v>
<v t="ekr.20230831011820.283"><vh>MixedTraverserVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20230831011820.284"><vh>MixedTraverserVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.285"><vh>MixedTraverserVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011820.286"><vh>MixedTraverserVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011820.287"><vh>MixedTraverserVisitor.visit_cast_expr</vh></v>
<v t="ekr.20230831011820.288"><vh>MixedTraverserVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011820.289"><vh>MixedTraverserVisitor.visit_type_application</vh></v>
<v t="ekr.20230831011820.290"><vh>MixedTraverserVisitor.visit_optional_type</vh></v>
</v>
</v>
<v t="ekr.20230831011820.350"><vh>@clean nodes.py</vh>
<v t="ekr.20230831011820.351"><vh>&lt;&lt; nodes.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.352"><vh>class Context</vh>
<v t="ekr.20230831011820.353"><vh>Context.__init__</vh></v>
<v t="ekr.20230831011820.354"><vh>Context.set_line</vh></v>
</v>
<v t="ekr.20230831011820.355"><vh>function: get_nongen_builtins</vh></v>
<v t="ekr.20230831011820.356"><vh>class Node</vh>
<v t="ekr.20230831011820.357"><vh>Node.__str__</vh></v>
<v t="ekr.20230831011820.358"><vh>Node.str_with_options</vh></v>
<v t="ekr.20230831011820.359"><vh>Node.accept</vh></v>
</v>
<v t="ekr.20230831011820.360"><vh>class Statement</vh>
<v t="ekr.20230831011820.361"><vh>Statement.accept</vh></v>
</v>
<v t="ekr.20230831011820.362"><vh>class Expression</vh>
<v t="ekr.20230831011820.363"><vh>Expression.accept</vh></v>
</v>
<v t="ekr.20230831011820.364"><vh>class FakeExpression</vh></v>
<v t="ekr.20230831011820.365"><vh>class SymbolNode</vh>
<v t="ekr.20230831011820.366"><vh>SymbolNode.name</vh></v>
<v t="ekr.20230831011820.367"><vh>SymbolNode.fullname</vh></v>
<v t="ekr.20230831011820.368"><vh>SymbolNode.serialize</vh></v>
<v t="ekr.20230831011820.369"><vh>SymbolNode.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.370"><vh>class MypyFile</vh>
<v t="ekr.20230831011820.371"><vh>MypyFile.__init__</vh></v>
<v t="ekr.20230831011820.372"><vh>MypyFile.local_definitions</vh></v>
<v t="ekr.20230831011820.373"><vh>MypyFile.name</vh></v>
<v t="ekr.20230831011820.374"><vh>MypyFile.fullname</vh></v>
<v t="ekr.20230831011820.375"><vh>MypyFile.accept</vh></v>
<v t="ekr.20230831011820.376"><vh>MypyFile.is_package_init_file</vh></v>
<v t="ekr.20230831011820.377"><vh>MypyFile.is_future_flag_set</vh></v>
<v t="ekr.20230831011820.378"><vh>MypyFile.serialize</vh></v>
<v t="ekr.20230831011820.379"><vh>MypyFile.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.380"><vh>class ImportBase</vh>
<v t="ekr.20230831011820.381"><vh>ImportBase.__init__</vh></v>
</v>
<v t="ekr.20230831011820.382"><vh>class Import</vh>
<v t="ekr.20230831011820.383"><vh>Import.__init__</vh></v>
<v t="ekr.20230831011820.384"><vh>Import.accept</vh></v>
</v>
<v t="ekr.20230831011820.385"><vh>class ImportFrom</vh>
<v t="ekr.20230831011820.386"><vh>ImportFrom.__init__</vh></v>
<v t="ekr.20230831011820.387"><vh>ImportFrom.accept</vh></v>
</v>
<v t="ekr.20230831011820.388"><vh>class ImportAll</vh>
<v t="ekr.20230831011820.389"><vh>ImportAll.__init__</vh></v>
<v t="ekr.20230831011820.390"><vh>ImportAll.accept</vh></v>
</v>
<v t="ekr.20230831011820.391"><vh>class FuncBase</vh>
<v t="ekr.20230831011820.392"><vh>FuncBase.__init__</vh></v>
<v t="ekr.20230831011820.393"><vh>FuncBase.name</vh></v>
<v t="ekr.20230831011820.394"><vh>FuncBase.fullname</vh></v>
</v>
<v t="ekr.20230831011820.395"><vh>class OverloadedFuncDef</vh>
<v t="ekr.20230831011820.396"><vh>OverloadedFuncDef.__init__</vh></v>
<v t="ekr.20230831011820.397"><vh>OverloadedFuncDef.name</vh></v>
<v t="ekr.20230831011820.398"><vh>OverloadedFuncDef.accept</vh></v>
<v t="ekr.20230831011820.399"><vh>OverloadedFuncDef.serialize</vh></v>
<v t="ekr.20230831011820.400"><vh>OverloadedFuncDef.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.401"><vh>class Argument</vh>
<v t="ekr.20230831011820.402"><vh>Argument.__init__</vh></v>
<v t="ekr.20230831011820.403"><vh>Argument.set_line</vh></v>
</v>
<v t="ekr.20230831011820.404"><vh>class FuncItem</vh>
<v t="ekr.20230831011820.405"><vh>FuncItem.__init__</vh></v>
<v t="ekr.20230831011820.406"><vh>FuncItem.max_fixed_argc</vh></v>
<v t="ekr.20230831011820.407"><vh>FuncItem.is_dynamic</vh></v>
</v>
<v t="ekr.20230831011820.408"><vh>class FuncDef</vh>
<v t="ekr.20230831011820.409"><vh>FuncDef.__init__</vh></v>
<v t="ekr.20230831011820.410"><vh>FuncDef.name</vh></v>
<v t="ekr.20230831011820.411"><vh>FuncDef.accept</vh></v>
<v t="ekr.20230831011820.412"><vh>FuncDef.serialize</vh></v>
<v t="ekr.20230831011820.413"><vh>FuncDef.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.414"><vh>class Decorator</vh>
<v t="ekr.20230831011820.415"><vh>Decorator.__init__</vh></v>
<v t="ekr.20230831011820.416"><vh>Decorator.name</vh></v>
<v t="ekr.20230831011820.417"><vh>Decorator.fullname</vh></v>
<v t="ekr.20230831011820.418"><vh>Decorator.is_final</vh></v>
<v t="ekr.20230831011820.419"><vh>Decorator.info</vh></v>
<v t="ekr.20230831011820.420"><vh>Decorator.type</vh></v>
<v t="ekr.20230831011820.421"><vh>Decorator.accept</vh></v>
<v t="ekr.20230831011820.422"><vh>Decorator.serialize</vh></v>
<v t="ekr.20230831011820.423"><vh>Decorator.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.424"><vh>class Var</vh>
<v t="ekr.20230831011820.425"><vh>Var.__init__</vh></v>
<v t="ekr.20230831011820.426"><vh>Var.name</vh></v>
<v t="ekr.20230831011820.427"><vh>Var.fullname</vh></v>
<v t="ekr.20230831011820.428"><vh>Var.accept</vh></v>
<v t="ekr.20230831011820.429"><vh>Var.serialize</vh></v>
<v t="ekr.20230831011820.430"><vh>Var.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.431"><vh>class ClassDef</vh>
<v t="ekr.20230831011820.432"><vh>ClassDef.__init__</vh></v>
<v t="ekr.20230831011820.433"><vh>ClassDef.fullname</vh></v>
<v t="ekr.20230831011820.434"><vh>ClassDef.fullname</vh></v>
<v t="ekr.20230831011820.435"><vh>ClassDef.accept</vh></v>
<v t="ekr.20230831011820.436"><vh>ClassDef.is_generic</vh></v>
<v t="ekr.20230831011820.437"><vh>ClassDef.serialize</vh></v>
<v t="ekr.20230831011820.438"><vh>ClassDef.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.439"><vh>class GlobalDecl</vh>
<v t="ekr.20230831011820.440"><vh>GlobalDecl.__init__</vh></v>
<v t="ekr.20230831011820.441"><vh>GlobalDecl.accept</vh></v>
</v>
<v t="ekr.20230831011820.442"><vh>class NonlocalDecl</vh>
<v t="ekr.20230831011820.443"><vh>NonlocalDecl.__init__</vh></v>
<v t="ekr.20230831011820.444"><vh>NonlocalDecl.accept</vh></v>
</v>
<v t="ekr.20230831011820.445"><vh>class Block</vh>
<v t="ekr.20230831011820.446"><vh>Block.__init__</vh></v>
<v t="ekr.20230831011820.447"><vh>Block.accept</vh></v>
</v>
<v t="ekr.20230831011820.448"><vh>class ExpressionStmt</vh>
<v t="ekr.20230831011820.449"><vh>ExpressionStmt.__init__</vh></v>
<v t="ekr.20230831011820.450"><vh>ExpressionStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.451"><vh>class AssignmentStmt</vh>
<v t="ekr.20230831011820.452"><vh>AssignmentStmt.__init__</vh></v>
<v t="ekr.20230831011820.453"><vh>AssignmentStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.454"><vh>class OperatorAssignmentStmt</vh>
<v t="ekr.20230831011820.455"><vh>OperatorAssignmentStmt.__init__</vh></v>
<v t="ekr.20230831011820.456"><vh>OperatorAssignmentStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.457"><vh>class WhileStmt</vh>
<v t="ekr.20230831011820.458"><vh>WhileStmt.__init__</vh></v>
<v t="ekr.20230831011820.459"><vh>WhileStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.460"><vh>class ForStmt</vh>
<v t="ekr.20230831011820.461"><vh>ForStmt.__init__</vh></v>
<v t="ekr.20230831011820.462"><vh>ForStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.463"><vh>class ReturnStmt</vh>
<v t="ekr.20230831011820.464"><vh>ReturnStmt.__init__</vh></v>
<v t="ekr.20230831011820.465"><vh>ReturnStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.466"><vh>class AssertStmt</vh>
<v t="ekr.20230831011820.467"><vh>AssertStmt.__init__</vh></v>
<v t="ekr.20230831011820.468"><vh>AssertStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.469"><vh>class DelStmt</vh>
<v t="ekr.20230831011820.470"><vh>DelStmt.__init__</vh></v>
<v t="ekr.20230831011820.471"><vh>DelStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.472"><vh>class BreakStmt</vh>
<v t="ekr.20230831011820.473"><vh>BreakStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.474"><vh>class ContinueStmt</vh>
<v t="ekr.20230831011820.475"><vh>ContinueStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.476"><vh>class PassStmt</vh>
<v t="ekr.20230831011820.477"><vh>PassStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.478"><vh>class IfStmt</vh>
<v t="ekr.20230831011820.479"><vh>IfStmt.__init__</vh></v>
<v t="ekr.20230831011820.480"><vh>IfStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.481"><vh>class RaiseStmt</vh>
<v t="ekr.20230831011820.482"><vh>RaiseStmt.__init__</vh></v>
<v t="ekr.20230831011820.483"><vh>RaiseStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.484"><vh>class TryStmt</vh>
<v t="ekr.20230831011820.485"><vh>TryStmt.__init__</vh></v>
<v t="ekr.20230831011820.486"><vh>TryStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.487"><vh>class WithStmt</vh>
<v t="ekr.20230831011820.488"><vh>WithStmt.__init__</vh></v>
<v t="ekr.20230831011820.489"><vh>WithStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.490"><vh>class MatchStmt</vh>
<v t="ekr.20230831011820.491"><vh>MatchStmt.__init__</vh></v>
<v t="ekr.20230831011820.492"><vh>MatchStmt.accept</vh></v>
</v>
<v t="ekr.20230831011820.493"><vh>class IntExpr</vh>
<v t="ekr.20230831011820.494"><vh>IntExpr.__init__</vh></v>
<v t="ekr.20230831011820.495"><vh>IntExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.496"><vh>class StrExpr</vh>
<v t="ekr.20230831011820.497"><vh>StrExpr.__init__</vh></v>
<v t="ekr.20230831011820.498"><vh>StrExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.499"><vh>function: is_StrExpr_list</vh></v>
<v t="ekr.20230831011820.500"><vh>class BytesExpr</vh>
<v t="ekr.20230831011820.501"><vh>BytesExpr.__init__</vh></v>
<v t="ekr.20230831011820.502"><vh>BytesExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.503"><vh>class FloatExpr</vh>
<v t="ekr.20230831011820.504"><vh>FloatExpr.__init__</vh></v>
<v t="ekr.20230831011820.505"><vh>FloatExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.506"><vh>class ComplexExpr</vh>
<v t="ekr.20230831011820.507"><vh>ComplexExpr.__init__</vh></v>
<v t="ekr.20230831011820.508"><vh>ComplexExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.509"><vh>class EllipsisExpr</vh>
<v t="ekr.20230831011820.510"><vh>EllipsisExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.511"><vh>class StarExpr</vh>
<v t="ekr.20230831011820.512"><vh>StarExpr.__init__</vh></v>
<v t="ekr.20230831011820.513"><vh>StarExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.514"><vh>class RefExpr</vh>
<v t="ekr.20230831011820.515"><vh>RefExpr.__init__</vh></v>
<v t="ekr.20230831011820.516"><vh>RefExpr.fullname</vh></v>
<v t="ekr.20230831011820.517"><vh>RefExpr.fullname</vh></v>
</v>
<v t="ekr.20230831011820.518"><vh>class NameExpr</vh>
<v t="ekr.20230831011820.519"><vh>NameExpr.__init__</vh></v>
<v t="ekr.20230831011820.520"><vh>NameExpr.accept</vh></v>
<v t="ekr.20230831011820.521"><vh>NameExpr.serialize</vh></v>
</v>
<v t="ekr.20230831011820.522"><vh>class MemberExpr</vh>
<v t="ekr.20230831011820.523"><vh>MemberExpr.__init__</vh></v>
<v t="ekr.20230831011820.524"><vh>MemberExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.525"><vh>class ArgKind</vh>
<v t="ekr.20230831011820.526"><vh>ArgKind.is_positional</vh></v>
<v t="ekr.20230831011820.527"><vh>ArgKind.is_named</vh></v>
<v t="ekr.20230831011820.528"><vh>ArgKind.is_required</vh></v>
<v t="ekr.20230831011820.529"><vh>ArgKind.is_optional</vh></v>
<v t="ekr.20230831011820.530"><vh>ArgKind.is_star</vh></v>
</v>
<v t="ekr.20230831011820.531"><vh>class CallExpr</vh>
<v t="ekr.20230831011820.532"><vh>CallExpr.__init__</vh></v>
<v t="ekr.20230831011820.533"><vh>CallExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.534"><vh>class YieldFromExpr</vh>
<v t="ekr.20230831011820.535"><vh>YieldFromExpr.__init__</vh></v>
<v t="ekr.20230831011820.536"><vh>YieldFromExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.537"><vh>class YieldExpr</vh>
<v t="ekr.20230831011820.538"><vh>YieldExpr.__init__</vh></v>
<v t="ekr.20230831011820.539"><vh>YieldExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.540"><vh>class IndexExpr</vh>
<v t="ekr.20230831011820.541"><vh>IndexExpr.__init__</vh></v>
<v t="ekr.20230831011820.542"><vh>IndexExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.543"><vh>class UnaryExpr</vh>
<v t="ekr.20230831011820.544"><vh>UnaryExpr.__init__</vh></v>
<v t="ekr.20230831011820.545"><vh>UnaryExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.546"><vh>class AssignmentExpr</vh>
<v t="ekr.20230831011820.547"><vh>AssignmentExpr.__init__</vh></v>
<v t="ekr.20230831011820.548"><vh>AssignmentExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.549"><vh>class OpExpr</vh>
<v t="ekr.20230831011820.550"><vh>OpExpr.__init__</vh></v>
<v t="ekr.20230831011820.551"><vh>OpExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.552"><vh>class ComparisonExpr</vh>
<v t="ekr.20230831011820.553"><vh>ComparisonExpr.__init__</vh></v>
<v t="ekr.20230831011820.554"><vh>ComparisonExpr.pairwise</vh></v>
<v t="ekr.20230831011820.555"><vh>ComparisonExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.556"><vh>class SliceExpr</vh>
<v t="ekr.20230831011820.557"><vh>SliceExpr.__init__</vh></v>
<v t="ekr.20230831011820.558"><vh>SliceExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.559"><vh>class CastExpr</vh>
<v t="ekr.20230831011820.560"><vh>CastExpr.__init__</vh></v>
<v t="ekr.20230831011820.561"><vh>CastExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.562"><vh>class AssertTypeExpr</vh>
<v t="ekr.20230831011820.563"><vh>AssertTypeExpr.__init__</vh></v>
<v t="ekr.20230831011820.564"><vh>AssertTypeExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.565"><vh>class RevealExpr</vh>
<v t="ekr.20230831011820.566"><vh>RevealExpr.__init__</vh></v>
<v t="ekr.20230831011820.567"><vh>RevealExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.568"><vh>class SuperExpr</vh>
<v t="ekr.20230831011820.569"><vh>SuperExpr.__init__</vh></v>
<v t="ekr.20230831011820.570"><vh>SuperExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.571"><vh>class LambdaExpr</vh>
<v t="ekr.20230831011820.572"><vh>LambdaExpr.name</vh></v>
<v t="ekr.20230831011820.573"><vh>LambdaExpr.expr</vh></v>
<v t="ekr.20230831011820.574"><vh>LambdaExpr.accept</vh></v>
<v t="ekr.20230831011820.575"><vh>LambdaExpr.is_dynamic</vh></v>
</v>
<v t="ekr.20230831011820.576"><vh>class ListExpr</vh>
<v t="ekr.20230831011820.577"><vh>ListExpr.__init__</vh></v>
<v t="ekr.20230831011820.578"><vh>ListExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.579"><vh>class DictExpr</vh>
<v t="ekr.20230831011820.580"><vh>DictExpr.__init__</vh></v>
<v t="ekr.20230831011820.581"><vh>DictExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.582"><vh>class TupleExpr</vh>
<v t="ekr.20230831011820.583"><vh>TupleExpr.__init__</vh></v>
<v t="ekr.20230831011820.584"><vh>TupleExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.585"><vh>class SetExpr</vh>
<v t="ekr.20230831011820.586"><vh>SetExpr.__init__</vh></v>
<v t="ekr.20230831011820.587"><vh>SetExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.588"><vh>class GeneratorExpr</vh>
<v t="ekr.20230831011820.589"><vh>GeneratorExpr.__init__</vh></v>
<v t="ekr.20230831011820.590"><vh>GeneratorExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.591"><vh>class ListComprehension</vh>
<v t="ekr.20230831011820.592"><vh>ListComprehension.__init__</vh></v>
<v t="ekr.20230831011820.593"><vh>ListComprehension.accept</vh></v>
</v>
<v t="ekr.20230831011820.594"><vh>class SetComprehension</vh>
<v t="ekr.20230831011820.595"><vh>SetComprehension.__init__</vh></v>
<v t="ekr.20230831011820.596"><vh>SetComprehension.accept</vh></v>
</v>
<v t="ekr.20230831011820.597"><vh>class DictionaryComprehension</vh>
<v t="ekr.20230831011820.598"><vh>DictionaryComprehension.__init__</vh></v>
<v t="ekr.20230831011820.599"><vh>DictionaryComprehension.accept</vh></v>
</v>
<v t="ekr.20230831011820.600"><vh>class ConditionalExpr</vh>
<v t="ekr.20230831011820.601"><vh>ConditionalExpr.__init__</vh></v>
<v t="ekr.20230831011820.602"><vh>ConditionalExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.603"><vh>class TypeApplication</vh>
<v t="ekr.20230831011820.604"><vh>TypeApplication.__init__</vh></v>
<v t="ekr.20230831011820.605"><vh>TypeApplication.accept</vh></v>
</v>
<v t="ekr.20230831011820.606"><vh>class TypeVarLikeExpr</vh>
<v t="ekr.20230831011820.607"><vh>TypeVarLikeExpr.__init__</vh></v>
<v t="ekr.20230831011820.608"><vh>TypeVarLikeExpr.name</vh></v>
<v t="ekr.20230831011820.609"><vh>TypeVarLikeExpr.fullname</vh></v>
</v>
<v t="ekr.20230831011820.610"><vh>class TypeVarExpr</vh>
<v t="ekr.20230831011820.611"><vh>TypeVarExpr.__init__</vh></v>
<v t="ekr.20230831011820.612"><vh>TypeVarExpr.accept</vh></v>
<v t="ekr.20230831011820.613"><vh>TypeVarExpr.serialize</vh></v>
<v t="ekr.20230831011820.614"><vh>TypeVarExpr.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.615"><vh>class ParamSpecExpr</vh>
<v t="ekr.20230831011820.616"><vh>ParamSpecExpr.accept</vh></v>
<v t="ekr.20230831011820.617"><vh>ParamSpecExpr.serialize</vh></v>
<v t="ekr.20230831011820.618"><vh>ParamSpecExpr.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.619"><vh>class TypeVarTupleExpr</vh>
<v t="ekr.20230831011820.620"><vh>TypeVarTupleExpr.__init__</vh></v>
<v t="ekr.20230831011820.621"><vh>TypeVarTupleExpr.accept</vh></v>
<v t="ekr.20230831011820.622"><vh>TypeVarTupleExpr.serialize</vh></v>
<v t="ekr.20230831011820.623"><vh>TypeVarTupleExpr.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.624"><vh>class TypeAliasExpr</vh>
<v t="ekr.20230831011820.625"><vh>TypeAliasExpr.__init__</vh></v>
<v t="ekr.20230831011820.626"><vh>TypeAliasExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.627"><vh>class NamedTupleExpr</vh>
<v t="ekr.20230831011820.628"><vh>NamedTupleExpr.__init__</vh></v>
<v t="ekr.20230831011820.629"><vh>NamedTupleExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.630"><vh>class TypedDictExpr</vh>
<v t="ekr.20230831011820.631"><vh>TypedDictExpr.__init__</vh></v>
<v t="ekr.20230831011820.632"><vh>TypedDictExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.633"><vh>class EnumCallExpr</vh>
<v t="ekr.20230831011820.634"><vh>EnumCallExpr.__init__</vh></v>
<v t="ekr.20230831011820.635"><vh>EnumCallExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.636"><vh>class PromoteExpr</vh>
<v t="ekr.20230831011820.637"><vh>PromoteExpr.__init__</vh></v>
<v t="ekr.20230831011820.638"><vh>PromoteExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.639"><vh>class NewTypeExpr</vh>
<v t="ekr.20230831011820.640"><vh>NewTypeExpr.__init__</vh></v>
<v t="ekr.20230831011820.641"><vh>NewTypeExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.642"><vh>class AwaitExpr</vh>
<v t="ekr.20230831011820.643"><vh>AwaitExpr.__init__</vh></v>
<v t="ekr.20230831011820.644"><vh>AwaitExpr.accept</vh></v>
</v>
<v t="ekr.20230831011820.645"><vh>class TempNode</vh>
<v t="ekr.20230831011820.646"><vh>TempNode.__init__</vh></v>
<v t="ekr.20230831011820.647"><vh>TempNode.__repr__</vh></v>
<v t="ekr.20230831011820.648"><vh>TempNode.accept</vh></v>
</v>
<v t="ekr.20230831011820.649"><vh>class TypeInfo</vh>
<v t="ekr.20230831011820.650"><vh>TypeInfo.__init__</vh></v>
<v t="ekr.20230831011820.651"><vh>TypeInfo.add_type_vars</vh></v>
<v t="ekr.20230831011820.652"><vh>TypeInfo.name</vh></v>
<v t="ekr.20230831011820.653"><vh>TypeInfo.fullname</vh></v>
<v t="ekr.20230831011820.654"><vh>TypeInfo.is_generic</vh></v>
<v t="ekr.20230831011820.655"><vh>TypeInfo.get</vh></v>
<v t="ekr.20230831011820.656"><vh>TypeInfo.get_containing_type_info</vh></v>
<v t="ekr.20230831011820.657"><vh>TypeInfo.protocol_members</vh></v>
<v t="ekr.20230831011820.658"><vh>TypeInfo.__getitem__</vh></v>
<v t="ekr.20230831011820.659"><vh>TypeInfo.__repr__</vh></v>
<v t="ekr.20230831011820.660"><vh>TypeInfo.__bool__</vh></v>
<v t="ekr.20230831011820.661"><vh>TypeInfo.has_readable_member</vh></v>
<v t="ekr.20230831011820.662"><vh>TypeInfo.get_method</vh></v>
<v t="ekr.20230831011820.663"><vh>TypeInfo.calculate_metaclass_type</vh></v>
<v t="ekr.20230831011820.664"><vh>TypeInfo.is_metaclass</vh></v>
<v t="ekr.20230831011820.665"><vh>TypeInfo.has_base</vh></v>
<v t="ekr.20230831011820.666"><vh>TypeInfo.direct_base_classes</vh></v>
<v t="ekr.20230831011820.667"><vh>TypeInfo.update_tuple_type</vh></v>
<v t="ekr.20230831011820.668"><vh>TypeInfo.update_typeddict_type</vh></v>
<v t="ekr.20230831011820.669"><vh>TypeInfo.__str__</vh></v>
<v t="ekr.20230831011820.670"><vh>TypeInfo.dump</vh></v>
<v t="ekr.20230831011820.671"><vh>TypeInfo.serialize</vh></v>
<v t="ekr.20230831011820.672"><vh>TypeInfo.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.673"><vh>class FakeInfo</vh>
<v t="ekr.20230831011820.674"><vh>FakeInfo.__init__</vh></v>
<v t="ekr.20230831011820.675"><vh>FakeInfo.__getattribute__</vh></v>
</v>
<v t="ekr.20230831011820.676"><vh>class TypeAlias</vh>
<v t="ekr.20230831011820.677"><vh>TypeAlias.__init__</vh></v>
<v t="ekr.20230831011820.678"><vh>TypeAlias.from_tuple_type</vh></v>
<v t="ekr.20230831011820.679"><vh>TypeAlias.from_typeddict_type</vh></v>
<v t="ekr.20230831011820.680"><vh>TypeAlias.name</vh></v>
<v t="ekr.20230831011820.681"><vh>TypeAlias.fullname</vh></v>
<v t="ekr.20230831011820.682"><vh>TypeAlias.has_param_spec_type</vh></v>
<v t="ekr.20230831011820.683"><vh>TypeAlias.serialize</vh></v>
<v t="ekr.20230831011820.684"><vh>TypeAlias.accept</vh></v>
<v t="ekr.20230831011820.685"><vh>TypeAlias.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.686"><vh>class PlaceholderNode</vh>
<v t="ekr.20230831011820.687"><vh>PlaceholderNode.__init__</vh></v>
<v t="ekr.20230831011820.688"><vh>PlaceholderNode.name</vh></v>
<v t="ekr.20230831011820.689"><vh>PlaceholderNode.fullname</vh></v>
<v t="ekr.20230831011820.690"><vh>PlaceholderNode.serialize</vh></v>
<v t="ekr.20230831011820.691"><vh>PlaceholderNode.accept</vh></v>
</v>
<v t="ekr.20230831011820.692"><vh>class SymbolTableNode</vh>
<v t="ekr.20230831011820.693"><vh>SymbolTableNode.__init__</vh></v>
<v t="ekr.20230831011820.694"><vh>SymbolTableNode.fullname</vh></v>
<v t="ekr.20230831011820.695"><vh>SymbolTableNode.type</vh></v>
<v t="ekr.20230831011820.696"><vh>SymbolTableNode.copy</vh></v>
<v t="ekr.20230831011820.697"><vh>SymbolTableNode.__str__</vh></v>
<v t="ekr.20230831011820.698"><vh>SymbolTableNode.serialize</vh></v>
<v t="ekr.20230831011820.699"><vh>SymbolTableNode.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.700"><vh>class SymbolTable</vh>
<v t="ekr.20230831011820.701"><vh>SymbolTable.__str__</vh></v>
<v t="ekr.20230831011820.702"><vh>SymbolTable.copy</vh></v>
<v t="ekr.20230831011820.703"><vh>SymbolTable.serialize</vh></v>
<v t="ekr.20230831011820.704"><vh>SymbolTable.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.705"><vh>class DataclassTransformSpec</vh>
<v t="ekr.20230831011820.706"><vh>DataclassTransformSpec.__init__</vh></v>
<v t="ekr.20230831011820.707"><vh>DataclassTransformSpec.serialize</vh></v>
<v t="ekr.20230831011820.708"><vh>DataclassTransformSpec.deserialize</vh></v>
</v>
<v t="ekr.20230831011820.709"><vh>function: get_flags</vh></v>
<v t="ekr.20230831011820.710"><vh>function: set_flags</vh></v>
<v t="ekr.20230831011820.711"><vh>function: get_member_expr_fullname</vh></v>
<v t="ekr.20230831011820.712"><vh>function: check_arg_kinds</vh></v>
<v t="ekr.20230831011820.713"><vh>function: check_arg_names</vh></v>
<v t="ekr.20230831011820.714"><vh>function: is_class_var</vh></v>
<v t="ekr.20230831011820.715"><vh>function: is_final_node</vh></v>
<v t="ekr.20230831011820.716"><vh>function: local_definitions</vh></v>
</v>
<v t="ekr.20230831011820.717"><vh>@clean operators.py</vh></v>
<v t="ekr.20230831011820.954"><vh>@clean refinfo.py</vh>
<v t="ekr.20230831011820.955"><vh>&lt;&lt; refinfo.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.956"><vh>class RefInfoVisitor</vh>
<v t="ekr.20230831011820.957"><vh>RefInfoVisitor.__init__</vh></v>
<v t="ekr.20230831011820.958"><vh>RefInfoVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011820.959"><vh>RefInfoVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011820.960"><vh>RefInfoVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011820.961"><vh>RefInfoVisitor.record_ref_expr</vh></v>
</v>
<v t="ekr.20230831011820.962"><vh>function: type_fullname</vh></v>
<v t="ekr.20230831011820.963"><vh>function: get_undocumented_ref_info_json</vh></v>
</v>
<v t="ekr.20230831011820.2133"><vh>@clean traverser.py</vh>
<v t="ekr.20230831011821.1"><vh>&lt;&lt; traverser.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.2"><vh>class TraverserVisitor</vh>
<v t="ekr.20230831011821.3"><vh>TraverserVisitor.__init__</vh></v>
<v t="ekr.20230831011821.4"><vh>TraverserVisitor.visit_mypy_file</vh></v>
<v t="ekr.20230831011821.5"><vh>TraverserVisitor.visit_block</vh></v>
<v t="ekr.20230831011821.6"><vh>TraverserVisitor.visit_func</vh></v>
<v t="ekr.20230831011821.7"><vh>TraverserVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011821.8"><vh>TraverserVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011821.9"><vh>TraverserVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011821.10"><vh>TraverserVisitor.visit_decorator</vh></v>
<v t="ekr.20230831011821.11"><vh>TraverserVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20230831011821.12"><vh>TraverserVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011821.13"><vh>TraverserVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011821.14"><vh>TraverserVisitor.visit_while_stmt</vh></v>
<v t="ekr.20230831011821.15"><vh>TraverserVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011821.16"><vh>TraverserVisitor.visit_return_stmt</vh></v>
<v t="ekr.20230831011821.17"><vh>TraverserVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20230831011821.18"><vh>TraverserVisitor.visit_del_stmt</vh></v>
<v t="ekr.20230831011821.19"><vh>TraverserVisitor.visit_if_stmt</vh></v>
<v t="ekr.20230831011821.20"><vh>TraverserVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20230831011821.21"><vh>TraverserVisitor.visit_try_stmt</vh></v>
<v t="ekr.20230831011821.22"><vh>TraverserVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011821.23"><vh>TraverserVisitor.visit_match_stmt</vh></v>
<v t="ekr.20230831011821.24"><vh>TraverserVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011821.25"><vh>TraverserVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011821.26"><vh>TraverserVisitor.visit_yield_expr</vh></v>
<v t="ekr.20230831011821.27"><vh>TraverserVisitor.visit_call_expr</vh></v>
<v t="ekr.20230831011821.28"><vh>TraverserVisitor.visit_op_expr</vh></v>
<v t="ekr.20230831011821.29"><vh>TraverserVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20230831011821.30"><vh>TraverserVisitor.visit_slice_expr</vh></v>
<v t="ekr.20230831011821.31"><vh>TraverserVisitor.visit_cast_expr</vh></v>
<v t="ekr.20230831011821.32"><vh>TraverserVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011821.33"><vh>TraverserVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20230831011821.34"><vh>TraverserVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011821.35"><vh>TraverserVisitor.visit_unary_expr</vh></v>
<v t="ekr.20230831011821.36"><vh>TraverserVisitor.visit_list_expr</vh></v>
<v t="ekr.20230831011821.37"><vh>TraverserVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20230831011821.38"><vh>TraverserVisitor.visit_dict_expr</vh></v>
<v t="ekr.20230831011821.39"><vh>TraverserVisitor.visit_set_expr</vh></v>
<v t="ekr.20230831011821.40"><vh>TraverserVisitor.visit_index_expr</vh></v>
<v t="ekr.20230831011821.41"><vh>TraverserVisitor.visit_generator_expr</vh></v>
<v t="ekr.20230831011821.42"><vh>TraverserVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011821.43"><vh>TraverserVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20230831011821.44"><vh>TraverserVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20230831011821.45"><vh>TraverserVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20230831011821.46"><vh>TraverserVisitor.visit_type_application</vh></v>
<v t="ekr.20230831011821.47"><vh>TraverserVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20230831011821.48"><vh>TraverserVisitor.visit_star_expr</vh></v>
<v t="ekr.20230831011821.49"><vh>TraverserVisitor.visit_await_expr</vh></v>
<v t="ekr.20230831011821.50"><vh>TraverserVisitor.visit_super_expr</vh></v>
<v t="ekr.20230831011821.51"><vh>TraverserVisitor.visit_as_pattern</vh></v>
<v t="ekr.20230831011821.52"><vh>TraverserVisitor.visit_or_pattern</vh></v>
<v t="ekr.20230831011821.53"><vh>TraverserVisitor.visit_value_pattern</vh></v>
<v t="ekr.20230831011821.54"><vh>TraverserVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20230831011821.55"><vh>TraverserVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20230831011821.56"><vh>TraverserVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20230831011821.57"><vh>TraverserVisitor.visit_class_pattern</vh></v>
<v t="ekr.20230831011821.58"><vh>TraverserVisitor.visit_import</vh></v>
<v t="ekr.20230831011821.59"><vh>TraverserVisitor.visit_import_from</vh></v>
</v>
<v t="ekr.20230831011821.60"><vh>class ExtendedTraverserVisitor</vh>
<v t="ekr.20230831011821.61"><vh>ExtendedTraverserVisitor.visit</vh></v>
<v t="ekr.20230831011821.62"><vh>ExtendedTraverserVisitor.visit_mypy_file</vh></v>
<v t="ekr.20230831011821.63"><vh>ExtendedTraverserVisitor.visit_import</vh></v>
<v t="ekr.20230831011821.64"><vh>ExtendedTraverserVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011821.65"><vh>ExtendedTraverserVisitor.visit_import_all</vh></v>
<v t="ekr.20230831011821.66"><vh>ExtendedTraverserVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011821.67"><vh>ExtendedTraverserVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011821.68"><vh>ExtendedTraverserVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011821.69"><vh>ExtendedTraverserVisitor.visit_global_decl</vh></v>
<v t="ekr.20230831011821.70"><vh>ExtendedTraverserVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20230831011821.71"><vh>ExtendedTraverserVisitor.visit_decorator</vh></v>
<v t="ekr.20230831011821.72"><vh>ExtendedTraverserVisitor.visit_type_alias</vh></v>
<v t="ekr.20230831011821.73"><vh>ExtendedTraverserVisitor.visit_block</vh></v>
<v t="ekr.20230831011821.74"><vh>ExtendedTraverserVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20230831011821.75"><vh>ExtendedTraverserVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011821.76"><vh>ExtendedTraverserVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011821.77"><vh>ExtendedTraverserVisitor.visit_while_stmt</vh></v>
<v t="ekr.20230831011821.78"><vh>ExtendedTraverserVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011821.79"><vh>ExtendedTraverserVisitor.visit_return_stmt</vh></v>
<v t="ekr.20230831011821.80"><vh>ExtendedTraverserVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20230831011821.81"><vh>ExtendedTraverserVisitor.visit_del_stmt</vh></v>
<v t="ekr.20230831011821.82"><vh>ExtendedTraverserVisitor.visit_if_stmt</vh></v>
<v t="ekr.20230831011821.83"><vh>ExtendedTraverserVisitor.visit_break_stmt</vh></v>
<v t="ekr.20230831011821.84"><vh>ExtendedTraverserVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20230831011821.85"><vh>ExtendedTraverserVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20230831011821.86"><vh>ExtendedTraverserVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20230831011821.87"><vh>ExtendedTraverserVisitor.visit_try_stmt</vh></v>
<v t="ekr.20230831011821.88"><vh>ExtendedTraverserVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011821.89"><vh>ExtendedTraverserVisitor.visit_match_stmt</vh></v>
<v t="ekr.20230831011821.90"><vh>ExtendedTraverserVisitor.visit_int_expr</vh></v>
<v t="ekr.20230831011821.91"><vh>ExtendedTraverserVisitor.visit_str_expr</vh></v>
<v t="ekr.20230831011821.92"><vh>ExtendedTraverserVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20230831011821.93"><vh>ExtendedTraverserVisitor.visit_float_expr</vh></v>
<v t="ekr.20230831011821.94"><vh>ExtendedTraverserVisitor.visit_complex_expr</vh></v>
<v t="ekr.20230831011821.95"><vh>ExtendedTraverserVisitor.visit_ellipsis</vh></v>
<v t="ekr.20230831011821.96"><vh>ExtendedTraverserVisitor.visit_star_expr</vh></v>
<v t="ekr.20230831011821.97"><vh>ExtendedTraverserVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011821.98"><vh>ExtendedTraverserVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011821.99"><vh>ExtendedTraverserVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011821.100"><vh>ExtendedTraverserVisitor.visit_yield_expr</vh></v>
<v t="ekr.20230831011821.101"><vh>ExtendedTraverserVisitor.visit_call_expr</vh></v>
<v t="ekr.20230831011821.102"><vh>ExtendedTraverserVisitor.visit_op_expr</vh></v>
<v t="ekr.20230831011821.103"><vh>ExtendedTraverserVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20230831011821.104"><vh>ExtendedTraverserVisitor.visit_cast_expr</vh></v>
<v t="ekr.20230831011821.105"><vh>ExtendedTraverserVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011821.106"><vh>ExtendedTraverserVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20230831011821.107"><vh>ExtendedTraverserVisitor.visit_super_expr</vh></v>
<v t="ekr.20230831011821.108"><vh>ExtendedTraverserVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011821.109"><vh>ExtendedTraverserVisitor.visit_unary_expr</vh></v>
<v t="ekr.20230831011821.110"><vh>ExtendedTraverserVisitor.visit_list_expr</vh></v>
<v t="ekr.20230831011821.111"><vh>ExtendedTraverserVisitor.visit_dict_expr</vh></v>
<v t="ekr.20230831011821.112"><vh>ExtendedTraverserVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20230831011821.113"><vh>ExtendedTraverserVisitor.visit_set_expr</vh></v>
<v t="ekr.20230831011821.114"><vh>ExtendedTraverserVisitor.visit_index_expr</vh></v>
<v t="ekr.20230831011821.115"><vh>ExtendedTraverserVisitor.visit_type_application</vh></v>
<v t="ekr.20230831011821.116"><vh>ExtendedTraverserVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20230831011821.117"><vh>ExtendedTraverserVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20230831011821.118"><vh>ExtendedTraverserVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20230831011821.119"><vh>ExtendedTraverserVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011821.120"><vh>ExtendedTraverserVisitor.visit_generator_expr</vh></v>
<v t="ekr.20230831011821.121"><vh>ExtendedTraverserVisitor.visit_slice_expr</vh></v>
<v t="ekr.20230831011821.122"><vh>ExtendedTraverserVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20230831011821.123"><vh>ExtendedTraverserVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20230831011821.124"><vh>ExtendedTraverserVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011821.125"><vh>ExtendedTraverserVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011821.126"><vh>ExtendedTraverserVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011821.127"><vh>ExtendedTraverserVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011821.128"><vh>ExtendedTraverserVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011821.129"><vh>ExtendedTraverserVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011821.130"><vh>ExtendedTraverserVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20230831011821.131"><vh>ExtendedTraverserVisitor.visit_await_expr</vh></v>
<v t="ekr.20230831011821.132"><vh>ExtendedTraverserVisitor.visit_as_pattern</vh></v>
<v t="ekr.20230831011821.133"><vh>ExtendedTraverserVisitor.visit_or_pattern</vh></v>
<v t="ekr.20230831011821.134"><vh>ExtendedTraverserVisitor.visit_value_pattern</vh></v>
<v t="ekr.20230831011821.135"><vh>ExtendedTraverserVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20230831011821.136"><vh>ExtendedTraverserVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20230831011821.137"><vh>ExtendedTraverserVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20230831011821.138"><vh>ExtendedTraverserVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20230831011821.139"><vh>ExtendedTraverserVisitor.visit_class_pattern</vh></v>
</v>
<v t="ekr.20230831011821.140"><vh>class ReturnSeeker</vh>
<v t="ekr.20230831011821.141"><vh>ReturnSeeker.__init__</vh></v>
<v t="ekr.20230831011821.142"><vh>ReturnSeeker.visit_return_stmt</vh></v>
</v>
<v t="ekr.20230831011821.143"><vh>function: has_return_statement</vh></v>
<v t="ekr.20230831011821.144"><vh>class FuncCollectorBase</vh>
<v t="ekr.20230831011821.145"><vh>FuncCollectorBase.__init__</vh></v>
<v t="ekr.20230831011821.146"><vh>FuncCollectorBase.visit_func_def</vh></v>
</v>
<v t="ekr.20230831011821.147"><vh>class YieldSeeker</vh>
<v t="ekr.20230831011821.148"><vh>YieldSeeker.__init__</vh></v>
<v t="ekr.20230831011821.149"><vh>YieldSeeker.visit_yield_expr</vh></v>
</v>
<v t="ekr.20230831011821.150"><vh>function: has_yield_expression</vh></v>
<v t="ekr.20230831011821.151"><vh>class YieldFromSeeker</vh>
<v t="ekr.20230831011821.152"><vh>YieldFromSeeker.__init__</vh></v>
<v t="ekr.20230831011821.153"><vh>YieldFromSeeker.visit_yield_from_expr</vh></v>
</v>
<v t="ekr.20230831011821.154"><vh>function: has_yield_from_expression</vh></v>
<v t="ekr.20230831011821.155"><vh>class AwaitSeeker</vh>
<v t="ekr.20230831011821.156"><vh>AwaitSeeker.__init__</vh></v>
<v t="ekr.20230831011821.157"><vh>AwaitSeeker.visit_await_expr</vh></v>
</v>
<v t="ekr.20230831011821.158"><vh>function: has_await_expression</vh></v>
<v t="ekr.20230831011821.159"><vh>class ReturnCollector</vh>
<v t="ekr.20230831011821.160"><vh>ReturnCollector.__init__</vh></v>
<v t="ekr.20230831011821.161"><vh>ReturnCollector.visit_return_stmt</vh></v>
</v>
<v t="ekr.20230831011821.162"><vh>function: all_return_statements</vh></v>
<v t="ekr.20230831011821.163"><vh>class YieldCollector</vh>
<v t="ekr.20230831011821.164"><vh>YieldCollector.__init__</vh></v>
<v t="ekr.20230831011821.165"><vh>YieldCollector.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011821.166"><vh>YieldCollector.visit_yield_expr</vh></v>
</v>
<v t="ekr.20230831011821.167"><vh>function: all_yield_expressions</vh></v>
<v t="ekr.20230831011821.168"><vh>class YieldFromCollector</vh>
<v t="ekr.20230831011821.169"><vh>YieldFromCollector.__init__</vh></v>
<v t="ekr.20230831011821.170"><vh>YieldFromCollector.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011821.171"><vh>YieldFromCollector.visit_yield_from_expr</vh></v>
</v>
<v t="ekr.20230831011821.172"><vh>function: all_yield_from_expressions</vh></v>
</v>
<v t="ekr.20230831011821.173"><vh>@clean treetransform.py</vh>
<v t="ekr.20230831011821.175"><vh>&lt;&lt; treetransform.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011821.176"><vh>class TransformVisitor</vh>
<v t="ekr.20230831011821.177"><vh>TransformVisitor.__init__</vh></v>
<v t="ekr.20230831011821.178"><vh>TransformVisitor.visit_mypy_file</vh></v>
<v t="ekr.20230831011821.179"><vh>TransformVisitor.visit_import</vh></v>
<v t="ekr.20230831011821.180"><vh>TransformVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011821.181"><vh>TransformVisitor.visit_import_all</vh></v>
<v t="ekr.20230831011821.182"><vh>TransformVisitor.copy_argument</vh></v>
<v t="ekr.20230831011821.183"><vh>TransformVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011821.184"><vh>TransformVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20230831011821.185"><vh>TransformVisitor.copy_function_attributes</vh></v>
<v t="ekr.20230831011821.186"><vh>TransformVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011821.187"><vh>TransformVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011821.188"><vh>TransformVisitor.visit_global_decl</vh></v>
<v t="ekr.20230831011821.189"><vh>TransformVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20230831011821.190"><vh>TransformVisitor.visit_block</vh></v>
<v t="ekr.20230831011821.191"><vh>TransformVisitor.visit_decorator</vh></v>
<v t="ekr.20230831011821.192"><vh>TransformVisitor.visit_var</vh></v>
<v t="ekr.20230831011821.193"><vh>TransformVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20230831011821.194"><vh>TransformVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011821.195"><vh>TransformVisitor.duplicate_assignment</vh></v>
<v t="ekr.20230831011821.196"><vh>TransformVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011821.197"><vh>TransformVisitor.visit_while_stmt</vh></v>
<v t="ekr.20230831011821.198"><vh>TransformVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011821.199"><vh>TransformVisitor.visit_return_stmt</vh></v>
<v t="ekr.20230831011821.200"><vh>TransformVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20230831011821.201"><vh>TransformVisitor.visit_del_stmt</vh></v>
<v t="ekr.20230831011821.202"><vh>TransformVisitor.visit_if_stmt</vh></v>
<v t="ekr.20230831011821.203"><vh>TransformVisitor.visit_break_stmt</vh></v>
<v t="ekr.20230831011821.204"><vh>TransformVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20230831011821.205"><vh>TransformVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20230831011821.206"><vh>TransformVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20230831011821.207"><vh>TransformVisitor.visit_try_stmt</vh></v>
<v t="ekr.20230831011821.208"><vh>TransformVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011821.209"><vh>TransformVisitor.visit_as_pattern</vh></v>
<v t="ekr.20230831011821.210"><vh>TransformVisitor.visit_or_pattern</vh></v>
<v t="ekr.20230831011821.211"><vh>TransformVisitor.visit_value_pattern</vh></v>
<v t="ekr.20230831011821.212"><vh>TransformVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20230831011821.213"><vh>TransformVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20230831011821.214"><vh>TransformVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20230831011821.215"><vh>TransformVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20230831011821.216"><vh>TransformVisitor.visit_class_pattern</vh></v>
<v t="ekr.20230831011821.217"><vh>TransformVisitor.visit_match_stmt</vh></v>
<v t="ekr.20230831011821.218"><vh>TransformVisitor.visit_star_expr</vh></v>
<v t="ekr.20230831011821.219"><vh>TransformVisitor.visit_int_expr</vh></v>
<v t="ekr.20230831011821.220"><vh>TransformVisitor.visit_str_expr</vh></v>
<v t="ekr.20230831011821.221"><vh>TransformVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20230831011821.222"><vh>TransformVisitor.visit_float_expr</vh></v>
<v t="ekr.20230831011821.223"><vh>TransformVisitor.visit_complex_expr</vh></v>
<v t="ekr.20230831011821.224"><vh>TransformVisitor.visit_ellipsis</vh></v>
<v t="ekr.20230831011821.225"><vh>TransformVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011821.226"><vh>TransformVisitor.duplicate_name</vh></v>
<v t="ekr.20230831011821.227"><vh>TransformVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011821.228"><vh>TransformVisitor.copy_ref</vh></v>
<v t="ekr.20230831011821.229"><vh>TransformVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011821.230"><vh>TransformVisitor.visit_yield_expr</vh></v>
<v t="ekr.20230831011821.231"><vh>TransformVisitor.visit_await_expr</vh></v>
<v t="ekr.20230831011821.232"><vh>TransformVisitor.visit_call_expr</vh></v>
<v t="ekr.20230831011821.233"><vh>TransformVisitor.visit_op_expr</vh></v>
<v t="ekr.20230831011821.234"><vh>TransformVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20230831011821.235"><vh>TransformVisitor.visit_cast_expr</vh></v>
<v t="ekr.20230831011821.236"><vh>TransformVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011821.237"><vh>TransformVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20230831011821.238"><vh>TransformVisitor.visit_super_expr</vh></v>
<v t="ekr.20230831011821.239"><vh>TransformVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011821.240"><vh>TransformVisitor.visit_unary_expr</vh></v>
<v t="ekr.20230831011821.241"><vh>TransformVisitor.visit_list_expr</vh></v>
<v t="ekr.20230831011821.242"><vh>TransformVisitor.visit_dict_expr</vh></v>
<v t="ekr.20230831011821.243"><vh>TransformVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20230831011821.244"><vh>TransformVisitor.visit_set_expr</vh></v>
<v t="ekr.20230831011821.245"><vh>TransformVisitor.visit_index_expr</vh></v>
<v t="ekr.20230831011821.246"><vh>TransformVisitor.visit_type_application</vh></v>
<v t="ekr.20230831011821.247"><vh>TransformVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20230831011821.248"><vh>TransformVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20230831011821.249"><vh>TransformVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011821.250"><vh>TransformVisitor.visit_generator_expr</vh></v>
<v t="ekr.20230831011821.251"><vh>TransformVisitor.duplicate_generator</vh></v>
<v t="ekr.20230831011821.252"><vh>TransformVisitor.visit_slice_expr</vh></v>
<v t="ekr.20230831011821.253"><vh>TransformVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20230831011821.254"><vh>TransformVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20230831011821.255"><vh>TransformVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011821.256"><vh>TransformVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011821.257"><vh>TransformVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011821.258"><vh>TransformVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20230831011821.259"><vh>TransformVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011821.260"><vh>TransformVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011821.261"><vh>TransformVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011821.262"><vh>TransformVisitor.visit__promote_expr</vh></v>
<v t="ekr.20230831011821.263"><vh>TransformVisitor.visit_temp_node</vh></v>
<v t="ekr.20230831011821.264"><vh>TransformVisitor.node</vh></v>
<v t="ekr.20230831011821.265"><vh>TransformVisitor.mypyfile</vh></v>
<v t="ekr.20230831011821.266"><vh>TransformVisitor.expr</vh></v>
<v t="ekr.20230831011821.267"><vh>TransformVisitor.stmt</vh></v>
<v t="ekr.20230831011821.268"><vh>TransformVisitor.pattern</vh></v>
<v t="ekr.20230831011821.269"><vh>TransformVisitor.optional_expr</vh></v>
<v t="ekr.20230831011821.270"><vh>TransformVisitor.block</vh></v>
<v t="ekr.20230831011821.271"><vh>TransformVisitor.optional_block</vh></v>
<v t="ekr.20230831011821.272"><vh>TransformVisitor.statements</vh></v>
<v t="ekr.20230831011821.273"><vh>TransformVisitor.expressions</vh></v>
<v t="ekr.20230831011821.274"><vh>TransformVisitor.optional_expressions</vh></v>
<v t="ekr.20230831011821.275"><vh>TransformVisitor.blocks</vh></v>
<v t="ekr.20230831011821.276"><vh>TransformVisitor.names</vh></v>
<v t="ekr.20230831011821.277"><vh>TransformVisitor.optional_names</vh></v>
<v t="ekr.20230831011821.278"><vh>TransformVisitor.type</vh></v>
<v t="ekr.20230831011821.279"><vh>TransformVisitor.optional_type</vh></v>
<v t="ekr.20230831011821.280"><vh>TransformVisitor.types</vh></v>
</v>
<v t="ekr.20230831011821.281"><vh>class FuncMapInitializer</vh>
<v t="ekr.20230831011821.282"><vh>FuncMapInitializer.__init__</vh></v>
<v t="ekr.20230831011821.283"><vh>FuncMapInitializer.visit_func_def</vh></v>
</v>
</v>
<v t="ekr.20230831011821.1068"><vh>@clean visitor.py</vh>
<v t="ekr.20230831011821.1069"><vh>&lt;&lt; visitor.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.1070"><vh>class ExpressionVisitor</vh>
<v t="ekr.20230831011821.1071"><vh>ExpressionVisitor.visit_int_expr</vh></v>
<v t="ekr.20230831011821.1072"><vh>ExpressionVisitor.visit_str_expr</vh></v>
<v t="ekr.20230831011821.1073"><vh>ExpressionVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20230831011821.1074"><vh>ExpressionVisitor.visit_float_expr</vh></v>
<v t="ekr.20230831011821.1075"><vh>ExpressionVisitor.visit_complex_expr</vh></v>
<v t="ekr.20230831011821.1076"><vh>ExpressionVisitor.visit_ellipsis</vh></v>
<v t="ekr.20230831011821.1077"><vh>ExpressionVisitor.visit_star_expr</vh></v>
<v t="ekr.20230831011821.1078"><vh>ExpressionVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011821.1079"><vh>ExpressionVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011821.1080"><vh>ExpressionVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011821.1081"><vh>ExpressionVisitor.visit_yield_expr</vh></v>
<v t="ekr.20230831011821.1082"><vh>ExpressionVisitor.visit_call_expr</vh></v>
<v t="ekr.20230831011821.1083"><vh>ExpressionVisitor.visit_op_expr</vh></v>
<v t="ekr.20230831011821.1084"><vh>ExpressionVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20230831011821.1085"><vh>ExpressionVisitor.visit_cast_expr</vh></v>
<v t="ekr.20230831011821.1086"><vh>ExpressionVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011821.1087"><vh>ExpressionVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20230831011821.1088"><vh>ExpressionVisitor.visit_super_expr</vh></v>
<v t="ekr.20230831011821.1089"><vh>ExpressionVisitor.visit_unary_expr</vh></v>
<v t="ekr.20230831011821.1090"><vh>ExpressionVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011821.1091"><vh>ExpressionVisitor.visit_list_expr</vh></v>
<v t="ekr.20230831011821.1092"><vh>ExpressionVisitor.visit_dict_expr</vh></v>
<v t="ekr.20230831011821.1093"><vh>ExpressionVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20230831011821.1094"><vh>ExpressionVisitor.visit_set_expr</vh></v>
<v t="ekr.20230831011821.1095"><vh>ExpressionVisitor.visit_index_expr</vh></v>
<v t="ekr.20230831011821.1096"><vh>ExpressionVisitor.visit_type_application</vh></v>
<v t="ekr.20230831011821.1097"><vh>ExpressionVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20230831011821.1098"><vh>ExpressionVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20230831011821.1099"><vh>ExpressionVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20230831011821.1100"><vh>ExpressionVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011821.1101"><vh>ExpressionVisitor.visit_generator_expr</vh></v>
<v t="ekr.20230831011821.1102"><vh>ExpressionVisitor.visit_slice_expr</vh></v>
<v t="ekr.20230831011821.1103"><vh>ExpressionVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20230831011821.1104"><vh>ExpressionVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20230831011821.1105"><vh>ExpressionVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011821.1106"><vh>ExpressionVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011821.1107"><vh>ExpressionVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011821.1108"><vh>ExpressionVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011821.1109"><vh>ExpressionVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011821.1110"><vh>ExpressionVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011821.1111"><vh>ExpressionVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20230831011821.1112"><vh>ExpressionVisitor.visit__promote_expr</vh></v>
<v t="ekr.20230831011821.1113"><vh>ExpressionVisitor.visit_await_expr</vh></v>
<v t="ekr.20230831011821.1114"><vh>ExpressionVisitor.visit_temp_node</vh></v>
</v>
<v t="ekr.20230831011821.1115"><vh>class StatementVisitor</vh>
<v t="ekr.20230831011821.1116"><vh>StatementVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011821.1117"><vh>StatementVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011821.1118"><vh>StatementVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011821.1119"><vh>StatementVisitor.visit_del_stmt</vh></v>
<v t="ekr.20230831011821.1120"><vh>StatementVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011821.1121"><vh>StatementVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011821.1122"><vh>StatementVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011821.1123"><vh>StatementVisitor.visit_global_decl</vh></v>
<v t="ekr.20230831011821.1124"><vh>StatementVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20230831011821.1125"><vh>StatementVisitor.visit_decorator</vh></v>
<v t="ekr.20230831011821.1126"><vh>StatementVisitor.visit_import</vh></v>
<v t="ekr.20230831011821.1127"><vh>StatementVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011821.1128"><vh>StatementVisitor.visit_import_all</vh></v>
<v t="ekr.20230831011821.1129"><vh>StatementVisitor.visit_block</vh></v>
<v t="ekr.20230831011821.1130"><vh>StatementVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20230831011821.1131"><vh>StatementVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011821.1132"><vh>StatementVisitor.visit_while_stmt</vh></v>
<v t="ekr.20230831011821.1133"><vh>StatementVisitor.visit_return_stmt</vh></v>
<v t="ekr.20230831011821.1134"><vh>StatementVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20230831011821.1135"><vh>StatementVisitor.visit_if_stmt</vh></v>
<v t="ekr.20230831011821.1136"><vh>StatementVisitor.visit_break_stmt</vh></v>
<v t="ekr.20230831011821.1137"><vh>StatementVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20230831011821.1138"><vh>StatementVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20230831011821.1139"><vh>StatementVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20230831011821.1140"><vh>StatementVisitor.visit_try_stmt</vh></v>
<v t="ekr.20230831011821.1141"><vh>StatementVisitor.visit_match_stmt</vh></v>
</v>
<v t="ekr.20230831011821.1142"><vh>class PatternVisitor</vh>
<v t="ekr.20230831011821.1143"><vh>PatternVisitor.visit_as_pattern</vh></v>
<v t="ekr.20230831011821.1144"><vh>PatternVisitor.visit_or_pattern</vh></v>
<v t="ekr.20230831011821.1145"><vh>PatternVisitor.visit_value_pattern</vh></v>
<v t="ekr.20230831011821.1146"><vh>PatternVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20230831011821.1147"><vh>PatternVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20230831011821.1148"><vh>PatternVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20230831011821.1149"><vh>PatternVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20230831011821.1150"><vh>PatternVisitor.visit_class_pattern</vh></v>
</v>
<v t="ekr.20230831011821.1151"><vh>class NodeVisitor</vh>
<v t="ekr.20230831011821.1152"><vh>NodeVisitor.visit_mypy_file</vh></v>
<v t="ekr.20230831011821.1153"><vh>NodeVisitor.visit_var</vh></v>
<v t="ekr.20230831011821.1154"><vh>NodeVisitor.visit_import</vh></v>
<v t="ekr.20230831011821.1155"><vh>NodeVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011821.1156"><vh>NodeVisitor.visit_import_all</vh></v>
<v t="ekr.20230831011821.1157"><vh>NodeVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011821.1158"><vh>NodeVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011821.1159"><vh>NodeVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011821.1160"><vh>NodeVisitor.visit_global_decl</vh></v>
<v t="ekr.20230831011821.1161"><vh>NodeVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20230831011821.1162"><vh>NodeVisitor.visit_decorator</vh></v>
<v t="ekr.20230831011821.1163"><vh>NodeVisitor.visit_type_alias</vh></v>
<v t="ekr.20230831011821.1164"><vh>NodeVisitor.visit_placeholder_node</vh></v>
<v t="ekr.20230831011821.1165"><vh>NodeVisitor.visit_block</vh></v>
<v t="ekr.20230831011821.1166"><vh>NodeVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20230831011821.1167"><vh>NodeVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011821.1168"><vh>NodeVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011821.1169"><vh>NodeVisitor.visit_while_stmt</vh></v>
<v t="ekr.20230831011821.1170"><vh>NodeVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011821.1171"><vh>NodeVisitor.visit_return_stmt</vh></v>
<v t="ekr.20230831011821.1172"><vh>NodeVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20230831011821.1173"><vh>NodeVisitor.visit_del_stmt</vh></v>
<v t="ekr.20230831011821.1174"><vh>NodeVisitor.visit_if_stmt</vh></v>
<v t="ekr.20230831011821.1175"><vh>NodeVisitor.visit_break_stmt</vh></v>
<v t="ekr.20230831011821.1176"><vh>NodeVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20230831011821.1177"><vh>NodeVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20230831011821.1178"><vh>NodeVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20230831011821.1179"><vh>NodeVisitor.visit_try_stmt</vh></v>
<v t="ekr.20230831011821.1180"><vh>NodeVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011821.1181"><vh>NodeVisitor.visit_match_stmt</vh></v>
<v t="ekr.20230831011821.1182"><vh>NodeVisitor.visit_int_expr</vh></v>
<v t="ekr.20230831011821.1183"><vh>NodeVisitor.visit_str_expr</vh></v>
<v t="ekr.20230831011821.1184"><vh>NodeVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20230831011821.1185"><vh>NodeVisitor.visit_float_expr</vh></v>
<v t="ekr.20230831011821.1186"><vh>NodeVisitor.visit_complex_expr</vh></v>
<v t="ekr.20230831011821.1187"><vh>NodeVisitor.visit_ellipsis</vh></v>
<v t="ekr.20230831011821.1188"><vh>NodeVisitor.visit_star_expr</vh></v>
<v t="ekr.20230831011821.1189"><vh>NodeVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011821.1190"><vh>NodeVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011821.1191"><vh>NodeVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011821.1192"><vh>NodeVisitor.visit_yield_expr</vh></v>
<v t="ekr.20230831011821.1193"><vh>NodeVisitor.visit_call_expr</vh></v>
<v t="ekr.20230831011821.1194"><vh>NodeVisitor.visit_op_expr</vh></v>
<v t="ekr.20230831011821.1195"><vh>NodeVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20230831011821.1196"><vh>NodeVisitor.visit_cast_expr</vh></v>
<v t="ekr.20230831011821.1197"><vh>NodeVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011821.1198"><vh>NodeVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20230831011821.1199"><vh>NodeVisitor.visit_super_expr</vh></v>
<v t="ekr.20230831011821.1200"><vh>NodeVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011821.1201"><vh>NodeVisitor.visit_unary_expr</vh></v>
<v t="ekr.20230831011821.1202"><vh>NodeVisitor.visit_list_expr</vh></v>
<v t="ekr.20230831011821.1203"><vh>NodeVisitor.visit_dict_expr</vh></v>
<v t="ekr.20230831011821.1204"><vh>NodeVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20230831011821.1205"><vh>NodeVisitor.visit_set_expr</vh></v>
<v t="ekr.20230831011821.1206"><vh>NodeVisitor.visit_index_expr</vh></v>
<v t="ekr.20230831011821.1207"><vh>NodeVisitor.visit_type_application</vh></v>
<v t="ekr.20230831011821.1208"><vh>NodeVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20230831011821.1209"><vh>NodeVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20230831011821.1210"><vh>NodeVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20230831011821.1211"><vh>NodeVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011821.1212"><vh>NodeVisitor.visit_generator_expr</vh></v>
<v t="ekr.20230831011821.1213"><vh>NodeVisitor.visit_slice_expr</vh></v>
<v t="ekr.20230831011821.1214"><vh>NodeVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20230831011821.1215"><vh>NodeVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20230831011821.1216"><vh>NodeVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011821.1217"><vh>NodeVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011821.1218"><vh>NodeVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011821.1219"><vh>NodeVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011821.1220"><vh>NodeVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011821.1221"><vh>NodeVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011821.1222"><vh>NodeVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20230831011821.1223"><vh>NodeVisitor.visit__promote_expr</vh></v>
<v t="ekr.20230831011821.1224"><vh>NodeVisitor.visit_await_expr</vh></v>
<v t="ekr.20230831011821.1225"><vh>NodeVisitor.visit_temp_node</vh></v>
<v t="ekr.20230831011821.1226"><vh>NodeVisitor.visit_as_pattern</vh></v>
<v t="ekr.20230831011821.1227"><vh>NodeVisitor.visit_or_pattern</vh></v>
<v t="ekr.20230831011821.1228"><vh>NodeVisitor.visit_value_pattern</vh></v>
<v t="ekr.20230831011821.1229"><vh>NodeVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20230831011821.1230"><vh>NodeVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20230831011821.1231"><vh>NodeVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20230831011821.1232"><vh>NodeVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20230831011821.1233"><vh>NodeVisitor.visit_class_pattern</vh></v>
</v>
</v>
</v>
<v t="ekr.20230831071117.1"><vh>--- Type checking</vh>
<v t="ekr.20230831011819.176"><vh>@clean checker.py</vh>
<v t="ekr.20230831011819.177"><vh>&lt;&lt; checker.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.178"><vh>class DeferredNode</vh></v>
<v t="ekr.20230831011819.179"><vh>class FineGrainedDeferredNode</vh></v>
<v t="ekr.20230831011819.180"><vh>class TypeRange</vh></v>
<v t="ekr.20230831011819.181"><vh>class PartialTypeScope</vh></v>
<v t="ekr.20230831011819.182"><vh>class TypeChecker</vh>
<v t="ekr.20230831011819.183"><vh>TypeChecker.__init__</vh></v>
<v t="ekr.20230831011819.184"><vh>TypeChecker.type_context</vh></v>
<v t="ekr.20230831011819.185"><vh>TypeChecker.reset</vh></v>
<v t="ekr.20230831011819.186"><vh>TypeChecker.check_first_pass</vh></v>
<v t="ekr.20230831011819.187"><vh>TypeChecker.check_second_pass</vh></v>
<v t="ekr.20230831011819.188"><vh>TypeChecker.check_partial</vh></v>
<v t="ekr.20230831011819.189"><vh>TypeChecker.check_top_level</vh></v>
<v t="ekr.20230831011819.190"><vh>TypeChecker.defer_node</vh></v>
<v t="ekr.20230831011819.191"><vh>TypeChecker.handle_cannot_determine_type</vh></v>
<v t="ekr.20230831011819.192"><vh>TypeChecker.accept</vh></v>
<v t="ekr.20230831011819.193"><vh>TypeChecker.accept_loop</vh></v>
<v t="ekr.20230831011819.194"><vh>TypeChecker.visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011819.195"><vh>TypeChecker._visit_overloaded_func_def</vh></v>
<v t="ekr.20230831011819.196"><vh>TypeChecker.extract_callable_type</vh></v>
<v t="ekr.20230831011819.197"><vh>TypeChecker.check_overlapping_overloads</vh></v>
<v t="ekr.20230831011819.198"><vh>TypeChecker.is_generator_return_type</vh></v>
<v t="ekr.20230831011819.199"><vh>TypeChecker.is_async_generator_return_type</vh></v>
<v t="ekr.20230831011819.200"><vh>TypeChecker.get_generator_yield_type</vh></v>
<v t="ekr.20230831011819.201"><vh>TypeChecker.get_generator_receive_type</vh></v>
<v t="ekr.20230831011819.202"><vh>TypeChecker.get_coroutine_return_type</vh></v>
<v t="ekr.20230831011819.203"><vh>TypeChecker.get_generator_return_type</vh></v>
<v t="ekr.20230831011819.204"><vh>TypeChecker.visit_func_def</vh></v>
<v t="ekr.20230831011819.205"><vh>TypeChecker._visit_func_def</vh></v>
<v t="ekr.20230831011819.206"><vh>TypeChecker.check_func_item</vh></v>
<v t="ekr.20230831011819.207"><vh>TypeChecker.enter_attribute_inference_context</vh></v>
<v t="ekr.20230831011819.208"><vh>TypeChecker.check_func_def</vh></v>
<v t="ekr.20230831011819.209"><vh>TypeChecker.is_var_redefined_in_outer_context</vh></v>
<v t="ekr.20230831011819.210"><vh>TypeChecker.check_unbound_return_typevar</vh></v>
<v t="ekr.20230831011819.211"><vh>TypeChecker.check_default_args</vh></v>
<v t="ekr.20230831011819.212"><vh>TypeChecker.is_forward_op_method</vh></v>
<v t="ekr.20230831011819.213"><vh>TypeChecker.is_reverse_op_method</vh></v>
<v t="ekr.20230831011819.214"><vh>TypeChecker.check_for_missing_annotations</vh></v>
<v t="ekr.20230831011819.215"><vh>TypeChecker.check___new___signature</vh></v>
<v t="ekr.20230831011819.216"><vh>TypeChecker.check_reverse_op_method</vh></v>
<v t="ekr.20230831011819.217"><vh>TypeChecker.check_overlapping_op_methods</vh></v>
<v t="ekr.20230831011819.218"><vh>TypeChecker.is_unsafe_overlapping_op</vh></v>
<v t="ekr.20230831011819.219"><vh>TypeChecker.check_inplace_operator_method</vh></v>
<v t="ekr.20230831011819.220"><vh>TypeChecker.check_getattr_method</vh></v>
<v t="ekr.20230831011819.221"><vh>TypeChecker.check_setattr_method</vh></v>
<v t="ekr.20230831011819.222"><vh>TypeChecker.check_slots_definition</vh></v>
<v t="ekr.20230831011819.223"><vh>TypeChecker.check_match_args</vh></v>
<v t="ekr.20230831011819.224"><vh>TypeChecker.expand_typevars</vh></v>
<v t="ekr.20230831011819.225"><vh>TypeChecker.check_explicit_override_decorator</vh></v>
<v t="ekr.20230831011819.226"><vh>TypeChecker.check_method_override</vh></v>
<v t="ekr.20230831011819.227"><vh>TypeChecker.check_method_or_accessor_override_for_base</vh></v>
<v t="ekr.20230831011819.228"><vh>TypeChecker.check_method_override_for_base_with_name</vh></v>
<v t="ekr.20230831011819.229"><vh>TypeChecker.bind_and_map_method</vh></v>
<v t="ekr.20230831011819.230"><vh>TypeChecker.get_op_other_domain</vh></v>
<v t="ekr.20230831011819.231"><vh>TypeChecker.check_override</vh></v>
<v t="ekr.20230831011819.232"><vh>TypeChecker.check__exit__return_type</vh></v>
<v t="ekr.20230831011819.233"><vh>TypeChecker.visit_class_def</vh></v>
<v t="ekr.20230831011819.234"><vh>TypeChecker.check_final_deletable</vh></v>
<v t="ekr.20230831011819.235"><vh>TypeChecker.check_init_subclass</vh></v>
<v t="ekr.20230831011819.236"><vh>TypeChecker.check_enum</vh></v>
<v t="ekr.20230831011819.237"><vh>TypeChecker.check_final_enum</vh></v>
<v t="ekr.20230831011819.238"><vh>TypeChecker.is_final_enum_value</vh></v>
<v t="ekr.20230831011819.239"><vh>TypeChecker.check_enum_bases</vh></v>
<v t="ekr.20230831011819.240"><vh>TypeChecker.check_enum_new</vh></v>
<v t="ekr.20230831011819.241"><vh>TypeChecker.check_protocol_variance</vh></v>
<v t="ekr.20230831011819.242"><vh>TypeChecker.check_multiple_inheritance</vh></v>
<v t="ekr.20230831011819.243"><vh>TypeChecker.determine_type_of_member</vh></v>
<v t="ekr.20230831011819.244"><vh>TypeChecker.check_compatibility</vh></v>
<v t="ekr.20230831011819.245"><vh>TypeChecker.check_metaclass_compatibility</vh></v>
<v t="ekr.20230831011819.246"><vh>TypeChecker.visit_import_from</vh></v>
<v t="ekr.20230831011819.247"><vh>TypeChecker.visit_import_all</vh></v>
<v t="ekr.20230831011819.248"><vh>TypeChecker.visit_import</vh></v>
<v t="ekr.20230831011819.249"><vh>TypeChecker.check_import</vh></v>
<v t="ekr.20230831011819.250"><vh>TypeChecker.visit_block</vh></v>
<v t="ekr.20230831011819.251"><vh>TypeChecker.should_report_unreachable_issues</vh></v>
<v t="ekr.20230831011819.252"><vh>TypeChecker.is_noop_for_reachability</vh></v>
<v t="ekr.20230831011819.253"><vh>TypeChecker.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011819.254"><vh>TypeChecker.check_type_alias_rvalue</vh></v>
<v t="ekr.20230831011819.255"><vh>TypeChecker.check_assignment</vh></v>
<v t="ekr.20230831011819.256"><vh>TypeChecker.get_variable_type_context</vh></v>
<v t="ekr.20230831011819.257"><vh>TypeChecker.try_infer_partial_generic_type_from_assignment</vh></v>
<v t="ekr.20230831011819.258"><vh>TypeChecker.check_compatibility_all_supers</vh></v>
<v t="ekr.20230831011819.259"><vh>TypeChecker.check_compatibility_super</vh></v>
<v t="ekr.20230831011819.260"><vh>TypeChecker.lvalue_type_from_base</vh></v>
<v t="ekr.20230831011819.261"><vh>TypeChecker.check_compatibility_classvar_super</vh></v>
<v t="ekr.20230831011819.262"><vh>TypeChecker.check_compatibility_final_super</vh></v>
<v t="ekr.20230831011819.263"><vh>TypeChecker.check_if_final_var_override_writable</vh></v>
<v t="ekr.20230831011819.264"><vh>TypeChecker.get_final_context</vh></v>
<v t="ekr.20230831011819.265"><vh>TypeChecker.enter_final_context</vh></v>
<v t="ekr.20230831011819.266"><vh>TypeChecker.check_final</vh></v>
<v t="ekr.20230831011819.267"><vh>TypeChecker.check_assignment_to_slots</vh></v>
<v t="ekr.20230831011819.268"><vh>TypeChecker.is_assignable_slot</vh></v>
<v t="ekr.20230831011819.269"><vh>TypeChecker.check_assignment_to_multiple_lvalues</vh></v>
<v t="ekr.20230831011819.270"><vh>TypeChecker.check_rvalue_count_in_assignment</vh></v>
<v t="ekr.20230831011819.271"><vh>TypeChecker.check_multi_assignment</vh></v>
<v t="ekr.20230831011819.272"><vh>TypeChecker.check_multi_assignment_from_union</vh></v>
<v t="ekr.20230831011819.273"><vh>TypeChecker.flatten_lvalues</vh></v>
<v t="ekr.20230831011819.274"><vh>TypeChecker.check_multi_assignment_from_tuple</vh></v>
<v t="ekr.20230831011819.275"><vh>TypeChecker.lvalue_type_for_inference</vh></v>
<v t="ekr.20230831011819.276"><vh>TypeChecker.split_around_star</vh></v>
<v t="ekr.20230831011819.277"><vh>TypeChecker.type_is_iterable</vh></v>
<v t="ekr.20230831011819.278"><vh>TypeChecker.check_multi_assignment_from_iterable</vh></v>
<v t="ekr.20230831011819.279"><vh>TypeChecker.check_lvalue</vh></v>
<v t="ekr.20230831011819.280"><vh>TypeChecker.is_definition</vh></v>
<v t="ekr.20230831011819.281"><vh>TypeChecker.infer_variable_type</vh></v>
<v t="ekr.20230831011819.282"><vh>TypeChecker.infer_partial_type</vh></v>
<v t="ekr.20230831011819.283"><vh>TypeChecker.is_valid_defaultdict_partial_value_type</vh></v>
<v t="ekr.20230831011819.284"><vh>TypeChecker.set_inferred_type</vh></v>
<v t="ekr.20230831011819.285"><vh>TypeChecker.set_inference_error_fallback_type</vh></v>
<v t="ekr.20230831011819.286"><vh>TypeChecker.inference_error_fallback_type</vh></v>
<v t="ekr.20230831011819.287"><vh>TypeChecker.simple_rvalue</vh></v>
<v t="ekr.20230831011819.288"><vh>TypeChecker.check_simple_assignment</vh></v>
<v t="ekr.20230831011819.289"><vh>TypeChecker.check_member_assignment</vh></v>
<v t="ekr.20230831011819.290"><vh>TypeChecker.check_indexed_assignment</vh></v>
<v t="ekr.20230831011819.291"><vh>TypeChecker.try_infer_partial_type_from_indexed_assignment</vh></v>
<v t="ekr.20230831011819.292"><vh>TypeChecker.type_requires_usage</vh></v>
<v t="ekr.20230831011819.293"><vh>TypeChecker.visit_expression_stmt</vh></v>
<v t="ekr.20230831011819.294"><vh>TypeChecker.visit_return_stmt</vh></v>
<v t="ekr.20230831011819.295"><vh>TypeChecker.check_return_stmt</vh></v>
<v t="ekr.20230831011819.296"><vh>TypeChecker.visit_if_stmt</vh></v>
<v t="ekr.20230831011819.297"><vh>TypeChecker.visit_while_stmt</vh></v>
<v t="ekr.20230831011819.298"><vh>TypeChecker.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20230831011819.299"><vh>TypeChecker.visit_assert_stmt</vh></v>
<v t="ekr.20230831011819.300"><vh>TypeChecker.visit_raise_stmt</vh></v>
<v t="ekr.20230831011819.301"><vh>TypeChecker.type_check_raise</vh></v>
<v t="ekr.20230831011819.302"><vh>TypeChecker.visit_try_stmt</vh></v>
<v t="ekr.20230831011819.303"><vh>TypeChecker.visit_try_without_finally</vh></v>
<v t="ekr.20230831011819.304"><vh>TypeChecker.check_except_handler_test</vh></v>
<v t="ekr.20230831011819.305"><vh>TypeChecker.default_exception_type</vh></v>
<v t="ekr.20230831011819.306"><vh>TypeChecker.wrap_exception_group</vh></v>
<v t="ekr.20230831011819.307"><vh>TypeChecker.get_types_from_except_handler</vh></v>
<v t="ekr.20230831011819.308"><vh>TypeChecker.visit_for_stmt</vh></v>
<v t="ekr.20230831011819.309"><vh>TypeChecker.analyze_async_iterable_item_type</vh></v>
<v t="ekr.20230831011819.310"><vh>TypeChecker.analyze_iterable_item_type</vh></v>
<v t="ekr.20230831011819.311"><vh>TypeChecker.analyze_iterable_item_type_without_expression</vh></v>
<v t="ekr.20230831011819.312"><vh>TypeChecker.analyze_range_native_int_type</vh></v>
<v t="ekr.20230831011819.313"><vh>TypeChecker.analyze_container_item_type</vh></v>
<v t="ekr.20230831011819.314"><vh>TypeChecker.analyze_index_variables</vh></v>
<v t="ekr.20230831011819.315"><vh>TypeChecker.visit_del_stmt</vh></v>
<v t="ekr.20230831011819.316"><vh>TypeChecker.visit_decorator</vh></v>
<v t="ekr.20230831011819.317"><vh>TypeChecker.visit_decorator_inner</vh></v>
<v t="ekr.20230831011819.318"><vh>TypeChecker.check_for_untyped_decorator</vh></v>
<v t="ekr.20230831011819.319"><vh>TypeChecker.check_incompatible_property_override</vh></v>
<v t="ekr.20230831011819.320"><vh>TypeChecker.visit_with_stmt</vh></v>
<v t="ekr.20230831011819.321"><vh>TypeChecker.check_untyped_after_decorator</vh></v>
<v t="ekr.20230831011819.322"><vh>TypeChecker.check_async_with_item</vh></v>
<v t="ekr.20230831011819.323"><vh>TypeChecker.check_with_item</vh></v>
<v t="ekr.20230831011819.324"><vh>TypeChecker.visit_break_stmt</vh></v>
<v t="ekr.20230831011819.325"><vh>TypeChecker.visit_continue_stmt</vh></v>
<v t="ekr.20230831011819.326"><vh>TypeChecker.visit_match_stmt</vh></v>
<v t="ekr.20230831011819.327"><vh>TypeChecker.infer_variable_types_from_type_maps</vh></v>
<v t="ekr.20230831011819.328"><vh>TypeChecker.remove_capture_conflicts</vh></v>
<v t="ekr.20230831011819.329"><vh>TypeChecker.make_fake_typeinfo</vh></v>
<v t="ekr.20230831011819.330"><vh>TypeChecker.intersect_instances</vh></v>
<v t="ekr.20230831011819.331"><vh>TypeChecker.intersect_instance_callable</vh></v>
<v t="ekr.20230831011819.332"><vh>TypeChecker.make_fake_callable</vh></v>
<v t="ekr.20230831011819.333"><vh>TypeChecker.partition_by_callable</vh></v>
<v t="ekr.20230831011819.334"><vh>TypeChecker.conditional_callable_type_map</vh></v>
<v t="ekr.20230831011819.335"><vh>TypeChecker.conditional_types_for_iterable</vh></v>
<v t="ekr.20230831011819.336"><vh>TypeChecker._is_truthy_type</vh></v>
<v t="ekr.20230831011819.337"><vh>TypeChecker._check_for_truthy_type</vh></v>
<v t="ekr.20230831011819.338"><vh>TypeChecker.find_type_equals_check</vh></v>
<v t="ekr.20230831011819.339"><vh>TypeChecker.find_isinstance_check</vh></v>
<v t="ekr.20230831011819.340"><vh>TypeChecker.find_isinstance_check_helper</vh></v>
<v t="ekr.20230831011819.341"><vh>TypeChecker.propagate_up_typemap_info</vh></v>
<v t="ekr.20230831011819.342"><vh>TypeChecker.refine_parent_types</vh></v>
<v t="ekr.20230831011819.343"><vh>TypeChecker.refine_identity_comparison_expression</vh></v>
<v t="ekr.20230831011819.344"><vh>TypeChecker.refine_away_none_in_comparison</vh></v>
<v t="ekr.20230831011819.345"><vh>TypeChecker.check_subtype</vh></v>
<v t="ekr.20230831011819.346"><vh>TypeChecker.check_subtype</vh></v>
<v t="ekr.20230831011819.347"><vh>TypeChecker.check_subtype</vh></v>
<v t="ekr.20230831011819.348"><vh>TypeChecker.get_precise_awaitable_type</vh></v>
<v t="ekr.20230831011819.349"><vh>TypeChecker.checking_await_set</vh></v>
<v t="ekr.20230831011819.350"><vh>TypeChecker.check_possible_missing_await</vh></v>
<v t="ekr.20230831011819.351"><vh>TypeChecker.contains_none</vh></v>
<v t="ekr.20230831011819.352"><vh>TypeChecker.named_type</vh></v>
<v t="ekr.20230831011819.353"><vh>TypeChecker.named_generic_type</vh></v>
<v t="ekr.20230831011819.354"><vh>TypeChecker.lookup_typeinfo</vh></v>
<v t="ekr.20230831011819.355"><vh>TypeChecker.type_type</vh></v>
<v t="ekr.20230831011819.356"><vh>TypeChecker.str_type</vh></v>
<v t="ekr.20230831011819.357"><vh>TypeChecker.store_type</vh></v>
<v t="ekr.20230831011819.358"><vh>TypeChecker.has_type</vh></v>
<v t="ekr.20230831011819.359"><vh>TypeChecker.lookup_type_or_none</vh></v>
<v t="ekr.20230831011819.360"><vh>TypeChecker.lookup_type</vh></v>
<v t="ekr.20230831011819.361"><vh>TypeChecker.store_types</vh></v>
<v t="ekr.20230831011819.362"><vh>TypeChecker.local_type_map</vh></v>
<v t="ekr.20230831011819.363"><vh>TypeChecker.in_checked_function</vh></v>
<v t="ekr.20230831011819.364"><vh>TypeChecker.lookup</vh></v>
<v t="ekr.20230831011819.365"><vh>TypeChecker.lookup_qualified</vh></v>
<v t="ekr.20230831011819.366"><vh>TypeChecker.enter_partial_types</vh></v>
<v t="ekr.20230831011819.367"><vh>TypeChecker.handle_partial_var_type</vh></v>
<v t="ekr.20230831011819.368"><vh>TypeChecker.is_defined_in_base_class</vh></v>
<v t="ekr.20230831011819.369"><vh>TypeChecker.find_partial_types</vh></v>
<v t="ekr.20230831011819.370"><vh>TypeChecker.find_partial_types_in_all_scopes</vh></v>
<v t="ekr.20230831011819.371"><vh>TypeChecker.temp_node</vh></v>
<v t="ekr.20230831011819.372"><vh>TypeChecker.fail</vh></v>
<v t="ekr.20230831011819.373"><vh>TypeChecker.note</vh></v>
<v t="ekr.20230831011819.374"><vh>TypeChecker.iterable_item_type</vh></v>
<v t="ekr.20230831011819.375"><vh>TypeChecker.function_type</vh></v>
<v t="ekr.20230831011819.376"><vh>TypeChecker.push_type_map</vh></v>
<v t="ekr.20230831011819.377"><vh>TypeChecker.infer_issubclass_maps</vh></v>
<v t="ekr.20230831011819.378"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20230831011819.379"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20230831011819.380"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20230831011819.381"><vh>TypeChecker.is_writable_attribute</vh></v>
<v t="ekr.20230831011819.382"><vh>TypeChecker.get_isinstance_type</vh></v>
<v t="ekr.20230831011819.383"><vh>TypeChecker.is_literal_enum</vh></v>
<v t="ekr.20230831011819.384"><vh>TypeChecker.add_any_attribute_to_type</vh></v>
<v t="ekr.20230831011819.385"><vh>TypeChecker.hasattr_type_maps</vh></v>
<v t="ekr.20230831011819.386"><vh>TypeChecker.partition_union_by_attr</vh></v>
<v t="ekr.20230831011819.387"><vh>TypeChecker.has_valid_attribute</vh></v>
<v t="ekr.20230831011819.388"><vh>TypeChecker.get_expression_type</vh></v>
</v>
<v t="ekr.20230831011819.389"><vh>class CollectArgTypeVarTypes</vh>
<v t="ekr.20230831011819.390"><vh>CollectArgTypeVarTypes.__init__</vh></v>
<v t="ekr.20230831011819.391"><vh>CollectArgTypeVarTypes.visit_type_var</vh></v>
</v>
<v t="ekr.20230831011819.392"><vh>function: conditional_types</vh></v>
<v t="ekr.20230831011819.393"><vh>function: conditional_types</vh></v>
<v t="ekr.20230831011819.394"><vh>function: conditional_types</vh></v>
<v t="ekr.20230831011819.395"><vh>function: conditional_types_to_typemaps</vh></v>
<v t="ekr.20230831011819.396"><vh>function: gen_unique_name</vh></v>
<v t="ekr.20230831011819.397"><vh>function: is_true_literal</vh></v>
<v t="ekr.20230831011819.398"><vh>function: is_false_literal</vh></v>
<v t="ekr.20230831011819.399"><vh>function: is_literal_none</vh></v>
<v t="ekr.20230831011819.400"><vh>function: is_literal_not_implemented</vh></v>
<v t="ekr.20230831011819.401"><vh>function: _is_empty_generator_function</vh></v>
<v t="ekr.20230831011819.402"><vh>function: builtin_item_type</vh></v>
<v t="ekr.20230831011819.403"><vh>function: and_conditional_maps</vh></v>
<v t="ekr.20230831011819.404"><vh>function: or_conditional_maps</vh></v>
<v t="ekr.20230831011819.405"><vh>function: reduce_conditional_maps</vh></v>
<v t="ekr.20230831011819.406"><vh>function: convert_to_typetype</vh></v>
<v t="ekr.20230831011819.407"><vh>function: flatten</vh></v>
<v t="ekr.20230831011819.408"><vh>function: flatten_types</vh></v>
<v t="ekr.20230831011819.409"><vh>function: expand_func</vh></v>
<v t="ekr.20230831011819.410"><vh>class TypeTransformVisitor</vh>
<v t="ekr.20230831011819.411"><vh>TypeTransformVisitor.__init__</vh></v>
<v t="ekr.20230831011819.412"><vh>TypeTransformVisitor.type</vh></v>
</v>
<v t="ekr.20230831011819.413"><vh>function: are_argument_counts_overlapping</vh></v>
<v t="ekr.20230831011819.414"><vh>function: is_unsafe_overlapping_overload_signatures</vh></v>
<v t="ekr.20230831011819.415"><vh>function: detach_callable</vh></v>
<v t="ekr.20230831011819.416"><vh>function: overload_can_never_match</vh></v>
<v t="ekr.20230831011819.417"><vh>function: is_more_general_arg_prefix</vh></v>
<v t="ekr.20230831011819.418"><vh>function: is_same_arg_prefix</vh></v>
<v t="ekr.20230831011819.419"><vh>function: infer_operator_assignment_method</vh></v>
<v t="ekr.20230831011819.420"><vh>function: is_valid_inferred_type</vh></v>
<v t="ekr.20230831011819.421"><vh>class InvalidInferredTypes</vh>
<v t="ekr.20230831011819.422"><vh>InvalidInferredTypes.__init__</vh></v>
<v t="ekr.20230831011819.423"><vh>InvalidInferredTypes.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.424"><vh>InvalidInferredTypes.visit_erased_type</vh></v>
<v t="ekr.20230831011819.425"><vh>InvalidInferredTypes.visit_type_var</vh></v>
</v>
<v t="ekr.20230831011819.426"><vh>class SetNothingToAny</vh>
<v t="ekr.20230831011819.427"><vh>SetNothingToAny.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.428"><vh>SetNothingToAny.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011819.429"><vh>function: is_node_static</vh></v>
<v t="ekr.20230831011819.430"><vh>class CheckerScope</vh>
<v t="ekr.20230831011819.431"><vh>CheckerScope.__init__</vh></v>
<v t="ekr.20230831011819.432"><vh>CheckerScope.top_function</vh></v>
<v t="ekr.20230831011819.433"><vh>CheckerScope.top_non_lambda_function</vh></v>
<v t="ekr.20230831011819.434"><vh>CheckerScope.active_class</vh></v>
<v t="ekr.20230831011819.435"><vh>CheckerScope.enclosing_class</vh></v>
<v t="ekr.20230831011819.436"><vh>CheckerScope.active_self_type</vh></v>
<v t="ekr.20230831011819.437"><vh>CheckerScope.push_function</vh></v>
<v t="ekr.20230831011819.438"><vh>CheckerScope.push_class</vh></v>
</v>
<v t="ekr.20230831011819.439"><vh>class DisjointDict</vh>
<v t="ekr.20230831011819.440"><vh>DisjointDict.__init__</vh></v>
<v t="ekr.20230831011819.441"><vh>DisjointDict.add_mapping</vh></v>
<v t="ekr.20230831011819.442"><vh>DisjointDict.items</vh></v>
<v t="ekr.20230831011819.443"><vh>DisjointDict._lookup_or_make_root_id</vh></v>
<v t="ekr.20230831011819.444"><vh>DisjointDict._lookup_root_id</vh></v>
</v>
<v t="ekr.20230831011819.445"><vh>function: group_comparison_operands</vh></v>
<v t="ekr.20230831011819.446"><vh>function: is_typed_callable</vh></v>
<v t="ekr.20230831011819.447"><vh>function: is_untyped_decorator</vh></v>
<v t="ekr.20230831011819.448"><vh>function: is_static</vh></v>
<v t="ekr.20230831011819.449"><vh>function: is_property</vh></v>
<v t="ekr.20230831011819.450"><vh>function: get_property_type</vh></v>
<v t="ekr.20230831011819.451"><vh>function: is_subtype_no_promote</vh></v>
<v t="ekr.20230831011819.452"><vh>function: is_overlapping_types_no_promote_no_uninhabited_no_none</vh></v>
<v t="ekr.20230831011819.453"><vh>function: is_private</vh></v>
<v t="ekr.20230831011819.454"><vh>function: is_string_literal</vh></v>
<v t="ekr.20230831011819.455"><vh>function: has_bool_item</vh></v>
<v t="ekr.20230831011819.456"><vh>function: collapse_walrus</vh></v>
<v t="ekr.20230831011819.457"><vh>function: find_last_var_assignment_line</vh></v>
<v t="ekr.20230831011819.458"><vh>class VarAssignVisitor</vh>
<v t="ekr.20230831011819.459"><vh>VarAssignVisitor.__init__</vh></v>
<v t="ekr.20230831011819.460"><vh>VarAssignVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011819.461"><vh>VarAssignVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011819.462"><vh>VarAssignVisitor.visit_member_expr</vh></v>
<v t="ekr.20230831011819.463"><vh>VarAssignVisitor.visit_index_expr</vh></v>
<v t="ekr.20230831011819.464"><vh>VarAssignVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011819.465"><vh>VarAssignVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011819.466"><vh>VarAssignVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011819.467"><vh>VarAssignVisitor.visit_as_pattern</vh></v>
<v t="ekr.20230831011819.468"><vh>VarAssignVisitor.visit_starred_pattern</vh></v>
</v>
</v>
<v t="ekr.20230831011819.469"><vh>@clean checkexpr.py</vh>
<v t="ekr.20230831011819.470"><vh>&lt;&lt; checkexpr.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.471"><vh>class TooManyUnions</vh></v>
<v t="ekr.20230831011819.472"><vh>function: allow_fast_container_literal</vh></v>
<v t="ekr.20230831011819.473"><vh>function: extract_refexpr_names</vh></v>
<v t="ekr.20230831011819.474"><vh>class Finished</vh></v>
<v t="ekr.20230831011819.475"><vh>class ExpressionChecker</vh>
<v t="ekr.20230831011819.476"><vh>ExpressionChecker.__init__</vh></v>
<v t="ekr.20230831011819.477"><vh>ExpressionChecker.reset</vh></v>
<v t="ekr.20230831011819.478"><vh>ExpressionChecker.visit_name_expr</vh></v>
<v t="ekr.20230831011819.479"><vh>ExpressionChecker.analyze_ref_expr</vh></v>
<v t="ekr.20230831011819.480"><vh>ExpressionChecker.analyze_var_ref</vh></v>
<v t="ekr.20230831011819.481"><vh>ExpressionChecker.module_type</vh></v>
<v t="ekr.20230831011819.482"><vh>ExpressionChecker.visit_call_expr</vh></v>
<v t="ekr.20230831011819.483"><vh>ExpressionChecker.refers_to_typeddict</vh></v>
<v t="ekr.20230831011819.484"><vh>ExpressionChecker.visit_call_expr_inner</vh></v>
<v t="ekr.20230831011819.485"><vh>ExpressionChecker.check_str_format_call</vh></v>
<v t="ekr.20230831011819.486"><vh>ExpressionChecker.method_fullname</vh></v>
<v t="ekr.20230831011819.487"><vh>ExpressionChecker.always_returns_none</vh></v>
<v t="ekr.20230831011819.488"><vh>ExpressionChecker.defn_returns_none</vh></v>
<v t="ekr.20230831011819.489"><vh>ExpressionChecker.check_runtime_protocol_test</vh></v>
<v t="ekr.20230831011819.490"><vh>ExpressionChecker.check_protocol_issubclass</vh></v>
<v t="ekr.20230831011819.491"><vh>ExpressionChecker.check_typeddict_call</vh></v>
<v t="ekr.20230831011819.492"><vh>ExpressionChecker.validate_typeddict_kwargs</vh></v>
<v t="ekr.20230831011819.493"><vh>ExpressionChecker.validate_star_typeddict_item</vh></v>
<v t="ekr.20230831011819.494"><vh>ExpressionChecker.valid_unpack_fallback_item</vh></v>
<v t="ekr.20230831011819.495"><vh>ExpressionChecker.match_typeddict_call_with_dict</vh></v>
<v t="ekr.20230831011819.496"><vh>ExpressionChecker.check_typeddict_call_with_dict</vh></v>
<v t="ekr.20230831011819.497"><vh>ExpressionChecker.typeddict_callable</vh></v>
<v t="ekr.20230831011819.498"><vh>ExpressionChecker.typeddict_callable_from_context</vh></v>
<v t="ekr.20230831011819.499"><vh>ExpressionChecker.check_typeddict_call_with_kwargs</vh></v>
<v t="ekr.20230831011819.500"><vh>ExpressionChecker.get_partial_self_var</vh></v>
<v t="ekr.20230831011819.501"><vh>ExpressionChecker.try_infer_partial_type</vh></v>
<v t="ekr.20230831011819.502"><vh>ExpressionChecker.get_partial_var</vh></v>
<v t="ekr.20230831011819.503"><vh>ExpressionChecker.try_infer_partial_value_type_from_call</vh></v>
<v t="ekr.20230831011819.504"><vh>ExpressionChecker.apply_function_plugin</vh></v>
<v t="ekr.20230831011819.505"><vh>ExpressionChecker.apply_signature_hook</vh></v>
<v t="ekr.20230831011819.506"><vh>ExpressionChecker.apply_function_signature_hook</vh></v>
<v t="ekr.20230831011819.507"><vh>ExpressionChecker.apply_method_signature_hook</vh></v>
<v t="ekr.20230831011819.508"><vh>ExpressionChecker.transform_callee_type</vh></v>
<v t="ekr.20230831011819.509"><vh>ExpressionChecker.is_generic_decorator_overload_call</vh></v>
<v t="ekr.20230831011819.510"><vh>ExpressionChecker.handle_decorator_overload_call</vh></v>
<v t="ekr.20230831011819.511"><vh>ExpressionChecker.check_call_expr_with_callee_type</vh></v>
<v t="ekr.20230831011819.512"><vh>ExpressionChecker.check_union_call_expr</vh></v>
<v t="ekr.20230831011819.513"><vh>ExpressionChecker.check_call</vh></v>
<v t="ekr.20230831011819.514"><vh>ExpressionChecker.check_callable_call</vh></v>
<v t="ekr.20230831011819.515"><vh>ExpressionChecker.can_return_none</vh></v>
<v t="ekr.20230831011819.516"><vh>ExpressionChecker.analyze_type_type_callee</vh></v>
<v t="ekr.20230831011819.517"><vh>ExpressionChecker.infer_arg_types_in_empty_context</vh></v>
<v t="ekr.20230831011819.518"><vh>ExpressionChecker.infer_more_unions_for_recursive_type</vh></v>
<v t="ekr.20230831011819.519"><vh>ExpressionChecker.infer_arg_types_in_context</vh></v>
<v t="ekr.20230831011819.520"><vh>ExpressionChecker.infer_function_type_arguments_using_context</vh></v>
<v t="ekr.20230831011819.521"><vh>ExpressionChecker.infer_function_type_arguments</vh></v>
<v t="ekr.20230831011819.522"><vh>ExpressionChecker.infer_function_type_arguments_pass2</vh></v>
<v t="ekr.20230831011819.523"><vh>ExpressionChecker.argument_infer_context</vh></v>
<v t="ekr.20230831011819.524"><vh>ExpressionChecker.get_arg_infer_passes</vh></v>
<v t="ekr.20230831011819.525"><vh>ExpressionChecker.apply_inferred_arguments</vh></v>
<v t="ekr.20230831011819.526"><vh>ExpressionChecker.check_argument_count</vh></v>
<v t="ekr.20230831011819.527"><vh>ExpressionChecker.check_for_extra_actual_arguments</vh></v>
<v t="ekr.20230831011819.528"><vh>ExpressionChecker.missing_classvar_callable_note</vh></v>
<v t="ekr.20230831011819.529"><vh>ExpressionChecker.check_argument_types</vh></v>
<v t="ekr.20230831011819.530"><vh>ExpressionChecker.check_arg</vh></v>
<v t="ekr.20230831011819.531"><vh>ExpressionChecker.check_overload_call</vh></v>
<v t="ekr.20230831011819.532"><vh>ExpressionChecker.plausible_overload_call_targets</vh></v>
<v t="ekr.20230831011819.533"><vh>ExpressionChecker.infer_overload_return_type</vh></v>
<v t="ekr.20230831011819.534"><vh>ExpressionChecker.overload_erased_call_targets</vh></v>
<v t="ekr.20230831011819.535"><vh>ExpressionChecker.possible_none_type_var_overlap</vh></v>
<v t="ekr.20230831011819.536"><vh>ExpressionChecker.union_overload_result</vh></v>
<v t="ekr.20230831011819.537"><vh>ExpressionChecker.real_union</vh></v>
<v t="ekr.20230831011819.538"><vh>ExpressionChecker.type_overrides_set</vh></v>
<v t="ekr.20230831011819.539"><vh>ExpressionChecker.combine_function_signatures</vh></v>
<v t="ekr.20230831011819.540"><vh>ExpressionChecker.erased_signature_similarity</vh></v>
<v t="ekr.20230831011819.541"><vh>ExpressionChecker.apply_generic_arguments</vh></v>
<v t="ekr.20230831011819.542"><vh>ExpressionChecker.check_any_type_call</vh></v>
<v t="ekr.20230831011819.543"><vh>ExpressionChecker.check_union_call</vh></v>
<v t="ekr.20230831011819.544"><vh>ExpressionChecker.visit_member_expr</vh></v>
<v t="ekr.20230831011819.545"><vh>ExpressionChecker.analyze_ordinary_member_access</vh></v>
<v t="ekr.20230831011819.546"><vh>ExpressionChecker.analyze_external_member_access</vh></v>
<v t="ekr.20230831011819.547"><vh>ExpressionChecker.is_literal_context</vh></v>
<v t="ekr.20230831011819.548"><vh>ExpressionChecker.infer_literal_expr_type</vh></v>
<v t="ekr.20230831011819.549"><vh>ExpressionChecker.concat_tuples</vh></v>
<v t="ekr.20230831011819.550"><vh>ExpressionChecker.visit_int_expr</vh></v>
<v t="ekr.20230831011819.551"><vh>ExpressionChecker.visit_str_expr</vh></v>
<v t="ekr.20230831011819.552"><vh>ExpressionChecker.visit_bytes_expr</vh></v>
<v t="ekr.20230831011819.553"><vh>ExpressionChecker.visit_float_expr</vh></v>
<v t="ekr.20230831011819.554"><vh>ExpressionChecker.visit_complex_expr</vh></v>
<v t="ekr.20230831011819.555"><vh>ExpressionChecker.visit_ellipsis</vh></v>
<v t="ekr.20230831011819.556"><vh>ExpressionChecker.visit_op_expr</vh></v>
<v t="ekr.20230831011819.557"><vh>ExpressionChecker.visit_comparison_expr</vh></v>
<v t="ekr.20230831011819.558"><vh>ExpressionChecker.find_partial_type_ref_fast_path</vh></v>
<v t="ekr.20230831011819.559"><vh>ExpressionChecker.dangerous_comparison</vh></v>
<v t="ekr.20230831011819.560"><vh>ExpressionChecker.check_method_call_by_name</vh></v>
<v t="ekr.20230831011819.561"><vh>ExpressionChecker.check_union_method_call_by_name</vh></v>
<v t="ekr.20230831011819.562"><vh>ExpressionChecker.check_method_call</vh></v>
<v t="ekr.20230831011819.563"><vh>ExpressionChecker.check_op_reversible</vh></v>
<v t="ekr.20230831011819.564"><vh>ExpressionChecker.check_op</vh></v>
<v t="ekr.20230831011819.565"><vh>ExpressionChecker.check_boolean_op</vh></v>
<v t="ekr.20230831011819.566"><vh>ExpressionChecker.check_list_multiply</vh></v>
<v t="ekr.20230831011819.567"><vh>ExpressionChecker.visit_assignment_expr</vh></v>
<v t="ekr.20230831011819.568"><vh>ExpressionChecker.visit_unary_expr</vh></v>
<v t="ekr.20230831011819.569"><vh>ExpressionChecker.visit_index_expr</vh></v>
<v t="ekr.20230831011819.570"><vh>ExpressionChecker.visit_index_expr_helper</vh></v>
<v t="ekr.20230831011819.571"><vh>ExpressionChecker.visit_index_with_type</vh></v>
<v t="ekr.20230831011819.572"><vh>ExpressionChecker.visit_tuple_slice_helper</vh></v>
<v t="ekr.20230831011819.573"><vh>ExpressionChecker.try_getting_int_literals</vh></v>
<v t="ekr.20230831011819.574"><vh>ExpressionChecker.nonliteral_tuple_index_helper</vh></v>
<v t="ekr.20230831011819.575"><vh>ExpressionChecker.visit_typeddict_index_expr</vh></v>
<v t="ekr.20230831011819.576"><vh>ExpressionChecker.visit_enum_index_expr</vh></v>
<v t="ekr.20230831011819.577"><vh>ExpressionChecker.visit_cast_expr</vh></v>
<v t="ekr.20230831011819.578"><vh>ExpressionChecker.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011819.579"><vh>ExpressionChecker.visit_reveal_expr</vh></v>
<v t="ekr.20230831011819.580"><vh>ExpressionChecker.visit_type_application</vh></v>
<v t="ekr.20230831011819.581"><vh>ExpressionChecker.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011819.582"><vh>ExpressionChecker.alias_type_in_runtime_context</vh></v>
<v t="ekr.20230831011819.583"><vh>ExpressionChecker.split_for_callable</vh></v>
<v t="ekr.20230831011819.584"><vh>ExpressionChecker.apply_type_arguments_to_callable</vh></v>
<v t="ekr.20230831011819.585"><vh>ExpressionChecker.visit_list_expr</vh></v>
<v t="ekr.20230831011819.586"><vh>ExpressionChecker.visit_set_expr</vh></v>
<v t="ekr.20230831011819.587"><vh>ExpressionChecker.fast_container_type</vh></v>
<v t="ekr.20230831011819.588"><vh>ExpressionChecker.check_lst_expr</vh></v>
<v t="ekr.20230831011819.589"><vh>ExpressionChecker.visit_tuple_expr</vh></v>
<v t="ekr.20230831011819.590"><vh>ExpressionChecker.fast_dict_type</vh></v>
<v t="ekr.20230831011819.591"><vh>ExpressionChecker.check_typeddict_literal_in_context</vh></v>
<v t="ekr.20230831011819.592"><vh>ExpressionChecker.visit_dict_expr</vh></v>
<v t="ekr.20230831011819.593"><vh>ExpressionChecker.find_typeddict_context</vh></v>
<v t="ekr.20230831011819.594"><vh>ExpressionChecker.visit_lambda_expr</vh></v>
<v t="ekr.20230831011819.595"><vh>ExpressionChecker.infer_lambda_type_using_context</vh></v>
<v t="ekr.20230831011819.596"><vh>ExpressionChecker.visit_super_expr</vh></v>
<v t="ekr.20230831011819.597"><vh>ExpressionChecker._super_arg_types</vh></v>
<v t="ekr.20230831011819.598"><vh>ExpressionChecker.visit_slice_expr</vh></v>
<v t="ekr.20230831011819.599"><vh>ExpressionChecker.visit_list_comprehension</vh></v>
<v t="ekr.20230831011819.600"><vh>ExpressionChecker.visit_set_comprehension</vh></v>
<v t="ekr.20230831011819.601"><vh>ExpressionChecker.visit_generator_expr</vh></v>
<v t="ekr.20230831011819.602"><vh>ExpressionChecker.check_generator_or_comprehension</vh></v>
<v t="ekr.20230831011819.603"><vh>ExpressionChecker.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011819.604"><vh>ExpressionChecker.check_for_comp</vh></v>
<v t="ekr.20230831011819.605"><vh>ExpressionChecker.visit_conditional_expr</vh></v>
<v t="ekr.20230831011819.606"><vh>ExpressionChecker.analyze_cond_branch</vh></v>
<v t="ekr.20230831011819.607"><vh>ExpressionChecker.accept</vh></v>
<v t="ekr.20230831011819.608"><vh>ExpressionChecker.named_type</vh></v>
<v t="ekr.20230831011819.609"><vh>ExpressionChecker.is_valid_var_arg</vh></v>
<v t="ekr.20230831011819.610"><vh>ExpressionChecker.is_valid_keyword_var_arg</vh></v>
<v t="ekr.20230831011819.611"><vh>ExpressionChecker.has_member</vh></v>
<v t="ekr.20230831011819.612"><vh>ExpressionChecker.not_ready_callback</vh></v>
<v t="ekr.20230831011819.613"><vh>ExpressionChecker.visit_yield_expr</vh></v>
<v t="ekr.20230831011819.614"><vh>ExpressionChecker.visit_await_expr</vh></v>
<v t="ekr.20230831011819.615"><vh>ExpressionChecker.check_awaitable_expr</vh></v>
<v t="ekr.20230831011819.616"><vh>ExpressionChecker.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011819.617"><vh>ExpressionChecker.visit_temp_node</vh></v>
<v t="ekr.20230831011819.618"><vh>ExpressionChecker.visit_type_var_expr</vh></v>
<v t="ekr.20230831011819.619"><vh>ExpressionChecker.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011819.620"><vh>ExpressionChecker.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011819.621"><vh>ExpressionChecker.visit_newtype_expr</vh></v>
<v t="ekr.20230831011819.622"><vh>ExpressionChecker.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011819.623"><vh>ExpressionChecker.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011819.624"><vh>ExpressionChecker.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011819.625"><vh>ExpressionChecker.visit__promote_expr</vh></v>
<v t="ekr.20230831011819.626"><vh>ExpressionChecker.visit_star_expr</vh></v>
<v t="ekr.20230831011819.627"><vh>ExpressionChecker.object_type</vh></v>
<v t="ekr.20230831011819.628"><vh>ExpressionChecker.bool_type</vh></v>
<v t="ekr.20230831011819.629"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
<v t="ekr.20230831011819.630"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
<v t="ekr.20230831011819.631"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
<v t="ekr.20230831011819.632"><vh>ExpressionChecker.has_abstract_type_part</vh></v>
<v t="ekr.20230831011819.633"><vh>ExpressionChecker.has_abstract_type</vh></v>
</v>
<v t="ekr.20230831011819.634"><vh>function: has_any_type</vh></v>
<v t="ekr.20230831011819.635"><vh>class HasAnyType</vh>
<v t="ekr.20230831011819.636"><vh>HasAnyType.__init__</vh></v>
<v t="ekr.20230831011819.637"><vh>HasAnyType.visit_any</vh></v>
<v t="ekr.20230831011819.638"><vh>HasAnyType.visit_callable_type</vh></v>
<v t="ekr.20230831011819.639"><vh>HasAnyType.visit_type_var</vh></v>
<v t="ekr.20230831011819.640"><vh>HasAnyType.visit_param_spec</vh></v>
<v t="ekr.20230831011819.641"><vh>HasAnyType.visit_type_var_tuple</vh></v>
</v>
<v t="ekr.20230831011819.642"><vh>function: has_coroutine_decorator</vh></v>
<v t="ekr.20230831011819.643"><vh>function: is_async_def</vh></v>
<v t="ekr.20230831011819.644"><vh>function: is_non_empty_tuple</vh></v>
<v t="ekr.20230831011819.645"><vh>function: is_duplicate_mapping</vh></v>
<v t="ekr.20230831011819.646"><vh>function: replace_callable_return_type</vh></v>
<v t="ekr.20230831011819.647"><vh>function: apply_poly</vh></v>
<v t="ekr.20230831011819.648"><vh>class PolyTranslationError</vh></v>
<v t="ekr.20230831011819.649"><vh>class PolyTranslator</vh>
<v t="ekr.20230831011819.650"><vh>PolyTranslator.__init__</vh></v>
<v t="ekr.20230831011819.651"><vh>PolyTranslator.collect_vars</vh></v>
<v t="ekr.20230831011819.652"><vh>PolyTranslator.visit_callable_type</vh></v>
<v t="ekr.20230831011819.653"><vh>PolyTranslator.visit_type_var</vh></v>
<v t="ekr.20230831011819.654"><vh>PolyTranslator.visit_param_spec</vh></v>
<v t="ekr.20230831011819.655"><vh>PolyTranslator.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.656"><vh>PolyTranslator.visit_type_alias_type</vh></v>
<v t="ekr.20230831011819.657"><vh>PolyTranslator.visit_instance</vh></v>
</v>
<v t="ekr.20230831011819.658"><vh>class ArgInferSecondPassQuery</vh>
<v t="ekr.20230831011819.659"><vh>ArgInferSecondPassQuery.__init__</vh></v>
<v t="ekr.20230831011819.660"><vh>ArgInferSecondPassQuery.visit_callable_type</vh></v>
</v>
<v t="ekr.20230831011819.661"><vh>class HasTypeVarQuery</vh>
<v t="ekr.20230831011819.662"><vh>HasTypeVarQuery.__init__</vh></v>
<v t="ekr.20230831011819.663"><vh>HasTypeVarQuery.visit_type_var</vh></v>
<v t="ekr.20230831011819.664"><vh>HasTypeVarQuery.visit_param_spec</vh></v>
<v t="ekr.20230831011819.665"><vh>HasTypeVarQuery.visit_type_var_tuple</vh></v>
</v>
<v t="ekr.20230831011819.666"><vh>function: has_erased_component</vh></v>
<v t="ekr.20230831011819.667"><vh>class HasErasedComponentsQuery</vh>
<v t="ekr.20230831011819.668"><vh>HasErasedComponentsQuery.__init__</vh></v>
<v t="ekr.20230831011819.669"><vh>HasErasedComponentsQuery.visit_erased_type</vh></v>
</v>
<v t="ekr.20230831011819.670"><vh>function: has_uninhabited_component</vh></v>
<v t="ekr.20230831011819.671"><vh>class HasUninhabitedComponentsQuery</vh>
<v t="ekr.20230831011819.672"><vh>HasUninhabitedComponentsQuery.__init__</vh></v>
<v t="ekr.20230831011819.673"><vh>HasUninhabitedComponentsQuery.visit_uninhabited_type</vh></v>
</v>
<v t="ekr.20230831011819.674"><vh>function: arg_approximate_similarity</vh></v>
<v t="ekr.20230831011819.675"><vh>function: any_causes_overload_ambiguity</vh></v>
<v t="ekr.20230831011819.676"><vh>function: all_same_types</vh></v>
<v t="ekr.20230831011819.677"><vh>function: merge_typevars_in_callables_by_name</vh></v>
<v t="ekr.20230831011819.678"><vh>function: try_getting_literal</vh></v>
<v t="ekr.20230831011819.679"><vh>function: is_expr_literal_type</vh></v>
<v t="ekr.20230831011819.680"><vh>function: has_bytes_component</vh></v>
<v t="ekr.20230831011819.681"><vh>function: type_info_from_type</vh></v>
<v t="ekr.20230831011819.682"><vh>function: is_operator_method</vh></v>
<v t="ekr.20230831011819.683"><vh>function: get_partial_instance_type</vh></v>
</v>
<v t="ekr.20230831011819.684"><vh>@clean checkmember.py</vh>
<v t="ekr.20230831011819.685"><vh>&lt;&lt; checkmember.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.686"><vh>class MemberContext</vh>
<v t="ekr.20230831011819.687"><vh>MemberContext.__init__</vh></v>
<v t="ekr.20230831011819.688"><vh>MemberContext.named_type</vh></v>
<v t="ekr.20230831011819.689"><vh>MemberContext.not_ready_callback</vh></v>
<v t="ekr.20230831011819.690"><vh>MemberContext.copy_modified</vh></v>
</v>
<v t="ekr.20230831011819.691"><vh>function: analyze_member_access</vh></v>
<v t="ekr.20230831011819.692"><vh>function: _analyze_member_access</vh></v>
<v t="ekr.20230831011819.693"><vh>function: may_be_awaitable_attribute</vh></v>
<v t="ekr.20230831011819.694"><vh>function: report_missing_attribute</vh></v>
<v t="ekr.20230831011819.695"><vh>function: analyze_instance_member_access</vh></v>
<v t="ekr.20230831011819.696"><vh>function: validate_super_call</vh></v>
<v t="ekr.20230831011819.697"><vh>function: analyze_type_callable_member_access</vh></v>
<v t="ekr.20230831011819.698"><vh>function: analyze_type_type_member_access</vh></v>
<v t="ekr.20230831011819.699"><vh>function: analyze_union_member_access</vh></v>
<v t="ekr.20230831011819.700"><vh>function: analyze_none_member_access</vh></v>
<v t="ekr.20230831011819.701"><vh>function: analyze_member_var_access</vh></v>
<v t="ekr.20230831011819.702"><vh>function: check_final_member</vh></v>
<v t="ekr.20230831011819.703"><vh>function: analyze_descriptor_access</vh></v>
<v t="ekr.20230831011819.704"><vh>function: is_instance_var</vh></v>
<v t="ekr.20230831011819.705"><vh>function: analyze_var</vh></v>
<v t="ekr.20230831011819.706"><vh>function: freeze_all_type_vars</vh></v>
<v t="ekr.20230831011819.707"><vh>class FreezeTypeVarsVisitor</vh>
<v t="ekr.20230831011819.708"><vh>FreezeTypeVarsVisitor.visit_callable_type</vh></v>
</v>
<v t="ekr.20230831011819.709"><vh>function: lookup_member_var_or_accessor</vh></v>
<v t="ekr.20230831011819.710"><vh>function: check_self_arg</vh></v>
<v t="ekr.20230831011819.711"><vh>function: analyze_class_attribute_access</vh></v>
<v t="ekr.20230831011819.712"><vh>function: apply_class_attr_hook</vh></v>
<v t="ekr.20230831011819.713"><vh>function: analyze_enum_class_attribute_access</vh></v>
<v t="ekr.20230831011819.714"><vh>function: analyze_typeddict_access</vh></v>
<v t="ekr.20230831011819.715"><vh>function: add_class_tvars</vh></v>
<v t="ekr.20230831011819.716"><vh>function: type_object_type</vh></v>
<v t="ekr.20230831011819.717"><vh>function: analyze_decorator_or_funcbase_access</vh></v>
<v t="ekr.20230831011819.718"><vh>function: is_valid_constructor</vh></v>
</v>
<v t="ekr.20230831011819.719"><vh>@clean checkpattern.py</vh>
<v t="ekr.20230831011819.720"><vh>&lt;&lt; checkpattern.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.721"><vh>class PatternType</vh></v>
<v t="ekr.20230831011819.722"><vh>class PatternChecker</vh>
<v t="ekr.20230831011819.723"><vh>PatternChecker.__init__</vh></v>
<v t="ekr.20230831011819.724"><vh>PatternChecker.accept</vh></v>
<v t="ekr.20230831011819.725"><vh>PatternChecker.visit_as_pattern</vh></v>
<v t="ekr.20230831011819.726"><vh>PatternChecker.visit_or_pattern</vh></v>
<v t="ekr.20230831011819.727"><vh>PatternChecker.visit_value_pattern</vh></v>
<v t="ekr.20230831011819.728"><vh>PatternChecker.visit_singleton_pattern</vh></v>
<v t="ekr.20230831011819.729"><vh>PatternChecker.visit_sequence_pattern</vh></v>
<v t="ekr.20230831011819.730"><vh>PatternChecker.get_sequence_type</vh></v>
<v t="ekr.20230831011819.731"><vh>PatternChecker.contract_starred_pattern_types</vh></v>
<v t="ekr.20230831011819.732"><vh>PatternChecker.expand_starred_pattern_types</vh></v>
<v t="ekr.20230831011819.733"><vh>PatternChecker.visit_starred_pattern</vh></v>
<v t="ekr.20230831011819.734"><vh>PatternChecker.visit_mapping_pattern</vh></v>
<v t="ekr.20230831011819.735"><vh>PatternChecker.get_mapping_item_type</vh></v>
<v t="ekr.20230831011819.736"><vh>PatternChecker.get_simple_mapping_item_type</vh></v>
<v t="ekr.20230831011819.737"><vh>PatternChecker.visit_class_pattern</vh></v>
<v t="ekr.20230831011819.738"><vh>PatternChecker.should_self_match</vh></v>
<v t="ekr.20230831011819.739"><vh>PatternChecker.can_match_sequence</vh></v>
<v t="ekr.20230831011819.740"><vh>PatternChecker.generate_types_from_names</vh></v>
<v t="ekr.20230831011819.741"><vh>PatternChecker.update_type_map</vh></v>
<v t="ekr.20230831011819.742"><vh>PatternChecker.construct_sequence_child</vh></v>
<v t="ekr.20230831011819.743"><vh>PatternChecker.early_non_match</vh></v>
</v>
<v t="ekr.20230831011819.744"><vh>function: get_match_arg_names</vh></v>
<v t="ekr.20230831011819.745"><vh>function: get_var</vh></v>
<v t="ekr.20230831011819.746"><vh>function: get_type_range</vh></v>
<v t="ekr.20230831011819.747"><vh>function: is_uninhabited</vh></v>
</v>
<v t="ekr.20230831011819.748"><vh>@clean checkstrformat.py</vh>
<v t="ekr.20230902063413.1"><vh>&lt;&lt; checkstrformat.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20230831011819.750"><vh>&lt;&lt; checkstrformat.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.751"><vh>function: compile_format_re</vh></v>
<v t="ekr.20230831011819.752"><vh>function: compile_new_format_re</vh></v>
<v t="ekr.20230831011819.753"><vh>class ConversionSpecifier</vh>
<v t="ekr.20230831011819.754"><vh>ConversionSpecifier.__init__</vh></v>
<v t="ekr.20230831011819.755"><vh>ConversionSpecifier.has_key</vh></v>
<v t="ekr.20230831011819.756"><vh>ConversionSpecifier.has_star</vh></v>
</v>
<v t="ekr.20230831011819.757"><vh>function: parse_conversion_specifiers</vh></v>
<v t="ekr.20230831011819.758"><vh>function: parse_format_value</vh></v>
<v t="ekr.20230831011819.759"><vh>function: find_non_escaped_targets</vh></v>
<v t="ekr.20230831011819.760"><vh>class StringFormatterChecker</vh>
<v t="ekr.20230831011819.761"><vh>StringFormatterChecker.__init__</vh></v>
<v t="ekr.20230831011819.762"><vh>StringFormatterChecker.check_str_format_call</vh></v>
<v t="ekr.20230831011819.763"><vh>StringFormatterChecker.check_specs_in_format_call</vh></v>
<v t="ekr.20230831011819.764"><vh>StringFormatterChecker.perform_special_format_checks</vh></v>
<v t="ekr.20230831011819.765"><vh>StringFormatterChecker.find_replacements_in_call</vh></v>
<v t="ekr.20230831011819.766"><vh>StringFormatterChecker.get_expr_by_position</vh></v>
<v t="ekr.20230831011819.767"><vh>StringFormatterChecker.get_expr_by_name</vh></v>
<v t="ekr.20230831011819.768"><vh>StringFormatterChecker.auto_generate_keys</vh></v>
<v t="ekr.20230831011819.769"><vh>StringFormatterChecker.apply_field_accessors</vh></v>
<v t="ekr.20230831011819.770"><vh>StringFormatterChecker.validate_and_transform_accessors</vh></v>
<v t="ekr.20230831011819.771"><vh>StringFormatterChecker.check_str_interpolation</vh></v>
<v t="ekr.20230831011819.772"><vh>StringFormatterChecker.analyze_conversion_specifiers</vh></v>
<v t="ekr.20230831011819.773"><vh>StringFormatterChecker.check_simple_str_interpolation</vh></v>
<v t="ekr.20230831011819.774"><vh>StringFormatterChecker.check_mapping_str_interpolation</vh></v>
<v t="ekr.20230831011819.775"><vh>StringFormatterChecker.build_dict_type</vh></v>
<v t="ekr.20230831011819.776"><vh>StringFormatterChecker.build_replacement_checkers</vh></v>
<v t="ekr.20230831011819.777"><vh>StringFormatterChecker.replacement_checkers</vh></v>
<v t="ekr.20230831011819.778"><vh>StringFormatterChecker.checkers_for_star</vh></v>
<v t="ekr.20230831011819.779"><vh>StringFormatterChecker.check_placeholder_type</vh></v>
<v t="ekr.20230831011819.780"><vh>StringFormatterChecker.checkers_for_regular_type</vh></v>
<v t="ekr.20230831011819.781"><vh>StringFormatterChecker.check_s_special_cases</vh></v>
<v t="ekr.20230831011819.782"><vh>StringFormatterChecker.checkers_for_c_type</vh></v>
<v t="ekr.20230831011819.783"><vh>StringFormatterChecker.conversion_type</vh></v>
<v t="ekr.20230831011819.784"><vh>StringFormatterChecker.named_type</vh></v>
<v t="ekr.20230831011819.785"><vh>StringFormatterChecker.accept</vh></v>
</v>
<v t="ekr.20230831011819.786"><vh>function: has_type_component</vh></v>
</v>
<v t="ekr.20230831011820.1572"><vh>@clean solve.py</vh>
<v t="ekr.20230831011820.1573"><vh>&lt;&lt; solve.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1574"><vh>function: solve_constraints</vh></v>
<v t="ekr.20230831011820.1575"><vh>function: solve_with_dependent</vh></v>
<v t="ekr.20230831011820.1576"><vh>function: solve_iteratively</vh></v>
<v t="ekr.20230831011820.1577"><vh>function: solve_one</vh></v>
<v t="ekr.20230831011820.1578"><vh>function: choose_free</vh></v>
<v t="ekr.20230831011820.1579"><vh>function: is_trivial_bound</vh></v>
<v t="ekr.20230831011820.1580"><vh>function: find_linear</vh></v>
<v t="ekr.20230831011820.1581"><vh>function: transitive_closure</vh></v>
<v t="ekr.20230831011820.1582"><vh>function: compute_dependencies</vh></v>
<v t="ekr.20230831011820.1583"><vh>function: check_linear</vh></v>
<v t="ekr.20230831011820.1584"><vh>function: get_vars</vh></v>
</v>
<v t="ekr.20230831011820.2058"><vh>@clean suggestions.py</vh>
<v t="ekr.20230831011820.2060"><vh>&lt;&lt; suggestions.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.2061"><vh>class PyAnnotateSignature</vh></v>
<v t="ekr.20230831011820.2062"><vh>class Callsite</vh></v>
<v t="ekr.20230831011820.2063"><vh>class SuggestionPlugin</vh>
<v t="ekr.20230831011820.2064"><vh>SuggestionPlugin.__init__</vh></v>
<v t="ekr.20230831011820.2065"><vh>SuggestionPlugin.get_function_hook</vh></v>
<v t="ekr.20230831011820.2066"><vh>SuggestionPlugin.get_method_hook</vh></v>
<v t="ekr.20230831011820.2067"><vh>SuggestionPlugin.log</vh></v>
</v>
<v t="ekr.20230831011820.2068"><vh>class ReturnFinder</vh>
<v t="ekr.20230831011820.2069"><vh>ReturnFinder.__init__</vh></v>
<v t="ekr.20230831011820.2070"><vh>ReturnFinder.visit_return_stmt</vh></v>
<v t="ekr.20230831011820.2071"><vh>ReturnFinder.visit_func_def</vh></v>
</v>
<v t="ekr.20230831011820.2072"><vh>function: get_return_types</vh></v>
<v t="ekr.20230831011820.2073"><vh>class ArgUseFinder</vh>
<v t="ekr.20230831011820.2074"><vh>ArgUseFinder.__init__</vh></v>
<v t="ekr.20230831011820.2075"><vh>ArgUseFinder.visit_call_expr</vh></v>
</v>
<v t="ekr.20230831011820.2076"><vh>function: get_arg_uses</vh></v>
<v t="ekr.20230831011820.2077"><vh>class SuggestionFailure</vh></v>
<v t="ekr.20230831011820.2078"><vh>function: is_explicit_any</vh></v>
<v t="ekr.20230831011820.2079"><vh>function: is_implicit_any</vh></v>
<v t="ekr.20230831011820.2080"><vh>class SuggestionEngine</vh>
<v t="ekr.20230831011820.2081"><vh>SuggestionEngine.__init__</vh></v>
<v t="ekr.20230831011820.2082"><vh>SuggestionEngine.suggest</vh></v>
<v t="ekr.20230831011820.2083"><vh>SuggestionEngine.suggest_callsites</vh></v>
<v t="ekr.20230831011820.2084"><vh>SuggestionEngine.restore_after</vh></v>
<v t="ekr.20230831011820.2085"><vh>SuggestionEngine.with_export_types</vh></v>
<v t="ekr.20230831011820.2086"><vh>SuggestionEngine.get_trivial_type</vh></v>
<v t="ekr.20230831011820.2087"><vh>SuggestionEngine.get_starting_type</vh></v>
<v t="ekr.20230831011820.2088"><vh>SuggestionEngine.get_args</vh></v>
<v t="ekr.20230831011820.2089"><vh>SuggestionEngine.get_default_arg_types</vh></v>
<v t="ekr.20230831011820.2090"><vh>SuggestionEngine.get_guesses</vh></v>
<v t="ekr.20230831011820.2091"><vh>SuggestionEngine.get_callsites</vh></v>
<v t="ekr.20230831011820.2092"><vh>SuggestionEngine.filter_options</vh></v>
<v t="ekr.20230831011820.2093"><vh>SuggestionEngine.find_best</vh></v>
<v t="ekr.20230831011820.2094"><vh>SuggestionEngine.get_guesses_from_parent</vh></v>
<v t="ekr.20230831011820.2095"><vh>SuggestionEngine.get_suggestion</vh></v>
<v t="ekr.20230831011820.2096"><vh>SuggestionEngine.format_args</vh></v>
<v t="ekr.20230831011820.2097"><vh>SuggestionEngine.find_node</vh></v>
<v t="ekr.20230831011820.2098"><vh>SuggestionEngine.find_node_by_module_and_name</vh></v>
<v t="ekr.20230831011820.2099"><vh>SuggestionEngine.find_node_by_file_and_line</vh></v>
<v t="ekr.20230831011820.2100"><vh>SuggestionEngine.extract_from_decorator</vh></v>
<v t="ekr.20230831011820.2101"><vh>SuggestionEngine.try_type</vh></v>
<v t="ekr.20230831011820.2102"><vh>SuggestionEngine.reload</vh></v>
<v t="ekr.20230831011820.2103"><vh>SuggestionEngine.ensure_loaded</vh></v>
<v t="ekr.20230831011820.2104"><vh>SuggestionEngine.named_type</vh></v>
<v t="ekr.20230831011820.2105"><vh>SuggestionEngine.json_suggestion</vh></v>
<v t="ekr.20230831011820.2106"><vh>SuggestionEngine.pyannotate_signature</vh></v>
<v t="ekr.20230831011820.2107"><vh>SuggestionEngine.format_signature</vh></v>
<v t="ekr.20230831011820.2108"><vh>SuggestionEngine.format_type</vh></v>
<v t="ekr.20230831011820.2109"><vh>SuggestionEngine.score_type</vh></v>
<v t="ekr.20230831011820.2110"><vh>SuggestionEngine.score_callable</vh></v>
</v>
<v t="ekr.20230831011820.2111"><vh>function: any_score_type</vh></v>
<v t="ekr.20230831011820.2112"><vh>function: any_score_callable</vh></v>
<v t="ekr.20230831011820.2113"><vh>function: is_tricky_callable</vh></v>
<v t="ekr.20230831011820.2114"><vh>class TypeFormatter</vh>
<v t="ekr.20230831011820.2115"><vh>TypeFormatter.__init__</vh></v>
<v t="ekr.20230831011820.2116"><vh>TypeFormatter.visit_any</vh></v>
<v t="ekr.20230831011820.2117"><vh>TypeFormatter.visit_instance</vh></v>
<v t="ekr.20230831011820.2118"><vh>TypeFormatter.visit_tuple_type</vh></v>
<v t="ekr.20230831011820.2119"><vh>TypeFormatter.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011820.2120"><vh>TypeFormatter.visit_typeddict_type</vh></v>
<v t="ekr.20230831011820.2121"><vh>TypeFormatter.visit_union_type</vh></v>
<v t="ekr.20230831011820.2122"><vh>TypeFormatter.visit_callable_type</vh></v>
</v>
<v t="ekr.20230831011820.2123"><vh>function: make_suggestion_anys</vh></v>
<v t="ekr.20230831011820.2124"><vh>class MakeSuggestionAny</vh>
<v t="ekr.20230831011820.2125"><vh>MakeSuggestionAny.visit_any</vh></v>
<v t="ekr.20230831011820.2126"><vh>MakeSuggestionAny.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011820.2127"><vh>function: generate_type_combinations</vh></v>
<v t="ekr.20230831011820.2128"><vh>function: count_errors</vh></v>
<v t="ekr.20230831011820.2129"><vh>function: refine_type</vh></v>
<v t="ekr.20230831011820.2130"><vh>function: refine_union</vh></v>
<v t="ekr.20230831011820.2131"><vh>function: refine_callable</vh></v>
<v t="ekr.20230831011820.2132"><vh>function: dedup</vh></v>
</v>
</v>
<v t="ekr.20230831070315.1"><vh>--- Types</vh>
<v t="ekr.20230831011819.13"><vh>@clean applytype.py</vh>
<v t="ekr.20230831011819.14"><vh>&lt;&lt; applytype.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.15"><vh>function: get_target_type</vh></v>
<v t="ekr.20230831011819.16"><vh>function: apply_generic_arguments</vh></v>
</v>
<v t="ekr.20230831011819.17"><vh>@clean argmap.py</vh>
<v t="ekr.20230831011819.18"><vh>&lt;&lt; argmap.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.19"><vh>function: map_actuals_to_formals</vh></v>
<v t="ekr.20230831011819.20"><vh>function: map_formals_to_actuals</vh></v>
<v t="ekr.20230831011819.21"><vh>class ArgTypeExpander</vh>
<v t="ekr.20230831011819.22"><vh>ArgTypeExpander.__init__</vh></v>
<v t="ekr.20230831011819.23"><vh>ArgTypeExpander.expand_actual_type</vh></v>
</v>
</v>
<v t="ekr.20230831011819.24"><vh>@clean binder.py</vh>
<v t="ekr.20230831011819.25"><vh>&lt;&lt; binder.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.26"><vh>class Frame</vh>
<v t="ekr.20230831011819.27"><vh>Frame.__init__</vh></v>
<v t="ekr.20230831011819.28"><vh>Frame.__repr__</vh></v>
</v>
<v t="ekr.20230831011819.29"><vh>class ConditionalTypeBinder</vh>
<v t="ekr.20230831011819.30"><vh>ConditionalTypeBinder.__init__</vh></v>
<v t="ekr.20230831011819.31"><vh>ConditionalTypeBinder._get_id</vh></v>
<v t="ekr.20230831011819.32"><vh>ConditionalTypeBinder._add_dependencies</vh></v>
<v t="ekr.20230831011819.33"><vh>ConditionalTypeBinder.push_frame</vh></v>
<v t="ekr.20230831011819.34"><vh>ConditionalTypeBinder._put</vh></v>
<v t="ekr.20230831011819.35"><vh>ConditionalTypeBinder._get</vh></v>
<v t="ekr.20230831011819.36"><vh>ConditionalTypeBinder.put</vh></v>
<v t="ekr.20230831011819.37"><vh>ConditionalTypeBinder.unreachable</vh></v>
<v t="ekr.20230831011819.38"><vh>ConditionalTypeBinder.suppress_unreachable_warnings</vh></v>
<v t="ekr.20230831011819.39"><vh>ConditionalTypeBinder.get</vh></v>
<v t="ekr.20230831011819.40"><vh>ConditionalTypeBinder.is_unreachable</vh></v>
<v t="ekr.20230831011819.41"><vh>ConditionalTypeBinder.is_unreachable_warning_suppressed</vh></v>
<v t="ekr.20230831011819.42"><vh>ConditionalTypeBinder.cleanse</vh></v>
<v t="ekr.20230831011819.43"><vh>ConditionalTypeBinder._cleanse_key</vh></v>
<v t="ekr.20230831011819.44"><vh>ConditionalTypeBinder.update_from_options</vh></v>
<v t="ekr.20230831011819.45"><vh>ConditionalTypeBinder.pop_frame</vh></v>
<v t="ekr.20230831011819.46"><vh>ConditionalTypeBinder.accumulate_type_assignments</vh></v>
<v t="ekr.20230831011819.47"><vh>ConditionalTypeBinder.assign_type</vh></v>
<v t="ekr.20230831011819.48"><vh>ConditionalTypeBinder.invalidate_dependencies</vh></v>
<v t="ekr.20230831011819.49"><vh>ConditionalTypeBinder.most_recent_enclosing_type</vh></v>
<v t="ekr.20230831011819.50"><vh>ConditionalTypeBinder.allow_jump</vh></v>
<v t="ekr.20230831011819.51"><vh>ConditionalTypeBinder.handle_break</vh></v>
<v t="ekr.20230831011819.52"><vh>ConditionalTypeBinder.handle_continue</vh></v>
<v t="ekr.20230831011819.53"><vh>ConditionalTypeBinder.frame_context</vh></v>
<v t="ekr.20230831011819.54"><vh>ConditionalTypeBinder.top_frame_context</vh></v>
</v>
<v t="ekr.20230831011819.55"><vh>function: get_declaration</vh></v>
</v>
<v t="ekr.20230831011819.56"><vh>@clean bogus_type.py</vh></v>
<v t="ekr.20230831011819.810"><vh>@clean constant_fold.py</vh>
<v t="ekr.20230831011819.812"><vh>&lt;&lt; constant_fold.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.813"><vh>function: constant_fold_expr</vh></v>
<v t="ekr.20230831011819.814"><vh>function: constant_fold_binary_op</vh></v>
<v t="ekr.20230831011819.815"><vh>function: constant_fold_binary_int_op</vh></v>
<v t="ekr.20230831011819.816"><vh>function: constant_fold_binary_float_op</vh></v>
<v t="ekr.20230831011819.817"><vh>function: constant_fold_unary_op</vh></v>
</v>
<v t="ekr.20230831011819.818"><vh>@clean constraints.py</vh>
<v t="ekr.20230831011819.819"><vh>&lt;&lt; constraints.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.820"><vh>class Constraint</vh>
<v t="ekr.20230831011819.821"><vh>Constraint.__init__</vh></v>
<v t="ekr.20230831011819.822"><vh>Constraint.__repr__</vh></v>
<v t="ekr.20230831011819.823"><vh>Constraint.__hash__</vh></v>
<v t="ekr.20230831011819.824"><vh>Constraint.__eq__</vh></v>
</v>
<v t="ekr.20230831011819.825"><vh>function: infer_constraints_for_callable</vh></v>
<v t="ekr.20230831011819.826"><vh>function: infer_constraints</vh></v>
<v t="ekr.20230831011819.827"><vh>function: _infer_constraints</vh></v>
<v t="ekr.20230831011819.828"><vh>function: infer_constraints_if_possible</vh></v>
<v t="ekr.20230831011819.829"><vh>function: select_trivial</vh></v>
<v t="ekr.20230831011819.830"><vh>function: merge_with_any</vh></v>
<v t="ekr.20230831011819.831"><vh>function: handle_recursive_union</vh></v>
<v t="ekr.20230831011819.832"><vh>function: any_constraints</vh></v>
<v t="ekr.20230831011819.833"><vh>function: filter_satisfiable</vh></v>
<v t="ekr.20230831011819.834"><vh>function: is_same_constraints</vh></v>
<v t="ekr.20230831011819.835"><vh>function: is_same_constraint</vh></v>
<v t="ekr.20230831011819.836"><vh>function: is_similar_constraints</vh></v>
<v t="ekr.20230831011819.837"><vh>function: _is_similar_constraints</vh></v>
<v t="ekr.20230831011819.838"><vh>function: simplify_away_incomplete_types</vh></v>
<v t="ekr.20230831011819.839"><vh>function: is_complete_type</vh></v>
<v t="ekr.20230831011819.840"><vh>class CompleteTypeVisitor</vh>
<v t="ekr.20230831011819.841"><vh>CompleteTypeVisitor.__init__</vh></v>
<v t="ekr.20230831011819.842"><vh>CompleteTypeVisitor.visit_uninhabited_type</vh></v>
</v>
<v t="ekr.20230831011819.843"><vh>class ConstraintBuilderVisitor</vh>
<v t="ekr.20230831011819.844"><vh>ConstraintBuilderVisitor.__init__</vh></v>
<v t="ekr.20230831011819.845"><vh>ConstraintBuilderVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011819.846"><vh>ConstraintBuilderVisitor.visit_any</vh></v>
<v t="ekr.20230831011819.847"><vh>ConstraintBuilderVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011819.848"><vh>ConstraintBuilderVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.849"><vh>ConstraintBuilderVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011819.850"><vh>ConstraintBuilderVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011819.851"><vh>ConstraintBuilderVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011819.852"><vh>ConstraintBuilderVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011819.853"><vh>ConstraintBuilderVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011819.854"><vh>ConstraintBuilderVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011819.855"><vh>ConstraintBuilderVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.856"><vh>ConstraintBuilderVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011819.857"><vh>ConstraintBuilderVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011819.858"><vh>ConstraintBuilderVisitor.visit_instance</vh></v>
<v t="ekr.20230831011819.859"><vh>ConstraintBuilderVisitor.infer_constraints_from_protocol_members</vh></v>
<v t="ekr.20230831011819.860"><vh>ConstraintBuilderVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011819.861"><vh>ConstraintBuilderVisitor.infer_against_overloaded</vh></v>
<v t="ekr.20230831011819.862"><vh>ConstraintBuilderVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011819.863"><vh>ConstraintBuilderVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011819.864"><vh>ConstraintBuilderVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011819.865"><vh>ConstraintBuilderVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20230831011819.866"><vh>ConstraintBuilderVisitor.infer_against_any</vh></v>
<v t="ekr.20230831011819.867"><vh>ConstraintBuilderVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011819.868"><vh>ConstraintBuilderVisitor.visit_type_type</vh></v>
</v>
<v t="ekr.20230831011819.869"><vh>function: neg_op</vh></v>
<v t="ekr.20230831011819.870"><vh>function: find_matching_overload_item</vh></v>
<v t="ekr.20230831011819.871"><vh>function: find_matching_overload_items</vh></v>
<v t="ekr.20230831011819.872"><vh>function: get_tuple_fallback_from_unpack</vh></v>
<v t="ekr.20230831011819.873"><vh>function: repack_callable_args</vh></v>
<v t="ekr.20230831011819.874"><vh>function: build_constraints_for_simple_unpack</vh></v>
<v t="ekr.20230831011819.875"><vh>function: build_constraints_for_unpack</vh></v>
<v t="ekr.20230831011819.876"><vh>function: infer_directed_arg_constraints</vh></v>
<v t="ekr.20230831011819.877"><vh>function: infer_callable_arguments_constraints</vh></v>
</v>
<v t="ekr.20230831011819.878"><vh>@clean copytype.py</vh>
<v t="ekr.20230831011819.879"><vh>&lt;&lt; copytype.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.880"><vh>function: copy_type</vh></v>
<v t="ekr.20230831011819.881"><vh>class TypeShallowCopier</vh>
<v t="ekr.20230831011819.882"><vh>TypeShallowCopier.visit_unbound_type</vh></v>
<v t="ekr.20230831011819.883"><vh>TypeShallowCopier.visit_any</vh></v>
<v t="ekr.20230831011819.884"><vh>TypeShallowCopier.visit_none_type</vh></v>
<v t="ekr.20230831011819.885"><vh>TypeShallowCopier.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.886"><vh>TypeShallowCopier.visit_erased_type</vh></v>
<v t="ekr.20230831011819.887"><vh>TypeShallowCopier.visit_deleted_type</vh></v>
<v t="ekr.20230831011819.888"><vh>TypeShallowCopier.visit_instance</vh></v>
<v t="ekr.20230831011819.889"><vh>TypeShallowCopier.visit_type_var</vh></v>
<v t="ekr.20230831011819.890"><vh>TypeShallowCopier.visit_param_spec</vh></v>
<v t="ekr.20230831011819.891"><vh>TypeShallowCopier.visit_parameters</vh></v>
<v t="ekr.20230831011819.892"><vh>TypeShallowCopier.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.893"><vh>TypeShallowCopier.visit_unpack_type</vh></v>
<v t="ekr.20230831011819.894"><vh>TypeShallowCopier.visit_partial_type</vh></v>
<v t="ekr.20230831011819.895"><vh>TypeShallowCopier.visit_callable_type</vh></v>
<v t="ekr.20230831011819.896"><vh>TypeShallowCopier.visit_tuple_type</vh></v>
<v t="ekr.20230831011819.897"><vh>TypeShallowCopier.visit_typeddict_type</vh></v>
<v t="ekr.20230831011819.898"><vh>TypeShallowCopier.visit_literal_type</vh></v>
<v t="ekr.20230831011819.899"><vh>TypeShallowCopier.visit_union_type</vh></v>
<v t="ekr.20230831011819.900"><vh>TypeShallowCopier.visit_overloaded</vh></v>
<v t="ekr.20230831011819.901"><vh>TypeShallowCopier.visit_type_type</vh></v>
<v t="ekr.20230831011819.902"><vh>TypeShallowCopier.visit_type_alias_type</vh></v>
<v t="ekr.20230831011819.903"><vh>TypeShallowCopier.copy_common</vh></v>
</v>
</v>
<v t="ekr.20230831011819.954"><vh>@clean erasetype.py</vh>
<v t="ekr.20230831011819.955"><vh>&lt;&lt; erasetype.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.956"><vh>function: erase_type</vh></v>
<v t="ekr.20230831011819.957"><vh>class EraseTypeVisitor</vh>
<v t="ekr.20230831011819.958"><vh>EraseTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011819.959"><vh>EraseTypeVisitor.visit_any</vh></v>
<v t="ekr.20230831011819.960"><vh>EraseTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011819.961"><vh>EraseTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.962"><vh>EraseTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011819.963"><vh>EraseTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011819.964"><vh>EraseTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011819.965"><vh>EraseTypeVisitor.visit_instance</vh></v>
<v t="ekr.20230831011819.966"><vh>EraseTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011819.967"><vh>EraseTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011819.968"><vh>EraseTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011819.969"><vh>EraseTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.970"><vh>EraseTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011819.971"><vh>EraseTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011819.972"><vh>EraseTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011819.973"><vh>EraseTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011819.974"><vh>EraseTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011819.975"><vh>EraseTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011819.976"><vh>EraseTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011819.977"><vh>EraseTypeVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011819.978"><vh>EraseTypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011819.979"><vh>function: erase_typevars</vh></v>
<v t="ekr.20230831011819.980"><vh>function: replace_meta_vars</vh></v>
<v t="ekr.20230831011819.981"><vh>class TypeVarEraser</vh>
<v t="ekr.20230831011819.982"><vh>TypeVarEraser.__init__</vh></v>
<v t="ekr.20230831011819.983"><vh>TypeVarEraser.visit_type_var</vh></v>
<v t="ekr.20230831011819.984"><vh>TypeVarEraser.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.985"><vh>TypeVarEraser.visit_param_spec</vh></v>
<v t="ekr.20230831011819.986"><vh>TypeVarEraser.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011819.987"><vh>function: remove_instance_last_known_values</vh></v>
<v t="ekr.20230831011819.988"><vh>class LastKnownValueEraser</vh>
<v t="ekr.20230831011819.989"><vh>LastKnownValueEraser.visit_instance</vh></v>
<v t="ekr.20230831011819.990"><vh>LastKnownValueEraser.visit_type_alias_type</vh></v>
<v t="ekr.20230831011819.991"><vh>LastKnownValueEraser.visit_union_type</vh></v>
</v>
</v>
<v t="ekr.20230831011819.1054"><vh>@clean evalexpr.py</vh>
<v t="ekr.20230831011819.1056"><vh>&lt;&lt; evalexpr.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1057"><vh>class _NodeEvaluator</vh>
<v t="ekr.20230831011819.1058"><vh>_NodeEvaluator.visit_int_expr</vh></v>
<v t="ekr.20230831011819.1059"><vh>_NodeEvaluator.visit_str_expr</vh></v>
<v t="ekr.20230831011819.1060"><vh>_NodeEvaluator.visit_bytes_expr</vh></v>
<v t="ekr.20230831011819.1061"><vh>_NodeEvaluator.visit_float_expr</vh></v>
<v t="ekr.20230831011819.1062"><vh>_NodeEvaluator.visit_complex_expr</vh></v>
<v t="ekr.20230831011819.1063"><vh>_NodeEvaluator.visit_ellipsis</vh></v>
<v t="ekr.20230831011819.1064"><vh>_NodeEvaluator.visit_star_expr</vh></v>
<v t="ekr.20230831011819.1065"><vh>_NodeEvaluator.visit_name_expr</vh></v>
<v t="ekr.20230831011819.1066"><vh>_NodeEvaluator.visit_member_expr</vh></v>
<v t="ekr.20230831011819.1067"><vh>_NodeEvaluator.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011819.1068"><vh>_NodeEvaluator.visit_yield_expr</vh></v>
<v t="ekr.20230831011819.1069"><vh>_NodeEvaluator.visit_call_expr</vh></v>
<v t="ekr.20230831011819.1070"><vh>_NodeEvaluator.visit_op_expr</vh></v>
<v t="ekr.20230831011819.1071"><vh>_NodeEvaluator.visit_comparison_expr</vh></v>
<v t="ekr.20230831011819.1072"><vh>_NodeEvaluator.visit_cast_expr</vh></v>
<v t="ekr.20230831011819.1073"><vh>_NodeEvaluator.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011819.1074"><vh>_NodeEvaluator.visit_reveal_expr</vh></v>
<v t="ekr.20230831011819.1075"><vh>_NodeEvaluator.visit_super_expr</vh></v>
<v t="ekr.20230831011819.1076"><vh>_NodeEvaluator.visit_unary_expr</vh></v>
<v t="ekr.20230831011819.1077"><vh>_NodeEvaluator.visit_assignment_expr</vh></v>
<v t="ekr.20230831011819.1078"><vh>_NodeEvaluator.visit_list_expr</vh></v>
<v t="ekr.20230831011819.1079"><vh>_NodeEvaluator.visit_dict_expr</vh></v>
<v t="ekr.20230831011819.1080"><vh>_NodeEvaluator.visit_tuple_expr</vh></v>
<v t="ekr.20230831011819.1081"><vh>_NodeEvaluator.visit_set_expr</vh></v>
<v t="ekr.20230831011819.1082"><vh>_NodeEvaluator.visit_index_expr</vh></v>
<v t="ekr.20230831011819.1083"><vh>_NodeEvaluator.visit_type_application</vh></v>
<v t="ekr.20230831011819.1084"><vh>_NodeEvaluator.visit_lambda_expr</vh></v>
<v t="ekr.20230831011819.1085"><vh>_NodeEvaluator.visit_list_comprehension</vh></v>
<v t="ekr.20230831011819.1086"><vh>_NodeEvaluator.visit_set_comprehension</vh></v>
<v t="ekr.20230831011819.1087"><vh>_NodeEvaluator.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011819.1088"><vh>_NodeEvaluator.visit_generator_expr</vh></v>
<v t="ekr.20230831011819.1089"><vh>_NodeEvaluator.visit_slice_expr</vh></v>
<v t="ekr.20230831011819.1090"><vh>_NodeEvaluator.visit_conditional_expr</vh></v>
<v t="ekr.20230831011819.1091"><vh>_NodeEvaluator.visit_type_var_expr</vh></v>
<v t="ekr.20230831011819.1092"><vh>_NodeEvaluator.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011819.1093"><vh>_NodeEvaluator.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011819.1094"><vh>_NodeEvaluator.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011819.1095"><vh>_NodeEvaluator.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011819.1096"><vh>_NodeEvaluator.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011819.1097"><vh>_NodeEvaluator.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011819.1098"><vh>_NodeEvaluator.visit_newtype_expr</vh></v>
<v t="ekr.20230831011819.1099"><vh>_NodeEvaluator.visit__promote_expr</vh></v>
<v t="ekr.20230831011819.1100"><vh>_NodeEvaluator.visit_await_expr</vh></v>
<v t="ekr.20230831011819.1101"><vh>_NodeEvaluator.visit_temp_node</vh></v>
</v>
<v t="ekr.20230831011819.1102"><vh>function: evaluate_expression</vh></v>
</v>
<v t="ekr.20230831011819.1103"><vh>@clean expandtype.py</vh>
<v t="ekr.20230831011819.1104"><vh>&lt;&lt; expandtype.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1105"><vh>function: expand_type</vh></v>
<v t="ekr.20230831011819.1106"><vh>function: expand_type</vh></v>
<v t="ekr.20230831011819.1107"><vh>function: expand_type</vh></v>
<v t="ekr.20230831011819.1108"><vh>function: expand_type</vh></v>
<v t="ekr.20230831011819.1109"><vh>function: expand_type_by_instance</vh></v>
<v t="ekr.20230831011819.1110"><vh>function: expand_type_by_instance</vh></v>
<v t="ekr.20230831011819.1111"><vh>function: expand_type_by_instance</vh></v>
<v t="ekr.20230831011819.1112"><vh>function: expand_type_by_instance</vh></v>
<v t="ekr.20230831011819.1113"><vh>function: freshen_function_type_vars</vh></v>
<v t="ekr.20230831011819.1114"><vh>class HasGenericCallable</vh>
<v t="ekr.20230831011819.1115"><vh>HasGenericCallable.__init__</vh></v>
<v t="ekr.20230831011819.1116"><vh>HasGenericCallable.visit_callable_type</vh></v>
</v>
<v t="ekr.20230831011819.1117"><vh>function: freshen_all_functions_type_vars</vh></v>
<v t="ekr.20230831011819.1118"><vh>class FreshenCallableVisitor</vh>
<v t="ekr.20230831011819.1119"><vh>FreshenCallableVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011819.1120"><vh>FreshenCallableVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011819.1121"><vh>class ExpandTypeVisitor</vh>
<v t="ekr.20230831011819.1122"><vh>ExpandTypeVisitor.__init__</vh></v>
<v t="ekr.20230831011819.1123"><vh>ExpandTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011819.1124"><vh>ExpandTypeVisitor.visit_any</vh></v>
<v t="ekr.20230831011819.1125"><vh>ExpandTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011819.1126"><vh>ExpandTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.1127"><vh>ExpandTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011819.1128"><vh>ExpandTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011819.1129"><vh>ExpandTypeVisitor.visit_instance</vh></v>
<v t="ekr.20230831011819.1130"><vh>ExpandTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011819.1131"><vh>ExpandTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011819.1132"><vh>ExpandTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.1133"><vh>ExpandTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011819.1134"><vh>ExpandTypeVisitor.expand_unpack</vh></v>
<v t="ekr.20230831011819.1135"><vh>ExpandTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011819.1136"><vh>ExpandTypeVisitor.interpolate_args_for_unpack</vh></v>
<v t="ekr.20230831011819.1137"><vh>ExpandTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011819.1138"><vh>ExpandTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011819.1139"><vh>ExpandTypeVisitor.expand_types_with_unpack</vh></v>
<v t="ekr.20230831011819.1140"><vh>ExpandTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011819.1141"><vh>ExpandTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011819.1142"><vh>ExpandTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011819.1143"><vh>ExpandTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011819.1144"><vh>ExpandTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011819.1145"><vh>ExpandTypeVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011819.1146"><vh>ExpandTypeVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20230831011819.1147"><vh>ExpandTypeVisitor.expand_types</vh></v>
</v>
<v t="ekr.20230831011819.1148"><vh>function: expand_self_type</vh></v>
<v t="ekr.20230831011819.1149"><vh>function: expand_self_type</vh></v>
<v t="ekr.20230831011819.1150"><vh>function: expand_self_type</vh></v>
<v t="ekr.20230831011819.1151"><vh>function: remove_trivial</vh></v>
</v>
<v t="ekr.20230831011819.1152"><vh>@clean exprtotype.py</vh>
<v t="ekr.20230831011819.1153"><vh>&lt;&lt; exprtotype.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1154"><vh>class TypeTranslationError</vh></v>
<v t="ekr.20230831011819.1155"><vh>function: _extract_argument_name</vh></v>
<v t="ekr.20230831011819.1156"><vh>function: expr_to_unanalyzed_type</vh></v>
</v>
<v t="ekr.20230831011819.1444"><vh>@clean infer.py</vh>
<v t="ekr.20230831011819.1445"><vh>&lt;&lt; infer.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1446"><vh>class ArgumentInferContext</vh></v>
<v t="ekr.20230831011819.1447"><vh>function: infer_function_type_arguments</vh></v>
<v t="ekr.20230831011819.1448"><vh>function: infer_type_arguments</vh></v>
</v>
<v t="ekr.20230831011819.1507"><vh>@clean join.py</vh>
<v t="ekr.20230831011819.1508"><vh>&lt;&lt; join.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1509"><vh>class InstanceJoiner</vh>
<v t="ekr.20230831011819.1510"><vh>InstanceJoiner.__init__</vh></v>
<v t="ekr.20230831011819.1511"><vh>InstanceJoiner.join_instances</vh></v>
<v t="ekr.20230831011819.1512"><vh>InstanceJoiner.join_instances_via_supertype</vh></v>
</v>
<v t="ekr.20230831011819.1513"><vh>function: join_simple</vh></v>
<v t="ekr.20230831011819.1514"><vh>function: trivial_join</vh></v>
<v t="ekr.20230831011819.1515"><vh>function: join_types</vh></v>
<v t="ekr.20230831011819.1516"><vh>function: join_types</vh></v>
<v t="ekr.20230831011819.1517"><vh>function: join_types</vh></v>
<v t="ekr.20230831011819.1518"><vh>class TypeJoinVisitor</vh>
<v t="ekr.20230831011819.1519"><vh>TypeJoinVisitor.__init__</vh></v>
<v t="ekr.20230831011819.1520"><vh>TypeJoinVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011819.1521"><vh>TypeJoinVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011819.1522"><vh>TypeJoinVisitor.visit_any</vh></v>
<v t="ekr.20230831011819.1523"><vh>TypeJoinVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011819.1524"><vh>TypeJoinVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011819.1525"><vh>TypeJoinVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011819.1526"><vh>TypeJoinVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011819.1527"><vh>TypeJoinVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011819.1528"><vh>TypeJoinVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011819.1529"><vh>TypeJoinVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011819.1530"><vh>TypeJoinVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011819.1531"><vh>TypeJoinVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011819.1532"><vh>TypeJoinVisitor.visit_instance</vh></v>
<v t="ekr.20230831011819.1533"><vh>TypeJoinVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011819.1534"><vh>TypeJoinVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011819.1535"><vh>TypeJoinVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011819.1536"><vh>TypeJoinVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011819.1537"><vh>TypeJoinVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011819.1538"><vh>TypeJoinVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011819.1539"><vh>TypeJoinVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011819.1540"><vh>TypeJoinVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20230831011819.1541"><vh>TypeJoinVisitor.default</vh></v>
</v>
<v t="ekr.20230831011819.1542"><vh>function: is_better</vh></v>
<v t="ekr.20230831011819.1543"><vh>function: normalize_callables</vh></v>
<v t="ekr.20230831011819.1544"><vh>function: is_similar_callables</vh></v>
<v t="ekr.20230831011819.1545"><vh>function: join_similar_callables</vh></v>
<v t="ekr.20230831011819.1546"><vh>function: combine_similar_callables</vh></v>
<v t="ekr.20230831011819.1547"><vh>function: combine_arg_names</vh></v>
<v t="ekr.20230831011819.1548"><vh>function: object_from_instance</vh></v>
<v t="ekr.20230831011819.1549"><vh>function: object_or_any_from_type</vh></v>
<v t="ekr.20230831011819.1550"><vh>function: join_type_list</vh></v>
<v t="ekr.20230831011819.1551"><vh>function: unpack_callback_protocol</vh></v>
</v>
<v t="ekr.20230831011819.1552"><vh>@clean literals.py</vh>
<v t="ekr.20230831011819.1553"><vh>&lt;&lt; literals.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1554"><vh>function: literal</vh></v>
<v t="ekr.20230831011819.1555"><vh>function: subkeys</vh></v>
<v t="ekr.20230831011819.1556"><vh>function: literal_hash</vh></v>
<v t="ekr.20230831011819.1557"><vh>function: extract_var_from_literal_hash</vh></v>
<v t="ekr.20230831011819.1558"><vh>class _Hasher</vh>
<v t="ekr.20230831011819.1559"><vh>_Hasher.visit_int_expr</vh></v>
<v t="ekr.20230831011819.1560"><vh>_Hasher.visit_str_expr</vh></v>
<v t="ekr.20230831011819.1561"><vh>_Hasher.visit_bytes_expr</vh></v>
<v t="ekr.20230831011819.1562"><vh>_Hasher.visit_float_expr</vh></v>
<v t="ekr.20230831011819.1563"><vh>_Hasher.visit_complex_expr</vh></v>
<v t="ekr.20230831011819.1564"><vh>_Hasher.visit_star_expr</vh></v>
<v t="ekr.20230831011819.1565"><vh>_Hasher.visit_name_expr</vh></v>
<v t="ekr.20230831011819.1566"><vh>_Hasher.visit_member_expr</vh></v>
<v t="ekr.20230831011819.1567"><vh>_Hasher.visit_op_expr</vh></v>
<v t="ekr.20230831011819.1568"><vh>_Hasher.visit_comparison_expr</vh></v>
<v t="ekr.20230831011819.1569"><vh>_Hasher.visit_unary_expr</vh></v>
<v t="ekr.20230831011819.1570"><vh>_Hasher.seq_expr</vh></v>
<v t="ekr.20230831011819.1571"><vh>_Hasher.visit_list_expr</vh></v>
<v t="ekr.20230831011819.1572"><vh>_Hasher.visit_dict_expr</vh></v>
<v t="ekr.20230831011819.1573"><vh>_Hasher.visit_tuple_expr</vh></v>
<v t="ekr.20230831011819.1574"><vh>_Hasher.visit_set_expr</vh></v>
<v t="ekr.20230831011819.1575"><vh>_Hasher.visit_index_expr</vh></v>
<v t="ekr.20230831011819.1576"><vh>_Hasher.visit_assignment_expr</vh></v>
<v t="ekr.20230831011819.1577"><vh>_Hasher.visit_call_expr</vh></v>
<v t="ekr.20230831011819.1578"><vh>_Hasher.visit_slice_expr</vh></v>
<v t="ekr.20230831011819.1579"><vh>_Hasher.visit_cast_expr</vh></v>
<v t="ekr.20230831011819.1580"><vh>_Hasher.visit_assert_type_expr</vh></v>
<v t="ekr.20230831011819.1581"><vh>_Hasher.visit_conditional_expr</vh></v>
<v t="ekr.20230831011819.1582"><vh>_Hasher.visit_ellipsis</vh></v>
<v t="ekr.20230831011819.1583"><vh>_Hasher.visit_yield_from_expr</vh></v>
<v t="ekr.20230831011819.1584"><vh>_Hasher.visit_yield_expr</vh></v>
<v t="ekr.20230831011819.1585"><vh>_Hasher.visit_reveal_expr</vh></v>
<v t="ekr.20230831011819.1586"><vh>_Hasher.visit_super_expr</vh></v>
<v t="ekr.20230831011819.1587"><vh>_Hasher.visit_type_application</vh></v>
<v t="ekr.20230831011819.1588"><vh>_Hasher.visit_lambda_expr</vh></v>
<v t="ekr.20230831011819.1589"><vh>_Hasher.visit_list_comprehension</vh></v>
<v t="ekr.20230831011819.1590"><vh>_Hasher.visit_set_comprehension</vh></v>
<v t="ekr.20230831011819.1591"><vh>_Hasher.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011819.1592"><vh>_Hasher.visit_generator_expr</vh></v>
<v t="ekr.20230831011819.1593"><vh>_Hasher.visit_type_var_expr</vh></v>
<v t="ekr.20230831011819.1594"><vh>_Hasher.visit_paramspec_expr</vh></v>
<v t="ekr.20230831011819.1595"><vh>_Hasher.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20230831011819.1596"><vh>_Hasher.visit_type_alias_expr</vh></v>
<v t="ekr.20230831011819.1597"><vh>_Hasher.visit_namedtuple_expr</vh></v>
<v t="ekr.20230831011819.1598"><vh>_Hasher.visit_enum_call_expr</vh></v>
<v t="ekr.20230831011819.1599"><vh>_Hasher.visit_typeddict_expr</vh></v>
<v t="ekr.20230831011819.1600"><vh>_Hasher.visit_newtype_expr</vh></v>
<v t="ekr.20230831011819.1601"><vh>_Hasher.visit__promote_expr</vh></v>
<v t="ekr.20230831011819.1602"><vh>_Hasher.visit_await_expr</vh></v>
<v t="ekr.20230831011819.1603"><vh>_Hasher.visit_temp_node</vh></v>
</v>
</v>
<v t="ekr.20230831011819.1604"><vh>@clean lookup.py</vh>
<v t="ekr.20230831011819.1606"><vh>&lt;&lt; lookup.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011819.1607"><vh>function: lookup_fully_qualified</vh></v>
</v>
<v t="ekr.20230831011820.6"><vh>@clean maptype.py</vh>
<v t="ekr.20230831011820.7"><vh>&lt;&lt; maptype.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.8"><vh>function: map_instance_to_supertype</vh></v>
<v t="ekr.20230831011820.9"><vh>function: map_instance_to_supertypes</vh></v>
<v t="ekr.20230831011820.10"><vh>function: class_derivation_paths</vh></v>
<v t="ekr.20230831011820.11"><vh>function: map_instance_to_direct_supertypes</vh></v>
<v t="ekr.20230831011820.12"><vh>function: instance_to_type_environment</vh></v>
</v>
<v t="ekr.20230831011820.13"><vh>@clean meet.py</vh>
<v t="ekr.20230831011820.14"><vh>&lt;&lt; meet.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.15"><vh>function: trivial_meet</vh></v>
<v t="ekr.20230831011820.16"><vh>function: meet_types</vh></v>
<v t="ekr.20230831011820.17"><vh>function: narrow_declared_type</vh></v>
<v t="ekr.20230831011820.18"><vh>function: get_possible_variants</vh></v>
<v t="ekr.20230831011820.19"><vh>function: is_enum_overlapping_union</vh></v>
<v t="ekr.20230831011820.20"><vh>function: is_literal_in_union</vh></v>
<v t="ekr.20230831011820.21"><vh>function: is_overlapping_types</vh></v>
<v t="ekr.20230831011820.22"><vh>function: is_overlapping_erased_types</vh></v>
<v t="ekr.20230831011820.23"><vh>function: are_typed_dicts_overlapping</vh></v>
<v t="ekr.20230831011820.24"><vh>function: are_tuples_overlapping</vh></v>
<v t="ekr.20230831011820.25"><vh>function: adjust_tuple</vh></v>
<v t="ekr.20230831011820.26"><vh>function: is_tuple</vh></v>
<v t="ekr.20230831011820.27"><vh>class TypeMeetVisitor</vh>
<v t="ekr.20230831011820.28"><vh>TypeMeetVisitor.__init__</vh></v>
<v t="ekr.20230831011820.29"><vh>TypeMeetVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011820.30"><vh>TypeMeetVisitor.visit_any</vh></v>
<v t="ekr.20230831011820.31"><vh>TypeMeetVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011820.32"><vh>TypeMeetVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011820.33"><vh>TypeMeetVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011820.34"><vh>TypeMeetVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011820.35"><vh>TypeMeetVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011820.36"><vh>TypeMeetVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011820.37"><vh>TypeMeetVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011820.38"><vh>TypeMeetVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011820.39"><vh>TypeMeetVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011820.40"><vh>TypeMeetVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011820.41"><vh>TypeMeetVisitor.visit_instance</vh></v>
<v t="ekr.20230831011820.42"><vh>TypeMeetVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011820.43"><vh>TypeMeetVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011820.44"><vh>TypeMeetVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011820.45"><vh>TypeMeetVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011820.46"><vh>TypeMeetVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011820.47"><vh>TypeMeetVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011820.48"><vh>TypeMeetVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011820.49"><vh>TypeMeetVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20230831011820.50"><vh>TypeMeetVisitor.meet</vh></v>
<v t="ekr.20230831011820.51"><vh>TypeMeetVisitor.default</vh></v>
</v>
<v t="ekr.20230831011820.52"><vh>function: meet_similar_callables</vh></v>
<v t="ekr.20230831011820.53"><vh>function: meet_type_list</vh></v>
<v t="ekr.20230831011820.54"><vh>function: typed_dict_mapping_pair</vh></v>
<v t="ekr.20230831011820.55"><vh>function: typed_dict_mapping_overlap</vh></v>
</v>
<v t="ekr.20230831011820.344"><vh>@clean mro.py</vh>
<v t="ekr.20230831011820.345"><vh>&lt;&lt; mro.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.346"><vh>function: calculate_mro</vh></v>
<v t="ekr.20230831011820.347"><vh>class MroError</vh></v>
<v t="ekr.20230831011820.348"><vh>function: linearize_hierarchy</vh></v>
<v t="ekr.20230831011820.349"><vh>function: merge</vh></v>
</v>
<v t="ekr.20230831011820.737"><vh>@clean partially_defined.py</vh>
<v t="ekr.20230831011820.738"><vh>&lt;&lt; partially_defined.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.739"><vh>class BranchState</vh>
<v t="ekr.20230831011820.740"><vh>BranchState.__init__</vh></v>
<v t="ekr.20230831011820.741"><vh>BranchState.copy</vh></v>
</v>
<v t="ekr.20230831011820.742"><vh>class BranchStatement</vh>
<v t="ekr.20230831011820.743"><vh>BranchStatement.__init__</vh></v>
<v t="ekr.20230831011820.744"><vh>BranchStatement.copy</vh></v>
<v t="ekr.20230831011820.745"><vh>BranchStatement.next_branch</vh></v>
<v t="ekr.20230831011820.746"><vh>BranchStatement.record_definition</vh></v>
<v t="ekr.20230831011820.747"><vh>BranchStatement.delete_var</vh></v>
<v t="ekr.20230831011820.748"><vh>BranchStatement.record_nested_branch</vh></v>
<v t="ekr.20230831011820.749"><vh>BranchStatement.skip_branch</vh></v>
<v t="ekr.20230831011820.750"><vh>BranchStatement.is_possibly_undefined</vh></v>
<v t="ekr.20230831011820.751"><vh>BranchStatement.is_undefined</vh></v>
<v t="ekr.20230831011820.752"><vh>BranchStatement.is_defined_in_a_branch</vh></v>
<v t="ekr.20230831011820.753"><vh>BranchStatement.done</vh></v>
</v>
<v t="ekr.20230831011820.754"><vh>class ScopeType</vh></v>
<v t="ekr.20230831011820.755"><vh>class Scope</vh>
<v t="ekr.20230831011820.756"><vh>Scope.__init__</vh></v>
<v t="ekr.20230831011820.757"><vh>Scope.copy</vh></v>
<v t="ekr.20230831011820.758"><vh>Scope.record_undefined_ref</vh></v>
<v t="ekr.20230831011820.759"><vh>Scope.pop_undefined_ref</vh></v>
</v>
<v t="ekr.20230831011820.760"><vh>class DefinedVariableTracker</vh>
<v t="ekr.20230831011820.761"><vh>DefinedVariableTracker.__init__</vh></v>
<v t="ekr.20230831011820.762"><vh>DefinedVariableTracker.copy</vh></v>
<v t="ekr.20230831011820.763"><vh>DefinedVariableTracker._scope</vh></v>
<v t="ekr.20230831011820.764"><vh>DefinedVariableTracker.enter_scope</vh></v>
<v t="ekr.20230831011820.765"><vh>DefinedVariableTracker.exit_scope</vh></v>
<v t="ekr.20230831011820.766"><vh>DefinedVariableTracker.in_scope</vh></v>
<v t="ekr.20230831011820.767"><vh>DefinedVariableTracker.start_branch_statement</vh></v>
<v t="ekr.20230831011820.768"><vh>DefinedVariableTracker.next_branch</vh></v>
<v t="ekr.20230831011820.769"><vh>DefinedVariableTracker.end_branch_statement</vh></v>
<v t="ekr.20230831011820.770"><vh>DefinedVariableTracker.skip_branch</vh></v>
<v t="ekr.20230831011820.771"><vh>DefinedVariableTracker.record_definition</vh></v>
<v t="ekr.20230831011820.772"><vh>DefinedVariableTracker.delete_var</vh></v>
<v t="ekr.20230831011820.773"><vh>DefinedVariableTracker.record_undefined_ref</vh></v>
<v t="ekr.20230831011820.774"><vh>DefinedVariableTracker.pop_undefined_ref</vh></v>
<v t="ekr.20230831011820.775"><vh>DefinedVariableTracker.is_possibly_undefined</vh></v>
<v t="ekr.20230831011820.776"><vh>DefinedVariableTracker.is_defined_in_different_branch</vh></v>
<v t="ekr.20230831011820.777"><vh>DefinedVariableTracker.is_undefined</vh></v>
</v>
<v t="ekr.20230831011820.778"><vh>class Loop</vh>
<v t="ekr.20230831011820.779"><vh>Loop.__init__</vh></v>
</v>
<v t="ekr.20230831011820.780"><vh>class PossiblyUndefinedVariableVisitor</vh>
<v t="ekr.20230831011820.781"><vh>PossiblyUndefinedVariableVisitor.__init__</vh></v>
<v t="ekr.20230831011820.782"><vh>PossiblyUndefinedVariableVisitor.var_used_before_def</vh></v>
<v t="ekr.20230831011820.783"><vh>PossiblyUndefinedVariableVisitor.variable_may_be_undefined</vh></v>
<v t="ekr.20230831011820.784"><vh>PossiblyUndefinedVariableVisitor.process_definition</vh></v>
<v t="ekr.20230831011820.785"><vh>PossiblyUndefinedVariableVisitor.visit_global_decl</vh></v>
<v t="ekr.20230831011820.786"><vh>PossiblyUndefinedVariableVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20230831011820.787"><vh>PossiblyUndefinedVariableVisitor.process_lvalue</vh></v>
<v t="ekr.20230831011820.788"><vh>PossiblyUndefinedVariableVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.789"><vh>PossiblyUndefinedVariableVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20230831011820.790"><vh>PossiblyUndefinedVariableVisitor.visit_if_stmt</vh></v>
<v t="ekr.20230831011820.791"><vh>PossiblyUndefinedVariableVisitor.visit_match_stmt</vh></v>
<v t="ekr.20230831011820.792"><vh>PossiblyUndefinedVariableVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011820.793"><vh>PossiblyUndefinedVariableVisitor.visit_func</vh></v>
<v t="ekr.20230831011820.794"><vh>PossiblyUndefinedVariableVisitor.visit_generator_expr</vh></v>
<v t="ekr.20230831011820.795"><vh>PossiblyUndefinedVariableVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20230831011820.796"><vh>PossiblyUndefinedVariableVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011820.797"><vh>PossiblyUndefinedVariableVisitor.visit_return_stmt</vh></v>
<v t="ekr.20230831011820.798"><vh>PossiblyUndefinedVariableVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20230831011820.799"><vh>PossiblyUndefinedVariableVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20230831011820.800"><vh>PossiblyUndefinedVariableVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20230831011820.801"><vh>PossiblyUndefinedVariableVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20230831011820.802"><vh>PossiblyUndefinedVariableVisitor.visit_break_stmt</vh></v>
<v t="ekr.20230831011820.803"><vh>PossiblyUndefinedVariableVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20230831011820.804"><vh>PossiblyUndefinedVariableVisitor.visit_try_stmt</vh></v>
<v t="ekr.20230831011820.805"><vh>PossiblyUndefinedVariableVisitor.process_try_stmt</vh></v>
<v t="ekr.20230831011820.806"><vh>PossiblyUndefinedVariableVisitor.visit_while_stmt</vh></v>
<v t="ekr.20230831011820.807"><vh>PossiblyUndefinedVariableVisitor.visit_as_pattern</vh></v>
<v t="ekr.20230831011820.808"><vh>PossiblyUndefinedVariableVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20230831011820.809"><vh>PossiblyUndefinedVariableVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011820.810"><vh>PossiblyUndefinedVariableVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011820.811"><vh>PossiblyUndefinedVariableVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011820.812"><vh>PossiblyUndefinedVariableVisitor.visit_import</vh></v>
<v t="ekr.20230831011820.813"><vh>PossiblyUndefinedVariableVisitor.visit_import_from</vh></v>
</v>
<v t="ekr.20230831011820.814"><vh>@clean patterns.py</vh>
<v t="ekr.20230831011820.815"><vh>&lt;&lt; patterns.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.816"><vh>class Pattern</vh>
<v t="ekr.20230831011820.817"><vh>Pattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.818"><vh>class AsPattern</vh>
<v t="ekr.20230831011820.819"><vh>AsPattern.__init__</vh></v>
<v t="ekr.20230831011820.820"><vh>AsPattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.821"><vh>class OrPattern</vh>
<v t="ekr.20230831011820.822"><vh>OrPattern.__init__</vh></v>
<v t="ekr.20230831011820.823"><vh>OrPattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.824"><vh>class ValuePattern</vh>
<v t="ekr.20230831011820.825"><vh>ValuePattern.__init__</vh></v>
<v t="ekr.20230831011820.826"><vh>ValuePattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.827"><vh>class SingletonPattern</vh>
<v t="ekr.20230831011820.828"><vh>SingletonPattern.__init__</vh></v>
<v t="ekr.20230831011820.829"><vh>SingletonPattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.830"><vh>class SequencePattern</vh>
<v t="ekr.20230831011820.831"><vh>SequencePattern.__init__</vh></v>
<v t="ekr.20230831011820.832"><vh>SequencePattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.833"><vh>class StarredPattern</vh>
<v t="ekr.20230831011820.834"><vh>StarredPattern.__init__</vh></v>
<v t="ekr.20230831011820.835"><vh>StarredPattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.836"><vh>class MappingPattern</vh>
<v t="ekr.20230831011820.837"><vh>MappingPattern.__init__</vh></v>
<v t="ekr.20230831011820.838"><vh>MappingPattern.accept</vh></v>
</v>
<v t="ekr.20230831011820.839"><vh>class ClassPattern</vh>
<v t="ekr.20230831011820.840"><vh>ClassPattern.__init__</vh></v>
<v t="ekr.20230831011820.841"><vh>ClassPattern.accept</vh></v>
</v>
</v>
</v>
<v t="ekr.20230831011820.930"><vh>@clean reachability.py</vh>
<v t="ekr.20230831011820.931"><vh>&lt;&lt; reachability.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.932"><vh>function: infer_reachability_of_if_statement</vh></v>
<v t="ekr.20230831011820.933"><vh>function: infer_reachability_of_match_statement</vh></v>
<v t="ekr.20230831011820.934"><vh>function: assert_will_always_fail</vh></v>
<v t="ekr.20230831011820.935"><vh>function: infer_condition_value</vh></v>
<v t="ekr.20230831011820.936"><vh>function: infer_pattern_value</vh></v>
<v t="ekr.20230831011820.937"><vh>function: consider_sys_version_info</vh></v>
<v t="ekr.20230831011820.938"><vh>function: consider_sys_platform</vh></v>
<v t="ekr.20230831011820.939"><vh>function: fixed_comparison</vh></v>
<v t="ekr.20230831011820.940"><vh>function: contains_int_or_tuple_of_ints</vh></v>
<v t="ekr.20230831011820.941"><vh>function: contains_sys_version_info</vh></v>
<v t="ekr.20230831011820.942"><vh>function: is_sys_attr</vh></v>
<v t="ekr.20230831011820.943"><vh>function: mark_block_unreachable</vh></v>
<v t="ekr.20230831011820.944"><vh>class MarkImportsUnreachableVisitor</vh>
<v t="ekr.20230831011820.945"><vh>MarkImportsUnreachableVisitor.visit_import</vh></v>
<v t="ekr.20230831011820.946"><vh>MarkImportsUnreachableVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011820.947"><vh>MarkImportsUnreachableVisitor.visit_import_all</vh></v>
</v>
<v t="ekr.20230831011820.948"><vh>function: mark_block_mypy_only</vh></v>
<v t="ekr.20230831011820.949"><vh>class MarkImportsMypyOnlyVisitor</vh>
<v t="ekr.20230831011820.950"><vh>MarkImportsMypyOnlyVisitor.visit_import</vh></v>
<v t="ekr.20230831011820.951"><vh>MarkImportsMypyOnlyVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011820.952"><vh>MarkImportsMypyOnlyVisitor.visit_import_all</vh></v>
<v t="ekr.20230831011820.953"><vh>MarkImportsMypyOnlyVisitor.visit_func_def</vh></v>
</v>
</v>
<v t="ekr.20230831011820.964"><vh>@clean renaming.py</vh>
<v t="ekr.20230831011820.965"><vh>&lt;&lt; renaming.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.966"><vh>class VariableRenameVisitor</vh>
<v t="ekr.20230831011820.967"><vh>VariableRenameVisitor.__init__</vh></v>
<v t="ekr.20230831011820.968"><vh>VariableRenameVisitor.visit_mypy_file</vh></v>
<v t="ekr.20230831011820.969"><vh>VariableRenameVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011820.970"><vh>VariableRenameVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011820.971"><vh>VariableRenameVisitor.visit_block</vh></v>
<v t="ekr.20230831011820.972"><vh>VariableRenameVisitor.visit_while_stmt</vh></v>
<v t="ekr.20230831011820.973"><vh>VariableRenameVisitor.visit_for_stmt</vh></v>
<v t="ekr.20230831011820.974"><vh>VariableRenameVisitor.visit_break_stmt</vh></v>
<v t="ekr.20230831011820.975"><vh>VariableRenameVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20230831011820.976"><vh>VariableRenameVisitor.visit_try_stmt</vh></v>
<v t="ekr.20230831011820.977"><vh>VariableRenameVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011820.978"><vh>VariableRenameVisitor.visit_import</vh></v>
<v t="ekr.20230831011820.979"><vh>VariableRenameVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011820.980"><vh>VariableRenameVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20230831011820.981"><vh>VariableRenameVisitor.visit_match_stmt</vh></v>
<v t="ekr.20230831011820.982"><vh>VariableRenameVisitor.visit_capture_pattern</vh></v>
<v t="ekr.20230831011820.983"><vh>VariableRenameVisitor.analyze_lvalue</vh></v>
<v t="ekr.20230831011820.984"><vh>VariableRenameVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011820.985"><vh>VariableRenameVisitor.handle_arg</vh></v>
<v t="ekr.20230831011820.986"><vh>VariableRenameVisitor.handle_def</vh></v>
<v t="ekr.20230831011820.987"><vh>VariableRenameVisitor.handle_refine</vh></v>
<v t="ekr.20230831011820.988"><vh>VariableRenameVisitor.handle_ref</vh></v>
<v t="ekr.20230831011820.989"><vh>VariableRenameVisitor.flush_refs</vh></v>
<v t="ekr.20230831011820.990"><vh>VariableRenameVisitor.clear</vh></v>
<v t="ekr.20230831011820.991"><vh>VariableRenameVisitor.enter_block</vh></v>
<v t="ekr.20230831011820.992"><vh>VariableRenameVisitor.enter_try</vh></v>
<v t="ekr.20230831011820.993"><vh>VariableRenameVisitor.enter_loop</vh></v>
<v t="ekr.20230831011820.994"><vh>VariableRenameVisitor.current_block</vh></v>
<v t="ekr.20230831011820.995"><vh>VariableRenameVisitor.enter_scope</vh></v>
<v t="ekr.20230831011820.996"><vh>VariableRenameVisitor.is_nested</vh></v>
<v t="ekr.20230831011820.997"><vh>VariableRenameVisitor.reject_redefinition_of_vars_in_scope</vh></v>
<v t="ekr.20230831011820.998"><vh>VariableRenameVisitor.reject_redefinition_of_vars_in_loop</vh></v>
<v t="ekr.20230831011820.999"><vh>VariableRenameVisitor.record_assignment</vh></v>
</v>
<v t="ekr.20230831011820.1000"><vh>class LimitedVariableRenameVisitor</vh>
<v t="ekr.20230831011820.1001"><vh>LimitedVariableRenameVisitor.__init__</vh></v>
<v t="ekr.20230831011820.1002"><vh>LimitedVariableRenameVisitor.visit_mypy_file</vh></v>
<v t="ekr.20230831011820.1003"><vh>LimitedVariableRenameVisitor.visit_func_def</vh></v>
<v t="ekr.20230831011820.1004"><vh>LimitedVariableRenameVisitor.visit_class_def</vh></v>
<v t="ekr.20230831011820.1005"><vh>LimitedVariableRenameVisitor.visit_with_stmt</vh></v>
<v t="ekr.20230831011820.1006"><vh>LimitedVariableRenameVisitor.analyze_lvalue</vh></v>
<v t="ekr.20230831011820.1007"><vh>LimitedVariableRenameVisitor.visit_import</vh></v>
<v t="ekr.20230831011820.1008"><vh>LimitedVariableRenameVisitor.visit_import_from</vh></v>
<v t="ekr.20230831011820.1009"><vh>LimitedVariableRenameVisitor.visit_import_all</vh></v>
<v t="ekr.20230831011820.1010"><vh>LimitedVariableRenameVisitor.visit_name_expr</vh></v>
<v t="ekr.20230831011820.1011"><vh>LimitedVariableRenameVisitor.enter_scope</vh></v>
<v t="ekr.20230831011820.1012"><vh>LimitedVariableRenameVisitor.reject_redefinition_of_vars_in_scope</vh></v>
<v t="ekr.20230831011820.1013"><vh>LimitedVariableRenameVisitor.record_skipped</vh></v>
<v t="ekr.20230831011820.1014"><vh>LimitedVariableRenameVisitor.flush_refs</vh></v>
</v>
<v t="ekr.20230831011820.1015"><vh>function: rename_refs</vh></v>
</v>
<v t="ekr.20230831011820.1088"><vh>@clean scope.py</vh>
<v t="ekr.20230831011820.1090"><vh>&lt;&lt; scope.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1091"><vh>class Scope</vh>
<v t="ekr.20230831011820.1092"><vh>Scope.__init__</vh></v>
<v t="ekr.20230831011820.1093"><vh>Scope.current_module_id</vh></v>
<v t="ekr.20230831011820.1094"><vh>Scope.current_target</vh></v>
<v t="ekr.20230831011820.1095"><vh>Scope.current_full_target</vh></v>
<v t="ekr.20230831011820.1096"><vh>Scope.current_type_name</vh></v>
<v t="ekr.20230831011820.1097"><vh>Scope.current_function_name</vh></v>
<v t="ekr.20230831011820.1098"><vh>Scope.module_scope</vh></v>
<v t="ekr.20230831011820.1099"><vh>Scope.function_scope</vh></v>
<v t="ekr.20230831011820.1100"><vh>Scope.outer_functions</vh></v>
<v t="ekr.20230831011820.1101"><vh>Scope.enter_class</vh></v>
<v t="ekr.20230831011820.1102"><vh>Scope.leave_class</vh></v>
<v t="ekr.20230831011820.1103"><vh>Scope.class_scope</vh></v>
<v t="ekr.20230831011820.1104"><vh>Scope.save</vh></v>
<v t="ekr.20230831011820.1105"><vh>Scope.saved_scope</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1585"><vh>@clean split_namespace.py</vh>
<v t="ekr.20230831011820.1587"><vh>&lt;&lt; split_namespace.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1588"><vh>class SplitNamespace</vh>
<v t="ekr.20230831011820.1589"><vh>SplitNamespace.__init__</vh></v>
<v t="ekr.20230831011820.1590"><vh>SplitNamespace._get</vh></v>
<v t="ekr.20230831011820.1591"><vh>SplitNamespace.__setattr__</vh></v>
<v t="ekr.20230831011820.1592"><vh>SplitNamespace.__getattr__</vh></v>
</v>
</v>
<v t="ekr.20230831011820.1593"><vh>@clean state.py</vh>
<v t="ekr.20230831011820.1594"><vh>&lt;&lt; state.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.1595"><vh>class StrictOptionalState</vh>
<v t="ekr.20230831011820.1596"><vh>StrictOptionalState.__init__</vh></v>
<v t="ekr.20230831011820.1597"><vh>StrictOptionalState.strict_optional_set</vh></v>
</v>
</v>
<v t="ekr.20230831011820.2006"><vh>@clean subtypes.py</vh>
<v t="ekr.20230831011820.2007"><vh>&lt;&lt; subtypes.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011820.2008"><vh>class SubtypeContext</vh>
<v t="ekr.20230831011820.2009"><vh>SubtypeContext.__init__</vh></v>
<v t="ekr.20230831011820.2010"><vh>SubtypeContext.check_context</vh></v>
</v>
<v t="ekr.20230831011820.2011"><vh>function: is_subtype</vh></v>
<v t="ekr.20230831011820.2012"><vh>function: is_proper_subtype</vh></v>
<v t="ekr.20230831011820.2013"><vh>function: is_equivalent</vh></v>
<v t="ekr.20230831011820.2014"><vh>function: is_same_type</vh></v>
<v t="ekr.20230831011820.2015"><vh>function: _is_subtype</vh></v>
<v t="ekr.20230831011820.2016"><vh>function: check_type_parameter</vh></v>
<v t="ekr.20230831011820.2017"><vh>class SubtypeVisitor</vh>
<v t="ekr.20230831011820.2018"><vh>SubtypeVisitor.__init__</vh></v>
<v t="ekr.20230831011820.2019"><vh>SubtypeVisitor.build_subtype_kind</vh></v>
<v t="ekr.20230831011820.2020"><vh>SubtypeVisitor._is_subtype</vh></v>
<v t="ekr.20230831011820.2021"><vh>SubtypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011820.2022"><vh>SubtypeVisitor.visit_any</vh></v>
<v t="ekr.20230831011820.2023"><vh>SubtypeVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011820.2024"><vh>SubtypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011820.2025"><vh>SubtypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011820.2026"><vh>SubtypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011820.2027"><vh>SubtypeVisitor.visit_instance</vh></v>
<v t="ekr.20230831011820.2028"><vh>SubtypeVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011820.2029"><vh>SubtypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011820.2030"><vh>SubtypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011820.2031"><vh>SubtypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011820.2032"><vh>SubtypeVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011820.2033"><vh>SubtypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011820.2034"><vh>SubtypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011820.2035"><vh>SubtypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011820.2036"><vh>SubtypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011820.2037"><vh>SubtypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011820.2038"><vh>SubtypeVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011820.2039"><vh>SubtypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011820.2040"><vh>SubtypeVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011820.2041"><vh>SubtypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011820.2042"><vh>function: pop_on_exit</vh></v>
<v t="ekr.20230831011820.2043"><vh>function: is_protocol_implementation</vh></v>
<v t="ekr.20230831011820.2044"><vh>function: find_member</vh></v>
<v t="ekr.20230831011820.2045"><vh>function: get_member_flags</vh></v>
<v t="ekr.20230831011820.2046"><vh>function: find_node_type</vh></v>
<v t="ekr.20230831011820.2047"><vh>function: non_method_protocol_members</vh></v>
<v t="ekr.20230831011820.2048"><vh>function: is_callable_compatible</vh></v>
<v t="ekr.20230831011820.2049"><vh>function: are_trivial_parameters</vh></v>
<v t="ekr.20230831011820.2050"><vh>function: are_parameters_compatible</vh></v>
<v t="ekr.20230831011820.2051"><vh>function: are_args_compatible</vh></v>
<v t="ekr.20230831011820.2052"><vh>function: flip_compat_check</vh></v>
<v t="ekr.20230831011820.2053"><vh>function: unify_generic_callable</vh></v>
<v t="ekr.20230831011820.2054"><vh>function: try_restrict_literal_union</vh></v>
<v t="ekr.20230831011820.2055"><vh>function: restrict_subtype_away</vh></v>
<v t="ekr.20230831011820.2056"><vh>function: covers_at_runtime</vh></v>
<v t="ekr.20230831011820.2057"><vh>function: is_more_precise</vh></v>
</v>
<v t="ekr.20230831011821.284"><vh>@clean tvar_scope.py</vh>
<v t="ekr.20230831011821.285"><vh>&lt;&lt; tvar_scope.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.286"><vh>class TypeVarLikeScope</vh>
<v t="ekr.20230831011821.287"><vh>TypeVarLikeScope.__init__</vh></v>
<v t="ekr.20230831011821.288"><vh>TypeVarLikeScope.get_function_scope</vh></v>
<v t="ekr.20230831011821.289"><vh>TypeVarLikeScope.allow_binding</vh></v>
<v t="ekr.20230831011821.290"><vh>TypeVarLikeScope.method_frame</vh></v>
<v t="ekr.20230831011821.291"><vh>TypeVarLikeScope.class_frame</vh></v>
<v t="ekr.20230831011821.292"><vh>TypeVarLikeScope.new_unique_func_id</vh></v>
<v t="ekr.20230831011821.293"><vh>TypeVarLikeScope.bind_new</vh></v>
<v t="ekr.20230831011821.294"><vh>TypeVarLikeScope.bind_existing</vh></v>
<v t="ekr.20230831011821.295"><vh>TypeVarLikeScope.get_binding</vh></v>
<v t="ekr.20230831011821.296"><vh>TypeVarLikeScope.__str__</vh></v>
</v>
</v>
<v t="ekr.20230831011821.297"><vh>@clean type_visitor.py</vh>
<v t="ekr.20230831011821.299"><vh>&lt;&lt; type_visitor.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011821.300"><vh>class TypeVisitor</vh>
<v t="ekr.20230831011821.301"><vh>TypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.302"><vh>TypeVisitor.visit_any</vh></v>
<v t="ekr.20230831011821.303"><vh>TypeVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011821.304"><vh>TypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011821.305"><vh>TypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011821.306"><vh>TypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011821.307"><vh>TypeVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011821.308"><vh>TypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011821.309"><vh>TypeVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011821.310"><vh>TypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.311"><vh>TypeVisitor.visit_instance</vh></v>
<v t="ekr.20230831011821.312"><vh>TypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011821.313"><vh>TypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011821.314"><vh>TypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011821.315"><vh>TypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011821.316"><vh>TypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011821.317"><vh>TypeVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011821.318"><vh>TypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011821.319"><vh>TypeVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011821.320"><vh>TypeVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20230831011821.321"><vh>TypeVisitor.visit_unpack_type</vh></v>
</v>
<v t="ekr.20230831011821.322"><vh>class SyntheticTypeVisitor</vh>
<v t="ekr.20230831011821.323"><vh>SyntheticTypeVisitor.visit_type_list</vh></v>
<v t="ekr.20230831011821.324"><vh>SyntheticTypeVisitor.visit_callable_argument</vh></v>
<v t="ekr.20230831011821.325"><vh>SyntheticTypeVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20230831011821.326"><vh>SyntheticTypeVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20230831011821.327"><vh>SyntheticTypeVisitor.visit_placeholder_type</vh></v>
</v>
<v t="ekr.20230831011821.328"><vh>class TypeTranslator</vh>
<v t="ekr.20230831011821.329"><vh>TypeTranslator.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.330"><vh>TypeTranslator.visit_any</vh></v>
<v t="ekr.20230831011821.331"><vh>TypeTranslator.visit_none_type</vh></v>
<v t="ekr.20230831011821.332"><vh>TypeTranslator.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011821.333"><vh>TypeTranslator.visit_erased_type</vh></v>
<v t="ekr.20230831011821.334"><vh>TypeTranslator.visit_deleted_type</vh></v>
<v t="ekr.20230831011821.335"><vh>TypeTranslator.visit_instance</vh></v>
<v t="ekr.20230831011821.336"><vh>TypeTranslator.visit_type_var</vh></v>
<v t="ekr.20230831011821.337"><vh>TypeTranslator.visit_param_spec</vh></v>
<v t="ekr.20230831011821.338"><vh>TypeTranslator.visit_parameters</vh></v>
<v t="ekr.20230831011821.339"><vh>TypeTranslator.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.340"><vh>TypeTranslator.visit_partial_type</vh></v>
<v t="ekr.20230831011821.341"><vh>TypeTranslator.visit_unpack_type</vh></v>
<v t="ekr.20230831011821.342"><vh>TypeTranslator.visit_callable_type</vh></v>
<v t="ekr.20230831011821.343"><vh>TypeTranslator.visit_tuple_type</vh></v>
<v t="ekr.20230831011821.344"><vh>TypeTranslator.visit_typeddict_type</vh></v>
<v t="ekr.20230831011821.345"><vh>TypeTranslator.visit_literal_type</vh></v>
<v t="ekr.20230831011821.346"><vh>TypeTranslator.visit_union_type</vh></v>
<v t="ekr.20230831011821.347"><vh>TypeTranslator.translate_types</vh></v>
<v t="ekr.20230831011821.348"><vh>TypeTranslator.translate_variables</vh></v>
<v t="ekr.20230831011821.349"><vh>TypeTranslator.visit_overloaded</vh></v>
<v t="ekr.20230831011821.350"><vh>TypeTranslator.visit_type_type</vh></v>
<v t="ekr.20230831011821.351"><vh>TypeTranslator.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011821.352"><vh>class TypeQuery</vh>
<v t="ekr.20230831011821.353"><vh>TypeQuery.__init__</vh></v>
<v t="ekr.20230831011821.354"><vh>TypeQuery.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.355"><vh>TypeQuery.visit_type_list</vh></v>
<v t="ekr.20230831011821.356"><vh>TypeQuery.visit_callable_argument</vh></v>
<v t="ekr.20230831011821.357"><vh>TypeQuery.visit_any</vh></v>
<v t="ekr.20230831011821.358"><vh>TypeQuery.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011821.359"><vh>TypeQuery.visit_none_type</vh></v>
<v t="ekr.20230831011821.360"><vh>TypeQuery.visit_erased_type</vh></v>
<v t="ekr.20230831011821.361"><vh>TypeQuery.visit_deleted_type</vh></v>
<v t="ekr.20230831011821.362"><vh>TypeQuery.visit_type_var</vh></v>
<v t="ekr.20230831011821.363"><vh>TypeQuery.visit_param_spec</vh></v>
<v t="ekr.20230831011821.364"><vh>TypeQuery.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.365"><vh>TypeQuery.visit_unpack_type</vh></v>
<v t="ekr.20230831011821.366"><vh>TypeQuery.visit_parameters</vh></v>
<v t="ekr.20230831011821.367"><vh>TypeQuery.visit_partial_type</vh></v>
<v t="ekr.20230831011821.368"><vh>TypeQuery.visit_instance</vh></v>
<v t="ekr.20230831011821.369"><vh>TypeQuery.visit_callable_type</vh></v>
<v t="ekr.20230831011821.370"><vh>TypeQuery.visit_tuple_type</vh></v>
<v t="ekr.20230831011821.371"><vh>TypeQuery.visit_typeddict_type</vh></v>
<v t="ekr.20230831011821.372"><vh>TypeQuery.visit_raw_expression_type</vh></v>
<v t="ekr.20230831011821.373"><vh>TypeQuery.visit_literal_type</vh></v>
<v t="ekr.20230831011821.374"><vh>TypeQuery.visit_union_type</vh></v>
<v t="ekr.20230831011821.375"><vh>TypeQuery.visit_overloaded</vh></v>
<v t="ekr.20230831011821.376"><vh>TypeQuery.visit_type_type</vh></v>
<v t="ekr.20230831011821.377"><vh>TypeQuery.visit_ellipsis_type</vh></v>
<v t="ekr.20230831011821.378"><vh>TypeQuery.visit_placeholder_type</vh></v>
<v t="ekr.20230831011821.379"><vh>TypeQuery.visit_type_alias_type</vh></v>
<v t="ekr.20230831011821.380"><vh>TypeQuery.query_types</vh></v>
</v>
<v t="ekr.20230831011821.381"><vh>class BoolTypeQuery</vh>
<v t="ekr.20230831011821.382"><vh>BoolTypeQuery.__init__</vh></v>
<v t="ekr.20230831011821.383"><vh>BoolTypeQuery.reset</vh></v>
<v t="ekr.20230831011821.384"><vh>BoolTypeQuery.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.385"><vh>BoolTypeQuery.visit_type_list</vh></v>
<v t="ekr.20230831011821.386"><vh>BoolTypeQuery.visit_callable_argument</vh></v>
<v t="ekr.20230831011821.387"><vh>BoolTypeQuery.visit_any</vh></v>
<v t="ekr.20230831011821.388"><vh>BoolTypeQuery.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011821.389"><vh>BoolTypeQuery.visit_none_type</vh></v>
<v t="ekr.20230831011821.390"><vh>BoolTypeQuery.visit_erased_type</vh></v>
<v t="ekr.20230831011821.391"><vh>BoolTypeQuery.visit_deleted_type</vh></v>
<v t="ekr.20230831011821.392"><vh>BoolTypeQuery.visit_type_var</vh></v>
<v t="ekr.20230831011821.393"><vh>BoolTypeQuery.visit_param_spec</vh></v>
<v t="ekr.20230831011821.394"><vh>BoolTypeQuery.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.395"><vh>BoolTypeQuery.visit_unpack_type</vh></v>
<v t="ekr.20230831011821.396"><vh>BoolTypeQuery.visit_parameters</vh></v>
<v t="ekr.20230831011821.397"><vh>BoolTypeQuery.visit_partial_type</vh></v>
<v t="ekr.20230831011821.398"><vh>BoolTypeQuery.visit_instance</vh></v>
<v t="ekr.20230831011821.399"><vh>BoolTypeQuery.visit_callable_type</vh></v>
<v t="ekr.20230831011821.400"><vh>BoolTypeQuery.visit_tuple_type</vh></v>
<v t="ekr.20230831011821.401"><vh>BoolTypeQuery.visit_typeddict_type</vh></v>
<v t="ekr.20230831011821.402"><vh>BoolTypeQuery.visit_raw_expression_type</vh></v>
<v t="ekr.20230831011821.403"><vh>BoolTypeQuery.visit_literal_type</vh></v>
<v t="ekr.20230831011821.404"><vh>BoolTypeQuery.visit_union_type</vh></v>
<v t="ekr.20230831011821.405"><vh>BoolTypeQuery.visit_overloaded</vh></v>
<v t="ekr.20230831011821.406"><vh>BoolTypeQuery.visit_type_type</vh></v>
<v t="ekr.20230831011821.407"><vh>BoolTypeQuery.visit_ellipsis_type</vh></v>
<v t="ekr.20230831011821.408"><vh>BoolTypeQuery.visit_placeholder_type</vh></v>
<v t="ekr.20230831011821.409"><vh>BoolTypeQuery.visit_type_alias_type</vh></v>
<v t="ekr.20230831011821.410"><vh>BoolTypeQuery.query_types</vh></v>
</v>
</v>
<v t="ekr.20230831011821.411"><vh>@clean typeanal.py</vh>
<v t="ekr.20230831011821.412"><vh>&lt;&lt; typeanal.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.413"><vh>function: analyze_type_alias</vh></v>
<v t="ekr.20230831011821.414"><vh>function: no_subscript_builtin_alias</vh></v>
<v t="ekr.20230831011821.415"><vh>class TypeAnalyser</vh>
<v t="ekr.20230831011821.416"><vh>TypeAnalyser.__init__</vh></v>
<v t="ekr.20230831011821.417"><vh>TypeAnalyser.lookup_qualified</vh></v>
<v t="ekr.20230831011821.418"><vh>TypeAnalyser.lookup_fully_qualified</vh></v>
<v t="ekr.20230831011821.419"><vh>TypeAnalyser.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.420"><vh>TypeAnalyser.visit_unbound_type_nonoptional</vh></v>
<v t="ekr.20230831011821.421"><vh>TypeAnalyser.pack_paramspec_args</vh></v>
<v t="ekr.20230831011821.422"><vh>TypeAnalyser.cannot_resolve_type</vh></v>
<v t="ekr.20230831011821.423"><vh>TypeAnalyser.apply_concatenate_operator</vh></v>
<v t="ekr.20230831011821.424"><vh>TypeAnalyser.try_analyze_special_unbound_type</vh></v>
<v t="ekr.20230831011821.425"><vh>TypeAnalyser.get_omitted_any</vh></v>
<v t="ekr.20230831011821.426"><vh>TypeAnalyser.analyze_type_with_type_info</vh></v>
<v t="ekr.20230831011821.427"><vh>TypeAnalyser.analyze_unbound_type_without_type_info</vh></v>
<v t="ekr.20230831011821.428"><vh>TypeAnalyser.visit_any</vh></v>
<v t="ekr.20230831011821.429"><vh>TypeAnalyser.visit_none_type</vh></v>
<v t="ekr.20230831011821.430"><vh>TypeAnalyser.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011821.431"><vh>TypeAnalyser.visit_erased_type</vh></v>
<v t="ekr.20230831011821.432"><vh>TypeAnalyser.visit_deleted_type</vh></v>
<v t="ekr.20230831011821.433"><vh>TypeAnalyser.visit_type_list</vh></v>
<v t="ekr.20230831011821.434"><vh>TypeAnalyser.visit_callable_argument</vh></v>
<v t="ekr.20230831011821.435"><vh>TypeAnalyser.visit_instance</vh></v>
<v t="ekr.20230831011821.436"><vh>TypeAnalyser.visit_type_alias_type</vh></v>
<v t="ekr.20230831011821.437"><vh>TypeAnalyser.visit_type_var</vh></v>
<v t="ekr.20230831011821.438"><vh>TypeAnalyser.visit_param_spec</vh></v>
<v t="ekr.20230831011821.439"><vh>TypeAnalyser.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.440"><vh>TypeAnalyser.visit_unpack_type</vh></v>
<v t="ekr.20230831011821.441"><vh>TypeAnalyser.visit_parameters</vh></v>
<v t="ekr.20230831011821.442"><vh>TypeAnalyser.visit_callable_type</vh></v>
<v t="ekr.20230831011821.443"><vh>TypeAnalyser.anal_type_guard</vh></v>
<v t="ekr.20230831011821.444"><vh>TypeAnalyser.anal_type_guard_arg</vh></v>
<v t="ekr.20230831011821.445"><vh>TypeAnalyser.anal_star_arg_type</vh></v>
<v t="ekr.20230831011821.446"><vh>TypeAnalyser.visit_overloaded</vh></v>
<v t="ekr.20230831011821.447"><vh>TypeAnalyser.visit_tuple_type</vh></v>
<v t="ekr.20230831011821.448"><vh>TypeAnalyser.visit_typeddict_type</vh></v>
<v t="ekr.20230831011821.449"><vh>TypeAnalyser.visit_raw_expression_type</vh></v>
<v t="ekr.20230831011821.450"><vh>TypeAnalyser.visit_literal_type</vh></v>
<v t="ekr.20230831011821.451"><vh>TypeAnalyser.visit_union_type</vh></v>
<v t="ekr.20230831011821.452"><vh>TypeAnalyser.visit_partial_type</vh></v>
<v t="ekr.20230831011821.453"><vh>TypeAnalyser.visit_ellipsis_type</vh></v>
<v t="ekr.20230831011821.454"><vh>TypeAnalyser.visit_type_type</vh></v>
<v t="ekr.20230831011821.455"><vh>TypeAnalyser.visit_placeholder_type</vh></v>
<v t="ekr.20230831011821.456"><vh>TypeAnalyser.analyze_callable_args_for_paramspec</vh></v>
<v t="ekr.20230831011821.457"><vh>TypeAnalyser.analyze_callable_args_for_concatenate</vh></v>
<v t="ekr.20230831011821.458"><vh>TypeAnalyser.analyze_callable_type</vh></v>
<v t="ekr.20230831011821.459"><vh>TypeAnalyser.analyze_callable_args</vh></v>
<v t="ekr.20230831011821.460"><vh>TypeAnalyser.analyze_literal_type</vh></v>
<v t="ekr.20230831011821.461"><vh>TypeAnalyser.analyze_literal_param</vh></v>
<v t="ekr.20230831011821.462"><vh>TypeAnalyser.analyze_type</vh></v>
<v t="ekr.20230831011821.463"><vh>TypeAnalyser.fail</vh></v>
<v t="ekr.20230831011821.464"><vh>TypeAnalyser.note</vh></v>
<v t="ekr.20230831011821.465"><vh>TypeAnalyser.tvar_scope_frame</vh></v>
<v t="ekr.20230831011821.466"><vh>TypeAnalyser.find_type_var_likes</vh></v>
<v t="ekr.20230831011821.467"><vh>TypeAnalyser.infer_type_variables</vh></v>
<v t="ekr.20230831011821.468"><vh>TypeAnalyser.bind_function_type_variables</vh></v>
<v t="ekr.20230831011821.469"><vh>TypeAnalyser.is_defined_type_var</vh></v>
<v t="ekr.20230831011821.470"><vh>TypeAnalyser.anal_array</vh></v>
<v t="ekr.20230831011821.471"><vh>TypeAnalyser.anal_type</vh></v>
<v t="ekr.20230831011821.472"><vh>TypeAnalyser.anal_var_def</vh></v>
<v t="ekr.20230831011821.473"><vh>TypeAnalyser.anal_var_defs</vh></v>
<v t="ekr.20230831011821.474"><vh>TypeAnalyser.named_type</vh></v>
<v t="ekr.20230831011821.475"><vh>TypeAnalyser.check_unpacks_in_list</vh></v>
<v t="ekr.20230831011821.476"><vh>TypeAnalyser.tuple_type</vh></v>
</v>
<v t="ekr.20230831011821.477"><vh>class MsgCallback</vh>
<v t="ekr.20230831011821.478"><vh>MsgCallback.__call__</vh></v>
</v>
<v t="ekr.20230831011821.479"><vh>function: get_omitted_any</vh></v>
<v t="ekr.20230831011821.480"><vh>function: fix_type_var_tuple_argument</vh></v>
<v t="ekr.20230831011821.481"><vh>function: fix_instance</vh></v>
<v t="ekr.20230831011821.482"><vh>function: instantiate_type_alias</vh></v>
<v t="ekr.20230831011821.483"><vh>function: set_any_tvars</vh></v>
<v t="ekr.20230831011821.484"><vh>function: flatten_tvars</vh></v>
<v t="ekr.20230831011821.485"><vh>class TypeVarLikeQuery</vh>
<v t="ekr.20230831011821.486"><vh>TypeVarLikeQuery.__init__</vh></v>
<v t="ekr.20230831011821.487"><vh>TypeVarLikeQuery._seems_like_callable</vh></v>
<v t="ekr.20230831011821.488"><vh>TypeVarLikeQuery.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.489"><vh>TypeVarLikeQuery.visit_callable_type</vh></v>
</v>
<v t="ekr.20230831011821.490"><vh>class DivergingAliasDetector</vh>
<v t="ekr.20230831011821.491"><vh>DivergingAliasDetector.__init__</vh></v>
<v t="ekr.20230831011821.492"><vh>DivergingAliasDetector.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011821.493"><vh>function: detect_diverging_alias</vh></v>
<v t="ekr.20230831011821.494"><vh>function: check_for_explicit_any</vh></v>
<v t="ekr.20230831011821.495"><vh>function: has_explicit_any</vh></v>
<v t="ekr.20230831011821.496"><vh>class HasExplicitAny</vh>
<v t="ekr.20230831011821.497"><vh>HasExplicitAny.__init__</vh></v>
<v t="ekr.20230831011821.498"><vh>HasExplicitAny.visit_any</vh></v>
<v t="ekr.20230831011821.499"><vh>HasExplicitAny.visit_typeddict_type</vh></v>
</v>
<v t="ekr.20230831011821.500"><vh>function: has_any_from_unimported_type</vh></v>
<v t="ekr.20230831011821.501"><vh>class HasAnyFromUnimportedType</vh>
<v t="ekr.20230831011821.502"><vh>HasAnyFromUnimportedType.__init__</vh></v>
<v t="ekr.20230831011821.503"><vh>HasAnyFromUnimportedType.visit_any</vh></v>
<v t="ekr.20230831011821.504"><vh>HasAnyFromUnimportedType.visit_typeddict_type</vh></v>
</v>
<v t="ekr.20230831011821.505"><vh>function: collect_all_inner_types</vh></v>
<v t="ekr.20230831011821.506"><vh>class CollectAllInnerTypesQuery</vh>
<v t="ekr.20230831011821.507"><vh>CollectAllInnerTypesQuery.__init__</vh></v>
<v t="ekr.20230831011821.508"><vh>CollectAllInnerTypesQuery.query_types</vh></v>
<v t="ekr.20230831011821.509"><vh>CollectAllInnerTypesQuery.combine_lists_strategy</vh></v>
</v>
<v t="ekr.20230831011821.510"><vh>function: make_optional_type</vh></v>
<v t="ekr.20230831011821.511"><vh>function: fix_instance_types</vh></v>
<v t="ekr.20230831011821.512"><vh>class InstanceFixer</vh>
<v t="ekr.20230831011821.513"><vh>InstanceFixer.__init__</vh></v>
<v t="ekr.20230831011821.514"><vh>InstanceFixer.visit_instance</vh></v>
</v>
<v t="ekr.20230831011821.515"><vh>function: find_self_type</vh></v>
<v t="ekr.20230831011821.516"><vh>class HasSelfType</vh>
<v t="ekr.20230831011821.517"><vh>HasSelfType.__init__</vh></v>
<v t="ekr.20230831011821.518"><vh>HasSelfType.visit_unbound_type</vh></v>
</v>
</v>
<v t="ekr.20230831011821.519"><vh>@clean typeops.py</vh>
<v t="ekr.20230831011821.521"><vh>&lt;&lt; typeops.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011821.522"><vh>function: is_recursive_pair</vh></v>
<v t="ekr.20230831011821.523"><vh>function: tuple_fallback</vh></v>
<v t="ekr.20230831011821.524"><vh>function: get_self_type</vh></v>
<v t="ekr.20230831011821.525"><vh>function: type_object_type_from_function</vh></v>
<v t="ekr.20230831011821.526"><vh>function: class_callable</vh></v>
<v t="ekr.20230831011821.527"><vh>function: map_type_from_supertype</vh></v>
<v t="ekr.20230831011821.528"><vh>function: supported_self_type</vh></v>
<v t="ekr.20230831011821.529"><vh>function: bind_self</vh></v>
<v t="ekr.20230831011821.530"><vh>function: erase_to_bound</vh></v>
<v t="ekr.20230831011821.531"><vh>function: callable_corresponding_argument</vh></v>
<v t="ekr.20230831011821.532"><vh>function: simple_literal_type</vh></v>
<v t="ekr.20230831011821.533"><vh>function: is_simple_literal</vh></v>
<v t="ekr.20230831011821.534"><vh>function: make_simplified_union</vh></v>
<v t="ekr.20230831011821.535"><vh>function: _remove_redundant_union_items</vh></v>
<v t="ekr.20230831011821.536"><vh>function: _get_type_special_method_bool_ret_type</vh></v>
<v t="ekr.20230831011821.537"><vh>function: true_only</vh></v>
<v t="ekr.20230831011821.538"><vh>function: false_only</vh></v>
<v t="ekr.20230831011821.539"><vh>function: true_or_false</vh></v>
<v t="ekr.20230831011821.540"><vh>function: erase_def_to_union_or_bound</vh></v>
<v t="ekr.20230831011821.541"><vh>function: erase_to_union_or_bound</vh></v>
<v t="ekr.20230831011821.542"><vh>function: function_type</vh></v>
<v t="ekr.20230831011821.543"><vh>function: callable_type</vh></v>
<v t="ekr.20230831011821.544"><vh>function: try_getting_str_literals</vh></v>
<v t="ekr.20230831011821.545"><vh>function: try_getting_str_literals_from_type</vh></v>
<v t="ekr.20230831011821.546"><vh>function: try_getting_int_literals_from_type</vh></v>
<v t="ekr.20230831011821.547"><vh>function: try_getting_literals_from_type</vh></v>
<v t="ekr.20230831011821.548"><vh>function: is_literal_type_like</vh></v>
<v t="ekr.20230831011821.549"><vh>function: is_singleton_type</vh></v>
<v t="ekr.20230831011821.550"><vh>function: try_expanding_sum_type_to_union</vh></v>
<v t="ekr.20230831011821.551"><vh>function: try_contracting_literals_in_union</vh></v>
<v t="ekr.20230831011821.552"><vh>function: coerce_to_literal</vh></v>
<v t="ekr.20230831011821.553"><vh>function: get_type_vars</vh></v>
<v t="ekr.20230831011821.554"><vh>function: get_all_type_vars</vh></v>
<v t="ekr.20230831011821.555"><vh>class TypeVarExtractor</vh>
<v t="ekr.20230831011821.556"><vh>TypeVarExtractor.__init__</vh></v>
<v t="ekr.20230831011821.557"><vh>TypeVarExtractor._merge</vh></v>
<v t="ekr.20230831011821.558"><vh>TypeVarExtractor.visit_type_var</vh></v>
<v t="ekr.20230831011821.559"><vh>TypeVarExtractor.visit_param_spec</vh></v>
<v t="ekr.20230831011821.560"><vh>TypeVarExtractor.visit_type_var_tuple</vh></v>
</v>
<v t="ekr.20230831011821.561"><vh>function: custom_special_method</vh></v>
<v t="ekr.20230831011821.562"><vh>function: separate_union_literals</vh></v>
<v t="ekr.20230831011821.563"><vh>function: try_getting_instance_fallback</vh></v>
<v t="ekr.20230831011821.564"><vh>function: fixup_partial_type</vh></v>
<v t="ekr.20230831011821.565"><vh>function: get_protocol_member</vh></v>
</v>
<v t="ekr.20230831011821.566"><vh>@clean types.py</vh>
<v t="ekr.20230831011821.567"><vh>&lt;&lt; types.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.568"><vh>class TypeOfAny</vh></v>
<v t="ekr.20230831011821.569"><vh>function: deserialize_type</vh></v>
<v t="ekr.20230831011821.570"><vh>class Type</vh>
<v t="ekr.20230831011821.571"><vh>Type.__init__</vh></v>
<v t="ekr.20230831011821.572"><vh>Type.can_be_true</vh></v>
<v t="ekr.20230831011821.573"><vh>Type.can_be_true</vh></v>
<v t="ekr.20230831011821.574"><vh>Type.can_be_false</vh></v>
<v t="ekr.20230831011821.575"><vh>Type.can_be_false</vh></v>
<v t="ekr.20230831011821.576"><vh>Type.can_be_true_default</vh></v>
<v t="ekr.20230831011821.577"><vh>Type.can_be_false_default</vh></v>
<v t="ekr.20230831011821.578"><vh>Type.accept</vh></v>
<v t="ekr.20230831011821.579"><vh>Type.__repr__</vh></v>
<v t="ekr.20230831011821.580"><vh>Type.str_with_options</vh></v>
<v t="ekr.20230831011821.581"><vh>Type.serialize</vh></v>
<v t="ekr.20230831011821.582"><vh>Type.deserialize</vh></v>
<v t="ekr.20230831011821.583"><vh>Type.is_singleton_type</vh></v>
</v>
<v t="ekr.20230831011821.584"><vh>class TypeAliasType</vh>
<v t="ekr.20230831011821.585"><vh>TypeAliasType.__init__</vh></v>
<v t="ekr.20230831011821.586"><vh>TypeAliasType._expand_once</vh></v>
<v t="ekr.20230831011821.587"><vh>TypeAliasType._partial_expansion</vh></v>
<v t="ekr.20230831011821.588"><vh>TypeAliasType.expand_all_if_possible</vh></v>
<v t="ekr.20230831011821.589"><vh>TypeAliasType.is_recursive</vh></v>
<v t="ekr.20230831011821.590"><vh>TypeAliasType.can_be_true_default</vh></v>
<v t="ekr.20230831011821.591"><vh>TypeAliasType.can_be_false_default</vh></v>
<v t="ekr.20230831011821.592"><vh>TypeAliasType.accept</vh></v>
<v t="ekr.20230831011821.593"><vh>TypeAliasType.__hash__</vh></v>
<v t="ekr.20230831011821.594"><vh>TypeAliasType.__eq__</vh></v>
<v t="ekr.20230831011821.595"><vh>TypeAliasType.serialize</vh></v>
<v t="ekr.20230831011821.596"><vh>TypeAliasType.deserialize</vh></v>
<v t="ekr.20230831011821.597"><vh>TypeAliasType.copy_modified</vh></v>
</v>
<v t="ekr.20230831011821.598"><vh>class TypeGuardedType</vh>
<v t="ekr.20230831011821.599"><vh>TypeGuardedType.__init__</vh></v>
<v t="ekr.20230831011821.600"><vh>TypeGuardedType.__repr__</vh></v>
</v>
<v t="ekr.20230831011821.601"><vh>class RequiredType</vh>
<v t="ekr.20230831011821.602"><vh>RequiredType.__init__</vh></v>
<v t="ekr.20230831011821.603"><vh>RequiredType.__repr__</vh></v>
<v t="ekr.20230831011821.604"><vh>RequiredType.accept</vh></v>
</v>
<v t="ekr.20230831011821.605"><vh>class ProperType</vh></v>
<v t="ekr.20230831011821.606"><vh>class TypeVarId</vh>
<v t="ekr.20230831011821.607"><vh>TypeVarId.__init__</vh></v>
<v t="ekr.20230831011821.608"><vh>TypeVarId.new</vh></v>
<v t="ekr.20230831011821.609"><vh>TypeVarId.__repr__</vh></v>
<v t="ekr.20230831011821.610"><vh>TypeVarId.__eq__</vh></v>
<v t="ekr.20230831011821.611"><vh>TypeVarId.__ne__</vh></v>
<v t="ekr.20230831011821.612"><vh>TypeVarId.__hash__</vh></v>
<v t="ekr.20230831011821.613"><vh>TypeVarId.is_meta_var</vh></v>
</v>
<v t="ekr.20230831011821.614"><vh>class TypeVarLikeType</vh>
<v t="ekr.20230831011821.615"><vh>TypeVarLikeType.__init__</vh></v>
<v t="ekr.20230831011821.616"><vh>TypeVarLikeType.serialize</vh></v>
<v t="ekr.20230831011821.617"><vh>TypeVarLikeType.deserialize</vh></v>
<v t="ekr.20230831011821.618"><vh>TypeVarLikeType.copy_modified</vh></v>
<v t="ekr.20230831011821.619"><vh>TypeVarLikeType.new_unification_variable</vh></v>
<v t="ekr.20230831011821.620"><vh>TypeVarLikeType.has_default</vh></v>
</v>
<v t="ekr.20230831011821.621"><vh>class TypeVarType</vh>
<v t="ekr.20230831011821.622"><vh>TypeVarType.__init__</vh></v>
<v t="ekr.20230831011821.623"><vh>TypeVarType.copy_modified</vh></v>
<v t="ekr.20230831011821.624"><vh>TypeVarType.accept</vh></v>
<v t="ekr.20230831011821.625"><vh>TypeVarType.__hash__</vh></v>
<v t="ekr.20230831011821.626"><vh>TypeVarType.__eq__</vh></v>
<v t="ekr.20230831011821.627"><vh>TypeVarType.serialize</vh></v>
<v t="ekr.20230831011821.628"><vh>TypeVarType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.629"><vh>class ParamSpecFlavor</vh></v>
<v t="ekr.20230831011821.630"><vh>class ParamSpecType</vh>
<v t="ekr.20230831011821.631"><vh>ParamSpecType.__init__</vh></v>
<v t="ekr.20230831011821.632"><vh>ParamSpecType.with_flavor</vh></v>
<v t="ekr.20230831011821.633"><vh>ParamSpecType.copy_modified</vh></v>
<v t="ekr.20230831011821.634"><vh>ParamSpecType.accept</vh></v>
<v t="ekr.20230831011821.635"><vh>ParamSpecType.name_with_suffix</vh></v>
<v t="ekr.20230831011821.636"><vh>ParamSpecType.__hash__</vh></v>
<v t="ekr.20230831011821.637"><vh>ParamSpecType.__eq__</vh></v>
<v t="ekr.20230831011821.638"><vh>ParamSpecType.serialize</vh></v>
<v t="ekr.20230831011821.639"><vh>ParamSpecType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.640"><vh>class TypeVarTupleType</vh>
<v t="ekr.20230831011821.641"><vh>TypeVarTupleType.__init__</vh></v>
<v t="ekr.20230831011821.642"><vh>TypeVarTupleType.serialize</vh></v>
<v t="ekr.20230831011821.643"><vh>TypeVarTupleType.deserialize</vh></v>
<v t="ekr.20230831011821.644"><vh>TypeVarTupleType.accept</vh></v>
<v t="ekr.20230831011821.645"><vh>TypeVarTupleType.__hash__</vh></v>
<v t="ekr.20230831011821.646"><vh>TypeVarTupleType.__eq__</vh></v>
<v t="ekr.20230831011821.647"><vh>TypeVarTupleType.copy_modified</vh></v>
</v>
<v t="ekr.20230831011821.648"><vh>class UnboundType</vh>
<v t="ekr.20230831011821.649"><vh>UnboundType.__init__</vh></v>
<v t="ekr.20230831011821.650"><vh>UnboundType.copy_modified</vh></v>
<v t="ekr.20230831011821.651"><vh>UnboundType.accept</vh></v>
<v t="ekr.20230831011821.652"><vh>UnboundType.__hash__</vh></v>
<v t="ekr.20230831011821.653"><vh>UnboundType.__eq__</vh></v>
<v t="ekr.20230831011821.654"><vh>UnboundType.serialize</vh></v>
<v t="ekr.20230831011821.655"><vh>UnboundType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.656"><vh>class CallableArgument</vh>
<v t="ekr.20230831011821.657"><vh>CallableArgument.__init__</vh></v>
<v t="ekr.20230831011821.658"><vh>CallableArgument.accept</vh></v>
<v t="ekr.20230831011821.659"><vh>CallableArgument.serialize</vh></v>
</v>
<v t="ekr.20230831011821.660"><vh>class TypeList</vh>
<v t="ekr.20230831011821.661"><vh>TypeList.__init__</vh></v>
<v t="ekr.20230831011821.662"><vh>TypeList.accept</vh></v>
<v t="ekr.20230831011821.663"><vh>TypeList.serialize</vh></v>
<v t="ekr.20230831011821.664"><vh>TypeList.__hash__</vh></v>
<v t="ekr.20230831011821.665"><vh>TypeList.__eq__</vh></v>
</v>
<v t="ekr.20230831011821.666"><vh>class UnpackType</vh>
<v t="ekr.20230831011821.667"><vh>UnpackType.__init__</vh></v>
<v t="ekr.20230831011821.668"><vh>UnpackType.accept</vh></v>
<v t="ekr.20230831011821.669"><vh>UnpackType.serialize</vh></v>
<v t="ekr.20230831011821.670"><vh>UnpackType.deserialize</vh></v>
<v t="ekr.20230831011821.671"><vh>UnpackType.__hash__</vh></v>
<v t="ekr.20230831011821.672"><vh>UnpackType.__eq__</vh></v>
</v>
<v t="ekr.20230831011821.673"><vh>class AnyType</vh>
<v t="ekr.20230831011821.674"><vh>AnyType.__init__</vh></v>
<v t="ekr.20230831011821.675"><vh>AnyType.is_from_error</vh></v>
<v t="ekr.20230831011821.676"><vh>AnyType.accept</vh></v>
<v t="ekr.20230831011821.677"><vh>AnyType.copy_modified</vh></v>
<v t="ekr.20230831011821.678"><vh>AnyType.__hash__</vh></v>
<v t="ekr.20230831011821.679"><vh>AnyType.__eq__</vh></v>
<v t="ekr.20230831011821.680"><vh>AnyType.serialize</vh></v>
<v t="ekr.20230831011821.681"><vh>AnyType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.682"><vh>class UninhabitedType</vh>
<v t="ekr.20230831011821.683"><vh>UninhabitedType.__init__</vh></v>
<v t="ekr.20230831011821.684"><vh>UninhabitedType.can_be_true_default</vh></v>
<v t="ekr.20230831011821.685"><vh>UninhabitedType.can_be_false_default</vh></v>
<v t="ekr.20230831011821.686"><vh>UninhabitedType.accept</vh></v>
<v t="ekr.20230831011821.687"><vh>UninhabitedType.__hash__</vh></v>
<v t="ekr.20230831011821.688"><vh>UninhabitedType.__eq__</vh></v>
<v t="ekr.20230831011821.689"><vh>UninhabitedType.serialize</vh></v>
<v t="ekr.20230831011821.690"><vh>UninhabitedType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.691"><vh>class NoneType</vh>
<v t="ekr.20230831011821.692"><vh>NoneType.__init__</vh></v>
<v t="ekr.20230831011821.693"><vh>NoneType.can_be_true_default</vh></v>
<v t="ekr.20230831011821.694"><vh>NoneType.__hash__</vh></v>
<v t="ekr.20230831011821.695"><vh>NoneType.__eq__</vh></v>
<v t="ekr.20230831011821.696"><vh>NoneType.accept</vh></v>
<v t="ekr.20230831011821.697"><vh>NoneType.serialize</vh></v>
<v t="ekr.20230831011821.698"><vh>NoneType.deserialize</vh></v>
<v t="ekr.20230831011821.699"><vh>NoneType.is_singleton_type</vh></v>
</v>
<v t="ekr.20230831011821.700"><vh>class ErasedType</vh>
<v t="ekr.20230831011821.701"><vh>ErasedType.accept</vh></v>
</v>
<v t="ekr.20230831011821.702"><vh>class DeletedType</vh>
<v t="ekr.20230831011821.703"><vh>DeletedType.__init__</vh></v>
<v t="ekr.20230831011821.704"><vh>DeletedType.accept</vh></v>
<v t="ekr.20230831011821.705"><vh>DeletedType.serialize</vh></v>
<v t="ekr.20230831011821.706"><vh>DeletedType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.707"><vh>class ExtraAttrs</vh>
<v t="ekr.20230831011821.708"><vh>ExtraAttrs.__init__</vh></v>
<v t="ekr.20230831011821.709"><vh>ExtraAttrs.__hash__</vh></v>
<v t="ekr.20230831011821.710"><vh>ExtraAttrs.__eq__</vh></v>
<v t="ekr.20230831011821.711"><vh>ExtraAttrs.copy</vh></v>
<v t="ekr.20230831011821.712"><vh>ExtraAttrs.__repr__</vh></v>
</v>
<v t="ekr.20230831011821.713"><vh>class Instance</vh>
<v t="ekr.20230831011821.714"><vh>Instance.__init__</vh></v>
<v t="ekr.20230831011821.715"><vh>Instance.accept</vh></v>
<v t="ekr.20230831011821.716"><vh>Instance.__hash__</vh></v>
<v t="ekr.20230831011821.717"><vh>Instance.__eq__</vh></v>
<v t="ekr.20230831011821.718"><vh>Instance.serialize</vh></v>
<v t="ekr.20230831011821.719"><vh>Instance.deserialize</vh></v>
<v t="ekr.20230831011821.720"><vh>Instance.copy_modified</vh></v>
<v t="ekr.20230831011821.721"><vh>Instance.copy_with_extra_attr</vh></v>
<v t="ekr.20230831011821.722"><vh>Instance.is_singleton_type</vh></v>
<v t="ekr.20230831011821.723"><vh>Instance.get_enum_values</vh></v>
</v>
<v t="ekr.20230831011821.724"><vh>class FunctionLike</vh>
<v t="ekr.20230831011821.725"><vh>FunctionLike.__init__</vh></v>
<v t="ekr.20230831011821.726"><vh>FunctionLike.is_type_obj</vh></v>
<v t="ekr.20230831011821.727"><vh>FunctionLike.type_object</vh></v>
<v t="ekr.20230831011821.728"><vh>FunctionLike.items</vh></v>
<v t="ekr.20230831011821.729"><vh>FunctionLike.with_name</vh></v>
<v t="ekr.20230831011821.730"><vh>FunctionLike.get_name</vh></v>
</v>
<v t="ekr.20230831011821.731"><vh>class FormalArgument</vh></v>
<v t="ekr.20230831011821.732"><vh>class Parameters</vh>
<v t="ekr.20230831011821.733"><vh>Parameters.__init__</vh></v>
<v t="ekr.20230831011821.734"><vh>Parameters.copy_modified</vh></v>
<v t="ekr.20230831011821.735"><vh>Parameters.var_arg</vh></v>
<v t="ekr.20230831011821.736"><vh>Parameters.kw_arg</vh></v>
<v t="ekr.20230831011821.737"><vh>Parameters.formal_arguments</vh></v>
<v t="ekr.20230831011821.738"><vh>Parameters.argument_by_name</vh></v>
<v t="ekr.20230831011821.739"><vh>Parameters.argument_by_position</vh></v>
<v t="ekr.20230831011821.740"><vh>Parameters.try_synthesizing_arg_from_kwarg</vh></v>
<v t="ekr.20230831011821.741"><vh>Parameters.try_synthesizing_arg_from_vararg</vh></v>
<v t="ekr.20230831011821.742"><vh>Parameters.accept</vh></v>
<v t="ekr.20230831011821.743"><vh>Parameters.serialize</vh></v>
<v t="ekr.20230831011821.744"><vh>Parameters.deserialize</vh></v>
<v t="ekr.20230831011821.745"><vh>Parameters.__hash__</vh></v>
<v t="ekr.20230831011821.746"><vh>Parameters.__eq__</vh></v>
</v>
<v t="ekr.20230831011821.747"><vh>class CallableType</vh>
<v t="ekr.20230831011821.748"><vh>CallableType.__init__</vh></v>
<v t="ekr.20230831011821.749"><vh>CallableType.copy_modified</vh></v>
<v t="ekr.20230831011821.750"><vh>CallableType.var_arg</vh></v>
<v t="ekr.20230831011821.751"><vh>CallableType.kw_arg</vh></v>
<v t="ekr.20230831011821.752"><vh>CallableType.is_var_arg</vh></v>
<v t="ekr.20230831011821.753"><vh>CallableType.is_kw_arg</vh></v>
<v t="ekr.20230831011821.754"><vh>CallableType.is_type_obj</vh></v>
<v t="ekr.20230831011821.755"><vh>CallableType.type_object</vh></v>
<v t="ekr.20230831011821.756"><vh>CallableType.accept</vh></v>
<v t="ekr.20230831011821.757"><vh>CallableType.with_name</vh></v>
<v t="ekr.20230831011821.758"><vh>CallableType.get_name</vh></v>
<v t="ekr.20230831011821.759"><vh>CallableType.max_possible_positional_args</vh></v>
<v t="ekr.20230831011821.760"><vh>CallableType.formal_arguments</vh></v>
<v t="ekr.20230831011821.761"><vh>CallableType.argument_by_name</vh></v>
<v t="ekr.20230831011821.762"><vh>CallableType.argument_by_position</vh></v>
<v t="ekr.20230831011821.763"><vh>CallableType.try_synthesizing_arg_from_kwarg</vh></v>
<v t="ekr.20230831011821.764"><vh>CallableType.try_synthesizing_arg_from_vararg</vh></v>
<v t="ekr.20230831011821.765"><vh>CallableType.items</vh></v>
<v t="ekr.20230831011821.766"><vh>CallableType.is_generic</vh></v>
<v t="ekr.20230831011821.767"><vh>CallableType.type_var_ids</vh></v>
<v t="ekr.20230831011821.768"><vh>CallableType.param_spec</vh></v>
<v t="ekr.20230831011821.769"><vh>CallableType.expand_param_spec</vh></v>
<v t="ekr.20230831011821.770"><vh>CallableType.with_unpacked_kwargs</vh></v>
<v t="ekr.20230831011821.771"><vh>CallableType.with_normalized_var_args</vh></v>
<v t="ekr.20230831011821.772"><vh>CallableType.__hash__</vh></v>
<v t="ekr.20230831011821.773"><vh>CallableType.__eq__</vh></v>
<v t="ekr.20230831011821.774"><vh>CallableType.serialize</vh></v>
<v t="ekr.20230831011821.775"><vh>CallableType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.776"><vh>class Overloaded</vh>
<v t="ekr.20230831011821.777"><vh>Overloaded.__init__</vh></v>
<v t="ekr.20230831011821.778"><vh>Overloaded.items</vh></v>
<v t="ekr.20230831011821.779"><vh>Overloaded.name</vh></v>
<v t="ekr.20230831011821.780"><vh>Overloaded.is_type_obj</vh></v>
<v t="ekr.20230831011821.781"><vh>Overloaded.type_object</vh></v>
<v t="ekr.20230831011821.782"><vh>Overloaded.with_name</vh></v>
<v t="ekr.20230831011821.783"><vh>Overloaded.get_name</vh></v>
<v t="ekr.20230831011821.784"><vh>Overloaded.with_unpacked_kwargs</vh></v>
<v t="ekr.20230831011821.785"><vh>Overloaded.accept</vh></v>
<v t="ekr.20230831011821.786"><vh>Overloaded.__hash__</vh></v>
<v t="ekr.20230831011821.787"><vh>Overloaded.__eq__</vh></v>
<v t="ekr.20230831011821.788"><vh>Overloaded.serialize</vh></v>
<v t="ekr.20230831011821.789"><vh>Overloaded.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.790"><vh>class TupleType</vh>
<v t="ekr.20230831011821.791"><vh>TupleType.__init__</vh></v>
<v t="ekr.20230831011821.792"><vh>TupleType.can_be_true_default</vh></v>
<v t="ekr.20230831011821.793"><vh>TupleType.can_be_false_default</vh></v>
<v t="ekr.20230831011821.794"><vh>TupleType.can_be_any_bool</vh></v>
<v t="ekr.20230831011821.795"><vh>TupleType.length</vh></v>
<v t="ekr.20230831011821.796"><vh>TupleType.accept</vh></v>
<v t="ekr.20230831011821.797"><vh>TupleType.__hash__</vh></v>
<v t="ekr.20230831011821.798"><vh>TupleType.__eq__</vh></v>
<v t="ekr.20230831011821.799"><vh>TupleType.serialize</vh></v>
<v t="ekr.20230831011821.800"><vh>TupleType.deserialize</vh></v>
<v t="ekr.20230831011821.801"><vh>TupleType.copy_modified</vh></v>
<v t="ekr.20230831011821.802"><vh>TupleType.slice</vh></v>
</v>
<v t="ekr.20230831011821.803"><vh>class TypedDictType</vh>
<v t="ekr.20230831011821.804"><vh>TypedDictType.__init__</vh></v>
<v t="ekr.20230831011821.805"><vh>TypedDictType.accept</vh></v>
<v t="ekr.20230831011821.806"><vh>TypedDictType.__hash__</vh></v>
<v t="ekr.20230831011821.807"><vh>TypedDictType.__eq__</vh></v>
<v t="ekr.20230831011821.808"><vh>TypedDictType.serialize</vh></v>
<v t="ekr.20230831011821.809"><vh>TypedDictType.deserialize</vh></v>
<v t="ekr.20230831011821.810"><vh>TypedDictType.is_final</vh></v>
<v t="ekr.20230831011821.811"><vh>TypedDictType.is_anonymous</vh></v>
<v t="ekr.20230831011821.812"><vh>TypedDictType.as_anonymous</vh></v>
<v t="ekr.20230831011821.813"><vh>TypedDictType.copy_modified</vh></v>
<v t="ekr.20230831011821.814"><vh>TypedDictType.create_anonymous_fallback</vh></v>
<v t="ekr.20230831011821.815"><vh>TypedDictType.names_are_wider_than</vh></v>
<v t="ekr.20230831011821.816"><vh>TypedDictType.zip</vh></v>
<v t="ekr.20230831011821.817"><vh>TypedDictType.zipall</vh></v>
</v>
<v t="ekr.20230831011821.818"><vh>class RawExpressionType</vh>
<v t="ekr.20230831011821.819"><vh>RawExpressionType.__init__</vh></v>
<v t="ekr.20230831011821.820"><vh>RawExpressionType.simple_name</vh></v>
<v t="ekr.20230831011821.821"><vh>RawExpressionType.accept</vh></v>
<v t="ekr.20230831011821.822"><vh>RawExpressionType.serialize</vh></v>
<v t="ekr.20230831011821.823"><vh>RawExpressionType.__hash__</vh></v>
<v t="ekr.20230831011821.824"><vh>RawExpressionType.__eq__</vh></v>
</v>
<v t="ekr.20230831011821.825"><vh>class LiteralType</vh>
<v t="ekr.20230831011821.826"><vh>LiteralType.__init__</vh></v>
<v t="ekr.20230831011821.827"><vh>LiteralType.can_be_false_default</vh></v>
<v t="ekr.20230831011821.828"><vh>LiteralType.can_be_true_default</vh></v>
<v t="ekr.20230831011821.829"><vh>LiteralType.accept</vh></v>
<v t="ekr.20230831011821.830"><vh>LiteralType.__hash__</vh></v>
<v t="ekr.20230831011821.831"><vh>LiteralType.__eq__</vh></v>
<v t="ekr.20230831011821.832"><vh>LiteralType.is_enum_literal</vh></v>
<v t="ekr.20230831011821.833"><vh>LiteralType.value_repr</vh></v>
<v t="ekr.20230831011821.834"><vh>LiteralType.serialize</vh></v>
<v t="ekr.20230831011821.835"><vh>LiteralType.deserialize</vh></v>
<v t="ekr.20230831011821.836"><vh>LiteralType.is_singleton_type</vh></v>
</v>
<v t="ekr.20230831011821.837"><vh>class UnionType</vh>
<v t="ekr.20230831011821.838"><vh>UnionType.__init__</vh></v>
<v t="ekr.20230831011821.839"><vh>UnionType.can_be_true_default</vh></v>
<v t="ekr.20230831011821.840"><vh>UnionType.can_be_false_default</vh></v>
<v t="ekr.20230831011821.841"><vh>UnionType.__hash__</vh></v>
<v t="ekr.20230831011821.842"><vh>UnionType.__eq__</vh></v>
<v t="ekr.20230831011821.843"><vh>UnionType.make_union</vh></v>
<v t="ekr.20230831011821.844"><vh>UnionType.make_union</vh></v>
<v t="ekr.20230831011821.845"><vh>UnionType.make_union</vh></v>
<v t="ekr.20230831011821.846"><vh>UnionType.length</vh></v>
<v t="ekr.20230831011821.847"><vh>UnionType.accept</vh></v>
<v t="ekr.20230831011821.848"><vh>UnionType.relevant_items</vh></v>
<v t="ekr.20230831011821.849"><vh>UnionType.serialize</vh></v>
<v t="ekr.20230831011821.850"><vh>UnionType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.851"><vh>class PartialType</vh>
<v t="ekr.20230831011821.852"><vh>PartialType.__init__</vh></v>
<v t="ekr.20230831011821.853"><vh>PartialType.accept</vh></v>
</v>
<v t="ekr.20230831011821.854"><vh>class EllipsisType</vh>
<v t="ekr.20230831011821.855"><vh>EllipsisType.accept</vh></v>
<v t="ekr.20230831011821.856"><vh>EllipsisType.serialize</vh></v>
</v>
<v t="ekr.20230831011821.857"><vh>class TypeType</vh>
<v t="ekr.20230831011821.858"><vh>TypeType.__init__</vh></v>
<v t="ekr.20230831011821.859"><vh>TypeType.make_normalized</vh></v>
<v t="ekr.20230831011821.860"><vh>TypeType.accept</vh></v>
<v t="ekr.20230831011821.861"><vh>TypeType.__hash__</vh></v>
<v t="ekr.20230831011821.862"><vh>TypeType.__eq__</vh></v>
<v t="ekr.20230831011821.863"><vh>TypeType.serialize</vh></v>
<v t="ekr.20230831011821.864"><vh>TypeType.deserialize</vh></v>
</v>
<v t="ekr.20230831011821.865"><vh>class PlaceholderType</vh>
<v t="ekr.20230831011821.866"><vh>PlaceholderType.__init__</vh></v>
<v t="ekr.20230831011821.867"><vh>PlaceholderType.accept</vh></v>
<v t="ekr.20230831011821.868"><vh>PlaceholderType.__hash__</vh></v>
<v t="ekr.20230831011821.869"><vh>PlaceholderType.__eq__</vh></v>
<v t="ekr.20230831011821.870"><vh>PlaceholderType.serialize</vh></v>
</v>
<v t="ekr.20230831011821.871"><vh>function: get_proper_type</vh></v>
<v t="ekr.20230831011821.872"><vh>function: get_proper_type</vh></v>
<v t="ekr.20230831011821.873"><vh>function: get_proper_type</vh></v>
<v t="ekr.20230831011821.874"><vh>function: get_proper_types</vh></v>
<v t="ekr.20230831011821.875"><vh>function: get_proper_types</vh></v>
<v t="ekr.20230831011821.876"><vh>function: get_proper_types</vh></v>
<v t="ekr.20230831011821.877"><vh>class TypeStrVisitor</vh>
<v t="ekr.20230831011821.878"><vh>TypeStrVisitor.__init__</vh></v>
<v t="ekr.20230831011821.879"><vh>TypeStrVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.880"><vh>TypeStrVisitor.visit_type_list</vh></v>
<v t="ekr.20230831011821.881"><vh>TypeStrVisitor.visit_callable_argument</vh></v>
<v t="ekr.20230831011821.882"><vh>TypeStrVisitor.visit_any</vh></v>
<v t="ekr.20230831011821.883"><vh>TypeStrVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011821.884"><vh>TypeStrVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011821.885"><vh>TypeStrVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011821.886"><vh>TypeStrVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011821.887"><vh>TypeStrVisitor.visit_instance</vh></v>
<v t="ekr.20230831011821.888"><vh>TypeStrVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011821.889"><vh>TypeStrVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011821.890"><vh>TypeStrVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011821.891"><vh>TypeStrVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.892"><vh>TypeStrVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011821.893"><vh>TypeStrVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011821.894"><vh>TypeStrVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011821.895"><vh>TypeStrVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011821.896"><vh>TypeStrVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20230831011821.897"><vh>TypeStrVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011821.898"><vh>TypeStrVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011821.899"><vh>TypeStrVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011821.900"><vh>TypeStrVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20230831011821.901"><vh>TypeStrVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011821.902"><vh>TypeStrVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20230831011821.903"><vh>TypeStrVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20230831011821.904"><vh>TypeStrVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011821.905"><vh>TypeStrVisitor.list_str</vh></v>
</v>
<v t="ekr.20230831011821.906"><vh>class TrivialSyntheticTypeTranslator</vh>
<v t="ekr.20230831011821.907"><vh>TrivialSyntheticTypeTranslator.visit_placeholder_type</vh></v>
<v t="ekr.20230831011821.908"><vh>TrivialSyntheticTypeTranslator.visit_callable_argument</vh></v>
<v t="ekr.20230831011821.909"><vh>TrivialSyntheticTypeTranslator.visit_ellipsis_type</vh></v>
<v t="ekr.20230831011821.910"><vh>TrivialSyntheticTypeTranslator.visit_raw_expression_type</vh></v>
<v t="ekr.20230831011821.911"><vh>TrivialSyntheticTypeTranslator.visit_type_list</vh></v>
</v>
<v t="ekr.20230831011821.912"><vh>class UnrollAliasVisitor</vh>
<v t="ekr.20230831011821.913"><vh>UnrollAliasVisitor.__init__</vh></v>
<v t="ekr.20230831011821.914"><vh>UnrollAliasVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011821.915"><vh>function: is_named_instance</vh></v>
<v t="ekr.20230831011821.916"><vh>class LocationSetter</vh>
<v t="ekr.20230831011821.917"><vh>LocationSetter.__init__</vh></v>
<v t="ekr.20230831011821.918"><vh>LocationSetter.visit_instance</vh></v>
</v>
<v t="ekr.20230831011821.919"><vh>class HasTypeVars</vh>
<v t="ekr.20230831011821.920"><vh>HasTypeVars.__init__</vh></v>
<v t="ekr.20230831011821.921"><vh>HasTypeVars.visit_type_var</vh></v>
<v t="ekr.20230831011821.922"><vh>HasTypeVars.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.923"><vh>HasTypeVars.visit_param_spec</vh></v>
</v>
<v t="ekr.20230831011821.924"><vh>function: has_type_vars</vh></v>
<v t="ekr.20230831011821.925"><vh>class HasRecursiveType</vh>
<v t="ekr.20230831011821.926"><vh>HasRecursiveType.__init__</vh></v>
<v t="ekr.20230831011821.927"><vh>HasRecursiveType.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20230831011821.928"><vh>function: has_recursive_types</vh></v>
<v t="ekr.20230831011821.929"><vh>function: split_with_prefix_and_suffix</vh></v>
<v t="ekr.20230831011821.930"><vh>function: extend_args_for_prefix_and_suffix</vh></v>
<v t="ekr.20230831011821.931"><vh>function: flatten_nested_unions</vh></v>
<v t="ekr.20230831011821.932"><vh>function: find_unpack_in_list</vh></v>
<v t="ekr.20230831011821.933"><vh>function: flatten_nested_tuples</vh></v>
<v t="ekr.20230831011821.934"><vh>function: is_literal_type</vh></v>
<v t="ekr.20230831011821.935"><vh>function: callable_with_ellipsis</vh></v>
<v t="ekr.20230831011821.936"><vh>function: remove_dups</vh></v>
<v t="ekr.20230831011821.937"><vh>class InstantiateAliasVisitor</vh>
<v t="ekr.20230831011821.938"><vh>InstantiateAliasVisitor.visit_union_type</vh></v>
</v>
</v>
<v t="ekr.20230831011821.939"><vh>@clean types_utils.py</vh>
<v t="ekr.20230831011821.941"><vh>&lt;&lt; types_utils.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011821.942"><vh>function: flatten_types</vh></v>
<v t="ekr.20230831011821.943"><vh>function: strip_type</vh></v>
<v t="ekr.20230831011821.944"><vh>function: is_invalid_recursive_alias</vh></v>
<v t="ekr.20230831011821.945"><vh>function: is_bad_type_type_item</vh></v>
<v t="ekr.20230831011821.946"><vh>function: is_union_with_any</vh></v>
<v t="ekr.20230831011821.947"><vh>function: is_generic_instance</vh></v>
<v t="ekr.20230831011821.948"><vh>function: is_overlapping_none</vh></v>
<v t="ekr.20230831011821.949"><vh>function: remove_optional</vh></v>
<v t="ekr.20230831011821.950"><vh>function: is_self_type_like</vh></v>
<v t="ekr.20230831011821.951"><vh>function: store_argument_type</vh></v>
</v>
<v t="ekr.20230831011821.952"><vh>@clean typestate.py</vh>
<v t="ekr.20230831011821.954"><vh>&lt;&lt; typestate.py: declarations &gt;&gt;</vh></v>
<v t="ekr.20230831011821.955"><vh>class TypeState</vh>
<v t="ekr.20230831011821.956"><vh>TypeState.__init__</vh></v>
<v t="ekr.20230831011821.957"><vh>TypeState.is_assumed_subtype</vh></v>
<v t="ekr.20230831011821.958"><vh>TypeState.is_assumed_proper_subtype</vh></v>
<v t="ekr.20230831011821.959"><vh>TypeState.get_assumptions</vh></v>
<v t="ekr.20230831011821.960"><vh>TypeState.reset_all_subtype_caches</vh></v>
<v t="ekr.20230831011821.961"><vh>TypeState.reset_subtype_caches_for</vh></v>
<v t="ekr.20230831011821.962"><vh>TypeState.reset_all_subtype_caches_for</vh></v>
<v t="ekr.20230831011821.963"><vh>TypeState.is_cached_subtype_check</vh></v>
<v t="ekr.20230831011821.964"><vh>TypeState.is_cached_negative_subtype_check</vh></v>
<v t="ekr.20230831011821.965"><vh>TypeState.record_subtype_cache_entry</vh></v>
<v t="ekr.20230831011821.966"><vh>TypeState.record_negative_subtype_cache_entry</vh></v>
<v t="ekr.20230831011821.967"><vh>TypeState.reset_protocol_deps</vh></v>
<v t="ekr.20230831011821.968"><vh>TypeState.record_protocol_subtype_check</vh></v>
<v t="ekr.20230831011821.969"><vh>TypeState._snapshot_protocol_deps</vh></v>
<v t="ekr.20230831011821.970"><vh>TypeState.update_protocol_deps</vh></v>
<v t="ekr.20230831011821.971"><vh>TypeState.add_all_protocol_deps</vh></v>
</v>
<v t="ekr.20230831011821.972"><vh>function: reset_global_state</vh></v>
</v>
<v t="ekr.20230831011821.973"><vh>@clean typetraverser.py</vh>
<v t="ekr.20230831011821.974"><vh>&lt;&lt; typetraverser.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.975"><vh>class TypeTraverserVisitor</vh>
<v t="ekr.20230831011821.976"><vh>TypeTraverserVisitor.visit_any</vh></v>
<v t="ekr.20230831011821.977"><vh>TypeTraverserVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20230831011821.978"><vh>TypeTraverserVisitor.visit_none_type</vh></v>
<v t="ekr.20230831011821.979"><vh>TypeTraverserVisitor.visit_erased_type</vh></v>
<v t="ekr.20230831011821.980"><vh>TypeTraverserVisitor.visit_deleted_type</vh></v>
<v t="ekr.20230831011821.981"><vh>TypeTraverserVisitor.visit_type_var</vh></v>
<v t="ekr.20230831011821.982"><vh>TypeTraverserVisitor.visit_param_spec</vh></v>
<v t="ekr.20230831011821.983"><vh>TypeTraverserVisitor.visit_parameters</vh></v>
<v t="ekr.20230831011821.984"><vh>TypeTraverserVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20230831011821.985"><vh>TypeTraverserVisitor.visit_literal_type</vh></v>
<v t="ekr.20230831011821.986"><vh>TypeTraverserVisitor.visit_instance</vh></v>
<v t="ekr.20230831011821.987"><vh>TypeTraverserVisitor.visit_callable_type</vh></v>
<v t="ekr.20230831011821.988"><vh>TypeTraverserVisitor.visit_tuple_type</vh></v>
<v t="ekr.20230831011821.989"><vh>TypeTraverserVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20230831011821.990"><vh>TypeTraverserVisitor.visit_union_type</vh></v>
<v t="ekr.20230831011821.991"><vh>TypeTraverserVisitor.visit_overloaded</vh></v>
<v t="ekr.20230831011821.992"><vh>TypeTraverserVisitor.visit_type_type</vh></v>
<v t="ekr.20230831011821.993"><vh>TypeTraverserVisitor.visit_callable_argument</vh></v>
<v t="ekr.20230831011821.994"><vh>TypeTraverserVisitor.visit_unbound_type</vh></v>
<v t="ekr.20230831011821.995"><vh>TypeTraverserVisitor.visit_type_list</vh></v>
<v t="ekr.20230831011821.996"><vh>TypeTraverserVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20230831011821.997"><vh>TypeTraverserVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20230831011821.998"><vh>TypeTraverserVisitor.visit_partial_type</vh></v>
<v t="ekr.20230831011821.999"><vh>TypeTraverserVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20230831011821.1000"><vh>TypeTraverserVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20230831011821.1001"><vh>TypeTraverserVisitor.visit_unpack_type</vh></v>
<v t="ekr.20230831011821.1002"><vh>TypeTraverserVisitor.traverse_types</vh></v>
</v>
</v>
<v t="ekr.20230831011821.1003"><vh>@clean typevars.py</vh>
<v t="ekr.20230831011821.1004"><vh>&lt;&lt; typevars.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.1005"><vh>function: fill_typevars</vh></v>
<v t="ekr.20230831011821.1006"><vh>function: fill_typevars_with_any</vh></v>
<v t="ekr.20230831011821.1007"><vh>function: has_no_typevars</vh></v>
</v>
<v t="ekr.20230831011821.1008"><vh>@clean typevartuples.py</vh>
<v t="ekr.20230831011821.1009"><vh>&lt;&lt; typevartuples.py: preamble &gt;&gt;</vh></v>
<v t="ekr.20230831011821.1010"><vh>function: split_with_instance</vh></v>
<v t="ekr.20230831011821.1011"><vh>function: split_with_mapped_and_template</vh></v>
<v t="ekr.20230831011821.1012"><vh>function: fully_split_with_mapped_and_template</vh></v>
<v t="ekr.20230831011821.1013"><vh>function: extract_unpack</vh></v>
</v>
</v>
</v>
<v t="ekr.20230902062730.1"><vh>--- important docstrings</vh>
<v t="ekr.20230902050513.1"></v>
<v t="ekr.20230902062700.1"></v>
</v>
<v t="ekr.20230902063457.1"><vh>--- is solve the key ??</vh></v>
<v t="ekr.20230831011820.1572"></v>
<v t="ekr.20230831011820.1577"></v>
<v t="ekr.20230907081811.1"><vh>@clean ekr_test_12352.py</vh></v>
<v t="ekr.20230907081931.1"><vh>@clean ekr_test.py</vh></v>
</vnodes>
<tnodes>
<t tx="ekr.20230613022138.1"># Disable flake8 tests</t>
<t tx="ekr.20230613022224.1">True: run flake8 on each saved file, but only if it has been changed.</t>
<t tx="ekr.20230613022630.1"></t>
<t tx="ekr.20230614124254.1">@language python
@tabwidth -4 # For a better match.
g.cls()
# All of the following work.
# dir_ = r'C:\Repos\ekr-mypy2\mypy'
# dir_ = None
# dir_ = ''
# dir_ = 'mypy/build.py'
dir_ = 'mypy'
c.recursiveImport(
    dir_=dir_,
    kind = '@clean', # '@auto', '@clean', '@nosent','@file',
    recursive = False,
    safe_at_file = False,
    theTypes = ['.py'],
    verbose = True,
)
if 1:
    last = c.lastTopLevel()
    last.expand()
    if last.hasChildren():
        last.firstChild().expand()
    c.redraw(last)
print('Done')</t>
<t tx="ekr.20230828050824.1">"""
Back up this .leo file.

os.environ['LEO_BACKUP'] must be the path to an existing (writable) directory.
"""
c.backup_helper(sub_dir='mypy.leo')
</t>
<t tx="ekr.20230831011819.10">def _run(main_wrapper: Callable[[TextIO, TextIO], None]) -&gt; tuple[str, str, int]:
    stdout = StringIO()
    stderr = StringIO()

    try:
        main_wrapper(stdout, stderr)
        exit_status = 0
    except SystemExit as system_exit:
        assert isinstance(system_exit.code, int)
        exit_status = system_exit.code

    return stdout.getvalue(), stderr.getvalue(), exit_status


</t>
<t tx="ekr.20230831011819.100">def _load_json_file(
    file: str, manager: BuildManager, log_success: str, log_error: str
) -&gt; dict[str, Any] | None:
    """A simple helper to read a JSON file with logging."""
    t0 = time.time()
    try:
        data = manager.metastore.read(file)
    except OSError:
        manager.log(log_error + file)
        return None
    manager.add_stats(metastore_read_time=time.time() - t0)
    # Only bother to compute the log message if we are logging it, since it could be big
    if manager.verbosity() &gt;= 2:
        manager.trace(log_success + data.rstrip())
    try:
        t1 = time.time()
        result = json.loads(data)
        manager.add_stats(data_json_load_time=time.time() - t1)
    except json.JSONDecodeError:
        manager.errors.set_file(file, None, manager.options)
        manager.errors.report(
            -1,
            -1,
            "Error reading JSON file;"
            " you likely have a bad cache.\n"
            "Try removing the {cache_dir} directory"
            " and run mypy again.".format(cache_dir=manager.options.cache_dir),
            blocker=True,
        )
        return None
    else:
        assert isinstance(result, dict)
        return result


</t>
<t tx="ekr.20230831011819.1000">@path mypy
&lt;&lt; errors.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1001">from __future__ import annotations

import os.path
import sys
import traceback
from collections import defaultdict
from typing import Callable, Final, Iterable, NoReturn, Optional, TextIO, Tuple, TypeVar
from typing_extensions import Literal, TypeAlias as _TypeAlias

from mypy import errorcodes as codes
from mypy.errorcodes import IMPORT, IMPORT_NOT_FOUND, IMPORT_UNTYPED, ErrorCode
from mypy.message_registry import ErrorMessage
from mypy.options import Options
from mypy.scope import Scope
from mypy.util import DEFAULT_SOURCE_OFFSET, is_typeshed_file
from mypy.version import __version__ as mypy_version

T = TypeVar("T")

# Show error codes for some note-level messages (these usually appear alone
# and not as a comment for a previous error-level message).
SHOW_NOTE_CODES: Final = {codes.ANNOTATION_UNCHECKED}

# Do not add notes with links to error code docs to errors with these codes.
# We can tweak this set as we get more experience about what is helpful and what is not.
HIDE_LINK_CODES: Final = {
    # This is a generic error code, so it has no useful docs
    codes.MISC,
    # These are trivial and have some custom notes (e.g. for list being invariant)
    codes.ASSIGNMENT,
    codes.ARG_TYPE,
    codes.RETURN_VALUE,
    # Undefined name/attribute errors are self-explanatory
    codes.ATTR_DEFINED,
    codes.NAME_DEFINED,
    # Overrides have a custom link to docs
    codes.OVERRIDE,
}

allowed_duplicates: Final = ["@overload", "Got:", "Expected:"]

BASE_RTD_URL: Final = "https://mypy.rtfd.io/en/stable/_refs.html#code"

# Keep track of the original error code when the error code of a message is changed.
# This is used to give notes about out-of-date "type: ignore" comments.
original_error_codes: Final = {codes.LITERAL_REQ: codes.MISC, codes.TYPE_ABSTRACT: codes.MISC}


</t>
<t tx="ekr.20230831011819.1002">class ErrorInfo:
    """Representation of a single error message."""

    @others
</t>
<t tx="ekr.20230831011819.1003"># Description of a sequence of imports that refer to the source file
# related to this error. Each item is a (path, line number) tuple.
import_ctx: list[tuple[str, int]]

# The path to source file that was the source of this error.
file = ""

# The fully-qualified id of the source module for this error.
module: str | None = None

# The name of the type in which this error is located at.
type: str | None = ""  # Unqualified, may be None

# The name of the function or member in which this error is located at.
function_or_member: str | None = ""  # Unqualified, may be None

# The line number related to this error within file.
line = 0  # -1 if unknown

# The column number related to this error with file.
column = 0  # -1 if unknown

# The end line number related to this error within file.
end_line = 0  # -1 if unknown

# The end column number related to this error with file.
end_column = 0  # -1 if unknown

# Either 'error' or 'note'
severity = ""

# The error message.
message = ""

# The error code.
code: ErrorCode | None = None

# If True, we should halt build after the file that generated this error.
blocker = False

# Only report this particular messages once per program.
only_once = False

# Do not remove duplicate copies of this message (ignored if only_once is True).
allow_dups = False

# Actual origin of the error message as tuple (path, line number, end line number)
# If end line number is unknown, use line number.
origin: tuple[str, Iterable[int]]

# Fine-grained incremental target where this was reported
target: str | None = None

# If True, don't show this message in output, but still record the error (needed
# by mypy daemon)
hidden = False

def __init__(
    self,
    import_ctx: list[tuple[str, int]],
    *,
    file: str,
    module: str | None,
    typ: str | None,
    function_or_member: str | None,
    line: int,
    column: int,
    end_line: int,
    end_column: int,
    severity: str,
    message: str,
    code: ErrorCode | None,
    blocker: bool,
    only_once: bool,
    allow_dups: bool,
    origin: tuple[str, Iterable[int]] | None = None,
    target: str | None = None,
    priority: int = 0,
) -&gt; None:
    self.import_ctx = import_ctx
    self.file = file
    self.module = module
    self.type = typ
    self.function_or_member = function_or_member
    self.line = line
    self.column = column
    self.end_line = end_line
    self.end_column = end_column
    self.severity = severity
    self.message = message
    self.code = code
    self.blocker = blocker
    self.only_once = only_once
    self.allow_dups = allow_dups
    self.origin = origin or (file, [line])
    self.target = target
    self.priority = priority


</t>
<t tx="ekr.20230831011819.1004"># Type used internally to represent errors:
#   (path, line, column, end_line, end_column, severity, message, allow_dups, code)
ErrorTuple: _TypeAlias = Tuple[
    Optional[str], int, int, int, int, str, str, bool, Optional[ErrorCode]
]


class ErrorWatcher:
    """Context manager that can be used to keep track of new errors recorded
    around a given operation.

    Errors maintain a stack of such watchers. The handler is called starting
    at the top of the stack, and is propagated down the stack unless filtered
    out by one of the ErrorWatcher instances.
    """

    @others
</t>
<t tx="ekr.20230831011819.1005">def __init__(
    self,
    errors: Errors,
    *,
    filter_errors: bool | Callable[[str, ErrorInfo], bool] = False,
    save_filtered_errors: bool = False,
):
    self.errors = errors
    self._has_new_errors = False
    self._filter = filter_errors
    self._filtered: list[ErrorInfo] | None = [] if save_filtered_errors else None

</t>
<t tx="ekr.20230831011819.1006">def __enter__(self) -&gt; ErrorWatcher:
    self.errors._watchers.append(self)
    return self

</t>
<t tx="ekr.20230831011819.1007">def __exit__(self, exc_type: object, exc_val: object, exc_tb: object) -&gt; Literal[False]:
    last = self.errors._watchers.pop()
    assert last == self
    return False

</t>
<t tx="ekr.20230831011819.1008">def on_error(self, file: str, info: ErrorInfo) -&gt; bool:
    """Handler called when a new error is recorded.

    The default implementation just sets the has_new_errors flag

    Return True to filter out the error, preventing it from being seen by other
    ErrorWatcher further down the stack and from being recorded by Errors
    """
    self._has_new_errors = True
    if isinstance(self._filter, bool):
        should_filter = self._filter
    elif callable(self._filter):
        should_filter = self._filter(file, info)
    else:
        raise AssertionError(f"invalid error filter: {type(self._filter)}")
    if should_filter and self._filtered is not None:
        self._filtered.append(info)

    return should_filter

</t>
<t tx="ekr.20230831011819.1009">def has_new_errors(self) -&gt; bool:
    return self._has_new_errors

</t>
<t tx="ekr.20230831011819.101">def _cache_dir_prefix(options: Options) -&gt; str:
    """Get current cache directory (or file if id is given)."""
    if options.bazel:
        # This is needed so the cache map works.
        return os.curdir
    cache_dir = options.cache_dir
    pyversion = options.python_version
    base = os.path.join(cache_dir, "%d.%d" % pyversion)
    return base


</t>
<t tx="ekr.20230831011819.1010">def filtered_errors(self) -&gt; list[ErrorInfo]:
    assert self._filtered is not None
    return self._filtered


</t>
<t tx="ekr.20230831011819.1011">class Errors:
    """Container for compile errors.

    This class generates and keeps tracks of compile errors and the
    current error context (nested imports).
    """

    @others
</t>
<t tx="ekr.20230831011819.1012"># Map from files to generated error messages. Is an OrderedDict so
# that it can be used to order messages based on the order the
# files were processed.
error_info_map: dict[str, list[ErrorInfo]]

# optimization for legacy codebases with many files with errors
has_blockers: set[str]

# Files that we have reported the errors for
flushed_files: set[str]

# Current error context: nested import context/stack, as a list of (path, line) pairs.
import_ctx: list[tuple[str, int]]

# Path name prefix that is removed from all paths, if set.
ignore_prefix: str | None = None

# Path to current file.
file: str = ""

# Ignore some errors on these lines of each file
# (path -&gt; line -&gt; error-codes)
ignored_lines: dict[str, dict[int, list[str]]]

# Lines that were skipped during semantic analysis e.g. due to ALWAYS_FALSE, MYPY_FALSE,
# or platform/version checks. Those lines would not be type-checked.
skipped_lines: dict[str, set[int]]

# Lines on which an error was actually ignored.
used_ignored_lines: dict[str, dict[int, list[str]]]

# Files where all errors should be ignored.
ignored_files: set[str]

# Collection of reported only_once messages.
only_once_messages: set[str]

# Set to True to show "In function "foo":" messages.
show_error_context: bool = False

# Set to True to show column numbers in error messages.
show_column_numbers: bool = False

# Set to True to show end line and end column in error messages.
# Ths implies `show_column_numbers`.
show_error_end: bool = False

# Set to True to show absolute file paths in error messages.
show_absolute_path: bool = False

# State for keeping track of the current fine-grained incremental mode target.
# (See mypy.server.update for more about targets.)
# Current module id.
target_module: str | None = None
scope: Scope | None = None

# Have we seen an import-related error so far? If yes, we filter out other messages
# in some cases to avoid reporting huge numbers of errors.
seen_import_error = False

_watchers: list[ErrorWatcher] = []

def __init__(
    self,
    options: Options,
    *,
    read_source: Callable[[str], list[str] | None] | None = None,
    hide_error_codes: bool | None = None,
) -&gt; None:
    self.options = options
    self.hide_error_codes = (
        hide_error_codes if hide_error_codes is not None else options.hide_error_codes
    )
    # We use fscache to read source code when showing snippets.
    self.read_source = read_source
    self.initialize()

</t>
<t tx="ekr.20230831011819.1013">def initialize(self) -&gt; None:
    self.error_info_map = {}
    self.flushed_files = set()
    self.import_ctx = []
    self.function_or_member = [None]
    self.ignored_lines = {}
    self.skipped_lines = {}
    self.used_ignored_lines = defaultdict(lambda: defaultdict(list))
    self.ignored_files = set()
    self.only_once_messages = set()
    self.has_blockers = set()
    self.scope = None
    self.target_module = None
    self.seen_import_error = False

</t>
<t tx="ekr.20230831011819.1014">def reset(self) -&gt; None:
    self.initialize()

</t>
<t tx="ekr.20230831011819.1015">def set_ignore_prefix(self, prefix: str) -&gt; None:
    """Set path prefix that will be removed from all paths."""
    prefix = os.path.normpath(prefix)
    # Add separator to the end, if not given.
    if os.path.basename(prefix) != "":
        prefix += os.sep
    self.ignore_prefix = prefix

</t>
<t tx="ekr.20230831011819.1016">def simplify_path(self, file: str) -&gt; str:
    if self.options.show_absolute_path:
        return os.path.abspath(file)
    else:
        file = os.path.normpath(file)
        return remove_path_prefix(file, self.ignore_prefix)

</t>
<t tx="ekr.20230831011819.1017">def set_file(
    self, file: str, module: str | None, options: Options, scope: Scope | None = None
) -&gt; None:
    """Set the path and module id of the current file."""
    # The path will be simplified later, in render_messages. That way
    #  * 'file' is always a key that uniquely identifies a source file
    #    that mypy read (simplified paths might not be unique); and
    #  * we only have to simplify in one place, while still supporting
    #    reporting errors for files other than the one currently being
    #    processed.
    self.file = file
    self.target_module = module
    self.scope = scope
    self.options = options

</t>
<t tx="ekr.20230831011819.1018">def set_file_ignored_lines(
    self, file: str, ignored_lines: dict[int, list[str]], ignore_all: bool = False
) -&gt; None:
    self.ignored_lines[file] = ignored_lines
    if ignore_all:
        self.ignored_files.add(file)

</t>
<t tx="ekr.20230831011819.1019">def set_skipped_lines(self, file: str, skipped_lines: set[int]) -&gt; None:
    self.skipped_lines[file] = skipped_lines

</t>
<t tx="ekr.20230831011819.102">def add_catch_all_gitignore(target_dir: str) -&gt; None:
    """Add catch-all .gitignore to an existing directory.

    No-op if the .gitignore already exists.
    """
    gitignore = os.path.join(target_dir, ".gitignore")
    try:
        with open(gitignore, "x") as f:
            print("# Automatically created by mypy", file=f)
            print("*", file=f)
    except FileExistsError:
        pass


</t>
<t tx="ekr.20230831011819.1020">def current_target(self) -&gt; str | None:
    """Retrieves the current target from the associated scope.

    If there is no associated scope, use the target module."""
    if self.scope is not None:
        return self.scope.current_target()
    return self.target_module

</t>
<t tx="ekr.20230831011819.1021">def current_module(self) -&gt; str | None:
    return self.target_module

</t>
<t tx="ekr.20230831011819.1022">def import_context(self) -&gt; list[tuple[str, int]]:
    """Return a copy of the import context."""
    return self.import_ctx.copy()

</t>
<t tx="ekr.20230831011819.1023">def set_import_context(self, ctx: list[tuple[str, int]]) -&gt; None:
    """Replace the entire import context with a new value."""
    self.import_ctx = ctx.copy()

</t>
<t tx="ekr.20230831011819.1024">def report(
    self,
    line: int,
    column: int | None,
    message: str,
    code: ErrorCode | None = None,
    *,
    blocker: bool = False,
    severity: str = "error",
    file: str | None = None,
    only_once: bool = False,
    allow_dups: bool = False,
    origin_span: Iterable[int] | None = None,
    offset: int = 0,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    """Report message at the given line using the current error context.

    Args:
        line: line number of error
        column: column number of error
        message: message to report
        code: error code (defaults to 'misc'; not shown for notes)
        blocker: if True, don't continue analysis after this error
        severity: 'error' or 'note'
        file: if non-None, override current file as context
        only_once: if True, only report this exact message once per build
        allow_dups: if True, allow duplicate copies of this message (ignored if only_once)
        origin_span: if non-None, override current context as origin
                     (type: ignores have effect here)
        end_line: if non-None, override current context as end
    """
    if self.scope:
        type = self.scope.current_type_name()
        if self.scope.ignored &gt; 0:
            type = None  # Omit type context if nested function
        function = self.scope.current_function_name()
    else:
        type = None
        function = None

    if column is None:
        column = -1
    if end_column is None:
        if column == -1:
            end_column = -1
        else:
            end_column = column + 1

    if file is None:
        file = self.file
    if offset:
        message = " " * offset + message

    if origin_span is None:
        origin_span = [line]

    if end_line is None:
        end_line = line

    code = code or (codes.MISC if not blocker else None)

    info = ErrorInfo(
        import_ctx=self.import_context(),
        file=file,
        module=self.current_module(),
        typ=type,
        function_or_member=function,
        line=line,
        column=column,
        end_line=end_line,
        end_column=end_column,
        severity=severity,
        message=message,
        code=code,
        blocker=blocker,
        only_once=only_once,
        allow_dups=allow_dups,
        origin=(self.file, origin_span),
        target=self.current_target(),
    )
    self.add_error_info(info)

</t>
<t tx="ekr.20230831011819.1025">def _add_error_info(self, file: str, info: ErrorInfo) -&gt; None:
    assert file not in self.flushed_files
    # process the stack of ErrorWatchers before modifying any internal state
    # in case we need to filter out the error entirely
    if self._filter_error(file, info):
        return
    if file not in self.error_info_map:
        self.error_info_map[file] = []
    self.error_info_map[file].append(info)
    if info.blocker:
        self.has_blockers.add(file)
    if info.code is IMPORT:
        self.seen_import_error = True

</t>
<t tx="ekr.20230831011819.1026">def _filter_error(self, file: str, info: ErrorInfo) -&gt; bool:
    """
    process ErrorWatcher stack from top to bottom,
    stopping early if error needs to be filtered out
    """
    i = len(self._watchers)
    while i &gt; 0:
        i -= 1
        w = self._watchers[i]
        if w.on_error(file, info):
            return True
    return False

</t>
<t tx="ekr.20230831011819.1027">def add_error_info(self, info: ErrorInfo) -&gt; None:
    file, lines = info.origin
    # process the stack of ErrorWatchers before modifying any internal state
    # in case we need to filter out the error entirely
    # NB: we need to do this both here and in _add_error_info, otherwise we
    # might incorrectly update the sets of ignored or only_once messages
    if self._filter_error(file, info):
        return
    if not info.blocker:  # Blockers cannot be ignored
        if file in self.ignored_lines:
            # Check each line in this context for "type: ignore" comments.
            # line == end_line for most nodes, so we only loop once.
            for scope_line in lines:
                if self.is_ignored_error(scope_line, info, self.ignored_lines[file]):
                    # Annotation requests us to ignore all errors on this line.
                    self.used_ignored_lines[file][scope_line].append(
                        (info.code or codes.MISC).code
                    )
                    return
        if file in self.ignored_files:
            return
    if info.only_once:
        if info.message in self.only_once_messages:
            return
        self.only_once_messages.add(info.message)
    if (
        self.seen_import_error
        and info.code not in (IMPORT, IMPORT_UNTYPED, IMPORT_NOT_FOUND)
        and self.has_many_errors()
    ):
        # Missing stubs can easily cause thousands of errors about
        # Any types, especially when upgrading to mypy 0.900,
        # which no longer bundles third-party library stubs. Avoid
        # showing too many errors to make it easier to see
        # import-related errors.
        info.hidden = True
        self.report_hidden_errors(info)
    self._add_error_info(file, info)
    ignored_codes = self.ignored_lines.get(file, {}).get(info.line, [])
    if ignored_codes and info.code:
        # Something is ignored on the line, but not this error, so maybe the error
        # code is incorrect.
        msg = f'Error code "{info.code.code}" not covered by "type: ignore" comment'
        if info.code in original_error_codes:
            # If there seems to be a "type: ignore" with a stale error
            # code, report a more specific note.
            old_code = original_error_codes[info.code].code
            if old_code in ignored_codes:
                msg = (
                    f'Error code changed to {info.code.code}; "type: ignore" comment '
                    + "may be out of date"
                )
        note = ErrorInfo(
            import_ctx=info.import_ctx,
            file=info.file,
            module=info.module,
            typ=info.type,
            function_or_member=info.function_or_member,
            line=info.line,
            column=info.column,
            end_line=info.end_line,
            end_column=info.end_column,
            severity="note",
            message=msg,
            code=None,
            blocker=False,
            only_once=False,
            allow_dups=False,
        )
        self._add_error_info(file, note)
    if (
        self.options.show_error_code_links
        and not self.options.hide_error_codes
        and info.code is not None
        and info.code not in HIDE_LINK_CODES
    ):
        message = f"See {BASE_RTD_URL}-{info.code.code} for more info"
        if message in self.only_once_messages:
            return
        self.only_once_messages.add(message)
        info = ErrorInfo(
            import_ctx=info.import_ctx,
            file=info.file,
            module=info.module,
            typ=info.type,
            function_or_member=info.function_or_member,
            line=info.line,
            column=info.column,
            end_line=info.end_line,
            end_column=info.end_column,
            severity="note",
            message=message,
            code=info.code,
            blocker=False,
            only_once=True,
            allow_dups=False,
            priority=20,
        )
        self._add_error_info(file, info)

</t>
<t tx="ekr.20230831011819.1028">def has_many_errors(self) -&gt; bool:
    if self.options.many_errors_threshold &lt; 0:
        return False
    if len(self.error_info_map) &gt;= self.options.many_errors_threshold:
        return True
    if (
        sum(len(errors) for errors in self.error_info_map.values())
        &gt;= self.options.many_errors_threshold
    ):
        return True
    return False

</t>
<t tx="ekr.20230831011819.1029">def report_hidden_errors(self, info: ErrorInfo) -&gt; None:
    message = (
        "(Skipping most remaining errors due to unresolved imports or missing stubs; "
        + "fix these first)"
    )
    if message in self.only_once_messages:
        return
    self.only_once_messages.add(message)
    new_info = ErrorInfo(
        import_ctx=info.import_ctx,
        file=info.file,
        module=info.module,
        typ=None,
        function_or_member=None,
        line=info.line,
        column=info.column,
        end_line=info.end_line,
        end_column=info.end_column,
        severity="note",
        message=message,
        code=None,
        blocker=False,
        only_once=True,
        allow_dups=False,
        origin=info.origin,
        target=info.target,
    )
    self._add_error_info(info.origin[0], new_info)

</t>
<t tx="ekr.20230831011819.103">def exclude_from_backups(target_dir: str) -&gt; None:
    """Exclude the directory from various archives and backups supporting CACHEDIR.TAG.

    If the CACHEDIR.TAG file exists the function is a no-op.
    """
    cachedir_tag = os.path.join(target_dir, "CACHEDIR.TAG")
    try:
        with open(cachedir_tag, "x") as f:
            f.write(
                """Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag automatically created by mypy.
# For information about cache directory tags see https://bford.info/cachedir/
"""
            )
    except FileExistsError:
        pass


</t>
<t tx="ekr.20230831011819.1030">def is_ignored_error(self, line: int, info: ErrorInfo, ignores: dict[int, list[str]]) -&gt; bool:
    if info.blocker:
        # Blocking errors can never be ignored
        return False
    if info.code and not self.is_error_code_enabled(info.code):
        return True
    if line not in ignores:
        return False
    if not ignores[line]:
        # Empty list means that we ignore all errors
        return True
    if info.code and self.is_error_code_enabled(info.code):
        return (
            info.code.code in ignores[line]
            or info.code.sub_code_of is not None
            and info.code.sub_code_of.code in ignores[line]
        )
    return False

</t>
<t tx="ekr.20230831011819.1031">def is_error_code_enabled(self, error_code: ErrorCode) -&gt; bool:
    if self.options:
        current_mod_disabled = self.options.disabled_error_codes
        current_mod_enabled = self.options.enabled_error_codes
    else:
        current_mod_disabled = set()
        current_mod_enabled = set()

    if error_code in current_mod_disabled:
        return False
    elif error_code in current_mod_enabled:
        return True
    elif error_code.sub_code_of is not None and error_code.sub_code_of in current_mod_disabled:
        return False
    else:
        return error_code.default_enabled

</t>
<t tx="ekr.20230831011819.1032">def clear_errors_in_targets(self, path: str, targets: set[str]) -&gt; None:
    """Remove errors in specific fine-grained targets within a file."""
    if path in self.error_info_map:
        new_errors = []
        has_blocker = False
        for info in self.error_info_map[path]:
            if info.target not in targets:
                new_errors.append(info)
                has_blocker |= info.blocker
            elif info.only_once:
                self.only_once_messages.remove(info.message)
        self.error_info_map[path] = new_errors
        if not has_blocker and path in self.has_blockers:
            self.has_blockers.remove(path)

</t>
<t tx="ekr.20230831011819.1033">def generate_unused_ignore_errors(self, file: str) -&gt; None:
    if (
        is_typeshed_file(self.options.abs_custom_typeshed_dir if self.options else None, file)
        or file in self.ignored_files
    ):
        return
    ignored_lines = self.ignored_lines[file]
    used_ignored_lines = self.used_ignored_lines[file]
    for line, ignored_codes in ignored_lines.items():
        if line in self.skipped_lines[file]:
            continue
        if codes.UNUSED_IGNORE.code in ignored_codes:
            continue
        used_ignored_codes = used_ignored_lines[line]
        unused_ignored_codes = set(ignored_codes) - set(used_ignored_codes)
        # `ignore` is used
        if not ignored_codes and used_ignored_codes:
            continue
        # All codes appearing in `ignore[...]` are used
        if ignored_codes and not unused_ignored_codes:
            continue
        # Display detail only when `ignore[...]` specifies more than one error code
        unused_codes_message = ""
        if len(ignored_codes) &gt; 1 and unused_ignored_codes:
            unused_codes_message = f"[{', '.join(sorted(unused_ignored_codes))}]"
        message = f'Unused "type: ignore{unused_codes_message}" comment'
        for unused in unused_ignored_codes:
            narrower = set(used_ignored_codes) &amp; codes.sub_code_map[unused]
            if narrower:
                message += f", use narrower [{', '.join(narrower)}] instead of [{unused}] code"
        # Don't use report since add_error_info will ignore the error!
        info = ErrorInfo(
            import_ctx=self.import_context(),
            file=file,
            module=self.current_module(),
            typ=None,
            function_or_member=None,
            line=line,
            column=-1,
            end_line=line,
            end_column=-1,
            severity="error",
            message=message,
            code=codes.UNUSED_IGNORE,
            blocker=False,
            only_once=False,
            allow_dups=False,
        )
        self._add_error_info(file, info)

</t>
<t tx="ekr.20230831011819.1034">def generate_ignore_without_code_errors(
    self, file: str, is_warning_unused_ignores: bool
) -&gt; None:
    if (
        is_typeshed_file(self.options.abs_custom_typeshed_dir if self.options else None, file)
        or file in self.ignored_files
    ):
        return

    used_ignored_lines = self.used_ignored_lines[file]

    # If the whole file is ignored, ignore it.
    if used_ignored_lines:
        _, used_codes = min(used_ignored_lines.items())
        if codes.FILE.code in used_codes:
            return

    for line, ignored_codes in self.ignored_lines[file].items():
        if ignored_codes:
            continue

        # If the ignore is itself unused and that would be warned about, let
        # that error stand alone
        if is_warning_unused_ignores and not used_ignored_lines[line]:
            continue

        codes_hint = ""
        ignored_codes = sorted(set(used_ignored_lines[line]))
        if ignored_codes:
            codes_hint = f' (consider "type: ignore[{", ".join(ignored_codes)}]" instead)'

        message = f'"type: ignore" comment without error code{codes_hint}'
        # Don't use report since add_error_info will ignore the error!
        info = ErrorInfo(
            import_ctx=self.import_context(),
            file=file,
            module=self.current_module(),
            typ=None,
            function_or_member=None,
            line=line,
            column=-1,
            end_line=line,
            end_column=-1,
            severity="error",
            message=message,
            code=codes.IGNORE_WITHOUT_CODE,
            blocker=False,
            only_once=False,
            allow_dups=False,
        )
        self._add_error_info(file, info)

</t>
<t tx="ekr.20230831011819.1035">def num_messages(self) -&gt; int:
    """Return the number of generated messages."""
    return sum(len(x) for x in self.error_info_map.values())

</t>
<t tx="ekr.20230831011819.1036">def is_errors(self) -&gt; bool:
    """Are there any generated messages?"""
    return bool(self.error_info_map)

</t>
<t tx="ekr.20230831011819.1037">def is_blockers(self) -&gt; bool:
    """Are the any errors that are blockers?"""
    return bool(self.has_blockers)

</t>
<t tx="ekr.20230831011819.1038">def blocker_module(self) -&gt; str | None:
    """Return the module with a blocking error, or None if not possible."""
    for path in self.has_blockers:
        for err in self.error_info_map[path]:
            if err.blocker:
                return err.module
    return None

</t>
<t tx="ekr.20230831011819.1039">def is_errors_for_file(self, file: str) -&gt; bool:
    """Are there any errors for the given file?"""
    return file in self.error_info_map

</t>
<t tx="ekr.20230831011819.104">def create_metastore(options: Options) -&gt; MetadataStore:
    """Create the appropriate metadata store."""
    if options.sqlite_cache:
        mds: MetadataStore = SqliteMetadataStore(_cache_dir_prefix(options))
    else:
        mds = FilesystemMetadataStore(_cache_dir_prefix(options))
    return mds


</t>
<t tx="ekr.20230831011819.1040">def prefer_simple_messages(self) -&gt; bool:
    """Should we generate simple/fast error messages?

    Return True if errors are not shown to user, i.e. errors are ignored
    or they are collected for internal use only.

    If True, we should prefer to generate a simple message quickly.
    All normal errors should still be reported.
    """
    if self.file in self.ignored_files:
        # Errors ignored, so no point generating fancy messages
        return True
    for _watcher in self._watchers:
        if _watcher._filter is True and _watcher._filtered is None:
            # Errors are filtered
            return True
    return False

</t>
<t tx="ekr.20230831011819.1041">def raise_error(self, use_stdout: bool = True) -&gt; NoReturn:
    """Raise a CompileError with the generated messages.

    Render the messages suitable for displaying.
    """
    # self.new_messages() will format all messages that haven't already
    # been returned from a file_messages() call.
    raise CompileError(
        self.new_messages(), use_stdout=use_stdout, module_with_blocker=self.blocker_module()
    )

</t>
<t tx="ekr.20230831011819.1042">def format_messages(
    self, error_info: list[ErrorInfo], source_lines: list[str] | None
) -&gt; list[str]:
    """Return a string list that represents the error messages.

    Use a form suitable for displaying to the user. If self.pretty
    is True also append a relevant trimmed source code line (only for
    severity 'error').
    """
    a: list[str] = []
    error_info = [info for info in error_info if not info.hidden]
    errors = self.render_messages(self.sort_messages(error_info))
    errors = self.remove_duplicates(errors)
    for (
        file,
        line,
        column,
        end_line,
        end_column,
        severity,
        message,
        allow_dups,
        code,
    ) in errors:
        s = ""
        if file is not None:
            if self.options.show_column_numbers and line &gt;= 0 and column &gt;= 0:
                srcloc = f"{file}:{line}:{1 + column}"
                if self.options.show_error_end and end_line &gt;= 0 and end_column &gt;= 0:
                    srcloc += f":{end_line}:{end_column}"
            elif line &gt;= 0:
                srcloc = f"{file}:{line}"
            else:
                srcloc = file
            s = f"{srcloc}: {severity}: {message}"
        else:
            s = message
        if (
            not self.hide_error_codes
            and code
            and (severity != "note" or code in SHOW_NOTE_CODES)
        ):
            # If note has an error code, it is related to a previous error. Avoid
            # displaying duplicate error codes.
            s = f"{s}  [{code.code}]"
        a.append(s)
        if self.options.pretty:
            # Add source code fragment and a location marker.
            if severity == "error" and source_lines and line &gt; 0:
                source_line = source_lines[line - 1]
                source_line_expanded = source_line.expandtabs()
                if column &lt; 0:
                    # Something went wrong, take first non-empty column.
                    column = len(source_line) - len(source_line.lstrip())

                # Shifts column after tab expansion
                column = len(source_line[:column].expandtabs())
                end_column = len(source_line[:end_column].expandtabs())

                # Note, currently coloring uses the offset to detect source snippets,
                # so these offsets should not be arbitrary.
                a.append(" " * DEFAULT_SOURCE_OFFSET + source_line_expanded)
                marker = "^"
                if end_line == line and end_column &gt; column:
                    marker = f'^{"~" * (end_column - column - 1)}'
                a.append(" " * (DEFAULT_SOURCE_OFFSET + column) + marker)
    return a

</t>
<t tx="ekr.20230831011819.1043">def file_messages(self, path: str) -&gt; list[str]:
    """Return a string list of new error messages from a given file.

    Use a form suitable for displaying to the user.
    """
    if path not in self.error_info_map:
        return []
    self.flushed_files.add(path)
    source_lines = None
    if self.options.pretty:
        assert self.read_source
        source_lines = self.read_source(path)
    return self.format_messages(self.error_info_map[path], source_lines)

</t>
<t tx="ekr.20230831011819.1044">def new_messages(self) -&gt; list[str]:
    """Return a string list of new error messages.

    Use a form suitable for displaying to the user.
    Errors from different files are ordered based on the order in which
    they first generated an error.
    """
    msgs = []
    for path in self.error_info_map.keys():
        if path not in self.flushed_files:
            msgs.extend(self.file_messages(path))
    return msgs

</t>
<t tx="ekr.20230831011819.1045">def targets(self) -&gt; set[str]:
    """Return a set of all targets that contain errors."""
    # TODO: Make sure that either target is always defined or that not being defined
    #       is okay for fine-grained incremental checking.
    return {
        info.target for errs in self.error_info_map.values() for info in errs if info.target
    }

</t>
<t tx="ekr.20230831011819.1046">def render_messages(self, errors: list[ErrorInfo]) -&gt; list[ErrorTuple]:
    """Translate the messages into a sequence of tuples.

    Each tuple is of form (path, line, col, severity, message, allow_dups, code).
    The rendered sequence includes information about error contexts.
    The path item may be None. If the line item is negative, the
    line number is not defined for the tuple.
    """
    result: list[ErrorTuple] = []
    prev_import_context: list[tuple[str, int]] = []
    prev_function_or_member: str | None = None
    prev_type: str | None = None

    for e in errors:
        # Report module import context, if different from previous message.
        if not self.options.show_error_context:
            pass
        elif e.import_ctx != prev_import_context:
            last = len(e.import_ctx) - 1
            i = last
            while i &gt;= 0:
                path, line = e.import_ctx[i]
                fmt = "{}:{}: note: In module imported here"
                if i &lt; last:
                    fmt = "{}:{}: note: ... from here"
                if i &gt; 0:
                    fmt += ","
                else:
                    fmt += ":"
                # Remove prefix to ignore from path (if present) to
                # simplify path.
                path = remove_path_prefix(path, self.ignore_prefix)
                result.append(
                    (None, -1, -1, -1, -1, "note", fmt.format(path, line), e.allow_dups, None)
                )
                i -= 1

        file = self.simplify_path(e.file)

        # Report context within a source file.
        if not self.options.show_error_context:
            pass
        elif e.function_or_member != prev_function_or_member or e.type != prev_type:
            if e.function_or_member is None:
                if e.type is None:
                    result.append(
                        (file, -1, -1, -1, -1, "note", "At top level:", e.allow_dups, None)
                    )
                else:
                    result.append(
                        (
                            file,
                            -1,
                            -1,
                            -1,
                            -1,
                            "note",
                            f'In class "{e.type}":',
                            e.allow_dups,
                            None,
                        )
                    )
            else:
                if e.type is None:
                    result.append(
                        (
                            file,
                            -1,
                            -1,
                            -1,
                            -1,
                            "note",
                            f'In function "{e.function_or_member}":',
                            e.allow_dups,
                            None,
                        )
                    )
                else:
                    result.append(
                        (
                            file,
                            -1,
                            -1,
                            -1,
                            -1,
                            "note",
                            'In member "{}" of class "{}":'.format(
                                e.function_or_member, e.type
                            ),
                            e.allow_dups,
                            None,
                        )
                    )
        elif e.type != prev_type:
            if e.type is None:
                result.append(
                    (file, -1, -1, -1, -1, "note", "At top level:", e.allow_dups, None)
                )
            else:
                result.append(
                    (file, -1, -1, -1, -1, "note", f'In class "{e.type}":', e.allow_dups, None)
                )

        if isinstance(e.message, ErrorMessage):
            result.append(
                (
                    file,
                    e.line,
                    e.column,
                    e.end_line,
                    e.end_column,
                    e.severity,
                    e.message.value,
                    e.allow_dups,
                    e.code,
                )
            )
        else:
            result.append(
                (
                    file,
                    e.line,
                    e.column,
                    e.end_line,
                    e.end_column,
                    e.severity,
                    e.message,
                    e.allow_dups,
                    e.code,
                )
            )

        prev_import_context = e.import_ctx
        prev_function_or_member = e.function_or_member
        prev_type = e.type

    return result

</t>
<t tx="ekr.20230831011819.1047">def sort_messages(self, errors: list[ErrorInfo]) -&gt; list[ErrorInfo]:
    """Sort an array of error messages locally by line number.

    I.e., sort a run of consecutive messages with the same
    context by line number, but otherwise retain the general
    ordering of the messages.
    """
    result: list[ErrorInfo] = []
    i = 0
    while i &lt; len(errors):
        i0 = i
        # Find neighbouring errors with the same context and file.
        while (
            i + 1 &lt; len(errors)
            and errors[i + 1].import_ctx == errors[i].import_ctx
            and errors[i + 1].file == errors[i].file
        ):
            i += 1
        i += 1

        # Sort the errors specific to a file according to line number and column.
        a = sorted(errors[i0:i], key=lambda x: (x.line, x.column))
        a = self.sort_within_context(a)
        result.extend(a)
    return result

</t>
<t tx="ekr.20230831011819.1048">def sort_within_context(self, errors: list[ErrorInfo]) -&gt; list[ErrorInfo]:
    """For the same location decide which messages to show first/last.

    Currently, we only compare within the same error code, to decide the
    order of various additional notes.
    """
    result = []
    i = 0
    while i &lt; len(errors):
        i0 = i
        # Find neighbouring errors with the same position and error code.
        while (
            i + 1 &lt; len(errors)
            and errors[i + 1].line == errors[i].line
            and errors[i + 1].column == errors[i].column
            and errors[i + 1].end_line == errors[i].end_line
            and errors[i + 1].end_column == errors[i].end_column
            and errors[i + 1].code == errors[i].code
        ):
            i += 1
        i += 1

        # Sort the messages specific to a given error by priority.
        a = sorted(errors[i0:i], key=lambda x: x.priority)
        result.extend(a)
    return result

</t>
<t tx="ekr.20230831011819.1049">def remove_duplicates(self, errors: list[ErrorTuple]) -&gt; list[ErrorTuple]:
    """Remove duplicates from a sorted error list."""
    res: list[ErrorTuple] = []
    i = 0
    while i &lt; len(errors):
        dup = False
        # Use slightly special formatting for member conflicts reporting.
        conflicts_notes = False
        j = i - 1
        # Find duplicates, unless duplicates are allowed.
        if not errors[i][7]:
            while j &gt;= 0 and errors[j][0] == errors[i][0]:
                if errors[j][6].strip() == "Got:":
                    conflicts_notes = True
                j -= 1
            j = i - 1
            while j &gt;= 0 and errors[j][0] == errors[i][0] and errors[j][1] == errors[i][1]:
                if (
                    errors[j][5] == errors[i][5]
                    and
                    # Allow duplicate notes in overload conflicts reporting.
                    not (
                        (errors[i][5] == "note" and errors[i][6].strip() in allowed_duplicates)
                        or (errors[i][6].strip().startswith("def ") and conflicts_notes)
                    )
                    and errors[j][6] == errors[i][6]
                ):  # ignore column
                    dup = True
                    break
                j -= 1
        if not dup:
            res.append(errors[i])
        i += 1
    return res


</t>
<t tx="ekr.20230831011819.105">def get_cache_names(id: str, path: str, options: Options) -&gt; tuple[str, str, str | None]:
    """Return the file names for the cache files.

    Args:
      id: module ID
      path: module path
      cache_dir: cache directory
      pyversion: Python version (major, minor)

    Returns:
      A tuple with the file names to be used for the meta JSON, the
      data JSON, and the fine-grained deps JSON, respectively.
    """
    if options.cache_map:
        pair = options.cache_map.get(normpath(path, options))
    else:
        pair = None
    if pair is not None:
        # The cache map paths were specified relative to the base directory,
        # but the filesystem metastore APIs operates relative to the cache
        # prefix directory.
        # Solve this by rewriting the paths as relative to the root dir.
        # This only makes sense when using the filesystem backed cache.
        root = _cache_dir_prefix(options)
        return (os.path.relpath(pair[0], root), os.path.relpath(pair[1], root), None)
    prefix = os.path.join(*id.split("."))
    is_package = os.path.basename(path).startswith("__init__.py")
    if is_package:
        prefix = os.path.join(prefix, "__init__")

    deps_json = None
    if options.cache_fine_grained:
        deps_json = prefix + ".deps.json"
    return (prefix + ".meta.json", prefix + ".data.json", deps_json)


</t>
<t tx="ekr.20230831011819.1050">class CompileError(Exception):
    """Exception raised when there is a compile error.

    It can be a parse, semantic analysis, type check or other
    compilation-related error.

    CompileErrors raised from an errors object carry all of the
    messages that have not been reported out by error streaming.
    This is patched up by build.build to contain either all error
    messages (if errors were streamed) or none (if they were not).

    """

    @others
</t>
<t tx="ekr.20230831011819.1051">messages: list[str]
use_stdout = False
# Can be set in case there was a module with a blocking error
module_with_blocker: str | None = None

def __init__(
    self, messages: list[str], use_stdout: bool = False, module_with_blocker: str | None = None
) -&gt; None:
    super().__init__("\n".join(messages))
    self.messages = messages
    self.use_stdout = use_stdout
    self.module_with_blocker = module_with_blocker


</t>
<t tx="ekr.20230831011819.1052">def remove_path_prefix(path: str, prefix: str | None) -&gt; str:
    """If path starts with prefix, return copy of path with the prefix removed.
    Otherwise, return path. If path is None, return None.
    """
    if prefix is not None and path.startswith(prefix):
        return path[len(prefix) :]
    else:
        return path


</t>
<t tx="ekr.20230831011819.1053">def report_internal_error(
    err: Exception,
    file: str | None,
    line: int,
    errors: Errors,
    options: Options,
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
) -&gt; NoReturn:
    """Report internal error and exit.

    This optionally starts pdb or shows a traceback.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr
    # Dump out errors so far, they often provide a clue.
    # But catch unexpected errors rendering them.
    try:
        for msg in errors.new_messages():
            print(msg)
    except Exception as e:
        print("Failed to dump errors:", repr(e), file=stderr)

    # Compute file:line prefix for official-looking error messages.
    if file:
        if line:
            prefix = f"{file}:{line}: "
        else:
            prefix = f"{file}: "
    else:
        prefix = ""

    # Print "INTERNAL ERROR" message.
    print(
        f"{prefix}error: INTERNAL ERROR --",
        "Please try using mypy master on GitHub:\n"
        "https://mypy.readthedocs.io/en/stable/common_issues.html"
        "#using-a-development-mypy-build",
        file=stderr,
    )
    if options.show_traceback:
        print("Please report a bug at https://github.com/python/mypy/issues", file=stderr)
    else:
        print(
            "If this issue continues with mypy master, "
            "please report a bug at https://github.com/python/mypy/issues",
            file=stderr,
        )
    print(f"version: {mypy_version}", file=stderr)

    # If requested, drop into pdb. This overrides show_tb.
    if options.pdb:
        print("Dropping into pdb", file=stderr)
        import pdb

        pdb.post_mortem(sys.exc_info()[2])

    # If requested, print traceback, else print note explaining how to get one.
    if options.raise_exceptions:
        raise err
    if not options.show_traceback:
        if not options.pdb:
            print(
                "{}: note: please use --show-traceback to print a traceback "
                "when reporting a bug".format(prefix),
                file=stderr,
            )
    else:
        tb = traceback.extract_stack()[:-2]
        tb2 = traceback.extract_tb(sys.exc_info()[2])
        print("Traceback (most recent call last):")
        for s in traceback.format_list(tb + tb2):
            print(s.rstrip("\n"))
        print(f"{type(err).__name__}: {err}", file=stdout)
        print(f"{prefix}: note: use --pdb to drop into pdb", file=stderr)

    # Exit.  The caller has nothing more to say.
    # We use exit code 2 to signal that this is no ordinary error.
    raise SystemExit(2)
</t>
<t tx="ekr.20230831011819.1054">@path mypy
"""

Evaluate an expression.

Used by stubtest; in a separate file because things break if we don't
put it in a mypyc-compiled file.

"""
&lt;&lt; evalexpr.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1056">import ast
from typing import Final

import mypy.nodes
from mypy.visitor import ExpressionVisitor

UNKNOWN = object()


</t>
<t tx="ekr.20230831011819.1057">class _NodeEvaluator(ExpressionVisitor[object]):
    @others
</t>
<t tx="ekr.20230831011819.1058">def visit_int_expr(self, o: mypy.nodes.IntExpr) -&gt; int:
    return o.value

</t>
<t tx="ekr.20230831011819.1059">def visit_str_expr(self, o: mypy.nodes.StrExpr) -&gt; str:
    return o.value

</t>
<t tx="ekr.20230831011819.106">def find_cache_meta(id: str, path: str, manager: BuildManager) -&gt; CacheMeta | None:
    """Find cache data for a module.

    Args:
      id: module ID
      path: module path
      manager: the build manager (for pyversion, log/trace, and build options)

    Returns:
      A CacheMeta instance if the cache data was found and appears
      valid; otherwise None.
    """
    # TODO: May need to take more build options into account
    meta_json, data_json, _ = get_cache_names(id, path, manager.options)
    manager.trace(f"Looking for {id} at {meta_json}")
    t0 = time.time()
    meta = _load_json_file(
        meta_json, manager, log_success=f"Meta {id} ", log_error=f"Could not load cache for {id}: "
    )
    t1 = time.time()
    if meta is None:
        return None
    if not isinstance(meta, dict):
        manager.log(f"Could not load cache for {id}: meta cache is not a dict: {repr(meta)}")
        return None
    m = cache_meta_from_dict(meta, data_json)
    t2 = time.time()
    manager.add_stats(
        load_meta_time=t2 - t0, load_meta_load_time=t1 - t0, load_meta_from_dict_time=t2 - t1
    )

    # Don't check for path match, that is dealt with in validate_meta().
    #
    # TODO: these `type: ignore`s wouldn't be necessary
    # if the type annotations for CacheMeta were more accurate
    # (all of these attributes can be `None`)
    if (
        m.id != id
        or m.mtime is None  # type: ignore[redundant-expr]
        or m.size is None  # type: ignore[redundant-expr]
        or m.dependencies is None  # type: ignore[redundant-expr]
        or m.data_mtime is None
    ):
        manager.log(f"Metadata abandoned for {id}: attributes are missing")
        return None

    # Ignore cache if generated by an older mypy version.
    if (
        (m.version_id != manager.version_id and not manager.options.skip_version_check)
        or m.options is None
        or len(m.dependencies) + len(m.suppressed) != len(m.dep_prios)
        or len(m.dependencies) + len(m.suppressed) != len(m.dep_lines)
    ):
        manager.log(f"Metadata abandoned for {id}: new attributes are missing")
        return None

    # Ignore cache if (relevant) options aren't the same.
    # Note that it's fine to mutilate cached_options since it's only used here.
    cached_options = m.options
    current_options = manager.options.clone_for_module(id).select_options_affecting_cache()
    if manager.options.skip_version_check:
        # When we're lax about version we're also lax about platform.
        cached_options["platform"] = current_options["platform"]
    if "debug_cache" in cached_options:
        # Older versions included debug_cache, but it's silly to compare it.
        del cached_options["debug_cache"]
    if cached_options != current_options:
        manager.log(f"Metadata abandoned for {id}: options differ")
        if manager.options.verbosity &gt;= 2:
            for key in sorted(set(cached_options) | set(current_options)):
                if cached_options.get(key) != current_options.get(key):
                    manager.trace(
                        "    {}: {} != {}".format(
                            key, cached_options.get(key), current_options.get(key)
                        )
                    )
        return None
    if manager.old_plugins_snapshot and manager.plugins_snapshot:
        # Check if plugins are still the same.
        if manager.plugins_snapshot != manager.old_plugins_snapshot:
            manager.log(f"Metadata abandoned for {id}: plugins differ")
            return None
    # So that plugins can return data with tuples in it without
    # things silently always invalidating modules, we round-trip
    # the config data. This isn't beautiful.
    plugin_data = json.loads(
        json.dumps(manager.plugin.report_config_data(ReportConfigContext(id, path, is_check=True)))
    )
    if m.plugin_data != plugin_data:
        manager.log(f"Metadata abandoned for {id}: plugin configuration differs")
        return None

    manager.add_stats(fresh_metas=1)
    return m


</t>
<t tx="ekr.20230831011819.1060">def visit_bytes_expr(self, o: mypy.nodes.BytesExpr) -&gt; object:
    # The value of a BytesExpr is a string created from the repr()
    # of the bytes object. Get the original bytes back.
    try:
        return ast.literal_eval(f"b'{o.value}'")
    except SyntaxError:
        return ast.literal_eval(f'b"{o.value}"')

</t>
<t tx="ekr.20230831011819.1061">def visit_float_expr(self, o: mypy.nodes.FloatExpr) -&gt; float:
    return o.value

</t>
<t tx="ekr.20230831011819.1062">def visit_complex_expr(self, o: mypy.nodes.ComplexExpr) -&gt; object:
    return o.value

</t>
<t tx="ekr.20230831011819.1063">def visit_ellipsis(self, o: mypy.nodes.EllipsisExpr) -&gt; object:
    return Ellipsis

</t>
<t tx="ekr.20230831011819.1064">def visit_star_expr(self, o: mypy.nodes.StarExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1065">def visit_name_expr(self, o: mypy.nodes.NameExpr) -&gt; object:
    if o.name == "True":
        return True
    elif o.name == "False":
        return False
    elif o.name == "None":
        return None
    # TODO: Handle more names by figuring out a way to hook into the
    # symbol table.
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1066">def visit_member_expr(self, o: mypy.nodes.MemberExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1067">def visit_yield_from_expr(self, o: mypy.nodes.YieldFromExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1068">def visit_yield_expr(self, o: mypy.nodes.YieldExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1069">def visit_call_expr(self, o: mypy.nodes.CallExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.107">def validate_meta(
    meta: CacheMeta | None, id: str, path: str | None, ignore_all: bool, manager: BuildManager
) -&gt; CacheMeta | None:
    """Checks whether the cached AST of this module can be used.

    Returns:
      None, if the cached AST is unusable.
      Original meta, if mtime/size matched.
      Meta with mtime updated to match source file, if hash/size matched but mtime/path didn't.
    """
    # This requires two steps. The first one is obvious: we check that the module source file
    # contents is the same as it was when the cache data file was created. The second one is not
    # too obvious: we check that the cache data file mtime has not changed; it is needed because
    # we use cache data file mtime to propagate information about changes in the dependencies.

    if meta is None:
        manager.log(f"Metadata not found for {id}")
        return None

    if meta.ignore_all and not ignore_all:
        manager.log(f"Metadata abandoned for {id}: errors were previously ignored")
        return None

    t0 = time.time()
    bazel = manager.options.bazel
    assert path is not None, "Internal error: meta was provided without a path"
    if not manager.options.skip_cache_mtime_checks:
        # Check data_json; assume if its mtime matches it's good.
        try:
            data_mtime = manager.getmtime(meta.data_json)
        except OSError:
            manager.log(f"Metadata abandoned for {id}: failed to stat data_json")
            return None
        if data_mtime != meta.data_mtime:
            manager.log(f"Metadata abandoned for {id}: data cache is modified")
            return None

    if bazel:
        # Normalize path under bazel to make sure it isn't absolute
        path = normpath(path, manager.options)
    try:
        st = manager.get_stat(path)
    except OSError:
        return None
    if not stat.S_ISDIR(st.st_mode) and not stat.S_ISREG(st.st_mode):
        manager.log(f"Metadata abandoned for {id}: file or directory {path} does not exist")
        return None

    manager.add_stats(validate_stat_time=time.time() - t0)

    # When we are using a fine-grained cache, we want our initial
    # build() to load all of the cache information and then do a
    # fine-grained incremental update to catch anything that has
    # changed since the cache was generated. We *don't* want to do a
    # coarse-grained incremental rebuild, so we accept the cache
    # metadata even if it doesn't match the source file.
    #
    # We still *do* the mtime/hash checks, however, to enable
    # fine-grained mode to take advantage of the mtime-updating
    # optimization when mtimes differ but hashes match.  There is
    # essentially no extra time cost to computing the hash here, since
    # it will be cached and will be needed for finding changed files
    # later anyways.
    fine_grained_cache = manager.use_fine_grained_cache()

    size = st.st_size
    # Bazel ensures the cache is valid.
    if size != meta.size and not bazel and not fine_grained_cache:
        manager.log(f"Metadata abandoned for {id}: file {path} has different size")
        return None

    # Bazel ensures the cache is valid.
    mtime = 0 if bazel else int(st.st_mtime)
    if not bazel and (mtime != meta.mtime or path != meta.path):
        if manager.quickstart_state and path in manager.quickstart_state:
            # If the mtime and the size of the file recorded in the quickstart dump matches
            # what we see on disk, we know (assume) that the hash matches the quickstart
            # data as well. If that hash matches the hash in the metadata, then we know
            # the file is up to date even though the mtime is wrong, without needing to hash it.
            qmtime, qsize, qhash = manager.quickstart_state[path]
            if int(qmtime) == mtime and qsize == size and qhash == meta.hash:
                manager.log(f"Metadata fresh (by quickstart) for {id}: file {path}")
                meta = meta._replace(mtime=mtime, path=path)
                return meta

        t0 = time.time()
        try:
            # dir means it is a namespace package
            if stat.S_ISDIR(st.st_mode):
                source_hash = ""
            else:
                source_hash = manager.fscache.hash_digest(path)
        except (OSError, UnicodeDecodeError, DecodeError):
            return None
        manager.add_stats(validate_hash_time=time.time() - t0)
        if source_hash != meta.hash:
            if fine_grained_cache:
                manager.log(f"Using stale metadata for {id}: file {path}")
                return meta
            else:
                manager.log(f"Metadata abandoned for {id}: file {path} has different hash")
                return None
        else:
            t0 = time.time()
            # Optimization: update mtime and path (otherwise, this mismatch will reappear).
            meta = meta._replace(mtime=mtime, path=path)
            # Construct a dict we can pass to json.dumps() (compare to write_cache()).
            meta_dict = {
                "id": id,
                "path": path,
                "mtime": mtime,
                "size": size,
                "hash": source_hash,
                "data_mtime": meta.data_mtime,
                "dependencies": meta.dependencies,
                "suppressed": meta.suppressed,
                "options": (manager.options.clone_for_module(id).select_options_affecting_cache()),
                "dep_prios": meta.dep_prios,
                "dep_lines": meta.dep_lines,
                "interface_hash": meta.interface_hash,
                "version_id": manager.version_id,
                "ignore_all": meta.ignore_all,
                "plugin_data": meta.plugin_data,
            }
            if manager.options.debug_cache:
                meta_str = json.dumps(meta_dict, indent=2, sort_keys=True)
            else:
                meta_str = json.dumps(meta_dict, separators=(",", ":"))
            meta_json, _, _ = get_cache_names(id, path, manager.options)
            manager.log(
                "Updating mtime for {}: file {}, meta {}, mtime {}".format(
                    id, path, meta_json, meta.mtime
                )
            )
            t1 = time.time()
            manager.metastore.write(meta_json, meta_str)  # Ignore errors, just an optimization.
            manager.add_stats(validate_update_time=time.time() - t1, validate_munging_time=t1 - t0)
            return meta

    # It's a match on (id, path, size, hash, mtime).
    manager.log(f"Metadata fresh for {id}: file {path}")
    return meta


</t>
<t tx="ekr.20230831011819.1070">def visit_op_expr(self, o: mypy.nodes.OpExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1071">def visit_comparison_expr(self, o: mypy.nodes.ComparisonExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1072">def visit_cast_expr(self, o: mypy.nodes.CastExpr) -&gt; object:
    return o.expr.accept(self)

</t>
<t tx="ekr.20230831011819.1073">def visit_assert_type_expr(self, o: mypy.nodes.AssertTypeExpr) -&gt; object:
    return o.expr.accept(self)

</t>
<t tx="ekr.20230831011819.1074">def visit_reveal_expr(self, o: mypy.nodes.RevealExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1075">def visit_super_expr(self, o: mypy.nodes.SuperExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1076">def visit_unary_expr(self, o: mypy.nodes.UnaryExpr) -&gt; object:
    operand = o.expr.accept(self)
    if operand is UNKNOWN:
        return UNKNOWN
    if o.op == "-":
        if isinstance(operand, (int, float, complex)):
            return -operand
    elif o.op == "+":
        if isinstance(operand, (int, float, complex)):
            return +operand
    elif o.op == "~":
        if isinstance(operand, int):
            return ~operand
    elif o.op == "not":
        if isinstance(operand, (bool, int, float, str, bytes)):
            return not operand
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1077">def visit_assignment_expr(self, o: mypy.nodes.AssignmentExpr) -&gt; object:
    return o.value.accept(self)

</t>
<t tx="ekr.20230831011819.1078">def visit_list_expr(self, o: mypy.nodes.ListExpr) -&gt; object:
    items = [item.accept(self) for item in o.items]
    if all(item is not UNKNOWN for item in items):
        return items
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1079">def visit_dict_expr(self, o: mypy.nodes.DictExpr) -&gt; object:
    items = [
        (UNKNOWN if key is None else key.accept(self), value.accept(self))
        for key, value in o.items
    ]
    if all(key is not UNKNOWN and value is not None for key, value in items):
        return dict(items)
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.108">def compute_hash(text: str) -&gt; str:
    # We use a crypto hash instead of the builtin hash(...) function
    # because the output of hash(...)  can differ between runs due to
    # hash randomization (enabled by default in Python 3.3).  See the
    # note in
    # https://docs.python.org/3/reference/datamodel.html#object.__hash__.
    return hash_digest(text.encode("utf-8"))


</t>
<t tx="ekr.20230831011819.1080">def visit_tuple_expr(self, o: mypy.nodes.TupleExpr) -&gt; object:
    items = [item.accept(self) for item in o.items]
    if all(item is not UNKNOWN for item in items):
        return tuple(items)
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1081">def visit_set_expr(self, o: mypy.nodes.SetExpr) -&gt; object:
    items = [item.accept(self) for item in o.items]
    if all(item is not UNKNOWN for item in items):
        return set(items)
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1082">def visit_index_expr(self, o: mypy.nodes.IndexExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1083">def visit_type_application(self, o: mypy.nodes.TypeApplication) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1084">def visit_lambda_expr(self, o: mypy.nodes.LambdaExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1085">def visit_list_comprehension(self, o: mypy.nodes.ListComprehension) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1086">def visit_set_comprehension(self, o: mypy.nodes.SetComprehension) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1087">def visit_dictionary_comprehension(self, o: mypy.nodes.DictionaryComprehension) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1088">def visit_generator_expr(self, o: mypy.nodes.GeneratorExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1089">def visit_slice_expr(self, o: mypy.nodes.SliceExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.109">def json_dumps(obj: Any, debug_cache: bool) -&gt; str:
    if debug_cache:
        return json.dumps(obj, indent=2, sort_keys=True)
    else:
        return json.dumps(obj, sort_keys=True, separators=(",", ":"))


</t>
<t tx="ekr.20230831011819.1090">def visit_conditional_expr(self, o: mypy.nodes.ConditionalExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1091">def visit_type_var_expr(self, o: mypy.nodes.TypeVarExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1092">def visit_paramspec_expr(self, o: mypy.nodes.ParamSpecExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1093">def visit_type_var_tuple_expr(self, o: mypy.nodes.TypeVarTupleExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1094">def visit_type_alias_expr(self, o: mypy.nodes.TypeAliasExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1095">def visit_namedtuple_expr(self, o: mypy.nodes.NamedTupleExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1096">def visit_enum_call_expr(self, o: mypy.nodes.EnumCallExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1097">def visit_typeddict_expr(self, o: mypy.nodes.TypedDictExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1098">def visit_newtype_expr(self, o: mypy.nodes.NewTypeExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1099">def visit__promote_expr(self, o: mypy.nodes.PromoteExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.11">def run(args: list[str]) -&gt; tuple[str, str, int]:
    # Lazy import to avoid needing to import all of mypy to call run_dmypy
    from mypy.main import main

    return _run(
        lambda stdout, stderr: main(args=args, stdout=stdout, stderr=stderr, clean_exit=True)
    )


</t>
<t tx="ekr.20230831011819.110">def write_cache(
    id: str,
    path: str,
    tree: MypyFile,
    dependencies: list[str],
    suppressed: list[str],
    dep_prios: list[int],
    dep_lines: list[int],
    old_interface_hash: str,
    source_hash: str,
    ignore_all: bool,
    manager: BuildManager,
) -&gt; tuple[str, CacheMeta | None]:
    """Write cache files for a module.

    Note that this mypy's behavior is still correct when any given
    write_cache() call is replaced with a no-op, so error handling
    code that bails without writing anything is okay.

    Args:
      id: module ID
      path: module path
      tree: the fully checked module data
      dependencies: module IDs on which this module depends
      suppressed: module IDs which were suppressed as dependencies
      dep_prios: priorities (parallel array to dependencies)
      dep_lines: import line locations (parallel array to dependencies)
      old_interface_hash: the hash from the previous version of the data cache file
      source_hash: the hash of the source code
      ignore_all: the ignore_all flag for this module
      manager: the build manager (for pyversion, log/trace)

    Returns:
      A tuple containing the interface hash and CacheMeta
      corresponding to the metadata that was written (the latter may
      be None if the cache could not be written).
    """
    metastore = manager.metastore
    # For Bazel we use relative paths and zero mtimes.
    bazel = manager.options.bazel

    # Obtain file paths.
    meta_json, data_json, _ = get_cache_names(id, path, manager.options)
    manager.log(f"Writing {id} {path} {meta_json} {data_json}")

    # Update tree.path so that in bazel mode it's made relative (since
    # sometimes paths leak out).
    if bazel:
        tree.path = path

    # Serialize data and analyze interface
    data = tree.serialize()
    data_str = json_dumps(data, manager.options.debug_cache)
    interface_hash = compute_hash(data_str)

    plugin_data = manager.plugin.report_config_data(ReportConfigContext(id, path, is_check=False))

    # Obtain and set up metadata
    try:
        st = manager.get_stat(path)
    except OSError as err:
        manager.log(f"Cannot get stat for {path}: {err}")
        # Remove apparently-invalid cache files.
        # (This is purely an optimization.)
        for filename in [data_json, meta_json]:
            try:
                os.remove(filename)
            except OSError:
                pass
        # Still return the interface hash we computed.
        return interface_hash, None

    # Write data cache file, if applicable
    # Note that for Bazel we don't record the data file's mtime.
    if old_interface_hash == interface_hash:
        manager.trace(f"Interface for {id} is unchanged")
    else:
        manager.trace(f"Interface for {id} has changed")
        if not metastore.write(data_json, data_str):
            # Most likely the error is the replace() call
            # (see https://github.com/python/mypy/issues/3215).
            manager.log(f"Error writing data JSON file {data_json}")
            # Let's continue without writing the meta file.  Analysis:
            # If the replace failed, we've changed nothing except left
            # behind an extraneous temporary file; if the replace
            # worked but the getmtime() call failed, the meta file
            # will be considered invalid on the next run because the
            # data_mtime field won't match the data file's mtime.
            # Both have the effect of slowing down the next run a
            # little bit due to an out-of-date cache file.
            return interface_hash, None

    try:
        data_mtime = manager.getmtime(data_json)
    except OSError:
        manager.log(f"Error in os.stat({data_json!r}), skipping cache write")
        return interface_hash, None

    mtime = 0 if bazel else int(st.st_mtime)
    size = st.st_size
    # Note that the options we store in the cache are the options as
    # specified by the command line/config file and *don't* reflect
    # updates made by inline config directives in the file. This is
    # important, or otherwise the options would never match when
    # verifying the cache.
    options = manager.options.clone_for_module(id)
    assert source_hash is not None
    meta = {
        "id": id,
        "path": path,
        "mtime": mtime,
        "size": size,
        "hash": source_hash,
        "data_mtime": data_mtime,
        "dependencies": dependencies,
        "suppressed": suppressed,
        "options": options.select_options_affecting_cache(),
        "dep_prios": dep_prios,
        "dep_lines": dep_lines,
        "interface_hash": interface_hash,
        "version_id": manager.version_id,
        "ignore_all": ignore_all,
        "plugin_data": plugin_data,
    }

    # Write meta cache file
    meta_str = json_dumps(meta, manager.options.debug_cache)
    if not metastore.write(meta_json, meta_str):
        # Most likely the error is the replace() call
        # (see https://github.com/python/mypy/issues/3215).
        # The next run will simply find the cache entry out of date.
        manager.log(f"Error writing meta JSON file {meta_json}")

    return interface_hash, cache_meta_from_dict(meta, data_json)


</t>
<t tx="ekr.20230831011819.1100">def visit_await_expr(self, o: mypy.nodes.AwaitExpr) -&gt; object:
    return UNKNOWN

</t>
<t tx="ekr.20230831011819.1101">def visit_temp_node(self, o: mypy.nodes.TempNode) -&gt; object:
    return UNKNOWN


</t>
<t tx="ekr.20230831011819.1102">_evaluator: Final = _NodeEvaluator()


def evaluate_expression(expr: mypy.nodes.Expression) -&gt; object:
    """Evaluate an expression at runtime.

    Return the result of the expression, or UNKNOWN if the expression cannot be
    evaluated.
    """
    return expr.accept(_evaluator)
</t>
<t tx="ekr.20230831011819.1103">@path mypy
&lt;&lt; expandtype.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1104">from __future__ import annotations

from typing import Final, Iterable, Mapping, Sequence, TypeVar, cast, overload

from mypy.nodes import ARG_STAR, Var
from mypy.state import state
from mypy.types import (
    ANY_STRATEGY,
    AnyType,
    BoolTypeQuery,
    CallableType,
    DeletedType,
    ErasedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecFlavor,
    ParamSpecType,
    PartialType,
    ProperType,
    TrivialSyntheticTypeTranslator,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    flatten_nested_unions,
    get_proper_type,
    split_with_prefix_and_suffix,
)
from mypy.typevartuples import split_with_instance

# Solving the import cycle:
import mypy.type_visitor  # ruff: isort: skip

# WARNING: these functions should never (directly or indirectly) depend on
# is_subtype(), meet_types(), join_types() etc.
# TODO: add a static dependency test for this.


@overload
</t>
<t tx="ekr.20230831011819.1105">def expand_type(typ: CallableType, env: Mapping[TypeVarId, Type]) -&gt; CallableType:
    ...


</t>
<t tx="ekr.20230831011819.1106">@overload
def expand_type(typ: ProperType, env: Mapping[TypeVarId, Type]) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20230831011819.1107">@overload
def expand_type(typ: Type, env: Mapping[TypeVarId, Type]) -&gt; Type:
    ...


</t>
<t tx="ekr.20230831011819.1108">def expand_type(typ: Type, env: Mapping[TypeVarId, Type]) -&gt; Type:
    """Substitute any type variable references in a type given by a type
    environment.
    """
    return typ.accept(ExpandTypeVisitor(env))


</t>
<t tx="ekr.20230831011819.1109">@overload
def expand_type_by_instance(typ: CallableType, instance: Instance) -&gt; CallableType:
    ...


</t>
<t tx="ekr.20230831011819.111">def delete_cache(id: str, path: str, manager: BuildManager) -&gt; None:
    """Delete cache files for a module.

    The cache files for a module are deleted when mypy finds errors there.
    This avoids inconsistent states with cache files from different mypy runs,
    see #4043 for an example.
    """
    # We don't delete .deps files on errors, since the dependencies
    # are mostly generated from other files and the metadata is
    # tracked separately.
    meta_path, data_path, _ = get_cache_names(id, path, manager.options)
    cache_paths = [meta_path, data_path]
    manager.log(f"Deleting {id} {path} {' '.join(x for x in cache_paths if x)}")

    for filename in cache_paths:
        try:
            manager.metastore.remove(filename)
        except OSError as e:
            if e.errno != errno.ENOENT:
                manager.log(f"Error deleting cache file {filename}: {e.strerror}")


</t>
<t tx="ekr.20230831011819.1110">@overload
def expand_type_by_instance(typ: ProperType, instance: Instance) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20230831011819.1111">@overload
def expand_type_by_instance(typ: Type, instance: Instance) -&gt; Type:
    ...


</t>
<t tx="ekr.20230831011819.1112">def expand_type_by_instance(typ: Type, instance: Instance) -&gt; Type:
    """Substitute type variables in type using values from an Instance.
    Type variables are considered to be bound by the class declaration."""
    if not instance.args:
        return typ
    else:
        variables: dict[TypeVarId, Type] = {}
        if instance.type.has_type_var_tuple_type:
            assert instance.type.type_var_tuple_prefix is not None
            assert instance.type.type_var_tuple_suffix is not None

            args_prefix, args_middle, args_suffix = split_with_instance(instance)
            tvars_prefix, tvars_middle, tvars_suffix = split_with_prefix_and_suffix(
                tuple(instance.type.defn.type_vars),
                instance.type.type_var_tuple_prefix,
                instance.type.type_var_tuple_suffix,
            )
            tvar = tvars_middle[0]
            assert isinstance(tvar, TypeVarTupleType)
            variables = {tvar.id: TupleType(list(args_middle), tvar.tuple_fallback)}
            instance_args = args_prefix + args_suffix
            tvars = tvars_prefix + tvars_suffix
        else:
            tvars = tuple(instance.type.defn.type_vars)
            instance_args = instance.args

        for binder, arg in zip(tvars, instance_args):
            assert isinstance(binder, TypeVarLikeType)
            variables[binder.id] = arg

        return expand_type(typ, variables)


</t>
<t tx="ekr.20230831011819.1113">F = TypeVar("F", bound=FunctionLike)


def freshen_function_type_vars(callee: F) -&gt; F:
    """Substitute fresh type variables for generic function type variables."""
    if isinstance(callee, CallableType):
        if not callee.is_generic():
            return cast(F, callee)
        tvs = []
        tvmap: dict[TypeVarId, Type] = {}
        for v in callee.variables:
            tv = v.new_unification_variable(v)
            tvs.append(tv)
            tvmap[v.id] = tv
        fresh = expand_type(callee, tvmap).copy_modified(variables=tvs)
        return cast(F, fresh)
    else:
        assert isinstance(callee, Overloaded)
        fresh_overload = Overloaded([freshen_function_type_vars(item) for item in callee.items])
        return cast(F, fresh_overload)


</t>
<t tx="ekr.20230831011819.1114">class HasGenericCallable(BoolTypeQuery):
    @others
</t>
<t tx="ekr.20230831011819.1115">def __init__(self) -&gt; None:
    super().__init__(ANY_STRATEGY)

</t>
<t tx="ekr.20230831011819.1116">def visit_callable_type(self, t: CallableType) -&gt; bool:
    return t.is_generic() or super().visit_callable_type(t)


</t>
<t tx="ekr.20230831011819.1117"># Share a singleton since this is performance sensitive
has_generic_callable: Final = HasGenericCallable()


T = TypeVar("T", bound=Type)


def freshen_all_functions_type_vars(t: T) -&gt; T:
    result: Type
    has_generic_callable.reset()
    if not t.accept(has_generic_callable):
        return t  # Fast path to avoid expensive freshening
    else:
        result = t.accept(FreshenCallableVisitor())
        assert isinstance(result, type(t))
        return result


</t>
<t tx="ekr.20230831011819.1118">class FreshenCallableVisitor(mypy.type_visitor.TypeTranslator):
    @others
</t>
<t tx="ekr.20230831011819.1119">def visit_callable_type(self, t: CallableType) -&gt; Type:
    result = super().visit_callable_type(t)
    assert isinstance(result, ProperType) and isinstance(result, CallableType)
    return freshen_function_type_vars(result)

</t>
<t tx="ekr.20230831011819.112">"""Dependency manager.

Design
======

Ideally
-------

A. Collapse cycles (each SCC -- strongly connected component --
   becomes one "supernode").

B. Topologically sort nodes based on dependencies.

C. Process from leaves towards roots.

Wrinkles
--------

a. Need to parse source modules to determine dependencies.

b. Processing order for modules within an SCC.

c. Must order mtimes of files to decide whether to re-process; depends
   on clock never resetting.

d. from P import M; checks filesystem whether module P.M exists in
   filesystem.

e. Race conditions, where somebody modifies a file while we're
   processing. Solved by using a FileSystemCache.


Steps
-----

1. For each explicitly given module find the source file location.

2. For each such module load and check the cache metadata, and decide
   whether it's valid.

3. Now recursively (or iteratively) find dependencies and add those to
   the graph:

   - for cached nodes use the list of dependencies from the cache
     metadata (this will be valid even if we later end up re-parsing
     the same source);

   - for uncached nodes parse the file and process all imports found,
     taking care of (a) above.

Step 3 should also address (d) above.

Once step 3 terminates we have the entire dependency graph, and for
each module we've either loaded the cache metadata or parsed the
source code.  (However, we may still need to parse those modules for
which we have cache metadata but that depend, directly or indirectly,
on at least one module for which the cache metadata is stale.)

Now we can execute steps A-C from the first section.  Finding SCCs for
step A shouldn't be hard; there's a recipe here:
https://code.activestate.com/recipes/578507/.  There's also a plethora
of topsort recipes, e.g. https://code.activestate.com/recipes/577413/.

For single nodes, processing is simple.  If the node was cached, we
deserialize the cache data and fix up cross-references.  Otherwise, we
do semantic analysis followed by type checking.  We also handle (c)
above; if a module has valid cache data *but* any of its
dependencies was processed from source, then the module should be
processed from source.

A relatively simple optimization (outside SCCs) we might do in the
future is as follows: if a node's cache data is valid, but one or more
of its dependencies are out of date so we have to re-parse the node
from source, once we have fully type-checked the node, we can decide
whether its symbol table actually changed compared to the cache data
(by reading the cache data and comparing it to the data we would be
writing).  If there is no change we can declare the node up to date,
and any node that depends (and for which we have cached data, and
whose other dependencies are up to date) on it won't need to be
re-parsed from source.

Import cycles
-------------

Finally we have to decide how to handle (c), import cycles.  Here
we'll need a modified version of the original state machine
(build.py), but we only need to do this per SCC, and we won't have to
deal with changes to the list of nodes while we're processing it.

If all nodes in the SCC have valid cache metadata and all dependencies
outside the SCC are still valid, we can proceed as follows:

  1. Load cache data for all nodes in the SCC.

  2. Fix up cross-references for all nodes in the SCC.

Otherwise, the simplest (but potentially slow) way to proceed is to
invalidate all cache data in the SCC and re-parse all nodes in the SCC
from source.  We can do this as follows:

  1. Parse source for all nodes in the SCC.

  2. Semantic analysis for all nodes in the SCC.

  3. Type check all nodes in the SCC.

(If there are more passes the process is the same -- each pass should
be done for all nodes before starting the next pass for any nodes in
the SCC.)

We could process the nodes in the SCC in any order.  For sentimental
reasons, I've decided to process them in the reverse order in which we
encountered them when originally constructing the graph.  That's how
the old build.py deals with cycles, and at least this reproduces the
previous implementation more accurately.

Can we do better than re-parsing all nodes in the SCC when any of its
dependencies are out of date?  It's doubtful.  The optimization
mentioned at the end of the previous section would require re-parsing
and type-checking a node and then comparing its symbol table to the
cached data; but because the node is part of a cycle we can't
technically type-check it until the semantic analysis of all other
nodes in the cycle has completed.  (This is an important issue because
Dropbox has a very large cycle in production code.  But I'd like to
deal with it later.)

Additional wrinkles
-------------------

During implementation more wrinkles were found.

- When a submodule of a package (e.g. x.y) is encountered, the parent
  package (e.g. x) must also be loaded, but it is not strictly a
  dependency.  See State.add_ancestors() below.
"""


class ModuleNotFound(Exception):
    """Control flow exception to signal that a module was not found."""


</t>
<t tx="ekr.20230831011819.1120">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Same as for ExpandTypeVisitor
    return t.copy_modified(args=[arg.accept(self) for arg in t.args])


</t>
<t tx="ekr.20230831011819.1121">class ExpandTypeVisitor(TrivialSyntheticTypeTranslator):
    """Visitor that substitutes type variables with values."""

    @others
</t>
<t tx="ekr.20230831011819.1122">variables: Mapping[TypeVarId, Type]  # TypeVar id -&gt; TypeVar value

def __init__(self, variables: Mapping[TypeVarId, Type]) -&gt; None:
    self.variables = variables

</t>
<t tx="ekr.20230831011819.1123">def visit_unbound_type(self, t: UnboundType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011819.1124">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011819.1125">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011819.1126">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011819.1127">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011819.1128">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    # This may happen during type inference if some function argument
    # type is a generic callable, and its erased form will appear in inferred
    # constraints, then solver may check subtyping between them, which will trigger
    # unify_generic_callables(), this is why we can get here. Another example is
    # when inferring type of lambda in generic context, the lambda body contains
    # a generic method in generic class.
    return t

</t>
<t tx="ekr.20230831011819.1129">def visit_instance(self, t: Instance) -&gt; Type:
    args = self.expand_types_with_unpack(list(t.args))
    if isinstance(args, list):
        return t.copy_modified(args=args)
    else:
        return args

</t>
<t tx="ekr.20230831011819.113">class State:
    """The state for a module.

    The source is only used for the -c command line option; in that
    case path is None.  Otherwise source is None and path isn't.
    """

    @others
</t>
<t tx="ekr.20230831011819.1130">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    # Normally upper bounds can't contain other type variables, the only exception is
    # special type variable Self`0 &lt;: C[T, S], where C is the class where Self is used.
    if t.id.raw_id == 0:
        t = t.copy_modified(upper_bound=t.upper_bound.accept(self))
    repl = self.variables.get(t.id, t)
    if isinstance(repl, ProperType) and isinstance(repl, Instance):
        # TODO: do we really need to do this?
        # If I try to remove this special-casing ~40 tests fail on reveal_type().
        return repl.copy_modified(last_known_value=None)
    return repl

</t>
<t tx="ekr.20230831011819.1131">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    # Set prefix to something empty, so we don't duplicate it below.
    repl = self.variables.get(t.id, t.copy_modified(prefix=Parameters([], [], [])))
    if isinstance(repl, ParamSpecType):
        return repl.copy_modified(
            flavor=t.flavor,
            prefix=t.prefix.copy_modified(
                arg_types=self.expand_types(t.prefix.arg_types + repl.prefix.arg_types),
                arg_kinds=t.prefix.arg_kinds + repl.prefix.arg_kinds,
                arg_names=t.prefix.arg_names + repl.prefix.arg_names,
            ),
        )
    elif isinstance(repl, Parameters):
        assert t.flavor == ParamSpecFlavor.BARE
        return Parameters(
            self.expand_types(t.prefix.arg_types + repl.arg_types),
            t.prefix.arg_kinds + repl.arg_kinds,
            t.prefix.arg_names + repl.arg_names,
            variables=[*t.prefix.variables, *repl.variables],
        )
    else:
        # TODO: replace this with "assert False"
        return repl

</t>
<t tx="ekr.20230831011819.1132">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    # Sometimes solver may need to expand a type variable with (a copy of) itself
    # (usually together with other TypeVars, but it is hard to filter out TypeVarTuples).
    repl = self.variables.get(t.id, t)
    if isinstance(repl, TypeVarTupleType):
        return repl
    raise NotImplementedError

</t>
<t tx="ekr.20230831011819.1133">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    # It is impossible to reasonably implement visit_unpack_type, because
    # unpacking inherently expands to something more like a list of types.
    #
    # Relevant sections that can call unpack should call expand_unpack()
    # instead.
    # However, if the item is a variadic tuple, we can simply carry it over.
    # In particular, if we expand A[*tuple[T, ...]] with substitutions {T: str},
    # it is hard to assert this without getting proper type.
    return UnpackType(t.type.accept(self))

</t>
<t tx="ekr.20230831011819.1134">def expand_unpack(self, t: UnpackType) -&gt; list[Type] | AnyType | UninhabitedType:
    assert isinstance(t.type, TypeVarTupleType)
    repl = get_proper_type(self.variables.get(t.type.id, t.type))
    if isinstance(repl, TupleType):
        return repl.items
    elif (
        isinstance(repl, Instance)
        and repl.type.fullname == "builtins.tuple"
        or isinstance(repl, TypeVarTupleType)
    ):
        return [UnpackType(typ=repl)]
    elif isinstance(repl, (AnyType, UninhabitedType)):
        # tuple[Any, ...] for Any would be better, but we don't have
        # the type info to construct that type here.
        return repl
    else:
        raise RuntimeError(f"Invalid type replacement to expand: {repl}")

</t>
<t tx="ekr.20230831011819.1135">def visit_parameters(self, t: Parameters) -&gt; Type:
    return t.copy_modified(arg_types=self.expand_types(t.arg_types))

</t>
<t tx="ekr.20230831011819.1136">def interpolate_args_for_unpack(self, t: CallableType, var_arg: UnpackType) -&gt; list[Type]:
    star_index = t.arg_kinds.index(ARG_STAR)
    prefix = self.expand_types(t.arg_types[:star_index])
    suffix = self.expand_types(t.arg_types[star_index + 1 :])

    var_arg_type = get_proper_type(var_arg.type)
    # We have something like Unpack[Tuple[Unpack[Ts], X1, X2]]
    if isinstance(var_arg_type, TupleType):
        expanded_tuple = var_arg_type.accept(self)
        assert isinstance(expanded_tuple, ProperType) and isinstance(expanded_tuple, TupleType)
        expanded_items = expanded_tuple.items
        fallback = var_arg_type.partial_fallback
    else:
        # We have plain Unpack[Ts]
        assert isinstance(var_arg_type, TypeVarTupleType)
        fallback = var_arg_type.tuple_fallback
        expanded_items_res = self.expand_unpack(var_arg)
        if isinstance(expanded_items_res, list):
            expanded_items = expanded_items_res
        else:
            # We got Any or &lt;nothing&gt;
            return prefix + [expanded_items_res] + suffix
    new_unpack = UnpackType(TupleType(expanded_items, fallback))
    return prefix + [new_unpack] + suffix

</t>
<t tx="ekr.20230831011819.1137">def visit_callable_type(self, t: CallableType) -&gt; CallableType:
    param_spec = t.param_spec()
    if param_spec is not None:
        repl = self.variables.get(param_spec.id)
        # If a ParamSpec in a callable type is substituted with a
        # callable type, we can't use normal substitution logic,
        # since ParamSpec is actually split into two components
        # *P.args and **P.kwargs in the original type. Instead, we
        # must expand both of them with all the argument types,
        # kinds and names in the replacement. The return type in
        # the replacement is ignored.
        if isinstance(repl, Parameters):
            # We need to expand both the types in the prefix and the ParamSpec itself
            t = t.expand_param_spec(repl)
            return t.copy_modified(
                arg_types=self.expand_types(t.arg_types),
                ret_type=t.ret_type.accept(self),
                type_guard=(t.type_guard.accept(self) if t.type_guard is not None else None),
            )
        elif isinstance(repl, ParamSpecType):
            # We're substituting one ParamSpec for another; this can mean that the prefix
            # changes, e.g. substitute Concatenate[int, P] in place of Q.
            prefix = repl.prefix
            clean_repl = repl.copy_modified(prefix=Parameters([], [], []))
            return t.copy_modified(
                arg_types=self.expand_types(t.arg_types[:-2] + prefix.arg_types)
                + [
                    clean_repl.with_flavor(ParamSpecFlavor.ARGS),
                    clean_repl.with_flavor(ParamSpecFlavor.KWARGS),
                ],
                arg_kinds=t.arg_kinds[:-2] + prefix.arg_kinds + t.arg_kinds[-2:],
                arg_names=t.arg_names[:-2] + prefix.arg_names + t.arg_names[-2:],
                ret_type=t.ret_type.accept(self),
                from_concatenate=t.from_concatenate or bool(repl.prefix.arg_types),
            )

    var_arg = t.var_arg()
    needs_normalization = False
    if var_arg is not None and isinstance(var_arg.typ, UnpackType):
        needs_normalization = True
        arg_types = self.interpolate_args_for_unpack(t, var_arg.typ)
    else:
        arg_types = self.expand_types(t.arg_types)
    expanded = t.copy_modified(
        arg_types=arg_types,
        ret_type=t.ret_type.accept(self),
        type_guard=(t.type_guard.accept(self) if t.type_guard is not None else None),
    )
    if needs_normalization:
        return expanded.with_normalized_var_args()
    return expanded

</t>
<t tx="ekr.20230831011819.1138">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    items: list[CallableType] = []
    for item in t.items:
        new_item = item.accept(self)
        assert isinstance(new_item, ProperType)
        assert isinstance(new_item, CallableType)
        items.append(new_item)
    return Overloaded(items)

</t>
<t tx="ekr.20230831011819.1139">def expand_types_with_unpack(
    self, typs: Sequence[Type]
) -&gt; list[Type] | AnyType | UninhabitedType:
    """Expands a list of types that has an unpack.

    In corner cases, this can return a type rather than a list, in which case this
    indicates use of Any or some error occurred earlier. In this case callers should
    simply propagate the resulting type.
    """
    items: list[Type] = []
    for item in typs:
        if isinstance(item, UnpackType) and isinstance(item.type, TypeVarTupleType):
            unpacked_items = self.expand_unpack(item)
            if isinstance(unpacked_items, (AnyType, UninhabitedType)):
                # TODO: better error for &lt;nothing&gt;, something like tuple of unknown?
                return unpacked_items
            else:
                items.extend(unpacked_items)
        else:
            # Must preserve original aliases when possible.
            items.append(item.accept(self))
    return items

</t>
<t tx="ekr.20230831011819.114">manager: BuildManager
order_counter: ClassVar[int] = 0
order: int  # Order in which modules were encountered
id: str  # Fully qualified module name
path: str | None = None  # Path to module source
abspath: str | None = None  # Absolute path to module source
xpath: str  # Path or '&lt;string&gt;'
source: str | None = None  # Module source code
source_hash: str | None = None  # Hash calculated based on the source code
meta_source_hash: str | None = None  # Hash of the source given in the meta, if any
meta: CacheMeta | None = None
data: str | None = None
tree: MypyFile | None = None
# We keep both a list and set of dependencies. A set because it makes it efficient to
# prevent duplicates and the list because I am afraid of changing the order of
# iteration over dependencies.
# They should be managed with add_dependency and suppress_dependency.
dependencies: list[str]  # Modules directly imported by the module
dependencies_set: set[str]  # The same but as a set for deduplication purposes
suppressed: list[str]  # Suppressed/missing dependencies
suppressed_set: set[str]  # Suppressed/missing dependencies
priorities: dict[str, int]

# Map each dependency to the line number where it is first imported
dep_line_map: dict[str, int]

# Parent package, its parent, etc.
ancestors: list[str] | None = None

# List of (path, line number) tuples giving context for import
import_context: list[tuple[str, int]]

# The State from which this module was imported, if any
caller_state: State | None = None

# If caller_state is set, the line number in the caller where the import occurred
caller_line = 0

# If True, indicate that the public interface of this module is unchanged
externally_same = True

# Contains a hash of the public interface in incremental mode
interface_hash: str = ""

# Options, specialized for this file
options: Options

# Whether to ignore all errors
ignore_all = False

# Whether the module has an error or any of its dependencies have one.
transitive_error = False

# Errors reported before semantic analysis, to allow fine-grained
# mode to keep reporting them.
early_errors: list[ErrorInfo]

# Type checker used for checking this file.  Use type_checker() for
# access and to construct this on demand.
_type_checker: TypeChecker | None = None

fine_grained_deps_loaded = False

# Cumulative time spent on this file, in microseconds (for profiling stats)
time_spent_us: int = 0

# Per-line type-checking time (cumulative time spent type-checking expressions
# on a given source code line).
per_line_checking_time_ns: dict[int, int]

def __init__(
    self,
    id: str | None,
    path: str | None,
    source: str | None,
    manager: BuildManager,
    caller_state: State | None = None,
    caller_line: int = 0,
    ancestor_for: State | None = None,
    root_source: bool = False,
    # If `temporary` is True, this State is being created to just
    # quickly parse/load the tree, without an intention to further
    # process it. With this flag, any changes to external state as well
    # as error reporting should be avoided.
    temporary: bool = False,
) -&gt; None:
    if not temporary:
        assert id or path or source is not None, "Neither id, path nor source given"
    self.manager = manager
    State.order_counter += 1
    self.order = State.order_counter
    self.caller_state = caller_state
    self.caller_line = caller_line
    if caller_state:
        self.import_context = caller_state.import_context.copy()
        self.import_context.append((caller_state.xpath, caller_line))
    else:
        self.import_context = []
    self.id = id or "__main__"
    self.options = manager.options.clone_for_module(self.id)
    self.early_errors = []
    self._type_checker = None
    if not path and source is None:
        assert id is not None
        try:
            path, follow_imports = find_module_and_diagnose(
                manager,
                id,
                self.options,
                caller_state,
                caller_line,
                ancestor_for,
                root_source,
                skip_diagnose=temporary,
            )
        except ModuleNotFound:
            if not temporary:
                manager.missing_modules.add(id)
            raise
        if follow_imports == "silent":
            self.ignore_all = True
    elif path and is_silent_import_module(manager, path) and not root_source:
        self.ignore_all = True
    self.path = path
    if path:
        self.abspath = os.path.abspath(path)
    self.xpath = path or "&lt;string&gt;"
    if path and source is None and self.manager.cache_enabled:
        self.meta = find_cache_meta(self.id, path, manager)
        # TODO: Get mtime if not cached.
        if self.meta is not None:
            self.interface_hash = self.meta.interface_hash
            self.meta_source_hash = self.meta.hash
    if path and source is None and self.manager.fscache.isdir(path):
        source = ""
    self.source = source
    self.add_ancestors()
    self.per_line_checking_time_ns = collections.defaultdict(int)
    t0 = time.time()
    self.meta = validate_meta(self.meta, self.id, self.path, self.ignore_all, manager)
    self.manager.add_stats(validate_meta_time=time.time() - t0)
    if self.meta:
        # Make copies, since we may modify these and want to
        # compare them to the originals later.
        self.dependencies = list(self.meta.dependencies)
        self.dependencies_set = set(self.dependencies)
        self.suppressed = list(self.meta.suppressed)
        self.suppressed_set = set(self.suppressed)
        all_deps = self.dependencies + self.suppressed
        assert len(all_deps) == len(self.meta.dep_prios)
        self.priorities = {id: pri for id, pri in zip(all_deps, self.meta.dep_prios)}
        assert len(all_deps) == len(self.meta.dep_lines)
        self.dep_line_map = {id: line for id, line in zip(all_deps, self.meta.dep_lines)}
        if temporary:
            self.load_tree(temporary=True)
        if not manager.use_fine_grained_cache():
            # Special case: if there were a previously missing package imported here
            # and it is not present, then we need to re-calculate dependencies.
            # This is to support patterns like this:
            #     from missing_package import missing_module  # type: ignore
            # At first mypy doesn't know that `missing_module` is a module
            # (it may be a variable, a class, or a function), so it is not added to
            # suppressed dependencies. Therefore, when the package with module is added,
            # we need to re-calculate dependencies.
            # NOTE: see comment below for why we skip this in fine grained mode.
            if exist_added_packages(self.suppressed, manager, self.options):
                self.parse_file()  # This is safe because the cache is anyway stale.
                self.compute_dependencies()
    else:
        # When doing a fine-grained cache load, pretend we only
        # know about modules that have cache information and defer
        # handling new modules until the fine-grained update.
        if manager.use_fine_grained_cache():
            manager.log(f"Deferring module to fine-grained update {path} ({id})")
            raise ModuleNotFound

        # Parse the file (and then some) to get the dependencies.
        self.parse_file()
        self.compute_dependencies()

</t>
<t tx="ekr.20230831011819.1140">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    items = self.expand_types_with_unpack(t.items)
    if isinstance(items, list):
        if len(items) == 1:
            # Normalize Tuple[*Tuple[X, ...]] -&gt; Tuple[X, ...]
            item = items[0]
            if isinstance(item, UnpackType):
                unpacked = get_proper_type(item.type)
                if isinstance(unpacked, Instance):
                    assert unpacked.type.fullname == "builtins.tuple"
                    return unpacked
        fallback = t.partial_fallback.accept(self)
        assert isinstance(fallback, ProperType) and isinstance(fallback, Instance)
        return t.copy_modified(items=items, fallback=fallback)
    else:
        return items

</t>
<t tx="ekr.20230831011819.1141">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    fallback = t.fallback.accept(self)
    assert isinstance(fallback, ProperType) and isinstance(fallback, Instance)
    return t.copy_modified(item_types=self.expand_types(t.items.values()), fallback=fallback)

</t>
<t tx="ekr.20230831011819.1142">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    # TODO: Verify this implementation is correct
    return t

</t>
<t tx="ekr.20230831011819.1143">def visit_union_type(self, t: UnionType) -&gt; Type:
    expanded = self.expand_types(t.items)
    # After substituting for type variables in t.items, some resulting types
    # might be subtypes of others, however calling  make_simplified_union()
    # can cause recursion, so we just remove strict duplicates.
    simplified = UnionType.make_union(
        remove_trivial(flatten_nested_unions(expanded)), t.line, t.column
    )
    # This call to get_proper_type() is unfortunate but is required to preserve
    # the invariant that ProperType will stay ProperType after applying expand_type(),
    # otherwise a single item union of a type alias will break it. Note this should not
    # cause infinite recursion since pathological aliases like A = Union[A, B] are
    # banned at the semantic analysis level.
    return get_proper_type(simplified)

</t>
<t tx="ekr.20230831011819.1144">def visit_partial_type(self, t: PartialType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011819.1145">def visit_type_type(self, t: TypeType) -&gt; Type:
    # TODO: Verify that the new item type is valid (instance or
    # union of instances or Any).  Sadly we can't report errors
    # here yet.
    item = t.item.accept(self)
    return TypeType.make_normalized(item)

</t>
<t tx="ekr.20230831011819.1146">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Target of the type alias cannot contain type variables (not bound by the type
    # alias itself), so we just expand the arguments.
    args = self.expand_types_with_unpack(t.args)
    if isinstance(args, list):
        # TODO: normalize if target is Tuple, and args are [*tuple[X, ...]]?
        return t.copy_modified(args=args)
    else:
        return args

</t>
<t tx="ekr.20230831011819.1147">def expand_types(self, types: Iterable[Type]) -&gt; list[Type]:
    a: list[Type] = []
    for t in types:
        a.append(t.accept(self))
    return a


</t>
<t tx="ekr.20230831011819.1148">@overload
def expand_self_type(var: Var, typ: ProperType, replacement: ProperType) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20230831011819.1149">@overload
def expand_self_type(var: Var, typ: Type, replacement: Type) -&gt; Type:
    ...


</t>
<t tx="ekr.20230831011819.115">@property
def xmeta(self) -&gt; CacheMeta:
    assert self.meta, "missing meta on allegedly fresh module"
    return self.meta

</t>
<t tx="ekr.20230831011819.1150">def expand_self_type(var: Var, typ: Type, replacement: Type) -&gt; Type:
    """Expand appearances of Self type in a variable type."""
    if var.info.self_type is not None and not var.is_property:
        return expand_type(typ, {var.info.self_type.id: replacement})
    return typ


</t>
<t tx="ekr.20230831011819.1151">def remove_trivial(types: Iterable[Type]) -&gt; list[Type]:
    """Make trivial simplifications on a list of types without calling is_subtype().

    This makes following simplifications:
        * Remove bottom types (taking into account strict optional setting)
        * Remove everything else if there is an `object`
        * Remove strict duplicate types
    """
    removed_none = False
    new_types = []
    all_types = set()
    for t in types:
        p_t = get_proper_type(t)
        if isinstance(p_t, UninhabitedType):
            continue
        if isinstance(p_t, NoneType) and not state.strict_optional:
            removed_none = True
            continue
        if isinstance(p_t, Instance) and p_t.type.fullname == "builtins.object":
            return [p_t]
        if p_t not in all_types:
            new_types.append(t)
            all_types.add(p_t)
    if new_types:
        return new_types
    if removed_none:
        return [NoneType()]
    return [UninhabitedType()]
</t>
<t tx="ekr.20230831011819.1152">@path mypy
"""Translate an Expression to a Type value."""
&lt;&lt; exprtotype.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1153">
from __future__ import annotations

from mypy.fastparse import parse_type_string
from mypy.nodes import (
    BytesExpr,
    CallExpr,
    ComplexExpr,
    EllipsisExpr,
    Expression,
    FloatExpr,
    IndexExpr,
    IntExpr,
    ListExpr,
    MemberExpr,
    NameExpr,
    OpExpr,
    RefExpr,
    StrExpr,
    TupleExpr,
    UnaryExpr,
    get_member_expr_fullname,
)
from mypy.options import Options
from mypy.types import (
    ANNOTATED_TYPE_NAMES,
    AnyType,
    CallableArgument,
    EllipsisType,
    ProperType,
    RawExpressionType,
    Type,
    TypeList,
    TypeOfAny,
    UnboundType,
    UnionType,
)


</t>
<t tx="ekr.20230831011819.1154">class TypeTranslationError(Exception):
    """Exception raised when an expression is not valid as a type."""


</t>
<t tx="ekr.20230831011819.1155">def _extract_argument_name(expr: Expression) -&gt; str | None:
    if isinstance(expr, NameExpr) and expr.name == "None":
        return None
    elif isinstance(expr, StrExpr):
        return expr.value
    else:
        raise TypeTranslationError()


</t>
<t tx="ekr.20230831011819.1156">def expr_to_unanalyzed_type(
    expr: Expression,
    options: Options | None = None,
    allow_new_syntax: bool = False,
    _parent: Expression | None = None,
) -&gt; ProperType:
    """Translate an expression to the corresponding type.

    The result is not semantically analyzed. It can be UnboundType or TypeList.
    Raise TypeTranslationError if the expression cannot represent a type.

    If allow_new_syntax is True, allow all type syntax independent of the target
    Python version (used in stubs).
    """
    # The `parent` parameter is used in recursive calls to provide context for
    # understanding whether an CallableArgument is ok.
    name: str | None = None
    if isinstance(expr, NameExpr):
        name = expr.name
        if name == "True":
            return RawExpressionType(True, "builtins.bool", line=expr.line, column=expr.column)
        elif name == "False":
            return RawExpressionType(False, "builtins.bool", line=expr.line, column=expr.column)
        else:
            return UnboundType(name, line=expr.line, column=expr.column)
    elif isinstance(expr, MemberExpr):
        fullname = get_member_expr_fullname(expr)
        if fullname:
            return UnboundType(fullname, line=expr.line, column=expr.column)
        else:
            raise TypeTranslationError()
    elif isinstance(expr, IndexExpr):
        base = expr_to_unanalyzed_type(expr.base, options, allow_new_syntax, expr)
        if isinstance(base, UnboundType):
            if base.args:
                raise TypeTranslationError()
            if isinstance(expr.index, TupleExpr):
                args = expr.index.items
            else:
                args = [expr.index]

            if isinstance(expr.base, RefExpr) and expr.base.fullname in ANNOTATED_TYPE_NAMES:
                # TODO: this is not the optimal solution as we are basically getting rid
                # of the Annotation definition and only returning the type information,
                # losing all the annotations.

                return expr_to_unanalyzed_type(args[0], options, allow_new_syntax, expr)
            else:
                base.args = tuple(
                    expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr) for arg in args
                )
            if not base.args:
                base.empty_tuple_index = True
            return base
        else:
            raise TypeTranslationError()
    elif (
        isinstance(expr, OpExpr)
        and expr.op == "|"
        and ((options and options.python_version &gt;= (3, 10)) or allow_new_syntax)
    ):
        return UnionType(
            [
                expr_to_unanalyzed_type(expr.left, options, allow_new_syntax),
                expr_to_unanalyzed_type(expr.right, options, allow_new_syntax),
            ]
        )
    elif isinstance(expr, CallExpr) and isinstance(_parent, ListExpr):
        c = expr.callee
        names = []
        # Go through the dotted member expr chain to get the full arg
        # constructor name to look up
        while True:
            if isinstance(c, NameExpr):
                names.append(c.name)
                break
            elif isinstance(c, MemberExpr):
                names.append(c.name)
                c = c.expr
            else:
                raise TypeTranslationError()
        arg_const = ".".join(reversed(names))

        # Go through the constructor args to get its name and type.
        name = None
        default_type = AnyType(TypeOfAny.unannotated)
        typ: Type = default_type
        for i, arg in enumerate(expr.args):
            if expr.arg_names[i] is not None:
                if expr.arg_names[i] == "name":
                    if name is not None:
                        # Two names
                        raise TypeTranslationError()
                    name = _extract_argument_name(arg)
                    continue
                elif expr.arg_names[i] == "type":
                    if typ is not default_type:
                        # Two types
                        raise TypeTranslationError()
                    typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
                    continue
                else:
                    raise TypeTranslationError()
            elif i == 0:
                typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
            elif i == 1:
                name = _extract_argument_name(arg)
            else:
                raise TypeTranslationError()
        return CallableArgument(typ, name, arg_const, expr.line, expr.column)
    elif isinstance(expr, ListExpr):
        return TypeList(
            [expr_to_unanalyzed_type(t, options, allow_new_syntax, expr) for t in expr.items],
            line=expr.line,
            column=expr.column,
        )
    elif isinstance(expr, StrExpr):
        return parse_type_string(expr.value, "builtins.str", expr.line, expr.column)
    elif isinstance(expr, BytesExpr):
        return parse_type_string(expr.value, "builtins.bytes", expr.line, expr.column)
    elif isinstance(expr, UnaryExpr):
        typ = expr_to_unanalyzed_type(expr.expr, options, allow_new_syntax)
        if isinstance(typ, RawExpressionType):
            if isinstance(typ.literal_value, int) and expr.op == "-":
                typ.literal_value *= -1
                return typ
        raise TypeTranslationError()
    elif isinstance(expr, IntExpr):
        return RawExpressionType(expr.value, "builtins.int", line=expr.line, column=expr.column)
    elif isinstance(expr, FloatExpr):
        # Floats are not valid parameters for RawExpressionType , so we just
        # pass in 'None' for now. We'll report the appropriate error at a later stage.
        return RawExpressionType(None, "builtins.float", line=expr.line, column=expr.column)
    elif isinstance(expr, ComplexExpr):
        # Same thing as above with complex numbers.
        return RawExpressionType(None, "builtins.complex", line=expr.line, column=expr.column)
    elif isinstance(expr, EllipsisExpr):
        return EllipsisType(expr.line)
    else:
        raise TypeTranslationError()
</t>
<t tx="ekr.20230831011819.1157">@path mypy
&lt;&lt; fastparse.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1158">from __future__ import annotations

import copy
import re
import sys
import warnings
from typing import Any, Callable, Final, List, Optional, Sequence, TypeVar, Union, cast
from typing_extensions import Literal, overload

from mypy import defaults, errorcodes as codes, message_registry
from mypy.errors import Errors
from mypy.message_registry import ErrorMessage
from mypy.nodes import (
    ARG_NAMED,
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    ArgKind,
    Argument,
    AssertStmt,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    ContinueStmt,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    FakeInfo,
    FloatExpr,
    ForStmt,
    FuncDef,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportBase,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    OverloadPart,
    PassStmt,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    Statement,
    StrExpr,
    SuperExpr,
    TempNode,
    TryStmt,
    TupleExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
    check_arg_names,
)
from mypy.options import Options
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.reachability import infer_reachability_of_if_statement, mark_block_unreachable
from mypy.sharedparse import argument_elide_name, special_function_elide_names
from mypy.traverser import TraverserVisitor
from mypy.types import (
    AnyType,
    CallableArgument,
    CallableType,
    EllipsisType,
    Instance,
    ProperType,
    RawExpressionType,
    TupleType,
    Type,
    TypeList,
    TypeOfAny,
    UnboundType,
    UnionType,
)
from mypy.util import bytes_to_human_readable_repr, unnamed_function

# pull this into a final variable to make mypyc be quiet about the
# the default argument warning
PY_MINOR_VERSION: Final = sys.version_info[1]

import ast as ast3

# TODO: Index, ExtSlice are deprecated in 3.9.
from ast import AST, Attribute, Call, FunctionType, Index, Name, Starred, UnaryOp, USub


</t>
<t tx="ekr.20230831011819.1159">def ast3_parse(
    source: str | bytes, filename: str, mode: str, feature_version: int = PY_MINOR_VERSION
) -&gt; AST:
    return ast3.parse(
        source,
        filename,
        mode,
        type_comments=True,  # This works the magic
        feature_version=feature_version,
    )


</t>
<t tx="ekr.20230831011819.116">def add_ancestors(self) -&gt; None:
    if self.path is not None:
        _, name = os.path.split(self.path)
        base, _ = os.path.splitext(name)
        if "." in base:
            # This is just a weird filename, don't add anything
            self.ancestors = []
            return
    # All parent packages are new ancestors.
    ancestors = []
    parent = self.id
    while "." in parent:
        parent, _ = parent.rsplit(".", 1)
        ancestors.append(parent)
    self.ancestors = ancestors

</t>
<t tx="ekr.20230831011819.1160">NamedExpr = ast3.NamedExpr
Constant = ast3.Constant

if sys.version_info &gt;= (3, 10):
    Match = ast3.Match
    MatchValue = ast3.MatchValue
    MatchSingleton = ast3.MatchSingleton
    MatchSequence = ast3.MatchSequence
    MatchStar = ast3.MatchStar
    MatchMapping = ast3.MatchMapping
    MatchClass = ast3.MatchClass
    MatchAs = ast3.MatchAs
    MatchOr = ast3.MatchOr
    AstNode = Union[ast3.expr, ast3.stmt, ast3.pattern, ast3.ExceptHandler]
else:
    Match = Any
    MatchValue = Any
    MatchSingleton = Any
    MatchSequence = Any
    MatchStar = Any
    MatchMapping = Any
    MatchClass = Any
    MatchAs = Any
    MatchOr = Any
    AstNode = Union[ast3.expr, ast3.stmt, ast3.ExceptHandler]
if sys.version_info &gt;= (3, 11):
    TryStar = ast3.TryStar
else:
    TryStar = Any

N = TypeVar("N", bound=Node)

# There is no way to create reasonable fallbacks at this stage,
# they must be patched later.
MISSING_FALLBACK: Final = FakeInfo("fallback can't be filled out until semanal")
_dummy_fallback: Final = Instance(MISSING_FALLBACK, [], -1)

TYPE_IGNORE_PATTERN: Final = re.compile(r"[^#]*#\s*type:\s*ignore\s*(.*)")


def parse(
    source: str | bytes,
    fnam: str,
    module: str | None,
    errors: Errors | None = None,
    options: Options | None = None,
) -&gt; MypyFile:
    """Parse a source file, without doing any semantic analysis.

    Return the parse tree. If errors is not provided, raise ParseError
    on failure. Otherwise, use the errors object to report parse errors.
    """
    ignore_errors = (options is not None and options.ignore_errors) or (
        errors is not None and fnam in errors.ignored_files
    )
    # If errors are ignored, we can drop many function bodies to speed up type checking.
    strip_function_bodies = ignore_errors and (options is None or not options.preserve_asts)
    raise_on_error = False
    if options is None:
        options = Options()
    if errors is None:
        errors = Errors(options)
        raise_on_error = True
    errors.set_file(fnam, module, options=options)
    is_stub_file = fnam.endswith(".pyi")
    if is_stub_file:
        feature_version = defaults.PYTHON3_VERSION[1]
        if options.python_version[0] == 3 and options.python_version[1] &gt; feature_version:
            feature_version = options.python_version[1]
    else:
        assert options.python_version[0] &gt;= 3
        feature_version = options.python_version[1]
    try:
        # Disable deprecation warnings about \u
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=DeprecationWarning)
            ast = ast3_parse(source, fnam, "exec", feature_version=feature_version)

        tree = ASTConverter(
            options=options,
            is_stub=is_stub_file,
            errors=errors,
            ignore_errors=ignore_errors,
            strip_function_bodies=strip_function_bodies,
        ).visit(ast)
        tree.path = fnam
        tree.is_stub = is_stub_file
    except SyntaxError as e:
        # alias to please mypyc
        is_py38_or_earlier = sys.version_info &lt; (3, 9)
        if is_py38_or_earlier and e.filename == "&lt;fstring&gt;":
            # In Python 3.8 and earlier, syntax errors in f-strings have lineno relative to the
            # start of the f-string. This would be misleading, as mypy will report the error as the
            # lineno within the file.
            e.lineno = None
        message = e.msg
        if feature_version &gt; sys.version_info.minor and message.startswith("invalid syntax"):
            python_version_str = f"{options.python_version[0]}.{options.python_version[1]}"
            message += f"; you likely need to run mypy using Python {python_version_str} or newer"
        errors.report(
            e.lineno if e.lineno is not None else -1,
            e.offset,
            message,
            blocker=True,
            code=codes.SYNTAX,
        )
        tree = MypyFile([], [], False, {})

    if raise_on_error and errors.is_errors():
        errors.raise_error()

    assert isinstance(tree, MypyFile)
    return tree


</t>
<t tx="ekr.20230831011819.1161">def parse_type_ignore_tag(tag: str | None) -&gt; list[str] | None:
    """Parse optional "[code, ...]" tag after "# type: ignore".

    Return:
     * [] if no tag was found (ignore all errors)
     * list of ignored error codes if a tag was found
     * None if the tag was invalid.
    """
    if not tag or tag.strip() == "" or tag.strip().startswith("#"):
        # No tag -- ignore all errors.
        return []
    m = re.match(r"\s*\[([^]#]*)\]\s*(#.*)?$", tag)
    if m is None:
        # Invalid "# type: ignore" comment.
        return None
    return [code.strip() for code in m.group(1).split(",")]


</t>
<t tx="ekr.20230831011819.1162">def parse_type_comment(
    type_comment: str, line: int, column: int, errors: Errors | None
) -&gt; tuple[list[str] | None, ProperType | None]:
    """Parse type portion of a type comment (+ optional type ignore).

    Return (ignore info, parsed type).
    """
    try:
        typ = ast3_parse(type_comment, "&lt;type_comment&gt;", "eval")
    except SyntaxError:
        if errors is not None:
            stripped_type = type_comment.split("#", 2)[0].strip()
            err_msg = message_registry.TYPE_COMMENT_SYNTAX_ERROR_VALUE.format(stripped_type)
            errors.report(line, column, err_msg.value, blocker=True, code=err_msg.code)
            return None, None
        else:
            raise
    else:
        extra_ignore = TYPE_IGNORE_PATTERN.match(type_comment)
        if extra_ignore:
            tag: str | None = extra_ignore.group(1)
            ignored: list[str] | None = parse_type_ignore_tag(tag)
            if ignored is None:
                if errors is not None:
                    errors.report(
                        line, column, message_registry.INVALID_TYPE_IGNORE.value, code=codes.SYNTAX
                    )
                else:
                    raise SyntaxError
        else:
            ignored = None
        assert isinstance(typ, ast3.Expression)
        converted = TypeConverter(
            errors, line=line, override_column=column, is_evaluated=False
        ).visit(typ.body)
        return ignored, converted


</t>
<t tx="ekr.20230831011819.1163">def parse_type_string(
    expr_string: str, expr_fallback_name: str, line: int, column: int
) -&gt; ProperType:
    """Parses a type that was originally present inside of an explicit string.

    For example, suppose we have the type `Foo["blah"]`. We should parse the
    string expression "blah" using this function.
    """
    try:
        _, node = parse_type_comment(expr_string.strip(), line=line, column=column, errors=None)
        if isinstance(node, UnboundType) and node.original_str_expr is None:
            node.original_str_expr = expr_string
            node.original_str_fallback = expr_fallback_name
            return node
        elif isinstance(node, UnionType):
            return node
        else:
            return RawExpressionType(expr_string, expr_fallback_name, line, column)
    except (SyntaxError, ValueError):
        # Note: the parser will raise a `ValueError` instead of a SyntaxError if
        # the string happens to contain things like \x00.
        return RawExpressionType(expr_string, expr_fallback_name, line, column)


</t>
<t tx="ekr.20230831011819.1164">def is_no_type_check_decorator(expr: ast3.expr) -&gt; bool:
    if isinstance(expr, Name):
        return expr.id == "no_type_check"
    elif isinstance(expr, Attribute):
        if isinstance(expr.value, Name):
            return expr.value.id == "typing" and expr.attr == "no_type_check"
    return False


</t>
<t tx="ekr.20230831011819.1165">class ASTConverter:
    @others
</t>
<t tx="ekr.20230831011819.1166">def __init__(
    self,
    options: Options,
    is_stub: bool,
    errors: Errors,
    *,
    ignore_errors: bool,
    strip_function_bodies: bool,
) -&gt; None:
    # 'C' for class, 'D' for function signature, 'F' for function, 'L' for lambda
    self.class_and_function_stack: list[Literal["C", "D", "F", "L"]] = []
    self.imports: list[ImportBase] = []

    self.options = options
    self.is_stub = is_stub
    self.errors = errors
    self.ignore_errors = ignore_errors
    self.strip_function_bodies = strip_function_bodies

    self.type_ignores: dict[int, list[str]] = {}

    # Cache of visit_X methods keyed by type of visited object
    self.visitor_cache: dict[type, Callable[[AST | None], Any]] = {}

</t>
<t tx="ekr.20230831011819.1167">def note(self, msg: str, line: int, column: int) -&gt; None:
    self.errors.report(line, column, msg, severity="note", code=codes.SYNTAX)

</t>
<t tx="ekr.20230831011819.1168">def fail(self, msg: ErrorMessage, line: int, column: int, blocker: bool = True) -&gt; None:
    if blocker or not self.options.ignore_errors:
        self.errors.report(line, column, msg.value, blocker=blocker, code=msg.code)

</t>
<t tx="ekr.20230831011819.1169">def fail_merge_overload(self, node: IfStmt) -&gt; None:
    self.fail(
        message_registry.FAILED_TO_MERGE_OVERLOADS,
        line=node.line,
        column=node.column,
        blocker=False,
    )

</t>
<t tx="ekr.20230831011819.117">def is_fresh(self) -&gt; bool:
    """Return whether the cache data for this file is fresh."""
    # NOTE: self.dependencies may differ from
    # self.meta.dependencies when a dependency is dropped due to
    # suppression by silent mode.  However when a suppressed
    # dependency is added back we find out later in the process.
    return (
        self.meta is not None
        and self.is_interface_fresh()
        and self.dependencies == self.meta.dependencies
    )

</t>
<t tx="ekr.20230831011819.1170">def visit(self, node: AST | None) -&gt; Any:
    if node is None:
        return None
    typeobj = type(node)
    visitor = self.visitor_cache.get(typeobj)
    if visitor is None:
        method = "visit_" + node.__class__.__name__
        visitor = getattr(self, method)
        self.visitor_cache[typeobj] = visitor
    return visitor(node)

</t>
<t tx="ekr.20230831011819.1171">def set_line(self, node: N, n: AstNode) -&gt; N:
    node.line = n.lineno
    node.column = n.col_offset
    node.end_line = getattr(n, "end_lineno", None)
    node.end_column = getattr(n, "end_col_offset", None)

    return node

</t>
<t tx="ekr.20230831011819.1172">def translate_opt_expr_list(self, l: Sequence[AST | None]) -&gt; list[Expression | None]:
    res: list[Expression | None] = []
    for e in l:
        exp = self.visit(e)
        res.append(exp)
    return res

</t>
<t tx="ekr.20230831011819.1173">def translate_expr_list(self, l: Sequence[AST]) -&gt; list[Expression]:
    return cast(List[Expression], self.translate_opt_expr_list(l))

</t>
<t tx="ekr.20230831011819.1174">def get_lineno(self, node: ast3.expr | ast3.stmt) -&gt; int:
    if (
        isinstance(node, (ast3.AsyncFunctionDef, ast3.ClassDef, ast3.FunctionDef))
        and node.decorator_list
    ):
        return node.decorator_list[0].lineno
    return node.lineno

</t>
<t tx="ekr.20230831011819.1175">def translate_stmt_list(
    self,
    stmts: Sequence[ast3.stmt],
    *,
    ismodule: bool = False,
    can_strip: bool = False,
    is_coroutine: bool = False,
) -&gt; list[Statement]:
    # A "# type: ignore" comment before the first statement of a module
    # ignores the whole module:
    if (
        ismodule
        and stmts
        and self.type_ignores
        and min(self.type_ignores) &lt; self.get_lineno(stmts[0])
    ):
        ignores = self.type_ignores[min(self.type_ignores)]
        if ignores:
            joined_ignores = ", ".join(ignores)
            self.fail(
                message_registry.TYPE_IGNORE_WITH_ERRCODE_ON_MODULE.format(joined_ignores),
                line=min(self.type_ignores),
                column=0,
                blocker=False,
            )
        self.errors.used_ignored_lines[self.errors.file][min(self.type_ignores)].append(
            codes.FILE.code
        )
        block = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        self.set_block_lines(block, stmts)
        mark_block_unreachable(block)
        return [block]

    stack = self.class_and_function_stack
    # Fast case for stripping function bodies
    if (
        can_strip
        and self.strip_function_bodies
        and len(stack) == 1
        and stack[0] == "F"
        and not is_coroutine
    ):
        return []

    res: list[Statement] = []
    for stmt in stmts:
        node = self.visit(stmt)
        res.append(node)

    # Slow case for stripping function bodies
    if can_strip and self.strip_function_bodies:
        if stack[-2:] == ["C", "F"]:
            if is_possible_trivial_body(res):
                can_strip = False
            else:
                # We only strip method bodies if they don't assign to an attribute, as
                # this may define an attribute which has an externally visible effect.
                visitor = FindAttributeAssign()
                for s in res:
                    s.accept(visitor)
                    if visitor.found:
                        can_strip = False
                        break

        if can_strip and stack[-1] == "F" and is_coroutine:
            # Yields inside an async function affect the return type and should not
            # be stripped.
            yield_visitor = FindYield()
            for s in res:
                s.accept(yield_visitor)
                if yield_visitor.found:
                    can_strip = False
                    break

        if can_strip:
            return []
    return res

</t>
<t tx="ekr.20230831011819.1176">def translate_type_comment(
    self, n: ast3.stmt | ast3.arg, type_comment: str | None
) -&gt; ProperType | None:
    if type_comment is None:
        return None
    else:
        lineno = n.lineno
        extra_ignore, typ = parse_type_comment(type_comment, lineno, n.col_offset, self.errors)
        if extra_ignore is not None:
            self.type_ignores[lineno] = extra_ignore
        return typ

</t>
<t tx="ekr.20230831011819.1177">op_map: Final[dict[type[AST], str]] = {
    ast3.Add: "+",
    ast3.Sub: "-",
    ast3.Mult: "*",
    ast3.MatMult: "@",
    ast3.Div: "/",
    ast3.Mod: "%",
    ast3.Pow: "**",
    ast3.LShift: "&lt;&lt;",
    ast3.RShift: "&gt;&gt;",
    ast3.BitOr: "|",
    ast3.BitXor: "^",
    ast3.BitAnd: "&amp;",
    ast3.FloorDiv: "//",
}

def from_operator(self, op: ast3.operator) -&gt; str:
    op_name = ASTConverter.op_map.get(type(op))
    if op_name is None:
        raise RuntimeError("Unknown operator " + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20230831011819.1178">comp_op_map: Final[dict[type[AST], str]] = {
    ast3.Gt: "&gt;",
    ast3.Lt: "&lt;",
    ast3.Eq: "==",
    ast3.GtE: "&gt;=",
    ast3.LtE: "&lt;=",
    ast3.NotEq: "!=",
    ast3.Is: "is",
    ast3.IsNot: "is not",
    ast3.In: "in",
    ast3.NotIn: "not in",
}

def from_comp_operator(self, op: ast3.cmpop) -&gt; str:
    op_name = ASTConverter.comp_op_map.get(type(op))
    if op_name is None:
        raise RuntimeError("Unknown comparison operator " + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20230831011819.1179">def set_block_lines(self, b: Block, stmts: Sequence[ast3.stmt]) -&gt; None:
    first, last = stmts[0], stmts[-1]
    b.line = first.lineno
    b.column = first.col_offset
    b.end_line = getattr(last, "end_lineno", None)
    b.end_column = getattr(last, "end_col_offset", None)
    if not b.body:
        return
    new_first = b.body[0]
    if isinstance(new_first, (Decorator, OverloadedFuncDef)):
        # Decorated function lines are different between Python versions.
        # copy the normalization we do for them to block first lines.
        b.line = new_first.line
        b.column = new_first.column

</t>
<t tx="ekr.20230831011819.118">def is_interface_fresh(self) -&gt; bool:
    return self.externally_same

</t>
<t tx="ekr.20230831011819.1180">def as_block(self, stmts: list[ast3.stmt]) -&gt; Block | None:
    b = None
    if stmts:
        b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        self.set_block_lines(b, stmts)
    return b

</t>
<t tx="ekr.20230831011819.1181">def as_required_block(
    self, stmts: list[ast3.stmt], *, can_strip: bool = False, is_coroutine: bool = False
) -&gt; Block:
    assert stmts  # must be non-empty
    b = Block(
        self.fix_function_overloads(
            self.translate_stmt_list(stmts, can_strip=can_strip, is_coroutine=is_coroutine)
        )
    )
    self.set_block_lines(b, stmts)
    return b

</t>
<t tx="ekr.20230831011819.1182">def fix_function_overloads(self, stmts: list[Statement]) -&gt; list[Statement]:
    ret: list[Statement] = []
    current_overload: list[OverloadPart] = []
    current_overload_name: str | None = None
    seen_unconditional_func_def = False
    last_if_stmt: IfStmt | None = None
    last_if_overload: Decorator | FuncDef | OverloadedFuncDef | None = None
    last_if_stmt_overload_name: str | None = None
    last_if_unknown_truth_value: IfStmt | None = None
    skipped_if_stmts: list[IfStmt] = []
    for stmt in stmts:
        if_overload_name: str | None = None
        if_block_with_overload: Block | None = None
        if_unknown_truth_value: IfStmt | None = None
        if isinstance(stmt, IfStmt) and seen_unconditional_func_def is False:
            # Check IfStmt block to determine if function overloads can be merged
            if_overload_name = self._check_ifstmt_for_overloads(stmt, current_overload_name)
            if if_overload_name is not None:
                (
                    if_block_with_overload,
                    if_unknown_truth_value,
                ) = self._get_executable_if_block_with_overloads(stmt)

        if (
            current_overload_name is not None
            and isinstance(stmt, (Decorator, FuncDef))
            and stmt.name == current_overload_name
        ):
            if last_if_stmt is not None:
                skipped_if_stmts.append(last_if_stmt)
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if last_if_unknown_truth_value:
                self.fail_merge_overload(last_if_unknown_truth_value)
                last_if_unknown_truth_value = None
            current_overload.append(stmt)
            if isinstance(stmt, FuncDef):
                seen_unconditional_func_def = True
        elif (
            current_overload_name is not None
            and isinstance(stmt, IfStmt)
            and if_overload_name == current_overload_name
        ):
            # IfStmt only contains stmts relevant to current_overload.
            # Check if stmts are reachable and add them to current_overload,
            # otherwise skip IfStmt to allow subsequent overload
            # or function definitions.
            skipped_if_stmts.append(stmt)
            if if_block_with_overload is None:
                if if_unknown_truth_value is not None:
                    self.fail_merge_overload(if_unknown_truth_value)
                continue
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if isinstance(if_block_with_overload.body[-1], OverloadedFuncDef):
                skipped_if_stmts.extend(cast(List[IfStmt], if_block_with_overload.body[:-1]))
                current_overload.extend(if_block_with_overload.body[-1].items)
            else:
                current_overload.append(
                    cast(Union[Decorator, FuncDef], if_block_with_overload.body[0])
                )
        else:
            if last_if_stmt is not None:
                ret.append(last_if_stmt)
                last_if_stmt_overload_name = current_overload_name
                last_if_stmt, last_if_overload = None, None
                last_if_unknown_truth_value = None

            if current_overload and current_overload_name == last_if_stmt_overload_name:
                # Remove last stmt (IfStmt) from ret if the overload names matched
                # Only happens if no executable block had been found in IfStmt
                popped = ret.pop()
                assert isinstance(popped, IfStmt)
                skipped_if_stmts.append(popped)
            if current_overload and skipped_if_stmts:
                # Add bare IfStmt (without overloads) to ret
                # Required for mypy to be able to still check conditions
                for if_stmt in skipped_if_stmts:
                    self._strip_contents_from_if_stmt(if_stmt)
                    ret.append(if_stmt)
                skipped_if_stmts = []
            if len(current_overload) == 1:
                ret.append(current_overload[0])
            elif len(current_overload) &gt; 1:
                ret.append(OverloadedFuncDef(current_overload))

            # If we have multiple decorated functions named "_" next to each, we want to treat
            # them as a series of regular FuncDefs instead of one OverloadedFuncDef because
            # most of mypy/mypyc assumes that all the functions in an OverloadedFuncDef are
            # related, but multiple underscore functions next to each other aren't necessarily
            # related
            seen_unconditional_func_def = False
            if isinstance(stmt, Decorator) and not unnamed_function(stmt.name):
                current_overload = [stmt]
                current_overload_name = stmt.name
            elif isinstance(stmt, IfStmt) and if_overload_name is not None:
                current_overload = []
                current_overload_name = if_overload_name
                last_if_stmt = stmt
                last_if_stmt_overload_name = None
                if if_block_with_overload is not None:
                    skipped_if_stmts.extend(
                        cast(List[IfStmt], if_block_with_overload.body[:-1])
                    )
                    last_if_overload = cast(
                        Union[Decorator, FuncDef, OverloadedFuncDef],
                        if_block_with_overload.body[-1],
                    )
                last_if_unknown_truth_value = if_unknown_truth_value
            else:
                current_overload = []
                current_overload_name = None
                ret.append(stmt)

    if current_overload and skipped_if_stmts:
        # Add bare IfStmt (without overloads) to ret
        # Required for mypy to be able to still check conditions
        for if_stmt in skipped_if_stmts:
            self._strip_contents_from_if_stmt(if_stmt)
            ret.append(if_stmt)
    if len(current_overload) == 1:
        ret.append(current_overload[0])
    elif len(current_overload) &gt; 1:
        ret.append(OverloadedFuncDef(current_overload))
    elif last_if_overload is not None:
        ret.append(last_if_overload)
    elif last_if_stmt is not None:
        ret.append(last_if_stmt)
    return ret

</t>
<t tx="ekr.20230831011819.1183">def _check_ifstmt_for_overloads(
    self, stmt: IfStmt, current_overload_name: str | None = None
) -&gt; str | None:
    """Check if IfStmt contains only overloads with the same name.
    Return overload_name if found, None otherwise.
    """
    # Check that block only contains a single Decorator, FuncDef, or OverloadedFuncDef.
    # Multiple overloads have already been merged as OverloadedFuncDef.
    if not (
        len(stmt.body[0].body) == 1
        and (
            isinstance(stmt.body[0].body[0], (Decorator, OverloadedFuncDef))
            or current_overload_name is not None
            and isinstance(stmt.body[0].body[0], FuncDef)
        )
        or len(stmt.body[0].body) &gt; 1
        and isinstance(stmt.body[0].body[-1], OverloadedFuncDef)
        and all(self._is_stripped_if_stmt(if_stmt) for if_stmt in stmt.body[0].body[:-1])
    ):
        return None

    overload_name = cast(
        Union[Decorator, FuncDef, OverloadedFuncDef], stmt.body[0].body[-1]
    ).name
    if stmt.else_body is None:
        return overload_name

    if len(stmt.else_body.body) == 1:
        # For elif: else_body contains an IfStmt itself -&gt; do a recursive check.
        if (
            isinstance(stmt.else_body.body[0], (Decorator, FuncDef, OverloadedFuncDef))
            and stmt.else_body.body[0].name == overload_name
        ):
            return overload_name
        if (
            isinstance(stmt.else_body.body[0], IfStmt)
            and self._check_ifstmt_for_overloads(stmt.else_body.body[0], current_overload_name)
            == overload_name
        ):
            return overload_name

    return None

</t>
<t tx="ekr.20230831011819.1184">def _get_executable_if_block_with_overloads(
    self, stmt: IfStmt
) -&gt; tuple[Block | None, IfStmt | None]:
    """Return block from IfStmt that will get executed.

    Return
        0 -&gt; A block if sure that alternative blocks are unreachable.
        1 -&gt; An IfStmt if the reachability of it can't be inferred,
             i.e. the truth value is unknown.
    """
    infer_reachability_of_if_statement(stmt, self.options)
    if stmt.else_body is None and stmt.body[0].is_unreachable is True:
        # always False condition with no else
        return None, None
    if (
        stmt.else_body is None
        or stmt.body[0].is_unreachable is False
        and stmt.else_body.is_unreachable is False
    ):
        # The truth value is unknown, thus not conclusive
        return None, stmt
    if stmt.else_body.is_unreachable is True:
        # else_body will be set unreachable if condition is always True
        return stmt.body[0], None
    if stmt.body[0].is_unreachable is True:
        # body will be set unreachable if condition is always False
        # else_body can contain an IfStmt itself (for elif) -&gt; do a recursive check
        if isinstance(stmt.else_body.body[0], IfStmt):
            return self._get_executable_if_block_with_overloads(stmt.else_body.body[0])
        return stmt.else_body, None
    return None, stmt

</t>
<t tx="ekr.20230831011819.1185">def _strip_contents_from_if_stmt(self, stmt: IfStmt) -&gt; None:
    """Remove contents from IfStmt.

    Needed to still be able to check the conditions after the contents
    have been merged with the surrounding function overloads.
    """
    if len(stmt.body) == 1:
        stmt.body[0].body = []
    if stmt.else_body and len(stmt.else_body.body) == 1:
        if isinstance(stmt.else_body.body[0], IfStmt):
            self._strip_contents_from_if_stmt(stmt.else_body.body[0])
        else:
            stmt.else_body.body = []

</t>
<t tx="ekr.20230831011819.1186">def _is_stripped_if_stmt(self, stmt: Statement) -&gt; bool:
    """Check stmt to make sure it is a stripped IfStmt.

    See also: _strip_contents_from_if_stmt
    """
    if not isinstance(stmt, IfStmt):
        return False

    if not (len(stmt.body) == 1 and len(stmt.body[0].body) == 0):
        # Body not empty
        return False

    if not stmt.else_body or len(stmt.else_body.body) == 0:
        # No or empty else_body
        return True

    # For elif, IfStmt are stored recursively in else_body
    return self._is_stripped_if_stmt(stmt.else_body.body[0])

</t>
<t tx="ekr.20230831011819.1187">def translate_module_id(self, id: str) -&gt; str:
    """Return the actual, internal module id for a source text id."""
    if id == self.options.custom_typing_module:
        return "typing"
    return id

</t>
<t tx="ekr.20230831011819.1188">def visit_Module(self, mod: ast3.Module) -&gt; MypyFile:
    self.type_ignores = {}
    for ti in mod.type_ignores:
        parsed = parse_type_ignore_tag(ti.tag)
        if parsed is not None:
            self.type_ignores[ti.lineno] = parsed
        else:
            self.fail(message_registry.INVALID_TYPE_IGNORE, ti.lineno, -1, blocker=False)
    body = self.fix_function_overloads(self.translate_stmt_list(mod.body, ismodule=True))
    return MypyFile(body, self.imports, False, self.type_ignores)

</t>
<t tx="ekr.20230831011819.1189"># --- stmt ---
# FunctionDef(identifier name, arguments args,
#             stmt* body, expr* decorator_list, expr? returns, string? type_comment)
# arguments = (arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults,
#              arg? kwarg, expr* defaults)
def visit_FunctionDef(self, n: ast3.FunctionDef) -&gt; FuncDef | Decorator:
    return self.do_func_def(n)

</t>
<t tx="ekr.20230831011819.119">def mark_as_rechecked(self) -&gt; None:
    """Marks this module as having been fully re-analyzed by the type-checker."""
    self.manager.rechecked_modules.add(self.id)

</t>
<t tx="ekr.20230831011819.1190"># AsyncFunctionDef(identifier name, arguments args,
#                  stmt* body, expr* decorator_list, expr? returns, string? type_comment)
def visit_AsyncFunctionDef(self, n: ast3.AsyncFunctionDef) -&gt; FuncDef | Decorator:
    return self.do_func_def(n, is_coroutine=True)

</t>
<t tx="ekr.20230831011819.1191">def do_func_def(
    self, n: ast3.FunctionDef | ast3.AsyncFunctionDef, is_coroutine: bool = False
) -&gt; FuncDef | Decorator:
    """Helper shared between visit_FunctionDef and visit_AsyncFunctionDef."""
    self.class_and_function_stack.append("D")
    no_type_check = bool(
        n.decorator_list and any(is_no_type_check_decorator(d) for d in n.decorator_list)
    )

    lineno = n.lineno
    args = self.transform_args(n.args, lineno, no_type_check=no_type_check)
    if special_function_elide_names(n.name):
        for arg in args:
            arg.pos_only = True

    arg_kinds = [arg.kind for arg in args]
    arg_names = [None if arg.pos_only else arg.variable.name for arg in args]

    arg_types: list[Type | None] = []
    if no_type_check:
        arg_types = [None] * len(args)
        return_type = None
    elif n.type_comment is not None:
        try:
            func_type_ast = ast3_parse(n.type_comment, "&lt;func_type&gt;", "func_type")
            assert isinstance(func_type_ast, FunctionType)
            # for ellipsis arg
            if (
                len(func_type_ast.argtypes) == 1
                and isinstance(func_type_ast.argtypes[0], Constant)
                and func_type_ast.argtypes[0].value is Ellipsis
            ):
                if n.returns:
                    # PEP 484 disallows both type annotations and type comments
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                arg_types = [
                    a.type_annotation
                    if a.type_annotation is not None
                    else AnyType(TypeOfAny.unannotated)
                    for a in args
                ]
            else:
                # PEP 484 disallows both type annotations and type comments
                if n.returns or any(a.type_annotation is not None for a in args):
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                translated_args: list[Type] = TypeConverter(
                    self.errors, line=lineno, override_column=n.col_offset
                ).translate_expr_list(func_type_ast.argtypes)
                # Use a cast to work around `list` invariance
                arg_types = cast(List[Optional[Type]], translated_args)
            return_type = TypeConverter(self.errors, line=lineno).visit(func_type_ast.returns)

            # add implicit self type
            in_method_scope = self.class_and_function_stack[-2:] == ["C", "D"]
            if in_method_scope and len(arg_types) &lt; len(args):
                arg_types.insert(0, AnyType(TypeOfAny.special_form))
        except SyntaxError:
            stripped_type = n.type_comment.split("#", 2)[0].strip()
            err_msg = message_registry.TYPE_COMMENT_SYNTAX_ERROR_VALUE.format(stripped_type)
            self.fail(err_msg, lineno, n.col_offset)
            if n.type_comment and n.type_comment[0] not in ["(", "#"]:
                self.note(
                    "Suggestion: wrap argument types in parentheses", lineno, n.col_offset
                )
            arg_types = [AnyType(TypeOfAny.from_error)] * len(args)
            return_type = AnyType(TypeOfAny.from_error)
    else:
        arg_types = [a.type_annotation for a in args]
        return_type = TypeConverter(
            self.errors, line=n.returns.lineno if n.returns else lineno
        ).visit(n.returns)

    for arg, arg_type in zip(args, arg_types):
        self.set_type_optional(arg_type, arg.initializer)

    func_type = None
    if any(arg_types) or return_type:
        if len(arg_types) != 1 and any(isinstance(t, EllipsisType) for t in arg_types):
            self.fail(message_registry.ELLIPSIS_WITH_OTHER_TYPEARGS, lineno, n.col_offset)
        elif len(arg_types) &gt; len(arg_kinds):
            self.fail(
                message_registry.TYPE_SIGNATURE_TOO_MANY_ARGS,
                lineno,
                n.col_offset,
                blocker=False,
            )
        elif len(arg_types) &lt; len(arg_kinds):
            self.fail(
                message_registry.TYPE_SIGNATURE_TOO_FEW_ARGS,
                lineno,
                n.col_offset,
                blocker=False,
            )
        else:
            func_type = CallableType(
                [a if a is not None else AnyType(TypeOfAny.unannotated) for a in arg_types],
                arg_kinds,
                arg_names,
                return_type if return_type is not None else AnyType(TypeOfAny.unannotated),
                _dummy_fallback,
            )

    # End position is always the same.
    end_line = getattr(n, "end_lineno", None)
    end_column = getattr(n, "end_col_offset", None)

    self.class_and_function_stack.pop()
    self.class_and_function_stack.append("F")
    body = self.as_required_block(n.body, can_strip=True, is_coroutine=is_coroutine)
    func_def = FuncDef(n.name, args, body, func_type)
    if isinstance(func_def.type, CallableType):
        # semanal.py does some in-place modifications we want to avoid
        func_def.unanalyzed_type = func_def.type.copy_modified()
    if is_coroutine:
        func_def.is_coroutine = True
    if func_type is not None:
        func_type.definition = func_def
        func_type.line = lineno

    if n.decorator_list:
        # Set deco_line to the old pre-3.8 lineno, in order to keep
        # existing "# type: ignore" comments working:
        deco_line = n.decorator_list[0].lineno

        var = Var(func_def.name)
        var.is_ready = False
        var.set_line(lineno)

        func_def.is_decorated = True
        func_def.deco_line = deco_line
        func_def.set_line(lineno, n.col_offset, end_line, end_column)

        deco = Decorator(func_def, self.translate_expr_list(n.decorator_list), var)
        first = n.decorator_list[0]
        deco.set_line(first.lineno, first.col_offset, end_line, end_column)
        retval: FuncDef | Decorator = deco
    else:
        # FuncDef overrides set_line -- can't use self.set_line
        func_def.set_line(lineno, n.col_offset, end_line, end_column)
        retval = func_def
    if self.options.include_docstrings:
        func_def.docstring = ast3.get_docstring(n, clean=False)
    self.class_and_function_stack.pop()
    return retval

</t>
<t tx="ekr.20230831011819.1192">def set_type_optional(self, type: Type | None, initializer: Expression | None) -&gt; None:
    if not self.options.implicit_optional:
        return
    # Indicate that type should be wrapped in an Optional if arg is initialized to None.
    optional = isinstance(initializer, NameExpr) and initializer.name == "None"
    if isinstance(type, UnboundType):
        type.optional = optional

</t>
<t tx="ekr.20230831011819.1193">def transform_args(
    self, args: ast3.arguments, line: int, no_type_check: bool = False
) -&gt; list[Argument]:
    new_args = []
    names: list[ast3.arg] = []
    posonlyargs = getattr(args, "posonlyargs", cast(List[ast3.arg], []))
    args_args = posonlyargs + args.args
    args_defaults = args.defaults
    num_no_defaults = len(args_args) - len(args_defaults)
    # positional arguments without defaults
    for i, a in enumerate(args_args[:num_no_defaults]):
        pos_only = i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, None, ARG_POS, no_type_check, pos_only))
        names.append(a)

    # positional arguments with defaults
    for i, (a, d) in enumerate(zip(args_args[num_no_defaults:], args_defaults)):
        pos_only = num_no_defaults + i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, d, ARG_OPT, no_type_check, pos_only))
        names.append(a)

    # *arg
    if args.vararg is not None:
        new_args.append(self.make_argument(args.vararg, None, ARG_STAR, no_type_check))
        names.append(args.vararg)

    # keyword-only arguments with defaults
    for a, kd in zip(args.kwonlyargs, args.kw_defaults):
        new_args.append(
            self.make_argument(
                a, kd, ARG_NAMED if kd is None else ARG_NAMED_OPT, no_type_check
            )
        )
        names.append(a)

    # **kwarg
    if args.kwarg is not None:
        new_args.append(self.make_argument(args.kwarg, None, ARG_STAR2, no_type_check))
        names.append(args.kwarg)

    check_arg_names([arg.variable.name for arg in new_args], names, self.fail_arg)

    return new_args

</t>
<t tx="ekr.20230831011819.1194">def make_argument(
    self,
    arg: ast3.arg,
    default: ast3.expr | None,
    kind: ArgKind,
    no_type_check: bool,
    pos_only: bool = False,
) -&gt; Argument:
    if no_type_check:
        arg_type = None
    else:
        annotation = arg.annotation
        type_comment = arg.type_comment
        if annotation is not None and type_comment is not None:
            self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, arg.lineno, arg.col_offset)
        arg_type = None
        if annotation is not None:
            arg_type = TypeConverter(self.errors, line=arg.lineno).visit(annotation)
        else:
            arg_type = self.translate_type_comment(arg, type_comment)
    if argument_elide_name(arg.arg):
        pos_only = True

    argument = Argument(Var(arg.arg), arg_type, self.visit(default), kind, pos_only)
    argument.set_line(
        arg.lineno,
        arg.col_offset,
        getattr(arg, "end_lineno", None),
        getattr(arg, "end_col_offset", None),
    )
    return argument

</t>
<t tx="ekr.20230831011819.1195">def fail_arg(self, msg: str, arg: ast3.arg) -&gt; None:
    self.fail(ErrorMessage(msg), arg.lineno, arg.col_offset)

</t>
<t tx="ekr.20230831011819.1196"># ClassDef(identifier name,
#  expr* bases,
#  keyword* keywords,
#  stmt* body,
#  expr* decorator_list)
def visit_ClassDef(self, n: ast3.ClassDef) -&gt; ClassDef:
    self.class_and_function_stack.append("C")
    keywords = [(kw.arg, self.visit(kw.value)) for kw in n.keywords if kw.arg]

    cdef = ClassDef(
        n.name,
        self.as_required_block(n.body),
        None,
        self.translate_expr_list(n.bases),
        metaclass=dict(keywords).get("metaclass"),
        keywords=keywords,
    )
    cdef.decorators = self.translate_expr_list(n.decorator_list)
    # Set lines to match the old mypy 0.700 lines, in order to keep
    # existing "# type: ignore" comments working:
    cdef.line = n.lineno
    cdef.deco_line = n.decorator_list[0].lineno if n.decorator_list else None

    if self.options.include_docstrings:
        cdef.docstring = ast3.get_docstring(n, clean=False)
    cdef.column = n.col_offset
    cdef.end_line = getattr(n, "end_lineno", None)
    cdef.end_column = getattr(n, "end_col_offset", None)
    self.class_and_function_stack.pop()
    return cdef

</t>
<t tx="ekr.20230831011819.1197"># Return(expr? value)
def visit_Return(self, n: ast3.Return) -&gt; ReturnStmt:
    node = ReturnStmt(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1198"># Delete(expr* targets)
def visit_Delete(self, n: ast3.Delete) -&gt; DelStmt:
    if len(n.targets) &gt; 1:
        tup = TupleExpr(self.translate_expr_list(n.targets))
        tup.set_line(n.lineno)
        node = DelStmt(tup)
    else:
        node = DelStmt(self.visit(n.targets[0]))
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1199"># Assign(expr* targets, expr? value, string? type_comment, expr? annotation)
def visit_Assign(self, n: ast3.Assign) -&gt; AssignmentStmt:
    lvalues = self.translate_expr_list(n.targets)
    rvalue = self.visit(n.value)
    typ = self.translate_type_comment(n, n.type_comment)
    s = AssignmentStmt(lvalues, rvalue, type=typ, new_syntax=False)
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.12">def run_dmypy(args: list[str]) -&gt; tuple[str, str, int]:
    from mypy.dmypy.client import main

    # A bunch of effort has been put into threading stdout and stderr
    # through the main API to avoid the threadsafety problems of
    # modifying sys.stdout/sys.stderr, but that hasn't been done for
    # the dmypy client, so we just do the non-threadsafe thing.
    def f(stdout: TextIO, stderr: TextIO) -&gt; None:
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        try:
            sys.stdout = stdout
            sys.stderr = stderr
            main(args)
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr

    return _run(f)
</t>
<t tx="ekr.20230831011819.120">def mark_interface_stale(self, *, on_errors: bool = False) -&gt; None:
    """Marks this module as having a stale public interface, and discards the cache data."""
    self.externally_same = False
    if not on_errors:
        self.manager.stale_modules.add(self.id)

</t>
<t tx="ekr.20230831011819.1200"># AnnAssign(expr target, expr annotation, expr? value, int simple)
def visit_AnnAssign(self, n: ast3.AnnAssign) -&gt; AssignmentStmt:
    line = n.lineno
    if n.value is None:  # always allow 'x: int'
        rvalue: Expression = TempNode(AnyType(TypeOfAny.special_form), no_rhs=True)
        rvalue.line = line
        rvalue.column = n.col_offset
    else:
        rvalue = self.visit(n.value)
    typ = TypeConverter(self.errors, line=line).visit(n.annotation)
    assert typ is not None
    typ.column = n.annotation.col_offset
    s = AssignmentStmt([self.visit(n.target)], rvalue, type=typ, new_syntax=True)
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.1201"># AugAssign(expr target, operator op, expr value)
def visit_AugAssign(self, n: ast3.AugAssign) -&gt; OperatorAssignmentStmt:
    s = OperatorAssignmentStmt(
        self.from_operator(n.op), self.visit(n.target), self.visit(n.value)
    )
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.1202"># For(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_For(self, n: ast3.For) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(
        self.visit(n.target),
        self.visit(n.iter),
        self.as_required_block(n.body),
        self.as_block(n.orelse),
        target_type,
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1203"># AsyncFor(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_AsyncFor(self, n: ast3.AsyncFor) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(
        self.visit(n.target),
        self.visit(n.iter),
        self.as_required_block(n.body),
        self.as_block(n.orelse),
        target_type,
    )
    node.is_async = True
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1204"># While(expr test, stmt* body, stmt* orelse)
def visit_While(self, n: ast3.While) -&gt; WhileStmt:
    node = WhileStmt(
        self.visit(n.test), self.as_required_block(n.body), self.as_block(n.orelse)
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1205"># If(expr test, stmt* body, stmt* orelse)
def visit_If(self, n: ast3.If) -&gt; IfStmt:
    node = IfStmt(
        [self.visit(n.test)], [self.as_required_block(n.body)], self.as_block(n.orelse)
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1206"># With(withitem* items, stmt* body, string? type_comment)
def visit_With(self, n: ast3.With) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = WithStmt(
        [self.visit(i.context_expr) for i in n.items],
        [self.visit(i.optional_vars) for i in n.items],
        self.as_required_block(n.body),
        target_type,
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1207"># AsyncWith(withitem* items, stmt* body, string? type_comment)
def visit_AsyncWith(self, n: ast3.AsyncWith) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    s = WithStmt(
        [self.visit(i.context_expr) for i in n.items],
        [self.visit(i.optional_vars) for i in n.items],
        self.as_required_block(n.body),
        target_type,
    )
    s.is_async = True
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.1208"># Raise(expr? exc, expr? cause)
def visit_Raise(self, n: ast3.Raise) -&gt; RaiseStmt:
    node = RaiseStmt(self.visit(n.exc), self.visit(n.cause))
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1209"># Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)
def visit_Try(self, n: ast3.Try) -&gt; TryStmt:
    vs = [
        self.set_line(NameExpr(h.name), h) if h.name is not None else None for h in n.handlers
    ]
    types = [self.visit(h.type) for h in n.handlers]
    handlers = [self.as_required_block(h.body) for h in n.handlers]

    node = TryStmt(
        self.as_required_block(n.body),
        vs,
        types,
        handlers,
        self.as_block(n.orelse),
        self.as_block(n.finalbody),
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.121">def check_blockers(self) -&gt; None:
    """Raise CompileError if a blocking error is detected."""
    if self.manager.errors.is_blockers():
        self.manager.log("Bailing due to blocking errors")
        self.manager.errors.raise_error()

</t>
<t tx="ekr.20230831011819.1210">def visit_TryStar(self, n: TryStar) -&gt; TryStmt:
    vs = [
        self.set_line(NameExpr(h.name), h) if h.name is not None else None for h in n.handlers
    ]
    types = [self.visit(h.type) for h in n.handlers]
    handlers = [self.as_required_block(h.body) for h in n.handlers]

    node = TryStmt(
        self.as_required_block(n.body),
        vs,
        types,
        handlers,
        self.as_block(n.orelse),
        self.as_block(n.finalbody),
    )
    node.is_star = True
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1211"># Assert(expr test, expr? msg)
def visit_Assert(self, n: ast3.Assert) -&gt; AssertStmt:
    node = AssertStmt(self.visit(n.test), self.visit(n.msg))
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1212"># Import(alias* names)
def visit_Import(self, n: ast3.Import) -&gt; Import:
    names: list[tuple[str, str | None]] = []
    for alias in n.names:
        name = self.translate_module_id(alias.name)
        asname = alias.asname
        if asname is None and name != alias.name:
            # if the module name has been translated (and it's not already
            # an explicit import-as), make it an implicit import-as the
            # original name
            asname = alias.name
        names.append((name, asname))
    i = Import(names)
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20230831011819.1213"># ImportFrom(identifier? module, alias* names, int? level)
def visit_ImportFrom(self, n: ast3.ImportFrom) -&gt; ImportBase:
    assert n.level is not None
    if len(n.names) == 1 and n.names[0].name == "*":
        mod = n.module if n.module is not None else ""
        i: ImportBase = ImportAll(mod, n.level)
    else:
        i = ImportFrom(
            self.translate_module_id(n.module) if n.module is not None else "",
            n.level,
            [(a.name, a.asname) for a in n.names],
        )
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20230831011819.1214"># Global(identifier* names)
def visit_Global(self, n: ast3.Global) -&gt; GlobalDecl:
    g = GlobalDecl(n.names)
    return self.set_line(g, n)

</t>
<t tx="ekr.20230831011819.1215"># Nonlocal(identifier* names)
def visit_Nonlocal(self, n: ast3.Nonlocal) -&gt; NonlocalDecl:
    d = NonlocalDecl(n.names)
    return self.set_line(d, n)

</t>
<t tx="ekr.20230831011819.1216"># Expr(expr value)
def visit_Expr(self, n: ast3.Expr) -&gt; ExpressionStmt:
    value = self.visit(n.value)
    node = ExpressionStmt(value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1217"># Pass
def visit_Pass(self, n: ast3.Pass) -&gt; PassStmt:
    s = PassStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.1218"># Break
def visit_Break(self, n: ast3.Break) -&gt; BreakStmt:
    s = BreakStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.1219"># Continue
def visit_Continue(self, n: ast3.Continue) -&gt; ContinueStmt:
    s = ContinueStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.122">@contextlib.contextmanager
def wrap_context(self, check_blockers: bool = True) -&gt; Iterator[None]:
    """Temporarily change the error import context to match this state.

    Also report an internal error if an unexpected exception was raised
    and raise an exception on a blocking error, unless
    check_blockers is False. Skipping blocking error reporting is used
    in the semantic analyzer so that we can report all blocking errors
    for a file (across multiple targets) to maintain backward
    compatibility.
    """
    save_import_context = self.manager.errors.import_context()
    self.manager.errors.set_import_context(self.import_context)
    try:
        yield
    except CompileError:
        raise
    except Exception as err:
        report_internal_error(
            err,
            self.path,
            0,
            self.manager.errors,
            self.options,
            self.manager.stdout,
            self.manager.stderr,
        )
    self.manager.errors.set_import_context(save_import_context)
    # TODO: Move this away once we've removed the old semantic analyzer?
    if check_blockers:
        self.check_blockers()

</t>
<t tx="ekr.20230831011819.1220"># --- expr ---

def visit_NamedExpr(self, n: NamedExpr) -&gt; AssignmentExpr:
    s = AssignmentExpr(self.visit(n.target), self.visit(n.value))
    return self.set_line(s, n)

</t>
<t tx="ekr.20230831011819.1221"># BoolOp(boolop op, expr* values)
def visit_BoolOp(self, n: ast3.BoolOp) -&gt; OpExpr:
    # mypy translates (1 and 2 and 3) as (1 and (2 and 3))
    assert len(n.values) &gt;= 2
    op_node = n.op
    if isinstance(op_node, ast3.And):
        op = "and"
    elif isinstance(op_node, ast3.Or):
        op = "or"
    else:
        raise RuntimeError("unknown BoolOp " + str(type(n)))

    # potentially inefficient!
    return self.group(op, self.translate_expr_list(n.values), n)

</t>
<t tx="ekr.20230831011819.1222">def group(self, op: str, vals: list[Expression], n: ast3.expr) -&gt; OpExpr:
    if len(vals) == 2:
        e = OpExpr(op, vals[0], vals[1])
    else:
        e = OpExpr(op, vals[0], self.group(op, vals[1:], n))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1223"># BinOp(expr left, operator op, expr right)
def visit_BinOp(self, n: ast3.BinOp) -&gt; OpExpr:
    op = self.from_operator(n.op)

    if op is None:
        raise RuntimeError("cannot translate BinOp " + str(type(n.op)))

    e = OpExpr(op, self.visit(n.left), self.visit(n.right))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1224"># UnaryOp(unaryop op, expr operand)
def visit_UnaryOp(self, n: ast3.UnaryOp) -&gt; UnaryExpr:
    op = None
    if isinstance(n.op, ast3.Invert):
        op = "~"
    elif isinstance(n.op, ast3.Not):
        op = "not"
    elif isinstance(n.op, ast3.UAdd):
        op = "+"
    elif isinstance(n.op, ast3.USub):
        op = "-"

    if op is None:
        raise RuntimeError("cannot translate UnaryOp " + str(type(n.op)))

    e = UnaryExpr(op, self.visit(n.operand))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1225"># Lambda(arguments args, expr body)
def visit_Lambda(self, n: ast3.Lambda) -&gt; LambdaExpr:
    body = ast3.Return(n.body)
    body.lineno = n.body.lineno
    body.col_offset = n.body.col_offset

    self.class_and_function_stack.append("L")
    e = LambdaExpr(self.transform_args(n.args, n.lineno), self.as_required_block([body]))
    self.class_and_function_stack.pop()
    e.set_line(n.lineno, n.col_offset)  # Overrides set_line -- can't use self.set_line
    return e

</t>
<t tx="ekr.20230831011819.1226"># IfExp(expr test, expr body, expr orelse)
def visit_IfExp(self, n: ast3.IfExp) -&gt; ConditionalExpr:
    e = ConditionalExpr(self.visit(n.test), self.visit(n.body), self.visit(n.orelse))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1227"># Dict(expr* keys, expr* values)
def visit_Dict(self, n: ast3.Dict) -&gt; DictExpr:
    e = DictExpr(
        list(zip(self.translate_opt_expr_list(n.keys), self.translate_expr_list(n.values)))
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1228"># Set(expr* elts)
def visit_Set(self, n: ast3.Set) -&gt; SetExpr:
    e = SetExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1229"># ListComp(expr elt, comprehension* generators)
def visit_ListComp(self, n: ast3.ListComp) -&gt; ListComprehension:
    e = ListComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.123">def load_fine_grained_deps(self) -&gt; dict[str, set[str]]:
    return self.manager.load_fine_grained_deps(self.id)

</t>
<t tx="ekr.20230831011819.1230"># SetComp(expr elt, comprehension* generators)
def visit_SetComp(self, n: ast3.SetComp) -&gt; SetComprehension:
    e = SetComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1231"># DictComp(expr key, expr value, comprehension* generators)
def visit_DictComp(self, n: ast3.DictComp) -&gt; DictionaryComprehension:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = DictionaryComprehension(
        self.visit(n.key), self.visit(n.value), targets, iters, ifs_list, is_async
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1232"># GeneratorExp(expr elt, comprehension* generators)
def visit_GeneratorExp(self, n: ast3.GeneratorExp) -&gt; GeneratorExpr:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = GeneratorExpr(self.visit(n.elt), targets, iters, ifs_list, is_async)
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1233"># Await(expr value)
def visit_Await(self, n: ast3.Await) -&gt; AwaitExpr:
    v = self.visit(n.value)
    e = AwaitExpr(v)
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1234"># Yield(expr? value)
def visit_Yield(self, n: ast3.Yield) -&gt; YieldExpr:
    e = YieldExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1235"># YieldFrom(expr value)
def visit_YieldFrom(self, n: ast3.YieldFrom) -&gt; YieldFromExpr:
    e = YieldFromExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1236"># Compare(expr left, cmpop* ops, expr* comparators)
def visit_Compare(self, n: ast3.Compare) -&gt; ComparisonExpr:
    operators = [self.from_comp_operator(o) for o in n.ops]
    operands = self.translate_expr_list([n.left] + n.comparators)
    e = ComparisonExpr(operators, operands)
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1237"># Call(expr func, expr* args, keyword* keywords)
# keyword = (identifier? arg, expr value)
def visit_Call(self, n: Call) -&gt; CallExpr:
    args = n.args
    keywords = n.keywords
    keyword_names = [k.arg for k in keywords]
    arg_types = self.translate_expr_list(
        [a.value if isinstance(a, Starred) else a for a in args] + [k.value for k in keywords]
    )
    arg_kinds = [ARG_STAR if type(a) is Starred else ARG_POS for a in args] + [
        ARG_STAR2 if arg is None else ARG_NAMED for arg in keyword_names
    ]
    e = CallExpr(
        self.visit(n.func),
        arg_types,
        arg_kinds,
        cast("List[Optional[str]]", [None] * len(args)) + keyword_names,
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1238"># Constant(object value) -- a constant, in Python 3.8.
def visit_Constant(self, n: Constant) -&gt; Any:
    val = n.value
    e: Any = None
    if val is None:
        e = NameExpr("None")
    elif isinstance(val, str):
        e = StrExpr(val)
    elif isinstance(val, bytes):
        e = BytesExpr(bytes_to_human_readable_repr(val))
    elif isinstance(val, bool):  # Must check before int!
        e = NameExpr(str(val))
    elif isinstance(val, int):
        e = IntExpr(val)
    elif isinstance(val, float):
        e = FloatExpr(val)
    elif isinstance(val, complex):
        e = ComplexExpr(val)
    elif val is Ellipsis:
        e = EllipsisExpr()
    else:
        raise RuntimeError("Constant not implemented for " + str(type(val)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1239"># JoinedStr(expr* values)
def visit_JoinedStr(self, n: ast3.JoinedStr) -&gt; Expression:
    # Each of n.values is a str or FormattedValue; we just concatenate
    # them all using ''.join.
    empty_string = StrExpr("")
    empty_string.set_line(n.lineno, n.col_offset)
    strs_to_join = ListExpr(self.translate_expr_list(n.values))
    strs_to_join.set_line(empty_string)
    # Don't make unnecessary join call if there is only one str to join
    if len(strs_to_join.items) == 1:
        return self.set_line(strs_to_join.items[0], n)
    elif len(strs_to_join.items) &gt; 1:
        last = strs_to_join.items[-1]
        if isinstance(last, StrExpr) and last.value == "":
            # 3.12 can add an empty literal at the end. Delete it for consistency
            # between Python versions.
            del strs_to_join.items[-1:]
    join_method = MemberExpr(empty_string, "join")
    join_method.set_line(empty_string)
    result_expression = CallExpr(join_method, [strs_to_join], [ARG_POS], [None])
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20230831011819.124">def load_tree(self, temporary: bool = False) -&gt; None:
    assert (
        self.meta is not None
    ), "Internal error: this method must be called only for cached modules"

    data = _load_json_file(
        self.meta.data_json, self.manager, "Load tree ", "Could not load tree: "
    )
    if data is None:
        return None

    t0 = time.time()
    # TODO: Assert data file wasn't changed.
    self.tree = MypyFile.deserialize(data)
    t1 = time.time()
    self.manager.add_stats(deserialize_time=t1 - t0)
    if not temporary:
        self.manager.modules[self.id] = self.tree
        self.manager.add_stats(fresh_trees=1)

</t>
<t tx="ekr.20230831011819.1240"># FormattedValue(expr value)
def visit_FormattedValue(self, n: ast3.FormattedValue) -&gt; Expression:
    # A FormattedValue is a component of a JoinedStr, or it can exist
    # on its own. We translate them to individual '{}'.format(value)
    # calls. Format specifier and conversion information is passed along
    # to allow mypyc to support f-strings with format specifiers and conversions.
    val_exp = self.visit(n.value)
    val_exp.set_line(n.lineno, n.col_offset)
    conv_str = "" if n.conversion &lt; 0 else "!" + chr(n.conversion)
    format_string = StrExpr("{" + conv_str + ":{}}")
    format_spec_exp = self.visit(n.format_spec) if n.format_spec is not None else StrExpr("")
    format_string.set_line(n.lineno, n.col_offset)
    format_method = MemberExpr(format_string, "format")
    format_method.set_line(format_string)
    result_expression = CallExpr(
        format_method, [val_exp, format_spec_exp], [ARG_POS, ARG_POS], [None, None]
    )
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20230831011819.1241"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; MemberExpr | SuperExpr:
    value = n.value
    member_expr = MemberExpr(self.visit(value), n.attr)
    obj = member_expr.expr
    if (
        isinstance(obj, CallExpr)
        and isinstance(obj.callee, NameExpr)
        and obj.callee.name == "super"
    ):
        e: MemberExpr | SuperExpr = SuperExpr(member_expr.name, obj)
    else:
        e = member_expr
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1242"># Subscript(expr value, slice slice, expr_context ctx)
def visit_Subscript(self, n: ast3.Subscript) -&gt; IndexExpr:
    e = IndexExpr(self.visit(n.value), self.visit(n.slice))
    self.set_line(e, n)
    # alias to please mypyc
    is_py38_or_earlier = sys.version_info &lt; (3, 9)
    if isinstance(n.slice, ast3.Slice) or (
        is_py38_or_earlier and isinstance(n.slice, ast3.ExtSlice)
    ):
        # Before Python 3.9, Slice has no line/column in the raw ast. To avoid incompatibility
        # visit_Slice doesn't set_line, even in Python 3.9 on.
        # ExtSlice also has no line/column info. In Python 3.9 on, line/column is set for
        # e.index when visiting n.slice.
        e.index.line = e.line
        e.index.column = e.column
    return e

</t>
<t tx="ekr.20230831011819.1243"># Starred(expr value, expr_context ctx)
def visit_Starred(self, n: Starred) -&gt; StarExpr:
    e = StarExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1244"># Name(identifier id, expr_context ctx)
def visit_Name(self, n: Name) -&gt; NameExpr:
    e = NameExpr(n.id)
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1245"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast3.List) -&gt; ListExpr | TupleExpr:
    expr_list: list[Expression] = [self.visit(e) for e in n.elts]
    if isinstance(n.ctx, ast3.Store):
        # [x, y] = z and (x, y) = z means exactly the same thing
        e: ListExpr | TupleExpr = TupleExpr(expr_list)
    else:
        e = ListExpr(expr_list)
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1246"># Tuple(expr* elts, expr_context ctx)
def visit_Tuple(self, n: ast3.Tuple) -&gt; TupleExpr:
    e = TupleExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20230831011819.1247"># --- slice ---

# Slice(expr? lower, expr? upper, expr? step)
def visit_Slice(self, n: ast3.Slice) -&gt; SliceExpr:
    return SliceExpr(self.visit(n.lower), self.visit(n.upper), self.visit(n.step))

</t>
<t tx="ekr.20230831011819.1248"># ExtSlice(slice* dims)
def visit_ExtSlice(self, n: ast3.ExtSlice) -&gt; TupleExpr:
    # cast for mypyc's benefit on Python 3.9
    return TupleExpr(self.translate_expr_list(cast(Any, n).dims))

</t>
<t tx="ekr.20230831011819.1249"># Index(expr value)
def visit_Index(self, n: Index) -&gt; Node:
    # cast for mypyc's benefit on Python 3.9
    value = self.visit(cast(Any, n).value)
    assert isinstance(value, Node)
    return value

</t>
<t tx="ekr.20230831011819.125">def fix_cross_refs(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We need to set allow_missing when doing a fine grained cache
    # load because we need to gracefully handle missing modules.
    fixup_module(self.tree, self.manager.modules, self.options.use_fine_grained_cache)

</t>
<t tx="ekr.20230831011819.1250"># Match(expr subject, match_case* cases) # python 3.10 and later
def visit_Match(self, n: Match) -&gt; MatchStmt:
    node = MatchStmt(
        self.visit(n.subject),
        [self.visit(c.pattern) for c in n.cases],
        [self.visit(c.guard) for c in n.cases],
        [self.as_required_block(c.body) for c in n.cases],
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1251">def visit_MatchValue(self, n: MatchValue) -&gt; ValuePattern:
    node = ValuePattern(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1252">def visit_MatchSingleton(self, n: MatchSingleton) -&gt; SingletonPattern:
    node = SingletonPattern(n.value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1253">def visit_MatchSequence(self, n: MatchSequence) -&gt; SequencePattern:
    patterns = [self.visit(p) for p in n.patterns]
    stars = [p for p in patterns if isinstance(p, StarredPattern)]
    assert len(stars) &lt; 2

    node = SequencePattern(patterns)
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1254">def visit_MatchStar(self, n: MatchStar) -&gt; StarredPattern:
    if n.name is None:
        node = StarredPattern(None)
    else:
        name = self.set_line(NameExpr(n.name), n)
        node = StarredPattern(name)

    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1255">def visit_MatchMapping(self, n: MatchMapping) -&gt; MappingPattern:
    keys = [self.visit(k) for k in n.keys]
    values = [self.visit(v) for v in n.patterns]

    if n.rest is None:
        rest = None
    else:
        rest = NameExpr(n.rest)

    node = MappingPattern(keys, values, rest)
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1256">def visit_MatchClass(self, n: MatchClass) -&gt; ClassPattern:
    class_ref = self.visit(n.cls)
    assert isinstance(class_ref, RefExpr)
    positionals = [self.visit(p) for p in n.patterns]
    keyword_keys = n.kwd_attrs
    keyword_values = [self.visit(p) for p in n.kwd_patterns]

    node = ClassPattern(class_ref, positionals, keyword_keys, keyword_values)
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1257"># MatchAs(expr pattern, identifier name)
def visit_MatchAs(self, n: MatchAs) -&gt; AsPattern:
    if n.name is None:
        name = None
    else:
        name = NameExpr(n.name)
        name = self.set_line(name, n)
    node = AsPattern(self.visit(n.pattern), name)
    return self.set_line(node, n)

</t>
<t tx="ekr.20230831011819.1258"># MatchOr(expr* pattern)
def visit_MatchOr(self, n: MatchOr) -&gt; OrPattern:
    node = OrPattern([self.visit(pattern) for pattern in n.patterns])
    return self.set_line(node, n)


</t>
<t tx="ekr.20230831011819.1259">class TypeConverter:
    @others
</t>
<t tx="ekr.20230831011819.126"># Methods for processing modules from source code.

def parse_file(self) -&gt; None:
    """Parse file and run first pass of semantic analysis.

    Everything done here is local to the file. Don't depend on imported
    modules in any way. Also record module dependencies based on imports.
    """
    if self.tree is not None:
        # The file was already parsed (in __init__()).
        return

    manager = self.manager

    # Can we reuse a previously parsed AST? This avoids redundant work in daemon.
    cached = self.id in manager.ast_cache
    modules = manager.modules
    if not cached:
        manager.log(f"Parsing {self.xpath} ({self.id})")
    else:
        manager.log(f"Using cached AST for {self.xpath} ({self.id})")

    t0 = time_ref()

    with self.wrap_context():
        source = self.source
        self.source = None  # We won't need it again.
        if self.path and source is None:
            try:
                path = manager.maybe_swap_for_shadow_path(self.path)
                source = decode_python_encoding(manager.fscache.read(path))
                self.source_hash = manager.fscache.hash_digest(path)
            except OSError as ioerr:
                # ioerr.strerror differs for os.stat failures between Windows and
                # other systems, but os.strerror(ioerr.errno) does not, so we use that.
                # (We want the error messages to be platform-independent so that the
                # tests have predictable output.)
                raise CompileError(
                    [
                        "mypy: can't read file '{}': {}".format(
                            self.path, os.strerror(ioerr.errno)
                        )
                    ],
                    module_with_blocker=self.id,
                ) from ioerr
            except (UnicodeDecodeError, DecodeError) as decodeerr:
                if self.path.endswith(".pyd"):
                    err = f"mypy: stubgen does not support .pyd files: '{self.path}'"
                else:
                    err = f"mypy: can't decode file '{self.path}': {str(decodeerr)}"
                raise CompileError([err], module_with_blocker=self.id) from decodeerr
        elif self.path and self.manager.fscache.isdir(self.path):
            source = ""
            self.source_hash = ""
        else:
            assert source is not None
            self.source_hash = compute_hash(source)

        self.parse_inline_configuration(source)
        if not cached:
            self.tree = manager.parse_file(
                self.id,
                self.xpath,
                source,
                self.ignore_all or self.options.ignore_errors,
                self.options,
            )

        else:
            # Reuse a cached AST
            self.tree = manager.ast_cache[self.id][0]
            manager.errors.set_file_ignored_lines(
                self.xpath,
                self.tree.ignored_lines,
                self.ignore_all or self.options.ignore_errors,
            )

    self.time_spent_us += time_spent_us(t0)

    if not cached:
        # Make a copy of any errors produced during parse time so that
        # fine-grained mode can repeat them when the module is
        # reprocessed.
        self.early_errors = list(manager.errors.error_info_map.get(self.xpath, []))
    else:
        self.early_errors = manager.ast_cache[self.id][1]

    modules[self.id] = self.tree

    if not cached:
        self.semantic_analysis_pass1()

    self.check_blockers()

    manager.ast_cache[self.id] = (self.tree, self.early_errors)

</t>
<t tx="ekr.20230831011819.1260">def __init__(
    self,
    errors: Errors | None,
    line: int = -1,
    override_column: int = -1,
    is_evaluated: bool = True,
) -&gt; None:
    self.errors = errors
    self.line = line
    self.override_column = override_column
    self.node_stack: list[AST] = []
    self.is_evaluated = is_evaluated

</t>
<t tx="ekr.20230831011819.1261">def convert_column(self, column: int) -&gt; int:
    """Apply column override if defined; otherwise return column.

    Column numbers are sometimes incorrect in the AST and the column
    override can be used to work around that.
    """
    if self.override_column &lt; 0:
        return column
    else:
        return self.override_column

</t>
<t tx="ekr.20230831011819.1262">def invalid_type(self, node: AST, note: str | None = None) -&gt; RawExpressionType:
    """Constructs a type representing some expression that normally forms an invalid type.
    For example, if we see a type hint that says "3 + 4", we would transform that
    expression into a RawExpressionType.

    The semantic analysis layer will report an "Invalid type" error when it
    encounters this type, along with the given note if one is provided.

    See RawExpressionType's docstring for more details on how it's used.
    """
    return RawExpressionType(
        None, "typing.Any", line=self.line, column=getattr(node, "col_offset", -1), note=note
    )

</t>
<t tx="ekr.20230831011819.1263">@overload
def visit(self, node: ast3.expr) -&gt; ProperType:
    ...

</t>
<t tx="ekr.20230831011819.1264">@overload
def visit(self, node: AST | None) -&gt; ProperType | None:
    ...

</t>
<t tx="ekr.20230831011819.1265">def visit(self, node: AST | None) -&gt; ProperType | None:
    """Modified visit -- keep track of the stack of nodes"""
    if node is None:
        return None
    self.node_stack.append(node)
    try:
        method = "visit_" + node.__class__.__name__
        visitor = getattr(self, method, None)
        if visitor is not None:
            typ = visitor(node)
            assert isinstance(typ, ProperType)
            return typ
        else:
            return self.invalid_type(node)
    finally:
        self.node_stack.pop()

</t>
<t tx="ekr.20230831011819.1266">def parent(self) -&gt; AST | None:
    """Return the AST node above the one we are processing"""
    if len(self.node_stack) &lt; 2:
        return None
    return self.node_stack[-2]

</t>
<t tx="ekr.20230831011819.1267">def fail(self, msg: ErrorMessage, line: int, column: int) -&gt; None:
    if self.errors:
        self.errors.report(line, column, msg.value, blocker=True, code=msg.code)

</t>
<t tx="ekr.20230831011819.1268">def note(self, msg: str, line: int, column: int) -&gt; None:
    if self.errors:
        self.errors.report(line, column, msg, severity="note", code=codes.SYNTAX)

</t>
<t tx="ekr.20230831011819.1269">def translate_expr_list(self, l: Sequence[ast3.expr]) -&gt; list[Type]:
    return [self.visit(e) for e in l]

</t>
<t tx="ekr.20230831011819.127">def parse_inline_configuration(self, source: str) -&gt; None:
    """Check for inline mypy: options directive and parse them."""
    flags = get_mypy_comments(source)
    if flags:
        changes, config_errors = parse_mypy_comments(flags, self.options)
        self.options = self.options.apply_changes(changes)
        self.manager.errors.set_file(self.xpath, self.id, self.options)
        for lineno, error in config_errors:
            self.manager.errors.report(lineno, 0, error)

</t>
<t tx="ekr.20230831011819.1270">def visit_Call(self, e: Call) -&gt; Type:
    # Parse the arg constructor
    f = e.func
    constructor = stringify_name(f)

    if not isinstance(self.parent(), ast3.List):
        note = None
        if constructor:
            note = "Suggestion: use {0}[...] instead of {0}(...)".format(constructor)
        return self.invalid_type(e, note=note)
    if not constructor:
        self.fail(message_registry.ARG_CONSTRUCTOR_NAME_EXPECTED, e.lineno, e.col_offset)

    name: str | None = None
    default_type = AnyType(TypeOfAny.special_form)
    typ: Type = default_type
    for i, arg in enumerate(e.args):
        if i == 0:
            converted = self.visit(arg)
            assert converted is not None
            typ = converted
        elif i == 1:
            name = self._extract_argument_name(arg)
        else:
            self.fail(message_registry.ARG_CONSTRUCTOR_TOO_MANY_ARGS, f.lineno, f.col_offset)
    for k in e.keywords:
        value = k.value
        if k.arg == "name":
            if name is not None:
                self.fail(
                    message_registry.MULTIPLE_VALUES_FOR_NAME_KWARG.format(constructor),
                    f.lineno,
                    f.col_offset,
                )
            name = self._extract_argument_name(value)
        elif k.arg == "type":
            if typ is not default_type:
                self.fail(
                    message_registry.MULTIPLE_VALUES_FOR_TYPE_KWARG.format(constructor),
                    f.lineno,
                    f.col_offset,
                )
            converted = self.visit(value)
            assert converted is not None
            typ = converted
        else:
            self.fail(
                message_registry.ARG_CONSTRUCTOR_UNEXPECTED_ARG.format(k.arg),
                value.lineno,
                value.col_offset,
            )
    return CallableArgument(typ, name, constructor, e.lineno, e.col_offset)

</t>
<t tx="ekr.20230831011819.1271">def translate_argument_list(self, l: Sequence[ast3.expr]) -&gt; TypeList:
    return TypeList([self.visit(e) for e in l], line=self.line)

</t>
<t tx="ekr.20230831011819.1272">def _extract_argument_name(self, n: ast3.expr) -&gt; str | None:
    if isinstance(n, Constant) and isinstance(n.value, str):
        return n.value.strip()
    elif isinstance(n, Constant) and n.value is None:
        return None
    self.fail(
        message_registry.ARG_NAME_EXPECTED_STRING_LITERAL.format(type(n).__name__),
        self.line,
        0,
    )
    return None

</t>
<t tx="ekr.20230831011819.1273">def visit_Name(self, n: Name) -&gt; Type:
    return UnboundType(n.id, line=self.line, column=self.convert_column(n.col_offset))

</t>
<t tx="ekr.20230831011819.1274">def visit_BinOp(self, n: ast3.BinOp) -&gt; Type:
    if not isinstance(n.op, ast3.BitOr):
        return self.invalid_type(n)

    left = self.visit(n.left)
    right = self.visit(n.right)
    return UnionType(
        [left, right],
        line=self.line,
        column=self.convert_column(n.col_offset),
        is_evaluated=self.is_evaluated,
        uses_pep604_syntax=True,
    )

</t>
<t tx="ekr.20230831011819.1275">def visit_Constant(self, n: Constant) -&gt; Type:
    val = n.value
    if val is None:
        # None is a type.
        return UnboundType("None", line=self.line)
    if isinstance(val, str):
        # Parse forward reference.
        return parse_type_string(val, "builtins.str", self.line, n.col_offset)
    if val is Ellipsis:
        # '...' is valid in some types.
        return EllipsisType(line=self.line)
    if isinstance(val, bool):
        # Special case for True/False.
        return RawExpressionType(val, "builtins.bool", line=self.line)
    if isinstance(val, (int, float, complex)):
        return self.numeric_type(val, n)
    if isinstance(val, bytes):
        contents = bytes_to_human_readable_repr(val)
        return RawExpressionType(contents, "builtins.bytes", self.line, column=n.col_offset)
    # Everything else is invalid.
    return self.invalid_type(n)

</t>
<t tx="ekr.20230831011819.1276"># UnaryOp(op, operand)
def visit_UnaryOp(self, n: UnaryOp) -&gt; Type:
    # We support specifically Literal[-4] and nothing else.
    # For example, Literal[+4] or Literal[~6] is not supported.
    typ = self.visit(n.operand)
    if isinstance(typ, RawExpressionType) and isinstance(n.op, USub):
        if isinstance(typ.literal_value, int):
            typ.literal_value *= -1
            return typ
    return self.invalid_type(n)

</t>
<t tx="ekr.20230831011819.1277">def numeric_type(self, value: object, n: AST) -&gt; Type:
    # The node's field has the type complex, but complex isn't *really*
    # a parent of int and float, and this causes isinstance below
    # to think that the complex branch is always picked. Avoid
    # this by throwing away the type.
    if isinstance(value, int):
        numeric_value: int | None = value
        type_name = "builtins.int"
    else:
        # Other kinds of numbers (floats, complex) are not valid parameters for
        # RawExpressionType so we just pass in 'None' for now. We'll report the
        # appropriate error at a later stage.
        numeric_value = None
        type_name = f"builtins.{type(value).__name__}"
    return RawExpressionType(
        numeric_value, type_name, line=self.line, column=getattr(n, "col_offset", -1)
    )

</t>
<t tx="ekr.20230831011819.1278">def visit_Index(self, n: ast3.Index) -&gt; Type:
    # cast for mypyc's benefit on Python 3.9
    value = self.visit(cast(Any, n).value)
    assert isinstance(value, Type)
    return value

</t>
<t tx="ekr.20230831011819.1279">def visit_Slice(self, n: ast3.Slice) -&gt; Type:
    return self.invalid_type(n, note="did you mean to use ',' instead of ':' ?")

</t>
<t tx="ekr.20230831011819.128">def semantic_analysis_pass1(self) -&gt; None:
    """Perform pass 1 of semantic analysis, which happens immediately after parsing.

    This pass can't assume that any other modules have been processed yet.
    """
    options = self.options
    assert self.tree is not None

    t0 = time_ref()

    # Do the first pass of semantic analysis: analyze the reachability
    # of blocks and import statements. We must do this before
    # processing imports, since this may mark some import statements as
    # unreachable.
    #
    # TODO: This should not be considered as a semantic analysis
    #     pass -- it's an independent pass.
    analyzer = SemanticAnalyzerPreAnalysis()
    with self.wrap_context():
        analyzer.visit_file(self.tree, self.xpath, self.id, options)
    self.manager.errors.set_skipped_lines(self.xpath, self.tree.skipped_lines)
    # TODO: Do this while constructing the AST?
    self.tree.names = SymbolTable()
    if not self.tree.is_stub:
        # Always perform some low-key variable renaming
        self.tree.accept(LimitedVariableRenameVisitor())
        if options.allow_redefinition:
            # Perform more renaming across the AST to allow variable redefinitions
            self.tree.accept(VariableRenameVisitor())
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20230831011819.1280"># Subscript(expr value, slice slice, expr_context ctx)  # Python 3.8 and before
# Subscript(expr value, expr slice, expr_context ctx)  # Python 3.9 and later
def visit_Subscript(self, n: ast3.Subscript) -&gt; Type:
    if sys.version_info &gt;= (3, 9):  # Really 3.9a5 or later
        sliceval: Any = n.slice
    # Python 3.8 or earlier use a different AST structure for subscripts
    elif isinstance(n.slice, ast3.Index):
        sliceval: Any = n.slice.value
    elif isinstance(n.slice, ast3.Slice):
        sliceval = copy.deepcopy(n.slice)  # so we don't mutate passed AST
        if getattr(sliceval, "col_offset", None) is None:
            # Fix column information so that we get Python 3.9+ message order
            sliceval.col_offset = sliceval.lower.col_offset
    else:
        assert isinstance(n.slice, ast3.ExtSlice)
        dims = copy.deepcopy(n.slice.dims)
        for s in dims:
            if getattr(s, "col_offset", None) is None:
                if isinstance(s, ast3.Index):
                    s.col_offset = s.value.col_offset
                elif isinstance(s, ast3.Slice):
                    assert s.lower is not None
                    s.col_offset = s.lower.col_offset
        sliceval = ast3.Tuple(dims, n.ctx)

    empty_tuple_index = False
    if isinstance(sliceval, ast3.Tuple):
        params = self.translate_expr_list(sliceval.elts)
        if len(sliceval.elts) == 0:
            empty_tuple_index = True
    else:
        params = [self.visit(sliceval)]

    value = self.visit(n.value)
    if isinstance(value, UnboundType) and not value.args:
        return UnboundType(
            value.name,
            params,
            line=self.line,
            column=value.column,
            empty_tuple_index=empty_tuple_index,
        )
    else:
        return self.invalid_type(n)

</t>
<t tx="ekr.20230831011819.1281">def visit_Tuple(self, n: ast3.Tuple) -&gt; Type:
    return TupleType(
        self.translate_expr_list(n.elts),
        _dummy_fallback,
        implicit=True,
        line=self.line,
        column=self.convert_column(n.col_offset),
    )

</t>
<t tx="ekr.20230831011819.1282"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; Type:
    before_dot = self.visit(n.value)

    if isinstance(before_dot, UnboundType) and not before_dot.args:
        return UnboundType(f"{before_dot.name}.{n.attr}", line=self.line)
    else:
        return self.invalid_type(n)

</t>
<t tx="ekr.20230831011819.1283"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast3.List) -&gt; Type:
    assert isinstance(n.ctx, ast3.Load)
    return self.translate_argument_list(n.elts)


</t>
<t tx="ekr.20230831011819.1284">def stringify_name(n: AST) -&gt; str | None:
    if isinstance(n, Name):
        return n.id
    elif isinstance(n, Attribute):
        sv = stringify_name(n.value)
        if sv is not None:
            return f"{sv}.{n.attr}"
    return None  # Can't do it.


</t>
<t tx="ekr.20230831011819.1285">class FindAttributeAssign(TraverserVisitor):
    """Check if an AST contains attribute assignments (e.g. self.x = 0)."""

    @others
</t>
<t tx="ekr.20230831011819.1286">def __init__(self) -&gt; None:
    self.lvalue = False
    self.found = False

</t>
<t tx="ekr.20230831011819.1287">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    self.lvalue = True
    for lv in s.lvalues:
        lv.accept(self)
    self.lvalue = False

</t>
<t tx="ekr.20230831011819.1288">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    self.lvalue = True
    for lv in s.target:
        if lv is not None:
            lv.accept(self)
    self.lvalue = False
    s.body.accept(self)

</t>
<t tx="ekr.20230831011819.1289">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    self.lvalue = True
    s.index.accept(self)
    self.lvalue = False
    s.body.accept(self)
    if s.else_body:
        s.else_body.accept(self)

</t>
<t tx="ekr.20230831011819.129">def add_dependency(self, dep: str) -&gt; None:
    if dep not in self.dependencies_set:
        self.dependencies.append(dep)
        self.dependencies_set.add(dep)
    if dep in self.suppressed_set:
        self.suppressed.remove(dep)
        self.suppressed_set.remove(dep)

</t>
<t tx="ekr.20230831011819.1290">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    # No need to look inside these
    pass

</t>
<t tx="ekr.20230831011819.1291">def visit_call_expr(self, e: CallExpr) -&gt; None:
    # No need to look inside these
    pass

</t>
<t tx="ekr.20230831011819.1292">def visit_index_expr(self, e: IndexExpr) -&gt; None:
    # No need to look inside these
    pass

</t>
<t tx="ekr.20230831011819.1293">def visit_member_expr(self, e: MemberExpr) -&gt; None:
    if self.lvalue:
        self.found = True


</t>
<t tx="ekr.20230831011819.1294">class FindYield(TraverserVisitor):
    """Check if an AST contains yields or yield froms."""

    @others
</t>
<t tx="ekr.20230831011819.1295">def __init__(self) -&gt; None:
    self.found = False

</t>
<t tx="ekr.20230831011819.1296">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    self.found = True

</t>
<t tx="ekr.20230831011819.1297">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    self.found = True


</t>
<t tx="ekr.20230831011819.1298">def is_possible_trivial_body(s: list[Statement]) -&gt; bool:
    """Could the statements form a "trivial" function body, such as 'pass'?

    This mimics mypy.semanal.is_trivial_body, but this runs before
    semantic analysis so some checks must be conservative.
    """
    l = len(s)
    if l == 0:
        return False
    i = 0
    if isinstance(s[0], ExpressionStmt) and isinstance(s[0].expr, StrExpr):
        # Skip docstring
        i += 1
    if i == l:
        return True
    if l &gt; i + 1:
        return False
    stmt = s[i]
    return isinstance(stmt, (PassStmt, RaiseStmt)) or (
        isinstance(stmt, ExpressionStmt) and isinstance(stmt.expr, EllipsisExpr)
    )
</t>
<t tx="ekr.20230831011819.1299">@path mypy
"""Routines for finding the sources that mypy will check"""
&lt;&lt; find_sources.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.13">@path mypy
&lt;&lt; applytype.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.130">def suppress_dependency(self, dep: str) -&gt; None:
    if dep in self.dependencies_set:
        self.dependencies.remove(dep)
        self.dependencies_set.remove(dep)
    if dep not in self.suppressed_set:
        self.suppressed.append(dep)
        self.suppressed_set.add(dep)

</t>
<t tx="ekr.20230831011819.1300">
from __future__ import annotations

import functools
import os
from typing import Final, Sequence

from mypy.fscache import FileSystemCache
from mypy.modulefinder import PYTHON_EXTENSIONS, BuildSource, matches_exclude, mypy_path
from mypy.options import Options

PY_EXTENSIONS: Final = tuple(PYTHON_EXTENSIONS)


</t>
<t tx="ekr.20230831011819.1301">class InvalidSourceList(Exception):
    """Exception indicating a problem in the list of sources given to mypy."""


</t>
<t tx="ekr.20230831011819.1302">def create_source_list(
    paths: Sequence[str],
    options: Options,
    fscache: FileSystemCache | None = None,
    allow_empty_dir: bool = False,
) -&gt; list[BuildSource]:
    """From a list of source files/directories, makes a list of BuildSources.

    Raises InvalidSourceList on errors.
    """
    fscache = fscache or FileSystemCache()
    finder = SourceFinder(fscache, options)

    sources = []
    for path in paths:
        path = os.path.normpath(path)
        if path.endswith(PY_EXTENSIONS):
            # Can raise InvalidSourceList if a directory doesn't have a valid module name.
            name, base_dir = finder.crawl_up(path)
            sources.append(BuildSource(path, name, None, base_dir))
        elif fscache.isdir(path):
            sub_sources = finder.find_sources_in_dir(path)
            if not sub_sources and not allow_empty_dir:
                raise InvalidSourceList(f"There are no .py[i] files in directory '{path}'")
            sources.extend(sub_sources)
        else:
            mod = os.path.basename(path) if options.scripts_are_modules else None
            sources.append(BuildSource(path, mod, None))
    return sources


</t>
<t tx="ekr.20230831011819.1303">def keyfunc(name: str) -&gt; tuple[bool, int, str]:
    """Determines sort order for directory listing.

    The desirable properties are:
    1) foo &lt; foo.pyi &lt; foo.py
    2) __init__.py[i] &lt; foo
    """
    base, suffix = os.path.splitext(name)
    for i, ext in enumerate(PY_EXTENSIONS):
        if suffix == ext:
            return (base != "__init__", i, base)
    return (base != "__init__", -1, name)


</t>
<t tx="ekr.20230831011819.1304">def normalise_package_base(root: str) -&gt; str:
    if not root:
        root = os.curdir
    root = os.path.abspath(root)
    if root.endswith(os.sep):
        root = root[:-1]
    return root


</t>
<t tx="ekr.20230831011819.1305">def get_explicit_package_bases(options: Options) -&gt; list[str] | None:
    """Returns explicit package bases to use if the option is enabled, or None if disabled.

    We currently use MYPYPATH and the current directory as the package bases. In the future,
    when --namespace-packages is the default could also use the values passed with the
    --package-root flag, see #9632.

    Values returned are normalised so we can use simple string comparisons in
    SourceFinder.is_explicit_package_base
    """
    if not options.explicit_package_bases:
        return None
    roots = mypy_path() + options.mypy_path + [os.getcwd()]
    return [normalise_package_base(root) for root in roots]


</t>
<t tx="ekr.20230831011819.1306">class SourceFinder:
    @others
</t>
<t tx="ekr.20230831011819.1307">def __init__(self, fscache: FileSystemCache, options: Options) -&gt; None:
    self.fscache = fscache
    self.explicit_package_bases = get_explicit_package_bases(options)
    self.namespace_packages = options.namespace_packages
    self.exclude = options.exclude
    self.verbosity = options.verbosity

</t>
<t tx="ekr.20230831011819.1308">def is_explicit_package_base(self, path: str) -&gt; bool:
    assert self.explicit_package_bases
    return normalise_package_base(path) in self.explicit_package_bases

</t>
<t tx="ekr.20230831011819.1309">def find_sources_in_dir(self, path: str) -&gt; list[BuildSource]:
    sources = []

    seen: set[str] = set()
    names = sorted(self.fscache.listdir(path), key=keyfunc)
    for name in names:
        # Skip certain names altogether
        if name in ("__pycache__", "site-packages", "node_modules") or name.startswith("."):
            continue
        subpath = os.path.join(path, name)

        if matches_exclude(subpath, self.exclude, self.fscache, self.verbosity &gt;= 2):
            continue

        if self.fscache.isdir(subpath):
            sub_sources = self.find_sources_in_dir(subpath)
            if sub_sources:
                seen.add(name)
                sources.extend(sub_sources)
        else:
            stem, suffix = os.path.splitext(name)
            if stem not in seen and suffix in PY_EXTENSIONS:
                seen.add(stem)
                module, base_dir = self.crawl_up(subpath)
                sources.append(BuildSource(subpath, module, None, base_dir))

    return sources

</t>
<t tx="ekr.20230831011819.131">def compute_dependencies(self) -&gt; None:
    """Compute a module's dependencies after parsing it.

    This is used when we parse a file that we didn't have
    up-to-date cache information for. When we have an up-to-date
    cache, we just use the cached info.
    """
    manager = self.manager
    assert self.tree is not None

    # Compute (direct) dependencies.
    # Add all direct imports (this is why we needed the first pass).
    # Also keep track of each dependency's source line.
    # Missing dependencies will be moved from dependencies to
    # suppressed when they fail to be loaded in load_graph.

    self.dependencies = []
    self.dependencies_set = set()
    self.suppressed = []
    self.suppressed_set = set()
    self.priorities = {}  # id -&gt; priority
    self.dep_line_map = {}  # id -&gt; line
    dep_entries = manager.all_imported_modules_in_file(
        self.tree
    ) + self.manager.plugin.get_additional_deps(self.tree)
    for pri, id, line in dep_entries:
        self.priorities[id] = min(pri, self.priorities.get(id, PRI_ALL))
        if id == self.id:
            continue
        self.add_dependency(id)
        if id not in self.dep_line_map:
            self.dep_line_map[id] = line
    # Every module implicitly depends on builtins.
    if self.id != "builtins":
        self.add_dependency("builtins")

    self.check_blockers()  # Can fail due to bogus relative imports

</t>
<t tx="ekr.20230831011819.1310">def crawl_up(self, path: str) -&gt; tuple[str, str]:
    """Given a .py[i] filename, return module and base directory.

    For example, given "xxx/yyy/foo/bar.py", we might return something like:
    ("foo.bar", "xxx/yyy")

    If namespace packages is off, we crawl upwards until we find a directory without
    an __init__.py

    If namespace packages is on, we crawl upwards until the nearest explicit base directory.
    Failing that, we return one past the highest directory containing an __init__.py

    We won't crawl past directories with invalid package names.
    The base directory returned is an absolute path.
    """
    path = os.path.abspath(path)
    parent, filename = os.path.split(path)

    module_name = strip_py(filename) or filename

    parent_module, base_dir = self.crawl_up_dir(parent)
    if module_name == "__init__":
        return parent_module, base_dir

    # Note that module_name might not actually be a valid identifier, but that's okay
    # Ignoring this possibility sidesteps some search path confusion
    module = module_join(parent_module, module_name)
    return module, base_dir

</t>
<t tx="ekr.20230831011819.1311">def crawl_up_dir(self, dir: str) -&gt; tuple[str, str]:
    return self._crawl_up_helper(dir) or ("", dir)

</t>
<t tx="ekr.20230831011819.1312">@functools.lru_cache  # noqa: B019
def _crawl_up_helper(self, dir: str) -&gt; tuple[str, str] | None:
    """Given a directory, maybe returns module and base directory.

    We return a non-None value if we were able to find something clearly intended as a base
    directory (as adjudicated by being an explicit base directory or by containing a package
    with __init__.py).

    This distinction is necessary for namespace packages, so that we know when to treat
    ourselves as a subpackage.
    """
    # stop crawling if we're an explicit base directory
    if self.explicit_package_bases is not None and self.is_explicit_package_base(dir):
        return "", dir

    parent, name = os.path.split(dir)
    if name.endswith("-stubs"):
        name = name[:-6]  # PEP-561 stub-only directory

    # recurse if there's an __init__.py
    init_file = self.get_init_file(dir)
    if init_file is not None:
        if not name.isidentifier():
            # in most cases the directory name is invalid, we'll just stop crawling upwards
            # but if there's an __init__.py in the directory, something is messed up
            raise InvalidSourceList(f"{name} is not a valid Python package name")
        # we're definitely a package, so we always return a non-None value
        mod_prefix, base_dir = self.crawl_up_dir(parent)
        return module_join(mod_prefix, name), base_dir

    # stop crawling if we're out of path components or our name is an invalid identifier
    if not name or not parent or not name.isidentifier():
        return None

    # stop crawling if namespace packages is off (since we don't have an __init__.py)
    if not self.namespace_packages:
        return None

    # at this point: namespace packages is on, we don't have an __init__.py and we're not an
    # explicit base directory
    result = self._crawl_up_helper(parent)
    if result is None:
        # we're not an explicit base directory and we don't have an __init__.py
        # and none of our parents are either, so return
        return None
    # one of our parents was an explicit base directory or had an __init__.py, so we're
    # definitely a subpackage! chain our name to the module.
    mod_prefix, base_dir = result
    return module_join(mod_prefix, name), base_dir

</t>
<t tx="ekr.20230831011819.1313">def get_init_file(self, dir: str) -&gt; str | None:
    """Check whether a directory contains a file named __init__.py[i].

    If so, return the file's name (with dir prefixed).  If not, return None.

    This prefers .pyi over .py (because of the ordering of PY_EXTENSIONS).
    """
    for ext in PY_EXTENSIONS:
        f = os.path.join(dir, "__init__" + ext)
        if self.fscache.isfile(f):
            return f
        if ext == ".py" and self.fscache.init_under_package_root(f):
            return f
    return None


</t>
<t tx="ekr.20230831011819.1314">def module_join(parent: str, child: str) -&gt; str:
    """Join module ids, accounting for a possibly empty parent."""
    if parent:
        return parent + "." + child
    return child


</t>
<t tx="ekr.20230831011819.1315">def strip_py(arg: str) -&gt; str | None:
    """Strip a trailing .py or .pyi suffix.

    Return None if no such suffix is found.
    """
    for ext in PY_EXTENSIONS:
        if arg.endswith(ext):
            return arg[: -len(ext)]
    return None
</t>
<t tx="ekr.20230831011819.1316">@path mypy
"""Fix up various things after deserialization."""
&lt;&lt; fixup.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1317">
from __future__ import annotations

from typing import Any, Final

from mypy.lookup import lookup_fully_qualified
from mypy.nodes import (
    Block,
    ClassDef,
    Decorator,
    FuncDef,
    MypyFile,
    OverloadedFuncDef,
    ParamSpecExpr,
    SymbolTable,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    TypeVarTupleExpr,
    Var,
)
from mypy.types import (
    NOT_READY,
    AnyType,
    CallableType,
    Instance,
    LiteralType,
    Overloaded,
    Parameters,
    ParamSpecType,
    TupleType,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UnionType,
    UnpackType,
)
from mypy.visitor import NodeVisitor


# N.B: we do a allow_missing fixup when fixing up a fine-grained
# incremental cache load (since there may be cross-refs into deleted
# modules)
</t>
<t tx="ekr.20230831011819.1318">def fixup_module(tree: MypyFile, modules: dict[str, MypyFile], allow_missing: bool) -&gt; None:
    node_fixer = NodeFixer(modules, allow_missing)
    node_fixer.visit_symbol_table(tree.names, tree.fullname)


</t>
<t tx="ekr.20230831011819.1319"># TODO: Fix up .info when deserializing, i.e. much earlier.
class NodeFixer(NodeVisitor[None]):
    @others
</t>
<t tx="ekr.20230831011819.132">def type_check_first_pass(self) -&gt; None:
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        self.type_checker().check_first_pass()
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20230831011819.1320">current_info: TypeInfo | None = None

def __init__(self, modules: dict[str, MypyFile], allow_missing: bool) -&gt; None:
    self.modules = modules
    self.allow_missing = allow_missing
    self.type_fixer = TypeFixer(self.modules, allow_missing)

</t>
<t tx="ekr.20230831011819.1321"># NOTE: This method isn't (yet) part of the NodeVisitor API.
def visit_type_info(self, info: TypeInfo) -&gt; None:
    save_info = self.current_info
    try:
        self.current_info = info
        if info.defn:
            info.defn.accept(self)
        if info.names:
            self.visit_symbol_table(info.names, info.fullname)
        if info.bases:
            for base in info.bases:
                base.accept(self.type_fixer)
        if info._promote:
            for p in info._promote:
                p.accept(self.type_fixer)
        if info.tuple_type:
            info.tuple_type.accept(self.type_fixer)
            info.update_tuple_type(info.tuple_type)
            if info.special_alias:
                info.special_alias.alias_tvars = list(info.defn.type_vars)
        if info.typeddict_type:
            info.typeddict_type.accept(self.type_fixer)
            info.update_typeddict_type(info.typeddict_type)
            if info.special_alias:
                info.special_alias.alias_tvars = list(info.defn.type_vars)
        if info.declared_metaclass:
            info.declared_metaclass.accept(self.type_fixer)
        if info.metaclass_type:
            info.metaclass_type.accept(self.type_fixer)
        if info.alt_promote:
            info.alt_promote.accept(self.type_fixer)
            instance = Instance(info, [])
            # Hack: We may also need to add a backwards promotion (from int to native int),
            # since it might not be serialized.
            if instance not in info.alt_promote.type._promote:
                info.alt_promote.type._promote.append(instance)
        if info._mro_refs:
            info.mro = [
                lookup_fully_qualified_typeinfo(
                    self.modules, name, allow_missing=self.allow_missing
                )
                for name in info._mro_refs
            ]
            info._mro_refs = None
    finally:
        self.current_info = save_info

</t>
<t tx="ekr.20230831011819.1322"># NOTE: This method *definitely* isn't part of the NodeVisitor API.
def visit_symbol_table(self, symtab: SymbolTable, table_fullname: str) -&gt; None:
    # Copy the items because we may mutate symtab.
    for key, value in list(symtab.items()):
        cross_ref = value.cross_ref
        if cross_ref is not None:  # Fix up cross-reference.
            value.cross_ref = None
            if cross_ref in self.modules:
                value.node = self.modules[cross_ref]
            else:
                stnode = lookup_fully_qualified(
                    cross_ref, self.modules, raise_on_missing=not self.allow_missing
                )
                if stnode is not None:
                    assert stnode.node is not None, (table_fullname + "." + key, cross_ref)
                    value.node = stnode.node
                elif not self.allow_missing:
                    assert False, f"Could not find cross-ref {cross_ref}"
                else:
                    # We have a missing crossref in allow missing mode, need to put something
                    value.node = missing_info(self.modules)
        else:
            if isinstance(value.node, TypeInfo):
                # TypeInfo has no accept().  TODO: Add it?
                self.visit_type_info(value.node)
            elif value.node is not None:
                value.node.accept(self)
            else:
                assert False, f"Unexpected empty node {key!r}: {value}"

</t>
<t tx="ekr.20230831011819.1323">def visit_func_def(self, func: FuncDef) -&gt; None:
    if self.current_info is not None:
        func.info = self.current_info
    if func.type is not None:
        func.type.accept(self.type_fixer)

</t>
<t tx="ekr.20230831011819.1324">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    if self.current_info is not None:
        o.info = self.current_info
    if o.type:
        o.type.accept(self.type_fixer)
    for item in o.items:
        item.accept(self)
    if o.impl:
        o.impl.accept(self)

</t>
<t tx="ekr.20230831011819.1325">def visit_decorator(self, d: Decorator) -&gt; None:
    if self.current_info is not None:
        d.var.info = self.current_info
    if d.func:
        d.func.accept(self)
    if d.var:
        d.var.accept(self)
    for node in d.decorators:
        node.accept(self)

</t>
<t tx="ekr.20230831011819.1326">def visit_class_def(self, c: ClassDef) -&gt; None:
    for v in c.type_vars:
        if isinstance(v, TypeVarType):
            for value in v.values:
                value.accept(self.type_fixer)
        v.upper_bound.accept(self.type_fixer)
        v.default.accept(self.type_fixer)

</t>
<t tx="ekr.20230831011819.1327">def visit_type_var_expr(self, tv: TypeVarExpr) -&gt; None:
    for value in tv.values:
        value.accept(self.type_fixer)
    tv.upper_bound.accept(self.type_fixer)
    tv.default.accept(self.type_fixer)

</t>
<t tx="ekr.20230831011819.1328">def visit_paramspec_expr(self, p: ParamSpecExpr) -&gt; None:
    p.upper_bound.accept(self.type_fixer)
    p.default.accept(self.type_fixer)

</t>
<t tx="ekr.20230831011819.1329">def visit_type_var_tuple_expr(self, tv: TypeVarTupleExpr) -&gt; None:
    tv.upper_bound.accept(self.type_fixer)
    tv.default.accept(self.type_fixer)

</t>
<t tx="ekr.20230831011819.133">def type_checker(self) -&gt; TypeChecker:
    if not self._type_checker:
        assert self.tree is not None, "Internal error: must be called on parsed file only"
        manager = self.manager
        self._type_checker = TypeChecker(
            manager.errors,
            manager.modules,
            self.options,
            self.tree,
            self.xpath,
            manager.plugin,
            self.per_line_checking_time_ns,
        )
    return self._type_checker

</t>
<t tx="ekr.20230831011819.1330">def visit_var(self, v: Var) -&gt; None:
    if self.current_info is not None:
        v.info = self.current_info
    if v.type is not None:
        v.type.accept(self.type_fixer)

</t>
<t tx="ekr.20230831011819.1331">def visit_type_alias(self, a: TypeAlias) -&gt; None:
    a.target.accept(self.type_fixer)
    for v in a.alias_tvars:
        v.accept(self.type_fixer)


</t>
<t tx="ekr.20230831011819.1332">class TypeFixer(TypeVisitor[None]):
    @others
</t>
<t tx="ekr.20230831011819.1333">def __init__(self, modules: dict[str, MypyFile], allow_missing: bool) -&gt; None:
    self.modules = modules
    self.allow_missing = allow_missing

</t>
<t tx="ekr.20230831011819.1334">def visit_instance(self, inst: Instance) -&gt; None:
    # TODO: Combine Instances that are exactly the same?
    type_ref = inst.type_ref
    if type_ref is None:
        return  # We've already been here.
    inst.type_ref = None
    inst.type = lookup_fully_qualified_typeinfo(
        self.modules, type_ref, allow_missing=self.allow_missing
    )
    # TODO: Is this needed or redundant?
    # Also fix up the bases, just in case.
    for base in inst.type.bases:
        if base.type is NOT_READY:
            base.accept(self)
    for a in inst.args:
        a.accept(self)
    if inst.last_known_value is not None:
        inst.last_known_value.accept(self)

</t>
<t tx="ekr.20230831011819.1335">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    type_ref = t.type_ref
    if type_ref is None:
        return  # We've already been here.
    t.type_ref = None
    t.alias = lookup_fully_qualified_alias(
        self.modules, type_ref, allow_missing=self.allow_missing
    )
    for a in t.args:
        a.accept(self)

</t>
<t tx="ekr.20230831011819.1336">def visit_any(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20230831011819.1337">def visit_callable_type(self, ct: CallableType) -&gt; None:
    if ct.fallback:
        ct.fallback.accept(self)
    for argt in ct.arg_types:
        # argt may be None, e.g. for __self in NamedTuple constructors.
        if argt is not None:
            argt.accept(self)
    if ct.ret_type is not None:
        ct.ret_type.accept(self)
    for v in ct.variables:
        v.accept(self)
    for arg in ct.bound_args:
        if arg:
            arg.accept(self)
    if ct.type_guard is not None:
        ct.type_guard.accept(self)

</t>
<t tx="ekr.20230831011819.1338">def visit_overloaded(self, t: Overloaded) -&gt; None:
    for ct in t.items:
        ct.accept(self)

</t>
<t tx="ekr.20230831011819.1339">def visit_erased_type(self, o: Any) -&gt; None:
    # This type should exist only temporarily during type inference
    raise RuntimeError("Shouldn't get here", o)

</t>
<t tx="ekr.20230831011819.134">def type_map(self) -&gt; dict[Expression, Type]:
    # We can extract the master type map directly since at this
    # point no temporary type maps can be active.
    assert len(self.type_checker()._type_maps) == 1
    return self.type_checker()._type_maps[0]

</t>
<t tx="ekr.20230831011819.1340">def visit_deleted_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20230831011819.1341">def visit_none_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20230831011819.1342">def visit_uninhabited_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20230831011819.1343">def visit_partial_type(self, o: Any) -&gt; None:
    raise RuntimeError("Shouldn't get here", o)

</t>
<t tx="ekr.20230831011819.1344">def visit_tuple_type(self, tt: TupleType) -&gt; None:
    if tt.items:
        for it in tt.items:
            it.accept(self)
    if tt.partial_fallback is not None:
        tt.partial_fallback.accept(self)

</t>
<t tx="ekr.20230831011819.1345">def visit_typeddict_type(self, tdt: TypedDictType) -&gt; None:
    if tdt.items:
        for it in tdt.items.values():
            it.accept(self)
    if tdt.fallback is not None:
        if tdt.fallback.type_ref is not None:
            if (
                lookup_fully_qualified(
                    tdt.fallback.type_ref,
                    self.modules,
                    raise_on_missing=not self.allow_missing,
                )
                is None
            ):
                # We reject fake TypeInfos for TypedDict fallbacks because
                # the latter are used in type checking and must be valid.
                tdt.fallback.type_ref = "typing._TypedDict"
        tdt.fallback.accept(self)

</t>
<t tx="ekr.20230831011819.1346">def visit_literal_type(self, lt: LiteralType) -&gt; None:
    lt.fallback.accept(self)

</t>
<t tx="ekr.20230831011819.1347">def visit_type_var(self, tvt: TypeVarType) -&gt; None:
    if tvt.values:
        for vt in tvt.values:
            vt.accept(self)
    tvt.upper_bound.accept(self)
    tvt.default.accept(self)

</t>
<t tx="ekr.20230831011819.1348">def visit_param_spec(self, p: ParamSpecType) -&gt; None:
    p.upper_bound.accept(self)
    p.default.accept(self)

</t>
<t tx="ekr.20230831011819.1349">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; None:
    t.upper_bound.accept(self)
    t.default.accept(self)

</t>
<t tx="ekr.20230831011819.135">def type_check_second_pass(self) -&gt; bool:
    if self.options.semantic_analysis_only:
        return False
    t0 = time_ref()
    with self.wrap_context():
        result = self.type_checker().check_second_pass()
    self.time_spent_us += time_spent_us(t0)
    return result

</t>
<t tx="ekr.20230831011819.1350">def visit_unpack_type(self, u: UnpackType) -&gt; None:
    u.type.accept(self)

</t>
<t tx="ekr.20230831011819.1351">def visit_parameters(self, p: Parameters) -&gt; None:
    for argt in p.arg_types:
        if argt is not None:
            argt.accept(self)
    for var in p.variables:
        var.accept(self)

</t>
<t tx="ekr.20230831011819.1352">def visit_unbound_type(self, o: UnboundType) -&gt; None:
    for a in o.args:
        a.accept(self)

</t>
<t tx="ekr.20230831011819.1353">def visit_union_type(self, ut: UnionType) -&gt; None:
    if ut.items:
        for it in ut.items:
            it.accept(self)

</t>
<t tx="ekr.20230831011819.1354">def visit_void(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20230831011819.1355">def visit_type_type(self, t: TypeType) -&gt; None:
    t.item.accept(self)


</t>
<t tx="ekr.20230831011819.1356">def lookup_fully_qualified_typeinfo(
    modules: dict[str, MypyFile], name: str, *, allow_missing: bool
) -&gt; TypeInfo:
    stnode = lookup_fully_qualified(name, modules, raise_on_missing=not allow_missing)
    node = stnode.node if stnode else None
    if isinstance(node, TypeInfo):
        return node
    else:
        # Looks like a missing TypeInfo during an initial daemon load, put something there
        assert (
            allow_missing
        ), "Should never get here in normal mode, got {}:{} instead of TypeInfo".format(
            type(node).__name__, node.fullname if node else ""
        )
        return missing_info(modules)


</t>
<t tx="ekr.20230831011819.1357">def lookup_fully_qualified_alias(
    modules: dict[str, MypyFile], name: str, *, allow_missing: bool
) -&gt; TypeAlias:
    stnode = lookup_fully_qualified(name, modules, raise_on_missing=not allow_missing)
    node = stnode.node if stnode else None
    if isinstance(node, TypeAlias):
        return node
    elif isinstance(node, TypeInfo):
        if node.special_alias:
            # Already fixed up.
            return node.special_alias
        if node.tuple_type:
            alias = TypeAlias.from_tuple_type(node)
        elif node.typeddict_type:
            alias = TypeAlias.from_typeddict_type(node)
        else:
            assert allow_missing
            return missing_alias()
        node.special_alias = alias
        return alias
    else:
        # Looks like a missing TypeAlias during an initial daemon load, put something there
        assert (
            allow_missing
        ), "Should never get here in normal mode, got {}:{} instead of TypeAlias".format(
            type(node).__name__, node.fullname if node else ""
        )
        return missing_alias()


</t>
<t tx="ekr.20230831011819.1358">_SUGGESTION: Final = "&lt;missing {}: *should* have gone away during fine-grained update&gt;"


def missing_info(modules: dict[str, MypyFile]) -&gt; TypeInfo:
    suggestion = _SUGGESTION.format("info")
    dummy_def = ClassDef(suggestion, Block([]))
    dummy_def.fullname = suggestion

    info = TypeInfo(SymbolTable(), dummy_def, "&lt;missing&gt;")
    obj_type = lookup_fully_qualified_typeinfo(modules, "builtins.object", allow_missing=False)
    info.bases = [Instance(obj_type, [])]
    info.mro = [info, obj_type]
    return info


</t>
<t tx="ekr.20230831011819.1359">def missing_alias() -&gt; TypeAlias:
    suggestion = _SUGGESTION.format("alias")
    return TypeAlias(AnyType(TypeOfAny.special_form), suggestion, line=-1, column=-1)
</t>
<t tx="ekr.20230831011819.136">def detect_possibly_undefined_vars(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    if self.tree.is_stub:
        # We skip stub files because they aren't actually executed.
        return
    manager = self.manager
    manager.errors.set_file(self.xpath, self.tree.fullname, options=self.options)
    if manager.errors.is_error_code_enabled(
        codes.POSSIBLY_UNDEFINED
    ) or manager.errors.is_error_code_enabled(codes.USED_BEFORE_DEF):
        self.tree.accept(
            PossiblyUndefinedVariableVisitor(
                MessageBuilder(manager.errors, manager.modules),
                self.type_map(),
                self.options,
                self.tree.names,
            )
        )

</t>
<t tx="ekr.20230831011819.1360">@path mypy
"""Generic node traverser visitor"""

from __future__ import annotations

from mypy.nodes import Block, MypyFile
from mypy.traverser import TraverserVisitor


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1362">class TreeFreer(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011819.1363">def visit_block(self, block: Block) -&gt; None:
    super().visit_block(block)
    block.body.clear()


</t>
<t tx="ekr.20230831011819.1364">def free_tree(tree: MypyFile) -&gt; None:
    """Free all the ASTs associated with a module.

    This needs to be done recursively, since symbol tables contain
    references to definitions, so those won't be freed but we want their
    contents to be.
    """
    tree.accept(TreeFreer())
    tree.defs.clear()
</t>
<t tx="ekr.20230831011819.1365">@path mypy
&lt;&lt; fscache.py: docstring &gt;&gt;
&lt;&lt; fscache.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1367">from __future__ import annotations

import os
import stat

from mypy_extensions import mypyc_attr

from mypy.util import hash_digest


@mypyc_attr(allow_interpreted_subclasses=True)  # for tests
</t>
<t tx="ekr.20230831011819.1368">class FileSystemCache:
    @others
</t>
<t tx="ekr.20230831011819.1369">def __init__(self) -&gt; None:
    # The package root is not flushed with the caches.
    # It is set by set_package_root() below.
    self.package_root: list[str] = []
    self.flush()

</t>
<t tx="ekr.20230831011819.137">def finish_passes(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    manager = self.manager
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        # Some tests (and tools) want to look at the set of all types.
        options = manager.options
        if options.export_types:
            manager.all_types.update(self.type_map())

        # We should always patch indirect dependencies, even in full (non-incremental) builds,
        # because the cache still may be written, and it must be correct.
        # TODO: find a more robust way to traverse *all* relevant types?
        expr_types = set(self.type_map().values())
        symbol_types = set()
        for _, sym, _ in self.tree.local_definitions():
            if sym.type is not None:
                symbol_types.add(sym.type)
            if isinstance(sym.node, TypeInfo):
                # TypeInfo symbols have some extra relevant types.
                symbol_types.update(sym.node.bases)
                if sym.node.metaclass_type:
                    symbol_types.add(sym.node.metaclass_type)
                if sym.node.typeddict_type:
                    symbol_types.add(sym.node.typeddict_type)
                if sym.node.tuple_type:
                    symbol_types.add(sym.node.tuple_type)
        self._patch_indirect_dependencies(
            self.type_checker().module_refs, expr_types | symbol_types
        )

        if self.options.dump_inference_stats:
            dump_type_stats(
                self.tree,
                self.xpath,
                modules=self.manager.modules,
                inferred=True,
                typemap=self.type_map(),
            )
        manager.report_file(self.tree, self.type_map(), self.options)

        self.update_fine_grained_deps(self.manager.fg_deps)

        if manager.options.export_ref_info:
            write_undocumented_ref_info(
                self, manager.metastore, manager.options, self.type_map()
            )

        self.free_state()
        if not manager.options.fine_grained_incremental and not manager.options.preserve_asts:
            free_tree(self.tree)
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20230831011819.1370">def set_package_root(self, package_root: list[str]) -&gt; None:
    self.package_root = package_root

</t>
<t tx="ekr.20230831011819.1371">def flush(self) -&gt; None:
    """Start another transaction and empty all caches."""
    self.stat_cache: dict[str, os.stat_result] = {}
    self.stat_error_cache: dict[str, OSError] = {}
    self.listdir_cache: dict[str, list[str]] = {}
    self.listdir_error_cache: dict[str, OSError] = {}
    self.isfile_case_cache: dict[str, bool] = {}
    self.exists_case_cache: dict[str, bool] = {}
    self.read_cache: dict[str, bytes] = {}
    self.read_error_cache: dict[str, Exception] = {}
    self.hash_cache: dict[str, str] = {}
    self.fake_package_cache: set[str] = set()

</t>
<t tx="ekr.20230831011819.1372">def stat(self, path: str) -&gt; os.stat_result:
    if path in self.stat_cache:
        return self.stat_cache[path]
    if path in self.stat_error_cache:
        raise copy_os_error(self.stat_error_cache[path])
    try:
        st = os.stat(path)
    except OSError as err:
        if self.init_under_package_root(path):
            try:
                return self._fake_init(path)
            except OSError:
                pass
        # Take a copy to get rid of associated traceback and frame objects.
        # Just assigning to __traceback__ doesn't free them.
        self.stat_error_cache[path] = copy_os_error(err)
        raise err
    self.stat_cache[path] = st
    return st

</t>
<t tx="ekr.20230831011819.1373">def init_under_package_root(self, path: str) -&gt; bool:
    """Is this path an __init__.py under a package root?

    This is used to detect packages that don't contain __init__.py
    files, which is needed to support Bazel.  The function should
    only be called for non-existing files.

    It will return True if it refers to a __init__.py file that
    Bazel would create, so that at runtime Python would think the
    directory containing it is a package.  For this to work you
    must pass one or more package roots using the --package-root
    flag.

    As an exceptional case, any directory that is a package root
    itself will not be considered to contain a __init__.py file.
    This is different from the rules Bazel itself applies, but is
    necessary for mypy to properly distinguish packages from other
    directories.

    See https://docs.bazel.build/versions/master/be/python.html,
    where this behavior is described under legacy_create_init.
    """
    if not self.package_root:
        return False
    dirname, basename = os.path.split(path)
    if basename != "__init__.py":
        return False
    if not os.path.basename(dirname).isidentifier():
        # Can't put an __init__.py in a place that's not an identifier
        return False
    try:
        st = self.stat(dirname)
    except OSError:
        return False
    else:
        if not stat.S_ISDIR(st.st_mode):
            return False
    ok = False
    drive, path = os.path.splitdrive(path)  # Ignore Windows drive name
    if os.path.isabs(path):
        path = os.path.relpath(path)
    path = os.path.normpath(path)
    for root in self.package_root:
        if path.startswith(root):
            if path == root + basename:
                # A package root itself is never a package.
                ok = False
                break
            else:
                ok = True
    return ok

</t>
<t tx="ekr.20230831011819.1374">def _fake_init(self, path: str) -&gt; os.stat_result:
    """Prime the cache with a fake __init__.py file.

    This makes code that looks for path believe an empty file by
    that name exists.  Should only be called after
    init_under_package_root() returns True.
    """
    dirname, basename = os.path.split(path)
    assert basename == "__init__.py", path
    assert not os.path.exists(path), path  # Not cached!
    dirname = os.path.normpath(dirname)
    st = self.stat(dirname)  # May raise OSError
    # Get stat result as a list so we can modify it.
    seq: list[float] = list(st)
    seq[stat.ST_MODE] = stat.S_IFREG | 0o444
    seq[stat.ST_INO] = 1
    seq[stat.ST_NLINK] = 1
    seq[stat.ST_SIZE] = 0
    st = os.stat_result(seq)
    self.stat_cache[path] = st
    # Make listdir() and read() also pretend this file exists.
    self.fake_package_cache.add(dirname)
    return st

</t>
<t tx="ekr.20230831011819.1375">def listdir(self, path: str) -&gt; list[str]:
    path = os.path.normpath(path)
    if path in self.listdir_cache:
        res = self.listdir_cache[path]
        # Check the fake cache.
        if path in self.fake_package_cache and "__init__.py" not in res:
            res.append("__init__.py")  # Updates the result as well as the cache
        return res
    if path in self.listdir_error_cache:
        raise copy_os_error(self.listdir_error_cache[path])
    try:
        results = os.listdir(path)
    except OSError as err:
        # Like above, take a copy to reduce memory use.
        self.listdir_error_cache[path] = copy_os_error(err)
        raise err
    self.listdir_cache[path] = results
    # Check the fake cache.
    if path in self.fake_package_cache and "__init__.py" not in results:
        results.append("__init__.py")
    return results

</t>
<t tx="ekr.20230831011819.1376">def isfile(self, path: str) -&gt; bool:
    try:
        st = self.stat(path)
    except OSError:
        return False
    return stat.S_ISREG(st.st_mode)

</t>
<t tx="ekr.20230831011819.1377">def isfile_case(self, path: str, prefix: str) -&gt; bool:
    """Return whether path exists and is a file.

    On case-insensitive filesystems (like Mac or Windows) this returns
    False if the case of path's last component does not exactly match
    the case found in the filesystem.

    We check also the case of other path components up to prefix.
    For example, if path is 'user-stubs/pack/mod.pyi' and prefix is 'user-stubs',
    we check that the case of 'pack' and 'mod.py' matches exactly, 'user-stubs' will be
    case insensitive on case insensitive filesystems.

    The caller must ensure that prefix is a valid file system prefix of path.
    """
    if not self.isfile(path):
        # Fast path
        return False
    if path in self.isfile_case_cache:
        return self.isfile_case_cache[path]
    head, tail = os.path.split(path)
    if not tail:
        self.isfile_case_cache[path] = False
        return False
    try:
        names = self.listdir(head)
        # This allows one to check file name case sensitively in
        # case-insensitive filesystems.
        res = tail in names
    except OSError:
        res = False
    if res:
        # Also recursively check the other path components in case sensitive way.
        res = self.exists_case(head, prefix)
    self.isfile_case_cache[path] = res
    return res

</t>
<t tx="ekr.20230831011819.1378">def exists_case(self, path: str, prefix: str) -&gt; bool:
    """Return whether path exists - checking path components in case sensitive
    fashion, up to prefix.
    """
    if path in self.exists_case_cache:
        return self.exists_case_cache[path]
    head, tail = os.path.split(path)
    if not head.startswith(prefix) or not tail:
        # Only perform the check for paths under prefix.
        self.exists_case_cache[path] = True
        return True
    try:
        names = self.listdir(head)
        # This allows one to check file name case sensitively in
        # case-insensitive filesystems.
        res = tail in names
    except OSError:
        res = False
    if res:
        # Also recursively check other path components.
        res = self.exists_case(head, prefix)
    self.exists_case_cache[path] = res
    return res

</t>
<t tx="ekr.20230831011819.1379">def isdir(self, path: str) -&gt; bool:
    try:
        st = self.stat(path)
    except OSError:
        return False
    return stat.S_ISDIR(st.st_mode)

</t>
<t tx="ekr.20230831011819.138">def free_state(self) -&gt; None:
    if self._type_checker:
        self._type_checker.reset()
        self._type_checker = None

</t>
<t tx="ekr.20230831011819.1380">def exists(self, path: str) -&gt; bool:
    try:
        self.stat(path)
    except FileNotFoundError:
        return False
    return True

</t>
<t tx="ekr.20230831011819.1381">def read(self, path: str) -&gt; bytes:
    if path in self.read_cache:
        return self.read_cache[path]
    if path in self.read_error_cache:
        raise self.read_error_cache[path]

    # Need to stat first so that the contents of file are from no
    # earlier instant than the mtime reported by self.stat().
    self.stat(path)

    dirname, basename = os.path.split(path)
    dirname = os.path.normpath(dirname)
    # Check the fake cache.
    if basename == "__init__.py" and dirname in self.fake_package_cache:
        data = b""
    else:
        try:
            with open(path, "rb") as f:
                data = f.read()
        except OSError as err:
            self.read_error_cache[path] = err
            raise

    self.read_cache[path] = data
    self.hash_cache[path] = hash_digest(data)
    return data

</t>
<t tx="ekr.20230831011819.1382">def hash_digest(self, path: str) -&gt; str:
    if path not in self.hash_cache:
        self.read(path)
    return self.hash_cache[path]

</t>
<t tx="ekr.20230831011819.1383">def samefile(self, f1: str, f2: str) -&gt; bool:
    s1 = self.stat(f1)
    s2 = self.stat(f2)
    return os.path.samestat(s1, s2)


</t>
<t tx="ekr.20230831011819.1384">def copy_os_error(e: OSError) -&gt; OSError:
    new = OSError(*e.args)
    new.errno = e.errno
    new.strerror = e.strerror
    new.filename = e.filename
    if e.filename2:
        new.filename2 = e.filename2
    return new
</t>
<t tx="ekr.20230831011819.1385">@path mypy
"""Watch parts of the file system for changes."""
&lt;&lt; fswatcher.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1386">
from __future__ import annotations

from typing import AbstractSet, Iterable, NamedTuple

from mypy.fscache import FileSystemCache


</t>
<t tx="ekr.20230831011819.1387">class FileData(NamedTuple):
    st_mtime: float
    st_size: int
    hash: str


</t>
<t tx="ekr.20230831011819.1388">class FileSystemWatcher:
    """Watcher for file system changes among specific paths.

    All file system access is performed using FileSystemCache. We
    detect changed files by stat()ing them all and comparing hashes
    of potentially changed files. If a file has both size and mtime
    unmodified, the file is assumed to be unchanged.

    An important goal of this class is to make it easier to eventually
    use file system events to detect file changes.

    Note: This class doesn't flush the file system cache. If you don't
    manually flush it, changes won't be seen.
    """

    @others
</t>
<t tx="ekr.20230831011819.1389"># TODO: Watching directories?
# TODO: Handle non-files

def __init__(self, fs: FileSystemCache) -&gt; None:
    self.fs = fs
    self._paths: set[str] = set()
    self._file_data: dict[str, FileData | None] = {}

</t>
<t tx="ekr.20230831011819.139">def _patch_indirect_dependencies(self, module_refs: set[str], types: set[Type]) -&gt; None:
    assert None not in types
    valid = self.valid_references()

    encountered = self.manager.indirection_detector.find_modules(types) | module_refs
    extra = encountered - valid

    for dep in sorted(extra):
        if dep not in self.manager.modules:
            continue
        if dep not in self.suppressed_set and dep not in self.manager.missing_modules:
            self.add_dependency(dep)
            self.priorities[dep] = PRI_INDIRECT
        elif dep not in self.suppressed_set and dep in self.manager.missing_modules:
            self.suppress_dependency(dep)

</t>
<t tx="ekr.20230831011819.1390">def dump_file_data(self) -&gt; dict[str, tuple[float, int, str]]:
    return {k: v for k, v in self._file_data.items() if v is not None}

</t>
<t tx="ekr.20230831011819.1391">def set_file_data(self, path: str, data: FileData) -&gt; None:
    self._file_data[path] = data

</t>
<t tx="ekr.20230831011819.1392">def add_watched_paths(self, paths: Iterable[str]) -&gt; None:
    for path in paths:
        if path not in self._paths:
            # By storing None this path will get reported as changed by
            # find_changed if it exists.
            self._file_data[path] = None
    self._paths |= set(paths)

</t>
<t tx="ekr.20230831011819.1393">def remove_watched_paths(self, paths: Iterable[str]) -&gt; None:
    for path in paths:
        if path in self._file_data:
            del self._file_data[path]
    self._paths -= set(paths)

</t>
<t tx="ekr.20230831011819.1394">def _update(self, path: str) -&gt; None:
    st = self.fs.stat(path)
    hash_digest = self.fs.hash_digest(path)
    self._file_data[path] = FileData(st.st_mtime, st.st_size, hash_digest)

</t>
<t tx="ekr.20230831011819.1395">def _find_changed(self, paths: Iterable[str]) -&gt; AbstractSet[str]:
    changed = set()
    for path in paths:
        old = self._file_data[path]
        try:
            st = self.fs.stat(path)
        except FileNotFoundError:
            if old is not None:
                # File was deleted.
                changed.add(path)
                self._file_data[path] = None
        else:
            if old is None:
                # File is new.
                changed.add(path)
                self._update(path)
            # Round mtimes down, to match the mtimes we write to meta files
            elif st.st_size != old.st_size or int(st.st_mtime) != int(old.st_mtime):
                # Only look for changes if size or mtime has changed as an
                # optimization, since calculating hash is expensive.
                new_hash = self.fs.hash_digest(path)
                self._update(path)
                if st.st_size != old.st_size or new_hash != old.hash:
                    # Changed file.
                    changed.add(path)
    return changed

</t>
<t tx="ekr.20230831011819.1396">def find_changed(self) -&gt; AbstractSet[str]:
    """Return paths that have changes since the last call, in the watched set."""
    return self._find_changed(self._paths)

</t>
<t tx="ekr.20230831011819.1397">def update_changed(self, remove: list[str], update: list[str]) -&gt; AbstractSet[str]:
    """Alternative to find_changed() given explicit changes.

    This only calls self.fs.stat() on added or updated files, not
    on all files.  It believes all other files are unchanged!

    Implies add_watched_paths() for add and update, and
    remove_watched_paths() for remove.
    """
    self.remove_watched_paths(remove)
    self.add_watched_paths(update)
    return self._find_changed(update)
</t>
<t tx="ekr.20230831011819.1398">@path mypy
&lt;&lt; gclogger.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1399">from __future__ import annotations

import gc
import time
from typing import Mapping


</t>
<t tx="ekr.20230831011819.14">from __future__ import annotations

from typing import Callable, Sequence

import mypy.subtypes
from mypy.expandtype import expand_type
from mypy.nodes import Context
from mypy.types import (
    AnyType,
    CallableType,
    ParamSpecType,
    PartialType,
    Type,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UninhabitedType,
    UnpackType,
    get_proper_type,
)


</t>
<t tx="ekr.20230831011819.140">def compute_fine_grained_deps(self) -&gt; dict[str, set[str]]:
    assert self.tree is not None
    if self.id in ("builtins", "typing", "types", "sys", "_typeshed"):
        # We don't track changes to core parts of typeshed -- the
        # assumption is that they are only changed as part of mypy
        # updates, which will invalidate everything anyway. These
        # will always be processed in the initial non-fine-grained
        # build. Other modules may be brought in as a result of an
        # fine-grained increment, and we may need these
        # dependencies then to handle cyclic imports.
        return {}
    from mypy.server.deps import get_dependencies  # Lazy import to speed up startup

    return get_dependencies(
        target=self.tree,
        type_map=self.type_map(),
        python_version=self.options.python_version,
        options=self.manager.options,
    )

</t>
<t tx="ekr.20230831011819.1400">class GcLogger:
    """Context manager to log GC stats and overall time."""

    @others
</t>
<t tx="ekr.20230831011819.1401">def __enter__(self) -&gt; GcLogger:
    self.gc_start_time: float | None = None
    self.gc_time = 0.0
    self.gc_calls = 0
    self.gc_collected = 0
    self.gc_uncollectable = 0
    gc.callbacks.append(self.gc_callback)
    self.start_time = time.time()
    return self

</t>
<t tx="ekr.20230831011819.1402">def gc_callback(self, phase: str, info: Mapping[str, int]) -&gt; None:
    if phase == "start":
        assert self.gc_start_time is None, "Start phase out of sequence"
        self.gc_start_time = time.time()
    elif phase == "stop":
        assert self.gc_start_time is not None, "Stop phase out of sequence"
        self.gc_calls += 1
        self.gc_time += time.time() - self.gc_start_time
        self.gc_start_time = None
        self.gc_collected += info["collected"]
        self.gc_uncollectable += info["uncollectable"]
    else:
        assert False, f"Unrecognized gc phase ({phase!r})"

</t>
<t tx="ekr.20230831011819.1403">def __exit__(self, *args: object) -&gt; None:
    while self.gc_callback in gc.callbacks:
        gc.callbacks.remove(self.gc_callback)

</t>
<t tx="ekr.20230831011819.1404">def get_stats(self) -&gt; Mapping[str, float]:
    end_time = time.time()
    result = {}
    result["gc_time"] = self.gc_time
    result["gc_calls"] = self.gc_calls
    result["gc_collected"] = self.gc_collected
    result["gc_uncollectable"] = self.gc_uncollectable
    result["build_time"] = end_time - self.start_time
    return result
</t>
<t tx="ekr.20230831011819.1405">@path mypy
"""Git utilities."""
&lt;&lt; git.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1406">
# Used also from setup.py, so don't pull in anything additional here (like mypy or typing):
from __future__ import annotations

import os
import subprocess


</t>
<t tx="ekr.20230831011819.1407">def is_git_repo(dir: str) -&gt; bool:
    """Is the given directory version-controlled with git?"""
    return os.path.exists(os.path.join(dir, ".git"))


</t>
<t tx="ekr.20230831011819.1408">def have_git() -&gt; bool:
    """Can we run the git executable?"""
    try:
        subprocess.check_output(["git", "--help"])
        return True
    except subprocess.CalledProcessError:
        return False
    except OSError:
        return False


</t>
<t tx="ekr.20230831011819.1409">def git_revision(dir: str) -&gt; bytes:
    """Get the SHA-1 of the HEAD of a git repository."""
    return subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=dir).strip()


</t>
<t tx="ekr.20230831011819.141">def update_fine_grained_deps(self, deps: dict[str, set[str]]) -&gt; None:
    options = self.manager.options
    if options.cache_fine_grained or options.fine_grained_incremental:
        from mypy.server.deps import merge_dependencies  # Lazy import to speed up startup

        merge_dependencies(self.compute_fine_grained_deps(), deps)
        type_state.update_protocol_deps(deps)

</t>
<t tx="ekr.20230831011819.1410">def is_dirty(dir: str) -&gt; bool:
    """Check whether a git repository has uncommitted changes."""
    output = subprocess.check_output(["git", "status", "-uno", "--porcelain"], cwd=dir)
    return output.strip() != b""
</t>
<t tx="ekr.20230831011819.1411">@path mypy
"""Helpers for manipulations with graphs."""
&lt;&lt; graph_utils.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1412">
from __future__ import annotations

from typing import AbstractSet, Iterable, Iterator, TypeVar

T = TypeVar("T")


</t>
<t tx="ekr.20230831011819.1413">def strongly_connected_components(
    vertices: AbstractSet[T], edges: dict[T, list[T]]
) -&gt; Iterator[set[T]]:
    """Compute Strongly Connected Components of a directed graph.

    Args:
      vertices: the labels for the vertices
      edges: for each vertex, gives the target vertices of its outgoing edges

    Returns:
      An iterator yielding strongly connected components, each
      represented as a set of vertices.  Each input vertex will occur
      exactly once; vertices not part of a SCC are returned as
      singleton sets.

    From https://code.activestate.com/recipes/578507/.
    """
    identified: set[T] = set()
    stack: list[T] = []
    index: dict[T, int] = {}
    boundaries: list[int] = []

    def dfs(v: T) -&gt; Iterator[set[T]]:
        index[v] = len(stack)
        stack.append(v)
        boundaries.append(index[v])

        for w in edges[v]:
            if w not in index:
                yield from dfs(w)
            elif w not in identified:
                while index[w] &lt; boundaries[-1]:
                    boundaries.pop()

        if boundaries[-1] == index[v]:
            boundaries.pop()
            scc = set(stack[index[v] :])
            del stack[index[v] :]
            identified.update(scc)
            yield scc

    for v in vertices:
        if v not in index:
            yield from dfs(v)


</t>
<t tx="ekr.20230831011819.1414">def prepare_sccs(
    sccs: list[set[T]], edges: dict[T, list[T]]
) -&gt; dict[AbstractSet[T], set[AbstractSet[T]]]:
    """Use original edges to organize SCCs in a graph by dependencies between them."""
    sccsmap = {v: frozenset(scc) for scc in sccs for v in scc}
    data: dict[AbstractSet[T], set[AbstractSet[T]]] = {}
    for scc in sccs:
        deps: set[AbstractSet[T]] = set()
        for v in scc:
            deps.update(sccsmap[x] for x in edges[v])
        data[frozenset(scc)] = deps
    return data


</t>
<t tx="ekr.20230831011819.1415">def topsort(data: dict[T, set[T]]) -&gt; Iterable[set[T]]:
    """Topological sort.

    Args:
      data: A map from vertices to all vertices that it has an edge
            connecting it to.  NOTE: This data structure
            is modified in place -- for normalization purposes,
            self-dependencies are removed and entries representing
            orphans are added.

    Returns:
      An iterator yielding sets of vertices that have an equivalent
      ordering.

    Example:
      Suppose the input has the following structure:

        {A: {B, C}, B: {D}, C: {D}}

      This is normalized to:

        {A: {B, C}, B: {D}, C: {D}, D: {}}

      The algorithm will yield the following values:

        {D}
        {B, C}
        {A}

    From https://code.activestate.com/recipes/577413/.
    """
    # TODO: Use a faster algorithm?
    for k, v in data.items():
        v.discard(k)  # Ignore self dependencies.
    for item in set.union(*data.values()) - set(data.keys()):
        data[item] = set()
    while True:
        ready = {item for item, dep in data.items() if not dep}
        if not ready:
            break
        yield ready
        data = {item: (dep - ready) for item, dep in data.items() if item not in ready}
    assert not data, f"A cyclic dependency exists amongst {data!r}"
</t>
<t tx="ekr.20230831011819.1416">@path mypy
&lt;&lt; indirection.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1417">from __future__ import annotations

from typing import Iterable, Set

import mypy.types as types
from mypy.types import TypeVisitor
from mypy.util import split_module_names


</t>
<t tx="ekr.20230831011819.1418">def extract_module_names(type_name: str | None) -&gt; list[str]:
    """Returns the module names of a fully qualified type name."""
    if type_name is not None:
        # Discard the first one, which is just the qualified name of the type
        possible_module_names = split_module_names(type_name)
        return possible_module_names[1:]
    else:
        return []


</t>
<t tx="ekr.20230831011819.1419">class TypeIndirectionVisitor(TypeVisitor[Set[str]]):
    """Returns all module references within a particular type."""

    @others
</t>
<t tx="ekr.20230831011819.142">def valid_references(self) -&gt; set[str]:
    assert self.ancestors is not None
    valid_refs = set(self.dependencies + self.suppressed + self.ancestors)
    valid_refs.add(self.id)

    if "os" in valid_refs:
        valid_refs.add("os.path")

    return valid_refs

</t>
<t tx="ekr.20230831011819.1420">def __init__(self) -&gt; None:
    self.cache: dict[types.Type, set[str]] = {}
    self.seen_aliases: set[types.TypeAliasType] = set()

</t>
<t tx="ekr.20230831011819.1421">def find_modules(self, typs: Iterable[types.Type]) -&gt; set[str]:
    self.seen_aliases.clear()
    return self._visit(typs)

</t>
<t tx="ekr.20230831011819.1422">def _visit(self, typ_or_typs: types.Type | Iterable[types.Type]) -&gt; set[str]:
    typs = [typ_or_typs] if isinstance(typ_or_typs, types.Type) else typ_or_typs
    output: set[str] = set()
    for typ in typs:
        if isinstance(typ, types.TypeAliasType):
            # Avoid infinite recursion for recursive type aliases.
            if typ in self.seen_aliases:
                continue
            self.seen_aliases.add(typ)
        if typ in self.cache:
            modules = self.cache[typ]
        else:
            modules = typ.accept(self)
            self.cache[typ] = set(modules)
        output.update(modules)
    return output

</t>
<t tx="ekr.20230831011819.1423">def visit_unbound_type(self, t: types.UnboundType) -&gt; set[str]:
    return self._visit(t.args)

</t>
<t tx="ekr.20230831011819.1424">def visit_any(self, t: types.AnyType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20230831011819.1425">def visit_none_type(self, t: types.NoneType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20230831011819.1426">def visit_uninhabited_type(self, t: types.UninhabitedType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20230831011819.1427">def visit_erased_type(self, t: types.ErasedType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20230831011819.1428">def visit_deleted_type(self, t: types.DeletedType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20230831011819.1429">def visit_type_var(self, t: types.TypeVarType) -&gt; set[str]:
    return self._visit(t.values) | self._visit(t.upper_bound) | self._visit(t.default)

</t>
<t tx="ekr.20230831011819.143">def write_cache(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We don't support writing cache files in fine-grained incremental mode.
    if (
        not self.path
        or self.options.cache_dir == os.devnull
        or self.options.fine_grained_incremental
    ):
        if self.options.debug_serialize:
            try:
                self.tree.serialize()
            except Exception:
                print(f"Error serializing {self.id}", file=self.manager.stdout)
                raise  # Propagate to display traceback
        return
    is_errors = self.transitive_error
    if is_errors:
        delete_cache(self.id, self.path, self.manager)
        self.meta = None
        self.mark_interface_stale(on_errors=True)
        return
    dep_prios = self.dependency_priorities()
    dep_lines = self.dependency_lines()
    assert self.source_hash is not None
    assert len(set(self.dependencies)) == len(
        self.dependencies
    ), f"Duplicates in dependencies list for {self.id} ({self.dependencies})"
    new_interface_hash, self.meta = write_cache(
        self.id,
        self.path,
        self.tree,
        list(self.dependencies),
        list(self.suppressed),
        dep_prios,
        dep_lines,
        self.interface_hash,
        self.source_hash,
        self.ignore_all,
        self.manager,
    )
    if new_interface_hash == self.interface_hash:
        self.manager.log(f"Cached module {self.id} has same interface")
    else:
        self.manager.log(f"Cached module {self.id} has changed interface")
        self.mark_interface_stale()
        self.interface_hash = new_interface_hash

</t>
<t tx="ekr.20230831011819.1430">def visit_param_spec(self, t: types.ParamSpecType) -&gt; set[str]:
    return self._visit(t.upper_bound) | self._visit(t.default)

</t>
<t tx="ekr.20230831011819.1431">def visit_type_var_tuple(self, t: types.TypeVarTupleType) -&gt; set[str]:
    return self._visit(t.upper_bound) | self._visit(t.default)

</t>
<t tx="ekr.20230831011819.1432">def visit_unpack_type(self, t: types.UnpackType) -&gt; set[str]:
    return t.type.accept(self)

</t>
<t tx="ekr.20230831011819.1433">def visit_parameters(self, t: types.Parameters) -&gt; set[str]:
    return self._visit(t.arg_types)

</t>
<t tx="ekr.20230831011819.1434">def visit_instance(self, t: types.Instance) -&gt; set[str]:
    out = self._visit(t.args)
    if t.type:
        # Uses of a class depend on everything in the MRO,
        # as changes to classes in the MRO can add types to methods,
        # change property types, change the MRO itself, etc.
        for s in t.type.mro:
            out.update(split_module_names(s.module_name))
        if t.type.metaclass_type is not None:
            out.update(split_module_names(t.type.metaclass_type.type.module_name))
    return out

</t>
<t tx="ekr.20230831011819.1435">def visit_callable_type(self, t: types.CallableType) -&gt; set[str]:
    out = self._visit(t.arg_types) | self._visit(t.ret_type)
    if t.definition is not None:
        out.update(extract_module_names(t.definition.fullname))
    return out

</t>
<t tx="ekr.20230831011819.1436">def visit_overloaded(self, t: types.Overloaded) -&gt; set[str]:
    return self._visit(t.items) | self._visit(t.fallback)

</t>
<t tx="ekr.20230831011819.1437">def visit_tuple_type(self, t: types.TupleType) -&gt; set[str]:
    return self._visit(t.items) | self._visit(t.partial_fallback)

</t>
<t tx="ekr.20230831011819.1438">def visit_typeddict_type(self, t: types.TypedDictType) -&gt; set[str]:
    return self._visit(t.items.values()) | self._visit(t.fallback)

</t>
<t tx="ekr.20230831011819.1439">def visit_literal_type(self, t: types.LiteralType) -&gt; set[str]:
    return self._visit(t.fallback)

</t>
<t tx="ekr.20230831011819.144">def verify_dependencies(self, suppressed_only: bool = False) -&gt; None:
    """Report errors for import targets in modules that don't exist.

    If suppressed_only is set, only check suppressed dependencies.
    """
    manager = self.manager
    assert self.ancestors is not None
    if suppressed_only:
        all_deps = self.suppressed
    else:
        # Strip out indirect dependencies. See comment in build.load_graph().
        dependencies = [
            dep for dep in self.dependencies if self.priorities.get(dep) != PRI_INDIRECT
        ]
        all_deps = dependencies + self.suppressed + self.ancestors
    for dep in all_deps:
        if dep in manager.modules:
            continue
        options = manager.options.clone_for_module(dep)
        if options.ignore_missing_imports:
            continue
        line = self.dep_line_map.get(dep, 1)
        try:
            if dep in self.ancestors:
                state: State | None = None
                ancestor: State | None = self
            else:
                state, ancestor = self, None
            # Called just for its side effects of producing diagnostics.
            find_module_and_diagnose(
                manager,
                dep,
                options,
                caller_state=state,
                caller_line=line,
                ancestor_for=ancestor,
            )
        except (ModuleNotFound, CompileError):
            # Swallow up any ModuleNotFounds or CompilerErrors while generating
            # a diagnostic. CompileErrors may get generated in
            # fine-grained mode when an __init__.py is deleted, if a module
            # that was in that package has targets reprocessed before
            # it is renamed.
            pass

</t>
<t tx="ekr.20230831011819.1440">def visit_union_type(self, t: types.UnionType) -&gt; set[str]:
    return self._visit(t.items)

</t>
<t tx="ekr.20230831011819.1441">def visit_partial_type(self, t: types.PartialType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20230831011819.1442">def visit_type_type(self, t: types.TypeType) -&gt; set[str]:
    return self._visit(t.item)

</t>
<t tx="ekr.20230831011819.1443">def visit_type_alias_type(self, t: types.TypeAliasType) -&gt; set[str]:
    return self._visit(types.get_proper_type(t))
</t>
<t tx="ekr.20230831011819.1444">@path mypy
"""Utilities for type argument inference."""
&lt;&lt; infer.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1445">
from __future__ import annotations

from typing import NamedTuple, Sequence

from mypy.constraints import (
    SUBTYPE_OF,
    SUPERTYPE_OF,
    infer_constraints,
    infer_constraints_for_callable,
)
from mypy.nodes import ArgKind
from mypy.solve import solve_constraints
from mypy.types import CallableType, Instance, Type, TypeVarLikeType


</t>
<t tx="ekr.20230831011819.1446">class ArgumentInferContext(NamedTuple):
    """Type argument inference context.

    We need this because we pass around ``Mapping`` and ``Iterable`` types.
    These types are only known by ``TypeChecker`` itself.
    It is required for ``*`` and ``**`` argument inference.

    https://github.com/python/mypy/issues/11144
    """

    mapping_type: Instance
    iterable_type: Instance


</t>
<t tx="ekr.20230831011819.1447">def infer_function_type_arguments(
    callee_type: CallableType,
    arg_types: Sequence[Type | None],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
    context: ArgumentInferContext,
    strict: bool = True,
    allow_polymorphic: bool = False,
) -&gt; tuple[list[Type | None], list[TypeVarLikeType]]:
    """Infer the type arguments of a generic function.

    Return an array of lower bound types for the type variables -1 (at
    index 0), -2 (at index 1), etc. A lower bound is None if a value
    could not be inferred.

    Arguments:
      callee_type: the target generic function
      arg_types: argument types at the call site (each optional; if None,
                 we are not considering this argument in the current pass)
      arg_kinds: nodes.ARG_* values for arg_types
      formal_to_actual: mapping from formal to actual variable indices
    """
    # Infer constraints.
    constraints = infer_constraints_for_callable(
        callee_type, arg_types, arg_kinds, formal_to_actual, context
    )

    # Solve constraints.
    type_vars = callee_type.variables
    return solve_constraints(type_vars, constraints, strict, allow_polymorphic)


</t>
<t tx="ekr.20230831011819.1448">def infer_type_arguments(
    type_vars: Sequence[TypeVarLikeType], template: Type, actual: Type, is_supertype: bool = False
) -&gt; list[Type | None]:
    # Like infer_function_type_arguments, but only match a single type
    # against a generic type.
    constraints = infer_constraints(template, actual, SUPERTYPE_OF if is_supertype else SUBTYPE_OF)
    return solve_constraints(type_vars, constraints)[0]
</t>
<t tx="ekr.20230831011819.1449">@path mypy
&lt;&lt; inspections.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.145">def dependency_priorities(self) -&gt; list[int]:
    return [self.priorities.get(dep, PRI_HIGH) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20230831011819.1450">from __future__ import annotations

import os
from collections import defaultdict
from functools import cmp_to_key
from typing import Callable

from mypy.build import State
from mypy.find_sources import InvalidSourceList, SourceFinder
from mypy.messages import format_type
from mypy.modulefinder import PYTHON_EXTENSIONS
from mypy.nodes import (
    LDEF,
    Decorator,
    Expression,
    FuncBase,
    MemberExpr,
    MypyFile,
    Node,
    OverloadedFuncDef,
    RefExpr,
    SymbolNode,
    TypeInfo,
    Var,
)
from mypy.server.update import FineGrainedBuildManager
from mypy.traverser import ExtendedTraverserVisitor
from mypy.typeops import tuple_fallback
from mypy.types import (
    FunctionLike,
    Instance,
    LiteralType,
    ProperType,
    TupleType,
    TypedDictType,
    TypeVarType,
    UnionType,
    get_proper_type,
)
from mypy.typevars import fill_typevars_with_any


</t>
<t tx="ekr.20230831011819.1451">def node_starts_after(o: Node, line: int, column: int) -&gt; bool:
    return o.line &gt; line or o.line == line and o.column &gt; column


</t>
<t tx="ekr.20230831011819.1452">def node_ends_before(o: Node, line: int, column: int) -&gt; bool:
    # Unfortunately, end positions for some statements are a mess,
    # e.g. overloaded functions, so we return False when we don't know.
    if o.end_line is not None and o.end_column is not None:
        if o.end_line &lt; line or o.end_line == line and o.end_column &lt; column:
            return True
    return False


</t>
<t tx="ekr.20230831011819.1453">def expr_span(expr: Expression) -&gt; str:
    """Format expression span as in mypy error messages."""
    return f"{expr.line}:{expr.column + 1}:{expr.end_line}:{expr.end_column}"


</t>
<t tx="ekr.20230831011819.1454">def get_instance_fallback(typ: ProperType) -&gt; list[Instance]:
    """Returns the Instance fallback for this type if one exists or None."""
    if isinstance(typ, Instance):
        return [typ]
    elif isinstance(typ, TupleType):
        return [tuple_fallback(typ)]
    elif isinstance(typ, TypedDictType):
        return [typ.fallback]
    elif isinstance(typ, FunctionLike):
        return [typ.fallback]
    elif isinstance(typ, LiteralType):
        return [typ.fallback]
    elif isinstance(typ, TypeVarType):
        if typ.values:
            res = []
            for t in typ.values:
                res.extend(get_instance_fallback(get_proper_type(t)))
            return res
        return get_instance_fallback(get_proper_type(typ.upper_bound))
    elif isinstance(typ, UnionType):
        res = []
        for t in typ.items:
            res.extend(get_instance_fallback(get_proper_type(t)))
        return res
    return []


</t>
<t tx="ekr.20230831011819.1455">def find_node(name: str, info: TypeInfo) -&gt; Var | FuncBase | None:
    """Find the node defining member 'name' in given TypeInfo."""
    # TODO: this code shares some logic with checkmember.py
    method = info.get_method(name)
    if method:
        if isinstance(method, Decorator):
            return method.var
        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            return dec.var
        return method
    else:
        # don't have such method, maybe variable?
        node = info.get(name)
        v = node.node if node else None
        if isinstance(v, Var):
            return v
    return None


</t>
<t tx="ekr.20230831011819.1456">def find_module_by_fullname(fullname: str, modules: dict[str, State]) -&gt; State | None:
    """Find module by a node fullname.

    This logic mimics the one we use in fixup, so should be good enough.
    """
    head = fullname
    # Special case: a module symbol is considered to be defined in itself, not in enclosing
    # package, since this is what users want when clicking go to definition on a module.
    if head in modules:
        return modules[head]
    while True:
        if "." not in head:
            return None
        head, tail = head.rsplit(".", maxsplit=1)
        mod = modules.get(head)
        if mod is not None:
            return mod


</t>
<t tx="ekr.20230831011819.1457">class SearchVisitor(ExtendedTraverserVisitor):
    """Visitor looking for an expression whose span matches given one exactly."""

    @others
</t>
<t tx="ekr.20230831011819.1458">def __init__(self, line: int, column: int, end_line: int, end_column: int) -&gt; None:
    self.line = line
    self.column = column
    self.end_line = end_line
    self.end_column = end_column
    self.result: Expression | None = None

</t>
<t tx="ekr.20230831011819.1459">def visit(self, o: Node) -&gt; bool:
    if node_starts_after(o, self.line, self.column):
        return False
    if node_ends_before(o, self.end_line, self.end_column):
        return False
    if (
        o.line == self.line
        and o.end_line == self.end_line
        and o.column == self.column
        and o.end_column == self.end_column
    ):
        if isinstance(o, Expression):
            self.result = o
    return self.result is None


</t>
<t tx="ekr.20230831011819.146">def dependency_lines(self) -&gt; list[int]:
    return [self.dep_line_map.get(dep, 1) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20230831011819.1460">def find_by_location(
    tree: MypyFile, line: int, column: int, end_line: int, end_column: int
) -&gt; Expression | None:
    """Find an expression matching given span, or None if not found."""
    if end_line &lt; line:
        raise ValueError('"end_line" must not be before "line"')
    if end_line == line and end_column &lt;= column:
        raise ValueError('"end_column" must be after "column"')
    visitor = SearchVisitor(line, column, end_line, end_column)
    tree.accept(visitor)
    return visitor.result


</t>
<t tx="ekr.20230831011819.1461">class SearchAllVisitor(ExtendedTraverserVisitor):
    """Visitor looking for all expressions whose spans enclose given position."""

    @others
</t>
<t tx="ekr.20230831011819.1462">def __init__(self, line: int, column: int) -&gt; None:
    self.line = line
    self.column = column
    self.result: list[Expression] = []

</t>
<t tx="ekr.20230831011819.1463">def visit(self, o: Node) -&gt; bool:
    if node_starts_after(o, self.line, self.column):
        return False
    if node_ends_before(o, self.line, self.column):
        return False
    if isinstance(o, Expression):
        self.result.append(o)
    return True


</t>
<t tx="ekr.20230831011819.1464">def find_all_by_location(tree: MypyFile, line: int, column: int) -&gt; list[Expression]:
    """Find all expressions enclosing given position starting from innermost."""
    visitor = SearchAllVisitor(line, column)
    tree.accept(visitor)
    return list(reversed(visitor.result))


</t>
<t tx="ekr.20230831011819.1465">class InspectionEngine:
    """Engine for locating and statically inspecting expressions."""

    @others
</t>
<t tx="ekr.20230831011819.1466">def __init__(
    self,
    fg_manager: FineGrainedBuildManager,
    *,
    verbosity: int = 0,
    limit: int = 0,
    include_span: bool = False,
    include_kind: bool = False,
    include_object_attrs: bool = False,
    union_attrs: bool = False,
    force_reload: bool = False,
) -&gt; None:
    self.fg_manager = fg_manager
    self.finder = SourceFinder(
        self.fg_manager.manager.fscache, self.fg_manager.manager.options
    )
    self.verbosity = verbosity
    self.limit = limit
    self.include_span = include_span
    self.include_kind = include_kind
    self.include_object_attrs = include_object_attrs
    self.union_attrs = union_attrs
    self.force_reload = force_reload
    # Module for which inspection was requested.
    self.module: State | None = None

</t>
<t tx="ekr.20230831011819.1467">def parse_location(self, location: str) -&gt; tuple[str, list[int]]:
    if location.count(":") not in [2, 4]:
        raise ValueError("Format should be file:line:column[:end_line:end_column]")
    parts = location.split(":")
    module, *rest = parts
    return module, [int(p) for p in rest]

</t>
<t tx="ekr.20230831011819.1468">def reload_module(self, state: State) -&gt; None:
    """Reload given module while temporary exporting types."""
    old = self.fg_manager.manager.options.export_types
    self.fg_manager.manager.options.export_types = True
    try:
        self.fg_manager.flush_cache()
        assert state.path is not None
        self.fg_manager.update([(state.id, state.path)], [])
    finally:
        self.fg_manager.manager.options.export_types = old

</t>
<t tx="ekr.20230831011819.1469">def expr_type(self, expression: Expression) -&gt; tuple[str, bool]:
    """Format type for an expression using current options.

    If type is known, second item returned is True. If type is not known, an error
    message is returned instead, and second item returned is False.
    """
    expr_type = self.fg_manager.manager.all_types.get(expression)
    if expr_type is None:
        return self.missing_type(expression), False

    type_str = format_type(
        expr_type, self.fg_manager.manager.options, verbosity=self.verbosity
    )
    return self.add_prefixes(type_str, expression), True

</t>
<t tx="ekr.20230831011819.147">def generate_unused_ignore_notes(self) -&gt; None:
    if (
        self.options.warn_unused_ignores
        or codes.UNUSED_IGNORE in self.options.enabled_error_codes
    ) and codes.UNUSED_IGNORE not in self.options.disabled_error_codes:
        # If this file was initially loaded from the cache, it may have suppressed
        # dependencies due to imports with ignores on them. We need to generate
        # those errors to avoid spuriously flagging them as unused ignores.
        if self.meta:
            self.verify_dependencies(suppressed_only=True)
        self.manager.errors.generate_unused_ignore_errors(self.xpath)

</t>
<t tx="ekr.20230831011819.1470">def object_type(self) -&gt; Instance:
    builtins = self.fg_manager.graph["builtins"].tree
    assert builtins is not None
    object_node = builtins.names["object"].node
    assert isinstance(object_node, TypeInfo)
    return Instance(object_node, [])

</t>
<t tx="ekr.20230831011819.1471">def collect_attrs(self, instances: list[Instance]) -&gt; dict[TypeInfo, list[str]]:
    """Collect attributes from all union/typevar variants."""

    def item_attrs(attr_dict: dict[TypeInfo, list[str]]) -&gt; set[str]:
        attrs = set()
        for base in attr_dict:
            attrs |= set(attr_dict[base])
        return attrs

    def cmp_types(x: TypeInfo, y: TypeInfo) -&gt; int:
        if x in y.mro:
            return 1
        if y in x.mro:
            return -1
        return 0

    # First gather all attributes for every union variant.
    assert instances
    all_attrs = []
    for instance in instances:
        attrs = {}
        mro = instance.type.mro
        if not self.include_object_attrs:
            mro = mro[:-1]
        for base in mro:
            attrs[base] = sorted(base.names)
        all_attrs.append(attrs)

    # Find attributes valid for all variants in a union or type variable.
    intersection = item_attrs(all_attrs[0])
    for item in all_attrs[1:]:
        intersection &amp;= item_attrs(item)

    # Combine attributes from all variants into a single dict while
    # also removing invalid attributes (unless using --union-attrs).
    combined_attrs = defaultdict(list)
    for item in all_attrs:
        for base in item:
            if base in combined_attrs:
                continue
            for name in item[base]:
                if self.union_attrs or name in intersection:
                    combined_attrs[base].append(name)

    # Sort bases by MRO, unrelated will appear in the order they appeared as union variants.
    sorted_bases = sorted(combined_attrs.keys(), key=cmp_to_key(cmp_types))
    result = {}
    for base in sorted_bases:
        if not combined_attrs[base]:
            # Skip bases where everytihng was filtered out.
            continue
        result[base] = combined_attrs[base]
    return result

</t>
<t tx="ekr.20230831011819.1472">def _fill_from_dict(
    self, attrs_strs: list[str], attrs_dict: dict[TypeInfo, list[str]]
) -&gt; None:
    for base in attrs_dict:
        cls_name = base.name if self.verbosity &lt; 1 else base.fullname
        attrs = [f'"{attr}"' for attr in attrs_dict[base]]
        attrs_strs.append(f'"{cls_name}": [{", ".join(attrs)}]')

</t>
<t tx="ekr.20230831011819.1473">def expr_attrs(self, expression: Expression) -&gt; tuple[str, bool]:
    """Format attributes that are valid for a given expression.

    If expression type is not an Instance, try using fallback. Attributes are
    returned as a JSON (ordered by MRO) that maps base class name to list of
    attributes. Attributes may appear in multiple bases if overridden (we simply
    follow usual mypy logic for creating new Vars etc).
    """
    expr_type = self.fg_manager.manager.all_types.get(expression)
    if expr_type is None:
        return self.missing_type(expression), False

    expr_type = get_proper_type(expr_type)
    instances = get_instance_fallback(expr_type)
    if not instances:
        # Everything is an object in Python.
        instances = [self.object_type()]

    attrs_dict = self.collect_attrs(instances)

    # Special case: modules have names apart from those from ModuleType.
    if isinstance(expression, RefExpr) and isinstance(expression.node, MypyFile):
        node = expression.node
        names = sorted(node.names)
        if "__builtins__" in names:
            # This is just to make tests stable. No one will really need ths name.
            names.remove("__builtins__")
        mod_dict = {f'"&lt;{node.fullname}&gt;"': [f'"{name}"' for name in names]}
    else:
        mod_dict = {}

    # Special case: for class callables, prepend with the class attributes.
    # TODO: also handle cases when such callable appears in a union.
    if isinstance(expr_type, FunctionLike) and expr_type.is_type_obj():
        template = fill_typevars_with_any(expr_type.type_object())
        class_dict = self.collect_attrs(get_instance_fallback(template))
    else:
        class_dict = {}

    # We don't use JSON dump to be sure keys order is always preserved.
    base_attrs = []
    if mod_dict:
        for mod in mod_dict:
            base_attrs.append(f'{mod}: [{", ".join(mod_dict[mod])}]')
    self._fill_from_dict(base_attrs, class_dict)
    self._fill_from_dict(base_attrs, attrs_dict)
    return self.add_prefixes(f'{{{", ".join(base_attrs)}}}', expression), True

</t>
<t tx="ekr.20230831011819.1474">def format_node(self, module: State, node: FuncBase | SymbolNode) -&gt; str:
    return f"{module.path}:{node.line}:{node.column + 1}:{node.name}"

</t>
<t tx="ekr.20230831011819.1475">def collect_nodes(self, expression: RefExpr) -&gt; list[FuncBase | SymbolNode]:
    """Collect nodes that can be referred to by an expression.

    Note: it can be more than one for example in case of a union attribute.
    """
    node: FuncBase | SymbolNode | None = expression.node
    nodes: list[FuncBase | SymbolNode]
    if node is None:
        # Tricky case: instance attribute
        if isinstance(expression, MemberExpr) and expression.kind is None:
            base_type = self.fg_manager.manager.all_types.get(expression.expr)
            if base_type is None:
                return []

            # Now we use the base type to figure out where the attribute is defined.
            base_type = get_proper_type(base_type)
            instances = get_instance_fallback(base_type)
            nodes = []
            for instance in instances:
                node = find_node(expression.name, instance.type)
                if node:
                    nodes.append(node)
            if not nodes:
                # Try checking class namespace if attribute is on a class object.
                if isinstance(base_type, FunctionLike) and base_type.is_type_obj():
                    instances = get_instance_fallback(
                        fill_typevars_with_any(base_type.type_object())
                    )
                    for instance in instances:
                        node = find_node(expression.name, instance.type)
                        if node:
                            nodes.append(node)
                else:
                    # Still no luck, give up.
                    return []
        else:
            return []
    else:
        # Easy case: a module-level definition
        nodes = [node]
    return nodes

</t>
<t tx="ekr.20230831011819.1476">def modules_for_nodes(
    self, nodes: list[FuncBase | SymbolNode], expression: RefExpr
) -&gt; tuple[dict[FuncBase | SymbolNode, State], bool]:
    """Gather modules where given nodes where defined.

    Also check if they need to be refreshed (cached nodes may have
    lines/columns missing).
    """
    modules = {}
    reload_needed = False
    for node in nodes:
        module = find_module_by_fullname(node.fullname, self.fg_manager.graph)
        if not module:
            if expression.kind == LDEF and self.module:
                module = self.module
            else:
                continue
        modules[node] = module
        if not module.tree or module.tree.is_cache_skeleton or self.force_reload:
            reload_needed |= not module.tree or module.tree.is_cache_skeleton
            self.reload_module(module)
    return modules, reload_needed

</t>
<t tx="ekr.20230831011819.1477">def expression_def(self, expression: Expression) -&gt; tuple[str, bool]:
    """Find and format definition location for an expression.

    If it is not a RefExpr, it is effectively skipped by returning an
    empty result.
    """
    if not isinstance(expression, RefExpr):
        # If there are no suitable matches at all, we return error later.
        return "", True

    nodes = self.collect_nodes(expression)

    if not nodes:
        return self.missing_node(expression), False

    modules, reload_needed = self.modules_for_nodes(nodes, expression)
    if reload_needed:
        # TODO: line/column are not stored in cache for vast majority of symbol nodes.
        # Adding them will make thing faster, but will have visible memory impact.
        nodes = self.collect_nodes(expression)
        modules, reload_needed = self.modules_for_nodes(nodes, expression)
        assert not reload_needed

    result = []
    for node in modules:
        result.append(self.format_node(modules[node], node))

    if not result:
        return self.missing_node(expression), False

    return self.add_prefixes(", ".join(result), expression), True

</t>
<t tx="ekr.20230831011819.1478">def missing_type(self, expression: Expression) -&gt; str:
    alt_suggestion = ""
    if not self.force_reload:
        alt_suggestion = " or try --force-reload"
    return (
        f'No known type available for "{type(expression).__name__}"'
        f" (maybe unreachable{alt_suggestion})"
    )

</t>
<t tx="ekr.20230831011819.1479">def missing_node(self, expression: Expression) -&gt; str:
    return (
        f'Cannot find definition for "{type(expression).__name__}"'
        f" at {expr_span(expression)}"
    )

</t>
<t tx="ekr.20230831011819.148">def generate_ignore_without_code_notes(self) -&gt; None:
    if self.manager.errors.is_error_code_enabled(codes.IGNORE_WITHOUT_CODE):
        self.manager.errors.generate_ignore_without_code_errors(
            self.xpath, self.options.warn_unused_ignores
        )


</t>
<t tx="ekr.20230831011819.1480">def add_prefixes(self, result: str, expression: Expression) -&gt; str:
    prefixes = []
    if self.include_kind:
        prefixes.append(f"{type(expression).__name__}")
    if self.include_span:
        prefixes.append(expr_span(expression))
    if prefixes:
        prefix = ":".join(prefixes) + " -&gt; "
    else:
        prefix = ""
    return prefix + result

</t>
<t tx="ekr.20230831011819.1481">def run_inspection_by_exact_location(
    self,
    tree: MypyFile,
    line: int,
    column: int,
    end_line: int,
    end_column: int,
    method: Callable[[Expression], tuple[str, bool]],
) -&gt; dict[str, object]:
    """Get type of an expression matching a span.

    Type or error is returned as a standard daemon response dict.
    """
    try:
        expression = find_by_location(tree, line, column - 1, end_line, end_column)
    except ValueError as err:
        return {"error": str(err)}

    if expression is None:
        span = f"{line}:{column}:{end_line}:{end_column}"
        return {"out": f"Can't find expression at span {span}", "err": "", "status": 1}

    inspection_str, success = method(expression)
    return {"out": inspection_str, "err": "", "status": 0 if success else 1}

</t>
<t tx="ekr.20230831011819.1482">def run_inspection_by_position(
    self,
    tree: MypyFile,
    line: int,
    column: int,
    method: Callable[[Expression], tuple[str, bool]],
) -&gt; dict[str, object]:
    """Get types of all expressions enclosing a position.

    Types and/or errors are returned as a standard daemon response dict.
    """
    expressions = find_all_by_location(tree, line, column - 1)
    if not expressions:
        position = f"{line}:{column}"
        return {
            "out": f"Can't find any expressions at position {position}",
            "err": "",
            "status": 1,
        }

    inspection_strs = []
    status = 0
    for expression in expressions:
        inspection_str, success = method(expression)
        if not success:
            status = 1
        if inspection_str:
            inspection_strs.append(inspection_str)
    if self.limit:
        inspection_strs = inspection_strs[: self.limit]
    return {"out": "\n".join(inspection_strs), "err": "", "status": status}

</t>
<t tx="ekr.20230831011819.1483">def find_module(self, file: str) -&gt; tuple[State | None, dict[str, object]]:
    """Find module by path, or return a suitable error message.

    Note we don't use exceptions to simplify handling 1 vs 2 statuses.
    """
    if not any(file.endswith(ext) for ext in PYTHON_EXTENSIONS):
        return None, {"error": "Source file is not a Python file"}

    try:
        module, _ = self.finder.crawl_up(os.path.normpath(file))
    except InvalidSourceList:
        return None, {"error": "Invalid source file name: " + file}

    state = self.fg_manager.graph.get(module)
    self.module = state
    return (
        state,
        {"out": f"Unknown module: {module}", "err": "", "status": 1} if state is None else {},
    )

</t>
<t tx="ekr.20230831011819.1484">def run_inspection(
    self, location: str, method: Callable[[Expression], tuple[str, bool]]
) -&gt; dict[str, object]:
    """Top-level logic to inspect expression(s) at a location.

    This can be re-used by various simple inspections.
    """
    try:
        file, pos = self.parse_location(location)
    except ValueError as err:
        return {"error": str(err)}

    state, err_dict = self.find_module(file)
    if state is None:
        assert err_dict
        return err_dict

    # Force reloading to load from cache, account for any edits, etc.
    if not state.tree or state.tree.is_cache_skeleton or self.force_reload:
        self.reload_module(state)
    assert state.tree is not None

    if len(pos) == 4:
        # Full span, return an exact match only.
        line, column, end_line, end_column = pos
        return self.run_inspection_by_exact_location(
            state.tree, line, column, end_line, end_column, method
        )
    assert len(pos) == 2
    # Inexact location, return all expressions.
    line, column = pos
    return self.run_inspection_by_position(state.tree, line, column, method)

</t>
<t tx="ekr.20230831011819.1485">def get_type(self, location: str) -&gt; dict[str, object]:
    """Get types of expression(s) at a location."""
    return self.run_inspection(location, self.expr_type)

</t>
<t tx="ekr.20230831011819.1486">def get_attrs(self, location: str) -&gt; dict[str, object]:
    """Get attributes of expression(s) at a location."""
    return self.run_inspection(location, self.expr_attrs)

</t>
<t tx="ekr.20230831011819.1487">def get_definition(self, location: str) -&gt; dict[str, object]:
    """Get symbol definitions of expression(s) at a location."""
    result = self.run_inspection(location, self.expression_def)
    if "out" in result and not result["out"]:
        # None of the expressions found turns out to be a RefExpr.
        _, location = location.split(":", maxsplit=1)
        result["out"] = f"No name or member expressions at {location}"
        result["status"] = 1
    return result
</t>
<t tx="ekr.20230831011819.1488">@path mypy
"""Cross platform abstractions for inter-process communication

On Unix, this uses AF_UNIX sockets.
On Windows, this uses NamedPipes.
"""

&lt;&lt; ipc.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.149"># Module import and diagnostic glue


def find_module_and_diagnose(
    manager: BuildManager,
    id: str,
    options: Options,
    caller_state: State | None = None,
    caller_line: int = 0,
    ancestor_for: State | None = None,
    root_source: bool = False,
    skip_diagnose: bool = False,
) -&gt; tuple[str, str]:
    """Find a module by name, respecting follow_imports and producing diagnostics.

    If the module is not found, then the ModuleNotFound exception is raised.

    Args:
      id: module to find
      options: the options for the module being loaded
      caller_state: the state of the importing module, if applicable
      caller_line: the line number of the import
      ancestor_for: the child module this is an ancestor of, if applicable
      root_source: whether this source was specified on the command line
      skip_diagnose: skip any error diagnosis and reporting (but ModuleNotFound is
          still raised if the module is missing)

    The specified value of follow_imports for a module can be overridden
    if the module is specified on the command line or if it is a stub,
    so we compute and return the "effective" follow_imports of the module.

    Returns a tuple containing (file path, target's effective follow_imports setting)
    """
    result = find_module_with_reason(id, manager)
    if isinstance(result, str):
        # For non-stubs, look at options.follow_imports:
        # - normal (default) -&gt; fully analyze
        # - silent -&gt; analyze but silence errors
        # - skip -&gt; don't analyze, make the type Any
        follow_imports = options.follow_imports
        if (
            root_source  # Honor top-level modules
            or (
                result.endswith(".pyi")  # Stubs are always normal
                and not options.follow_imports_for_stubs  # except when they aren't
            )
            or id in CORE_BUILTIN_MODULES  # core is always normal
        ):
            follow_imports = "normal"
        if skip_diagnose:
            pass
        elif follow_imports == "silent":
            # Still import it, but silence non-blocker errors.
            manager.log(f"Silencing {result} ({id})")
        elif follow_imports == "skip" or follow_imports == "error":
            # In 'error' mode, produce special error messages.
            if id not in manager.missing_modules:
                manager.log(f"Skipping {result} ({id})")
            if follow_imports == "error":
                if ancestor_for:
                    skipping_ancestor(manager, id, result, ancestor_for)
                else:
                    skipping_module(manager, caller_line, caller_state, id, result)
            raise ModuleNotFound
        if is_silent_import_module(manager, result) and not root_source:
            follow_imports = "silent"
        return (result, follow_imports)
    else:
        # Could not find a module.  Typically the reason is a
        # misspelled module name, missing stub, module not in
        # search path or the module has not been installed.

        ignore_missing_imports = options.ignore_missing_imports

        id_components = id.split(".")
        # Don't honor a global (not per-module) ignore_missing_imports
        # setting for modules that used to have bundled stubs, as
        # otherwise updating mypy can silently result in new false
        # negatives. (Unless there are stubs but they are incomplete.)
        global_ignore_missing_imports = manager.options.ignore_missing_imports
        if (
            any(
                ".".join(id_components[:i]) in legacy_bundled_packages
                for i in range(len(id_components), 0, -1)
            )
            and global_ignore_missing_imports
            and not options.ignore_missing_imports_per_module
            and result is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
        ):
            ignore_missing_imports = False

        if skip_diagnose:
            raise ModuleNotFound
        if caller_state:
            if not (ignore_missing_imports or in_partial_package(id, manager)):
                module_not_found(manager, caller_line, caller_state, id, result)
            raise ModuleNotFound
        elif root_source:
            # If we can't find a root source it's always fatal.
            # TODO: This might hide non-fatal errors from
            # root sources processed earlier.
            raise CompileError([f"mypy: can't find module '{id}'"])
        else:
            raise ModuleNotFound


</t>
<t tx="ekr.20230831011819.1490">from __future__ import annotations

import base64
import os
import shutil
import sys
import tempfile
from types import TracebackType
from typing import Callable, Final

if sys.platform == "win32":
    # This may be private, but it is needed for IPC on Windows, and is basically stable
    import ctypes

    import _winapi

    _IPCHandle = int

    kernel32 = ctypes.windll.kernel32
    DisconnectNamedPipe: Callable[[_IPCHandle], int] = kernel32.DisconnectNamedPipe
    FlushFileBuffers: Callable[[_IPCHandle], int] = kernel32.FlushFileBuffers
else:
    import socket

    _IPCHandle = socket.socket


</t>
<t tx="ekr.20230831011819.1491">class IPCException(Exception):
    """Exception for IPC issues."""


</t>
<t tx="ekr.20230831011819.1492">class IPCBase:
    """Base class for communication between the dmypy client and server.

    This contains logic shared between the client and server, such as reading
    and writing.
    """

    @others
</t>
<t tx="ekr.20230831011819.1493">connection: _IPCHandle

def __init__(self, name: str, timeout: float | None) -&gt; None:
    self.name = name
    self.timeout = timeout

</t>
<t tx="ekr.20230831011819.1494">def read(self, size: int = 100000) -&gt; bytes:
    """Read bytes from an IPC connection until its empty."""
    bdata = bytearray()
    if sys.platform == "win32":
        while True:
            ov, err = _winapi.ReadFile(self.connection, size, overlapped=True)
            try:
                if err == _winapi.ERROR_IO_PENDING:
                    timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                    res = _winapi.WaitForSingleObject(ov.event, timeout)
                    if res != _winapi.WAIT_OBJECT_0:
                        raise IPCException(f"Bad result from I/O wait: {res}")
            except BaseException:
                ov.cancel()
                raise
            _, err = ov.GetOverlappedResult(True)
            more = ov.getbuffer()
            if more:
                bdata.extend(more)
            if err == 0:
                # we are done!
                break
            elif err == _winapi.ERROR_MORE_DATA:
                # read again
                continue
            elif err == _winapi.ERROR_OPERATION_ABORTED:
                raise IPCException("ReadFile operation aborted.")
    else:
        while True:
            more = self.connection.recv(size)
            if not more:
                break
            bdata.extend(more)
    return bytes(bdata)

</t>
<t tx="ekr.20230831011819.1495">def write(self, data: bytes) -&gt; None:
    """Write bytes to an IPC connection."""
    if sys.platform == "win32":
        try:
            ov, err = _winapi.WriteFile(self.connection, data, overlapped=True)
            try:
                if err == _winapi.ERROR_IO_PENDING:
                    timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                    res = _winapi.WaitForSingleObject(ov.event, timeout)
                    if res != _winapi.WAIT_OBJECT_0:
                        raise IPCException(f"Bad result from I/O wait: {res}")
                elif err != 0:
                    raise IPCException(f"Failed writing to pipe with error: {err}")
            except BaseException:
                ov.cancel()
                raise
            bytes_written, err = ov.GetOverlappedResult(True)
            assert err == 0, err
            assert bytes_written == len(data)
        except OSError as e:
            raise IPCException(f"Failed to write with error: {e.winerror}") from e
    else:
        self.connection.sendall(data)
        self.connection.shutdown(socket.SHUT_WR)

</t>
<t tx="ekr.20230831011819.1496">def close(self) -&gt; None:
    if sys.platform == "win32":
        if self.connection != _winapi.NULL:
            _winapi.CloseHandle(self.connection)
    else:
        self.connection.close()


</t>
<t tx="ekr.20230831011819.1497">class IPCClient(IPCBase):
    """The client side of an IPC connection."""

    @others
</t>
<t tx="ekr.20230831011819.1498">def __init__(self, name: str, timeout: float | None) -&gt; None:
    super().__init__(name, timeout)
    if sys.platform == "win32":
        timeout = int(self.timeout * 1000) if self.timeout else _winapi.NMPWAIT_WAIT_FOREVER
        try:
            _winapi.WaitNamedPipe(self.name, timeout)
        except FileNotFoundError as e:
            raise IPCException(f"The NamedPipe at {self.name} was not found.") from e
        except OSError as e:
            if e.winerror == _winapi.ERROR_SEM_TIMEOUT:
                raise IPCException("Timed out waiting for connection.") from e
            else:
                raise
        try:
            self.connection = _winapi.CreateFile(
                self.name,
                _winapi.GENERIC_READ | _winapi.GENERIC_WRITE,
                0,
                _winapi.NULL,
                _winapi.OPEN_EXISTING,
                _winapi.FILE_FLAG_OVERLAPPED,
                _winapi.NULL,
            )
        except OSError as e:
            if e.winerror == _winapi.ERROR_PIPE_BUSY:
                raise IPCException("The connection is busy.") from e
            else:
                raise
        _winapi.SetNamedPipeHandleState(
            self.connection, _winapi.PIPE_READMODE_MESSAGE, None, None
        )
    else:
        self.connection = socket.socket(socket.AF_UNIX)
        self.connection.settimeout(timeout)
        self.connection.connect(name)

</t>
<t tx="ekr.20230831011819.1499">def __enter__(self) -&gt; IPCClient:
    return self

</t>
<t tx="ekr.20230831011819.15">def get_target_type(
    tvar: TypeVarLikeType,
    type: Type,
    callable: CallableType,
    report_incompatible_typevar_value: Callable[[CallableType, Type, str, Context], None],
    context: Context,
    skip_unsatisfied: bool,
) -&gt; Type | None:
    p_type = get_proper_type(type)
    if isinstance(p_type, UninhabitedType) and tvar.has_default():
        return tvar.default
    if isinstance(tvar, ParamSpecType):
        return type
    if isinstance(tvar, TypeVarTupleType):
        return type
    assert isinstance(tvar, TypeVarType)
    values = tvar.values
    if values:
        if isinstance(p_type, AnyType):
            return type
        if isinstance(p_type, TypeVarType) and p_type.values:
            # Allow substituting T1 for T if every allowed value of T1
            # is also a legal value of T.
            if all(any(mypy.subtypes.is_same_type(v, v1) for v in values) for v1 in p_type.values):
                return type
        matching = []
        for value in values:
            if mypy.subtypes.is_subtype(type, value):
                matching.append(value)
        if matching:
            best = matching[0]
            # If there are more than one matching value, we select the narrowest
            for match in matching[1:]:
                if mypy.subtypes.is_subtype(match, best):
                    best = match
            return best
        if skip_unsatisfied:
            return None
        report_incompatible_typevar_value(callable, type, tvar.name, context)
    else:
        upper_bound = tvar.upper_bound
        if not mypy.subtypes.is_subtype(type, upper_bound):
            if skip_unsatisfied:
                return None
            report_incompatible_typevar_value(callable, type, tvar.name, context)
    return type


</t>
<t tx="ekr.20230831011819.150">def exist_added_packages(suppressed: list[str], manager: BuildManager, options: Options) -&gt; bool:
    """Find if there are any newly added packages that were previously suppressed.

    Exclude everything not in build for follow-imports=skip.
    """
    for dep in suppressed:
        if dep in manager.source_set.source_modules:
            # We don't need to add any special logic for this. If a module
            # is added to build, importers will be invalidated by normal mechanism.
            continue
        path = find_module_simple(dep, manager)
        if not path:
            continue
        if options.follow_imports == "skip" and (
            not path.endswith(".pyi") or options.follow_imports_for_stubs
        ):
            continue
        if "__init__.py" in path:
            # It is better to have a bit lenient test, this will only slightly reduce
            # performance, while having a too strict test may affect correctness.
            return True
    return False


</t>
<t tx="ekr.20230831011819.1500">def __exit__(
    self,
    exc_ty: type[BaseException] | None = None,
    exc_val: BaseException | None = None,
    exc_tb: TracebackType | None = None,
) -&gt; None:
    self.close()


</t>
<t tx="ekr.20230831011819.1501">class IPCServer(IPCBase):
    @others
</t>
<t tx="ekr.20230831011819.1502">BUFFER_SIZE: Final = 2**16

def __init__(self, name: str, timeout: float | None = None) -&gt; None:
    if sys.platform == "win32":
        name = r"\\.\pipe\{}-{}.pipe".format(
            name, base64.urlsafe_b64encode(os.urandom(6)).decode()
        )
    else:
        name = f"{name}.sock"
    super().__init__(name, timeout)
    if sys.platform == "win32":
        self.connection = _winapi.CreateNamedPipe(
            self.name,
            _winapi.PIPE_ACCESS_DUPLEX
            | _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE
            | _winapi.FILE_FLAG_OVERLAPPED,
            _winapi.PIPE_READMODE_MESSAGE
            | _winapi.PIPE_TYPE_MESSAGE
            | _winapi.PIPE_WAIT
            | 0x8,  # PIPE_REJECT_REMOTE_CLIENTS
            1,  # one instance
            self.BUFFER_SIZE,
            self.BUFFER_SIZE,
            _winapi.NMPWAIT_WAIT_FOREVER,
            0,  # Use default security descriptor
        )
        if self.connection == -1:  # INVALID_HANDLE_VALUE
            err = _winapi.GetLastError()
            raise IPCException(f"Invalid handle to pipe: {err}")
    else:
        self.sock_directory = tempfile.mkdtemp()
        sockfile = os.path.join(self.sock_directory, self.name)
        self.sock = socket.socket(socket.AF_UNIX)
        self.sock.bind(sockfile)
        self.sock.listen(1)
        if timeout is not None:
            self.sock.settimeout(timeout)

</t>
<t tx="ekr.20230831011819.1503">def __enter__(self) -&gt; IPCServer:
    if sys.platform == "win32":
        # NOTE: It is theoretically possible that this will hang forever if the
        # client never connects, though this can be "solved" by killing the server
        try:
            ov = _winapi.ConnectNamedPipe(self.connection, overlapped=True)
        except OSError as e:
            # Don't raise if the client already exists, or the client already connected
            if e.winerror not in (_winapi.ERROR_PIPE_CONNECTED, _winapi.ERROR_NO_DATA):
                raise
        else:
            try:
                timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                res = _winapi.WaitForSingleObject(ov.event, timeout)
                assert res == _winapi.WAIT_OBJECT_0
            except BaseException:
                ov.cancel()
                _winapi.CloseHandle(self.connection)
                raise
            _, err = ov.GetOverlappedResult(True)
            assert err == 0
    else:
        try:
            self.connection, _ = self.sock.accept()
        except socket.timeout as e:
            raise IPCException("The socket timed out") from e
    return self

</t>
<t tx="ekr.20230831011819.1504">def __exit__(
    self,
    exc_ty: type[BaseException] | None = None,
    exc_val: BaseException | None = None,
    exc_tb: TracebackType | None = None,
) -&gt; None:
    if sys.platform == "win32":
        try:
            # Wait for the client to finish reading the last write before disconnecting
            if not FlushFileBuffers(self.connection):
                raise IPCException(
                    "Failed to flush NamedPipe buffer, maybe the client hung up?"
                )
        finally:
            DisconnectNamedPipe(self.connection)
    else:
        self.close()

</t>
<t tx="ekr.20230831011819.1505">def cleanup(self) -&gt; None:
    if sys.platform == "win32":
        self.close()
    else:
        shutil.rmtree(self.sock_directory)

</t>
<t tx="ekr.20230831011819.1506">@property
def connection_name(self) -&gt; str:
    if sys.platform == "win32":
        return self.name
    else:
        name = self.sock.getsockname()
        assert isinstance(name, str)
        return name
</t>
<t tx="ekr.20230831011819.1507">@path mypy
"""Calculation of the least upper bound types (joins)."""
&lt;&lt; join.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1508">
from __future__ import annotations

from typing import overload

import mypy.typeops
from mypy.maptype import map_instance_to_supertype
from mypy.nodes import CONTRAVARIANT, COVARIANT, INVARIANT
from mypy.state import state
from mypy.subtypes import (
    SubtypeContext,
    find_member,
    is_equivalent,
    is_proper_subtype,
    is_protocol_implementation,
    is_subtype,
)
from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)


</t>
<t tx="ekr.20230831011819.1509">class InstanceJoiner:
    @others
</t>
<t tx="ekr.20230831011819.151">def find_module_simple(id: str, manager: BuildManager) -&gt; str | None:
    """Find a filesystem path for module `id` or `None` if not found."""
    x = find_module_with_reason(id, manager)
    if isinstance(x, ModuleNotFoundReason):
        return None
    return x


</t>
<t tx="ekr.20230831011819.1510">def __init__(self) -&gt; None:
    self.seen_instances: list[tuple[Instance, Instance]] = []

</t>
<t tx="ekr.20230831011819.1511">def join_instances(self, t: Instance, s: Instance) -&gt; ProperType:
    if (t, s) in self.seen_instances or (s, t) in self.seen_instances:
        return object_from_instance(t)

    self.seen_instances.append((t, s))

    # Calculate the join of two instance types
    if t.type == s.type:
        # Simplest case: join two types with the same base type (but
        # potentially different arguments).

        # Combine type arguments.
        args: list[Type] = []
        # N.B: We use zip instead of indexing because the lengths might have
        # mismatches during daemon reprocessing.
        for ta, sa, type_var in zip(t.args, s.args, t.type.defn.type_vars):
            ta_proper = get_proper_type(ta)
            sa_proper = get_proper_type(sa)
            new_type: Type | None = None
            if isinstance(ta_proper, AnyType):
                new_type = AnyType(TypeOfAny.from_another_any, ta_proper)
            elif isinstance(sa_proper, AnyType):
                new_type = AnyType(TypeOfAny.from_another_any, sa_proper)
            elif isinstance(type_var, TypeVarType):
                if type_var.variance == COVARIANT:
                    new_type = join_types(ta, sa, self)
                    if len(type_var.values) != 0 and new_type not in type_var.values:
                        self.seen_instances.pop()
                        return object_from_instance(t)
                    if not is_subtype(new_type, type_var.upper_bound):
                        self.seen_instances.pop()
                        return object_from_instance(t)
                # TODO: contravariant case should use meet but pass seen instances as
                # an argument to keep track of recursive checks.
                elif type_var.variance in (INVARIANT, CONTRAVARIANT):
                    if not is_equivalent(ta, sa):
                        self.seen_instances.pop()
                        return object_from_instance(t)
                    # If the types are different but equivalent, then an Any is involved
                    # so using a join in the contravariant case is also OK.
                    new_type = join_types(ta, sa, self)
            else:
                # ParamSpec type variables behave the same, independent of variance
                if not is_equivalent(ta, sa):
                    return get_proper_type(type_var.upper_bound)
                new_type = join_types(ta, sa, self)
            assert new_type is not None
            args.append(new_type)
        result: ProperType = Instance(t.type, args)
    elif t.type.bases and is_proper_subtype(
        t, s, subtype_context=SubtypeContext(ignore_type_params=True)
    ):
        result = self.join_instances_via_supertype(t, s)
    else:
        # Now t is not a subtype of s, and t != s. Now s could be a subtype
        # of t; alternatively, we need to find a common supertype. This works
        # in of the both cases.
        result = self.join_instances_via_supertype(s, t)

    self.seen_instances.pop()
    return result

</t>
<t tx="ekr.20230831011819.1512">def join_instances_via_supertype(self, t: Instance, s: Instance) -&gt; ProperType:
    # Give preference to joins via duck typing relationship, so that
    # join(int, float) == float, for example.
    for p in t.type._promote:
        if is_subtype(p, s):
            return join_types(p, s, self)
    for p in s.type._promote:
        if is_subtype(p, t):
            return join_types(t, p, self)

    # Compute the "best" supertype of t when joined with s.
    # The definition of "best" may evolve; for now it is the one with
    # the longest MRO.  Ties are broken by using the earlier base.
    best: ProperType | None = None
    for base in t.type.bases:
        mapped = map_instance_to_supertype(t, base.type)
        res = self.join_instances(mapped, s)
        if best is None or is_better(res, best):
            best = res
    assert best is not None
    for promote in t.type._promote:
        if isinstance(promote, Instance):
            res = self.join_instances(promote, s)
            if is_better(res, best):
                best = res
    return best


</t>
<t tx="ekr.20230831011819.1513">def join_simple(declaration: Type | None, s: Type, t: Type) -&gt; ProperType:
    """Return a simple least upper bound given the declared type.

    This function should be only used by binder, and should not recurse.
    For all other uses, use `join_types()`.
    """
    declaration = get_proper_type(declaration)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if (s.can_be_true, s.can_be_false) != (t.can_be_true, t.can_be_false):
        # if types are restricted in different ways, use the more general versions
        s = mypy.typeops.true_or_false(s)
        t = mypy.typeops.true_or_false(t)

    if isinstance(s, AnyType):
        return s

    if isinstance(s, ErasedType):
        return t

    if is_proper_subtype(s, t, ignore_promotions=True):
        return t

    if is_proper_subtype(t, s, ignore_promotions=True):
        return s

    if isinstance(declaration, UnionType):
        return mypy.typeops.make_simplified_union([s, t])

    if isinstance(s, NoneType) and not isinstance(t, NoneType):
        s, t = t, s

    if isinstance(s, UninhabitedType) and not isinstance(t, UninhabitedType):
        s, t = t, s

    # Meets/joins require callable type normalization.
    s, t = normalize_callables(s, t)

    if isinstance(s, UnionType) and not isinstance(t, UnionType):
        s, t = t, s

    value = t.accept(TypeJoinVisitor(s))
    if declaration is None or is_subtype(value, declaration):
        return value

    return declaration


</t>
<t tx="ekr.20230831011819.1514">def trivial_join(s: Type, t: Type) -&gt; Type:
    """Return one of types (expanded) if it is a supertype of other, otherwise top type."""
    if is_subtype(s, t):
        return t
    elif is_subtype(t, s):
        return s
    else:
        return object_or_any_from_type(get_proper_type(t))


</t>
<t tx="ekr.20230831011819.1515">@overload
def join_types(
    s: ProperType, t: ProperType, instance_joiner: InstanceJoiner | None = None
) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20230831011819.1516">@overload
def join_types(s: Type, t: Type, instance_joiner: InstanceJoiner | None = None) -&gt; Type:
    ...


</t>
<t tx="ekr.20230831011819.1517">def join_types(s: Type, t: Type, instance_joiner: InstanceJoiner | None = None) -&gt; Type:
    """Return the least upper bound of s and t.

    For example, the join of 'int' and 'object' is 'object'.
    """
    if mypy.typeops.is_recursive_pair(s, t):
        # This case can trigger an infinite recursion, general support for this will be
        # tricky so we use a trivial join (like for protocols).
        return trivial_join(s, t)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if (s.can_be_true, s.can_be_false) != (t.can_be_true, t.can_be_false):
        # if types are restricted in different ways, use the more general versions
        s = mypy.typeops.true_or_false(s)
        t = mypy.typeops.true_or_false(t)

    if isinstance(s, UnionType) and not isinstance(t, UnionType):
        s, t = t, s

    if isinstance(s, AnyType):
        return s

    if isinstance(s, ErasedType):
        return t

    if isinstance(s, NoneType) and not isinstance(t, NoneType):
        s, t = t, s

    if isinstance(s, UninhabitedType) and not isinstance(t, UninhabitedType):
        s, t = t, s

    # Meets/joins require callable type normalization.
    s, t = normalize_callables(s, t)

    # Use a visitor to handle non-trivial cases.
    return t.accept(TypeJoinVisitor(s, instance_joiner))


</t>
<t tx="ekr.20230831011819.1518">class TypeJoinVisitor(TypeVisitor[ProperType]):
    """Implementation of the least upper bound algorithm.

    Attributes:
      s: The other (left) type operand.
    """

    @others
</t>
<t tx="ekr.20230831011819.1519">def __init__(self, s: ProperType, instance_joiner: InstanceJoiner | None = None) -&gt; None:
    self.s = s
    self.instance_joiner = instance_joiner

</t>
<t tx="ekr.20230831011819.152">def find_module_with_reason(id: str, manager: BuildManager) -&gt; ModuleSearchResult:
    """Find a filesystem path for module `id` or the reason it can't be found."""
    t0 = time.time()
    x = manager.find_module_cache.find_module(id)
    manager.add_stats(find_module_time=time.time() - t0, find_module_calls=1)
    return x


</t>
<t tx="ekr.20230831011819.1520">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.1521">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    if is_proper_subtype(self.s, t):
        return t
    else:
        return mypy.typeops.make_simplified_union([self.s, t])

</t>
<t tx="ekr.20230831011819.1522">def visit_any(self, t: AnyType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011819.1523">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    if state.strict_optional:
        if isinstance(self.s, (NoneType, UninhabitedType)):
            return t
        elif isinstance(self.s, UnboundType):
            return AnyType(TypeOfAny.special_form)
        else:
            return mypy.typeops.make_simplified_union([self.s, t])
    else:
        return self.s

</t>
<t tx="ekr.20230831011819.1524">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20230831011819.1525">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20230831011819.1526">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20230831011819.1527">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    if isinstance(self.s, TypeVarType) and self.s.id == t.id:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011819.1528">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    if self.s == t:
        return t
    return self.default(self.s)

</t>
<t tx="ekr.20230831011819.1529">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    if self.s == t:
        return t
    return self.default(self.s)

</t>
<t tx="ekr.20230831011819.153">def in_partial_package(id: str, manager: BuildManager) -&gt; bool:
    """Check if a missing module can potentially be a part of a package.

    This checks if there is any existing parent __init__.pyi stub that
    defines a module-level __getattr__ (a.k.a. partial stub package).
    """
    while "." in id:
        parent, _ = id.rsplit(".", 1)
        if parent in manager.modules:
            parent_mod: MypyFile | None = manager.modules[parent]
        else:
            # Parent is not in build, try quickly if we can find it.
            try:
                parent_st = State(
                    id=parent, path=None, source=None, manager=manager, temporary=True
                )
            except (ModuleNotFound, CompileError):
                parent_mod = None
            else:
                parent_mod = parent_st.tree
        if parent_mod is not None:
            # Bail out soon, complete subpackage found
            return parent_mod.is_partial_stub_package
        id = parent
    return False


</t>
<t tx="ekr.20230831011819.1530">def visit_unpack_type(self, t: UnpackType) -&gt; UnpackType:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011819.1531">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    if isinstance(self.s, Parameters):
        if len(t.arg_types) != len(self.s.arg_types):
            return self.default(self.s)
        return t.copy_modified(
            # Note that since during constraint inference we already treat whole ParamSpec as
            # contravariant, we should join individual items, not meet them like for Callables
            arg_types=[join_types(s_a, t_a) for s_a, t_a in zip(self.s.arg_types, t.arg_types)]
        )
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011819.1532">def visit_instance(self, t: Instance) -&gt; ProperType:
    if isinstance(self.s, Instance):
        if self.instance_joiner is None:
            self.instance_joiner = InstanceJoiner()
        nominal = self.instance_joiner.join_instances(t, self.s)
        structural: Instance | None = None
        if t.type.is_protocol and is_protocol_implementation(self.s, t):
            structural = t
        elif self.s.type.is_protocol and is_protocol_implementation(t, self.s):
            structural = self.s
        # Structural join is preferred in the case where we have found both
        # structural and nominal and they have same MRO length (see two comments
        # in join_instances_via_supertype). Otherwise, just return the nominal join.
        if not structural or is_better(nominal, structural):
            return nominal
        return structural
    elif isinstance(self.s, FunctionLike):
        if t.type.is_protocol:
            call = unpack_callback_protocol(t)
            if call:
                return join_types(call, self.s)
        return join_types(t, self.s.fallback)
    elif isinstance(self.s, TypeType):
        return join_types(t, self.s)
    elif isinstance(self.s, TypedDictType):
        return join_types(t, self.s)
    elif isinstance(self.s, TupleType):
        return join_types(t, self.s)
    elif isinstance(self.s, LiteralType):
        return join_types(t, self.s)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011819.1533">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    if isinstance(self.s, CallableType) and is_similar_callables(t, self.s):
        if is_equivalent(t, self.s):
            return combine_similar_callables(t, self.s)
        result = join_similar_callables(t, self.s)
        # We set the from_type_type flag to suppress error when a collection of
        # concrete class objects gets inferred as their common abstract superclass.
        if not (
            (t.is_type_obj() and t.type_object().is_abstract)
            or (self.s.is_type_obj() and self.s.type_object().is_abstract)
        ):
            result.from_type_type = True
        if any(
            isinstance(tp, (NoneType, UninhabitedType))
            for tp in get_proper_types(result.arg_types)
        ):
            # We don't want to return unusable Callable, attempt fallback instead.
            return join_types(t.fallback, self.s)
        return result
    elif isinstance(self.s, Overloaded):
        # Switch the order of arguments to that we'll get to visit_overloaded.
        return join_types(t, self.s)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = unpack_callback_protocol(self.s)
        if call:
            return join_types(t, call)
    return join_types(t.fallback, self.s)

</t>
<t tx="ekr.20230831011819.1534">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    # This is more complex than most other cases. Here are some
    # examples that illustrate how this works.
    #
    # First let's define a concise notation:
    #  - Cn are callable types (for n in 1, 2, ...)
    #  - Ov(C1, C2, ...) is an overloaded type with items C1, C2, ...
    #  - Callable[[T, ...], S] is written as [T, ...] -&gt; S.
    #
    # We want some basic properties to hold (assume Cn are all
    # unrelated via Any-similarity):
    #
    #   join(Ov(C1, C2), C1) == C1
    #   join(Ov(C1, C2), Ov(C1, C2)) == Ov(C1, C2)
    #   join(Ov(C1, C2), Ov(C1, C3)) == C1
    #   join(Ov(C2, C2), C3) == join of fallback types
    #
    # The presence of Any types makes things more interesting. The join is the
    # most general type we can get with respect to Any:
    #
    #   join(Ov([int] -&gt; int, [str] -&gt; str), [Any] -&gt; str) == Any -&gt; str
    #
    # We could use a simplification step that removes redundancies, but that's not
    # implemented right now. Consider this example, where we get a redundancy:
    #
    #   join(Ov([int, Any] -&gt; Any, [str, Any] -&gt; Any), [Any, int] -&gt; Any) ==
    #       Ov([Any, int] -&gt; Any, [Any, int] -&gt; Any)
    #
    # TODO: Consider more cases of callable subtyping.
    result: list[CallableType] = []
    s = self.s
    if isinstance(s, FunctionLike):
        # The interesting case where both types are function types.
        for t_item in t.items:
            for s_item in s.items:
                if is_similar_callables(t_item, s_item):
                    if is_equivalent(t_item, s_item):
                        result.append(combine_similar_callables(t_item, s_item))
                    elif is_subtype(t_item, s_item):
                        result.append(s_item)
        if result:
            # TODO: Simplify redundancies from the result.
            if len(result) == 1:
                return result[0]
            else:
                return Overloaded(result)
        return join_types(t.fallback, s.fallback)
    elif isinstance(s, Instance) and s.type.is_protocol:
        call = unpack_callback_protocol(s)
        if call:
            return join_types(t, call)
    return join_types(t.fallback, s)

</t>
<t tx="ekr.20230831011819.1535">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    # When given two fixed-length tuples:
    # * If they have the same length, join their subtypes item-wise:
    #   Tuple[int, bool] + Tuple[bool, bool] becomes Tuple[int, bool]
    # * If lengths do not match, return a variadic tuple:
    #   Tuple[bool, int] + Tuple[bool] becomes Tuple[int, ...]
    #
    # Otherwise, `t` is a fixed-length tuple but `self.s` is NOT:
    # * Joining with a variadic tuple returns variadic tuple:
    #   Tuple[int, bool] + Tuple[bool, ...] becomes Tuple[int, ...]
    # * Joining with any Sequence also returns a Sequence:
    #   Tuple[int, bool] + List[bool] becomes Sequence[int]
    if isinstance(self.s, TupleType) and self.s.length() == t.length():
        if self.instance_joiner is None:
            self.instance_joiner = InstanceJoiner()
        fallback = self.instance_joiner.join_instances(
            mypy.typeops.tuple_fallback(self.s), mypy.typeops.tuple_fallback(t)
        )
        assert isinstance(fallback, Instance)
        if self.s.length() == t.length():
            items: list[Type] = []
            for i in range(t.length()):
                items.append(join_types(t.items[i], self.s.items[i]))
            return TupleType(items, fallback)
        else:
            return fallback
    else:
        return join_types(self.s, mypy.typeops.tuple_fallback(t))

</t>
<t tx="ekr.20230831011819.1536">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    if isinstance(self.s, TypedDictType):
        items = {
            item_name: s_item_type
            for (item_name, s_item_type, t_item_type) in self.s.zip(t)
            if (
                is_equivalent(s_item_type, t_item_type)
                and (item_name in t.required_keys) == (item_name in self.s.required_keys)
            )
        }
        fallback = self.s.create_anonymous_fallback()
        # We need to filter by items.keys() since some required keys present in both t and
        # self.s might be missing from the join if the types are incompatible.
        required_keys = set(items.keys()) &amp; t.required_keys &amp; self.s.required_keys
        return TypedDictType(items, required_keys, fallback)
    elif isinstance(self.s, Instance):
        return join_types(self.s, t.fallback)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011819.1537">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    if isinstance(self.s, LiteralType):
        if t == self.s:
            return t
        if self.s.fallback.type.is_enum and t.fallback.type.is_enum:
            return mypy.typeops.make_simplified_union([self.s, t])
        return join_types(self.s.fallback, t.fallback)
    else:
        return join_types(self.s, t.fallback)

</t>
<t tx="ekr.20230831011819.1538">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # We only have partial information so we can't decide the join result. We should
    # never get here.
    assert False, "Internal error"

</t>
<t tx="ekr.20230831011819.1539">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    if isinstance(self.s, TypeType):
        return TypeType.make_normalized(join_types(t.item, self.s.item), line=t.line)
    elif isinstance(self.s, Instance) and self.s.type.fullname == "builtins.type":
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011819.154">def module_not_found(
    manager: BuildManager,
    line: int,
    caller_state: State,
    target: str,
    reason: ModuleNotFoundReason,
) -&gt; None:
    errors = manager.errors
    save_import_context = errors.import_context()
    errors.set_import_context(caller_state.import_context)
    errors.set_file(caller_state.xpath, caller_state.id, caller_state.options)
    if target == "builtins":
        errors.report(
            line, 0, "Cannot find 'builtins' module. Typeshed appears broken!", blocker=True
        )
        errors.raise_error()
    else:
        daemon = manager.options.fine_grained_incremental
        msg, notes = reason.error_message_templates(daemon)
        if reason == ModuleNotFoundReason.NOT_FOUND:
            code = codes.IMPORT_NOT_FOUND
        elif (
            reason == ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS
            or reason == ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
        ):
            code = codes.IMPORT_UNTYPED
        else:
            code = codes.IMPORT
        errors.report(line, 0, msg.format(module=target), code=code)

        components = target.split(".")
        for i in range(len(components), 0, -1):
            module = ".".join(components[:i])
            if module in legacy_bundled_packages or module in non_bundled_packages:
                break

        for note in notes:
            if "{stub_dist}" in note:
                note = note.format(stub_dist=stub_distribution_name(module))
            errors.report(line, 0, note, severity="note", only_once=True, code=codes.IMPORT)
        if reason is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
            manager.missing_stub_packages.add(stub_distribution_name(module))
    errors.set_import_context(save_import_context)


</t>
<t tx="ekr.20230831011819.1540">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, f"This should be never called, got {t}"

</t>
<t tx="ekr.20230831011819.1541">def default(self, typ: Type) -&gt; ProperType:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return object_from_instance(typ)
    elif isinstance(typ, UnboundType):
        return AnyType(TypeOfAny.special_form)
    elif isinstance(typ, TupleType):
        return self.default(mypy.typeops.tuple_fallback(typ))
    elif isinstance(typ, TypedDictType):
        return self.default(typ.fallback)
    elif isinstance(typ, FunctionLike):
        return self.default(typ.fallback)
    elif isinstance(typ, TypeVarType):
        return self.default(typ.upper_bound)
    elif isinstance(typ, ParamSpecType):
        return self.default(typ.upper_bound)
    else:
        return AnyType(TypeOfAny.special_form)


</t>
<t tx="ekr.20230831011819.1542">def is_better(t: Type, s: Type) -&gt; bool:
    # Given two possible results from join_instances_via_supertype(),
    # indicate whether t is the better one.
    t = get_proper_type(t)
    s = get_proper_type(s)

    if isinstance(t, Instance):
        if not isinstance(s, Instance):
            return True
        # Use len(mro) as a proxy for the better choice.
        if len(t.type.mro) &gt; len(s.type.mro):
            return True
    return False


</t>
<t tx="ekr.20230831011819.1543">def normalize_callables(s: ProperType, t: ProperType) -&gt; tuple[ProperType, ProperType]:
    if isinstance(s, (CallableType, Overloaded)):
        s = s.with_unpacked_kwargs()
    if isinstance(t, (CallableType, Overloaded)):
        t = t.with_unpacked_kwargs()
    return s, t


</t>
<t tx="ekr.20230831011819.1544">def is_similar_callables(t: CallableType, s: CallableType) -&gt; bool:
    """Return True if t and s have identical numbers of
    arguments, default arguments and varargs.
    """
    return (
        len(t.arg_types) == len(s.arg_types)
        and t.min_args == s.min_args
        and t.is_var_arg == s.is_var_arg
    )


</t>
<t tx="ekr.20230831011819.1545">def join_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    from mypy.meet import meet_types

    arg_types: list[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(meet_types(t.arg_types[i], s.arg_types[i]))
    # TODO in combine_similar_callables also applies here (names and kinds; user metaclasses)
    # The fallback type can be either 'function', 'type', or some user-provided metaclass.
    # The result should always use 'function' as a fallback if either operands are using it.
    if t.fallback.type.fullname == "builtins.function":
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(
        arg_types=arg_types,
        arg_names=combine_arg_names(t, s),
        ret_type=join_types(t.ret_type, s.ret_type),
        fallback=fallback,
        name=None,
    )


</t>
<t tx="ekr.20230831011819.1546">def combine_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    arg_types: list[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(join_types(t.arg_types[i], s.arg_types[i]))
    # TODO kinds and argument names
    # TODO what should happen if one fallback is 'type' and the other is a user-provided metaclass?
    # The fallback type can be either 'function', 'type', or some user-provided metaclass.
    # The result should always use 'function' as a fallback if either operands are using it.
    if t.fallback.type.fullname == "builtins.function":
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(
        arg_types=arg_types,
        arg_names=combine_arg_names(t, s),
        ret_type=join_types(t.ret_type, s.ret_type),
        fallback=fallback,
        name=None,
    )


</t>
<t tx="ekr.20230831011819.1547">def combine_arg_names(t: CallableType, s: CallableType) -&gt; list[str | None]:
    """Produces a list of argument names compatible with both callables.

    For example, suppose 't' and 's' have the following signatures:

    - t: (a: int, b: str, X: str) -&gt; None
    - s: (a: int, b: str, Y: str) -&gt; None

    This function would return ["a", "b", None]. This information
    is then used above to compute the join of t and s, which results
    in a signature of (a: int, b: str, str) -&gt; None.

    Note that the third argument's name is omitted and 't' and 's'
    are both valid subtypes of this inferred signature.

    Precondition: is_similar_types(t, s) is true.
    """
    num_args = len(t.arg_types)
    new_names = []
    for i in range(num_args):
        t_name = t.arg_names[i]
        s_name = s.arg_names[i]
        if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
            new_names.append(t_name)
        else:
            new_names.append(None)
    return new_names


</t>
<t tx="ekr.20230831011819.1548">def object_from_instance(instance: Instance) -&gt; Instance:
    """Construct the type 'builtins.object' from an instance type."""
    # Use the fact that 'object' is always the last class in the mro.
    res = Instance(instance.type.mro[-1], [])
    return res


</t>
<t tx="ekr.20230831011819.1549">def object_or_any_from_type(typ: ProperType) -&gt; ProperType:
    # Similar to object_from_instance() but tries hard for all types.
    # TODO: find a better way to get object, or make this more reliable.
    if isinstance(typ, Instance):
        return object_from_instance(typ)
    elif isinstance(typ, (CallableType, TypedDictType, LiteralType)):
        return object_from_instance(typ.fallback)
    elif isinstance(typ, TupleType):
        return object_from_instance(typ.partial_fallback)
    elif isinstance(typ, TypeType):
        return object_or_any_from_type(typ.item)
    elif isinstance(typ, TypeVarType) and isinstance(typ.upper_bound, ProperType):
        return object_or_any_from_type(typ.upper_bound)
    elif isinstance(typ, UnionType):
        for item in typ.items:
            if isinstance(item, ProperType):
                candidate = object_or_any_from_type(item)
                if isinstance(candidate, Instance):
                    return candidate
    return AnyType(TypeOfAny.implementation_artifact)


</t>
<t tx="ekr.20230831011819.155">def skipping_module(
    manager: BuildManager, line: int, caller_state: State | None, id: str, path: str
) -&gt; None:
    """Produce an error for an import ignored due to --follow_imports=error"""
    assert caller_state, (id, path)
    save_import_context = manager.errors.import_context()
    manager.errors.set_import_context(caller_state.import_context)
    manager.errors.set_file(caller_state.xpath, caller_state.id, manager.options)
    manager.errors.report(line, 0, f'Import of "{id}" ignored', severity="error")
    manager.errors.report(
        line,
        0,
        "(Using --follow-imports=error, module not passed on command line)",
        severity="note",
        only_once=True,
    )
    manager.errors.set_import_context(save_import_context)


</t>
<t tx="ekr.20230831011819.1550">def join_type_list(types: list[Type]) -&gt; Type:
    if not types:
        # This is a little arbitrary but reasonable. Any empty tuple should be compatible
        # with all variable length tuples, and this makes it possible.
        return UninhabitedType()
    joined = types[0]
    for t in types[1:]:
        joined = join_types(joined, t)
    return joined


</t>
<t tx="ekr.20230831011819.1551">def unpack_callback_protocol(t: Instance) -&gt; ProperType | None:
    assert t.type.is_protocol
    if t.type.protocol_members == ["__call__"]:
        return get_proper_type(find_member("__call__", t, t, is_operator=True))
    return None
</t>
<t tx="ekr.20230831011819.1552">@path mypy
&lt;&lt; literals.py: preamble &gt;&gt;
@others


_hasher: Final = _Hasher()
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1553">from __future__ import annotations

from typing import Any, Final, Iterable, Optional, Tuple
from typing_extensions import TypeAlias as _TypeAlias

from mypy.nodes import (
    LITERAL_NO,
    LITERAL_TYPE,
    LITERAL_YES,
    AssertTypeExpr,
    AssignmentExpr,
    AwaitExpr,
    BytesExpr,
    CallExpr,
    CastExpr,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    FloatExpr,
    GeneratorExpr,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MemberExpr,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    OpExpr,
    ParamSpecExpr,
    PromoteExpr,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    StrExpr,
    SuperExpr,
    TempNode,
    TupleExpr,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    YieldExpr,
    YieldFromExpr,
)
from mypy.visitor import ExpressionVisitor

# [Note Literals and literal_hash]
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Mypy uses the term "literal" to refer to any expression built out of
# the following:
#
# * Plain literal expressions, like `1` (integer, float, string, etc.)
#
# * Compound literal expressions, like `(lit1, lit2)` (list, dict,
#   set, or tuple)
#
# * Operator expressions, like `lit1 + lit2`
#
# * Variable references, like `x`
#
# * Member references, like `lit.m`
#
# * Index expressions, like `lit[0]`
#
# A typical "literal" looks like `x[(i,j+1)].m`.
#
# An expression that is a literal has a `literal_hash`, with the
# following properties.
#
# * `literal_hash` is a Key: a tuple containing basic data types and
#   possibly other Keys. So it can be used as a key in a dictionary
#   that will be compared by value (as opposed to the Node itself,
#   which is compared by identity).
#
# * Two expressions have equal `literal_hash`es if and only if they
#   are syntactically equal expressions. (NB: Actually, we also
#   identify as equal expressions like `3` and `3.0`; is this a good
#   idea?)
#
# * The elements of `literal_hash` that are tuples are exactly the
#   subexpressions of the original expression (e.g. the base and index
#   of an index expression, or the operands of an operator expression).


</t>
<t tx="ekr.20230831011819.1554">def literal(e: Expression) -&gt; int:
    if isinstance(e, ComparisonExpr):
        return min(literal(o) for o in e.operands)

    elif isinstance(e, OpExpr):
        return min(literal(e.left), literal(e.right))

    elif isinstance(e, (MemberExpr, UnaryExpr, StarExpr)):
        return literal(e.expr)

    elif isinstance(e, AssignmentExpr):
        return literal(e.target)

    elif isinstance(e, IndexExpr):
        if literal(e.index) == LITERAL_YES:
            return literal(e.base)
        else:
            return LITERAL_NO

    elif isinstance(e, NameExpr):
        if isinstance(e.node, Var) and e.node.is_final and e.node.final_value is not None:
            return LITERAL_YES
        return LITERAL_TYPE

    if isinstance(e, (IntExpr, FloatExpr, ComplexExpr, StrExpr, BytesExpr)):
        return LITERAL_YES

    if literal_hash(e):
        return LITERAL_YES

    return LITERAL_NO


</t>
<t tx="ekr.20230831011819.1555">Key: _TypeAlias = Tuple[Any, ...]


def subkeys(key: Key) -&gt; Iterable[Key]:
    return [elt for elt in key if isinstance(elt, tuple)]


</t>
<t tx="ekr.20230831011819.1556">def literal_hash(e: Expression) -&gt; Key | None:
    return e.accept(_hasher)


</t>
<t tx="ekr.20230831011819.1557">def extract_var_from_literal_hash(key: Key) -&gt; Var | None:
    """If key refers to a Var node, return it.

    Return None otherwise.
    """
    if len(key) == 2 and key[0] == "Var" and isinstance(key[1], Var):
        return key[1]
    return None


</t>
<t tx="ekr.20230831011819.1558">class _Hasher(ExpressionVisitor[Optional[Key]]):
    @others
</t>
<t tx="ekr.20230831011819.1559">def visit_int_expr(self, e: IntExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20230831011819.156">def skipping_ancestor(manager: BuildManager, id: str, path: str, ancestor_for: State) -&gt; None:
    """Produce an error for an ancestor ignored due to --follow_imports=error"""
    # TODO: Read the path (the __init__.py file) and return
    # immediately if it's empty or only contains comments.
    # But beware, some package may be the ancestor of many modules,
    # so we'd need to cache the decision.
    manager.errors.set_import_context([])
    manager.errors.set_file(ancestor_for.xpath, ancestor_for.id, manager.options)
    manager.errors.report(
        -1, -1, f'Ancestor package "{id}" ignored', severity="error", only_once=True
    )
    manager.errors.report(
        -1,
        -1,
        "(Using --follow-imports=error, submodule passed on command line)",
        severity="note",
        only_once=True,
    )


</t>
<t tx="ekr.20230831011819.1560">def visit_str_expr(self, e: StrExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20230831011819.1561">def visit_bytes_expr(self, e: BytesExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20230831011819.1562">def visit_float_expr(self, e: FloatExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20230831011819.1563">def visit_complex_expr(self, e: ComplexExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20230831011819.1564">def visit_star_expr(self, e: StarExpr) -&gt; Key:
    return ("Star", literal_hash(e.expr))

</t>
<t tx="ekr.20230831011819.1565">def visit_name_expr(self, e: NameExpr) -&gt; Key:
    if isinstance(e.node, Var) and e.node.is_final and e.node.final_value is not None:
        return ("Literal", e.node.final_value)
    # N.B: We use the node itself as the key, and not the name,
    # because using the name causes issues when there is shadowing
    # (for example, in list comprehensions).
    return ("Var", e.node)

</t>
<t tx="ekr.20230831011819.1566">def visit_member_expr(self, e: MemberExpr) -&gt; Key:
    return ("Member", literal_hash(e.expr), e.name)

</t>
<t tx="ekr.20230831011819.1567">def visit_op_expr(self, e: OpExpr) -&gt; Key:
    return ("Binary", e.op, literal_hash(e.left), literal_hash(e.right))

</t>
<t tx="ekr.20230831011819.1568">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; Key:
    rest: tuple[str | Key | None, ...] = tuple(e.operators)
    rest += tuple(literal_hash(o) for o in e.operands)
    return ("Comparison",) + rest

</t>
<t tx="ekr.20230831011819.1569">def visit_unary_expr(self, e: UnaryExpr) -&gt; Key:
    return ("Unary", e.op, literal_hash(e.expr))

</t>
<t tx="ekr.20230831011819.157">def log_configuration(manager: BuildManager, sources: list[BuildSource]) -&gt; None:
    """Output useful configuration information to LOG and TRACE"""

    manager.log()
    configuration_vars = [
        ("Mypy Version", __version__),
        ("Config File", (manager.options.config_file or "Default")),
        ("Configured Executable", manager.options.python_executable or "None"),
        ("Current Executable", sys.executable),
        ("Cache Dir", manager.options.cache_dir),
        ("Compiled", str(not __file__.endswith(".py"))),
        ("Exclude", manager.options.exclude),
    ]

    for conf_name, conf_value in configuration_vars:
        manager.log(f"{conf_name + ':':24}{conf_value}")

    for source in sources:
        manager.log(f"{'Found source:':24}{source}")

    # Complete list of searched paths can get very long, put them under TRACE
    for path_type, paths in manager.search_paths._asdict().items():
        if not paths:
            manager.trace(f"No {path_type}")
            continue

        manager.trace(f"{path_type}:")

        for pth in paths:
            manager.trace(f"    {pth}")


</t>
<t tx="ekr.20230831011819.1570">def seq_expr(self, e: ListExpr | TupleExpr | SetExpr, name: str) -&gt; Key | None:
    if all(literal(x) == LITERAL_YES for x in e.items):
        rest: tuple[Key | None, ...] = tuple(literal_hash(x) for x in e.items)
        return (name,) + rest
    return None

</t>
<t tx="ekr.20230831011819.1571">def visit_list_expr(self, e: ListExpr) -&gt; Key | None:
    return self.seq_expr(e, "List")

</t>
<t tx="ekr.20230831011819.1572">def visit_dict_expr(self, e: DictExpr) -&gt; Key | None:
    if all(a and literal(a) == literal(b) == LITERAL_YES for a, b in e.items):
        rest: tuple[Key | None, ...] = tuple(
            (literal_hash(a) if a else None, literal_hash(b)) for a, b in e.items
        )
        return ("Dict",) + rest
    return None

</t>
<t tx="ekr.20230831011819.1573">def visit_tuple_expr(self, e: TupleExpr) -&gt; Key | None:
    return self.seq_expr(e, "Tuple")

</t>
<t tx="ekr.20230831011819.1574">def visit_set_expr(self, e: SetExpr) -&gt; Key | None:
    return self.seq_expr(e, "Set")

</t>
<t tx="ekr.20230831011819.1575">def visit_index_expr(self, e: IndexExpr) -&gt; Key | None:
    if literal(e.index) == LITERAL_YES:
        return ("Index", literal_hash(e.base), literal_hash(e.index))
    return None

</t>
<t tx="ekr.20230831011819.1576">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; Key | None:
    return literal_hash(e.target)

</t>
<t tx="ekr.20230831011819.1577">def visit_call_expr(self, e: CallExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1578">def visit_slice_expr(self, e: SliceExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1579">def visit_cast_expr(self, e: CastExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.158"># The driver


def dispatch(sources: list[BuildSource], manager: BuildManager, stdout: TextIO) -&gt; Graph:
    log_configuration(manager, sources)

    t0 = time.time()
    graph = load_graph(sources, manager)

    # This is a kind of unfortunate hack to work around some of fine-grained's
    # fragility: if we have loaded less than 50% of the specified files from
    # cache in fine-grained cache mode, load the graph again honestly.
    # In this case, we just turn the cache off entirely, so we don't need
    # to worry about some files being loaded and some from cache and so
    # that fine-grained mode never *writes* to the cache.
    if manager.use_fine_grained_cache() and len(graph) &lt; 0.50 * len(sources):
        manager.log("Redoing load_graph without cache because too much was missing")
        manager.cache_enabled = False
        graph = load_graph(sources, manager)

    t1 = time.time()
    manager.add_stats(
        graph_size=len(graph),
        stubs_found=sum(g.path is not None and g.path.endswith(".pyi") for g in graph.values()),
        graph_load_time=(t1 - t0),
        fm_cache_size=len(manager.find_module_cache.results),
    )
    if not graph:
        print("Nothing to do?!", file=stdout)
        return graph
    manager.log(f"Loaded graph with {len(graph)} nodes ({t1 - t0:.3f} sec)")
    if manager.options.dump_graph:
        dump_graph(graph, stdout)
        return graph

    # Fine grained dependencies that didn't have an associated module in the build
    # are serialized separately, so we read them after we load the graph.
    # We need to read them both for running in daemon mode and if we are generating
    # a fine-grained cache (so that we can properly update them incrementally).
    # The `read_deps_cache` will also validate
    # the deps cache against the loaded individual cache files.
    if manager.options.cache_fine_grained or manager.use_fine_grained_cache():
        t2 = time.time()
        fg_deps_meta = read_deps_cache(manager, graph)
        manager.add_stats(load_fg_deps_time=time.time() - t2)
        if fg_deps_meta is not None:
            manager.fg_deps_meta = fg_deps_meta
        elif manager.stats.get("fresh_metas", 0) &gt; 0:
            # Clear the stats so we don't infinite loop because of positive fresh_metas
            manager.stats.clear()
            # There were some cache files read, but no fine-grained dependencies loaded.
            manager.log("Error reading fine-grained dependencies cache -- aborting cache load")
            manager.cache_enabled = False
            manager.log("Falling back to full run -- reloading graph...")
            return dispatch(sources, manager, stdout)

    # If we are loading a fine-grained incremental mode cache, we
    # don't want to do a real incremental reprocess of the
    # graph---we'll handle it all later.
    if not manager.use_fine_grained_cache():
        process_graph(graph, manager)
        # Update plugins snapshot.
        write_plugins_snapshot(manager)
        manager.old_plugins_snapshot = manager.plugins_snapshot
        if manager.options.cache_fine_grained or manager.options.fine_grained_incremental:
            # If we are running a daemon or are going to write cache for further fine grained use,
            # then we need to collect fine grained protocol dependencies.
            # Since these are a global property of the program, they are calculated after we
            # processed the whole graph.
            type_state.add_all_protocol_deps(manager.fg_deps)
            if not manager.options.fine_grained_incremental:
                rdeps = generate_deps_for_cache(manager, graph)
                write_deps_cache(rdeps, manager, graph)

    if manager.options.dump_deps:
        # This speeds up startup a little when not using the daemon mode.
        from mypy.server.deps import dump_all_dependencies

        dump_all_dependencies(
            manager.modules, manager.all_types, manager.options.python_version, manager.options
        )

    return graph


</t>
<t tx="ekr.20230831011819.1580">def visit_assert_type_expr(self, e: AssertTypeExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1581">def visit_conditional_expr(self, e: ConditionalExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1582">def visit_ellipsis(self, e: EllipsisExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1583">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1584">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1585">def visit_reveal_expr(self, e: RevealExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1586">def visit_super_expr(self, e: SuperExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1587">def visit_type_application(self, e: TypeApplication) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1588">def visit_lambda_expr(self, e: LambdaExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1589">def visit_list_comprehension(self, e: ListComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.159">class NodeInfo:
    """Some info about a node in the graph of SCCs."""

    @others
</t>
<t tx="ekr.20230831011819.1590">def visit_set_comprehension(self, e: SetComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1591">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1592">def visit_generator_expr(self, e: GeneratorExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1593">def visit_type_var_expr(self, e: TypeVarExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1594">def visit_paramspec_expr(self, e: ParamSpecExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1595">def visit_type_var_tuple_expr(self, e: TypeVarTupleExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1596">def visit_type_alias_expr(self, e: TypeAliasExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1597">def visit_namedtuple_expr(self, e: NamedTupleExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1598">def visit_enum_call_expr(self, e: EnumCallExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1599">def visit_typeddict_expr(self, e: TypedDictExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.16">def apply_generic_arguments(
    callable: CallableType,
    orig_types: Sequence[Type | None],
    report_incompatible_typevar_value: Callable[[CallableType, Type, str, Context], None],
    context: Context,
    skip_unsatisfied: bool = False,
) -&gt; CallableType:
    """Apply generic type arguments to a callable type.

    For example, applying [int] to 'def [T] (T) -&gt; T' results in
    'def (int) -&gt; int'.

    Note that each type can be None; in this case, it will not be applied.

    If `skip_unsatisfied` is True, then just skip the types that don't satisfy type variable
    bound or constraints, instead of giving an error.
    """
    tvars = callable.variables
    assert len(tvars) == len(orig_types)
    # Check that inferred type variable values are compatible with allowed
    # values and bounds.  Also, promote subtype values to allowed values.
    # Create a map from type variable id to target type.
    id_to_type: dict[TypeVarId, Type] = {}

    for tvar, type in zip(tvars, orig_types):
        assert not isinstance(type, PartialType), "Internal error: must never apply partial type"
        if type is None:
            continue

        target_type = get_target_type(
            tvar, type, callable, report_incompatible_typevar_value, context, skip_unsatisfied
        )
        if target_type is not None:
            id_to_type[tvar.id] = target_type

    # TODO: validate arg_kinds/arg_names for ParamSpec and TypeVarTuple replacements,
    # not just type variable bounds above.
    param_spec = callable.param_spec()
    if param_spec is not None:
        nt = id_to_type.get(param_spec.id)
        if nt is not None:
            # ParamSpec expansion is special-cased, so we need to always expand callable
            # as a whole, not expanding arguments individually.
            callable = expand_type(callable, id_to_type)
            assert isinstance(callable, CallableType)
            return callable.copy_modified(
                variables=[tv for tv in tvars if tv.id not in id_to_type]
            )

    # Apply arguments to argument types.
    var_arg = callable.var_arg()
    if var_arg is not None and isinstance(var_arg.typ, UnpackType):
        callable = expand_type(callable, id_to_type)
        assert isinstance(callable, CallableType)
        return callable.copy_modified(variables=[tv for tv in tvars if tv.id not in id_to_type])
    else:
        callable = callable.copy_modified(
            arg_types=[expand_type(at, id_to_type) for at in callable.arg_types]
        )

    # Apply arguments to TypeGuard if any.
    if callable.type_guard is not None:
        type_guard = expand_type(callable.type_guard, id_to_type)
    else:
        type_guard = None

    # The callable may retain some type vars if only some were applied.
    # TODO: move apply_poly() logic from checkexpr.py here when new inference
    # becomes universally used (i.e. in all passes + in unification).
    # With this new logic we can actually *add* some new free variables.
    remaining_tvars = [tv for tv in tvars if tv.id not in id_to_type]

    return callable.copy_modified(
        ret_type=expand_type(callable.ret_type, id_to_type),
        variables=remaining_tvars,
        type_guard=type_guard,
    )
</t>
<t tx="ekr.20230831011819.160">def __init__(self, index: int, scc: list[str]) -&gt; None:
    self.node_id = "n%d" % index
    self.scc = scc
    self.sizes: dict[str, int] = {}  # mod -&gt; size in bytes
    self.deps: dict[str, int] = {}  # node_id -&gt; pri

</t>
<t tx="ekr.20230831011819.1600">def visit_newtype_expr(self, e: NewTypeExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1601">def visit__promote_expr(self, e: PromoteExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1602">def visit_await_expr(self, e: AwaitExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20230831011819.1603">def visit_temp_node(self, e: TempNode) -&gt; None:
    return None
</t>
<t tx="ekr.20230831011819.1604">@path mypy
"""
This is a module for various lookup functions:
functions that will find a semantic node by its name.
"""

&lt;&lt; lookup.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1606">from __future__ import annotations

from mypy.nodes import MypyFile, SymbolTableNode, TypeInfo

# TODO: gradually move existing lookup functions to this module.


</t>
<t tx="ekr.20230831011819.1607">def lookup_fully_qualified(
    name: str, modules: dict[str, MypyFile], *, raise_on_missing: bool = False
) -&gt; SymbolTableNode | None:
    """Find a symbol using it fully qualified name.

    The algorithm has two steps: first we try splitting the name on '.' to find
    the module, then iteratively look for each next chunk after a '.' (e.g. for
    nested classes).

    This function should *not* be used to find a module. Those should be looked
    in the modules dictionary.
    """
    head = name
    rest = []
    # 1. Find a module tree in modules dictionary.
    while True:
        if "." not in head:
            if raise_on_missing:
                assert "." in head, f"Cannot find module for {name}"
            return None
        head, tail = head.rsplit(".", maxsplit=1)
        rest.append(tail)
        mod = modules.get(head)
        if mod is not None:
            break
    names = mod.names
    # 2. Find the symbol in the module tree.
    if not rest:
        # Looks like a module, don't use this to avoid confusions.
        if raise_on_missing:
            assert rest, f"Cannot find {name}, got a module symbol"
        return None
    while True:
        key = rest.pop()
        if key not in names:
            if raise_on_missing:
                assert key in names, f"Cannot find component {key!r} for {name!r}"
            return None
        stnode = names[key]
        if not rest:
            return stnode
        node = stnode.node
        # In fine-grained mode, could be a cross-reference to a deleted module
        # or a Var made up for a missing module.
        if not isinstance(node, TypeInfo):
            if raise_on_missing:
                assert node, f"Cannot find {name}"
            return None
        names = node.names
</t>
<t tx="ekr.20230831011819.1608">@path mypy
"""Mypy type checker command line tool."""
&lt;&lt; main.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.1609">
from __future__ import annotations

import argparse
import os
import subprocess
import sys
import time
from gettext import gettext
from typing import IO, Any, Final, NoReturn, Sequence, TextIO

from mypy import build, defaults, state, util
from mypy.config_parser import (
    get_config_module_names,
    parse_config_file,
    parse_version,
    validate_package_allow_list,
)
from mypy.errorcodes import error_codes
from mypy.errors import CompileError
from mypy.find_sources import InvalidSourceList, create_source_list
from mypy.fscache import FileSystemCache
from mypy.modulefinder import BuildSource, FindModuleCache, SearchPaths, get_search_dirs, mypy_path
from mypy.options import INCOMPLETE_FEATURES, BuildType, Options
from mypy.split_namespace import SplitNamespace
from mypy.version import __version__

orig_stat: Final = os.stat
MEM_PROFILE: Final = False  # If True, dump memory profile


</t>
<t tx="ekr.20230831011819.161">def dumps(self) -&gt; str:
    """Convert to JSON string."""
    total_size = sum(self.sizes.values())
    return "[{}, {}, {},\n     {},\n     {}]".format(
        json.dumps(self.node_id),
        json.dumps(total_size),
        json.dumps(self.scc),
        json.dumps(self.sizes),
        json.dumps(self.deps),
    )


</t>
<t tx="ekr.20230831011819.1610">def stat_proxy(path: str) -&gt; os.stat_result:
    try:
        st = orig_stat(path)
    except os.error as err:
        print(f"stat({path!r}) -&gt; {err}")
        raise
    else:
        print(
            "stat(%r) -&gt; (st_mode=%o, st_mtime=%d, st_size=%d)"
            % (path, st.st_mode, st.st_mtime, st.st_size)
        )
        return st


</t>
<t tx="ekr.20230831011819.1611">def main(
    *,
    args: list[str] | None = None,
    stdout: TextIO = sys.stdout,
    stderr: TextIO = sys.stderr,
    clean_exit: bool = False,
) -&gt; None:
    """Main entry point to the type checker.

    Args:
        args: Custom command-line arguments.  If not given, sys.argv[1:] will
            be used.
        clean_exit: Don't hard kill the process on exit. This allows catching
            SystemExit.
    """
    util.check_python_version("mypy")
    t0 = time.time()
    # To log stat() calls: os.stat = stat_proxy
    sys.setrecursionlimit(2**14)
    if args is None:
        args = sys.argv[1:]

    fscache = FileSystemCache()
    sources, options = process_options(args, stdout=stdout, stderr=stderr, fscache=fscache)
    if clean_exit:
        options.fast_exit = False

    formatter = util.FancyFormatter(stdout, stderr, options.hide_error_codes)

    if options.install_types and (stdout is not sys.stdout or stderr is not sys.stderr):
        # Since --install-types performs user input, we want regular stdout and stderr.
        fail("error: --install-types not supported in this mode of running mypy", stderr, options)

    if options.non_interactive and not options.install_types:
        fail("error: --non-interactive is only supported with --install-types", stderr, options)

    if options.install_types and not options.incremental:
        fail(
            "error: --install-types not supported with incremental mode disabled", stderr, options
        )

    if options.install_types and options.python_executable is None:
        fail(
            "error: --install-types not supported without python executable or site packages",
            stderr,
            options,
        )

    if options.install_types and not sources:
        install_types(formatter, options, non_interactive=options.non_interactive)
        return

    res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)

    if options.non_interactive:
        missing_pkgs = read_types_packages_to_install(options.cache_dir, after_run=True)
        if missing_pkgs:
            # Install missing type packages and rerun build.
            install_types(formatter, options, after_run=True, non_interactive=True)
            fscache.flush()
            print()
            res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)
        show_messages(messages, stderr, formatter, options)

    if MEM_PROFILE:
        from mypy.memprofile import print_memory_profile

        print_memory_profile()

    code = 0
    n_errors, n_notes, n_files = util.count_stats(messages)
    if messages and n_notes &lt; len(messages):
        code = 2 if blockers else 1
    if options.error_summary:
        if n_errors:
            summary = formatter.format_error(
                n_errors, n_files, len(sources), blockers=blockers, use_color=options.color_output
            )
            stdout.write(summary + "\n")
        # Only notes should also output success
        elif not messages or n_notes == len(messages):
            stdout.write(formatter.format_success(len(sources), options.color_output) + "\n")
        stdout.flush()

    if options.install_types and not options.non_interactive:
        result = install_types(formatter, options, after_run=True, non_interactive=False)
        if result:
            print()
            print("note: Run mypy again for up-to-date results with installed types")
            code = 2

    if options.fast_exit:
        # Exit without freeing objects -- it's faster.
        #
        # NOTE: We don't flush all open files on exit (or run other destructors)!
        util.hard_exit(code)
    elif code:
        sys.exit(code)

    # HACK: keep res alive so that mypyc won't free it before the hard_exit
    list([res])


</t>
<t tx="ekr.20230831011819.1612">def run_build(
    sources: list[BuildSource],
    options: Options,
    fscache: FileSystemCache,
    t0: float,
    stdout: TextIO,
    stderr: TextIO,
) -&gt; tuple[build.BuildResult | None, list[str], bool]:
    formatter = util.FancyFormatter(stdout, stderr, options.hide_error_codes)

    messages = []

    def flush_errors(new_messages: list[str], serious: bool) -&gt; None:
        if options.pretty:
            new_messages = formatter.fit_in_terminal(new_messages)
        messages.extend(new_messages)
        if options.non_interactive:
            # Collect messages and possibly show them later.
            return
        f = stderr if serious else stdout
        show_messages(new_messages, f, formatter, options)

    serious = False
    blockers = False
    res = None
    try:
        # Keep a dummy reference (res) for memory profiling afterwards, as otherwise
        # the result could be freed.
        res = build.build(sources, options, None, flush_errors, fscache, stdout, stderr)
    except CompileError as e:
        blockers = True
        if not e.use_stdout:
            serious = True
    if (
        options.warn_unused_configs
        and options.unused_configs
        and not options.incremental
        and not options.non_interactive
    ):
        print(
            "Warning: unused section(s) in %s: %s"
            % (
                options.config_file,
                get_config_module_names(
                    options.config_file,
                    [
                        glob
                        for glob in options.per_module_options.keys()
                        if glob in options.unused_configs
                    ],
                ),
            ),
            file=stderr,
        )
    maybe_write_junit_xml(time.time() - t0, serious, messages, options)
    return res, messages, blockers


</t>
<t tx="ekr.20230831011819.1613">def show_messages(
    messages: list[str], f: TextIO, formatter: util.FancyFormatter, options: Options
) -&gt; None:
    for msg in messages:
        if options.color_output:
            msg = formatter.colorize(msg)
        f.write(msg + "\n")
    f.flush()


</t>
<t tx="ekr.20230831011819.1614"># Make the help output a little less jarring.
class AugmentedHelpFormatter(argparse.RawDescriptionHelpFormatter):
    @others
</t>
<t tx="ekr.20230831011819.1615">def __init__(self, prog: str) -&gt; None:
    super().__init__(prog=prog, max_help_position=28)

</t>
<t tx="ekr.20230831011819.1616">def _fill_text(self, text: str, width: int, indent: str) -&gt; str:
    if "\n" in text:
        # Assume we want to manually format the text
        return super()._fill_text(text, width, indent)
    else:
        # Assume we want argparse to manage wrapping, indenting, and
        # formatting the text for us.
        return argparse.HelpFormatter._fill_text(self, text, width, indent)


</t>
<t tx="ekr.20230831011819.1617"># Define pairs of flag prefixes with inverse meaning.
flag_prefix_pairs: Final = [("allow", "disallow"), ("show", "hide")]
flag_prefix_map: Final[dict[str, str]] = {}
for a, b in flag_prefix_pairs:
    flag_prefix_map[a] = b
    flag_prefix_map[b] = a


def invert_flag_name(flag: str) -&gt; str:
    split = flag[2:].split("-", 1)
    if len(split) == 2:
        prefix, rest = split
        if prefix in flag_prefix_map:
            return f"--{flag_prefix_map[prefix]}-{rest}"
        elif prefix == "no":
            return f"--{rest}"

    return f"--no-{flag[2:]}"


</t>
<t tx="ekr.20230831011819.1618">class PythonExecutableInferenceError(Exception):
    """Represents a failure to infer the version or executable while searching."""


</t>
<t tx="ekr.20230831011819.1619">def python_executable_prefix(v: str) -&gt; list[str]:
    if sys.platform == "win32":
        # on Windows, all Python executables are named `python`. To handle this, there
        # is the `py` launcher, which can be passed a version e.g. `py -3.8`, and it will
        # execute an installed Python 3.8 interpreter. See also:
        # https://docs.python.org/3/using/windows.html#python-launcher-for-windows
        return ["py", f"-{v}"]
    else:
        return [f"python{v}"]


</t>
<t tx="ekr.20230831011819.162">def dump_timing_stats(path: str, graph: Graph) -&gt; None:
    """Dump timing stats for each file in the given graph."""
    with open(path, "w") as f:
        for id in sorted(graph):
            f.write(f"{id} {graph[id].time_spent_us}\n")


</t>
<t tx="ekr.20230831011819.1620">def _python_executable_from_version(python_version: tuple[int, int]) -&gt; str:
    if sys.version_info[:2] == python_version:
        return sys.executable
    str_ver = ".".join(map(str, python_version))
    try:
        sys_exe = (
            subprocess.check_output(
                python_executable_prefix(str_ver) + ["-c", "import sys; print(sys.executable)"],
                stderr=subprocess.STDOUT,
            )
            .decode()
            .strip()
        )
        return sys_exe
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        raise PythonExecutableInferenceError(
            "failed to find a Python executable matching version {},"
            " perhaps try --python-executable, or --no-site-packages?".format(python_version)
        ) from e


</t>
<t tx="ekr.20230831011819.1621">def infer_python_executable(options: Options, special_opts: argparse.Namespace) -&gt; None:
    """Infer the Python executable from the given version.

    This function mutates options based on special_opts to infer the correct Python executable
    to use.
    """
    # TODO: (ethanhs) Look at folding these checks and the site packages subprocess calls into
    # one subprocess call for speed.

    # Use the command line specified executable, or fall back to one set in the
    # config file. If an executable is not specified, infer it from the version
    # (unless no_executable is set)
    python_executable = special_opts.python_executable or options.python_executable

    if python_executable is None:
        if not special_opts.no_executable and not options.no_site_packages:
            python_executable = _python_executable_from_version(options.python_version)
    options.python_executable = python_executable


</t>
<t tx="ekr.20230831011819.1622">HEADER: Final = """%(prog)s [-h] [-v] [-V] [more options; see below]
            [-m MODULE] [-p PACKAGE] [-c PROGRAM_TEXT] [files ...]"""


DESCRIPTION: Final = """
Mypy is a program that will type check your Python code.

Pass in any files or folders you want to type check. Mypy will
recursively traverse any provided folders to find .py files:

    $ mypy my_program.py my_src_folder

For more information on getting started, see:

- https://mypy.readthedocs.io/en/stable/getting_started.html

For more details on both running mypy and using the flags below, see:

- https://mypy.readthedocs.io/en/stable/running_mypy.html
- https://mypy.readthedocs.io/en/stable/command_line.html

You can also use a config file to configure mypy instead of using
command line flags. For more details, see:

- https://mypy.readthedocs.io/en/stable/config_file.html
"""

FOOTER: Final = """Environment variables:
  Define MYPYPATH for additional module search path entries.
  Define MYPY_CACHE_DIR to override configuration cache_dir path."""


class CapturableArgumentParser(argparse.ArgumentParser):

    """Override ArgumentParser methods that use sys.stdout/sys.stderr directly.

    This is needed because hijacking sys.std* is not thread-safe,
    yet output must be captured to properly support mypy.api.run.
    """

    @others
</t>
<t tx="ekr.20230831011819.1623">def __init__(self, *args: Any, **kwargs: Any):
    self.stdout = kwargs.pop("stdout", sys.stdout)
    self.stderr = kwargs.pop("stderr", sys.stderr)
    super().__init__(*args, **kwargs)

</t>
<t tx="ekr.20230831011819.1624"># =====================
# Help-printing methods
# =====================
def print_usage(self, file: IO[str] | None = None) -&gt; None:
    if file is None:
        file = self.stdout
    self._print_message(self.format_usage(), file)

</t>
<t tx="ekr.20230831011819.1625">def print_help(self, file: IO[str] | None = None) -&gt; None:
    if file is None:
        file = self.stdout
    self._print_message(self.format_help(), file)

</t>
<t tx="ekr.20230831011819.1626">def _print_message(self, message: str, file: IO[str] | None = None) -&gt; None:
    if message:
        if file is None:
            file = self.stderr
        file.write(message)

</t>
<t tx="ekr.20230831011819.1627"># ===============
# Exiting methods
# ===============
def exit(self, status: int = 0, message: str | None = None) -&gt; NoReturn:
    if message:
        self._print_message(message, self.stderr)
    sys.exit(status)

</t>
<t tx="ekr.20230831011819.1628">def error(self, message: str) -&gt; NoReturn:
    """error(message: string)

    Prints a usage message incorporating the message to stderr and
    exits.

    If you override this in a subclass, it should not return -- it
    should either exit or raise an exception.
    """
    self.print_usage(self.stderr)
    args = {"prog": self.prog, "message": message}
    self.exit(2, gettext("%(prog)s: error: %(message)s\n") % args)


</t>
<t tx="ekr.20230831011819.1629">class CapturableVersionAction(argparse.Action):

    """Supplement CapturableArgumentParser to handle --version.

    This is nearly identical to argparse._VersionAction except,
    like CapturableArgumentParser, it allows output to be captured.

    Another notable difference is that version is mandatory.
    This allows removing a line in __call__ that falls back to parser.version
    (which does not appear to exist).
    """

    @others
</t>
<t tx="ekr.20230831011819.163">def dump_line_checking_stats(path: str, graph: Graph) -&gt; None:
    """Dump per-line expression type checking stats."""
    with open(path, "w") as f:
        for id in sorted(graph):
            if not graph[id].per_line_checking_time_ns:
                continue
            f.write(f"{id}:\n")
            for line in sorted(graph[id].per_line_checking_time_ns):
                line_time = graph[id].per_line_checking_time_ns[line]
                f.write(f"{line:&gt;5} {line_time/1000:8.1f}\n")


</t>
<t tx="ekr.20230831011819.1630">def __init__(
    self,
    option_strings: Sequence[str],
    version: str,
    dest: str = argparse.SUPPRESS,
    default: str = argparse.SUPPRESS,
    help: str = "show program's version number and exit",
    stdout: IO[str] | None = None,
):
    super().__init__(
        option_strings=option_strings, dest=dest, default=default, nargs=0, help=help
    )
    self.version = version
    self.stdout = stdout or sys.stdout

</t>
<t tx="ekr.20230831011819.1631">def __call__(
    self,
    parser: argparse.ArgumentParser,
    namespace: argparse.Namespace,
    values: str | Sequence[Any] | None,
    option_string: str | None = None,
) -&gt; NoReturn:
    formatter = parser._get_formatter()
    formatter.add_text(self.version)
    parser._print_message(formatter.format_help(), self.stdout)
    parser.exit()


</t>
<t tx="ekr.20230831011819.1632">def process_options(
    args: list[str],
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
    require_targets: bool = True,
    server_options: bool = False,
    fscache: FileSystemCache | None = None,
    program: str = "mypy",
    header: str = HEADER,
) -&gt; tuple[list[BuildSource], Options]:
    """Parse command line arguments.

    If a FileSystemCache is passed in, and package_root options are given,
    call fscache.set_package_root() to set the cache's package root.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr

    parser = CapturableArgumentParser(
        prog=program,
        usage=header,
        description=DESCRIPTION,
        epilog=FOOTER,
        fromfile_prefix_chars="@",
        formatter_class=AugmentedHelpFormatter,
        add_help=False,
        stdout=stdout,
        stderr=stderr,
    )

    strict_flag_names: list[str] = []
    strict_flag_assignments: list[tuple[str, bool]] = []

    def add_invertible_flag(
        flag: str,
        *,
        inverse: str | None = None,
        default: bool,
        dest: str | None = None,
        help: str,
        strict_flag: bool = False,
        group: argparse._ActionsContainer | None = None,
    ) -&gt; None:
        if inverse is None:
            inverse = invert_flag_name(flag)
        if group is None:
            group = parser

        if help is not argparse.SUPPRESS:
            help += f" (inverse: {inverse})"

        arg = group.add_argument(
            flag, action="store_false" if default else "store_true", dest=dest, help=help
        )
        dest = arg.dest
        group.add_argument(
            inverse,
            action="store_true" if default else "store_false",
            dest=dest,
            help=argparse.SUPPRESS,
        )
        if strict_flag:
            assert dest is not None
            strict_flag_names.append(flag)
            strict_flag_assignments.append((dest, not default))

    # Unless otherwise specified, arguments will be parsed directly onto an
    # Options object.  Options that require further processing should have
    # their `dest` prefixed with `special-opts:`, which will cause them to be
    # parsed into the separate special_opts namespace object.

    # Note: we have a style guide for formatting the mypy --help text. See
    # https://github.com/python/mypy/wiki/Documentation-Conventions

    general_group = parser.add_argument_group(title="Optional arguments")
    general_group.add_argument(
        "-h", "--help", action="help", help="Show this help message and exit"
    )
    general_group.add_argument(
        "-v", "--verbose", action="count", dest="verbosity", help="More verbose messages"
    )

    compilation_status = "no" if __file__.endswith(".py") else "yes"
    general_group.add_argument(
        "-V",
        "--version",
        action=CapturableVersionAction,
        version="%(prog)s " + __version__ + f" (compiled: {compilation_status})",
        help="Show program's version number and exit",
        stdout=stdout,
    )

    config_group = parser.add_argument_group(
        title="Config file",
        description="Use a config file instead of command line arguments. "
        "This is useful if you are using many flags or want "
        "to set different options per each module.",
    )
    config_group.add_argument(
        "--config-file",
        help="Configuration file, must have a [mypy] section "
        "(defaults to {})".format(", ".join(defaults.CONFIG_FILES)),
    )
    add_invertible_flag(
        "--warn-unused-configs",
        default=False,
        strict_flag=True,
        help="Warn about unused '[mypy-&lt;pattern&gt;]' or '[[tool.mypy.overrides]]' "
        "config sections",
        group=config_group,
    )

    imports_group = parser.add_argument_group(
        title="Import discovery", description="Configure how imports are discovered and followed."
    )
    add_invertible_flag(
        "--no-namespace-packages",
        dest="namespace_packages",
        default=True,
        help="Support namespace packages (PEP 420, __init__.py-less)",
        group=imports_group,
    )
    imports_group.add_argument(
        "--ignore-missing-imports",
        action="store_true",
        help="Silently ignore imports of missing modules",
    )
    imports_group.add_argument(
        "--follow-imports",
        choices=["normal", "silent", "skip", "error"],
        default="normal",
        help="How to treat imports (default normal)",
    )
    imports_group.add_argument(
        "--python-executable",
        action="store",
        metavar="EXECUTABLE",
        help="Python executable used for finding PEP 561 compliant installed"
        " packages and stubs",
        dest="special-opts:python_executable",
    )
    imports_group.add_argument(
        "--no-site-packages",
        action="store_true",
        dest="special-opts:no_executable",
        help="Do not search for installed PEP 561 compliant packages",
    )
    imports_group.add_argument(
        "--no-silence-site-packages",
        action="store_true",
        help="Do not silence errors in PEP 561 compliant installed packages",
    )

    platform_group = parser.add_argument_group(
        title="Platform configuration",
        description="Type check code assuming it will be run under certain "
        "runtime conditions. By default, mypy assumes your code "
        "will be run using the same operating system and Python "
        "version you are using to run mypy itself.",
    )
    platform_group.add_argument(
        "--python-version",
        type=parse_version,
        metavar="x.y",
        help="Type check code assuming it will be running on Python x.y",
        dest="special-opts:python_version",
    )
    platform_group.add_argument(
        "--platform",
        action="store",
        metavar="PLATFORM",
        help="Type check special-cased code for the given OS platform "
        "(defaults to sys.platform)",
    )
    platform_group.add_argument(
        "--always-true",
        metavar="NAME",
        action="append",
        default=[],
        help="Additional variable to be considered True (may be repeated)",
    )
    platform_group.add_argument(
        "--always-false",
        metavar="NAME",
        action="append",
        default=[],
        help="Additional variable to be considered False (may be repeated)",
    )

    disallow_any_group = parser.add_argument_group(
        title="Disallow dynamic typing",
        description="Disallow the use of the dynamic 'Any' type under certain conditions.",
    )
    disallow_any_group.add_argument(
        "--disallow-any-unimported",
        default=False,
        action="store_true",
        help="Disallow Any types resulting from unfollowed imports",
    )
    disallow_any_group.add_argument(
        "--disallow-any-expr",
        default=False,
        action="store_true",
        help="Disallow all expressions that have type Any",
    )
    disallow_any_group.add_argument(
        "--disallow-any-decorated",
        default=False,
        action="store_true",
        help="Disallow functions that have Any in their signature "
        "after decorator transformation",
    )
    disallow_any_group.add_argument(
        "--disallow-any-explicit",
        default=False,
        action="store_true",
        help="Disallow explicit Any in type positions",
    )
    add_invertible_flag(
        "--disallow-any-generics",
        default=False,
        strict_flag=True,
        help="Disallow usage of generic types that do not specify explicit type parameters",
        group=disallow_any_group,
    )
    add_invertible_flag(
        "--disallow-subclassing-any",
        default=False,
        strict_flag=True,
        help="Disallow subclassing values of type 'Any' when defining classes",
        group=disallow_any_group,
    )

    untyped_group = parser.add_argument_group(
        title="Untyped definitions and calls",
        description="Configure how untyped definitions and calls are handled. "
        "Note: by default, mypy ignores any untyped function definitions "
        "and assumes any calls to such functions have a return "
        "type of 'Any'.",
    )
    add_invertible_flag(
        "--disallow-untyped-calls",
        default=False,
        strict_flag=True,
        help="Disallow calling functions without type annotations"
        " from functions with type annotations",
        group=untyped_group,
    )
    untyped_group.add_argument(
        "--untyped-calls-exclude",
        metavar="MODULE",
        action="append",
        default=[],
        help="Disable --disallow-untyped-calls for functions/methods coming"
        " from specific package, module, or class",
    )
    add_invertible_flag(
        "--disallow-untyped-defs",
        default=False,
        strict_flag=True,
        help="Disallow defining functions without type annotations"
        " or with incomplete type annotations",
        group=untyped_group,
    )
    add_invertible_flag(
        "--disallow-incomplete-defs",
        default=False,
        strict_flag=True,
        help="Disallow defining functions with incomplete type annotations "
        "(while still allowing entirely unannotated definitions)",
        group=untyped_group,
    )
    add_invertible_flag(
        "--check-untyped-defs",
        default=False,
        strict_flag=True,
        help="Type check the interior of functions without type annotations",
        group=untyped_group,
    )
    add_invertible_flag(
        "--disallow-untyped-decorators",
        default=False,
        strict_flag=True,
        help="Disallow decorating typed functions with untyped decorators",
        group=untyped_group,
    )

    none_group = parser.add_argument_group(
        title="None and Optional handling",
        description="Adjust how values of type 'None' are handled. For more context on "
        "how mypy handles values of type 'None', see: "
        "https://mypy.readthedocs.io/en/stable/kinds_of_types.html#no-strict-optional",
    )
    add_invertible_flag(
        "--implicit-optional",
        default=False,
        help="Assume arguments with default values of None are Optional",
        group=none_group,
    )
    none_group.add_argument("--strict-optional", action="store_true", help=argparse.SUPPRESS)
    none_group.add_argument(
        "--no-strict-optional",
        action="store_false",
        dest="strict_optional",
        help="Disable strict Optional checks (inverse: --strict-optional)",
    )

    add_invertible_flag(
        "--force-uppercase-builtins", default=False, help=argparse.SUPPRESS, group=none_group
    )

    add_invertible_flag(
        "--force-union-syntax", default=False, help=argparse.SUPPRESS, group=none_group
    )

    lint_group = parser.add_argument_group(
        title="Configuring warnings",
        description="Detect code that is sound but redundant or problematic.",
    )
    add_invertible_flag(
        "--warn-redundant-casts",
        default=False,
        strict_flag=True,
        help="Warn about casting an expression to its inferred type",
        group=lint_group,
    )
    add_invertible_flag(
        "--warn-unused-ignores",
        default=False,
        strict_flag=True,
        help="Warn about unneeded '# type: ignore' comments",
        group=lint_group,
    )
    add_invertible_flag(
        "--no-warn-no-return",
        dest="warn_no_return",
        default=True,
        help="Do not warn about functions that end without returning",
        group=lint_group,
    )
    add_invertible_flag(
        "--warn-return-any",
        default=False,
        strict_flag=True,
        help="Warn about returning values of type Any from non-Any typed functions",
        group=lint_group,
    )
    add_invertible_flag(
        "--warn-unreachable",
        default=False,
        strict_flag=False,
        help="Warn about statements or expressions inferred to be unreachable",
        group=lint_group,
    )

    # Note: this group is intentionally added here even though we don't add
    # --strict to this group near the end.
    #
    # That way, this group will appear after the various strictness groups
    # but before the remaining flags.
    # We add `--strict` near the end so we don't accidentally miss any strictness
    # flags that are added after this group.
    strictness_group = parser.add_argument_group(title="Miscellaneous strictness flags")

    add_invertible_flag(
        "--allow-untyped-globals",
        default=False,
        strict_flag=False,
        help="Suppress toplevel errors caused by missing annotations",
        group=strictness_group,
    )

    add_invertible_flag(
        "--allow-redefinition",
        default=False,
        strict_flag=False,
        help="Allow unconditional variable redefinition with a new type",
        group=strictness_group,
    )

    add_invertible_flag(
        "--no-implicit-reexport",
        default=True,
        strict_flag=True,
        dest="implicit_reexport",
        help="Treat imports as private unless aliased",
        group=strictness_group,
    )

    add_invertible_flag(
        "--strict-equality",
        default=False,
        strict_flag=True,
        help="Prohibit equality, identity, and container checks for non-overlapping types",
        group=strictness_group,
    )

    add_invertible_flag(
        "--extra-checks",
        default=False,
        strict_flag=True,
        help="Enable additional checks that are technically correct but may be impractical "
        "in real code. For example, this prohibits partial overlap in TypedDict updates, "
        "and makes arguments prepended via Concatenate positional-only",
        group=strictness_group,
    )

    strict_help = "Strict mode; enables the following flags: {}".format(
        ", ".join(strict_flag_names)
    )
    strictness_group.add_argument(
        "--strict", action="store_true", dest="special-opts:strict", help=strict_help
    )

    strictness_group.add_argument(
        "--disable-error-code",
        metavar="NAME",
        action="append",
        default=[],
        help="Disable a specific error code",
    )
    strictness_group.add_argument(
        "--enable-error-code",
        metavar="NAME",
        action="append",
        default=[],
        help="Enable a specific error code",
    )

    error_group = parser.add_argument_group(
        title="Configuring error messages",
        description="Adjust the amount of detail shown in error messages.",
    )
    add_invertible_flag(
        "--show-error-context",
        default=False,
        dest="show_error_context",
        help='Precede errors with "note:" messages explaining context',
        group=error_group,
    )
    add_invertible_flag(
        "--show-column-numbers",
        default=False,
        help="Show column numbers in error messages",
        group=error_group,
    )
    add_invertible_flag(
        "--show-error-end",
        default=False,
        help="Show end line/end column numbers in error messages."
        " This implies --show-column-numbers",
        group=error_group,
    )
    add_invertible_flag(
        "--hide-error-codes",
        default=False,
        help="Hide error codes in error messages",
        group=error_group,
    )
    add_invertible_flag(
        "--show-error-code-links",
        default=False,
        help="Show links to error code documentation",
        group=error_group,
    )
    add_invertible_flag(
        "--pretty",
        default=False,
        help="Use visually nicer output in error messages:"
        " Use soft word wrap, show source code snippets,"
        " and show error location markers",
        group=error_group,
    )
    add_invertible_flag(
        "--no-color-output",
        dest="color_output",
        default=True,
        help="Do not colorize error messages",
        group=error_group,
    )
    add_invertible_flag(
        "--no-error-summary",
        dest="error_summary",
        default=True,
        help="Do not show error stats summary",
        group=error_group,
    )
    add_invertible_flag(
        "--show-absolute-path",
        default=False,
        help="Show absolute paths to files",
        group=error_group,
    )
    error_group.add_argument(
        "--soft-error-limit",
        default=defaults.MANY_ERRORS_THRESHOLD,
        type=int,
        dest="many_errors_threshold",
        help=argparse.SUPPRESS,
    )

    incremental_group = parser.add_argument_group(
        title="Incremental mode",
        description="Adjust how mypy incrementally type checks and caches modules. "
        "Mypy caches type information about modules into a cache to "
        "let you speed up future invocations of mypy. Also see "
        "mypy's daemon mode: "
        "mypy.readthedocs.io/en/stable/mypy_daemon.html#mypy-daemon",
    )
    incremental_group.add_argument(
        "-i", "--incremental", action="store_true", help=argparse.SUPPRESS
    )
    incremental_group.add_argument(
        "--no-incremental",
        action="store_false",
        dest="incremental",
        help="Disable module cache (inverse: --incremental)",
    )
    incremental_group.add_argument(
        "--cache-dir",
        action="store",
        metavar="DIR",
        help="Store module cache info in the given folder in incremental mode "
        "(defaults to '{}')".format(defaults.CACHE_DIR),
    )
    add_invertible_flag(
        "--sqlite-cache",
        default=False,
        help="Use a sqlite database to store the cache",
        group=incremental_group,
    )
    incremental_group.add_argument(
        "--cache-fine-grained",
        action="store_true",
        help="Include fine-grained dependency information in the cache for the mypy daemon",
    )
    incremental_group.add_argument(
        "--skip-version-check",
        action="store_true",
        help="Allow using cache written by older mypy version",
    )
    incremental_group.add_argument(
        "--skip-cache-mtime-checks",
        action="store_true",
        help="Skip cache internal consistency checks based on mtime",
    )

    internals_group = parser.add_argument_group(
        title="Advanced options", description="Debug and customize mypy internals."
    )
    internals_group.add_argument("--pdb", action="store_true", help="Invoke pdb on fatal error")
    internals_group.add_argument(
        "--show-traceback", "--tb", action="store_true", help="Show traceback on fatal error"
    )
    internals_group.add_argument(
        "--raise-exceptions", action="store_true", help="Raise exception on fatal error"
    )
    internals_group.add_argument(
        "--custom-typing-module",
        metavar="MODULE",
        dest="custom_typing_module",
        help="Use a custom typing module",
    )
    internals_group.add_argument(
        "--new-type-inference",
        action="store_true",
        help="Enable new experimental type inference algorithm",
    )
    internals_group.add_argument(
        "--disable-recursive-aliases",
        action="store_true",
        help="Disable experimental support for recursive type aliases",
    )
    # Deprecated reverse variant of the above.
    internals_group.add_argument(
        "--enable-recursive-aliases", action="store_true", help=argparse.SUPPRESS
    )
    parser.add_argument(
        "--enable-incomplete-feature",
        action="append",
        metavar="FEATURE",
        help="Enable support of incomplete/experimental features for early preview",
    )
    internals_group.add_argument(
        "--custom-typeshed-dir", metavar="DIR", help="Use the custom typeshed in DIR"
    )
    add_invertible_flag(
        "--warn-incomplete-stub",
        default=False,
        help="Warn if missing type annotation in typeshed, only relevant with"
        " --disallow-untyped-defs or --disallow-incomplete-defs enabled",
        group=internals_group,
    )
    internals_group.add_argument(
        "--shadow-file",
        nargs=2,
        metavar=("SOURCE_FILE", "SHADOW_FILE"),
        dest="shadow_file",
        action="append",
        help="When encountering SOURCE_FILE, read and type check "
        "the contents of SHADOW_FILE instead.",
    )
    internals_group.add_argument("--fast-exit", action="store_true", help=argparse.SUPPRESS)
    internals_group.add_argument(
        "--no-fast-exit", action="store_false", dest="fast_exit", help=argparse.SUPPRESS
    )
    # This flag is useful for mypy tests, where function bodies may be omitted. Plugin developers
    # may want to use this as well in their tests.
    add_invertible_flag(
        "--allow-empty-bodies", default=False, help=argparse.SUPPRESS, group=internals_group
    )
    # This undocumented feature exports limited line-level dependency information.
    internals_group.add_argument("--export-ref-info", action="store_true", help=argparse.SUPPRESS)

    report_group = parser.add_argument_group(
        title="Report generation", description="Generate a report in the specified format."
    )
    for report_type in sorted(defaults.REPORTER_NAMES):
        if report_type not in {"memory-xml"}:
            report_group.add_argument(
                f"--{report_type.replace('_', '-')}-report",
                metavar="DIR",
                dest=f"special-opts:{report_type}_report",
            )

    other_group = parser.add_argument_group(title="Miscellaneous")
    other_group.add_argument("--quickstart-file", help=argparse.SUPPRESS)
    other_group.add_argument("--junit-xml", help="Write junit.xml to the given file")
    other_group.add_argument(
        "--find-occurrences",
        metavar="CLASS.MEMBER",
        dest="special-opts:find_occurrences",
        help="Print out all usages of a class member (experimental)",
    )
    other_group.add_argument(
        "--scripts-are-modules",
        action="store_true",
        help="Script x becomes module x instead of __main__",
    )

    add_invertible_flag(
        "--install-types",
        default=False,
        strict_flag=False,
        help="Install detected missing library stub packages using pip",
        group=other_group,
    )
    add_invertible_flag(
        "--non-interactive",
        default=False,
        strict_flag=False,
        help=(
            "Install stubs without asking for confirmation and hide "
            + "errors, with --install-types"
        ),
        group=other_group,
        inverse="--interactive",
    )

    if server_options:
        # TODO: This flag is superfluous; remove after a short transition (2018-03-16)
        other_group.add_argument(
            "--experimental",
            action="store_true",
            dest="fine_grained_incremental",
            help="Enable fine-grained incremental mode",
        )
        other_group.add_argument(
            "--use-fine-grained-cache",
            action="store_true",
            help="Use the cache in fine-grained incremental mode",
        )

    # hidden options
    parser.add_argument(
        "--stats", action="store_true", dest="dump_type_stats", help=argparse.SUPPRESS
    )
    parser.add_argument(
        "--inferstats", action="store_true", dest="dump_inference_stats", help=argparse.SUPPRESS
    )
    parser.add_argument("--dump-build-stats", action="store_true", help=argparse.SUPPRESS)
    # Dump timing stats for each processed file into the given output file
    parser.add_argument("--timing-stats", dest="timing_stats", help=argparse.SUPPRESS)
    # Dump per line type checking timing stats for each processed file into the given
    # output file. Only total time spent in each top level expression will be shown.
    # Times are show in microseconds.
    parser.add_argument(
        "--line-checking-stats", dest="line_checking_stats", help=argparse.SUPPRESS
    )
    # --debug-cache will disable any cache-related compressions/optimizations,
    # which will make the cache writing process output pretty-printed JSON (which
    # is easier to debug).
    parser.add_argument("--debug-cache", action="store_true", help=argparse.SUPPRESS)
    # --dump-deps will dump all fine-grained dependencies to stdout
    parser.add_argument("--dump-deps", action="store_true", help=argparse.SUPPRESS)
    # --dump-graph will dump the contents of the graph of SCCs and exit.
    parser.add_argument("--dump-graph", action="store_true", help=argparse.SUPPRESS)
    # --semantic-analysis-only does exactly that.
    parser.add_argument("--semantic-analysis-only", action="store_true", help=argparse.SUPPRESS)
    # --local-partial-types disallows partial types spanning module top level and a function
    # (implicitly defined in fine-grained incremental mode)
    parser.add_argument("--local-partial-types", action="store_true", help=argparse.SUPPRESS)
    # --logical-deps adds some more dependencies that are not semantically needed, but
    # may be helpful to determine relative importance of classes and functions for overall
    # type precision in a code base. It also _removes_ some deps, so this flag should be never
    # used except for generating code stats. This also automatically enables --cache-fine-grained.
    # NOTE: This is an experimental option that may be modified or removed at any time.
    parser.add_argument("--logical-deps", action="store_true", help=argparse.SUPPRESS)
    # --bazel changes some behaviors for use with Bazel (https://bazel.build).
    parser.add_argument("--bazel", action="store_true", help=argparse.SUPPRESS)
    # --package-root adds a directory below which directories are considered
    # packages even without __init__.py.  May be repeated.
    parser.add_argument(
        "--package-root", metavar="ROOT", action="append", default=[], help=argparse.SUPPRESS
    )
    # --cache-map FILE ... gives a mapping from source files to cache files.
    # Each triple of arguments is a source file, a cache meta file, and a cache data file.
    # Modules not mentioned in the file will go through cache_dir.
    # Must be followed by another flag or by '--' (and then only file args may follow).
    parser.add_argument(
        "--cache-map", nargs="+", dest="special-opts:cache_map", help=argparse.SUPPRESS
    )
    # --debug-serialize will run tree.serialize() even if cache generation is disabled.
    # Useful for mypy_primer to detect serialize errors earlier.
    parser.add_argument("--debug-serialize", action="store_true", help=argparse.SUPPRESS)
    # This one is deprecated, but we will keep it for few releases.
    parser.add_argument(
        "--enable-incomplete-features", action="store_true", help=argparse.SUPPRESS
    )
    parser.add_argument(
        "--disable-bytearray-promotion", action="store_true", help=argparse.SUPPRESS
    )
    parser.add_argument(
        "--disable-memoryview-promotion", action="store_true", help=argparse.SUPPRESS
    )
    # This flag is deprecated, it has been moved to --extra-checks
    parser.add_argument("--strict-concatenate", action="store_true", help=argparse.SUPPRESS)

    # options specifying code to check
    code_group = parser.add_argument_group(
        title="Running code",
        description="Specify the code you want to type check. For more details, see "
        "mypy.readthedocs.io/en/stable/running_mypy.html#running-mypy",
    )
    add_invertible_flag(
        "--explicit-package-bases",
        default=False,
        help="Use current directory and MYPYPATH to determine module names of files passed",
        group=code_group,
    )
    add_invertible_flag(
        "--fast-module-lookup", default=False, help=argparse.SUPPRESS, group=code_group
    )
    code_group.add_argument(
        "--exclude",
        action="append",
        metavar="PATTERN",
        default=[],
        help=(
            "Regular expression to match file names, directory names or paths which mypy should "
            "ignore while recursively discovering files to check, e.g. --exclude '/setup\\.py$'. "
            "May be specified more than once, eg. --exclude a --exclude b"
        ),
    )
    code_group.add_argument(
        "-m",
        "--module",
        action="append",
        metavar="MODULE",
        default=[],
        dest="special-opts:modules",
        help="Type-check module; can repeat for more modules",
    )
    code_group.add_argument(
        "-p",
        "--package",
        action="append",
        metavar="PACKAGE",
        default=[],
        dest="special-opts:packages",
        help="Type-check package recursively; can be repeated",
    )
    code_group.add_argument(
        "-c",
        "--command",
        action="append",
        metavar="PROGRAM_TEXT",
        dest="special-opts:command",
        help="Type-check program passed in as string",
    )
    code_group.add_argument(
        metavar="files",
        nargs="*",
        dest="special-opts:files",
        help="Type-check given files or directories",
    )

    # Parse arguments once into a dummy namespace so we can get the
    # filename for the config file and know if the user requested all strict options.
    dummy = argparse.Namespace()
    parser.parse_args(args, dummy)
    config_file = dummy.config_file
    # Don't explicitly test if "config_file is not None" for this check.
    # This lets `--config-file=` (an empty string) be used to disable all config files.
    if config_file and not os.path.exists(config_file):
        parser.error(f"Cannot find config file '{config_file}'")

    options = Options()
    strict_option_set = False

    def set_strict_flags() -&gt; None:
        nonlocal strict_option_set
        strict_option_set = True
        for dest, value in strict_flag_assignments:
            setattr(options, dest, value)

    # Parse config file first, so command line can override.
    parse_config_file(options, set_strict_flags, config_file, stdout, stderr)

    # Set strict flags before parsing (if strict mode enabled), so other command
    # line options can override.
    if getattr(dummy, "special-opts:strict"):
        set_strict_flags()

    # Override cache_dir if provided in the environment
    environ_cache_dir = os.getenv("MYPY_CACHE_DIR", "")
    if environ_cache_dir.strip():
        options.cache_dir = environ_cache_dir
    options.cache_dir = os.path.expanduser(options.cache_dir)

    # Parse command line for real, using a split namespace.
    special_opts = argparse.Namespace()
    parser.parse_args(args, SplitNamespace(options, special_opts, "special-opts:"))

    # The python_version is either the default, which can be overridden via a config file,
    # or stored in special_opts and is passed via the command line.
    options.python_version = special_opts.python_version or options.python_version
    if options.python_version &lt; (3,):
        parser.error(
            "Mypy no longer supports checking Python 2 code. "
            "Consider pinning to mypy&lt;0.980 if you need to check Python 2 code."
        )
    try:
        infer_python_executable(options, special_opts)
    except PythonExecutableInferenceError as e:
        parser.error(str(e))

    if special_opts.no_executable or options.no_site_packages:
        options.python_executable = None

    # Paths listed in the config file will be ignored if any paths, modules or packages
    # are passed on the command line.
    if not (special_opts.files or special_opts.packages or special_opts.modules):
        if options.files:
            special_opts.files = options.files
        if options.packages:
            special_opts.packages = options.packages
        if options.modules:
            special_opts.modules = options.modules

    # Check for invalid argument combinations.
    if require_targets:
        code_methods = sum(
            bool(c)
            for c in [
                special_opts.modules + special_opts.packages,
                special_opts.command,
                special_opts.files,
            ]
        )
        if code_methods == 0 and not options.install_types:
            parser.error("Missing target module, package, files, or command.")
        elif code_methods &gt; 1:
            parser.error("May only specify one of: module/package, files, or command.")
    if options.explicit_package_bases and not options.namespace_packages:
        parser.error(
            "Can only use --explicit-package-bases with --namespace-packages, since otherwise "
            "examining __init__.py's is sufficient to determine module names for files"
        )

    # Check for overlapping `--always-true` and `--always-false` flags.
    overlap = set(options.always_true) &amp; set(options.always_false)
    if overlap:
        parser.error(
            "You can't make a variable always true and always false (%s)"
            % ", ".join(sorted(overlap))
        )

    validate_package_allow_list(options.untyped_calls_exclude)

    # Process `--enable-error-code` and `--disable-error-code` flags
    disabled_codes = set(options.disable_error_code)
    enabled_codes = set(options.enable_error_code)

    valid_error_codes = set(error_codes.keys())

    invalid_codes = (enabled_codes | disabled_codes) - valid_error_codes
    if invalid_codes:
        parser.error(f"Invalid error code(s): {', '.join(sorted(invalid_codes))}")

    options.disabled_error_codes |= {error_codes[code] for code in disabled_codes}
    options.enabled_error_codes |= {error_codes[code] for code in enabled_codes}

    # Enabling an error code always overrides disabling
    options.disabled_error_codes -= options.enabled_error_codes

    # Validate incomplete features.
    for feature in options.enable_incomplete_feature:
        if feature not in INCOMPLETE_FEATURES:
            parser.error(f"Unknown incomplete feature: {feature}")
    if options.enable_incomplete_features:
        print(
            "Warning: --enable-incomplete-features is deprecated, use"
            " --enable-incomplete-feature=FEATURE instead"
        )
        options.enable_incomplete_feature = list(INCOMPLETE_FEATURES)

    # Compute absolute path for custom typeshed (if present).
    if options.custom_typeshed_dir is not None:
        options.abs_custom_typeshed_dir = os.path.abspath(options.custom_typeshed_dir)

    # Set build flags.
    if special_opts.find_occurrences:
        _find_occurrences = tuple(special_opts.find_occurrences.split("."))
        if len(_find_occurrences) &lt; 2:
            parser.error("Can only find occurrences of class members.")
        if len(_find_occurrences) != 2:
            parser.error("Can only find occurrences of non-nested class members.")
        state.find_occurrences = _find_occurrences  # type: ignore[assignment]

    # Set reports.
    for flag, val in vars(special_opts).items():
        if flag.endswith("_report") and val is not None:
            report_type = flag[:-7].replace("_", "-")
            report_dir = val
            options.report_dirs[report_type] = report_dir

    # Process --package-root.
    if options.package_root:
        process_package_roots(fscache, parser, options)

    # Process --cache-map.
    if special_opts.cache_map:
        if options.sqlite_cache:
            parser.error("--cache-map is incompatible with --sqlite-cache")

        process_cache_map(parser, special_opts, options)

    # An explicitly specified cache_fine_grained implies local_partial_types
    # (because otherwise the cache is not compatible with dmypy)
    if options.cache_fine_grained:
        options.local_partial_types = True

    #  Implicitly show column numbers if error location end is shown
    if options.show_error_end:
        options.show_column_numbers = True

    # Let logical_deps imply cache_fine_grained (otherwise the former is useless).
    if options.logical_deps:
        options.cache_fine_grained = True

    if options.enable_recursive_aliases:
        print(
            "Warning: --enable-recursive-aliases is deprecated;"
            " recursive types are enabled by default"
        )
    if options.strict_concatenate and not strict_option_set:
        print("Warning: --strict-concatenate is deprecated; use --extra-checks instead")

    # Set target.
    if special_opts.modules + special_opts.packages:
        options.build_type = BuildType.MODULE
        sys_path, _ = get_search_dirs(options.python_executable)
        search_paths = SearchPaths(
            (os.getcwd(),), tuple(mypy_path() + options.mypy_path), tuple(sys_path), ()
        )
        targets = []
        # TODO: use the same cache that the BuildManager will
        cache = FindModuleCache(search_paths, fscache, options)
        for p in special_opts.packages:
            if os.sep in p or os.altsep and os.altsep in p:
                fail(f"Package name '{p}' cannot have a slash in it.", stderr, options)
            p_targets = cache.find_modules_recursive(p)
            if not p_targets:
                fail(f"Can't find package '{p}'", stderr, options)
            targets.extend(p_targets)
        for m in special_opts.modules:
            targets.append(BuildSource(None, m, None))
        return targets, options
    elif special_opts.command:
        options.build_type = BuildType.PROGRAM_TEXT
        targets = [BuildSource(None, None, "\n".join(special_opts.command))]
        return targets, options
    else:
        try:
            targets = create_source_list(special_opts.files, options, fscache)
        # Variable named e2 instead of e to work around mypyc bug #620
        # which causes issues when using the same variable to catch
        # exceptions of different types.
        except InvalidSourceList as e2:
            fail(str(e2), stderr, options)
        return targets, options


</t>
<t tx="ekr.20230831011819.1633">def process_package_roots(
    fscache: FileSystemCache | None, parser: argparse.ArgumentParser, options: Options
) -&gt; None:
    """Validate and normalize package_root."""
    if fscache is None:
        parser.error("--package-root does not work here (no fscache)")
    assert fscache is not None  # Since mypy doesn't know parser.error() raises.
    # Do some stuff with drive letters to make Windows happy (esp. tests).
    current_drive, _ = os.path.splitdrive(os.getcwd())
    dot = os.curdir
    dotslash = os.curdir + os.sep
    dotdotslash = os.pardir + os.sep
    trivial_paths = {dot, dotslash}
    package_root = []
    for root in options.package_root:
        if os.path.isabs(root):
            parser.error(f"Package root cannot be absolute: {root!r}")
        drive, root = os.path.splitdrive(root)
        if drive and drive != current_drive:
            parser.error(f"Package root must be on current drive: {drive + root!r}")
        # Empty package root is always okay.
        if root:
            root = os.path.relpath(root)  # Normalize the heck out of it.
            if not root.endswith(os.sep):
                root = root + os.sep
            if root.startswith(dotdotslash):
                parser.error(f"Package root cannot be above current directory: {root!r}")
            if root in trivial_paths:
                root = ""
        package_root.append(root)
    options.package_root = package_root
    # Pass the package root on the the filesystem cache.
    fscache.set_package_root(package_root)


</t>
<t tx="ekr.20230831011819.164">def dump_graph(graph: Graph, stdout: TextIO | None = None) -&gt; None:
    """Dump the graph as a JSON string to stdout.

    This copies some of the work by process_graph()
    (sorted_components() and order_ascc()).
    """
    stdout = stdout or sys.stdout
    nodes = []
    sccs = sorted_components(graph)
    for i, ascc in enumerate(sccs):
        scc = order_ascc(graph, ascc)
        node = NodeInfo(i, scc)
        nodes.append(node)
    inv_nodes = {}  # module -&gt; node_id
    for node in nodes:
        for mod in node.scc:
            inv_nodes[mod] = node.node_id
    for node in nodes:
        for mod in node.scc:
            state = graph[mod]
            size = 0
            if state.path:
                try:
                    size = os.path.getsize(state.path)
                except os.error:
                    pass
            node.sizes[mod] = size
            for dep in state.dependencies:
                if dep in state.priorities:
                    pri = state.priorities[dep]
                    if dep in inv_nodes:
                        dep_id = inv_nodes[dep]
                        if dep_id != node.node_id and (
                            dep_id not in node.deps or pri &lt; node.deps[dep_id]
                        ):
                            node.deps[dep_id] = pri
    print("[" + ",\n ".join(node.dumps() for node in nodes) + "\n]", file=stdout)


</t>
<t tx="ekr.20230831011819.165">def load_graph(
    sources: list[BuildSource],
    manager: BuildManager,
    old_graph: Graph | None = None,
    new_modules: list[State] | None = None,
) -&gt; Graph:
    """Given some source files, load the full dependency graph.

    If an old_graph is passed in, it is used as the starting point and
    modified during graph loading.

    If a new_modules is passed in, any modules that are loaded are
    added to the list. This is an argument and not a return value
    so that the caller can access it even if load_graph fails.

    As this may need to parse files, this can raise CompileError in case
    there are syntax errors.
    """

    graph: Graph = old_graph if old_graph is not None else {}

    # The deque is used to implement breadth-first traversal.
    # TODO: Consider whether to go depth-first instead.  This may
    # affect the order in which we process files within import cycles.
    new = new_modules if new_modules is not None else []
    entry_points: set[str] = set()
    # Seed the graph with the initial root sources.
    for bs in sources:
        try:
            st = State(
                id=bs.module,
                path=bs.path,
                source=bs.text,
                manager=manager,
                root_source=not bs.followed,
            )
        except ModuleNotFound:
            continue
        if st.id in graph:
            manager.errors.set_file(st.xpath, st.id, manager.options)
            manager.errors.report(
                -1,
                -1,
                f'Duplicate module named "{st.id}" (also at "{graph[st.id].xpath}")',
                blocker=True,
            )
            manager.errors.report(
                -1,
                -1,
                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "
                "for more info",
                severity="note",
            )
            manager.errors.report(
                -1,
                -1,
                "Common resolutions include: a) using `--exclude` to avoid checking one of them, "
                "b) adding `__init__.py` somewhere, c) using `--explicit-package-bases` or "
                "adjusting MYPYPATH",
                severity="note",
            )

            manager.errors.raise_error()
        graph[st.id] = st
        new.append(st)
        entry_points.add(bs.module)

    # Note: Running this each time could be slow in the daemon. If it's a problem, we
    # can do more work to maintain this incrementally.
    seen_files = {st.abspath: st for st in graph.values() if st.path}

    # Collect dependencies.  We go breadth-first.
    # More nodes might get added to new as we go, but that's fine.
    for st in new:
        assert st.ancestors is not None
        # Strip out indirect dependencies.  These will be dealt with
        # when they show up as direct dependencies, and there's a
        # scenario where they hurt:
        # - Suppose A imports B and B imports C.
        # - Suppose on the next round:
        #   - C is deleted;
        #   - B is updated to remove the dependency on C;
        #   - A is unchanged.
        # - In this case A's cached *direct* dependencies are still valid
        #   (since direct dependencies reflect the imports found in the source)
        #   but A's cached *indirect* dependency on C is wrong.
        dependencies = [dep for dep in st.dependencies if st.priorities.get(dep) != PRI_INDIRECT]
        if not manager.use_fine_grained_cache():
            # TODO: Ideally we could skip here modules that appeared in st.suppressed
            # because they are not in build with `follow-imports=skip`.
            # This way we could avoid overhead of cloning options in `State.__init__()`
            # below to get the option value. This is quite minor performance loss however.
            added = [dep for dep in st.suppressed if find_module_simple(dep, manager)]
        else:
            # During initial loading we don't care about newly added modules,
            # they will be taken care of during fine grained update. See also
            # comment about this in `State.__init__()`.
            added = []
        for dep in st.ancestors + dependencies + st.suppressed:
            ignored = dep in st.suppressed_set and dep not in entry_points
            if ignored and dep not in added:
                manager.missing_modules.add(dep)
            elif dep not in graph:
                try:
                    if dep in st.ancestors:
                        # TODO: Why not 'if dep not in st.dependencies' ?
                        # Ancestors don't have import context.
                        newst = State(
                            id=dep, path=None, source=None, manager=manager, ancestor_for=st
                        )
                    else:
                        newst = State(
                            id=dep,
                            path=None,
                            source=None,
                            manager=manager,
                            caller_state=st,
                            caller_line=st.dep_line_map.get(dep, 1),
                        )
                except ModuleNotFound:
                    if dep in st.dependencies_set:
                        st.suppress_dependency(dep)
                else:
                    if newst.path:
                        newst_path = os.path.abspath(newst.path)

                        if newst_path in seen_files:
                            manager.errors.report(
                                -1,
                                0,
                                "Source file found twice under different module names: "
                                '"{}" and "{}"'.format(seen_files[newst_path].id, newst.id),
                                blocker=True,
                            )
                            manager.errors.report(
                                -1,
                                0,
                                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "
                                "for more info",
                                severity="note",
                            )
                            manager.errors.report(
                                -1,
                                0,
                                "Common resolutions include: a) adding `__init__.py` somewhere, "
                                "b) using `--explicit-package-bases` or adjusting MYPYPATH",
                                severity="note",
                            )
                            manager.errors.raise_error()

                        seen_files[newst_path] = newst

                    assert newst.id not in graph, newst.id
                    graph[newst.id] = newst
                    new.append(newst)
            if dep in graph and dep in st.suppressed_set:
                # Previously suppressed file is now visible
                st.add_dependency(dep)
    manager.plugin.set_modules(manager.modules)
    return graph


</t>
<t tx="ekr.20230831011819.166">def process_graph(graph: Graph, manager: BuildManager) -&gt; None:
    """Process everything in dependency order."""
    sccs = sorted_components(graph)
    manager.log("Found %d SCCs; largest has %d nodes" % (len(sccs), max(len(scc) for scc in sccs)))

    fresh_scc_queue: list[list[str]] = []

    # We're processing SCCs from leaves (those without further
    # dependencies) to roots (those from which everything else can be
    # reached).
    for ascc in sccs:
        # Order the SCC's nodes using a heuristic.
        # Note that ascc is a set, and scc is a list.
        scc = order_ascc(graph, ascc)
        # Make the order of the SCC that includes 'builtins' and 'typing',
        # among other things, predictable. Various things may  break if
        # the order changes.
        if "builtins" in ascc:
            scc = sorted(scc, reverse=True)
            # If builtins is in the list, move it last.  (This is a bit of
            # a hack, but it's necessary because the builtins module is
            # part of a small cycle involving at least {builtins, abc,
            # typing}.  Of these, builtins must be processed last or else
            # some builtin objects will be incompletely processed.)
            scc.remove("builtins")
            scc.append("builtins")
        if manager.options.verbosity &gt;= 2:
            for id in scc:
                manager.trace(
                    f"Priorities for {id}:",
                    " ".join(
                        "%s:%d" % (x, graph[id].priorities[x])
                        for x in graph[id].dependencies
                        if x in ascc and x in graph[id].priorities
                    ),
                )
        # Because the SCCs are presented in topological sort order, we
        # don't need to look at dependencies recursively for staleness
        # -- the immediate dependencies are sufficient.
        stale_scc = {id for id in scc if not graph[id].is_fresh()}
        fresh = not stale_scc
        deps = set()
        for id in scc:
            deps.update(graph[id].dependencies)
        deps -= ascc
        stale_deps = {id for id in deps if id in graph and not graph[id].is_interface_fresh()}
        fresh = fresh and not stale_deps
        undeps = set()
        if fresh:
            # Check if any dependencies that were suppressed according
            # to the cache have been added back in this run.
            # NOTE: Newly suppressed dependencies are handled by is_fresh().
            for id in scc:
                undeps.update(graph[id].suppressed)
            undeps &amp;= graph.keys()
            if undeps:
                fresh = False
        if fresh:
            # All cache files are fresh.  Check that no dependency's
            # cache file is newer than any scc node's cache file.
            oldest_in_scc = min(graph[id].xmeta.data_mtime for id in scc)
            viable = {id for id in stale_deps if graph[id].meta is not None}
            newest_in_deps = (
                0 if not viable else max(graph[dep].xmeta.data_mtime for dep in viable)
            )
            if manager.options.verbosity &gt;= 3:  # Dump all mtimes for extreme debugging.
                all_ids = sorted(ascc | viable, key=lambda id: graph[id].xmeta.data_mtime)
                for id in all_ids:
                    if id in scc:
                        if graph[id].xmeta.data_mtime &lt; newest_in_deps:
                            key = "*id:"
                        else:
                            key = "id:"
                    else:
                        if graph[id].xmeta.data_mtime &gt; oldest_in_scc:
                            key = "+dep:"
                        else:
                            key = "dep:"
                    manager.trace(" %5s %.0f %s" % (key, graph[id].xmeta.data_mtime, id))
            # If equal, give the benefit of the doubt, due to 1-sec time granularity
            # (on some platforms).
            if oldest_in_scc &lt; newest_in_deps:
                fresh = False
                fresh_msg = f"out of date by {newest_in_deps - oldest_in_scc:.0f} seconds"
            else:
                fresh_msg = "fresh"
        elif undeps:
            fresh_msg = f"stale due to changed suppression ({' '.join(sorted(undeps))})"
        elif stale_scc:
            fresh_msg = "inherently stale"
            if stale_scc != ascc:
                fresh_msg += f" ({' '.join(sorted(stale_scc))})"
            if stale_deps:
                fresh_msg += f" with stale deps ({' '.join(sorted(stale_deps))})"
        else:
            fresh_msg = f"stale due to deps ({' '.join(sorted(stale_deps))})"

        # Initialize transitive_error for all SCC members from union
        # of transitive_error of dependencies.
        if any(graph[dep].transitive_error for dep in deps if dep in graph):
            for id in scc:
                graph[id].transitive_error = True

        scc_str = " ".join(scc)
        if fresh:
            manager.trace(f"Queuing {fresh_msg} SCC ({scc_str})")
            fresh_scc_queue.append(scc)
        else:
            if fresh_scc_queue:
                manager.log(f"Processing {len(fresh_scc_queue)} queued fresh SCCs")
                # Defer processing fresh SCCs until we actually run into a stale SCC
                # and need the earlier modules to be loaded.
                #
                # Note that `process_graph` may end with us not having processed every
                # single fresh SCC. This is intentional -- we don't need those modules
                # loaded if there are no more stale SCCs to be rechecked.
                #
                # Also note we shouldn't have to worry about transitive_error here,
                # since modules with transitive errors aren't written to the cache,
                # and if any dependencies were changed, this SCC would be stale.
                # (Also, in quick_and_dirty mode we don't care about transitive errors.)
                #
                # TODO: see if it's possible to determine if we need to process only a
                # _subset_ of the past SCCs instead of having to process them all.
                for prev_scc in fresh_scc_queue:
                    process_fresh_modules(graph, prev_scc, manager)
                fresh_scc_queue = []
            size = len(scc)
            if size == 1:
                manager.log(f"Processing SCC singleton ({scc_str}) as {fresh_msg}")
            else:
                manager.log("Processing SCC of size %d (%s) as %s" % (size, scc_str, fresh_msg))
            process_stale_scc(graph, scc, manager)

    sccs_left = len(fresh_scc_queue)
    nodes_left = sum(len(scc) for scc in fresh_scc_queue)
    manager.add_stats(sccs_left=sccs_left, nodes_left=nodes_left)
    if sccs_left:
        manager.log(
            "{} fresh SCCs ({} nodes) left in queue (and will remain unprocessed)".format(
                sccs_left, nodes_left
            )
        )
        manager.trace(str(fresh_scc_queue))
    else:
        manager.log("No fresh SCCs left in queue")


</t>
<t tx="ekr.20230831011819.167">def order_ascc(graph: Graph, ascc: AbstractSet[str], pri_max: int = PRI_ALL) -&gt; list[str]:
    """Come up with the ideal processing order within an SCC.

    Using the priorities assigned by all_imported_modules_in_file(),
    try to reduce the cycle to a DAG, by omitting arcs representing
    dependencies of lower priority.

    In the simplest case, if we have A &lt;--&gt; B where A has a top-level
    "import B" (medium priority) but B only has the reverse "import A"
    inside a function (low priority), we turn the cycle into a DAG by
    dropping the B --&gt; A arc, which leaves only A --&gt; B.

    If all arcs have the same priority, we fall back to sorting by
    reverse global order (the order in which modules were first
    encountered).

    The algorithm is recursive, as follows: when as arcs of different
    priorities are present, drop all arcs of the lowest priority,
    identify SCCs in the resulting graph, and apply the algorithm to
    each SCC thus found.  The recursion is bounded because at each
    recursion the spread in priorities is (at least) one less.

    In practice there are only a few priority levels (less than a
    dozen) and in the worst case we just carry out the same algorithm
    for finding SCCs N times.  Thus the complexity is no worse than
    the complexity of the original SCC-finding algorithm -- see
    strongly_connected_components() below for a reference.
    """
    if len(ascc) == 1:
        return [s for s in ascc]
    pri_spread = set()
    for id in ascc:
        state = graph[id]
        for dep in state.dependencies:
            if dep in ascc:
                pri = state.priorities.get(dep, PRI_HIGH)
                if pri &lt; pri_max:
                    pri_spread.add(pri)
    if len(pri_spread) == 1:
        # Filtered dependencies are uniform -- order by global order.
        return sorted(ascc, key=lambda id: -graph[id].order)
    pri_max = max(pri_spread)
    sccs = sorted_components(graph, ascc, pri_max)
    # The recursion is bounded by the len(pri_spread) check above.
    return [s for ss in sccs for s in order_ascc(graph, ss, pri_max)]


</t>
<t tx="ekr.20230831011819.168">def process_fresh_modules(graph: Graph, modules: list[str], manager: BuildManager) -&gt; None:
    """Process the modules in one group of modules from their cached data.

    This can be used to process an SCC of modules
    This involves loading the tree from JSON and then doing various cleanups.
    """
    t0 = time.time()
    for id in modules:
        graph[id].load_tree()
    t1 = time.time()
    for id in modules:
        graph[id].fix_cross_refs()
    t2 = time.time()
    manager.add_stats(process_fresh_time=t2 - t0, load_tree_time=t1 - t0)


</t>
<t tx="ekr.20230831011819.169">def process_stale_scc(graph: Graph, scc: list[str], manager: BuildManager) -&gt; None:
    """Process the modules in one SCC from source code.

    Exception: If quick_and_dirty is set, use the cache for fresh modules.
    """
    stale = scc
    for id in stale:
        # We may already have parsed the module, or not.
        # If the former, parse_file() is a no-op.
        graph[id].parse_file()
    if "typing" in scc:
        # For historical reasons we need to manually add typing aliases
        # for built-in generic collections, see docstring of
        # SemanticAnalyzerPass2.add_builtin_aliases for details.
        typing_mod = graph["typing"].tree
        assert typing_mod, "The typing module was not parsed"
    mypy.semanal_main.semantic_analysis_for_scc(graph, scc, manager.errors)

    # Track what modules aren't yet done so we can finish them as soon
    # as possible, saving memory.
    unfinished_modules = set(stale)
    for id in stale:
        graph[id].type_check_first_pass()
        if not graph[id].type_checker().deferred_nodes:
            unfinished_modules.discard(id)
            graph[id].detect_possibly_undefined_vars()
            graph[id].finish_passes()

    while unfinished_modules:
        for id in stale:
            if id not in unfinished_modules:
                continue
            if not graph[id].type_check_second_pass():
                unfinished_modules.discard(id)
                graph[id].detect_possibly_undefined_vars()
                graph[id].finish_passes()
    for id in stale:
        graph[id].generate_unused_ignore_notes()
        graph[id].generate_ignore_without_code_notes()
    if any(manager.errors.is_errors_for_file(graph[id].xpath) for id in stale):
        for id in stale:
            graph[id].transitive_error = True
    for id in stale:
        manager.flush_errors(manager.errors.file_messages(graph[id].xpath), False)
        graph[id].write_cache()
        graph[id].mark_as_rechecked()


</t>
<t tx="ekr.20230831011819.17">@path mypy
"""Utilities for mapping between actual and formal arguments (and their types)."""
&lt;&lt; argmap.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.170">def sorted_components(
    graph: Graph, vertices: AbstractSet[str] | None = None, pri_max: int = PRI_ALL
) -&gt; list[AbstractSet[str]]:
    """Return the graph's SCCs, topologically sorted by dependencies.

    The sort order is from leaves (nodes without dependencies) to
    roots (nodes on which no other nodes depend).

    This works for a subset of the full dependency graph too;
    dependencies that aren't present in graph.keys() are ignored.
    """
    # Compute SCCs.
    if vertices is None:
        vertices = set(graph)
    edges = {id: deps_filtered(graph, vertices, id, pri_max) for id in vertices}
    sccs = list(strongly_connected_components(vertices, edges))
    # Topsort.
    res = []
    for ready in topsort(prepare_sccs(sccs, edges)):
        # Sort the sets in ready by reversed smallest State.order.  Examples:
        #
        # - If ready is [{x}, {y}], x.order == 1, y.order == 2, we get
        #   [{y}, {x}].
        #
        # - If ready is [{a, b}, {c, d}], a.order == 1, b.order == 3,
        #   c.order == 2, d.order == 4, the sort keys become [1, 2]
        #   and the result is [{c, d}, {a, b}].
        res.extend(sorted(ready, key=lambda scc: -min(graph[id].order for id in scc)))
    return res


</t>
<t tx="ekr.20230831011819.171">def deps_filtered(graph: Graph, vertices: AbstractSet[str], id: str, pri_max: int) -&gt; list[str]:
    """Filter dependencies for id with pri &lt; pri_max."""
    if id not in vertices:
        return []
    state = graph[id]
    return [
        dep
        for dep in state.dependencies
        if dep in vertices and state.priorities.get(dep, PRI_HIGH) &lt; pri_max
    ]


</t>
<t tx="ekr.20230831011819.172">def missing_stubs_file(cache_dir: str) -&gt; str:
    return os.path.join(cache_dir, "missing_stubs")


</t>
<t tx="ekr.20230831011819.173">def record_missing_stub_packages(cache_dir: str, missing_stub_packages: set[str]) -&gt; None:
    """Write a file containing missing stub packages.

    This allows a subsequent "mypy --install-types" run (without other arguments)
    to install missing stub packages.
    """
    fnam = missing_stubs_file(cache_dir)
    if missing_stub_packages:
        with open(fnam, "w") as f:
            for pkg in sorted(missing_stub_packages):
                f.write(f"{pkg}\n")
    else:
        if os.path.isfile(fnam):
            os.remove(fnam)


</t>
<t tx="ekr.20230831011819.174">def is_silent_import_module(manager: BuildManager, path: str) -&gt; bool:
    if manager.options.no_silence_site_packages:
        return False
    # Silence errors in site-package dirs and typeshed
    return any(
        is_sub_path(path, dir)
        for dir in manager.search_paths.package_path + manager.search_paths.typeshed_path
    )


</t>
<t tx="ekr.20230831011819.175">def write_undocumented_ref_info(
    state: State, metastore: MetadataStore, options: Options, type_map: dict[Expression, Type]
) -&gt; None:
    # This exports some dependency information in a rather ad-hoc fashion, which
    # can be helpful for some tools. This is all highly experimental and could be
    # removed at any time.

    from mypy.refinfo import get_undocumented_ref_info_json

    if not state.tree:
        # We need a full AST for this.
        return

    _, data_file, _ = get_cache_names(state.id, state.xpath, options)
    ref_info_file = ".".join(data_file.split(".")[:-2]) + ".refs.json"
    assert not ref_info_file.startswith(".")

    deps_json = get_undocumented_ref_info_json(state.tree, type_map)
    metastore.write(ref_info_file, json.dumps(deps_json, separators=(",", ":")))
</t>
<t tx="ekr.20230831011819.176">@path mypy
"""Mypy type checker."""
&lt;&lt; checker.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.177">
from __future__ import annotations

import itertools
from collections import defaultdict
from contextlib import contextmanager, nullcontext
from typing import (
    AbstractSet,
    Callable,
    Dict,
    Final,
    Generic,
    Iterable,
    Iterator,
    Mapping,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
    TypeVar,
    Union,
    cast,
    overload,
)
from typing_extensions import TypeAlias as _TypeAlias

import mypy.checkexpr
from mypy import errorcodes as codes, message_registry, nodes, operators
from mypy.binder import ConditionalTypeBinder, Frame, get_declaration
from mypy.checkmember import (
    MemberContext,
    analyze_decorator_or_funcbase_access,
    analyze_descriptor_access,
    analyze_member_access,
    type_object_type,
)
from mypy.checkpattern import PatternChecker
from mypy.constraints import SUPERTYPE_OF
from mypy.erasetype import erase_type, erase_typevars, remove_instance_last_known_values
from mypy.errorcodes import TYPE_VAR, UNUSED_AWAITABLE, UNUSED_COROUTINE, ErrorCode
from mypy.errors import Errors, ErrorWatcher, report_internal_error
from mypy.expandtype import expand_self_type, expand_type, expand_type_by_instance
from mypy.join import join_types
from mypy.literals import Key, extract_var_from_literal_hash, literal, literal_hash
from mypy.maptype import map_instance_to_supertype
from mypy.meet import is_overlapping_erased_types, is_overlapping_types
from mypy.message_registry import ErrorMessage
from mypy.messages import (
    SUGGESTED_TEST_FIXTURES,
    MessageBuilder,
    append_invariance_notes,
    format_type,
    format_type_bare,
    format_type_distinctly,
    make_inferred_type_note,
    pretty_seq,
)
from mypy.mro import MroError, calculate_mro
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    CONTRAVARIANT,
    COVARIANT,
    FUNC_NO_INFO,
    GDEF,
    IMPLICITLY_ABSTRACT,
    INVARIANT,
    IS_ABSTRACT,
    LDEF,
    LITERAL_TYPE,
    MDEF,
    NOT_ABSTRACT,
    AssertStmt,
    AssignmentExpr,
    AssignmentStmt,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    Context,
    ContinueStmt,
    Decorator,
    DelStmt,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    FloatExpr,
    ForStmt,
    FuncBase,
    FuncDef,
    FuncItem,
    IfStmt,
    Import,
    ImportAll,
    ImportBase,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListExpr,
    Lvalue,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    PassStmt,
    PromoteExpr,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    StarExpr,
    Statement,
    StrExpr,
    SymbolNode,
    SymbolTable,
    SymbolTableNode,
    TempNode,
    TryStmt,
    TupleExpr,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    YieldExpr,
    is_final_node,
)
from mypy.options import Options
from mypy.patterns import AsPattern, StarredPattern
from mypy.plugin import CheckerPluginInterface, Plugin
from mypy.plugins import dataclasses as dataclasses_plugin
from mypy.scope import Scope
from mypy.semanal import is_trivial_body, refers_to_fullname, set_callable_name
from mypy.semanal_enum import ENUM_BASES, ENUM_SPECIAL_PROPS
from mypy.sharedparse import BINARY_MAGIC_METHODS
from mypy.state import state
from mypy.subtypes import (
    find_member,
    is_callable_compatible,
    is_equivalent,
    is_more_precise,
    is_proper_subtype,
    is_same_type,
    is_subtype,
    restrict_subtype_away,
    unify_generic_callable,
)
from mypy.traverser import TraverserVisitor, all_return_statements, has_return_statement
from mypy.treetransform import TransformVisitor
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type, make_optional_type
from mypy.typeops import (
    bind_self,
    coerce_to_literal,
    custom_special_method,
    erase_def_to_union_or_bound,
    erase_to_bound,
    erase_to_union_or_bound,
    false_only,
    fixup_partial_type,
    function_type,
    get_type_vars,
    is_literal_type_like,
    is_singleton_type,
    make_simplified_union,
    map_type_from_supertype,
    true_only,
    try_expanding_sum_type_to_union,
    try_getting_int_literals_from_type,
    try_getting_str_literals,
    try_getting_str_literals_from_type,
    tuple_fallback,
)
from mypy.types import (
    ANY_STRATEGY,
    MYPYC_NATIVE_INT_NAMES,
    OVERLOAD_NAMES,
    AnyType,
    BoolTypeQuery,
    CallableType,
    DeletedType,
    ErasedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeGuardedType,
    TypeOfAny,
    TypeTranslator,
    TypeType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    flatten_nested_unions,
    get_proper_type,
    get_proper_types,
    is_literal_type,
    is_named_instance,
)
from mypy.types_utils import is_overlapping_none, remove_optional, store_argument_type, strip_type
from mypy.typetraverser import TypeTraverserVisitor
from mypy.typevars import fill_typevars, fill_typevars_with_any, has_no_typevars
from mypy.util import is_dunder, is_sunder, is_typeshed_file
from mypy.visitor import NodeVisitor

T = TypeVar("T")

DEFAULT_LAST_PASS: Final = 1  # Pass numbers start at 0

DeferredNodeType: _TypeAlias = Union[FuncDef, LambdaExpr, OverloadedFuncDef, Decorator]
FineGrainedDeferredNodeType: _TypeAlias = Union[FuncDef, MypyFile, OverloadedFuncDef]


# A node which is postponed to be processed during the next pass.
# In normal mode one can defer functions and methods (also decorated and/or overloaded)
# and lambda expressions. Nested functions can't be deferred -- only top-level functions
# and methods of classes not defined within a function can be deferred.
</t>
<t tx="ekr.20230831011819.178">class DeferredNode(NamedTuple):
    node: DeferredNodeType
    # And its TypeInfo (for semantic analysis self type handling
    active_typeinfo: TypeInfo | None


</t>
<t tx="ekr.20230831011819.179"># Same as above, but for fine-grained mode targets. Only top-level functions/methods
# and module top levels are allowed as such.
class FineGrainedDeferredNode(NamedTuple):
    node: FineGrainedDeferredNodeType
    active_typeinfo: TypeInfo | None


</t>
<t tx="ekr.20230831011819.18">
from __future__ import annotations

from typing import TYPE_CHECKING, Callable, Sequence

from mypy import nodes
from mypy.maptype import map_instance_to_supertype
from mypy.types import (
    AnyType,
    Instance,
    ParamSpecType,
    TupleType,
    Type,
    TypedDictType,
    TypeOfAny,
    get_proper_type,
)

if TYPE_CHECKING:
    from mypy.infer import ArgumentInferContext


</t>
<t tx="ekr.20230831011819.180"># Data structure returned by find_isinstance_check representing
# information learned from the truth or falsehood of a condition.  The
# dict maps nodes representing expressions like 'a[0].x' to their
# refined types under the assumption that the condition has a
# particular truth value. A value of None means that the condition can
# never have that truth value.

# NB: The keys of this dict are nodes in the original source program,
# which are compared by reference equality--effectively, being *the
# same* expression of the program, not just two identical expressions
# (such as two references to the same variable). TODO: it would
# probably be better to have the dict keyed by the nodes' literal_hash
# field instead.
TypeMap: _TypeAlias = Optional[Dict[Expression, Type]]


# An object that represents either a precise type or a type with an upper bound;
# it is important for correct type inference with isinstance.
class TypeRange(NamedTuple):
    item: Type
    is_upper_bound: bool  # False =&gt; precise type


</t>
<t tx="ekr.20230831011819.181"># Keeps track of partial types in a single scope. In fine-grained incremental
# mode partial types initially defined at the top level cannot be completed in
# a function, and we use the 'is_function' attribute to enforce this.
class PartialTypeScope(NamedTuple):
    map: dict[Var, Context]
    is_function: bool
    is_local: bool


</t>
<t tx="ekr.20230831011819.182">class TypeChecker(NodeVisitor[None], CheckerPluginInterface):
    """Mypy type checker.

    Type check mypy source files that have been semantically analyzed.

    You must create a separate instance for each source file.
    """

    @others
</t>
<t tx="ekr.20230831011819.183"># Are we type checking a stub?
is_stub = False
# Error message reporter
errors: Errors
# Utility for generating messages
msg: MessageBuilder
# Types of type checked nodes. The first item is the "master" type
# map that will store the final, exported types. Additional items
# are temporary type maps used during type inference, and these
# will be eventually popped and either discarded or merged into
# the master type map.
#
# Avoid accessing this directly, but prefer the lookup_type(),
# has_type() etc. helpers instead.
_type_maps: list[dict[Expression, Type]]

# Helper for managing conditional types
binder: ConditionalTypeBinder
# Helper for type checking expressions
expr_checker: mypy.checkexpr.ExpressionChecker

pattern_checker: PatternChecker

tscope: Scope
scope: CheckerScope
# Stack of function return types
return_types: list[Type]
# Flags; true for dynamically typed functions
dynamic_funcs: list[bool]
# Stack of collections of variables with partial types
partial_types: list[PartialTypeScope]
# Vars for which partial type errors are already reported
# (to avoid logically duplicate errors with different error context).
partial_reported: set[Var]
globals: SymbolTable
modules: dict[str, MypyFile]
# Nodes that couldn't be checked because some types weren't available. We'll run
# another pass and try these again.
deferred_nodes: list[DeferredNode]
# Type checking pass number (0 = first pass)
pass_num = 0
# Last pass number to take
last_pass = DEFAULT_LAST_PASS
# Have we deferred the current function? If yes, don't infer additional
# types during this pass within the function.
current_node_deferred = False
# Is this file a typeshed stub?
is_typeshed_stub = False
options: Options
# Used for collecting inferred attribute types so that they can be checked
# for consistency.
inferred_attribute_types: dict[Var, Type] | None = None
# Don't infer partial None types if we are processing assignment from Union
no_partial_types: bool = False

# The set of all dependencies (suppressed or not) that this module accesses, either
# directly or indirectly.
module_refs: set[str]

# A map from variable nodes to a snapshot of the frame ids of the
# frames that were active when the variable was declared. This can
# be used to determine nearest common ancestor frame of a variable's
# declaration and the current frame, which lets us determine if it
# was declared in a different branch of the same `if` statement
# (if that frame is a conditional_frame).
var_decl_frames: dict[Var, set[int]]

# Plugin that provides special type checking rules for specific library
# functions such as open(), etc.
plugin: Plugin

def __init__(
    self,
    errors: Errors,
    modules: dict[str, MypyFile],
    options: Options,
    tree: MypyFile,
    path: str,
    plugin: Plugin,
    per_line_checking_time_ns: dict[int, int],
) -&gt; None:
    """Construct a type checker.

    Use errors to report type check errors.
    """
    self.errors = errors
    self.modules = modules
    self.options = options
    self.tree = tree
    self.path = path
    self.msg = MessageBuilder(errors, modules)
    self.plugin = plugin
    self.tscope = Scope()
    self.scope = CheckerScope(tree)
    self.binder = ConditionalTypeBinder()
    self.globals = tree.names
    self.return_types = []
    self.dynamic_funcs = []
    self.partial_types = []
    self.partial_reported = set()
    self.var_decl_frames = {}
    self.deferred_nodes = []
    self._type_maps = [{}]
    self.module_refs = set()
    self.pass_num = 0
    self.current_node_deferred = False
    self.is_stub = tree.is_stub
    self.is_typeshed_stub = is_typeshed_file(options.abs_custom_typeshed_dir, path)
    self.inferred_attribute_types = None

    # If True, process function definitions. If False, don't. This is used
    # for processing module top levels in fine-grained incremental mode.
    self.recurse_into_functions = True
    # This internal flag is used to track whether we a currently type-checking
    # a final declaration (assignment), so that some errors should be suppressed.
    # Should not be set manually, use get_final_context/enter_final_context instead.
    # NOTE: we use the context manager to avoid "threading" an additional `is_final_def`
    # argument through various `checker` and `checkmember` functions.
    self._is_final_def = False

    # This flag is set when we run type-check or attribute access check for the purpose
    # of giving a note on possibly missing "await". It is used to avoid infinite recursion.
    self.checking_missing_await = False

    # While this is True, allow passing an abstract class where Type[T] is expected.
    # although this is technically unsafe, this is desirable in some context, for
    # example when type-checking class decorators.
    self.allow_abstract_call = False

    # Child checker objects for specific AST node types
    self.expr_checker = mypy.checkexpr.ExpressionChecker(
        self, self.msg, self.plugin, per_line_checking_time_ns
    )
    self.pattern_checker = PatternChecker(self, self.msg, self.plugin, options)

</t>
<t tx="ekr.20230831011819.184">@property
def type_context(self) -&gt; list[Type | None]:
    return self.expr_checker.type_context

</t>
<t tx="ekr.20230831011819.185">def reset(self) -&gt; None:
    """Cleanup stale state that might be left over from a typechecking run.

    This allows us to reuse TypeChecker objects in fine-grained
    incremental mode.
    """
    # TODO: verify this is still actually worth it over creating new checkers
    self.partial_reported.clear()
    self.module_refs.clear()
    self.binder = ConditionalTypeBinder()
    self._type_maps[1:] = []
    self._type_maps[0].clear()
    self.temp_type_map = None
    self.expr_checker.reset()

    assert self.inferred_attribute_types is None
    assert self.partial_types == []
    assert self.deferred_nodes == []
    assert len(self.scope.stack) == 1
    assert self.partial_types == []

</t>
<t tx="ekr.20230831011819.186">def check_first_pass(self) -&gt; None:
    """Type check the entire file, but defer functions with unresolved references.

    Unresolved references are forward references to variables
    whose types haven't been inferred yet.  They may occur later
    in the same file or in a different file that's being processed
    later (usually due to an import cycle).

    Deferred functions will be processed by check_second_pass().
    """
    self.recurse_into_functions = True
    with state.strict_optional_set(self.options.strict_optional):
        self.errors.set_file(
            self.path, self.tree.fullname, scope=self.tscope, options=self.options
        )
        with self.tscope.module_scope(self.tree.fullname):
            with self.enter_partial_types(), self.binder.top_frame_context():
                for d in self.tree.defs:
                    if self.binder.is_unreachable():
                        if not self.should_report_unreachable_issues():
                            break
                        if not self.is_noop_for_reachability(d):
                            self.msg.unreachable_statement(d)
                            break
                    else:
                        self.accept(d)

            assert not self.current_node_deferred

            all_ = self.globals.get("__all__")
            if all_ is not None and all_.type is not None:
                all_node = all_.node
                assert all_node is not None
                seq_str = self.named_generic_type(
                    "typing.Sequence", [self.named_type("builtins.str")]
                )
                if not is_subtype(all_.type, seq_str):
                    str_seq_s, all_s = format_type_distinctly(
                        seq_str, all_.type, options=self.options
                    )
                    self.fail(
                        message_registry.ALL_MUST_BE_SEQ_STR.format(str_seq_s, all_s), all_node
                    )

</t>
<t tx="ekr.20230831011819.187">def check_second_pass(
    self, todo: Sequence[DeferredNode | FineGrainedDeferredNode] | None = None
) -&gt; bool:
    """Run second or following pass of type checking.

    This goes through deferred nodes, returning True if there were any.
    """
    self.recurse_into_functions = True
    with state.strict_optional_set(self.options.strict_optional):
        if not todo and not self.deferred_nodes:
            return False
        self.errors.set_file(
            self.path, self.tree.fullname, scope=self.tscope, options=self.options
        )
        with self.tscope.module_scope(self.tree.fullname):
            self.pass_num += 1
            if not todo:
                todo = self.deferred_nodes
            else:
                assert not self.deferred_nodes
            self.deferred_nodes = []
            done: set[DeferredNodeType | FineGrainedDeferredNodeType] = set()
            for node, active_typeinfo in todo:
                if node in done:
                    continue
                # This is useful for debugging:
                # print("XXX in pass %d, class %s, function %s" %
                #       (self.pass_num, type_name, node.fullname or node.name))
                done.add(node)
                with self.tscope.class_scope(
                    active_typeinfo
                ) if active_typeinfo else nullcontext():
                    with self.scope.push_class(
                        active_typeinfo
                    ) if active_typeinfo else nullcontext():
                        self.check_partial(node)
        return True

</t>
<t tx="ekr.20230831011819.188">def check_partial(self, node: DeferredNodeType | FineGrainedDeferredNodeType) -&gt; None:
    if isinstance(node, MypyFile):
        self.check_top_level(node)
    else:
        self.recurse_into_functions = True
        if isinstance(node, LambdaExpr):
            self.expr_checker.accept(node)
        else:
            self.accept(node)

</t>
<t tx="ekr.20230831011819.189">def check_top_level(self, node: MypyFile) -&gt; None:
    """Check only the top-level of a module, skipping function definitions."""
    self.recurse_into_functions = False
    with self.enter_partial_types():
        with self.binder.top_frame_context():
            for d in node.defs:
                d.accept(self)

    assert not self.current_node_deferred
</t>
<t tx="ekr.20230831011819.19">def map_actuals_to_formals(
    actual_kinds: list[nodes.ArgKind],
    actual_names: Sequence[str | None] | None,
    formal_kinds: list[nodes.ArgKind],
    formal_names: Sequence[str | None],
    actual_arg_type: Callable[[int], Type],
) -&gt; list[list[int]]:
    """Calculate mapping between actual (caller) args and formals.

    The result contains a list of caller argument indexes mapping to each
    callee argument index, indexed by callee index.

    The caller_arg_type argument should evaluate to the type of the actual
    argument type with the given index.
    """
    nformals = len(formal_kinds)
    formal_to_actual: list[list[int]] = [[] for i in range(nformals)]
    ambiguous_actual_kwargs: list[int] = []
    fi = 0
    for ai, actual_kind in enumerate(actual_kinds):
        if actual_kind == nodes.ARG_POS:
            if fi &lt; nformals:
                if not formal_kinds[fi].is_star():
                    formal_to_actual[fi].append(ai)
                    fi += 1
                elif formal_kinds[fi] == nodes.ARG_STAR:
                    formal_to_actual[fi].append(ai)
        elif actual_kind == nodes.ARG_STAR:
            # We need to know the actual type to map varargs.
            actualt = get_proper_type(actual_arg_type(ai))
            if isinstance(actualt, TupleType):
                # A tuple actual maps to a fixed number of formals.
                for _ in range(len(actualt.items)):
                    if fi &lt; nformals:
                        if formal_kinds[fi] != nodes.ARG_STAR2:
                            formal_to_actual[fi].append(ai)
                        else:
                            break
                        if formal_kinds[fi] != nodes.ARG_STAR:
                            fi += 1
            else:
                # Assume that it is an iterable (if it isn't, there will be
                # an error later).
                while fi &lt; nformals:
                    if formal_kinds[fi].is_named(star=True):
                        break
                    else:
                        formal_to_actual[fi].append(ai)
                    if formal_kinds[fi] == nodes.ARG_STAR:
                        break
                    fi += 1
        elif actual_kind.is_named():
            assert actual_names is not None, "Internal error: named kinds without names given"
            name = actual_names[ai]
            if name in formal_names:
                formal_to_actual[formal_names.index(name)].append(ai)
            elif nodes.ARG_STAR2 in formal_kinds:
                formal_to_actual[formal_kinds.index(nodes.ARG_STAR2)].append(ai)
        else:
            assert actual_kind == nodes.ARG_STAR2
            actualt = get_proper_type(actual_arg_type(ai))
            if isinstance(actualt, TypedDictType):
                for name in actualt.items:
                    if name in formal_names:
                        formal_to_actual[formal_names.index(name)].append(ai)
                    elif nodes.ARG_STAR2 in formal_kinds:
                        formal_to_actual[formal_kinds.index(nodes.ARG_STAR2)].append(ai)
            else:
                # We don't exactly know which **kwargs are provided by the
                # caller, so we'll defer until all the other unambiguous
                # actuals have been processed
                ambiguous_actual_kwargs.append(ai)

    if ambiguous_actual_kwargs:
        # Assume the ambiguous kwargs will fill the remaining arguments.
        #
        # TODO: If there are also tuple varargs, we might be missing some potential
        #       matches if the tuple was short enough to not match everything.
        unmatched_formals = [
            fi
            for fi in range(nformals)
            if (
                formal_names[fi]
                and (
                    not formal_to_actual[fi]
                    or actual_kinds[formal_to_actual[fi][0]] == nodes.ARG_STAR
                )
                and formal_kinds[fi] != nodes.ARG_STAR
            )
            or formal_kinds[fi] == nodes.ARG_STAR2
        ]
        for ai in ambiguous_actual_kwargs:
            for fi in unmatched_formals:
                formal_to_actual[fi].append(ai)

    return formal_to_actual


</t>
<t tx="ekr.20230831011819.190">    # TODO: Handle __all__

def defer_node(self, node: DeferredNodeType, enclosing_class: TypeInfo | None) -&gt; None:
    """Defer a node for processing during next type-checking pass.

    Args:
        node: function/method being deferred
        enclosing_class: for methods, the class where the method is defined
    NOTE: this can't handle nested functions/methods.
    """
    # We don't freeze the entire scope since only top-level functions and methods
    # can be deferred. Only module/class level scope information is needed.
    # Module-level scope information is preserved in the TypeChecker instance.
    self.deferred_nodes.append(DeferredNode(node, enclosing_class))

</t>
<t tx="ekr.20230831011819.191">def handle_cannot_determine_type(self, name: str, context: Context) -&gt; None:
    node = self.scope.top_non_lambda_function()
    if self.pass_num &lt; self.last_pass and isinstance(node, FuncDef):
        # Don't report an error yet. Just defer. Note that we don't defer
        # lambdas because they are coupled to the surrounding function
        # through the binder and the inferred type of the lambda, so it
        # would get messy.
        enclosing_class = self.scope.enclosing_class()
        self.defer_node(node, enclosing_class)
        # Set a marker so that we won't infer additional types in this
        # function. Any inferred types could be bogus, because there's at
        # least one type that we don't know.
        self.current_node_deferred = True
    else:
        self.msg.cannot_determine_type(name, context)

</t>
<t tx="ekr.20230831011819.192">def accept(self, stmt: Statement) -&gt; None:
    """Type check a node in the given type context."""
    try:
        stmt.accept(self)
    except Exception as err:
        report_internal_error(err, self.errors.file, stmt.line, self.errors, self.options)

</t>
<t tx="ekr.20230831011819.193">def accept_loop(
    self,
    body: Statement,
    else_body: Statement | None = None,
    *,
    exit_condition: Expression | None = None,
) -&gt; None:
    """Repeatedly type check a loop body until the frame doesn't change.
    If exit_condition is set, assume it must be False on exit from the loop.

    Then check the else_body.
    """
    # The outer frame accumulates the results of all iterations
    with self.binder.frame_context(can_skip=False, conditional_frame=True):
        while True:
            with self.binder.frame_context(can_skip=True, break_frame=2, continue_frame=1):
                self.accept(body)
            if not self.binder.last_pop_changed:
                break
        if exit_condition:
            _, else_map = self.find_isinstance_check(exit_condition)
            self.push_type_map(else_map)
        if else_body:
            self.accept(else_body)

</t>
<t tx="ekr.20230831011819.194">#
# Definitions
#

def visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.tscope.function_scope(defn):
        self._visit_overloaded_func_def(defn)

</t>
<t tx="ekr.20230831011819.195">def _visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    num_abstract = 0
    if not defn.items:
        # In this case we have already complained about none of these being
        # valid overloads.
        return None
    if len(defn.items) == 1:
        self.fail(message_registry.MULTIPLE_OVERLOADS_REQUIRED, defn)

    if defn.is_property:
        # HACK: Infer the type of the property.
        assert isinstance(defn.items[0], Decorator)
        self.visit_decorator(defn.items[0])
    for fdef in defn.items:
        assert isinstance(fdef, Decorator)
        if defn.is_property:
            self.check_func_item(fdef.func, name=fdef.func.name, allow_empty=True)
        else:
            # Perform full check for real overloads to infer type of all decorated
            # overload variants.
            self.visit_decorator_inner(fdef, allow_empty=True)
        if fdef.func.abstract_status in (IS_ABSTRACT, IMPLICITLY_ABSTRACT):
            num_abstract += 1
    if num_abstract not in (0, len(defn.items)):
        self.fail(message_registry.INCONSISTENT_ABSTRACT_OVERLOAD, defn)
    if defn.impl:
        defn.impl.accept(self)
    if not defn.is_property:
        self.check_overlapping_overloads(defn)
        if defn.type is None:
            item_types = []
            for item in defn.items:
                assert isinstance(item, Decorator)
                item_type = self.extract_callable_type(item.var.type, item)
                if item_type is not None:
                    item_types.append(item_type)
            if item_types:
                defn.type = Overloaded(item_types)
    # Check override validity after we analyzed current definition.
    if defn.info:
        found_method_base_classes = self.check_method_override(defn)
        if (
            defn.is_explicit_override
            and not found_method_base_classes
            and found_method_base_classes is not None
        ):
            self.msg.no_overridable_method(defn.name, defn)
        self.check_explicit_override_decorator(defn, found_method_base_classes, defn.impl)
        self.check_inplace_operator_method(defn)
    return None

</t>
<t tx="ekr.20230831011819.196">def extract_callable_type(self, inner_type: Type | None, ctx: Context) -&gt; CallableType | None:
    """Get type as seen by an overload item caller."""
    inner_type = get_proper_type(inner_type)
    outer_type: CallableType | None = None
    if inner_type is not None and not isinstance(inner_type, AnyType):
        if isinstance(inner_type, CallableType):
            outer_type = inner_type
        elif isinstance(inner_type, Instance):
            inner_call = get_proper_type(
                analyze_member_access(
                    name="__call__",
                    typ=inner_type,
                    context=ctx,
                    is_lvalue=False,
                    is_super=False,
                    is_operator=True,
                    msg=self.msg,
                    original_type=inner_type,
                    chk=self,
                )
            )
            if isinstance(inner_call, CallableType):
                outer_type = inner_call
        if outer_type is None:
            self.msg.not_callable(inner_type, ctx)
    return outer_type

</t>
<t tx="ekr.20230831011819.197">def check_overlapping_overloads(self, defn: OverloadedFuncDef) -&gt; None:
    # At this point we should have set the impl already, and all remaining
    # items are decorators

    if self.msg.errors.file in self.msg.errors.ignored_files:
        # This is a little hacky, however, the quadratic check here is really expensive, this
        # method has no side effects, so we should skip it if we aren't going to report
        # anything. In some other places we swallow errors in stubs, but this error is very
        # useful for stubs!
        return

    # Compute some info about the implementation (if it exists) for use below
    impl_type: CallableType | None = None
    if defn.impl:
        if isinstance(defn.impl, FuncDef):
            inner_type: Type | None = defn.impl.type
        elif isinstance(defn.impl, Decorator):
            inner_type = defn.impl.var.type
        else:
            assert False, "Impl isn't the right type"

        # This can happen if we've got an overload with a different
        # decorator or if the implementation is untyped -- we gave up on the types.
        impl_type = self.extract_callable_type(inner_type, defn.impl)

    is_descriptor_get = defn.info and defn.name == "__get__"
    for i, item in enumerate(defn.items):
        assert isinstance(item, Decorator)
        sig1 = self.extract_callable_type(item.var.type, item)
        if sig1 is None:
            continue

        for j, item2 in enumerate(defn.items[i + 1 :]):
            assert isinstance(item2, Decorator)
            sig2 = self.extract_callable_type(item2.var.type, item2)
            if sig2 is None:
                continue

            if not are_argument_counts_overlapping(sig1, sig2):
                continue

            if overload_can_never_match(sig1, sig2):
                self.msg.overloaded_signature_will_never_match(i + 1, i + j + 2, item2.func)
            elif not is_descriptor_get:
                # Note: we force mypy to check overload signatures in strict-optional mode
                # so we don't incorrectly report errors when a user tries typing an overload
                # that happens to have a 'if the argument is None' fallback.
                #
                # For example, the following is fine in strict-optional mode but would throw
                # the unsafe overlap error when strict-optional is disabled:
                #
                #     @overload
                #     def foo(x: None) -&gt; int: ...
                #     @overload
                #     def foo(x: str) -&gt; str: ...
                #
                # See Python 2's map function for a concrete example of this kind of overload.
                current_class = self.scope.active_class()
                type_vars = current_class.defn.type_vars if current_class else []
                with state.strict_optional_set(True):
                    if is_unsafe_overlapping_overload_signatures(sig1, sig2, type_vars):
                        self.msg.overloaded_signatures_overlap(i + 1, i + j + 2, item.func)

        if impl_type is not None:
            assert defn.impl is not None

            # We perform a unification step that's very similar to what
            # 'is_callable_compatible' would have done if we had set
            # 'unify_generics' to True -- the only difference is that
            # we check and see if the impl_type's return value is a
            # *supertype* of the overload alternative, not a *subtype*.
            #
            # This is to match the direction the implementation's return
            # needs to be compatible in.
            if impl_type.variables:
                impl: CallableType | None = unify_generic_callable(
                    # Normalize both before unifying
                    impl_type.with_unpacked_kwargs(),
                    sig1.with_unpacked_kwargs(),
                    ignore_return=False,
                    return_constraint_direction=SUPERTYPE_OF,
                )
                if impl is None:
                    self.msg.overloaded_signatures_typevar_specific(i + 1, defn.impl)
                    continue
            else:
                impl = impl_type

            # Prevent extra noise from inconsistent use of @classmethod by copying
            # the first arg from the method being checked against.
            if sig1.arg_types and defn.info:
                impl = impl.copy_modified(arg_types=[sig1.arg_types[0]] + impl.arg_types[1:])

            # Is the overload alternative's arguments subtypes of the implementation's?
            if not is_callable_compatible(
                impl, sig1, is_compat=is_subtype, ignore_return=True
            ):
                self.msg.overloaded_signatures_arg_specific(i + 1, defn.impl)

            # Is the overload alternative's return type a subtype of the implementation's?
            if not (
                is_subtype(sig1.ret_type, impl.ret_type)
                or is_subtype(impl.ret_type, sig1.ret_type)
            ):
                self.msg.overloaded_signatures_ret_specific(i + 1, defn.impl)

</t>
<t tx="ekr.20230831011819.198"># Here's the scoop about generators and coroutines.
#
# There are two kinds of generators: classic generators (functions
# with `yield` or `yield from` in the body) and coroutines
# (functions declared with `async def`).  The latter are specified
# in PEP 492 and only available in Python &gt;= 3.5.
#
# Classic generators can be parameterized with three types:
# - ty is the Yield type (the type of y in `yield y`)
# - tc is the type reCeived by yield (the type of c in `c = yield`).
# - tr is the Return type (the type of r in `return r`)
#
# A classic generator must define a return type that's either
# `Generator[ty, tc, tr]`, Iterator[ty], or Iterable[ty] (or
# object or Any).  If tc/tr are not given, both are None.
#
# A coroutine must define a return type corresponding to tr; the
# other two are unconstrained.  The "external" return type (seen
# by the caller) is Awaitable[tr].
#
# In addition, there's the synthetic type AwaitableGenerator: it
# inherits from both Awaitable and Generator and can be used both
# in `yield from` and in `await`.  This type is set automatically
# for functions decorated with `@types.coroutine` or
# `@asyncio.coroutine`.  Its single parameter corresponds to tr.
#
# PEP 525 adds a new type, the asynchronous generator, which was
# first released in Python 3.6. Async generators are `async def`
# functions that can also `yield` values. They can be parameterized
# with two types, ty and tc, because they cannot return a value.
#
# There are several useful methods, each taking a type t and a
# flag c indicating whether it's for a generator or coroutine:
#
# - is_generator_return_type(t, c) returns whether t is a Generator,
#   Iterator, Iterable (if not c), or Awaitable (if c), or
#   AwaitableGenerator (regardless of c).
# - is_async_generator_return_type(t) returns whether t is an
#   AsyncGenerator.
# - get_generator_yield_type(t, c) returns ty.
# - get_generator_receive_type(t, c) returns tc.
# - get_generator_return_type(t, c) returns tr.

def is_generator_return_type(self, typ: Type, is_coroutine: bool) -&gt; bool:
    """Is `typ` a valid type for a generator/coroutine?

    True if `typ` is a *supertype* of Generator or Awaitable.
    Also true it it's *exactly* AwaitableGenerator (modulo type parameters).
    """
    typ = get_proper_type(typ)
    if is_coroutine:
        # This means we're in Python 3.5 or later.
        at = self.named_generic_type("typing.Awaitable", [AnyType(TypeOfAny.special_form)])
        if is_subtype(at, typ):
            return True
    else:
        any_type = AnyType(TypeOfAny.special_form)
        gt = self.named_generic_type("typing.Generator", [any_type, any_type, any_type])
        if is_subtype(gt, typ):
            return True
    return isinstance(typ, Instance) and typ.type.fullname == "typing.AwaitableGenerator"

</t>
<t tx="ekr.20230831011819.199">def is_async_generator_return_type(self, typ: Type) -&gt; bool:
    """Is `typ` a valid type for an async generator?

    True if `typ` is a supertype of AsyncGenerator.
    """
    try:
        any_type = AnyType(TypeOfAny.special_form)
        agt = self.named_generic_type("typing.AsyncGenerator", [any_type, any_type])
    except KeyError:
        # we're running on a version of typing that doesn't have AsyncGenerator yet
        return False
    return is_subtype(agt, typ)

</t>
<t tx="ekr.20230831011819.20">def map_formals_to_actuals(
    actual_kinds: list[nodes.ArgKind],
    actual_names: Sequence[str | None] | None,
    formal_kinds: list[nodes.ArgKind],
    formal_names: list[str | None],
    actual_arg_type: Callable[[int], Type],
) -&gt; list[list[int]]:
    """Calculate the reverse mapping of map_actuals_to_formals."""
    formal_to_actual = map_actuals_to_formals(
        actual_kinds, actual_names, formal_kinds, formal_names, actual_arg_type
    )
    # Now reverse the mapping.
    actual_to_formal: list[list[int]] = [[] for _ in actual_kinds]
    for formal, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            actual_to_formal[actual].append(formal)
    return actual_to_formal


</t>
<t tx="ekr.20230831011819.200">def get_generator_yield_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given the declared return type of a generator (t), return the type it yields (ty)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif isinstance(return_type, UnionType):
        return make_simplified_union(
            [self.get_generator_yield_type(item, is_coroutine) for item in return_type.items]
        )
    elif not self.is_generator_return_type(
        return_type, is_coroutine
    ) and not self.is_async_generator_return_type(return_type):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == "typing.Awaitable":
        # Awaitable: ty is Any.
        return AnyType(TypeOfAny.special_form)
    elif return_type.args:
        # AwaitableGenerator, Generator, AsyncGenerator, Iterator, or Iterable; ty is args[0].
        ret_type = return_type.args[0]
        # TODO not best fix, better have dedicated yield token
        return ret_type
    else:
        # If the function's declared supertype of Generator has no type
        # parameters (i.e. is `object`), then the yielded values can't
        # be accessed so any type is acceptable.  IOW, ty is Any.
        # (However, see https://github.com/python/mypy/issues/1933)
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.201">def get_generator_receive_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given a declared generator return type (t), return the type its yield receives (tc)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif isinstance(return_type, UnionType):
        return make_simplified_union(
            [self.get_generator_receive_type(item, is_coroutine) for item in return_type.items]
        )
    elif not self.is_generator_return_type(
        return_type, is_coroutine
    ) and not self.is_async_generator_return_type(return_type):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == "typing.Awaitable":
        # Awaitable, AwaitableGenerator: tc is Any.
        return AnyType(TypeOfAny.special_form)
    elif (
        return_type.type.fullname in ("typing.Generator", "typing.AwaitableGenerator")
        and len(return_type.args) &gt;= 3
    ):
        # Generator: tc is args[1].
        return return_type.args[1]
    elif return_type.type.fullname == "typing.AsyncGenerator" and len(return_type.args) &gt;= 2:
        return return_type.args[1]
    else:
        # `return_type` is a supertype of Generator, so callers won't be able to send it
        # values.  IOW, tc is None.
        return NoneType()

</t>
<t tx="ekr.20230831011819.202">def get_coroutine_return_type(self, return_type: Type) -&gt; Type:
    return_type = get_proper_type(return_type)
    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    assert isinstance(return_type, Instance), "Should only be called on coroutine functions."
    # Note: return type is the 3rd type parameter of Coroutine.
    return return_type.args[2]

</t>
<t tx="ekr.20230831011819.203">def get_generator_return_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given the declared return type of a generator (t), return the type it returns (tr)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif isinstance(return_type, UnionType):
        return make_simplified_union(
            [self.get_generator_return_type(item, is_coroutine) for item in return_type.items]
        )
    elif not self.is_generator_return_type(return_type, is_coroutine):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == "typing.Awaitable" and len(return_type.args) == 1:
        # Awaitable: tr is args[0].
        return return_type.args[0]
    elif (
        return_type.type.fullname in ("typing.Generator", "typing.AwaitableGenerator")
        and len(return_type.args) &gt;= 3
    ):
        # AwaitableGenerator, Generator: tr is args[2].
        return return_type.args[2]
    else:
        # Supertype of Generator (Iterator, Iterable, object): tr is any.
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.204">def visit_func_def(self, defn: FuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.tscope.function_scope(defn):
        self._visit_func_def(defn)

</t>
<t tx="ekr.20230831011819.205">def _visit_func_def(self, defn: FuncDef) -&gt; None:
    """Type check a function definition."""
    self.check_func_item(defn, name=defn.name)
    if defn.info:
        if not defn.is_dynamic() and not defn.is_overload and not defn.is_decorated:
            # If the definition is the implementation for an
            # overload, the legality of the override has already
            # been typechecked, and decorated methods will be
            # checked when the decorator is.
            found_method_base_classes = self.check_method_override(defn)
            self.check_explicit_override_decorator(defn, found_method_base_classes)
        self.check_inplace_operator_method(defn)
    if defn.original_def:
        # Override previous definition.
        new_type = self.function_type(defn)
        if isinstance(defn.original_def, FuncDef):
            # Function definition overrides function definition.
            old_type = self.function_type(defn.original_def)
            if not is_same_type(new_type, old_type):
                self.msg.incompatible_conditional_function_def(defn, old_type, new_type)
        else:
            # Function definition overrides a variable initialized via assignment or a
            # decorated function.
            orig_type = defn.original_def.type
            if orig_type is None:
                # If other branch is unreachable, we don't type check it and so we might
                # not have a type for the original definition
                return
            if isinstance(orig_type, PartialType):
                if orig_type.type is None:
                    # Ah this is a partial type. Give it the type of the function.
                    orig_def = defn.original_def
                    if isinstance(orig_def, Decorator):
                        var = orig_def.var
                    else:
                        var = orig_def
                    partial_types = self.find_partial_types(var)
                    if partial_types is not None:
                        var.type = new_type
                        del partial_types[var]
                else:
                    # Trying to redefine something like partial empty list as function.
                    self.fail(message_registry.INCOMPATIBLE_REDEFINITION, defn)
            else:
                name_expr = NameExpr(defn.name)
                name_expr.node = defn.original_def
                self.binder.assign_type(name_expr, new_type, orig_type)
                self.check_subtype(
                    new_type,
                    orig_type,
                    defn,
                    message_registry.INCOMPATIBLE_REDEFINITION,
                    "redefinition with type",
                    "original type",
                )

</t>
<t tx="ekr.20230831011819.206">def check_func_item(
    self,
    defn: FuncItem,
    type_override: CallableType | None = None,
    name: str | None = None,
    allow_empty: bool = False,
) -&gt; None:
    """Type check a function.

    If type_override is provided, use it as the function type.
    """
    self.dynamic_funcs.append(defn.is_dynamic() and not type_override)

    with self.enter_partial_types(is_function=True):
        typ = self.function_type(defn)
        if type_override:
            typ = type_override.copy_modified(line=typ.line, column=typ.column)
        if isinstance(typ, CallableType):
            with self.enter_attribute_inference_context():
                self.check_func_def(defn, typ, name, allow_empty)
        else:
            raise RuntimeError("Not supported")

    self.dynamic_funcs.pop()
    self.current_node_deferred = False

    if name == "__exit__":
        self.check__exit__return_type(defn)
    if name == "__post_init__":
        if dataclasses_plugin.is_processed_dataclass(defn.info):
            dataclasses_plugin.check_post_init(self, defn, defn.info)

</t>
<t tx="ekr.20230831011819.207">@contextmanager
def enter_attribute_inference_context(self) -&gt; Iterator[None]:
    old_types = self.inferred_attribute_types
    self.inferred_attribute_types = {}
    yield None
    self.inferred_attribute_types = old_types

</t>
<t tx="ekr.20230831011819.208">def check_func_def(
    self, defn: FuncItem, typ: CallableType, name: str | None, allow_empty: bool = False
) -&gt; None:
    """Type check a function definition."""
    # Expand type variables with value restrictions to ordinary types.
    expanded = self.expand_typevars(defn, typ)
    original_typ = typ
    for item, typ in expanded:
        old_binder = self.binder
        self.binder = ConditionalTypeBinder()
        with self.binder.top_frame_context():
            defn.expanded.append(item)

            # We may be checking a function definition or an anonymous
            # function. In the first case, set up another reference with the
            # precise type.
            if isinstance(item, FuncDef):
                fdef = item
                # Check if __init__ has an invalid return type.
                if (
                    fdef.info
                    and fdef.name in ("__init__", "__init_subclass__")
                    and not isinstance(
                        get_proper_type(typ.ret_type), (NoneType, UninhabitedType)
                    )
                    and not self.dynamic_funcs[-1]
                ):
                    self.fail(
                        message_registry.MUST_HAVE_NONE_RETURN_TYPE.format(fdef.name), item
                    )

                # Check validity of __new__ signature
                if fdef.info and fdef.name == "__new__":
                    self.check___new___signature(fdef, typ)

                self.check_for_missing_annotations(fdef)
                if self.options.disallow_any_unimported:
                    if fdef.type and isinstance(fdef.type, CallableType):
                        ret_type = fdef.type.ret_type
                        if has_any_from_unimported_type(ret_type):
                            self.msg.unimported_type_becomes_any("Return type", ret_type, fdef)
                        for idx, arg_type in enumerate(fdef.type.arg_types):
                            if has_any_from_unimported_type(arg_type):
                                prefix = f'Argument {idx + 1} to "{fdef.name}"'
                                self.msg.unimported_type_becomes_any(prefix, arg_type, fdef)
                check_for_explicit_any(
                    fdef.type, self.options, self.is_typeshed_stub, self.msg, context=fdef
                )

            if name:  # Special method names
                if defn.info and self.is_reverse_op_method(name):
                    self.check_reverse_op_method(item, typ, name, defn)
                elif name in ("__getattr__", "__getattribute__"):
                    self.check_getattr_method(typ, defn, name)
                elif name == "__setattr__":
                    self.check_setattr_method(typ, defn)

            # Refuse contravariant return type variable
            if isinstance(typ.ret_type, TypeVarType):
                if typ.ret_type.variance == CONTRAVARIANT:
                    self.fail(
                        message_registry.RETURN_TYPE_CANNOT_BE_CONTRAVARIANT, typ.ret_type
                    )
                self.check_unbound_return_typevar(typ)
            elif (
                isinstance(original_typ.ret_type, TypeVarType) and original_typ.ret_type.values
            ):
                # Since type vars with values are expanded, the return type is changed
                # to a raw value. This is a hack to get it back.
                self.check_unbound_return_typevar(original_typ)

            # Check that Generator functions have the appropriate return type.
            if defn.is_generator:
                if defn.is_async_generator:
                    if not self.is_async_generator_return_type(typ.ret_type):
                        self.fail(
                            message_registry.INVALID_RETURN_TYPE_FOR_ASYNC_GENERATOR, typ
                        )
                else:
                    if not self.is_generator_return_type(typ.ret_type, defn.is_coroutine):
                        self.fail(message_registry.INVALID_RETURN_TYPE_FOR_GENERATOR, typ)

            # Fix the type if decorated with `@types.coroutine` or `@asyncio.coroutine`.
            if defn.is_awaitable_coroutine:
                # Update the return type to AwaitableGenerator.
                # (This doesn't exist in typing.py, only in typing.pyi.)
                t = typ.ret_type
                c = defn.is_coroutine
                ty = self.get_generator_yield_type(t, c)
                tc = self.get_generator_receive_type(t, c)
                if c:
                    tr = self.get_coroutine_return_type(t)
                else:
                    tr = self.get_generator_return_type(t, c)
                ret_type = self.named_generic_type(
                    "typing.AwaitableGenerator", [ty, tc, tr, t]
                )
                typ = typ.copy_modified(ret_type=ret_type)
                defn.type = typ

            # Push return type.
            self.return_types.append(typ.ret_type)

            # Store argument types.
            for i in range(len(typ.arg_types)):
                arg_type = typ.arg_types[i]
                with self.scope.push_function(defn):
                    # We temporary push the definition to get the self type as
                    # visible from *inside* of this function/method.
                    ref_type: Type | None = self.scope.active_self_type()
                if (
                    isinstance(defn, FuncDef)
                    and ref_type is not None
                    and i == 0
                    and (not defn.is_static or defn.name == "__new__")
                    and typ.arg_kinds[0] not in [nodes.ARG_STAR, nodes.ARG_STAR2]
                ):
                    if defn.is_class or defn.name == "__new__":
                        ref_type = mypy.types.TypeType.make_normalized(ref_type)
                    erased = get_proper_type(erase_to_bound(arg_type))
                    if not is_subtype(ref_type, erased, ignore_type_params=True):
                        if (
                            isinstance(erased, Instance)
                            and erased.type.is_protocol
                            or isinstance(erased, TypeType)
                            and isinstance(erased.item, Instance)
                            and erased.item.type.is_protocol
                        ):
                            # We allow the explicit self-type to be not a supertype of
                            # the current class if it is a protocol. For such cases
                            # the consistency check will be performed at call sites.
                            msg = None
                        elif typ.arg_names[i] in {"self", "cls"}:
                            msg = message_registry.ERASED_SELF_TYPE_NOT_SUPERTYPE.format(
                                erased.str_with_options(self.options),
                                ref_type.str_with_options(self.options),
                            )
                        else:
                            msg = message_registry.MISSING_OR_INVALID_SELF_TYPE
                        if msg:
                            self.fail(msg, defn)
                elif isinstance(arg_type, TypeVarType):
                    # Refuse covariant parameter type variables
                    # TODO: check recursively for inner type variables
                    if (
                        arg_type.variance == COVARIANT
                        and defn.name not in ("__init__", "__new__", "__post_init__")
                        and not is_private(defn.name)  # private methods are not inherited
                    ):
                        ctx: Context = arg_type
                        if ctx.line &lt; 0:
                            ctx = typ
                        self.fail(message_registry.FUNCTION_PARAMETER_CANNOT_BE_COVARIANT, ctx)
                # Need to store arguments again for the expanded item.
                store_argument_type(item, i, typ, self.named_generic_type)

            # Type check initialization expressions.
            body_is_trivial = is_trivial_body(defn.body)
            self.check_default_args(item, body_is_trivial)

        # Type check body in a new scope.
        with self.binder.top_frame_context():
            # Copy some type narrowings from an outer function when it seems safe enough
            # (i.e. we can't find an assignment that might change the type of the
            # variable afterwards).
            new_frame: Frame | None = None
            for frame in old_binder.frames:
                for key, narrowed_type in frame.types.items():
                    key_var = extract_var_from_literal_hash(key)
                    if key_var is not None and not self.is_var_redefined_in_outer_context(
                        key_var, defn.line
                    ):
                        # It seems safe to propagate the type narrowing to a nested scope.
                        if new_frame is None:
                            new_frame = self.binder.push_frame()
                        new_frame.types[key] = narrowed_type
                        self.binder.declarations[key] = old_binder.declarations[key]
            with self.scope.push_function(defn):
                # We suppress reachability warnings for empty generator functions
                # (return; yield) which have a "yield" that's unreachable by definition
                # since it's only there to promote the function into a generator function.
                #
                # We also suppress reachability warnings when we use TypeVars with value
                # restrictions: we only want to report a warning if a certain statement is
                # marked as being suppressed in *all* of the expansions, but we currently
                # have no good way of doing this.
                #
                # TODO: Find a way of working around this limitation
                if _is_empty_generator_function(item) or len(expanded) &gt;= 2:
                    self.binder.suppress_unreachable_warnings()
                self.accept(item.body)
            unreachable = self.binder.is_unreachable()
            if new_frame is not None:
                self.binder.pop_frame(True, 0)

        if not unreachable:
            if defn.is_generator or is_named_instance(
                self.return_types[-1], "typing.AwaitableGenerator"
            ):
                return_type = self.get_generator_return_type(
                    self.return_types[-1], defn.is_coroutine
                )
            elif defn.is_coroutine:
                return_type = self.get_coroutine_return_type(self.return_types[-1])
            else:
                return_type = self.return_types[-1]
            return_type = get_proper_type(return_type)

            allow_empty = allow_empty or self.options.allow_empty_bodies

            show_error = (
                not body_is_trivial
                or
                # Allow empty bodies for abstract methods, overloads, in tests and stubs.
                (
                    not allow_empty
                    and not (
                        isinstance(defn, FuncDef) and defn.abstract_status != NOT_ABSTRACT
                    )
                    and not self.is_stub
                )
            )

            # Ignore plugin generated methods, these usually don't need any bodies.
            if defn.info is not FUNC_NO_INFO and (
                defn.name not in defn.info.names or defn.info.names[defn.name].plugin_generated
            ):
                show_error = False

            # Ignore also definitions that appear in `if TYPE_CHECKING: ...` blocks.
            # These can't be called at runtime anyway (similar to plugin-generated).
            if isinstance(defn, FuncDef) and defn.is_mypy_only:
                show_error = False

            # We want to minimize the fallout from checking empty bodies
            # that was absent in many mypy versions.
            if body_is_trivial and is_subtype(NoneType(), return_type):
                show_error = False

            may_be_abstract = (
                body_is_trivial
                and defn.info is not FUNC_NO_INFO
                and defn.info.metaclass_type is not None
                and defn.info.metaclass_type.type.has_base("abc.ABCMeta")
            )

            if self.options.warn_no_return:
                if (
                    not self.current_node_deferred
                    and not isinstance(return_type, (NoneType, AnyType))
                    and show_error
                ):
                    # Control flow fell off the end of a function that was
                    # declared to return a non-None type.
                    if isinstance(return_type, UninhabitedType):
                        # This is a NoReturn function
                        msg = message_registry.INVALID_IMPLICIT_RETURN
                    else:
                        msg = message_registry.MISSING_RETURN_STATEMENT
                    if body_is_trivial:
                        msg = msg._replace(code=codes.EMPTY_BODY)
                    self.fail(msg, defn)
                    if may_be_abstract:
                        self.note(message_registry.EMPTY_BODY_ABSTRACT, defn)
            elif show_error:
                msg = message_registry.INCOMPATIBLE_RETURN_VALUE_TYPE
                if body_is_trivial:
                    msg = msg._replace(code=codes.EMPTY_BODY)
                # similar to code in check_return_stmt
                if (
                    not self.check_subtype(
                        subtype_label="implicitly returns",
                        subtype=NoneType(),
                        supertype_label="expected",
                        supertype=return_type,
                        context=defn,
                        msg=msg,
                    )
                    and may_be_abstract
                ):
                    self.note(message_registry.EMPTY_BODY_ABSTRACT, defn)

        self.return_types.pop()

        self.binder = old_binder

</t>
<t tx="ekr.20230831011819.209">def is_var_redefined_in_outer_context(self, v: Var, after_line: int) -&gt; bool:
    """Can the variable be assigned to at module top level or outer function?

    Note that this doesn't do a full CFG analysis but uses a line number based
    heuristic that isn't correct in some (rare) cases.
    """
    outers = self.tscope.outer_functions()
    if not outers:
        # Top-level function -- outer context is top level, and we can't reason about
        # globals
        return True
    for outer in outers:
        if isinstance(outer, FuncDef):
            if find_last_var_assignment_line(outer.body, v) &gt;= after_line:
                return True
    return False

</t>
<t tx="ekr.20230831011819.21">class ArgTypeExpander:
    """Utility class for mapping actual argument types to formal arguments.

    One of the main responsibilities is to expand caller tuple *args and TypedDict
    **kwargs, and to keep track of which tuple/TypedDict items have already been
    consumed.

    Example:

       def f(x: int, *args: str) -&gt; None: ...
       f(*(1, 'x', 1.1))

    We'd call expand_actual_type three times:

      1. The first call would provide 'int' as the actual type of 'x' (from '1').
      2. The second call would provide 'str' as one of the actual types for '*args'.
      2. The third call would provide 'float' as one of the actual types for '*args'.

    A single instance can process all the arguments for a single call. Each call
    needs a separate instance since instances have per-call state.
    """

    @others
</t>
<t tx="ekr.20230831011819.210">def check_unbound_return_typevar(self, typ: CallableType) -&gt; None:
    """Fails when the return typevar is not defined in arguments."""
    if isinstance(typ.ret_type, TypeVarType) and typ.ret_type in typ.variables:
        arg_type_visitor = CollectArgTypeVarTypes()
        for argtype in typ.arg_types:
            argtype.accept(arg_type_visitor)

        if typ.ret_type not in arg_type_visitor.arg_types:
            self.fail(message_registry.UNBOUND_TYPEVAR, typ.ret_type, code=TYPE_VAR)
            upper_bound = get_proper_type(typ.ret_type.upper_bound)
            if not (
                isinstance(upper_bound, Instance)
                and upper_bound.type.fullname == "builtins.object"
            ):
                self.note(
                    "Consider using the upper bound "
                    f"{format_type(typ.ret_type.upper_bound, self.options)} instead",
                    context=typ.ret_type,
                )

</t>
<t tx="ekr.20230831011819.211">def check_default_args(self, item: FuncItem, body_is_trivial: bool) -&gt; None:
    for arg in item.arguments:
        if arg.initializer is None:
            continue
        if body_is_trivial and isinstance(arg.initializer, EllipsisExpr):
            continue
        name = arg.variable.name
        msg = "Incompatible default for "
        if name.startswith("__tuple_arg_"):
            msg += f"tuple argument {name[12:]}"
        else:
            msg += f'argument "{name}"'
        if (
            not self.options.implicit_optional
            and isinstance(arg.initializer, NameExpr)
            and arg.initializer.fullname == "builtins.None"
        ):
            notes = [
                "PEP 484 prohibits implicit Optional. "
                "Accordingly, mypy has changed its default to no_implicit_optional=True",
                "Use https://github.com/hauntsaninja/no_implicit_optional to automatically "
                "upgrade your codebase",
            ]
        else:
            notes = None
        self.check_simple_assignment(
            arg.variable.type,
            arg.initializer,
            context=arg.initializer,
            msg=ErrorMessage(msg, code=codes.ASSIGNMENT),
            lvalue_name="argument",
            rvalue_name="default",
            notes=notes,
        )

</t>
<t tx="ekr.20230831011819.212">def is_forward_op_method(self, method_name: str) -&gt; bool:
    return method_name in operators.reverse_op_methods

</t>
<t tx="ekr.20230831011819.213">def is_reverse_op_method(self, method_name: str) -&gt; bool:
    return method_name in operators.reverse_op_method_set

</t>
<t tx="ekr.20230831011819.214">def check_for_missing_annotations(self, fdef: FuncItem) -&gt; None:
    # Check for functions with unspecified/not fully specified types.
    def is_unannotated_any(t: Type) -&gt; bool:
        if not isinstance(t, ProperType):
            return False
        return isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated

    has_explicit_annotation = isinstance(fdef.type, CallableType) and any(
        not is_unannotated_any(t) for t in fdef.type.arg_types + [fdef.type.ret_type]
    )

    show_untyped = not self.is_typeshed_stub or self.options.warn_incomplete_stub
    check_incomplete_defs = self.options.disallow_incomplete_defs and has_explicit_annotation
    if show_untyped and (self.options.disallow_untyped_defs or check_incomplete_defs):
        if fdef.type is None and self.options.disallow_untyped_defs:
            if not fdef.arguments or (
                len(fdef.arguments) == 1
                and (fdef.arg_names[0] == "self" or fdef.arg_names[0] == "cls")
            ):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
                if not has_return_statement(fdef) and not fdef.is_generator:
                    self.note(
                        'Use "-&gt; None" if function does not return a value',
                        fdef,
                        code=codes.NO_UNTYPED_DEF,
                    )
            else:
                self.fail(message_registry.FUNCTION_TYPE_EXPECTED, fdef)
        elif isinstance(fdef.type, CallableType):
            ret_type = get_proper_type(fdef.type.ret_type)
            if is_unannotated_any(ret_type):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_generator:
                if is_unannotated_any(
                    self.get_generator_return_type(ret_type, fdef.is_coroutine)
                ):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_coroutine and isinstance(ret_type, Instance):
                if is_unannotated_any(self.get_coroutine_return_type(ret_type)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            if any(is_unannotated_any(t) for t in fdef.type.arg_types):
                self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)

</t>
<t tx="ekr.20230831011819.215">def check___new___signature(self, fdef: FuncDef, typ: CallableType) -&gt; None:
    self_type = fill_typevars_with_any(fdef.info)
    bound_type = bind_self(typ, self_type, is_classmethod=True)
    # Check that __new__ (after binding cls) returns an instance
    # type (or any).
    if fdef.info.is_metaclass():
        # This is a metaclass, so it must return a new unrelated type.
        self.check_subtype(
            bound_type.ret_type,
            self.type_type(),
            fdef,
            message_registry.INVALID_NEW_TYPE,
            "returns",
            "but must return a subtype of",
        )
    elif not isinstance(
        get_proper_type(bound_type.ret_type), (AnyType, Instance, TupleType, UninhabitedType)
    ):
        self.fail(
            message_registry.NON_INSTANCE_NEW_TYPE.format(
                format_type(bound_type.ret_type, self.options)
            ),
            fdef,
        )
    else:
        # And that it returns a subtype of the class
        self.check_subtype(
            bound_type.ret_type,
            self_type,
            fdef,
            message_registry.INVALID_NEW_TYPE,
            "returns",
            "but must return a subtype of",
        )

</t>
<t tx="ekr.20230831011819.216">def check_reverse_op_method(
    self, defn: FuncItem, reverse_type: CallableType, reverse_name: str, context: Context
) -&gt; None:
    """Check a reverse operator method such as __radd__."""
    # Decides whether it's worth calling check_overlapping_op_methods().

    # This used to check for some very obscure scenario.  It now
    # just decides whether it's worth calling
    # check_overlapping_op_methods().

    assert defn.info

    # First check for a valid signature
    method_type = CallableType(
        [AnyType(TypeOfAny.special_form), AnyType(TypeOfAny.special_form)],
        [nodes.ARG_POS, nodes.ARG_POS],
        [None, None],
        AnyType(TypeOfAny.special_form),
        self.named_type("builtins.function"),
    )
    if not is_subtype(reverse_type, method_type):
        self.msg.invalid_signature(reverse_type, context)
        return

    if reverse_name in ("__eq__", "__ne__"):
        # These are defined for all objects =&gt; can't cause trouble.
        return

    # With 'Any' or 'object' return type we are happy, since any possible
    # return value is valid.
    ret_type = get_proper_type(reverse_type.ret_type)
    if isinstance(ret_type, AnyType):
        return
    if isinstance(ret_type, Instance):
        if ret_type.type.fullname == "builtins.object":
            return
    if reverse_type.arg_kinds[0] == ARG_STAR:
        reverse_type = reverse_type.copy_modified(
            arg_types=[reverse_type.arg_types[0]] * 2,
            arg_kinds=[ARG_POS] * 2,
            arg_names=[reverse_type.arg_names[0], "_"],
        )
    assert len(reverse_type.arg_types) &gt;= 2

    forward_name = operators.normal_from_reverse_op[reverse_name]
    forward_inst = get_proper_type(reverse_type.arg_types[1])
    if isinstance(forward_inst, TypeVarType):
        forward_inst = get_proper_type(forward_inst.upper_bound)
    elif isinstance(forward_inst, TupleType):
        forward_inst = tuple_fallback(forward_inst)
    elif isinstance(forward_inst, (FunctionLike, TypedDictType, LiteralType)):
        forward_inst = forward_inst.fallback
    if isinstance(forward_inst, TypeType):
        item = forward_inst.item
        if isinstance(item, Instance):
            opt_meta = item.type.metaclass_type
            if opt_meta is not None:
                forward_inst = opt_meta

    def has_readable_member(typ: UnionType | Instance, name: str) -&gt; bool:
        # TODO: Deal with attributes of TupleType etc.
        if isinstance(typ, Instance):
            return typ.type.has_readable_member(name)
        return all(
            (isinstance(x, UnionType) and has_readable_member(x, name))
            or (isinstance(x, Instance) and x.type.has_readable_member(name))
            for x in get_proper_types(typ.relevant_items())
        )

    if not (
        isinstance(forward_inst, (Instance, UnionType))
        and has_readable_member(forward_inst, forward_name)
    ):
        return
    forward_base = reverse_type.arg_types[1]
    forward_type = self.expr_checker.analyze_external_member_access(
        forward_name, forward_base, context=defn
    )
    self.check_overlapping_op_methods(
        reverse_type,
        reverse_name,
        defn.info,
        forward_type,
        forward_name,
        forward_base,
        context=defn,
    )

</t>
<t tx="ekr.20230831011819.217">def check_overlapping_op_methods(
    self,
    reverse_type: CallableType,
    reverse_name: str,
    reverse_class: TypeInfo,
    forward_type: Type,
    forward_name: str,
    forward_base: Type,
    context: Context,
) -&gt; None:
    """Check for overlapping method and reverse method signatures.

    This function assumes that:

    -   The reverse method has valid argument count and kinds.
    -   If the reverse operator method accepts some argument of type
        X, the forward operator method also belong to class X.

        For example, if we have the reverse operator `A.__radd__(B)`, then the
        corresponding forward operator must have the type `B.__add__(...)`.
    """

    # Note: Suppose we have two operator methods "A.__rOP__(B) -&gt; R1" and
    # "B.__OP__(C) -&gt; R2". We check if these two methods are unsafely overlapping
    # by using the following algorithm:
    #
    # 1. Rewrite "B.__OP__(C) -&gt; R1"  to "temp1(B, C) -&gt; R1"
    #
    # 2. Rewrite "A.__rOP__(B) -&gt; R2" to "temp2(B, A) -&gt; R2"
    #
    # 3. Treat temp1 and temp2 as if they were both variants in the same
    #    overloaded function. (This mirrors how the Python runtime calls
    #    operator methods: we first try __OP__, then __rOP__.)
    #
    #    If the first signature is unsafely overlapping with the second,
    #    report an error.
    #
    # 4. However, if temp1 shadows temp2 (e.g. the __rOP__ method can never
    #    be called), do NOT report an error.
    #
    #    This behavior deviates from how we handle overloads -- many of the
    #    modules in typeshed seem to define __OP__ methods that shadow the
    #    corresponding __rOP__ method.
    #
    # Note: we do not attempt to handle unsafe overlaps related to multiple
    # inheritance. (This is consistent with how we handle overloads: we also
    # do not try checking unsafe overlaps due to multiple inheritance there.)

    for forward_item in flatten_nested_unions([forward_type]):
        forward_item = get_proper_type(forward_item)
        if isinstance(forward_item, CallableType):
            if self.is_unsafe_overlapping_op(forward_item, forward_base, reverse_type):
                self.msg.operator_method_signatures_overlap(
                    reverse_class, reverse_name, forward_base, forward_name, context
                )
        elif isinstance(forward_item, Overloaded):
            for item in forward_item.items:
                if self.is_unsafe_overlapping_op(item, forward_base, reverse_type):
                    self.msg.operator_method_signatures_overlap(
                        reverse_class, reverse_name, forward_base, forward_name, context
                    )
        elif not isinstance(forward_item, AnyType):
            self.msg.forward_operator_not_callable(forward_name, context)

</t>
<t tx="ekr.20230831011819.218">def is_unsafe_overlapping_op(
    self, forward_item: CallableType, forward_base: Type, reverse_type: CallableType
) -&gt; bool:
    # TODO: check argument kinds?
    if len(forward_item.arg_types) &lt; 1:
        # Not a valid operator method -- can't succeed anyway.
        return False

    # Erase the type if necessary to make sure we don't have a single
    # TypeVar in forward_tweaked. (Having a function signature containing
    # just a single TypeVar can lead to unpredictable behavior.)
    forward_base_erased = forward_base
    if isinstance(forward_base, TypeVarType):
        forward_base_erased = erase_to_bound(forward_base)

    # Construct normalized function signatures corresponding to the
    # operator methods. The first argument is the left operand and the
    # second operand is the right argument -- we switch the order of
    # the arguments of the reverse method.

    forward_tweaked = forward_item.copy_modified(
        arg_types=[forward_base_erased, forward_item.arg_types[0]],
        arg_kinds=[nodes.ARG_POS] * 2,
        arg_names=[None] * 2,
    )
    reverse_tweaked = reverse_type.copy_modified(
        arg_types=[reverse_type.arg_types[1], reverse_type.arg_types[0]],
        arg_kinds=[nodes.ARG_POS] * 2,
        arg_names=[None] * 2,
    )

    reverse_base_erased = reverse_type.arg_types[0]
    if isinstance(reverse_base_erased, TypeVarType):
        reverse_base_erased = erase_to_bound(reverse_base_erased)

    if is_same_type(reverse_base_erased, forward_base_erased):
        return False
    elif is_subtype(reverse_base_erased, forward_base_erased):
        first = reverse_tweaked
        second = forward_tweaked
    else:
        first = forward_tweaked
        second = reverse_tweaked

    current_class = self.scope.active_class()
    type_vars = current_class.defn.type_vars if current_class else []
    return is_unsafe_overlapping_overload_signatures(first, second, type_vars)

</t>
<t tx="ekr.20230831011819.219">def check_inplace_operator_method(self, defn: FuncBase) -&gt; None:
    """Check an inplace operator method such as __iadd__.

    They cannot arbitrarily overlap with __add__.
    """
    method = defn.name
    if method not in operators.inplace_operator_methods:
        return
    typ = bind_self(self.function_type(defn))
    cls = defn.info
    other_method = "__" + method[3:]
    if cls.has_readable_member(other_method):
        instance = fill_typevars(cls)
        typ2 = get_proper_type(
            self.expr_checker.analyze_external_member_access(other_method, instance, defn)
        )
        fail = False
        if isinstance(typ2, FunctionLike):
            if not is_more_general_arg_prefix(typ, typ2):
                fail = True
        else:
            # TODO overloads
            fail = True
        if fail:
            self.msg.signatures_incompatible(method, other_method, defn)

</t>
<t tx="ekr.20230831011819.22">def __init__(self, context: ArgumentInferContext) -&gt; None:
    # Next tuple *args index to use.
    self.tuple_index = 0
    # Keyword arguments in TypedDict **kwargs used.
    self.kwargs_used: set[str] = set()
    # Type context for `*` and `**` arg kinds.
    self.context = context

</t>
<t tx="ekr.20230831011819.220">def check_getattr_method(self, typ: Type, context: Context, name: str) -&gt; None:
    if len(self.scope.stack) == 1:
        # module scope
        if name == "__getattribute__":
            self.fail(message_registry.MODULE_LEVEL_GETATTRIBUTE, context)
            return
        # __getattr__ is fine at the module level as of Python 3.7 (PEP 562). We could
        # show an error for Python &lt; 3.7, but that would be annoying in code that supports
        # both 3.7 and older versions.
        method_type = CallableType(
            [self.named_type("builtins.str")],
            [nodes.ARG_POS],
            [None],
            AnyType(TypeOfAny.special_form),
            self.named_type("builtins.function"),
        )
    elif self.scope.active_class():
        method_type = CallableType(
            [AnyType(TypeOfAny.special_form), self.named_type("builtins.str")],
            [nodes.ARG_POS, nodes.ARG_POS],
            [None, None],
            AnyType(TypeOfAny.special_form),
            self.named_type("builtins.function"),
        )
    else:
        return
    if not is_subtype(typ, method_type):
        self.msg.invalid_signature_for_special_method(typ, context, name)

</t>
<t tx="ekr.20230831011819.221">def check_setattr_method(self, typ: Type, context: Context) -&gt; None:
    if not self.scope.active_class():
        return
    method_type = CallableType(
        [
            AnyType(TypeOfAny.special_form),
            self.named_type("builtins.str"),
            AnyType(TypeOfAny.special_form),
        ],
        [nodes.ARG_POS, nodes.ARG_POS, nodes.ARG_POS],
        [None, None, None],
        NoneType(),
        self.named_type("builtins.function"),
    )
    if not is_subtype(typ, method_type):
        self.msg.invalid_signature_for_special_method(typ, context, "__setattr__")

</t>
<t tx="ekr.20230831011819.222">def check_slots_definition(self, typ: Type, context: Context) -&gt; None:
    """Check the type of __slots__."""
    str_type = self.named_type("builtins.str")
    expected_type = UnionType(
        [str_type, self.named_generic_type("typing.Iterable", [str_type])]
    )
    self.check_subtype(
        typ,
        expected_type,
        context,
        message_registry.INVALID_TYPE_FOR_SLOTS,
        "actual type",
        "expected type",
        code=codes.ASSIGNMENT,
    )

</t>
<t tx="ekr.20230831011819.223">def check_match_args(self, var: Var, typ: Type, context: Context) -&gt; None:
    """Check that __match_args__ contains literal strings"""
    if not self.scope.active_class():
        return
    typ = get_proper_type(typ)
    if not isinstance(typ, TupleType) or not all(
        [is_string_literal(item) for item in typ.items]
    ):
        self.msg.note(
            "__match_args__ must be a tuple containing string literals for checking "
            "of match statements to work",
            context,
            code=codes.LITERAL_REQ,
        )

</t>
<t tx="ekr.20230831011819.224">def expand_typevars(
    self, defn: FuncItem, typ: CallableType
) -&gt; list[tuple[FuncItem, CallableType]]:
    # TODO use generator
    subst: list[list[tuple[TypeVarId, Type]]] = []
    tvars = list(typ.variables) or []
    if defn.info:
        # Class type variables
        tvars += defn.info.defn.type_vars or []
    # TODO(PEP612): audit for paramspec
    for tvar in tvars:
        if isinstance(tvar, TypeVarType) and tvar.values:
            subst.append([(tvar.id, value) for value in tvar.values])
    # Make a copy of the function to check for each combination of
    # value restricted type variables. (Except when running mypyc,
    # where we need one canonical version of the function.)
    if subst and not (self.options.mypyc or self.options.inspections):
        result: list[tuple[FuncItem, CallableType]] = []
        for substitutions in itertools.product(*subst):
            mapping = dict(substitutions)
            result.append((expand_func(defn, mapping), expand_type(typ, mapping)))
        return result
    else:
        return [(defn, typ)]

</t>
<t tx="ekr.20230831011819.225">def check_explicit_override_decorator(
    self,
    defn: FuncDef | OverloadedFuncDef,
    found_method_base_classes: list[TypeInfo] | None,
    context: Context | None = None,
) -&gt; None:
    if (
        found_method_base_classes
        and not defn.is_explicit_override
        and defn.name not in ("__init__", "__new__")
    ):
        self.msg.explicit_override_decorator_missing(
            defn.name, found_method_base_classes[0].fullname, context or defn
        )

</t>
<t tx="ekr.20230831011819.226">def check_method_override(
    self, defn: FuncDef | OverloadedFuncDef | Decorator
) -&gt; list[TypeInfo] | None:
    """Check if function definition is compatible with base classes.

    This may defer the method if a signature is not available in at least one base class.
    Return ``None`` if that happens.

    Return a list of base classes which contain an attribute with the method name.
    """
    # Check against definitions in base classes.
    found_method_base_classes: list[TypeInfo] = []
    for base in defn.info.mro[1:]:
        result = self.check_method_or_accessor_override_for_base(defn, base)
        if result is None:
            # Node was deferred, we will have another attempt later.
            return None
        if result:
            found_method_base_classes.append(base)
    return found_method_base_classes

</t>
<t tx="ekr.20230831011819.227">def check_method_or_accessor_override_for_base(
    self, defn: FuncDef | OverloadedFuncDef | Decorator, base: TypeInfo
) -&gt; bool | None:
    """Check if method definition is compatible with a base class.

    Return ``None`` if the node was deferred because one of the corresponding
    superclass nodes is not ready.

    Return ``True`` if an attribute with the method name was found in the base class.
    """
    found_base_method = False
    if base:
        name = defn.name
        base_attr = base.names.get(name)
        if base_attr:
            # First, check if we override a final (always an error, even with Any types).
            if is_final_node(base_attr.node):
                self.msg.cant_override_final(name, base.name, defn)
            # Second, final can't override anything writeable independently of types.
            if defn.is_final:
                self.check_if_final_var_override_writable(name, base_attr.node, defn)
            found_base_method = True

        # Check the type of override.
        if name not in ("__init__", "__new__", "__init_subclass__", "__post_init__"):
            # Check method override
            # (__init__, __new__, __init_subclass__ are special).
            if self.check_method_override_for_base_with_name(defn, name, base):
                return None
            if name in operators.inplace_operator_methods:
                # Figure out the name of the corresponding operator method.
                method = "__" + name[3:]
                # An inplace operator method such as __iadd__ might not be
                # always introduced safely if a base class defined __add__.
                # TODO can't come up with an example where this is
                #      necessary; now it's "just in case"
                if self.check_method_override_for_base_with_name(defn, method, base):
                    return None
    return found_base_method

</t>
<t tx="ekr.20230831011819.228">def check_method_override_for_base_with_name(
    self, defn: FuncDef | OverloadedFuncDef | Decorator, name: str, base: TypeInfo
) -&gt; bool:
    """Check if overriding an attribute `name` of `base` with `defn` is valid.

    Return True if the supertype node was not analysed yet, and `defn` was deferred.
    """
    base_attr = base.names.get(name)
    if base_attr:
        # The name of the method is defined in the base class.

        # Point errors at the 'def' line (important for backward compatibility
        # of type ignores).
        if not isinstance(defn, Decorator):
            context = defn
        else:
            context = defn.func

        # Construct the type of the overriding method.
        # TODO: this logic is much less complete than similar one in checkmember.py
        if isinstance(defn, (FuncDef, OverloadedFuncDef)):
            typ: Type = self.function_type(defn)
            override_class_or_static = defn.is_class or defn.is_static
            override_class = defn.is_class
        else:
            assert defn.var.is_ready
            assert defn.var.type is not None
            typ = defn.var.type
            override_class_or_static = defn.func.is_class or defn.func.is_static
            override_class = defn.func.is_class
        typ = get_proper_type(typ)
        if isinstance(typ, FunctionLike) and not is_static(context):
            typ = bind_self(typ, self.scope.active_self_type(), is_classmethod=override_class)
        # Map the overridden method type to subtype context so that
        # it can be checked for compatibility.
        original_type = get_proper_type(base_attr.type)
        original_node = base_attr.node
        # `original_type` can be partial if (e.g.) it is originally an
        # instance variable from an `__init__` block that becomes deferred.
        if original_type is None or isinstance(original_type, PartialType):
            if self.pass_num &lt; self.last_pass:
                # If there are passes left, defer this node until next pass,
                # otherwise try reconstructing the method type from available information.
                self.defer_node(defn, defn.info)
                return True
            elif isinstance(original_node, (FuncDef, OverloadedFuncDef)):
                original_type = self.function_type(original_node)
            elif isinstance(original_node, Decorator):
                original_type = self.function_type(original_node.func)
            elif isinstance(original_node, Var):
                # Super type can define method as an attribute.
                # See https://github.com/python/mypy/issues/10134

                # We also check that sometimes `original_node.type` is None.
                # This is the case when we use something like `__hash__ = None`.
                if original_node.type is not None:
                    original_type = get_proper_type(original_node.type)
                else:
                    original_type = NoneType()
            else:
                # Will always fail to typecheck below, since we know the node is a method
                original_type = NoneType()
        if isinstance(original_node, (FuncDef, OverloadedFuncDef)):
            original_class_or_static = original_node.is_class or original_node.is_static
        elif isinstance(original_node, Decorator):
            fdef = original_node.func
            original_class_or_static = fdef.is_class or fdef.is_static
        else:
            original_class_or_static = False  # a variable can't be class or static

        if isinstance(original_type, FunctionLike):
            original_type = self.bind_and_map_method(base_attr, original_type, defn.info, base)
            if original_node and is_property(original_node):
                original_type = get_property_type(original_type)

        if isinstance(typ, FunctionLike) and is_property(defn):
            typ = get_property_type(typ)
            if (
                isinstance(original_node, Var)
                and not original_node.is_final
                and (not original_node.is_property or original_node.is_settable_property)
                and isinstance(defn, Decorator)
            ):
                # We only give an error where no other similar errors will be given.
                if not isinstance(original_type, AnyType):
                    self.msg.fail(
                        "Cannot override writeable attribute with read-only property",
                        # Give an error on function line to match old behaviour.
                        defn.func,
                        code=codes.OVERRIDE,
                    )

        if isinstance(original_type, AnyType) or isinstance(typ, AnyType):
            pass
        elif isinstance(original_type, FunctionLike) and isinstance(typ, FunctionLike):
            # Check that the types are compatible.
            # TODO overloaded signatures
            self.check_override(
                typ,
                original_type,
                defn.name,
                name,
                base.name,
                original_class_or_static,
                override_class_or_static,
                context,
            )
        elif is_equivalent(original_type, typ):
            # Assume invariance for a non-callable attribute here. Note
            # that this doesn't affect read-only properties which can have
            # covariant overrides.
            #
            pass
        elif (
            original_node
            and not self.is_writable_attribute(original_node)
            and is_subtype(typ, original_type)
        ):
            # If the attribute is read-only, allow covariance
            pass
        else:
            self.msg.signature_incompatible_with_supertype(
                defn.name, name, base.name, context, original=original_type, override=typ
            )
    return False

</t>
<t tx="ekr.20230831011819.229">def bind_and_map_method(
    self, sym: SymbolTableNode, typ: FunctionLike, sub_info: TypeInfo, super_info: TypeInfo
) -&gt; FunctionLike:
    """Bind self-type and map type variables for a method.

    Arguments:
        sym: a symbol that points to method definition
        typ: method type on the definition
        sub_info: class where the method is used
        super_info: class where the method was defined
    """
    if isinstance(sym.node, (FuncDef, OverloadedFuncDef, Decorator)) and not is_static(
        sym.node
    ):
        if isinstance(sym.node, Decorator):
            is_class_method = sym.node.func.is_class
        else:
            is_class_method = sym.node.is_class

        mapped_typ = cast(FunctionLike, map_type_from_supertype(typ, sub_info, super_info))
        active_self_type = self.scope.active_self_type()
        if isinstance(mapped_typ, Overloaded) and active_self_type:
            # If we have an overload, filter to overloads that match the self type.
            # This avoids false positives for concrete subclasses of generic classes,
            # see testSelfTypeOverrideCompatibility for an example.
            filtered_items = []
            for item in mapped_typ.items:
                if not item.arg_types:
                    filtered_items.append(item)
                item_arg = item.arg_types[0]
                if isinstance(item_arg, TypeVarType):
                    item_arg = item_arg.upper_bound
                if is_subtype(active_self_type, item_arg):
                    filtered_items.append(item)
            # If we don't have any filtered_items, maybe it's always a valid override
            # of the superclass? However if you get to that point you're in murky type
            # territory anyway, so we just preserve the type and have the behaviour match
            # that of older versions of mypy.
            if filtered_items:
                mapped_typ = Overloaded(filtered_items)

        return bind_self(mapped_typ, active_self_type, is_class_method)
    else:
        return cast(FunctionLike, map_type_from_supertype(typ, sub_info, super_info))

</t>
<t tx="ekr.20230831011819.23">def expand_actual_type(
    self,
    actual_type: Type,
    actual_kind: nodes.ArgKind,
    formal_name: str | None,
    formal_kind: nodes.ArgKind,
) -&gt; Type:
    """Return the actual (caller) type(s) of a formal argument with the given kinds.

    If the actual argument is a tuple *args, return the next individual tuple item that
    maps to the formal arg.

    If the actual argument is a TypedDict **kwargs, return the next matching typed dict
    value type based on formal argument name and kind.

    This is supposed to be called for each formal, in order. Call multiple times per
    formal if multiple actuals map to a formal.
    """
    original_actual = actual_type
    actual_type = get_proper_type(actual_type)
    if actual_kind == nodes.ARG_STAR:
        if isinstance(actual_type, Instance) and actual_type.args:
            from mypy.subtypes import is_subtype

            if is_subtype(actual_type, self.context.iterable_type):
                return map_instance_to_supertype(
                    actual_type, self.context.iterable_type.type
                ).args[0]
            else:
                # We cannot properly unpack anything other
                # than `Iterable` type with `*`.
                # Just return `Any`, other parts of code would raise
                # a different error for improper use.
                return AnyType(TypeOfAny.from_error)
        elif isinstance(actual_type, TupleType):
            # Get the next tuple item of a tuple *arg.
            if self.tuple_index &gt;= len(actual_type.items):
                # Exhausted a tuple -- continue to the next *args.
                self.tuple_index = 1
            else:
                self.tuple_index += 1
            return actual_type.items[self.tuple_index - 1]
        elif isinstance(actual_type, ParamSpecType):
            # ParamSpec is valid in *args but it can't be unpacked.
            return actual_type
        else:
            return AnyType(TypeOfAny.from_error)
    elif actual_kind == nodes.ARG_STAR2:
        from mypy.subtypes import is_subtype

        if isinstance(actual_type, TypedDictType):
            if formal_kind != nodes.ARG_STAR2 and formal_name in actual_type.items:
                # Lookup type based on keyword argument name.
                assert formal_name is not None
            else:
                # Pick an arbitrary item if no specified keyword is expected.
                formal_name = (set(actual_type.items.keys()) - self.kwargs_used).pop()
            self.kwargs_used.add(formal_name)
            return actual_type.items[formal_name]
        elif (
            isinstance(actual_type, Instance)
            and len(actual_type.args) &gt; 1
            and is_subtype(actual_type, self.context.mapping_type)
        ):
            # Only `Mapping` type can be unpacked with `**`.
            # Other types will produce an error somewhere else.
            return map_instance_to_supertype(actual_type, self.context.mapping_type.type).args[
                1
            ]
        elif isinstance(actual_type, ParamSpecType):
            # ParamSpec is valid in **kwargs but it can't be unpacked.
            return actual_type
        else:
            return AnyType(TypeOfAny.from_error)
    else:
        # No translation for other kinds -- 1:1 mapping.
        return original_actual
</t>
<t tx="ekr.20230831011819.230">def get_op_other_domain(self, tp: FunctionLike) -&gt; Type | None:
    if isinstance(tp, CallableType):
        if tp.arg_kinds and tp.arg_kinds[0] == ARG_POS:
            return tp.arg_types[0]
        return None
    elif isinstance(tp, Overloaded):
        raw_items = [self.get_op_other_domain(it) for it in tp.items]
        items = [it for it in raw_items if it]
        if items:
            return make_simplified_union(items)
        return None
    else:
        assert False, "Need to check all FunctionLike subtypes here"

</t>
<t tx="ekr.20230831011819.231">def check_override(
    self,
    override: FunctionLike,
    original: FunctionLike,
    name: str,
    name_in_super: str,
    supertype: str,
    original_class_or_static: bool,
    override_class_or_static: bool,
    node: Context,
) -&gt; None:
    """Check a method override with given signatures.

    Arguments:
      override:  The signature of the overriding method.
      original:  The signature of the original supertype method.
      name:      The name of the subtype. This and the next argument are
                 only used for generating error messages.
      supertype: The name of the supertype.
    """
    # Use boolean variable to clarify code.
    fail = False
    op_method_wider_note = False
    if not is_subtype(override, original, ignore_pos_arg_names=True):
        fail = True
    elif isinstance(override, Overloaded) and self.is_forward_op_method(name):
        # Operator method overrides cannot extend the domain, as
        # this could be unsafe with reverse operator methods.
        original_domain = self.get_op_other_domain(original)
        override_domain = self.get_op_other_domain(override)
        if (
            original_domain
            and override_domain
            and not is_subtype(override_domain, original_domain)
        ):
            fail = True
            op_method_wider_note = True
    if isinstance(override, FunctionLike):
        if original_class_or_static and not override_class_or_static:
            fail = True
        elif isinstance(original, CallableType) and isinstance(override, CallableType):
            if original.type_guard is not None and override.type_guard is None:
                fail = True

    if is_private(name):
        fail = False

    if fail:
        emitted_msg = False

        # Normalize signatures, so we get better diagnostics.
        if isinstance(override, (CallableType, Overloaded)):
            override = override.with_unpacked_kwargs()
        if isinstance(original, (CallableType, Overloaded)):
            original = original.with_unpacked_kwargs()

        if (
            isinstance(override, CallableType)
            and isinstance(original, CallableType)
            and len(override.arg_types) == len(original.arg_types)
            and override.min_args == original.min_args
        ):
            # Give more detailed messages for the common case of both
            # signatures having the same number of arguments and no
            # overloads.

            # override might have its own generic function type
            # variables. If an argument or return type of override
            # does not have the correct subtyping relationship
            # with the original type even after these variables
            # are erased, then it is definitely an incompatibility.

            override_ids = override.type_var_ids()
            type_name = None
            if isinstance(override.definition, FuncDef):
                type_name = override.definition.info.name

            def erase_override(t: Type) -&gt; Type:
                return erase_typevars(t, ids_to_erase=override_ids)

            for i in range(len(override.arg_types)):
                if not is_subtype(
                    original.arg_types[i], erase_override(override.arg_types[i])
                ):
                    arg_type_in_super = original.arg_types[i]

                    if isinstance(node, FuncDef):
                        context: Context = node.arguments[i + len(override.bound_args)]
                    else:
                        context = node
                    self.msg.argument_incompatible_with_supertype(
                        i + 1,
                        name,
                        type_name,
                        name_in_super,
                        arg_type_in_super,
                        supertype,
                        context,
                        secondary_context=node,
                    )
                    emitted_msg = True

            if not is_subtype(erase_override(override.ret_type), original.ret_type):
                self.msg.return_type_incompatible_with_supertype(
                    name, name_in_super, supertype, original.ret_type, override.ret_type, node
                )
                emitted_msg = True
        elif isinstance(override, Overloaded) and isinstance(original, Overloaded):
            # Give a more detailed message in the case where the user is trying to
            # override an overload, and the subclass's overload is plausible, except
            # that the order of the variants are wrong.
            #
            # For example, if the parent defines the overload f(int) -&gt; int and f(str) -&gt; str
            # (in that order), and if the child swaps the two and does f(str) -&gt; str and
            # f(int) -&gt; int
            order = []
            for child_variant in override.items:
                for i, parent_variant in enumerate(original.items):
                    if is_subtype(child_variant, parent_variant):
                        order.append(i)
                        break

            if len(order) == len(original.items) and order != sorted(order):
                self.msg.overload_signature_incompatible_with_supertype(
                    name, name_in_super, supertype, node
                )
                emitted_msg = True

        if not emitted_msg:
            # Fall back to generic incompatibility message.
            self.msg.signature_incompatible_with_supertype(
                name, name_in_super, supertype, node, original=original, override=override
            )
        if op_method_wider_note:
            self.note(
                "Overloaded operator methods can't have wider argument types in overrides",
                node,
                code=codes.OVERRIDE,
            )

</t>
<t tx="ekr.20230831011819.232">def check__exit__return_type(self, defn: FuncItem) -&gt; None:
    """Generate error if the return type of __exit__ is problematic.

    If __exit__ always returns False but the return type is declared
    as bool, mypy thinks that a with statement may "swallow"
    exceptions even though this is not the case, resulting in
    invalid reachability inference.
    """
    if not defn.type or not isinstance(defn.type, CallableType):
        return

    ret_type = get_proper_type(defn.type.ret_type)
    if not has_bool_item(ret_type):
        return

    returns = all_return_statements(defn)
    if not returns:
        return

    if all(
        isinstance(ret.expr, NameExpr) and ret.expr.fullname == "builtins.False"
        for ret in returns
    ):
        self.msg.incorrect__exit__return(defn)

</t>
<t tx="ekr.20230831011819.233">def visit_class_def(self, defn: ClassDef) -&gt; None:
    """Type check a class definition."""
    typ = defn.info
    for base in typ.mro[1:]:
        if base.is_final:
            self.fail(message_registry.CANNOT_INHERIT_FROM_FINAL.format(base.name), defn)
    with self.tscope.class_scope(defn.info), self.enter_partial_types(is_class=True):
        old_binder = self.binder
        self.binder = ConditionalTypeBinder()
        with self.binder.top_frame_context():
            with self.scope.push_class(defn.info):
                self.accept(defn.defs)
        self.binder = old_binder
        if not (defn.info.typeddict_type or defn.info.tuple_type or defn.info.is_enum):
            # If it is not a normal class (not a special form) check class keywords.
            self.check_init_subclass(defn)
        if not defn.has_incompatible_baseclass:
            # Otherwise we've already found errors; more errors are not useful
            self.check_multiple_inheritance(typ)
        self.check_metaclass_compatibility(typ)
        self.check_final_deletable(typ)

        if defn.decorators:
            sig: Type = type_object_type(defn.info, self.named_type)
            # Decorators are applied in reverse order.
            for decorator in reversed(defn.decorators):
                if isinstance(decorator, CallExpr) and isinstance(
                    decorator.analyzed, PromoteExpr
                ):
                    # _promote is a special type checking related construct.
                    continue

                dec = self.expr_checker.accept(decorator)
                temp = self.temp_node(sig, context=decorator)
                fullname = None
                if isinstance(decorator, RefExpr):
                    fullname = decorator.fullname or None

                # TODO: Figure out how to have clearer error messages.
                # (e.g. "class decorator must be a function that accepts a type."
                old_allow_abstract_call = self.allow_abstract_call
                self.allow_abstract_call = True
                sig, _ = self.expr_checker.check_call(
                    dec, [temp], [nodes.ARG_POS], defn, callable_name=fullname
                )
                self.allow_abstract_call = old_allow_abstract_call
            # TODO: Apply the sig to the actual TypeInfo so we can handle decorators
            # that completely swap out the type.  (e.g. Callable[[Type[A]], Type[B]])
    if typ.defn.type_vars:
        for base_inst in typ.bases:
            for base_tvar, base_decl_tvar in zip(
                base_inst.args, base_inst.type.defn.type_vars
            ):
                if (
                    isinstance(base_tvar, TypeVarType)
                    and base_tvar.variance != INVARIANT
                    and isinstance(base_decl_tvar, TypeVarType)
                    and base_decl_tvar.variance != base_tvar.variance
                ):
                    self.fail(
                        f'Variance of TypeVar "{base_tvar.name}" incompatible '
                        "with variance in parent type",
                        context=defn,
                        code=codes.TYPE_VAR,
                    )

    if typ.is_protocol and typ.defn.type_vars:
        self.check_protocol_variance(defn)
    if not defn.has_incompatible_baseclass and defn.info.is_enum:
        self.check_enum(defn)

</t>
<t tx="ekr.20230831011819.234">def check_final_deletable(self, typ: TypeInfo) -&gt; None:
    # These checks are only for mypyc. Only perform some checks that are easier
    # to implement here than in mypyc.
    for attr in typ.deletable_attributes:
        node = typ.names.get(attr)
        if node and isinstance(node.node, Var) and node.node.is_final:
            self.fail(message_registry.CANNOT_MAKE_DELETABLE_FINAL, node.node)

</t>
<t tx="ekr.20230831011819.235">def check_init_subclass(self, defn: ClassDef) -&gt; None:
    """Check that keywords in a class definition are valid arguments for __init_subclass__().

    In this example:
        1   class Base:
        2       def __init_subclass__(cls, thing: int):
        3           pass
        4   class Child(Base, thing=5):
        5       def __init_subclass__(cls):
        6           pass
        7   Child()

    Base.__init_subclass__(thing=5) is called at line 4. This is what we simulate here.
    Child.__init_subclass__ is never called.
    """
    if defn.info.metaclass_type and defn.info.metaclass_type.type.fullname not in (
        "builtins.type",
        "abc.ABCMeta",
    ):
        # We can't safely check situations when both __init_subclass__ and a custom
        # metaclass are present.
        return
    # At runtime, only Base.__init_subclass__ will be called, so
    # we skip the current class itself.
    for base in defn.info.mro[1:]:
        if "__init_subclass__" not in base.names:
            continue
        name_expr = NameExpr(defn.name)
        name_expr.node = base
        callee = MemberExpr(name_expr, "__init_subclass__")
        args = list(defn.keywords.values())
        arg_names: list[str | None] = list(defn.keywords.keys())
        # 'metaclass' keyword is consumed by the rest of the type machinery,
        # and is never passed to __init_subclass__ implementations
        if "metaclass" in arg_names:
            idx = arg_names.index("metaclass")
            arg_names.pop(idx)
            args.pop(idx)
        arg_kinds = [ARG_NAMED] * len(args)
        call_expr = CallExpr(callee, args, arg_kinds, arg_names)
        call_expr.line = defn.line
        call_expr.column = defn.column
        call_expr.end_line = defn.end_line
        self.expr_checker.accept(call_expr, allow_none_return=True, always_allow_any=True)
        # We are only interested in the first Base having __init_subclass__,
        # all other bases have already been checked.
        break

</t>
<t tx="ekr.20230831011819.236">def check_enum(self, defn: ClassDef) -&gt; None:
    assert defn.info.is_enum
    if defn.info.fullname not in ENUM_BASES:
        for sym in defn.info.names.values():
            if (
                isinstance(sym.node, Var)
                and sym.node.has_explicit_value
                and sym.node.name == "__members__"
            ):
                # `__members__` will always be overwritten by `Enum` and is considered
                # read-only so we disallow assigning a value to it
                self.fail(message_registry.ENUM_MEMBERS_ATTR_WILL_BE_OVERRIDEN, sym.node)
    for base in defn.info.mro[1:-1]:  # we don't need self and `object`
        if base.is_enum and base.fullname not in ENUM_BASES:
            self.check_final_enum(defn, base)

    self.check_enum_bases(defn)
    self.check_enum_new(defn)

</t>
<t tx="ekr.20230831011819.237">def check_final_enum(self, defn: ClassDef, base: TypeInfo) -&gt; None:
    for sym in base.names.values():
        if self.is_final_enum_value(sym):
            self.fail(f'Cannot extend enum with existing members: "{base.name}"', defn)
            break

</t>
<t tx="ekr.20230831011819.238">def is_final_enum_value(self, sym: SymbolTableNode) -&gt; bool:
    if isinstance(sym.node, (FuncBase, Decorator)):
        return False  # A method is fine
    if not isinstance(sym.node, Var):
        return True  # Can be a class or anything else

    # Now, only `Var` is left, we need to check:
    # 1. Private name like in `__prop = 1`
    # 2. Dunder name like `__hash__ = some_hasher`
    # 3. Sunder name like `_order_ = 'a, b, c'`
    # 4. If it is a method / descriptor like in `method = classmethod(func)`
    if (
        is_private(sym.node.name)
        or is_dunder(sym.node.name)
        or is_sunder(sym.node.name)
        # TODO: make sure that `x = @class/staticmethod(func)`
        # and `x = property(prop)` both work correctly.
        # Now they are incorrectly counted as enum members.
        or isinstance(get_proper_type(sym.node.type), FunctionLike)
    ):
        return False

    return self.is_stub or sym.node.has_explicit_value

</t>
<t tx="ekr.20230831011819.239">def check_enum_bases(self, defn: ClassDef) -&gt; None:
    """
    Non-enum mixins cannot appear after enum bases; this is disallowed at runtime:

        class Foo: ...
        class Bar(enum.Enum, Foo): ...

    But any number of enum mixins can appear in a class definition
    (even if multiple enum bases define __new__). So this is fine:

        class Foo(enum.Enum):
            def __new__(cls, val): ...
        class Bar(enum.Enum):
            def __new__(cls, val): ...
        class Baz(int, Foo, Bar, enum.Flag): ...
    """
    enum_base: Instance | None = None
    for base in defn.info.bases:
        if enum_base is None and base.type.is_enum:
            enum_base = base
            continue
        elif enum_base is not None and not base.type.is_enum:
            self.fail(
                f'No non-enum mixin classes are allowed after "{enum_base.str_with_options(self.options)}"',
                defn,
            )
            break

</t>
<t tx="ekr.20230831011819.24">@path mypy
&lt;&lt; binder.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.240">def check_enum_new(self, defn: ClassDef) -&gt; None:
    def has_new_method(info: TypeInfo) -&gt; bool:
        new_method = info.get("__new__")
        return bool(
            new_method
            and new_method.node
            and new_method.node.fullname != "builtins.object.__new__"
        )

    has_new = False
    for base in defn.info.bases:
        candidate = False

        if base.type.is_enum:
            # If we have an `Enum`, then we need to check all its bases.
            candidate = any(not b.is_enum and has_new_method(b) for b in base.type.mro[1:-1])
        else:
            candidate = has_new_method(base.type)

        if candidate and has_new:
            self.fail(
                "Only a single data type mixin is allowed for Enum subtypes, "
                'found extra "{}"'.format(base.str_with_options(self.options)),
                defn,
            )
        elif candidate:
            has_new = True

</t>
<t tx="ekr.20230831011819.241">def check_protocol_variance(self, defn: ClassDef) -&gt; None:
    """Check that protocol definition is compatible with declared
    variances of type variables.

    Note that we also prohibit declaring protocol classes as invariant
    if they are actually covariant/contravariant, since this may break
    transitivity of subtyping, see PEP 544.
    """
    info = defn.info
    object_type = Instance(info.mro[-1], [])
    tvars = info.defn.type_vars
    for i, tvar in enumerate(tvars):
        up_args: list[Type] = [
            object_type if i == j else AnyType(TypeOfAny.special_form)
            for j, _ in enumerate(tvars)
        ]
        down_args: list[Type] = [
            UninhabitedType() if i == j else AnyType(TypeOfAny.special_form)
            for j, _ in enumerate(tvars)
        ]
        up, down = Instance(info, up_args), Instance(info, down_args)
        # TODO: add advanced variance checks for recursive protocols
        if is_subtype(down, up, ignore_declared_variance=True):
            expected = COVARIANT
        elif is_subtype(up, down, ignore_declared_variance=True):
            expected = CONTRAVARIANT
        else:
            expected = INVARIANT
        if isinstance(tvar, TypeVarType) and expected != tvar.variance:
            self.msg.bad_proto_variance(tvar.variance, tvar.name, expected, defn)

</t>
<t tx="ekr.20230831011819.242">def check_multiple_inheritance(self, typ: TypeInfo) -&gt; None:
    """Check for multiple inheritance related errors."""
    if len(typ.bases) &lt;= 1:
        # No multiple inheritance.
        return
    # Verify that inherited attributes are compatible.
    mro = typ.mro[1:]
    for i, base in enumerate(mro):
        # Attributes defined in both the type and base are skipped.
        # Normal checks for attribute compatibility should catch any problems elsewhere.
        non_overridden_attrs = base.names.keys() - typ.names.keys()
        for name in non_overridden_attrs:
            if is_private(name):
                continue
            for base2 in mro[i + 1 :]:
                # We only need to check compatibility of attributes from classes not
                # in a subclass relationship. For subclasses, normal (single inheritance)
                # checks suffice (these are implemented elsewhere).
                if name in base2.names and base2 not in base.mro:
                    self.check_compatibility(name, base, base2, typ)

</t>
<t tx="ekr.20230831011819.243">def determine_type_of_member(self, sym: SymbolTableNode) -&gt; Type | None:
    if sym.type is not None:
        return sym.type
    if isinstance(sym.node, FuncBase):
        return self.function_type(sym.node)
    if isinstance(sym.node, TypeInfo):
        if sym.node.typeddict_type:
            # We special-case TypedDict, because they don't define any constructor.
            return self.expr_checker.typeddict_callable(sym.node)
        else:
            return type_object_type(sym.node, self.named_type)
    if isinstance(sym.node, TypeVarExpr):
        # Use of TypeVars is rejected in an expression/runtime context, so
        # we don't need to check supertype compatibility for them.
        return AnyType(TypeOfAny.special_form)
    if isinstance(sym.node, TypeAlias):
        with self.msg.filter_errors():
            # Suppress any errors, they will be given when analyzing the corresponding node.
            # Here we may have incorrect options and location context.
            return self.expr_checker.alias_type_in_runtime_context(sym.node, ctx=sym.node)
    # TODO: handle more node kinds here.
    return None

</t>
<t tx="ekr.20230831011819.244">def check_compatibility(
    self, name: str, base1: TypeInfo, base2: TypeInfo, ctx: TypeInfo
) -&gt; None:
    """Check if attribute name in base1 is compatible with base2 in multiple inheritance.

    Assume base1 comes before base2 in the MRO, and that base1 and base2 don't have
    a direct subclass relationship (i.e., the compatibility requirement only derives from
    multiple inheritance).

    This check verifies that a definition taken from base1 (and mapped to the current
    class ctx), is type compatible with the definition taken from base2 (also mapped), so
    that unsafe subclassing like this can be detected:
        class A(Generic[T]):
            def foo(self, x: T) -&gt; None: ...

        class B:
            def foo(self, x: str) -&gt; None: ...

        class C(B, A[int]): ...  # this is unsafe because...

        x: A[int] = C()
        x.foo  # ...runtime type is (str) -&gt; None, while static type is (int) -&gt; None
    """
    if name in ("__init__", "__new__", "__init_subclass__"):
        # __init__ and friends can be incompatible -- it's a special case.
        return
    first = base1.names[name]
    second = base2.names[name]
    first_type = get_proper_type(self.determine_type_of_member(first))
    second_type = get_proper_type(self.determine_type_of_member(second))

    # start with the special case that Instance can be a subtype of FunctionLike
    call = None
    if isinstance(first_type, Instance):
        call = find_member("__call__", first_type, first_type, is_operator=True)
    if call and isinstance(second_type, FunctionLike):
        second_sig = self.bind_and_map_method(second, second_type, ctx, base2)
        ok = is_subtype(call, second_sig, ignore_pos_arg_names=True)
    elif isinstance(first_type, FunctionLike) and isinstance(second_type, FunctionLike):
        if first_type.is_type_obj() and second_type.is_type_obj():
            # For class objects only check the subtype relationship of the classes,
            # since we allow incompatible overrides of '__init__'/'__new__'
            ok = is_subtype(
                left=fill_typevars_with_any(first_type.type_object()),
                right=fill_typevars_with_any(second_type.type_object()),
            )
        else:
            # First bind/map method types when necessary.
            first_sig = self.bind_and_map_method(first, first_type, ctx, base1)
            second_sig = self.bind_and_map_method(second, second_type, ctx, base2)
            ok = is_subtype(first_sig, second_sig, ignore_pos_arg_names=True)
    elif first_type and second_type:
        if isinstance(first.node, Var):
            first_type = expand_self_type(first.node, first_type, fill_typevars(ctx))
        if isinstance(second.node, Var):
            second_type = expand_self_type(second.node, second_type, fill_typevars(ctx))
        ok = is_equivalent(first_type, second_type)
        if not ok:
            second_node = base2[name].node
            if (
                isinstance(second_type, FunctionLike)
                and second_node is not None
                and is_property(second_node)
            ):
                second_type = get_property_type(second_type)
                ok = is_subtype(first_type, second_type)
    else:
        if first_type is None:
            self.msg.cannot_determine_type_in_base(name, base1.name, ctx)
        if second_type is None:
            self.msg.cannot_determine_type_in_base(name, base2.name, ctx)
        ok = True
    # Final attributes can never be overridden, but can override
    # non-final read-only attributes.
    if is_final_node(second.node):
        self.msg.cant_override_final(name, base2.name, ctx)
    if is_final_node(first.node):
        self.check_if_final_var_override_writable(name, second.node, ctx)
    # Some attributes like __slots__ and __deletable__ are special, and the type can
    # vary across class hierarchy.
    if isinstance(second.node, Var) and second.node.allow_incompatible_override:
        ok = True
    if not ok:
        self.msg.base_class_definitions_incompatible(name, base1, base2, ctx)

</t>
<t tx="ekr.20230831011819.245">def check_metaclass_compatibility(self, typ: TypeInfo) -&gt; None:
    """Ensures that metaclasses of all parent types are compatible."""
    if (
        typ.is_metaclass()
        or typ.is_protocol
        or typ.is_named_tuple
        or typ.is_enum
        or typ.typeddict_type is not None
    ):
        return  # Reasonable exceptions from this check

    metaclasses = [
        entry.metaclass_type
        for entry in typ.mro[1:-1]
        if entry.metaclass_type
        and not is_named_instance(entry.metaclass_type, "builtins.type")
    ]
    if not metaclasses:
        return
    if typ.metaclass_type is not None and all(
        is_subtype(typ.metaclass_type, meta) for meta in metaclasses
    ):
        return
    self.fail(
        "Metaclass conflict: the metaclass of a derived class must be "
        "a (non-strict) subclass of the metaclasses of all its bases",
        typ,
    )

</t>
<t tx="ekr.20230831011819.246">def visit_import_from(self, node: ImportFrom) -&gt; None:
    self.check_import(node)

</t>
<t tx="ekr.20230831011819.247">def visit_import_all(self, node: ImportAll) -&gt; None:
    self.check_import(node)

</t>
<t tx="ekr.20230831011819.248">def visit_import(self, node: Import) -&gt; None:
    self.check_import(node)

</t>
<t tx="ekr.20230831011819.249">def check_import(self, node: ImportBase) -&gt; None:
    for assign in node.assignments:
        lvalue = assign.lvalues[0]
        lvalue_type, _, __ = self.check_lvalue(lvalue)
        if lvalue_type is None:
            # TODO: This is broken.
            lvalue_type = AnyType(TypeOfAny.special_form)
        assert isinstance(assign.rvalue, NameExpr)
        message = message_registry.INCOMPATIBLE_IMPORT_OF.format(assign.rvalue.name)
        self.check_simple_assignment(
            lvalue_type,
            assign.rvalue,
            node,
            msg=message,
            lvalue_name="local name",
            rvalue_name="imported name",
        )

</t>
<t tx="ekr.20230831011819.25">from __future__ import annotations

from collections import defaultdict
from contextlib import contextmanager
from typing import DefaultDict, Iterator, List, Optional, Tuple, Union, cast
from typing_extensions import TypeAlias as _TypeAlias

from mypy.erasetype import remove_instance_last_known_values
from mypy.join import join_simple
from mypy.literals import Key, literal, literal_hash, subkeys
from mypy.nodes import Expression, IndexExpr, MemberExpr, NameExpr, RefExpr, TypeInfo, Var
from mypy.subtypes import is_same_type, is_subtype
from mypy.types import (
    AnyType,
    NoneType,
    PartialType,
    Type,
    TypeOfAny,
    TypeType,
    UnionType,
    get_proper_type,
)
from mypy.typevars import fill_typevars_with_any

BindableExpression: _TypeAlias = Union[IndexExpr, MemberExpr, NameExpr]


</t>
<t tx="ekr.20230831011819.250">#
# Statements
#

def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        # This block was marked as being unreachable during semantic analysis.
        # It turns out any blocks marked in this way are *intentionally* marked
        # as unreachable -- so we don't display an error.
        self.binder.unreachable()
        return
    for s in b.body:
        if self.binder.is_unreachable():
            if not self.should_report_unreachable_issues():
                break
            if not self.is_noop_for_reachability(s):
                self.msg.unreachable_statement(s)
                break
        else:
            self.accept(s)

</t>
<t tx="ekr.20230831011819.251">def should_report_unreachable_issues(self) -&gt; bool:
    return (
        self.in_checked_function()
        and self.options.warn_unreachable
        and not self.current_node_deferred
        and not self.binder.is_unreachable_warning_suppressed()
    )

</t>
<t tx="ekr.20230831011819.252">def is_noop_for_reachability(self, s: Statement) -&gt; bool:
    """Returns 'true' if the given statement either throws an error of some kind
    or is a no-op.

    We use this function while handling the '--warn-unreachable' flag. When
    that flag is present, we normally report an error on any unreachable statement.
    But if that statement is just something like a 'pass' or a just-in-case 'assert False',
    reporting an error would be annoying.
    """
    if isinstance(s, AssertStmt) and is_false_literal(s.expr):
        return True
    elif isinstance(s, (RaiseStmt, PassStmt)):
        return True
    elif isinstance(s, ExpressionStmt):
        if isinstance(s.expr, EllipsisExpr):
            return True
        elif isinstance(s.expr, CallExpr):
            with self.expr_checker.msg.filter_errors():
                typ = get_proper_type(
                    self.expr_checker.accept(
                        s.expr, allow_none_return=True, always_allow_any=True
                    )
                )

            if isinstance(typ, UninhabitedType):
                return True
    return False

</t>
<t tx="ekr.20230831011819.253">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    """Type check an assignment statement.

    Handle all kinds of assignment statements (simple, indexed, multiple).
    """
    # Avoid type checking type aliases in stubs to avoid false
    # positives about modern type syntax available in stubs such
    # as X | Y.
    if not (s.is_alias_def and self.is_stub):
        with self.enter_final_context(s.is_final_def):
            self.check_assignment(s.lvalues[-1], s.rvalue, s.type is None, s.new_syntax)

    if s.is_alias_def:
        self.check_type_alias_rvalue(s)

    if (
        s.type is not None
        and self.options.disallow_any_unimported
        and has_any_from_unimported_type(s.type)
    ):
        if isinstance(s.lvalues[-1], TupleExpr):
            # This is a multiple assignment. Instead of figuring out which type is problematic,
            # give a generic error message.
            self.msg.unimported_type_becomes_any(
                "A type on this line", AnyType(TypeOfAny.special_form), s
            )
        else:
            self.msg.unimported_type_becomes_any("Type of variable", s.type, s)
    check_for_explicit_any(s.type, self.options, self.is_typeshed_stub, self.msg, context=s)

    if len(s.lvalues) &gt; 1:
        # Chained assignment (e.g. x = y = ...).
        # Make sure that rvalue type will not be reinferred.
        if not self.has_type(s.rvalue):
            self.expr_checker.accept(s.rvalue)
        rvalue = self.temp_node(self.lookup_type(s.rvalue), s)
        for lv in s.lvalues[:-1]:
            with self.enter_final_context(s.is_final_def):
                self.check_assignment(lv, rvalue, s.type is None)

    self.check_final(s)
    if (
        s.is_final_def
        and s.type
        and not has_no_typevars(s.type)
        and self.scope.active_class() is not None
    ):
        self.fail(message_registry.DEPENDENT_FINAL_IN_CLASS_BODY, s)

    if s.unanalyzed_type and not self.in_checked_function():
        self.msg.annotation_in_unchecked_function(context=s)

</t>
<t tx="ekr.20230831011819.254">def check_type_alias_rvalue(self, s: AssignmentStmt) -&gt; None:
    alias_type = self.expr_checker.accept(s.rvalue)
    self.store_type(s.lvalues[-1], alias_type)

</t>
<t tx="ekr.20230831011819.255">def check_assignment(
    self,
    lvalue: Lvalue,
    rvalue: Expression,
    infer_lvalue_type: bool = True,
    new_syntax: bool = False,
) -&gt; None:
    """Type check a single assignment: lvalue = rvalue."""
    if isinstance(lvalue, (TupleExpr, ListExpr)):
        self.check_assignment_to_multiple_lvalues(
            lvalue.items, rvalue, rvalue, infer_lvalue_type
        )
    else:
        self.try_infer_partial_generic_type_from_assignment(lvalue, rvalue, "=")
        lvalue_type, index_lvalue, inferred = self.check_lvalue(lvalue)
        # If we're assigning to __getattr__ or similar methods, check that the signature is
        # valid.
        if isinstance(lvalue, NameExpr) and lvalue.node:
            name = lvalue.node.name
            if name in ("__setattr__", "__getattribute__", "__getattr__"):
                # If an explicit type is given, use that.
                if lvalue_type:
                    signature = lvalue_type
                else:
                    signature = self.expr_checker.accept(rvalue)
                if signature:
                    if name == "__setattr__":
                        self.check_setattr_method(signature, lvalue)
                    else:
                        self.check_getattr_method(signature, lvalue, name)

            if name == "__slots__":
                typ = lvalue_type or self.expr_checker.accept(rvalue)
                self.check_slots_definition(typ, lvalue)
            if name == "__match_args__" and inferred is not None:
                typ = self.expr_checker.accept(rvalue)
                self.check_match_args(inferred, typ, lvalue)
            if name == "__post_init__":
                if dataclasses_plugin.is_processed_dataclass(self.scope.active_class()):
                    self.fail(message_registry.DATACLASS_POST_INIT_MUST_BE_A_FUNCTION, rvalue)

        # Defer PartialType's super type checking.
        if (
            isinstance(lvalue, RefExpr)
            and not (isinstance(lvalue_type, PartialType) and lvalue_type.type is None)
            and not (isinstance(lvalue, NameExpr) and lvalue.name == "__match_args__")
        ):
            if self.check_compatibility_all_supers(lvalue, lvalue_type, rvalue):
                # We hit an error on this line; don't check for any others
                return

        if isinstance(lvalue, MemberExpr) and lvalue.name == "__match_args__":
            self.fail(message_registry.CANNOT_MODIFY_MATCH_ARGS, lvalue)

        if lvalue_type:
            if isinstance(lvalue_type, PartialType) and lvalue_type.type is None:
                # Try to infer a proper type for a variable with a partial None type.
                rvalue_type = self.expr_checker.accept(rvalue)
                if isinstance(get_proper_type(rvalue_type), NoneType):
                    # This doesn't actually provide any additional information -- multiple
                    # None initializers preserve the partial None type.
                    return

                var = lvalue_type.var
                if is_valid_inferred_type(rvalue_type, is_lvalue_final=var.is_final):
                    partial_types = self.find_partial_types(var)
                    if partial_types is not None:
                        if not self.current_node_deferred:
                            # Partial type can't be final, so strip any literal values.
                            rvalue_type = remove_instance_last_known_values(rvalue_type)
                            inferred_type = make_simplified_union([rvalue_type, NoneType()])
                            self.set_inferred_type(var, lvalue, inferred_type)
                        else:
                            var.type = None
                        del partial_types[var]
                        lvalue_type = var.type
                else:
                    # Try to infer a partial type. No need to check the return value, as
                    # an error will be reported elsewhere.
                    self.infer_partial_type(lvalue_type.var, lvalue, rvalue_type)
                # Handle None PartialType's super type checking here, after it's resolved.
                if isinstance(lvalue, RefExpr) and self.check_compatibility_all_supers(
                    lvalue, lvalue_type, rvalue
                ):
                    # We hit an error on this line; don't check for any others
                    return
            elif (
                is_literal_none(rvalue)
                and isinstance(lvalue, NameExpr)
                and isinstance(lvalue.node, Var)
                and lvalue.node.is_initialized_in_class
                and not new_syntax
            ):
                # Allow None's to be assigned to class variables with non-Optional types.
                rvalue_type = lvalue_type
            elif (
                isinstance(lvalue, MemberExpr) and lvalue.kind is None
            ):  # Ignore member access to modules
                instance_type = self.expr_checker.accept(lvalue.expr)
                rvalue_type, lvalue_type, infer_lvalue_type = self.check_member_assignment(
                    instance_type, lvalue_type, rvalue, context=rvalue
                )
            else:
                # Hacky special case for assigning a literal None
                # to a variable defined in a previous if
                # branch. When we detect this, we'll go back and
                # make the type optional. This is somewhat
                # unpleasant, and a generalization of this would
                # be an improvement!
                if (
                    is_literal_none(rvalue)
                    and isinstance(lvalue, NameExpr)
                    and lvalue.kind == LDEF
                    and isinstance(lvalue.node, Var)
                    and lvalue.node.type
                    and lvalue.node in self.var_decl_frames
                    and not isinstance(get_proper_type(lvalue_type), AnyType)
                ):
                    decl_frame_map = self.var_decl_frames[lvalue.node]
                    # Check if the nearest common ancestor frame for the definition site
                    # and the current site is the enclosing frame of an if/elif/else block.
                    has_if_ancestor = False
                    for frame in reversed(self.binder.frames):
                        if frame.id in decl_frame_map:
                            has_if_ancestor = frame.conditional_frame
                            break
                    if has_if_ancestor:
                        lvalue_type = make_optional_type(lvalue_type)
                        self.set_inferred_type(lvalue.node, lvalue, lvalue_type)

                rvalue_type = self.check_simple_assignment(lvalue_type, rvalue, context=rvalue)

            # Special case: only non-abstract non-protocol classes can be assigned to
            # variables with explicit type Type[A], where A is protocol or abstract.
            p_rvalue_type = get_proper_type(rvalue_type)
            p_lvalue_type = get_proper_type(lvalue_type)
            if (
                isinstance(p_rvalue_type, CallableType)
                and p_rvalue_type.is_type_obj()
                and (
                    p_rvalue_type.type_object().is_abstract
                    or p_rvalue_type.type_object().is_protocol
                )
                and isinstance(p_lvalue_type, TypeType)
                and isinstance(p_lvalue_type.item, Instance)
                and (
                    p_lvalue_type.item.type.is_abstract or p_lvalue_type.item.type.is_protocol
                )
            ):
                self.msg.concrete_only_assign(p_lvalue_type, rvalue)
                return
            if rvalue_type and infer_lvalue_type and not isinstance(lvalue_type, PartialType):
                # Don't use type binder for definitions of special forms, like named tuples.
                if not (isinstance(lvalue, NameExpr) and lvalue.is_special_form):
                    self.binder.assign_type(lvalue, rvalue_type, lvalue_type, False)

        elif index_lvalue:
            self.check_indexed_assignment(index_lvalue, rvalue, lvalue)

        if inferred:
            type_context = self.get_variable_type_context(inferred)
            rvalue_type = self.expr_checker.accept(rvalue, type_context=type_context)
            if not (
                inferred.is_final
                or (isinstance(lvalue, NameExpr) and lvalue.name == "__match_args__")
            ):
                rvalue_type = remove_instance_last_known_values(rvalue_type)
            self.infer_variable_type(inferred, lvalue, rvalue_type, rvalue)
        self.check_assignment_to_slots(lvalue)

</t>
<t tx="ekr.20230831011819.256"># (type, operator) tuples for augmented assignments supported with partial types
partial_type_augmented_ops: Final = {("builtins.list", "+"), ("builtins.set", "|")}

def get_variable_type_context(self, inferred: Var) -&gt; Type | None:
    type_contexts = []
    if inferred.info:
        for base in inferred.info.mro[1:]:
            base_type, base_node = self.lvalue_type_from_base(inferred, base)
            if (
                base_type
                and not (isinstance(base_node, Var) and base_node.invalid_partial_type)
                and not isinstance(base_type, PartialType)
            ):
                type_contexts.append(base_type)
    # Use most derived supertype as type context if available.
    if not type_contexts:
        return None
    candidate = type_contexts[0]
    for other in type_contexts:
        if is_proper_subtype(other, candidate):
            candidate = other
        elif not is_subtype(candidate, other):
            # Multiple incompatible candidates, cannot use any of them as context.
            return None
    return candidate

</t>
<t tx="ekr.20230831011819.257">def try_infer_partial_generic_type_from_assignment(
    self, lvalue: Lvalue, rvalue: Expression, op: str
) -&gt; None:
    """Try to infer a precise type for partial generic type from assignment.

    'op' is '=' for normal assignment and a binary operator ('+', ...) for
    augmented assignment.

    Example where this happens:

        x = []
        if foo():
            x = [1]  # Infer List[int] as type of 'x'
    """
    var = None
    if (
        isinstance(lvalue, NameExpr)
        and isinstance(lvalue.node, Var)
        and isinstance(lvalue.node.type, PartialType)
    ):
        var = lvalue.node
    elif isinstance(lvalue, MemberExpr):
        var = self.expr_checker.get_partial_self_var(lvalue)
    if var is not None:
        typ = var.type
        assert isinstance(typ, PartialType)
        if typ.type is None:
            return
        # Return if this is an unsupported augmented assignment.
        if op != "=" and (typ.type.fullname, op) not in self.partial_type_augmented_ops:
            return
        # TODO: some logic here duplicates the None partial type counterpart
        #       inlined in check_assignment(), see #8043.
        partial_types = self.find_partial_types(var)
        if partial_types is None:
            return
        rvalue_type = self.expr_checker.accept(rvalue)
        rvalue_type = get_proper_type(rvalue_type)
        if isinstance(rvalue_type, Instance):
            if rvalue_type.type == typ.type and is_valid_inferred_type(rvalue_type):
                var.type = rvalue_type
                del partial_types[var]
        elif isinstance(rvalue_type, AnyType):
            var.type = fill_typevars_with_any(typ.type)
            del partial_types[var]

</t>
<t tx="ekr.20230831011819.258">def check_compatibility_all_supers(
    self, lvalue: RefExpr, lvalue_type: Type | None, rvalue: Expression
) -&gt; bool:
    lvalue_node = lvalue.node
    # Check if we are a class variable with at least one base class
    if (
        isinstance(lvalue_node, Var)
        and lvalue.kind in (MDEF, None)
        and len(lvalue_node.info.bases) &gt; 0  # None for Vars defined via self
    ):
        for base in lvalue_node.info.mro[1:]:
            tnode = base.names.get(lvalue_node.name)
            if tnode is not None:
                if not self.check_compatibility_classvar_super(lvalue_node, base, tnode.node):
                    # Show only one error per variable
                    break

                if not self.check_compatibility_final_super(lvalue_node, base, tnode.node):
                    # Show only one error per variable
                    break

        direct_bases = lvalue_node.info.direct_base_classes()
        last_immediate_base = direct_bases[-1] if direct_bases else None

        for base in lvalue_node.info.mro[1:]:
            # The type of "__slots__" and some other attributes usually doesn't need to
            # be compatible with a base class. We'll still check the type of "__slots__"
            # against "object" as an exception.
            if lvalue_node.allow_incompatible_override and not (
                lvalue_node.name == "__slots__" and base.fullname == "builtins.object"
            ):
                continue

            if is_private(lvalue_node.name):
                continue

            base_type, base_node = self.lvalue_type_from_base(lvalue_node, base)
            if isinstance(base_type, PartialType):
                base_type = None

            if base_type:
                assert base_node is not None
                if not self.check_compatibility_super(
                    lvalue, lvalue_type, rvalue, base, base_type, base_node
                ):
                    # Only show one error per variable; even if other
                    # base classes are also incompatible
                    return True
                if base is last_immediate_base:
                    # At this point, the attribute was found to be compatible with all
                    # immediate parents.
                    break
    return False

</t>
<t tx="ekr.20230831011819.259">def check_compatibility_super(
    self,
    lvalue: RefExpr,
    lvalue_type: Type | None,
    rvalue: Expression,
    base: TypeInfo,
    base_type: Type,
    base_node: Node,
) -&gt; bool:
    lvalue_node = lvalue.node
    assert isinstance(lvalue_node, Var)

    # Do not check whether the rvalue is compatible if the
    # lvalue had a type defined; this is handled by other
    # parts, and all we have to worry about in that case is
    # that lvalue is compatible with the base class.
    compare_node = None
    if lvalue_type:
        compare_type = lvalue_type
        compare_node = lvalue.node
    else:
        compare_type = self.expr_checker.accept(rvalue, base_type)
        if isinstance(rvalue, NameExpr):
            compare_node = rvalue.node
            if isinstance(compare_node, Decorator):
                compare_node = compare_node.func

    base_type = get_proper_type(base_type)
    compare_type = get_proper_type(compare_type)
    if compare_type:
        if isinstance(base_type, CallableType) and isinstance(compare_type, CallableType):
            base_static = is_node_static(base_node)
            compare_static = is_node_static(compare_node)

            # In case compare_static is unknown, also check
            # if 'definition' is set. The most common case for
            # this is with TempNode(), where we lose all
            # information about the real rvalue node (but only get
            # the rvalue type)
            if compare_static is None and compare_type.definition:
                compare_static = is_node_static(compare_type.definition)

            # Compare against False, as is_node_static can return None
            if base_static is False and compare_static is False:
                # Class-level function objects and classmethods become bound
                # methods: the former to the instance, the latter to the
                # class
                base_type = bind_self(base_type, self.scope.active_self_type())
                compare_type = bind_self(compare_type, self.scope.active_self_type())

            # If we are a static method, ensure to also tell the
            # lvalue it now contains a static method
            if base_static and compare_static:
                lvalue_node.is_staticmethod = True

        return self.check_subtype(
            compare_type,
            base_type,
            rvalue,
            message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT,
            "expression has type",
            f'base class "{base.name}" defined the type as',
        )
    return True

</t>
<t tx="ekr.20230831011819.26">class Frame:
    """A Frame represents a specific point in the execution of a program.
    It carries information about the current types of expressions at
    that point, arising either from assignments to those expressions
    or the result of isinstance checks. It also records whether it is
    possible to reach that point at all.

    This information is not copied into a new Frame when it is pushed
    onto the stack, so a given Frame only has information about types
    that were assigned in that frame.
    """

    @others
</t>
<t tx="ekr.20230831011819.260">def lvalue_type_from_base(
    self, expr_node: Var, base: TypeInfo
) -&gt; tuple[Type | None, Node | None]:
    """For a NameExpr that is part of a class, walk all base classes and try
    to find the first class that defines a Type for the same name."""
    expr_name = expr_node.name
    base_var = base.names.get(expr_name)

    if base_var:
        base_node = base_var.node
        base_type = base_var.type
        if isinstance(base_node, Var) and base_type is not None:
            base_type = expand_self_type(base_node, base_type, fill_typevars(expr_node.info))
        if isinstance(base_node, Decorator):
            base_node = base_node.func
            base_type = base_node.type

        if base_type:
            if not has_no_typevars(base_type):
                self_type = self.scope.active_self_type()
                assert self_type is not None, "Internal error: base lookup outside class"
                if isinstance(self_type, TupleType):
                    instance = tuple_fallback(self_type)
                else:
                    instance = self_type
                itype = map_instance_to_supertype(instance, base)
                base_type = expand_type_by_instance(base_type, itype)

            base_type = get_proper_type(base_type)
            if isinstance(base_type, CallableType) and isinstance(base_node, FuncDef):
                # If we are a property, return the Type of the return
                # value, not the Callable
                if base_node.is_property:
                    base_type = get_proper_type(base_type.ret_type)
            if isinstance(base_type, FunctionLike) and isinstance(
                base_node, OverloadedFuncDef
            ):
                # Same for properties with setter
                if base_node.is_property:
                    base_type = base_type.items[0].ret_type

            return base_type, base_node

    return None, None

</t>
<t tx="ekr.20230831011819.261">def check_compatibility_classvar_super(
    self, node: Var, base: TypeInfo, base_node: Node | None
) -&gt; bool:
    if not isinstance(base_node, Var):
        return True
    if node.is_classvar and not base_node.is_classvar:
        self.fail(message_registry.CANNOT_OVERRIDE_INSTANCE_VAR.format(base.name), node)
        return False
    elif not node.is_classvar and base_node.is_classvar:
        self.fail(message_registry.CANNOT_OVERRIDE_CLASS_VAR.format(base.name), node)
        return False
    return True

</t>
<t tx="ekr.20230831011819.262">def check_compatibility_final_super(
    self, node: Var, base: TypeInfo, base_node: Node | None
) -&gt; bool:
    """Check if an assignment overrides a final attribute in a base class.

    This only checks situations where either a node in base class is not a variable
    but a final method, or where override is explicitly declared as final.
    In these cases we give a more detailed error message. In addition, we check that
    a final variable doesn't override writeable attribute, which is not safe.

    Other situations are checked in `check_final()`.
    """
    if not isinstance(base_node, (Var, FuncBase, Decorator)):
        return True
    if base_node.is_final and (node.is_final or not isinstance(base_node, Var)):
        # Give this error only for explicit override attempt with `Final`, or
        # if we are overriding a final method with variable.
        # Other override attempts will be flagged as assignment to constant
        # in `check_final()`.
        self.msg.cant_override_final(node.name, base.name, node)
        return False
    if node.is_final:
        if base.fullname in ENUM_BASES or node.name in ENUM_SPECIAL_PROPS:
            return True
        self.check_if_final_var_override_writable(node.name, base_node, node)
    return True

</t>
<t tx="ekr.20230831011819.263">def check_if_final_var_override_writable(
    self, name: str, base_node: Node | None, ctx: Context
) -&gt; None:
    """Check that a final variable doesn't override writeable attribute.

    This is done to prevent situations like this:
        class C:
            attr = 1
        class D(C):
            attr: Final = 2

        x: C = D()
        x.attr = 3  # Oops!
    """
    writable = True
    if base_node:
        writable = self.is_writable_attribute(base_node)
    if writable:
        self.msg.final_cant_override_writable(name, ctx)

</t>
<t tx="ekr.20230831011819.264">def get_final_context(self) -&gt; bool:
    """Check whether we a currently checking a final declaration."""
    return self._is_final_def

</t>
<t tx="ekr.20230831011819.265">@contextmanager
def enter_final_context(self, is_final_def: bool) -&gt; Iterator[None]:
    """Store whether the current checked assignment is a final declaration."""
    old_ctx = self._is_final_def
    self._is_final_def = is_final_def
    try:
        yield
    finally:
        self._is_final_def = old_ctx

</t>
<t tx="ekr.20230831011819.266">def check_final(self, s: AssignmentStmt | OperatorAssignmentStmt | AssignmentExpr) -&gt; None:
    """Check if this assignment does not assign to a final attribute.

    This function performs the check only for name assignments at module
    and class scope. The assignments to `obj.attr` and `Cls.attr` are checked
    in checkmember.py.
    """
    if isinstance(s, AssignmentStmt):
        lvs = self.flatten_lvalues(s.lvalues)
    elif isinstance(s, AssignmentExpr):
        lvs = [s.target]
    else:
        lvs = [s.lvalue]
    is_final_decl = s.is_final_def if isinstance(s, AssignmentStmt) else False
    if is_final_decl and self.scope.active_class():
        lv = lvs[0]
        assert isinstance(lv, RefExpr)
        if lv.node is not None:
            assert isinstance(lv.node, Var)
            if (
                lv.node.final_unset_in_class
                and not lv.node.final_set_in_init
                and not self.is_stub
                and  # It is OK to skip initializer in stub files.
                # Avoid extra error messages, if there is no type in Final[...],
                # then we already reported the error about missing r.h.s.
                isinstance(s, AssignmentStmt)
                and s.type is not None
            ):
                self.msg.final_without_value(s)
    for lv in lvs:
        if isinstance(lv, RefExpr) and isinstance(lv.node, Var):
            name = lv.node.name
            cls = self.scope.active_class()
            if cls is not None:
                # These additional checks exist to give more error messages
                # even if the final attribute was overridden with a new symbol
                # (which is itself an error)...
                for base in cls.mro[1:]:
                    sym = base.names.get(name)
                    # We only give this error if base node is variable,
                    # overriding final method will be caught in
                    # `check_compatibility_final_super()`.
                    if sym and isinstance(sym.node, Var):
                        if sym.node.is_final and not is_final_decl:
                            self.msg.cant_assign_to_final(name, sym.node.info is None, s)
                            # ...but only once
                            break
            if lv.node.is_final and not is_final_decl:
                self.msg.cant_assign_to_final(name, lv.node.info is None, s)

</t>
<t tx="ekr.20230831011819.267">def check_assignment_to_slots(self, lvalue: Lvalue) -&gt; None:
    if not isinstance(lvalue, MemberExpr):
        return

    inst = get_proper_type(self.expr_checker.accept(lvalue.expr))
    if not isinstance(inst, Instance):
        return
    if inst.type.slots is None:
        return  # Slots do not exist, we can allow any assignment
    if lvalue.name in inst.type.slots:
        return  # We are assigning to an existing slot
    for base_info in inst.type.mro[:-1]:
        if base_info.names.get("__setattr__") is not None:
            # When type has `__setattr__` defined,
            # we can assign any dynamic value.
            # We exclude object, because it always has `__setattr__`.
            return

    definition = inst.type.get(lvalue.name)
    if definition is None:
        # We don't want to duplicate
        # `"SomeType" has no attribute "some_attr"`
        # error twice.
        return
    if self.is_assignable_slot(lvalue, definition.type):
        return

    self.fail(
        message_registry.NAME_NOT_IN_SLOTS.format(lvalue.name, inst.type.fullname), lvalue
    )

</t>
<t tx="ekr.20230831011819.268">def is_assignable_slot(self, lvalue: Lvalue, typ: Type | None) -&gt; bool:
    if getattr(lvalue, "node", None):
        return False  # This is a definition

    typ = get_proper_type(typ)
    if typ is None or isinstance(typ, AnyType):
        return True  # Any can be literally anything, like `@propery`
    if isinstance(typ, Instance):
        # When working with instances, we need to know if they contain
        # `__set__` special method. Like `@property` does.
        # This makes assigning to properties possible,
        # even without extra slot spec.
        return typ.type.get("__set__") is not None
    if isinstance(typ, FunctionLike):
        return True  # Can be a property, or some other magic
    if isinstance(typ, UnionType):
        return all(self.is_assignable_slot(lvalue, u) for u in typ.items)
    return False

</t>
<t tx="ekr.20230831011819.269">def check_assignment_to_multiple_lvalues(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    context: Context,
    infer_lvalue_type: bool = True,
) -&gt; None:
    if isinstance(rvalue, (TupleExpr, ListExpr)):
        # Recursively go into Tuple or List expression rhs instead of
        # using the type of rhs, because this allowed more fine grained
        # control in cases like: a, b = [int, str] where rhs would get
        # type List[object]
        rvalues: list[Expression] = []
        iterable_type: Type | None = None
        last_idx: int | None = None
        for idx_rval, rval in enumerate(rvalue.items):
            if isinstance(rval, StarExpr):
                typs = get_proper_type(self.expr_checker.accept(rval.expr))
                if isinstance(typs, TupleType):
                    rvalues.extend([TempNode(typ) for typ in typs.items])
                elif self.type_is_iterable(typs) and isinstance(typs, Instance):
                    if iterable_type is not None and iterable_type != self.iterable_item_type(
                        typs, rvalue
                    ):
                        self.fail(message_registry.CONTIGUOUS_ITERABLE_EXPECTED, context)
                    else:
                        if last_idx is None or last_idx + 1 == idx_rval:
                            rvalues.append(rval)
                            last_idx = idx_rval
                            iterable_type = self.iterable_item_type(typs, rvalue)
                        else:
                            self.fail(message_registry.CONTIGUOUS_ITERABLE_EXPECTED, context)
                else:
                    self.fail(message_registry.ITERABLE_TYPE_EXPECTED.format(typs), context)
            else:
                rvalues.append(rval)
        iterable_start: int | None = None
        iterable_end: int | None = None
        for i, rval in enumerate(rvalues):
            if isinstance(rval, StarExpr):
                typs = get_proper_type(self.expr_checker.accept(rval.expr))
                if self.type_is_iterable(typs) and isinstance(typs, Instance):
                    if iterable_start is None:
                        iterable_start = i
                    iterable_end = i
        if (
            iterable_start is not None
            and iterable_end is not None
            and iterable_type is not None
        ):
            iterable_num = iterable_end - iterable_start + 1
            rvalue_needed = len(lvalues) - (len(rvalues) - iterable_num)
            if rvalue_needed &gt; 0:
                rvalues = (
                    rvalues[0:iterable_start]
                    + [TempNode(iterable_type) for i in range(rvalue_needed)]
                    + rvalues[iterable_end + 1 :]
                )

        if self.check_rvalue_count_in_assignment(lvalues, len(rvalues), context):
            star_index = next(
                (i for i, lv in enumerate(lvalues) if isinstance(lv, StarExpr)), len(lvalues)
            )

            left_lvs = lvalues[:star_index]
            star_lv = (
                cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
            )
            right_lvs = lvalues[star_index + 1 :]

            left_rvs, star_rvs, right_rvs = self.split_around_star(
                rvalues, star_index, len(lvalues)
            )

            lr_pairs = list(zip(left_lvs, left_rvs))
            if star_lv:
                rv_list = ListExpr(star_rvs)
                rv_list.set_line(rvalue)
                lr_pairs.append((star_lv.expr, rv_list))
            lr_pairs.extend(zip(right_lvs, right_rvs))

            for lv, rv in lr_pairs:
                self.check_assignment(lv, rv, infer_lvalue_type)
    else:
        self.check_multi_assignment(lvalues, rvalue, context, infer_lvalue_type)

</t>
<t tx="ekr.20230831011819.27">def __init__(self, id: int, conditional_frame: bool = False) -&gt; None:
    self.id = id
    self.types: dict[Key, Type] = {}
    self.unreachable = False
    self.conditional_frame = conditional_frame
    self.suppress_unreachable_warnings = False

</t>
<t tx="ekr.20230831011819.270">def check_rvalue_count_in_assignment(
    self, lvalues: list[Lvalue], rvalue_count: int, context: Context
) -&gt; bool:
    if any(isinstance(lvalue, StarExpr) for lvalue in lvalues):
        if len(lvalues) - 1 &gt; rvalue_count:
            self.msg.wrong_number_values_to_unpack(rvalue_count, len(lvalues) - 1, context)
            return False
    elif rvalue_count != len(lvalues):
        self.msg.wrong_number_values_to_unpack(rvalue_count, len(lvalues), context)
        return False
    return True

</t>
<t tx="ekr.20230831011819.271">def check_multi_assignment(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    context: Context,
    infer_lvalue_type: bool = True,
    rv_type: Type | None = None,
    undefined_rvalue: bool = False,
) -&gt; None:
    """Check the assignment of one rvalue to a number of lvalues."""

    # Infer the type of an ordinary rvalue expression.
    # TODO: maybe elsewhere; redundant.
    rvalue_type = get_proper_type(rv_type or self.expr_checker.accept(rvalue))

    if isinstance(rvalue_type, TypeVarLikeType):
        rvalue_type = get_proper_type(rvalue_type.upper_bound)

    if isinstance(rvalue_type, UnionType):
        # If this is an Optional type in non-strict Optional code, unwrap it.
        relevant_items = rvalue_type.relevant_items()
        if len(relevant_items) == 1:
            rvalue_type = get_proper_type(relevant_items[0])

    if isinstance(rvalue_type, AnyType):
        for lv in lvalues:
            if isinstance(lv, StarExpr):
                lv = lv.expr
            temp_node = self.temp_node(
                AnyType(TypeOfAny.from_another_any, source_any=rvalue_type), context
            )
            self.check_assignment(lv, temp_node, infer_lvalue_type)
    elif isinstance(rvalue_type, TupleType):
        self.check_multi_assignment_from_tuple(
            lvalues, rvalue, rvalue_type, context, undefined_rvalue, infer_lvalue_type
        )
    elif isinstance(rvalue_type, UnionType):
        self.check_multi_assignment_from_union(
            lvalues, rvalue, rvalue_type, context, infer_lvalue_type
        )
    elif isinstance(rvalue_type, Instance) and rvalue_type.type.fullname == "builtins.str":
        self.msg.unpacking_strings_disallowed(context)
    else:
        self.check_multi_assignment_from_iterable(
            lvalues, rvalue_type, context, infer_lvalue_type
        )

</t>
<t tx="ekr.20230831011819.272">def check_multi_assignment_from_union(
    self,
    lvalues: list[Expression],
    rvalue: Expression,
    rvalue_type: UnionType,
    context: Context,
    infer_lvalue_type: bool,
) -&gt; None:
    """Check assignment to multiple lvalue targets when rvalue type is a Union[...].
    For example:

        t: Union[Tuple[int, int], Tuple[str, str]]
        x, y = t
        reveal_type(x)  # Union[int, str]

    The idea in this case is to process the assignment for every item of the union.
    Important note: the types are collected in two places, 'union_types' contains
    inferred types for first assignments, 'assignments' contains the narrowed types
    for binder.
    """
    self.no_partial_types = True
    transposed: tuple[list[Type], ...] = tuple([] for _ in self.flatten_lvalues(lvalues))
    # Notify binder that we want to defer bindings and instead collect types.
    with self.binder.accumulate_type_assignments() as assignments:
        for item in rvalue_type.items:
            # Type check the assignment separately for each union item and collect
            # the inferred lvalue types for each union item.
            self.check_multi_assignment(
                lvalues,
                rvalue,
                context,
                infer_lvalue_type=infer_lvalue_type,
                rv_type=item,
                undefined_rvalue=True,
            )
            for t, lv in zip(transposed, self.flatten_lvalues(lvalues)):
                # We can access _type_maps directly since temporary type maps are
                # only created within expressions.
                t.append(self._type_maps[0].pop(lv, AnyType(TypeOfAny.special_form)))
    union_types = tuple(make_simplified_union(col) for col in transposed)
    for expr, items in assignments.items():
        # Bind a union of types collected in 'assignments' to every expression.
        if isinstance(expr, StarExpr):
            expr = expr.expr

        # TODO: See todo in binder.py, ConditionalTypeBinder.assign_type
        # It's unclear why the 'declared_type' param is sometimes 'None'
        clean_items: list[tuple[Type, Type]] = []
        for type, declared_type in items:
            assert declared_type is not None
            clean_items.append((type, declared_type))

        types, declared_types = zip(*clean_items)
        self.binder.assign_type(
            expr,
            make_simplified_union(list(types)),
            make_simplified_union(list(declared_types)),
            False,
        )
    for union, lv in zip(union_types, self.flatten_lvalues(lvalues)):
        # Properly store the inferred types.
        _1, _2, inferred = self.check_lvalue(lv)
        if inferred:
            self.set_inferred_type(inferred, lv, union)
        else:
            self.store_type(lv, union)
    self.no_partial_types = False

</t>
<t tx="ekr.20230831011819.273">def flatten_lvalues(self, lvalues: list[Expression]) -&gt; list[Expression]:
    res: list[Expression] = []
    for lv in lvalues:
        if isinstance(lv, (TupleExpr, ListExpr)):
            res.extend(self.flatten_lvalues(lv.items))
        if isinstance(lv, StarExpr):
            # Unwrap StarExpr, since it is unwrapped by other helpers.
            lv = lv.expr
        res.append(lv)
    return res

</t>
<t tx="ekr.20230831011819.274">def check_multi_assignment_from_tuple(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    rvalue_type: TupleType,
    context: Context,
    undefined_rvalue: bool,
    infer_lvalue_type: bool = True,
) -&gt; None:
    if self.check_rvalue_count_in_assignment(lvalues, len(rvalue_type.items), context):
        star_index = next(
            (i for i, lv in enumerate(lvalues) if isinstance(lv, StarExpr)), len(lvalues)
        )

        left_lvs = lvalues[:star_index]
        star_lv = cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
        right_lvs = lvalues[star_index + 1 :]

        if not undefined_rvalue:
            # Infer rvalue again, now in the correct type context.
            lvalue_type = self.lvalue_type_for_inference(lvalues, rvalue_type)
            reinferred_rvalue_type = get_proper_type(
                self.expr_checker.accept(rvalue, lvalue_type)
            )

            if isinstance(reinferred_rvalue_type, UnionType):
                # If this is an Optional type in non-strict Optional code, unwrap it.
                relevant_items = reinferred_rvalue_type.relevant_items()
                if len(relevant_items) == 1:
                    reinferred_rvalue_type = get_proper_type(relevant_items[0])
            if isinstance(reinferred_rvalue_type, UnionType):
                self.check_multi_assignment_from_union(
                    lvalues, rvalue, reinferred_rvalue_type, context, infer_lvalue_type
                )
                return
            if isinstance(reinferred_rvalue_type, AnyType):
                # We can get Any if the current node is
                # deferred. Doing more inference in deferred nodes
                # is hard, so give up for now.  We can also get
                # here if reinferring types above changes the
                # inferred return type for an overloaded function
                # to be ambiguous.
                return
            assert isinstance(reinferred_rvalue_type, TupleType)
            rvalue_type = reinferred_rvalue_type

        left_rv_types, star_rv_types, right_rv_types = self.split_around_star(
            rvalue_type.items, star_index, len(lvalues)
        )

        for lv, rv_type in zip(left_lvs, left_rv_types):
            self.check_assignment(lv, self.temp_node(rv_type, context), infer_lvalue_type)
        if star_lv:
            list_expr = ListExpr(
                [self.temp_node(rv_type, context) for rv_type in star_rv_types]
            )
            list_expr.set_line(context)
            self.check_assignment(star_lv.expr, list_expr, infer_lvalue_type)
        for lv, rv_type in zip(right_lvs, right_rv_types):
            self.check_assignment(lv, self.temp_node(rv_type, context), infer_lvalue_type)

</t>
<t tx="ekr.20230831011819.275">def lvalue_type_for_inference(self, lvalues: list[Lvalue], rvalue_type: TupleType) -&gt; Type:
    star_index = next(
        (i for i, lv in enumerate(lvalues) if isinstance(lv, StarExpr)), len(lvalues)
    )
    left_lvs = lvalues[:star_index]
    star_lv = cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
    right_lvs = lvalues[star_index + 1 :]
    left_rv_types, star_rv_types, right_rv_types = self.split_around_star(
        rvalue_type.items, star_index, len(lvalues)
    )

    type_parameters: list[Type] = []

    def append_types_for_inference(lvs: list[Expression], rv_types: list[Type]) -&gt; None:
        for lv, rv_type in zip(lvs, rv_types):
            sub_lvalue_type, index_expr, inferred = self.check_lvalue(lv)
            if sub_lvalue_type and not isinstance(sub_lvalue_type, PartialType):
                type_parameters.append(sub_lvalue_type)
            else:  # index lvalue
                # TODO Figure out more precise type context, probably
                #      based on the type signature of the _set method.
                type_parameters.append(rv_type)

    append_types_for_inference(left_lvs, left_rv_types)

    if star_lv:
        sub_lvalue_type, index_expr, inferred = self.check_lvalue(star_lv.expr)
        if sub_lvalue_type and not isinstance(sub_lvalue_type, PartialType):
            type_parameters.extend([sub_lvalue_type] * len(star_rv_types))
        else:  # index lvalue
            # TODO Figure out more precise type context, probably
            #      based on the type signature of the _set method.
            type_parameters.extend(star_rv_types)

    append_types_for_inference(right_lvs, right_rv_types)

    return TupleType(type_parameters, self.named_type("builtins.tuple"))

</t>
<t tx="ekr.20230831011819.276">def split_around_star(
    self, items: list[T], star_index: int, length: int
) -&gt; tuple[list[T], list[T], list[T]]:
    """Splits a list of items in three to match another list of length 'length'
    that contains a starred expression at 'star_index' in the following way:

    star_index = 2, length = 5 (i.e., [a,b,*,c,d]), items = [1,2,3,4,5,6,7]
    returns in: ([1,2], [3,4,5], [6,7])
    """
    nr_right_of_star = length - star_index - 1
    right_index = -nr_right_of_star if nr_right_of_star != 0 else len(items)
    left = items[:star_index]
    star = items[star_index:right_index]
    right = items[right_index:]
    return left, star, right

</t>
<t tx="ekr.20230831011819.277">def type_is_iterable(self, type: Type) -&gt; bool:
    type = get_proper_type(type)
    if isinstance(type, CallableType) and type.is_type_obj():
        type = type.fallback
    return is_subtype(
        type, self.named_generic_type("typing.Iterable", [AnyType(TypeOfAny.special_form)])
    )

</t>
<t tx="ekr.20230831011819.278">def check_multi_assignment_from_iterable(
    self,
    lvalues: list[Lvalue],
    rvalue_type: Type,
    context: Context,
    infer_lvalue_type: bool = True,
) -&gt; None:
    rvalue_type = get_proper_type(rvalue_type)
    if self.type_is_iterable(rvalue_type) and isinstance(
        rvalue_type, (Instance, CallableType, TypeType, Overloaded)
    ):
        item_type = self.iterable_item_type(rvalue_type, context)
        for lv in lvalues:
            if isinstance(lv, StarExpr):
                items_type = self.named_generic_type("builtins.list", [item_type])
                self.check_assignment(
                    lv.expr, self.temp_node(items_type, context), infer_lvalue_type
                )
            else:
                self.check_assignment(
                    lv, self.temp_node(item_type, context), infer_lvalue_type
                )
    else:
        self.msg.type_not_iterable(rvalue_type, context)

</t>
<t tx="ekr.20230831011819.279">def check_lvalue(self, lvalue: Lvalue) -&gt; tuple[Type | None, IndexExpr | None, Var | None]:
    lvalue_type = None
    index_lvalue = None
    inferred = None

    if self.is_definition(lvalue) and (
        not isinstance(lvalue, NameExpr) or isinstance(lvalue.node, Var)
    ):
        if isinstance(lvalue, NameExpr):
            assert isinstance(lvalue.node, Var)
            inferred = lvalue.node
        else:
            assert isinstance(lvalue, MemberExpr)
            self.expr_checker.accept(lvalue.expr)
            inferred = lvalue.def_var
    elif isinstance(lvalue, IndexExpr):
        index_lvalue = lvalue
    elif isinstance(lvalue, MemberExpr):
        lvalue_type = self.expr_checker.analyze_ordinary_member_access(lvalue, True)
        self.store_type(lvalue, lvalue_type)
    elif isinstance(lvalue, NameExpr):
        lvalue_type = self.expr_checker.analyze_ref_expr(lvalue, lvalue=True)
        self.store_type(lvalue, lvalue_type)
    elif isinstance(lvalue, (TupleExpr, ListExpr)):
        types = [
            self.check_lvalue(sub_expr)[0] or
            # This type will be used as a context for further inference of rvalue,
            # we put Uninhabited if there is no information available from lvalue.
            UninhabitedType()
            for sub_expr in lvalue.items
        ]
        lvalue_type = TupleType(types, self.named_type("builtins.tuple"))
    elif isinstance(lvalue, StarExpr):
        lvalue_type, _, _ = self.check_lvalue(lvalue.expr)
    else:
        lvalue_type = self.expr_checker.accept(lvalue)

    return lvalue_type, index_lvalue, inferred

</t>
<t tx="ekr.20230831011819.28">def __repr__(self) -&gt; str:
    return f"Frame({self.id}, {self.types}, {self.unreachable}, {self.conditional_frame})"


</t>
<t tx="ekr.20230831011819.280">def is_definition(self, s: Lvalue) -&gt; bool:
    if isinstance(s, NameExpr):
        if s.is_inferred_def:
            return True
        # If the node type is not defined, this must the first assignment
        # that we process =&gt; this is a definition, even though the semantic
        # analyzer did not recognize this as such. This can arise in code
        # that uses isinstance checks, if type checking of the primary
        # definition is skipped due to an always False type check.
        node = s.node
        if isinstance(node, Var):
            return node.type is None
    elif isinstance(s, MemberExpr):
        return s.is_inferred_def
    return False

</t>
<t tx="ekr.20230831011819.281">def infer_variable_type(
    self, name: Var, lvalue: Lvalue, init_type: Type, context: Context
) -&gt; None:
    """Infer the type of initialized variables from initializer type."""
    if isinstance(init_type, DeletedType):
        self.msg.deleted_as_rvalue(init_type, context)
    elif (
        not is_valid_inferred_type(init_type, is_lvalue_final=name.is_final)
        and not self.no_partial_types
    ):
        # We cannot use the type of the initialization expression for full type
        # inference (it's not specific enough), but we might be able to give
        # partial type which will be made more specific later. A partial type
        # gets generated in assignment like 'x = []' where item type is not known.
        if not self.infer_partial_type(name, lvalue, init_type):
            self.msg.need_annotation_for_var(name, context, self.options.python_version)
            self.set_inference_error_fallback_type(name, lvalue, init_type)
    elif (
        isinstance(lvalue, MemberExpr)
        and self.inferred_attribute_types is not None
        and lvalue.def_var
        and lvalue.def_var in self.inferred_attribute_types
        and not is_same_type(self.inferred_attribute_types[lvalue.def_var], init_type)
    ):
        # Multiple, inconsistent types inferred for an attribute.
        self.msg.need_annotation_for_var(name, context, self.options.python_version)
        name.type = AnyType(TypeOfAny.from_error)
    else:
        # Infer type of the target.

        # Make the type more general (strip away function names etc.).
        init_type = strip_type(init_type)

        self.set_inferred_type(name, lvalue, init_type)

</t>
<t tx="ekr.20230831011819.282">def infer_partial_type(self, name: Var, lvalue: Lvalue, init_type: Type) -&gt; bool:
    init_type = get_proper_type(init_type)
    if isinstance(init_type, NoneType):
        partial_type = PartialType(None, name)
    elif isinstance(init_type, Instance):
        fullname = init_type.type.fullname
        is_ref = isinstance(lvalue, RefExpr)
        if (
            is_ref
            and (
                fullname == "builtins.list"
                or fullname == "builtins.set"
                or fullname == "builtins.dict"
                or fullname == "collections.OrderedDict"
            )
            and all(
                isinstance(t, (NoneType, UninhabitedType))
                for t in get_proper_types(init_type.args)
            )
        ):
            partial_type = PartialType(init_type.type, name)
        elif is_ref and fullname == "collections.defaultdict":
            arg0 = get_proper_type(init_type.args[0])
            arg1 = get_proper_type(init_type.args[1])
            if isinstance(
                arg0, (NoneType, UninhabitedType)
            ) and self.is_valid_defaultdict_partial_value_type(arg1):
                arg1 = erase_type(arg1)
                assert isinstance(arg1, Instance)
                partial_type = PartialType(init_type.type, name, arg1)
            else:
                return False
        else:
            return False
    else:
        return False
    self.set_inferred_type(name, lvalue, partial_type)
    self.partial_types[-1].map[name] = lvalue
    return True

</t>
<t tx="ekr.20230831011819.283">def is_valid_defaultdict_partial_value_type(self, t: ProperType) -&gt; bool:
    """Check if t can be used as the basis for a partial defaultdict value type.

    Examples:

      * t is 'int' --&gt; True
      * t is 'list[&lt;nothing&gt;]' --&gt; True
      * t is 'dict[...]' --&gt; False (only generic types with a single type
        argument supported)
    """
    if not isinstance(t, Instance):
        return False
    if len(t.args) == 0:
        return True
    if len(t.args) == 1:
        arg = get_proper_type(t.args[0])
        if self.options.new_type_inference:
            allowed = isinstance(arg, (UninhabitedType, NoneType))
        else:
            # Allow leaked TypeVars for legacy inference logic.
            allowed = isinstance(arg, (UninhabitedType, NoneType, TypeVarType))
        if allowed:
            return True
    return False

</t>
<t tx="ekr.20230831011819.284">def set_inferred_type(self, var: Var, lvalue: Lvalue, type: Type) -&gt; None:
    """Store inferred variable type.

    Store the type to both the variable node and the expression node that
    refers to the variable (lvalue). If var is None, do nothing.
    """
    if var and not self.current_node_deferred:
        var.type = type
        var.is_inferred = True
        if var not in self.var_decl_frames:
            # Used for the hack to improve optional type inference in conditionals
            self.var_decl_frames[var] = {frame.id for frame in self.binder.frames}
        if isinstance(lvalue, MemberExpr) and self.inferred_attribute_types is not None:
            # Store inferred attribute type so that we can check consistency afterwards.
            if lvalue.def_var is not None:
                self.inferred_attribute_types[lvalue.def_var] = type
        self.store_type(lvalue, type)

</t>
<t tx="ekr.20230831011819.285">def set_inference_error_fallback_type(self, var: Var, lvalue: Lvalue, type: Type) -&gt; None:
    """Store best known type for variable if type inference failed.

    If a program ignores error on type inference error, the variable should get some
    inferred type so that if can used later on in the program. Example:

      x = []  # type: ignore
      x.append(1)   # Should be ok!

    We implement this here by giving x a valid type (replacing inferred &lt;nothing&gt; with Any).
    """
    fallback = self.inference_error_fallback_type(type)
    self.set_inferred_type(var, lvalue, fallback)

</t>
<t tx="ekr.20230831011819.286">def inference_error_fallback_type(self, type: Type) -&gt; Type:
    fallback = type.accept(SetNothingToAny())
    # Type variables may leak from inference, see https://github.com/python/mypy/issues/5738,
    # we therefore need to erase them.
    return erase_typevars(fallback)

</t>
<t tx="ekr.20230831011819.287">def simple_rvalue(self, rvalue: Expression) -&gt; bool:
    """Returns True for expressions for which inferred type should not depend on context.

    Note that this function can still return False for some expressions where inferred type
    does not depend on context. It only exists for performance optimizations.
    """
    if isinstance(rvalue, (IntExpr, StrExpr, BytesExpr, FloatExpr, RefExpr)):
        return True
    if isinstance(rvalue, CallExpr):
        if isinstance(rvalue.callee, RefExpr) and isinstance(rvalue.callee.node, FuncBase):
            typ = rvalue.callee.node.type
            if isinstance(typ, CallableType):
                return not typ.variables
            elif isinstance(typ, Overloaded):
                return not any(item.variables for item in typ.items)
    return False

</t>
<t tx="ekr.20230831011819.288">def check_simple_assignment(
    self,
    lvalue_type: Type | None,
    rvalue: Expression,
    context: Context,
    msg: ErrorMessage = message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT,
    lvalue_name: str = "variable",
    rvalue_name: str = "expression",
    *,
    notes: list[str] | None = None,
) -&gt; Type:
    if self.is_stub and isinstance(rvalue, EllipsisExpr):
        # '...' is always a valid initializer in a stub.
        return AnyType(TypeOfAny.special_form)
    else:
        always_allow_any = lvalue_type is not None and not isinstance(
            get_proper_type(lvalue_type), AnyType
        )
        rvalue_type = self.expr_checker.accept(
            rvalue, lvalue_type, always_allow_any=always_allow_any
        )
        if (
            isinstance(get_proper_type(lvalue_type), UnionType)
            # Skip literal types, as they have special logic (for better errors).
            and not isinstance(get_proper_type(rvalue_type), LiteralType)
            and not self.simple_rvalue(rvalue)
        ):
            # Try re-inferring r.h.s. in empty context, and use that if it
            # results in a narrower type. We don't do this always because this
            # may cause some perf impact, plus we want to partially preserve
            # the old behavior. This helps with various practical examples, see
            # e.g. testOptionalTypeNarrowedByGenericCall.
            with self.msg.filter_errors() as local_errors, self.local_type_map() as type_map:
                alt_rvalue_type = self.expr_checker.accept(
                    rvalue, None, always_allow_any=always_allow_any
                )
            if (
                not local_errors.has_new_errors()
                # Skip Any type, since it is special cased in binder.
                and not isinstance(get_proper_type(alt_rvalue_type), AnyType)
                and is_valid_inferred_type(alt_rvalue_type)
                and is_proper_subtype(alt_rvalue_type, rvalue_type)
            ):
                rvalue_type = alt_rvalue_type
                self.store_types(type_map)
        if isinstance(rvalue_type, DeletedType):
            self.msg.deleted_as_rvalue(rvalue_type, context)
        if isinstance(lvalue_type, DeletedType):
            self.msg.deleted_as_lvalue(lvalue_type, context)
        elif lvalue_type:
            self.check_subtype(
                # Preserve original aliases for error messages when possible.
                rvalue_type,
                lvalue_type,
                context,
                msg,
                f"{rvalue_name} has type",
                f"{lvalue_name} has type",
                notes=notes,
            )
        return rvalue_type

</t>
<t tx="ekr.20230831011819.289">def check_member_assignment(
    self, instance_type: Type, attribute_type: Type, rvalue: Expression, context: Context
) -&gt; tuple[Type, Type, bool]:
    """Type member assignment.

    This defers to check_simple_assignment, unless the member expression
    is a descriptor, in which case this checks descriptor semantics as well.

    Return the inferred rvalue_type, inferred lvalue_type, and whether to use the binder
    for this assignment.

    Note: this method exists here and not in checkmember.py, because we need to take
    care about interaction between binder and __set__().
    """
    instance_type = get_proper_type(instance_type)
    attribute_type = get_proper_type(attribute_type)
    # Descriptors don't participate in class-attribute access
    if (isinstance(instance_type, FunctionLike) and instance_type.is_type_obj()) or isinstance(
        instance_type, TypeType
    ):
        rvalue_type = self.check_simple_assignment(attribute_type, rvalue, context)
        return rvalue_type, attribute_type, True

    if not isinstance(attribute_type, Instance):
        # TODO: support __set__() for union types.
        rvalue_type = self.check_simple_assignment(attribute_type, rvalue, context)
        return rvalue_type, attribute_type, True

    mx = MemberContext(
        is_lvalue=False,
        is_super=False,
        is_operator=False,
        original_type=instance_type,
        context=context,
        self_type=None,
        msg=self.msg,
        chk=self,
    )
    get_type = analyze_descriptor_access(attribute_type, mx)
    if not attribute_type.type.has_readable_member("__set__"):
        # If there is no __set__, we type-check that the assigned value matches
        # the return type of __get__. This doesn't match the python semantics,
        # (which allow you to override the descriptor with any value), but preserves
        # the type of accessing the attribute (even after the override).
        rvalue_type = self.check_simple_assignment(get_type, rvalue, context)
        return rvalue_type, get_type, True

    dunder_set = attribute_type.type.get_method("__set__")
    if dunder_set is None:
        self.fail(
            message_registry.DESCRIPTOR_SET_NOT_CALLABLE.format(
                attribute_type.str_with_options(self.options)
            ),
            context,
        )
        return AnyType(TypeOfAny.from_error), get_type, False

    bound_method = analyze_decorator_or_funcbase_access(
        defn=dunder_set,
        itype=attribute_type,
        info=attribute_type.type,
        self_type=attribute_type,
        name="__set__",
        mx=mx,
    )
    typ = map_instance_to_supertype(attribute_type, dunder_set.info)
    dunder_set_type = expand_type_by_instance(bound_method, typ)

    callable_name = self.expr_checker.method_fullname(attribute_type, "__set__")
    dunder_set_type = self.expr_checker.transform_callee_type(
        callable_name,
        dunder_set_type,
        [TempNode(instance_type, context=context), rvalue],
        [nodes.ARG_POS, nodes.ARG_POS],
        context,
        object_type=attribute_type,
    )

    # For non-overloaded setters, the result should be type-checked like a regular assignment.
    # Hence, we first only try to infer the type by using the rvalue as type context.
    type_context = rvalue
    with self.msg.filter_errors():
        _, inferred_dunder_set_type = self.expr_checker.check_call(
            dunder_set_type,
            [TempNode(instance_type, context=context), type_context],
            [nodes.ARG_POS, nodes.ARG_POS],
            context,
            object_type=attribute_type,
            callable_name=callable_name,
        )

    # And now we in fact type check the call, to show errors related to wrong arguments
    # count, etc., replacing the type context for non-overloaded setters only.
    inferred_dunder_set_type = get_proper_type(inferred_dunder_set_type)
    if isinstance(inferred_dunder_set_type, CallableType):
        type_context = TempNode(AnyType(TypeOfAny.special_form), context=context)
    self.expr_checker.check_call(
        dunder_set_type,
        [TempNode(instance_type, context=context), type_context],
        [nodes.ARG_POS, nodes.ARG_POS],
        context,
        object_type=attribute_type,
        callable_name=callable_name,
    )

    # In the following cases, a message already will have been recorded in check_call.
    if (not isinstance(inferred_dunder_set_type, CallableType)) or (
        len(inferred_dunder_set_type.arg_types) &lt; 2
    ):
        return AnyType(TypeOfAny.from_error), get_type, False

    set_type = inferred_dunder_set_type.arg_types[1]
    # Special case: if the rvalue_type is a subtype of both '__get__' and '__set__' types,
    # and '__get__' type is narrower than '__set__', then we invoke the binder to narrow type
    # by this assignment. Technically, this is not safe, but in practice this is
    # what a user expects.
    rvalue_type = self.check_simple_assignment(set_type, rvalue, context)
    infer = is_subtype(rvalue_type, get_type) and is_subtype(get_type, set_type)
    return rvalue_type if infer else set_type, get_type, infer

</t>
<t tx="ekr.20230831011819.29">Assigns = DefaultDict[Expression, List[Tuple[Type, Optional[Type]]]]


class ConditionalTypeBinder:
    """Keep track of conditional types of variables.

    NB: Variables are tracked by literal expression, so it is possible
    to confuse the binder; for example,

    ```
    class A:
        a: Union[int, str] = None
    x = A()
    lst = [x]
    reveal_type(x.a)      # Union[int, str]
    x.a = 1
    reveal_type(x.a)      # int
    reveal_type(lst[0].a) # Union[int, str]
    lst[0].a = 'a'
    reveal_type(x.a)      # int
    reveal_type(lst[0].a) # str
    ```
    """

    @others
</t>
<t tx="ekr.20230831011819.290">def check_indexed_assignment(
    self, lvalue: IndexExpr, rvalue: Expression, context: Context
) -&gt; None:
    """Type check indexed assignment base[index] = rvalue.

    The lvalue argument is the base[index] expression.
    """
    self.try_infer_partial_type_from_indexed_assignment(lvalue, rvalue)
    basetype = get_proper_type(self.expr_checker.accept(lvalue.base))
    method_type = self.expr_checker.analyze_external_member_access(
        "__setitem__", basetype, lvalue
    )

    lvalue.method_type = method_type
    res_type, _ = self.expr_checker.check_method_call(
        "__setitem__",
        basetype,
        method_type,
        [lvalue.index, rvalue],
        [nodes.ARG_POS, nodes.ARG_POS],
        context,
    )
    res_type = get_proper_type(res_type)
    if isinstance(res_type, UninhabitedType) and not res_type.ambiguous:
        self.binder.unreachable()

</t>
<t tx="ekr.20230831011819.291">def try_infer_partial_type_from_indexed_assignment(
    self, lvalue: IndexExpr, rvalue: Expression
) -&gt; None:
    # TODO: Should we share some of this with try_infer_partial_type?
    var = None
    if isinstance(lvalue.base, RefExpr) and isinstance(lvalue.base.node, Var):
        var = lvalue.base.node
    elif isinstance(lvalue.base, MemberExpr):
        var = self.expr_checker.get_partial_self_var(lvalue.base)
    if isinstance(var, Var):
        if isinstance(var.type, PartialType):
            type_type = var.type.type
            if type_type is None:
                return  # The partial type is None.
            partial_types = self.find_partial_types(var)
            if partial_types is None:
                return
            typename = type_type.fullname
            if (
                typename == "builtins.dict"
                or typename == "collections.OrderedDict"
                or typename == "collections.defaultdict"
            ):
                # TODO: Don't infer things twice.
                key_type = self.expr_checker.accept(lvalue.index)
                value_type = self.expr_checker.accept(rvalue)
                if (
                    is_valid_inferred_type(key_type)
                    and is_valid_inferred_type(value_type)
                    and not self.current_node_deferred
                    and not (
                        typename == "collections.defaultdict"
                        and var.type.value_type is not None
                        and not is_equivalent(value_type, var.type.value_type)
                    )
                ):
                    var.type = self.named_generic_type(typename, [key_type, value_type])
                    del partial_types[var]

</t>
<t tx="ekr.20230831011819.292">def type_requires_usage(self, typ: Type) -&gt; tuple[str, ErrorCode] | None:
    """Some types require usage in all cases. The classic example is
    an unused coroutine.

    In the case that it does require usage, returns a note to attach
    to the error message.
    """
    proper_type = get_proper_type(typ)
    if isinstance(proper_type, Instance):
        # We use different error codes for generic awaitable vs coroutine.
        # Coroutines are on by default, whereas generic awaitables are not.
        if proper_type.type.fullname == "typing.Coroutine":
            return ("Are you missing an await?", UNUSED_COROUTINE)
        if proper_type.type.get("__await__") is not None:
            return ("Are you missing an await?", UNUSED_AWAITABLE)
    return None

</t>
<t tx="ekr.20230831011819.293">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    expr_type = self.expr_checker.accept(s.expr, allow_none_return=True, always_allow_any=True)
    error_note_and_code = self.type_requires_usage(expr_type)
    if error_note_and_code:
        error_note, code = error_note_and_code
        self.fail(
            message_registry.TYPE_MUST_BE_USED.format(format_type(expr_type, self.options)),
            s,
            code=code,
        )
        self.note(error_note, s, code=code)

</t>
<t tx="ekr.20230831011819.294">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    """Type check a return statement."""
    self.check_return_stmt(s)
    self.binder.unreachable()

</t>
<t tx="ekr.20230831011819.295">def check_return_stmt(self, s: ReturnStmt) -&gt; None:
    defn = self.scope.top_function()
    if defn is not None:
        if defn.is_generator:
            return_type = self.get_generator_return_type(
                self.return_types[-1], defn.is_coroutine
            )
        elif defn.is_coroutine:
            return_type = self.get_coroutine_return_type(self.return_types[-1])
        else:
            return_type = self.return_types[-1]
        return_type = get_proper_type(return_type)

        is_lambda = isinstance(self.scope.top_function(), LambdaExpr)
        if isinstance(return_type, UninhabitedType):
            # Avoid extra error messages for failed inference in lambdas
            if not is_lambda or not return_type.ambiguous:
                self.fail(message_registry.NO_RETURN_EXPECTED, s)
                return

        if s.expr:
            declared_none_return = isinstance(return_type, NoneType)
            declared_any_return = isinstance(return_type, AnyType)

            # This controls whether or not we allow a function call that
            # returns None as the expression of this return statement.
            # E.g. `return f()` for some `f` that returns None.  We allow
            # this only if we're in a lambda or in a function that returns
            # `None` or `Any`.
            allow_none_func_call = is_lambda or declared_none_return or declared_any_return

            # Return with a value.
            typ = get_proper_type(
                self.expr_checker.accept(
                    s.expr, return_type, allow_none_return=allow_none_func_call
                )
            )

            if defn.is_async_generator:
                self.fail(message_registry.RETURN_IN_ASYNC_GENERATOR, s)
                return
            # Returning a value of type Any is always fine.
            if isinstance(typ, AnyType):
                # (Unless you asked to be warned in that case, and the
                # function is not declared to return Any)
                if (
                    self.options.warn_return_any
                    and not self.current_node_deferred
                    and not is_proper_subtype(AnyType(TypeOfAny.special_form), return_type)
                    and not (
                        defn.name in BINARY_MAGIC_METHODS
                        and is_literal_not_implemented(s.expr)
                    )
                    and not (
                        isinstance(return_type, Instance)
                        and return_type.type.fullname == "builtins.object"
                    )
                    and not is_lambda
                ):
                    self.msg.incorrectly_returning_any(return_type, s)
                return

            # Disallow return expressions in functions declared to return
            # None, subject to two exceptions below.
            if declared_none_return:
                # Lambdas are allowed to have None returns.
                # Functions returning a value of type None are allowed to have a None return.
                if is_lambda or isinstance(typ, NoneType):
                    return
                self.fail(message_registry.NO_RETURN_VALUE_EXPECTED, s)
            else:
                self.check_subtype(
                    subtype_label="got",
                    subtype=typ,
                    supertype_label="expected",
                    supertype=return_type,
                    context=s.expr,
                    outer_context=s,
                    msg=message_registry.INCOMPATIBLE_RETURN_VALUE_TYPE,
                )
        else:
            # Empty returns are valid in Generators with Any typed returns, but not in
            # coroutines.
            if (
                defn.is_generator
                and not defn.is_coroutine
                and isinstance(return_type, AnyType)
            ):
                return

            if isinstance(return_type, (NoneType, AnyType)):
                return

            if self.in_checked_function():
                self.fail(message_registry.RETURN_VALUE_EXPECTED, s)

</t>
<t tx="ekr.20230831011819.296">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    """Type check an if statement."""
    # This frame records the knowledge from previous if/elif clauses not being taken.
    # Fall-through to the original frame is handled explicitly in each block.
    with self.binder.frame_context(can_skip=False, conditional_frame=True, fall_through=0):
        for e, b in zip(s.expr, s.body):
            t = get_proper_type(self.expr_checker.accept(e))

            if isinstance(t, DeletedType):
                self.msg.deleted_as_rvalue(t, s)

            if_map, else_map = self.find_isinstance_check(e)

            # XXX Issue a warning if condition is always False?
            with self.binder.frame_context(can_skip=True, fall_through=2):
                self.push_type_map(if_map)
                self.accept(b)

            # XXX Issue a warning if condition is always True?
            self.push_type_map(else_map)

        with self.binder.frame_context(can_skip=False, fall_through=2):
            if s.else_body:
                self.accept(s.else_body)

</t>
<t tx="ekr.20230831011819.297">def visit_while_stmt(self, s: WhileStmt) -&gt; None:
    """Type check a while statement."""
    if_stmt = IfStmt([s.expr], [s.body], None)
    if_stmt.set_line(s)
    self.accept_loop(if_stmt, s.else_body, exit_condition=s.expr)

</t>
<t tx="ekr.20230831011819.298">def visit_operator_assignment_stmt(self, s: OperatorAssignmentStmt) -&gt; None:
    """Type check an operator assignment statement, e.g. x += 1."""
    self.try_infer_partial_generic_type_from_assignment(s.lvalue, s.rvalue, s.op)
    if isinstance(s.lvalue, MemberExpr):
        # Special case, some additional errors may be given for
        # assignments to read-only or final attributes.
        lvalue_type = self.expr_checker.visit_member_expr(s.lvalue, True)
    else:
        lvalue_type = self.expr_checker.accept(s.lvalue)
    inplace, method = infer_operator_assignment_method(lvalue_type, s.op)
    if inplace:
        # There is __ifoo__, treat as x = x.__ifoo__(y)
        rvalue_type, method_type = self.expr_checker.check_op(method, lvalue_type, s.rvalue, s)
        if not is_subtype(rvalue_type, lvalue_type):
            self.msg.incompatible_operator_assignment(s.op, s)
    else:
        # There is no __ifoo__, treat as x = x &lt;foo&gt; y
        expr = OpExpr(s.op, s.lvalue, s.rvalue)
        expr.set_line(s)
        self.check_assignment(
            lvalue=s.lvalue, rvalue=expr, infer_lvalue_type=True, new_syntax=False
        )
    self.check_final(s)

</t>
<t tx="ekr.20230831011819.299">def visit_assert_stmt(self, s: AssertStmt) -&gt; None:
    self.expr_checker.accept(s.expr)

    if isinstance(s.expr, TupleExpr) and len(s.expr.items) &gt; 0:
        self.fail(message_registry.MALFORMED_ASSERT, s)

    # If this is asserting some isinstance check, bind that type in the following code
    true_map, else_map = self.find_isinstance_check(s.expr)
    if s.msg is not None:
        self.expr_checker.analyze_cond_branch(else_map, s.msg, None)
    self.push_type_map(true_map)

</t>
<t tx="ekr.20230831011819.3">@path mypy
# This page intentionally left blank
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.30"># Stored assignments for situations with tuple/list lvalue and rvalue of union type.
# This maps an expression to a list of bound types for every item in the union type.
type_assignments: Assigns | None = None

def __init__(self) -&gt; None:
    self.next_id = 1

    # The stack of frames currently used.  These map
    # literal_hash(expr) -- literals like 'foo.bar' --
    # to types. The last element of this list is the
    # top-most, current frame. Each earlier element
    # records the state as of when that frame was last
    # on top of the stack.
    self.frames = [Frame(self._get_id())]

    # For frames higher in the stack, we record the set of
    # Frames that can escape there, either by falling off
    # the end of the frame or by a loop control construct
    # or raised exception. The last element of self.frames
    # has no corresponding element in this list.
    self.options_on_return: list[list[Frame]] = []

    # Maps literal_hash(expr) to get_declaration(expr)
    # for every expr stored in the binder
    self.declarations: dict[Key, Type | None] = {}
    # Set of other keys to invalidate if a key is changed, e.g. x -&gt; {x.a, x[0]}
    # Whenever a new key (e.g. x.a.b) is added, we update this
    self.dependencies: dict[Key, set[Key]] = {}

    # Whether the last pop changed the newly top frame on exit
    self.last_pop_changed = False

    self.try_frames: set[int] = set()
    self.break_frames: list[int] = []
    self.continue_frames: list[int] = []

</t>
<t tx="ekr.20230831011819.300">def visit_raise_stmt(self, s: RaiseStmt) -&gt; None:
    """Type check a raise statement."""
    if s.expr:
        self.type_check_raise(s.expr, s)
    if s.from_expr:
        self.type_check_raise(s.from_expr, s, optional=True)
    self.binder.unreachable()

</t>
<t tx="ekr.20230831011819.301">def type_check_raise(self, e: Expression, s: RaiseStmt, optional: bool = False) -&gt; None:
    typ = get_proper_type(self.expr_checker.accept(e))
    if isinstance(typ, DeletedType):
        self.msg.deleted_as_rvalue(typ, e)
        return

    exc_type = self.named_type("builtins.BaseException")
    expected_type_items = [exc_type, TypeType(exc_type)]
    if optional:
        # This is used for `x` part in a case like `raise e from x`,
        # where we allow `raise e from None`.
        expected_type_items.append(NoneType())

    self.check_subtype(
        typ, UnionType.make_union(expected_type_items), s, message_registry.INVALID_EXCEPTION
    )

    if isinstance(typ, FunctionLike):
        # https://github.com/python/mypy/issues/11089
        self.expr_checker.check_call(typ, [], [], e)

</t>
<t tx="ekr.20230831011819.302">def visit_try_stmt(self, s: TryStmt) -&gt; None:
    """Type check a try statement."""
    # Our enclosing frame will get the result if the try/except falls through.
    # This one gets all possible states after the try block exited abnormally
    # (by exception, return, break, etc.)
    with self.binder.frame_context(can_skip=False, fall_through=0):
        # Not only might the body of the try statement exit
        # abnormally, but so might an exception handler or else
        # clause. The finally clause runs in *all* cases, so we
        # need an outer try frame to catch all intermediate states
        # in case an exception is raised during an except or else
        # clause. As an optimization, only create the outer try
        # frame when there actually is a finally clause.
        self.visit_try_without_finally(s, try_frame=bool(s.finally_body))
        if s.finally_body:
            # First we check finally_body is type safe on all abnormal exit paths
            self.accept(s.finally_body)

    if s.finally_body:
        # Then we try again for the more restricted set of options
        # that can fall through. (Why do we need to check the
        # finally clause twice? Depending on whether the finally
        # clause was reached by the try clause falling off the end
        # or exiting abnormally, after completing the finally clause
        # either flow will continue to after the entire try statement
        # or the exception/return/etc. will be processed and control
        # flow will escape. We need to check that the finally clause
        # type checks in both contexts, but only the resulting types
        # from the latter context affect the type state in the code
        # that follows the try statement.)
        if not self.binder.is_unreachable():
            self.accept(s.finally_body)

</t>
<t tx="ekr.20230831011819.303">def visit_try_without_finally(self, s: TryStmt, try_frame: bool) -&gt; None:
    """Type check a try statement, ignoring the finally block.

    On entry, the top frame should receive all flow that exits the
    try block abnormally (i.e., such that the else block does not
    execute), and its parent should receive all flow that exits
    the try block normally.
    """
    # This frame will run the else block if the try fell through.
    # In that case, control flow continues to the parent of what
    # was the top frame on entry.
    with self.binder.frame_context(can_skip=False, fall_through=2, try_frame=try_frame):
        # This frame receives exit via exception, and runs exception handlers
        with self.binder.frame_context(can_skip=False, conditional_frame=True, fall_through=2):
            # Finally, the body of the try statement
            with self.binder.frame_context(can_skip=False, fall_through=2, try_frame=True):
                self.accept(s.body)
            for i in range(len(s.handlers)):
                with self.binder.frame_context(can_skip=True, fall_through=4):
                    typ = s.types[i]
                    if typ:
                        t = self.check_except_handler_test(typ, s.is_star)
                        var = s.vars[i]
                        if var:
                            # To support local variables, we make this a definition line,
                            # causing assignment to set the variable's type.
                            var.is_inferred_def = True
                            self.check_assignment(var, self.temp_node(t, var))
                    self.accept(s.handlers[i])
                    var = s.vars[i]
                    if var:
                        # Exception variables are deleted.
                        # Unfortunately, this doesn't let us detect usage before the
                        # try/except block.
                        source = var.name
                        if isinstance(var.node, Var):
                            var.node.type = DeletedType(source=source)
                        self.binder.cleanse(var)
        if s.else_body:
            self.accept(s.else_body)

</t>
<t tx="ekr.20230831011819.304">def check_except_handler_test(self, n: Expression, is_star: bool) -&gt; Type:
    """Type check an exception handler test clause."""
    typ = self.expr_checker.accept(n)

    all_types: list[Type] = []
    test_types = self.get_types_from_except_handler(typ, n)

    for ttype in get_proper_types(test_types):
        if isinstance(ttype, AnyType):
            all_types.append(ttype)
            continue

        if isinstance(ttype, FunctionLike):
            item = ttype.items[0]
            if not item.is_type_obj():
                self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
                return self.default_exception_type(is_star)
            exc_type = erase_typevars(item.ret_type)
        elif isinstance(ttype, TypeType):
            exc_type = ttype.item
        else:
            self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
            return self.default_exception_type(is_star)

        if not is_subtype(exc_type, self.named_type("builtins.BaseException")):
            self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
            return self.default_exception_type(is_star)

        all_types.append(exc_type)

    if is_star:
        new_all_types: list[Type] = []
        for typ in all_types:
            if is_proper_subtype(typ, self.named_type("builtins.BaseExceptionGroup")):
                self.fail(message_registry.INVALID_EXCEPTION_GROUP, n)
                new_all_types.append(AnyType(TypeOfAny.from_error))
            else:
                new_all_types.append(typ)
        return self.wrap_exception_group(new_all_types)
    return make_simplified_union(all_types)

</t>
<t tx="ekr.20230831011819.305">def default_exception_type(self, is_star: bool) -&gt; Type:
    """Exception type to return in case of a previous type error."""
    any_type = AnyType(TypeOfAny.from_error)
    if is_star:
        return self.named_generic_type("builtins.ExceptionGroup", [any_type])
    return any_type

</t>
<t tx="ekr.20230831011819.306">def wrap_exception_group(self, types: Sequence[Type]) -&gt; Type:
    """Transform except* variable type into an appropriate exception group."""
    arg = make_simplified_union(types)
    if is_subtype(arg, self.named_type("builtins.Exception")):
        base = "builtins.ExceptionGroup"
    else:
        base = "builtins.BaseExceptionGroup"
    return self.named_generic_type(base, [arg])

</t>
<t tx="ekr.20230831011819.307">def get_types_from_except_handler(self, typ: Type, n: Expression) -&gt; list[Type]:
    """Helper for check_except_handler_test to retrieve handler types."""
    typ = get_proper_type(typ)
    if isinstance(typ, TupleType):
        return typ.items
    elif isinstance(typ, UnionType):
        return [
            union_typ
            for item in typ.relevant_items()
            for union_typ in self.get_types_from_except_handler(item, n)
        ]
    elif is_named_instance(typ, "builtins.tuple"):
        # variadic tuple
        return [typ.args[0]]
    else:
        return [typ]

</t>
<t tx="ekr.20230831011819.308">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    """Type check a for statement."""
    if s.is_async:
        iterator_type, item_type = self.analyze_async_iterable_item_type(s.expr)
    else:
        iterator_type, item_type = self.analyze_iterable_item_type(s.expr)
    s.inferred_item_type = item_type
    s.inferred_iterator_type = iterator_type
    self.analyze_index_variables(s.index, item_type, s.index_type is None, s)
    self.accept_loop(s.body, s.else_body)

</t>
<t tx="ekr.20230831011819.309">def analyze_async_iterable_item_type(self, expr: Expression) -&gt; tuple[Type, Type]:
    """Analyse async iterable expression and return iterator and iterator item types."""
    echk = self.expr_checker
    iterable = echk.accept(expr)
    iterator = echk.check_method_call_by_name("__aiter__", iterable, [], [], expr)[0]
    awaitable = echk.check_method_call_by_name("__anext__", iterator, [], [], expr)[0]
    item_type = echk.check_awaitable_expr(
        awaitable, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_FOR
    )
    return iterator, item_type

</t>
<t tx="ekr.20230831011819.31">def _get_id(self) -&gt; int:
    self.next_id += 1
    return self.next_id

</t>
<t tx="ekr.20230831011819.310">def analyze_iterable_item_type(self, expr: Expression) -&gt; tuple[Type, Type]:
    """Analyse iterable expression and return iterator and iterator item types."""
    echk = self.expr_checker
    iterable = get_proper_type(echk.accept(expr))
    iterator = echk.check_method_call_by_name("__iter__", iterable, [], [], expr)[0]

    int_type = self.analyze_range_native_int_type(expr)
    if int_type:
        return iterator, int_type

    if (
        isinstance(iterable, TupleType)
        and iterable.partial_fallback.type.fullname == "builtins.tuple"
    ):
        return iterator, tuple_fallback(iterable).args[0]
    else:
        # Non-tuple iterable.
        return iterator, echk.check_method_call_by_name("__next__", iterator, [], [], expr)[0]

</t>
<t tx="ekr.20230831011819.311">def analyze_iterable_item_type_without_expression(
    self, type: Type, context: Context
) -&gt; tuple[Type, Type]:
    """Analyse iterable type and return iterator and iterator item types."""
    echk = self.expr_checker
    iterable = get_proper_type(type)
    iterator = echk.check_method_call_by_name("__iter__", iterable, [], [], context)[0]

    if isinstance(iterable, TupleType):
        joined: Type = UninhabitedType()
        for item in iterable.items:
            joined = join_types(joined, item)
        return iterator, joined
    else:
        # Non-tuple iterable.
        return (
            iterator,
            echk.check_method_call_by_name("__next__", iterator, [], [], context)[0],
        )

</t>
<t tx="ekr.20230831011819.312">def analyze_range_native_int_type(self, expr: Expression) -&gt; Type | None:
    """Try to infer native int item type from arguments to range(...).

    For example, return i64 if the expression is "range(0, i64(n))".

    Return None if unsuccessful.
    """
    if (
        isinstance(expr, CallExpr)
        and isinstance(expr.callee, RefExpr)
        and expr.callee.fullname == "builtins.range"
        and 1 &lt;= len(expr.args) &lt;= 3
        and all(kind == ARG_POS for kind in expr.arg_kinds)
    ):
        native_int: Type | None = None
        ok = True
        for arg in expr.args:
            argt = get_proper_type(self.lookup_type(arg))
            if isinstance(argt, Instance) and argt.type.fullname in MYPYC_NATIVE_INT_NAMES:
                if native_int is None:
                    native_int = argt
                elif argt != native_int:
                    ok = False
        if ok and native_int:
            return native_int
    return None

</t>
<t tx="ekr.20230831011819.313">def analyze_container_item_type(self, typ: Type) -&gt; Type | None:
    """Check if a type is a nominal container of a union of such.

    Return the corresponding container item type.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        types: list[Type] = []
        for item in typ.items:
            c_type = self.analyze_container_item_type(item)
            if c_type:
                types.append(c_type)
        return UnionType.make_union(types)
    if isinstance(typ, Instance) and typ.type.has_base("typing.Container"):
        supertype = self.named_type("typing.Container").type
        super_instance = map_instance_to_supertype(typ, supertype)
        assert len(super_instance.args) == 1
        return super_instance.args[0]
    if isinstance(typ, TupleType):
        return self.analyze_container_item_type(tuple_fallback(typ))
    return None

</t>
<t tx="ekr.20230831011819.314">def analyze_index_variables(
    self, index: Expression, item_type: Type, infer_lvalue_type: bool, context: Context
) -&gt; None:
    """Type check or infer for loop or list comprehension index vars."""
    self.check_assignment(index, self.temp_node(item_type, context), infer_lvalue_type)

</t>
<t tx="ekr.20230831011819.315">def visit_del_stmt(self, s: DelStmt) -&gt; None:
    if isinstance(s.expr, IndexExpr):
        e = s.expr
        m = MemberExpr(e.base, "__delitem__")
        m.line = s.line
        m.column = s.column
        c = CallExpr(m, [e.index], [nodes.ARG_POS], [None])
        c.line = s.line
        c.column = s.column
        self.expr_checker.accept(c, allow_none_return=True)
    else:
        s.expr.accept(self.expr_checker)
        for elt in flatten(s.expr):
            if isinstance(elt, NameExpr):
                self.binder.assign_type(
                    elt, DeletedType(source=elt.name), get_declaration(elt), False
                )

</t>
<t tx="ekr.20230831011819.316">def visit_decorator(self, e: Decorator) -&gt; None:
    for d in e.decorators:
        if isinstance(d, RefExpr):
            if d.fullname == "typing.no_type_check":
                e.var.type = AnyType(TypeOfAny.special_form)
                e.var.is_ready = True
                return
    self.visit_decorator_inner(e)

</t>
<t tx="ekr.20230831011819.317">def visit_decorator_inner(self, e: Decorator, allow_empty: bool = False) -&gt; None:
    if self.recurse_into_functions:
        with self.tscope.function_scope(e.func):
            self.check_func_item(e.func, name=e.func.name, allow_empty=allow_empty)

    # Process decorators from the inside out to determine decorated signature, which
    # may be different from the declared signature.
    sig: Type = self.function_type(e.func)
    for d in reversed(e.decorators):
        if refers_to_fullname(d, OVERLOAD_NAMES):
            if not allow_empty:
                self.fail(message_registry.MULTIPLE_OVERLOADS_REQUIRED, e)
            continue
        dec = self.expr_checker.accept(d)
        temp = self.temp_node(sig, context=e)
        fullname = None
        if isinstance(d, RefExpr):
            fullname = d.fullname or None
        # if this is a expression like @b.a where b is an object, get the type of b
        # so we can pass it the method hook in the plugins
        object_type: Type | None = None
        if fullname is None and isinstance(d, MemberExpr) and self.has_type(d.expr):
            object_type = self.lookup_type(d.expr)
            fullname = self.expr_checker.method_fullname(object_type, d.name)
        self.check_for_untyped_decorator(e.func, dec, d)
        sig, t2 = self.expr_checker.check_call(
            dec, [temp], [nodes.ARG_POS], e, callable_name=fullname, object_type=object_type
        )
    self.check_untyped_after_decorator(sig, e.func)
    sig = set_callable_name(sig, e.func)
    e.var.type = sig
    e.var.is_ready = True
    if e.func.is_property:
        if isinstance(sig, CallableType):
            if len([k for k in sig.arg_kinds if k.is_required()]) &gt; 1:
                self.msg.fail("Too many arguments for property", e)
        self.check_incompatible_property_override(e)
    # For overloaded functions we already checked override for overload as a whole.
    if allow_empty:
        return
    if e.func.info and not e.func.is_dynamic() and not e.is_overload:
        found_method_base_classes = self.check_method_override(e)
        if (
            e.func.is_explicit_override
            and not found_method_base_classes
            and found_method_base_classes is not None
        ):
            self.msg.no_overridable_method(e.func.name, e.func)
        self.check_explicit_override_decorator(e.func, found_method_base_classes)

    if e.func.info and e.func.name in ("__init__", "__new__"):
        if e.type and not isinstance(get_proper_type(e.type), (FunctionLike, AnyType)):
            self.fail(message_registry.BAD_CONSTRUCTOR_TYPE, e)

</t>
<t tx="ekr.20230831011819.318">def check_for_untyped_decorator(
    self, func: FuncDef, dec_type: Type, dec_expr: Expression
) -&gt; None:
    if (
        self.options.disallow_untyped_decorators
        and is_typed_callable(func.type)
        and is_untyped_decorator(dec_type)
    ):
        self.msg.typed_function_untyped_decorator(func.name, dec_expr)

</t>
<t tx="ekr.20230831011819.319">def check_incompatible_property_override(self, e: Decorator) -&gt; None:
    if not e.var.is_settable_property and e.func.info:
        name = e.func.name
        for base in e.func.info.mro[1:]:
            base_attr = base.names.get(name)
            if not base_attr:
                continue
            if (
                isinstance(base_attr.node, OverloadedFuncDef)
                and base_attr.node.is_property
                and cast(Decorator, base_attr.node.items[0]).var.is_settable_property
            ):
                self.fail(message_registry.READ_ONLY_PROPERTY_OVERRIDES_READ_WRITE, e)

</t>
<t tx="ekr.20230831011819.32">def _add_dependencies(self, key: Key, value: Key | None = None) -&gt; None:
    if value is None:
        value = key
    else:
        self.dependencies.setdefault(key, set()).add(value)
    for elt in subkeys(key):
        self._add_dependencies(elt, value)

</t>
<t tx="ekr.20230831011819.320">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    exceptions_maybe_suppressed = False
    for expr, target in zip(s.expr, s.target):
        if s.is_async:
            exit_ret_type = self.check_async_with_item(expr, target, s.unanalyzed_type is None)
        else:
            exit_ret_type = self.check_with_item(expr, target, s.unanalyzed_type is None)

        # Based on the return type, determine if this context manager 'swallows'
        # exceptions or not. We determine this using a heuristic based on the
        # return type of the __exit__ method -- see the discussion in
        # https://github.com/python/mypy/issues/7214 and the section about context managers
        # in https://github.com/python/typeshed/blob/main/CONTRIBUTING.md#conventions
        # for more details.

        exit_ret_type = get_proper_type(exit_ret_type)
        if is_literal_type(exit_ret_type, "builtins.bool", False):
            continue

        if is_literal_type(exit_ret_type, "builtins.bool", True) or (
            isinstance(exit_ret_type, Instance)
            and exit_ret_type.type.fullname == "builtins.bool"
            and state.strict_optional
        ):
            # Note: if strict-optional is disabled, this bool instance
            # could actually be an Optional[bool].
            exceptions_maybe_suppressed = True

    if exceptions_maybe_suppressed:
        # Treat this 'with' block in the same way we'd treat a 'try: BODY; except: pass'
        # block. This means control flow can continue after the 'with' even if the 'with'
        # block immediately returns.
        with self.binder.frame_context(can_skip=True, try_frame=True):
            self.accept(s.body)
    else:
        self.accept(s.body)

</t>
<t tx="ekr.20230831011819.321">def check_untyped_after_decorator(self, typ: Type, func: FuncDef) -&gt; None:
    if not self.options.disallow_any_decorated or self.is_stub:
        return

    if mypy.checkexpr.has_any_type(typ):
        self.msg.untyped_decorated_function(typ, func)

</t>
<t tx="ekr.20230831011819.322">def check_async_with_item(
    self, expr: Expression, target: Expression | None, infer_lvalue_type: bool
) -&gt; Type:
    echk = self.expr_checker
    ctx = echk.accept(expr)
    obj = echk.check_method_call_by_name("__aenter__", ctx, [], [], expr)[0]
    obj = echk.check_awaitable_expr(
        obj, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AENTER
    )
    if target:
        self.check_assignment(target, self.temp_node(obj, expr), infer_lvalue_type)
    arg = self.temp_node(AnyType(TypeOfAny.special_form), expr)
    res, _ = echk.check_method_call_by_name(
        "__aexit__", ctx, [arg] * 3, [nodes.ARG_POS] * 3, expr
    )
    return echk.check_awaitable_expr(
        res, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AEXIT
    )

</t>
<t tx="ekr.20230831011819.323">def check_with_item(
    self, expr: Expression, target: Expression | None, infer_lvalue_type: bool
) -&gt; Type:
    echk = self.expr_checker
    ctx = echk.accept(expr)
    obj = echk.check_method_call_by_name("__enter__", ctx, [], [], expr)[0]
    if target:
        self.check_assignment(target, self.temp_node(obj, expr), infer_lvalue_type)
    arg = self.temp_node(AnyType(TypeOfAny.special_form), expr)
    res, _ = echk.check_method_call_by_name(
        "__exit__", ctx, [arg] * 3, [nodes.ARG_POS] * 3, expr
    )
    return res

</t>
<t tx="ekr.20230831011819.324">def visit_break_stmt(self, s: BreakStmt) -&gt; None:
    self.binder.handle_break()

</t>
<t tx="ekr.20230831011819.325">def visit_continue_stmt(self, s: ContinueStmt) -&gt; None:
    self.binder.handle_continue()
    return None

</t>
<t tx="ekr.20230831011819.326">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    with self.binder.frame_context(can_skip=False, fall_through=0):
        subject_type = get_proper_type(self.expr_checker.accept(s.subject))

        if isinstance(subject_type, DeletedType):
            self.msg.deleted_as_rvalue(subject_type, s)

        # We infer types of patterns twice. The first pass is used
        # to infer the types of capture variables. The type of a
        # capture variable may depend on multiple patterns (it
        # will be a union of all capture types). This pass ignores
        # guard expressions.
        pattern_types = [self.pattern_checker.accept(p, subject_type) for p in s.patterns]
        type_maps: list[TypeMap] = [t.captures for t in pattern_types]
        inferred_types = self.infer_variable_types_from_type_maps(type_maps)

        # The second pass narrows down the types and type checks bodies.
        for p, g, b in zip(s.patterns, s.guards, s.bodies):
            current_subject_type = self.expr_checker.narrow_type_from_binder(
                s.subject, subject_type
            )
            pattern_type = self.pattern_checker.accept(p, current_subject_type)
            with self.binder.frame_context(can_skip=True, fall_through=2):
                if b.is_unreachable or isinstance(
                    get_proper_type(pattern_type.type), UninhabitedType
                ):
                    self.push_type_map(None)
                    else_map: TypeMap = {}
                else:
                    pattern_map, else_map = conditional_types_to_typemaps(
                        s.subject, pattern_type.type, pattern_type.rest_type
                    )
                    self.remove_capture_conflicts(pattern_type.captures, inferred_types)
                    self.push_type_map(pattern_map)
                    self.push_type_map(pattern_type.captures)
                if g is not None:
                    with self.binder.frame_context(can_skip=False, fall_through=3):
                        gt = get_proper_type(self.expr_checker.accept(g))

                        if isinstance(gt, DeletedType):
                            self.msg.deleted_as_rvalue(gt, s)

                        guard_map, guard_else_map = self.find_isinstance_check(g)
                        else_map = or_conditional_maps(else_map, guard_else_map)

                        # If the guard narrowed the subject, copy the narrowed types over
                        if isinstance(p, AsPattern):
                            case_target = p.pattern or p.name
                            if isinstance(case_target, NameExpr):
                                for type_map in (guard_map, else_map):
                                    if not type_map:
                                        continue
                                    for expr in list(type_map):
                                        if not (
                                            isinstance(expr, NameExpr)
                                            and expr.fullname == case_target.fullname
                                        ):
                                            continue
                                        type_map[s.subject] = type_map[expr]

                        self.push_type_map(guard_map)
                        self.accept(b)
                else:
                    self.accept(b)
            self.push_type_map(else_map)

        # This is needed due to a quirk in frame_context. Without it types will stay narrowed
        # after the match.
        with self.binder.frame_context(can_skip=False, fall_through=2):
            pass

</t>
<t tx="ekr.20230831011819.327">def infer_variable_types_from_type_maps(self, type_maps: list[TypeMap]) -&gt; dict[Var, Type]:
    all_captures: dict[Var, list[tuple[NameExpr, Type]]] = defaultdict(list)
    for tm in type_maps:
        if tm is not None:
            for expr, typ in tm.items():
                if isinstance(expr, NameExpr):
                    node = expr.node
                    assert isinstance(node, Var)
                    all_captures[node].append((expr, typ))

    inferred_types: dict[Var, Type] = {}
    for var, captures in all_captures.items():
        already_exists = False
        types: list[Type] = []
        for expr, typ in captures:
            types.append(typ)

            previous_type, _, _ = self.check_lvalue(expr)
            if previous_type is not None:
                already_exists = True
                if self.check_subtype(
                    typ,
                    previous_type,
                    expr,
                    msg=message_registry.INCOMPATIBLE_TYPES_IN_CAPTURE,
                    subtype_label="pattern captures type",
                    supertype_label="variable has type",
                ):
                    inferred_types[var] = previous_type

        if not already_exists:
            new_type = UnionType.make_union(types)
            # Infer the union type at the first occurrence
            first_occurrence, _ = captures[0]
            inferred_types[var] = new_type
            self.infer_variable_type(var, first_occurrence, new_type, first_occurrence)
    return inferred_types

</t>
<t tx="ekr.20230831011819.328">def remove_capture_conflicts(self, type_map: TypeMap, inferred_types: dict[Var, Type]) -&gt; None:
    if type_map:
        for expr, typ in list(type_map.items()):
            if isinstance(expr, NameExpr):
                node = expr.node
                assert isinstance(node, Var)
                if node not in inferred_types or not is_subtype(typ, inferred_types[node]):
                    del type_map[expr]

</t>
<t tx="ekr.20230831011819.329">def make_fake_typeinfo(
    self,
    curr_module_fullname: str,
    class_gen_name: str,
    class_short_name: str,
    bases: list[Instance],
) -&gt; tuple[ClassDef, TypeInfo]:
    # Build the fake ClassDef and TypeInfo together.
    # The ClassDef is full of lies and doesn't actually contain a body.
    # Use format_bare to generate a nice name for error messages.
    # We skip fully filling out a handful of TypeInfo fields because they
    # should be irrelevant for a generated type like this:
    # is_protocol, protocol_members, is_abstract
    cdef = ClassDef(class_short_name, Block([]))
    cdef.fullname = curr_module_fullname + "." + class_gen_name
    info = TypeInfo(SymbolTable(), cdef, curr_module_fullname)
    cdef.info = info
    info.bases = bases
    calculate_mro(info)
    info.metaclass_type = info.calculate_metaclass_type()
    return cdef, info

</t>
<t tx="ekr.20230831011819.33">def push_frame(self, conditional_frame: bool = False) -&gt; Frame:
    """Push a new frame into the binder."""
    f = Frame(self._get_id(), conditional_frame)
    self.frames.append(f)
    self.options_on_return.append([])
    return f

</t>
<t tx="ekr.20230831011819.330">def intersect_instances(
    self, instances: tuple[Instance, Instance], errors: list[tuple[str, str]]
) -&gt; Instance | None:
    """Try creating an ad-hoc intersection of the given instances.

    Note that this function does *not* try and create a full-fledged
    intersection type. Instead, it returns an instance of a new ad-hoc
    subclass of the given instances.

    This is mainly useful when you need a way of representing some
    theoretical subclass of the instances the user may be trying to use
    the generated intersection can serve as a placeholder.

    This function will create a fresh subclass every time you call it,
    even if you pass in the exact same arguments. So this means calling
    `self.intersect_intersection([inst_1, inst_2], ctx)` twice will result
    in instances of two distinct subclasses of inst_1 and inst_2.

    This is by design: we want each ad-hoc intersection to be unique since
    they're supposed represent some other unknown subclass.

    Returns None if creating the subclass is impossible (e.g. due to
    MRO errors or incompatible signatures). If we do successfully create
    a subclass, its TypeInfo will automatically be added to the global scope.
    """
    curr_module = self.scope.stack[0]
    assert isinstance(curr_module, MypyFile)

    # First, retry narrowing while allowing promotions (they are disabled by default
    # for isinstance() checks, etc). This way we will still type-check branches like
    # x: complex = 1
    # if isinstance(x, int):
    #     ...
    left, right = instances
    if is_proper_subtype(left, right, ignore_promotions=False):
        return left
    if is_proper_subtype(right, left, ignore_promotions=False):
        return right

    def _get_base_classes(instances_: tuple[Instance, Instance]) -&gt; list[Instance]:
        base_classes_ = []
        for inst in instances_:
            if inst.type.is_intersection:
                expanded = inst.type.bases
            else:
                expanded = [inst]

            for expanded_inst in expanded:
                base_classes_.append(expanded_inst)
        return base_classes_

    def _make_fake_typeinfo_and_full_name(
        base_classes_: list[Instance], curr_module_: MypyFile
    ) -&gt; tuple[TypeInfo, str]:
        names_list = pretty_seq([x.type.name for x in base_classes_], "and")
        short_name = f"&lt;subclass of {names_list}&gt;"
        full_name_ = gen_unique_name(short_name, curr_module_.names)
        cdef, info_ = self.make_fake_typeinfo(
            curr_module_.fullname, full_name_, short_name, base_classes_
        )
        return info_, full_name_

    base_classes = _get_base_classes(instances)
    # We use the pretty_names_list for error messages but can't
    # use it for the real name that goes into the symbol table
    # because it can have dots in it.
    pretty_names_list = pretty_seq(
        format_type_distinctly(*base_classes, options=self.options, bare=True), "and"
    )
    try:
        info, full_name = _make_fake_typeinfo_and_full_name(base_classes, curr_module)
        with self.msg.filter_errors() as local_errors:
            self.check_multiple_inheritance(info)
        if local_errors.has_new_errors():
            # "class A(B, C)" unsafe, now check "class A(C, B)":
            base_classes = _get_base_classes(instances[::-1])
            info, full_name = _make_fake_typeinfo_and_full_name(base_classes, curr_module)
            with self.msg.filter_errors() as local_errors:
                self.check_multiple_inheritance(info)
        info.is_intersection = True
    except MroError:
        errors.append((pretty_names_list, "inconsistent method resolution order"))
        return None
    if local_errors.has_new_errors():
        errors.append((pretty_names_list, "incompatible method signatures"))
        return None

    curr_module.names[full_name] = SymbolTableNode(GDEF, info)
    return Instance(info, [], extra_attrs=instances[0].extra_attrs or instances[1].extra_attrs)

</t>
<t tx="ekr.20230831011819.331">def intersect_instance_callable(self, typ: Instance, callable_type: CallableType) -&gt; Instance:
    """Creates a fake type that represents the intersection of an Instance and a CallableType.

    It operates by creating a bare-minimum dummy TypeInfo that
    subclasses type and adds a __call__ method matching callable_type.
    """

    # In order for this to work in incremental mode, the type we generate needs to
    # have a valid fullname and a corresponding entry in a symbol table. We generate
    # a unique name inside the symbol table of the current module.
    cur_module = self.scope.stack[0]
    assert isinstance(cur_module, MypyFile)
    gen_name = gen_unique_name(f"&lt;callable subtype of {typ.type.name}&gt;", cur_module.names)

    # Synthesize a fake TypeInfo
    short_name = format_type_bare(typ, self.options)
    cdef, info = self.make_fake_typeinfo(cur_module.fullname, gen_name, short_name, [typ])

    # Build up a fake FuncDef so we can populate the symbol table.
    func_def = FuncDef("__call__", [], Block([]), callable_type)
    func_def._fullname = cdef.fullname + ".__call__"
    func_def.info = info
    info.names["__call__"] = SymbolTableNode(MDEF, func_def)

    cur_module.names[gen_name] = SymbolTableNode(GDEF, info)

    return Instance(info, [], extra_attrs=typ.extra_attrs)

</t>
<t tx="ekr.20230831011819.332">def make_fake_callable(self, typ: Instance) -&gt; Instance:
    """Produce a new type that makes type Callable with a generic callable type."""

    fallback = self.named_type("builtins.function")
    callable_type = CallableType(
        [AnyType(TypeOfAny.explicit), AnyType(TypeOfAny.explicit)],
        [nodes.ARG_STAR, nodes.ARG_STAR2],
        [None, None],
        ret_type=AnyType(TypeOfAny.explicit),
        fallback=fallback,
        is_ellipsis_args=True,
    )

    return self.intersect_instance_callable(typ, callable_type)

</t>
<t tx="ekr.20230831011819.333">def partition_by_callable(
    self, typ: Type, unsound_partition: bool
) -&gt; tuple[list[Type], list[Type]]:
    """Partitions a type into callable subtypes and uncallable subtypes.

    Thus, given:
    `callables, uncallables = partition_by_callable(type)`

    If we assert `callable(type)` then `type` has type Union[*callables], and
    If we assert `not callable(type)` then `type` has type Union[*uncallables]

    If unsound_partition is set, assume that anything that is not
    clearly callable is in fact not callable. Otherwise we generate a
    new subtype that *is* callable.

    Guaranteed to not return [], [].
    """
    typ = get_proper_type(typ)

    if isinstance(typ, (FunctionLike, TypeType)):
        return [typ], []

    if isinstance(typ, AnyType):
        return [typ], [typ]

    if isinstance(typ, NoneType):
        return [], [typ]

    if isinstance(typ, UnionType):
        callables = []
        uncallables = []
        for subtype in typ.items:
            # Use unsound_partition when handling unions in order to
            # allow the expected type discrimination.
            subcallables, subuncallables = self.partition_by_callable(
                subtype, unsound_partition=True
            )
            callables.extend(subcallables)
            uncallables.extend(subuncallables)
        return callables, uncallables

    if isinstance(typ, TypeVarType):
        # We could do better probably?
        # Refine the the type variable's bound as our type in the case that
        # callable() is true. This unfortunately loses the information that
        # the type is a type variable in that branch.
        # This matches what is done for isinstance, but it may be possible to
        # do better.
        # If it is possible for the false branch to execute, return the original
        # type to avoid losing type information.
        callables, uncallables = self.partition_by_callable(
            erase_to_union_or_bound(typ), unsound_partition
        )
        uncallables = [typ] if uncallables else []
        return callables, uncallables

    # A TupleType is callable if its fallback is, but needs special handling
    # when we dummy up a new type.
    ityp = typ
    if isinstance(typ, TupleType):
        ityp = tuple_fallback(typ)

    if isinstance(ityp, Instance):
        method = ityp.type.get_method("__call__")
        if method and method.type:
            callables, uncallables = self.partition_by_callable(
                method.type, unsound_partition=False
            )
            if callables and not uncallables:
                # Only consider the type callable if its __call__ method is
                # definitely callable.
                return [typ], []

        if not unsound_partition:
            fake = self.make_fake_callable(ityp)
            if isinstance(typ, TupleType):
                fake.type.tuple_type = TupleType(typ.items, fake)
                return [fake.type.tuple_type], [typ]
            return [fake], [typ]

    if unsound_partition:
        return [], [typ]
    else:
        # We don't know how properly make the type callable.
        return [typ], [typ]

</t>
<t tx="ekr.20230831011819.334">def conditional_callable_type_map(
    self, expr: Expression, current_type: Type | None
) -&gt; tuple[TypeMap, TypeMap]:
    """Takes in an expression and the current type of the expression.

    Returns a 2-tuple: The first element is a map from the expression to
    the restricted type if it were callable. The second element is a
    map from the expression to the type it would hold if it weren't
    callable.
    """
    if not current_type:
        return {}, {}

    if isinstance(get_proper_type(current_type), AnyType):
        return {}, {}

    callables, uncallables = self.partition_by_callable(current_type, unsound_partition=False)

    if callables and uncallables:
        callable_map = {expr: UnionType.make_union(callables)} if callables else None
        uncallable_map = {expr: UnionType.make_union(uncallables)} if uncallables else None
        return callable_map, uncallable_map

    elif callables:
        return {}, None

    return None, {}

</t>
<t tx="ekr.20230831011819.335">def conditional_types_for_iterable(
    self, item_type: Type, iterable_type: Type
) -&gt; tuple[Type | None, Type | None]:
    """
    Narrows the type of `iterable_type` based on the type of `item_type`.
    For now, we only support narrowing unions of TypedDicts based on left operand being literal string(s).
    """
    if_types: list[Type] = []
    else_types: list[Type] = []

    iterable_type = get_proper_type(iterable_type)
    if isinstance(iterable_type, UnionType):
        possible_iterable_types = get_proper_types(iterable_type.relevant_items())
    else:
        possible_iterable_types = [iterable_type]

    item_str_literals = try_getting_str_literals_from_type(item_type)

    for possible_iterable_type in possible_iterable_types:
        if item_str_literals and isinstance(possible_iterable_type, TypedDictType):
            for key in item_str_literals:
                if key in possible_iterable_type.required_keys:
                    if_types.append(possible_iterable_type)
                elif (
                    key in possible_iterable_type.items or not possible_iterable_type.is_final
                ):
                    if_types.append(possible_iterable_type)
                    else_types.append(possible_iterable_type)
                else:
                    else_types.append(possible_iterable_type)
        else:
            if_types.append(possible_iterable_type)
            else_types.append(possible_iterable_type)

    return (
        UnionType.make_union(if_types) if if_types else None,
        UnionType.make_union(else_types) if else_types else None,
    )

</t>
<t tx="ekr.20230831011819.336">def _is_truthy_type(self, t: ProperType) -&gt; bool:
    return (
        (
            isinstance(t, Instance)
            and bool(t.type)
            and not t.type.has_readable_member("__bool__")
            and not t.type.has_readable_member("__len__")
            and t.type.fullname != "builtins.object"
        )
        or isinstance(t, FunctionLike)
        or (
            isinstance(t, UnionType)
            and all(self._is_truthy_type(t) for t in get_proper_types(t.items))
        )
    )

</t>
<t tx="ekr.20230831011819.337">def _check_for_truthy_type(self, t: Type, expr: Expression) -&gt; None:
    if not state.strict_optional:
        return  # if everything can be None, all bets are off

    t = get_proper_type(t)
    if not self._is_truthy_type(t):
        return

    def format_expr_type() -&gt; str:
        typ = format_type(t, self.options)
        if isinstance(expr, MemberExpr):
            return f'Member "{expr.name}" has type {typ}'
        elif isinstance(expr, RefExpr) and expr.fullname:
            return f'"{expr.fullname}" has type {typ}'
        elif isinstance(expr, CallExpr):
            if isinstance(expr.callee, MemberExpr):
                return f'"{expr.callee.name}" returns {typ}'
            elif isinstance(expr.callee, RefExpr) and expr.callee.fullname:
                return f'"{expr.callee.fullname}" returns {typ}'
            return f"Call returns {typ}"
        else:
            return f"Expression has type {typ}"

    def get_expr_name() -&gt; str:
        if isinstance(expr, (NameExpr, MemberExpr)):
            return f'"{expr.name}"'
        else:
            # return type if expr has no name
            return format_type(t, self.options)

    if isinstance(t, FunctionLike):
        self.fail(message_registry.FUNCTION_ALWAYS_TRUE.format(get_expr_name()), expr)
    elif isinstance(t, UnionType):
        self.fail(message_registry.TYPE_ALWAYS_TRUE_UNIONTYPE.format(format_expr_type()), expr)
    elif isinstance(t, Instance) and t.type.fullname == "typing.Iterable":
        _, info = self.make_fake_typeinfo("typing", "Collection", "Collection", [])
        self.fail(
            message_registry.ITERABLE_ALWAYS_TRUE.format(
                format_expr_type(), format_type(Instance(info, t.args), self.options)
            ),
            expr,
        )
    else:
        self.fail(message_registry.TYPE_ALWAYS_TRUE.format(format_expr_type()), expr)

</t>
<t tx="ekr.20230831011819.338">def find_type_equals_check(
    self, node: ComparisonExpr, expr_indices: list[int]
) -&gt; tuple[TypeMap, TypeMap]:
    """Narrow types based on any checks of the type ``type(x) == T``

    Args:
        node: The node that might contain the comparison
        expr_indices: The list of indices of expressions in ``node`` that are being
            compared
    """

    def is_type_call(expr: CallExpr) -&gt; bool:
        """Is expr a call to type with one argument?"""
        return refers_to_fullname(expr.callee, "builtins.type") and len(expr.args) == 1

    # exprs that are being passed into type
    exprs_in_type_calls: list[Expression] = []
    # type that is being compared to type(expr)
    type_being_compared: list[TypeRange] | None = None
    # whether the type being compared to is final
    is_final = False

    for index in expr_indices:
        expr = node.operands[index]

        if isinstance(expr, CallExpr) and is_type_call(expr):
            exprs_in_type_calls.append(expr.args[0])
        else:
            current_type = self.get_isinstance_type(expr)
            if current_type is None:
                continue
            if type_being_compared is not None:
                # It doesn't really make sense to have several types being
                # compared to the output of type (like type(x) == int == str)
                # because whether that's true is solely dependent on what the
                # types being compared are, so we don't try to narrow types any
                # further because we can't really get any information about the
                # type of x from that check
                return {}, {}
            else:
                if isinstance(expr, RefExpr) and isinstance(expr.node, TypeInfo):
                    is_final = expr.node.is_final
                type_being_compared = current_type

    if not exprs_in_type_calls:
        return {}, {}

    if_maps: list[TypeMap] = []
    else_maps: list[TypeMap] = []
    for expr in exprs_in_type_calls:
        current_if_type, current_else_type = self.conditional_types_with_intersection(
            self.lookup_type(expr), type_being_compared, expr
        )
        current_if_map, current_else_map = conditional_types_to_typemaps(
            expr, current_if_type, current_else_type
        )
        if_maps.append(current_if_map)
        else_maps.append(current_else_map)

    def combine_maps(list_maps: list[TypeMap]) -&gt; TypeMap:
        """Combine all typemaps in list_maps into one typemap"""
        result_map = {}
        for d in list_maps:
            if d is not None:
                result_map.update(d)
        return result_map

    if_map = combine_maps(if_maps)
    # type(x) == T is only true when x has the same type as T, meaning
    # that it can be false if x is an instance of a subclass of T. That means
    # we can't do any narrowing in the else case unless T is final, in which
    # case T can't be subclassed
    if is_final:
        else_map = combine_maps(else_maps)
    else:
        else_map = {}
    return if_map, else_map

</t>
<t tx="ekr.20230831011819.339">def find_isinstance_check(self, node: Expression) -&gt; tuple[TypeMap, TypeMap]:
    """Find any isinstance checks (within a chain of ands).  Includes
    implicit and explicit checks for None and calls to callable.
    Also includes TypeGuard functions.

    Return value is a map of variables to their types if the condition
    is true and a map of variables to their types if the condition is false.

    If either of the values in the tuple is None, then that particular
    branch can never occur.

    May return {}, {}.
    Can return None, None in situations involving NoReturn.
    """
    if_map, else_map = self.find_isinstance_check_helper(node)
    new_if_map = self.propagate_up_typemap_info(if_map)
    new_else_map = self.propagate_up_typemap_info(else_map)
    return new_if_map, new_else_map

</t>
<t tx="ekr.20230831011819.34">def _put(self, key: Key, type: Type, index: int = -1) -&gt; None:
    self.frames[index].types[key] = type

</t>
<t tx="ekr.20230831011819.340">def find_isinstance_check_helper(self, node: Expression) -&gt; tuple[TypeMap, TypeMap]:
    if is_true_literal(node):
        return {}, None
    if is_false_literal(node):
        return None, {}

    if isinstance(node, CallExpr) and len(node.args) != 0:
        expr = collapse_walrus(node.args[0])
        if refers_to_fullname(node.callee, "builtins.isinstance"):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                return conditional_types_to_typemaps(
                    expr,
                    *self.conditional_types_with_intersection(
                        self.lookup_type(expr), self.get_isinstance_type(node.args[1]), expr
                    ),
                )
        elif refers_to_fullname(node.callee, "builtins.issubclass"):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                return self.infer_issubclass_maps(node, expr)
        elif refers_to_fullname(node.callee, "builtins.callable"):
            if len(node.args) != 1:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                vartype = self.lookup_type(expr)
                return self.conditional_callable_type_map(expr, vartype)
        elif refers_to_fullname(node.callee, "builtins.hasattr"):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            attr = try_getting_str_literals(node.args[1], self.lookup_type(node.args[1]))
            if literal(expr) == LITERAL_TYPE and attr and len(attr) == 1:
                return self.hasattr_type_maps(expr, self.lookup_type(expr), attr[0])
        elif isinstance(node.callee, RefExpr):
            if node.callee.type_guard is not None:
                # TODO: Follow *args, **kwargs
                if node.arg_kinds[0] != nodes.ARG_POS:
                    # the first argument might be used as a kwarg
                    called_type = get_proper_type(self.lookup_type(node.callee))
                    assert isinstance(called_type, (CallableType, Overloaded))

                    # *assuming* the overloaded function is correct, there's a couple cases:
                    #  1) The first argument has different names, but is pos-only. We don't
                    #     care about this case, the argument must be passed positionally.
                    #  2) The first argument allows keyword reference, therefore must be the
                    #     same between overloads.
                    name = called_type.items[0].arg_names[0]

                    if name in node.arg_names:
                        idx = node.arg_names.index(name)
                        # we want the idx-th variable to be narrowed
                        expr = collapse_walrus(node.args[idx])
                    else:
                        self.fail(message_registry.TYPE_GUARD_POS_ARG_REQUIRED, node)
                        return {}, {}
                if literal(expr) == LITERAL_TYPE:
                    # Note: we wrap the target type, so that we can special case later.
                    # Namely, for isinstance() we use a normal meet, while TypeGuard is
                    # considered "always right" (i.e. even if the types are not overlapping).
                    # Also note that a care must be taken to unwrap this back at read places
                    # where we use this to narrow down declared type.
                    return {expr: TypeGuardedType(node.callee.type_guard)}, {}
    elif isinstance(node, ComparisonExpr):
        # Step 1: Obtain the types of each operand and whether or not we can
        # narrow their types. (For example, we shouldn't try narrowing the
        # types of literal string or enum expressions).

        operands = [collapse_walrus(x) for x in node.operands]
        operand_types = []
        narrowable_operand_index_to_hash = {}
        for i, expr in enumerate(operands):
            if not self.has_type(expr):
                return {}, {}
            expr_type = self.lookup_type(expr)
            operand_types.append(expr_type)

            if (
                literal(expr) == LITERAL_TYPE
                and not is_literal_none(expr)
                and not self.is_literal_enum(expr)
            ):
                h = literal_hash(expr)
                if h is not None:
                    narrowable_operand_index_to_hash[i] = h

        # Step 2: Group operands chained by either the 'is' or '==' operands
        # together. For all other operands, we keep them in groups of size 2.
        # So the expression:
        #
        #   x0 == x1 == x2 &lt; x3 &lt; x4 is x5 is x6 is not x7 is not x8
        #
        # ...is converted into the simplified operator list:
        #
        #  [("==", [0, 1, 2]), ("&lt;", [2, 3]), ("&lt;", [3, 4]),
        #   ("is", [4, 5, 6]), ("is not", [6, 7]), ("is not", [7, 8])]
        #
        # We group identity/equality expressions so we can propagate information
        # we discover about one operand across the entire chain. We don't bother
        # handling 'is not' and '!=' chains in a special way: those are very rare
        # in practice.

        simplified_operator_list = group_comparison_operands(
            node.pairwise(), narrowable_operand_index_to_hash, {"==", "is"}
        )

        # Step 3: Analyze each group and infer more precise type maps for each
        # assignable operand, if possible. We combine these type maps together
        # in the final step.

        partial_type_maps = []
        for operator, expr_indices in simplified_operator_list:
            if operator in {"is", "is not", "==", "!="}:
                # is_valid_target:
                #   Controls which types we're allowed to narrow exprs to. Note that
                #   we cannot use 'is_literal_type_like' in both cases since doing
                #   'x = 10000 + 1; x is 10001' is not always True in all Python
                #   implementations.
                #
                # coerce_only_in_literal_context:
                #   If true, coerce types into literal types only if one or more of
                #   the provided exprs contains an explicit Literal type. This could
                #   technically be set to any arbitrary value, but it seems being liberal
                #   with narrowing when using 'is' and conservative when using '==' seems
                #   to break the least amount of real-world code.
                #
                # should_narrow_by_identity:
                #   Set to 'false' only if the user defines custom __eq__ or __ne__ methods
                #   that could cause identity-based narrowing to produce invalid results.
                if operator in {"is", "is not"}:
                    is_valid_target: Callable[[Type], bool] = is_singleton_type
                    coerce_only_in_literal_context = False
                    should_narrow_by_identity = True
                else:

                    def is_exactly_literal_type(t: Type) -&gt; bool:
                        return isinstance(get_proper_type(t), LiteralType)

                    def has_no_custom_eq_checks(t: Type) -&gt; bool:
                        return not custom_special_method(
                            t, "__eq__", check_all=False
                        ) and not custom_special_method(t, "__ne__", check_all=False)

                    is_valid_target = is_exactly_literal_type
                    coerce_only_in_literal_context = True

                    expr_types = [operand_types[i] for i in expr_indices]
                    should_narrow_by_identity = all(map(has_no_custom_eq_checks, expr_types))

                if_map: TypeMap = {}
                else_map: TypeMap = {}
                if should_narrow_by_identity:
                    if_map, else_map = self.refine_identity_comparison_expression(
                        operands,
                        operand_types,
                        expr_indices,
                        narrowable_operand_index_to_hash.keys(),
                        is_valid_target,
                        coerce_only_in_literal_context,
                    )

                # Strictly speaking, we should also skip this check if the objects in the expr
                # chain have custom __eq__ or __ne__ methods. But we (maybe optimistically)
                # assume nobody would actually create a custom objects that considers itself
                # equal to None.
                if if_map == {} and else_map == {}:
                    if_map, else_map = self.refine_away_none_in_comparison(
                        operands,
                        operand_types,
                        expr_indices,
                        narrowable_operand_index_to_hash.keys(),
                    )

                # If we haven't been able to narrow types yet, we might be dealing with a
                # explicit type(x) == some_type check
                if if_map == {} and else_map == {}:
                    if_map, else_map = self.find_type_equals_check(node, expr_indices)
            elif operator in {"in", "not in"}:
                assert len(expr_indices) == 2
                left_index, right_index = expr_indices
                item_type = operand_types[left_index]
                iterable_type = operand_types[right_index]

                if_map, else_map = {}, {}

                if left_index in narrowable_operand_index_to_hash:
                    # We only try and narrow away 'None' for now
                    if is_overlapping_none(item_type):
                        collection_item_type = get_proper_type(
                            builtin_item_type(iterable_type)
                        )
                        if (
                            collection_item_type is not None
                            and not is_overlapping_none(collection_item_type)
                            and not (
                                isinstance(collection_item_type, Instance)
                                and collection_item_type.type.fullname == "builtins.object"
                            )
                            and is_overlapping_erased_types(item_type, collection_item_type)
                        ):
                            if_map[operands[left_index]] = remove_optional(item_type)

                if right_index in narrowable_operand_index_to_hash:
                    if_type, else_type = self.conditional_types_for_iterable(
                        item_type, iterable_type
                    )
                    expr = operands[right_index]
                    if if_type is None:
                        if_map = None
                    else:
                        if_map[expr] = if_type
                    if else_type is None:
                        else_map = None
                    else:
                        else_map[expr] = else_type

            else:
                if_map = {}
                else_map = {}

            if operator in {"is not", "!=", "not in"}:
                if_map, else_map = else_map, if_map

            partial_type_maps.append((if_map, else_map))

        return reduce_conditional_maps(partial_type_maps)
    elif isinstance(node, AssignmentExpr):
        if_map = {}
        else_map = {}

        if_assignment_map, else_assignment_map = self.find_isinstance_check(node.target)

        if if_assignment_map is not None:
            if_map.update(if_assignment_map)
        if else_assignment_map is not None:
            else_map.update(else_assignment_map)

        if_condition_map, else_condition_map = self.find_isinstance_check(node.value)

        if if_condition_map is not None:
            if_map.update(if_condition_map)
        if else_condition_map is not None:
            else_map.update(else_condition_map)

        return (
            (None if if_assignment_map is None or if_condition_map is None else if_map),
            (None if else_assignment_map is None or else_condition_map is None else else_map),
        )
    elif isinstance(node, OpExpr) and node.op == "and":
        left_if_vars, left_else_vars = self.find_isinstance_check(node.left)
        right_if_vars, right_else_vars = self.find_isinstance_check(node.right)

        # (e1 and e2) is true if both e1 and e2 are true,
        # and false if at least one of e1 and e2 is false.
        return (
            and_conditional_maps(left_if_vars, right_if_vars),
            or_conditional_maps(left_else_vars, right_else_vars),
        )
    elif isinstance(node, OpExpr) and node.op == "or":
        left_if_vars, left_else_vars = self.find_isinstance_check(node.left)
        right_if_vars, right_else_vars = self.find_isinstance_check(node.right)

        # (e1 or e2) is true if at least one of e1 or e2 is true,
        # and false if both e1 and e2 are false.
        return (
            or_conditional_maps(left_if_vars, right_if_vars),
            and_conditional_maps(left_else_vars, right_else_vars),
        )
    elif isinstance(node, UnaryExpr) and node.op == "not":
        left, right = self.find_isinstance_check(node.expr)
        return right, left

    # Restrict the type of the variable to True-ish/False-ish in the if and else branches
    # respectively
    original_vartype = self.lookup_type(node)
    self._check_for_truthy_type(original_vartype, node)
    vartype = try_expanding_sum_type_to_union(original_vartype, "builtins.bool")

    if_type = true_only(vartype)
    else_type = false_only(vartype)
    if_map = {node: if_type} if not isinstance(if_type, UninhabitedType) else None
    else_map = {node: else_type} if not isinstance(else_type, UninhabitedType) else None
    return if_map, else_map

</t>
<t tx="ekr.20230831011819.341">def propagate_up_typemap_info(self, new_types: TypeMap) -&gt; TypeMap:
    """Attempts refining parent expressions of any MemberExpr or IndexExprs in new_types.

    Specifically, this function accepts two mappings of expression to original types:
    the original mapping (existing_types), and a new mapping (new_types) intended to
    update the original.

    This function iterates through new_types and attempts to use the information to try
    refining any parent types that happen to be unions.

    For example, suppose there are two types "A = Tuple[int, int]" and "B = Tuple[str, str]".
    Next, suppose that 'new_types' specifies the expression 'foo[0]' has a refined type
    of 'int' and that 'foo' was previously deduced to be of type Union[A, B].

    Then, this function will observe that since A[0] is an int and B[0] is not, the type of
    'foo' can be further refined from Union[A, B] into just B.

    We perform this kind of "parent narrowing" for member lookup expressions and indexing
    expressions into tuples, namedtuples, and typeddicts. We repeat this narrowing
    recursively if the parent is also a "lookup expression". So for example, if we have
    the expression "foo['bar'].baz[0]", we'd potentially end up refining types for the
    expressions "foo", "foo['bar']", and "foo['bar'].baz".

    We return the newly refined map. This map is guaranteed to be a superset of 'new_types'.
    """
    if new_types is None:
        return None
    output_map = {}
    for expr, expr_type in new_types.items():
        # The original inferred type should always be present in the output map, of course
        output_map[expr] = expr_type

        # Next, try using this information to refine the parent types, if applicable.
        new_mapping = self.refine_parent_types(expr, expr_type)
        for parent_expr, proposed_parent_type in new_mapping.items():
            # We don't try inferring anything if we've already inferred something for
            # the parent expression.
            # TODO: Consider picking the narrower type instead of always discarding this?
            if parent_expr in new_types:
                continue
            output_map[parent_expr] = proposed_parent_type
    return output_map

</t>
<t tx="ekr.20230831011819.342">def refine_parent_types(self, expr: Expression, expr_type: Type) -&gt; Mapping[Expression, Type]:
    """Checks if the given expr is a 'lookup operation' into a union and iteratively refines
    the parent types based on the 'expr_type'.

    For example, if 'expr' is an expression like 'a.b.c.d', we'll potentially return refined
    types for expressions 'a', 'a.b', and 'a.b.c'.

    For more details about what a 'lookup operation' is and how we use the expr_type to refine
    the parent types of lookup_expr, see the docstring in 'propagate_up_typemap_info'.
    """
    output: dict[Expression, Type] = {}

    # Note: parent_expr and parent_type are progressively refined as we crawl up the
    # parent lookup chain.
    while True:
        # First, check if this expression is one that's attempting to
        # "lookup" some key in the parent type. If so, save the parent type
        # and create function that will try replaying the same lookup
        # operation against arbitrary types.
        if isinstance(expr, MemberExpr):
            parent_expr = collapse_walrus(expr.expr)
            parent_type = self.lookup_type_or_none(parent_expr)
            member_name = expr.name

            def replay_lookup(new_parent_type: ProperType) -&gt; Type | None:
                with self.msg.filter_errors() as w:
                    member_type = analyze_member_access(
                        name=member_name,
                        typ=new_parent_type,
                        context=parent_expr,
                        is_lvalue=False,
                        is_super=False,
                        is_operator=False,
                        msg=self.msg,
                        original_type=new_parent_type,
                        chk=self,
                        in_literal_context=False,
                    )
                if w.has_new_errors():
                    return None
                else:
                    return member_type

        elif isinstance(expr, IndexExpr):
            parent_expr = collapse_walrus(expr.base)
            parent_type = self.lookup_type_or_none(parent_expr)

            index_type = self.lookup_type_or_none(expr.index)
            if index_type is None:
                return output

            str_literals = try_getting_str_literals_from_type(index_type)
            if str_literals is not None:
                # Refactoring these two indexing replay functions is surprisingly
                # tricky -- see https://github.com/python/mypy/pull/7917, which
                # was blocked by https://github.com/mypyc/mypyc/issues/586
                def replay_lookup(new_parent_type: ProperType) -&gt; Type | None:
                    if not isinstance(new_parent_type, TypedDictType):
                        return None
                    try:
                        assert str_literals is not None
                        member_types = [new_parent_type.items[key] for key in str_literals]
                    except KeyError:
                        return None
                    return make_simplified_union(member_types)

            else:
                int_literals = try_getting_int_literals_from_type(index_type)
                if int_literals is not None:

                    def replay_lookup(new_parent_type: ProperType) -&gt; Type | None:
                        if not isinstance(new_parent_type, TupleType):
                            return None
                        try:
                            assert int_literals is not None
                            member_types = [new_parent_type.items[key] for key in int_literals]
                        except IndexError:
                            return None
                        return make_simplified_union(member_types)

                else:
                    return output
        else:
            return output

        # If we somehow didn't previously derive the parent type, abort completely
        # with what we have so far: something went wrong at an earlier stage.
        if parent_type is None:
            return output

        # We currently only try refining the parent type if it's a Union.
        # If not, there's no point in trying to refine any further parents
        # since we have no further information we can use to refine the lookup
        # chain, so we end early as an optimization.
        parent_type = get_proper_type(parent_type)
        if not isinstance(parent_type, UnionType):
            return output

        # Take each element in the parent union and replay the original lookup procedure
        # to figure out which parents are compatible.
        new_parent_types = []
        for item in flatten_nested_unions(parent_type.items):
            member_type = replay_lookup(get_proper_type(item))
            if member_type is None:
                # We were unable to obtain the member type. So, we give up on refining this
                # parent type entirely and abort.
                return output

            if is_overlapping_types(member_type, expr_type):
                new_parent_types.append(item)

        # If none of the parent types overlap (if we derived an empty union), something
        # went wrong. We should never hit this case, but deriving the uninhabited type or
        # reporting an error both seem unhelpful. So we abort.
        if not new_parent_types:
            return output

        expr = parent_expr
        expr_type = output[parent_expr] = make_simplified_union(new_parent_types)

</t>
<t tx="ekr.20230831011819.343">def refine_identity_comparison_expression(
    self,
    operands: list[Expression],
    operand_types: list[Type],
    chain_indices: list[int],
    narrowable_operand_indices: AbstractSet[int],
    is_valid_target: Callable[[ProperType], bool],
    coerce_only_in_literal_context: bool,
) -&gt; tuple[TypeMap, TypeMap]:
    """Produce conditional type maps refining expressions by an identity/equality comparison.

    The 'operands' and 'operand_types' lists should be the full list of operands used
    in the overall comparison expression. The 'chain_indices' list is the list of indices
    actually used within this identity comparison chain.

    So if we have the expression:

        a &lt;= b is c is d &lt;= e

    ...then 'operands' and 'operand_types' would be lists of length 5 and 'chain_indices'
    would be the list [1, 2, 3].

    The 'narrowable_operand_indices' parameter is the set of all indices we are allowed
    to refine the types of: that is, all operands that will potentially be a part of
    the output TypeMaps.

    Although this function could theoretically try setting the types of the operands
    in the chains to the meet, doing that causes too many issues in real-world code.
    Instead, we use 'is_valid_target' to identify which of the given chain types
    we could plausibly use as the refined type for the expressions in the chain.

    Similarly, 'coerce_only_in_literal_context' controls whether we should try coercing
    expressions in the chain to a Literal type. Performing this coercion is sometimes
    too aggressive of a narrowing, depending on context.
    """
    should_coerce = True
    if coerce_only_in_literal_context:

        def should_coerce_inner(typ: Type) -&gt; bool:
            typ = get_proper_type(typ)
            return is_literal_type_like(typ) or (
                isinstance(typ, Instance) and typ.type.is_enum
            )

        should_coerce = any(should_coerce_inner(operand_types[i]) for i in chain_indices)

    target: Type | None = None
    possible_target_indices = []
    for i in chain_indices:
        expr_type = operand_types[i]
        if should_coerce:
            expr_type = coerce_to_literal(expr_type)
        if not is_valid_target(get_proper_type(expr_type)):
            continue
        if target and not is_same_type(target, expr_type):
            # We have multiple disjoint target types. So the 'if' branch
            # must be unreachable.
            return None, {}
        target = expr_type
        possible_target_indices.append(i)

    # There's nothing we can currently infer if none of the operands are valid targets,
    # so we end early and infer nothing.
    if target is None:
        return {}, {}

    # If possible, use an unassignable expression as the target.
    # We skip refining the type of the target below, so ideally we'd
    # want to pick an expression we were going to skip anyways.
    singleton_index = -1
    for i in possible_target_indices:
        if i not in narrowable_operand_indices:
            singleton_index = i

    # But if none of the possible singletons are unassignable ones, we give up
    # and arbitrarily pick the last item, mostly because other parts of the
    # type narrowing logic bias towards picking the rightmost item and it'd be
    # nice to stay consistent.
    #
    # That said, it shouldn't matter which index we pick. For example, suppose we
    # have this if statement, where 'x' and 'y' both have singleton types:
    #
    #     if x is y:
    #         reveal_type(x)
    #         reveal_type(y)
    #     else:
    #         reveal_type(x)
    #         reveal_type(y)
    #
    # At this point, 'x' and 'y' *must* have the same singleton type: we would have
    # ended early in the first for-loop in this function if they weren't.
    #
    # So, we should always get the same result in the 'if' case no matter which
    # index we pick. And while we do end up getting different results in the 'else'
    # case depending on the index (e.g. if we pick 'y', then its type stays the same
    # while 'x' is narrowed to '&lt;uninhabited&gt;'), this distinction is also moot: mypy
    # currently will just mark the whole branch as unreachable if either operand is
    # narrowed to &lt;uninhabited&gt;.
    if singleton_index == -1:
        singleton_index = possible_target_indices[-1]

    sum_type_name = None
    target = get_proper_type(target)
    if isinstance(target, LiteralType) and (
        target.is_enum_literal() or isinstance(target.value, bool)
    ):
        sum_type_name = target.fallback.type.fullname

    target_type = [TypeRange(target, is_upper_bound=False)]

    partial_type_maps = []
    for i in chain_indices:
        # If we try refining a type against itself, conditional_type_map
        # will end up assuming that the 'else' branch is unreachable. This is
        # typically not what we want: generally the user will intend for the
        # target type to be some fixed 'sentinel' value and will want to refine
        # the other exprs against this one instead.
        if i == singleton_index:
            continue

        # Naturally, we can't refine operands which are not permitted to be refined.
        if i not in narrowable_operand_indices:
            continue

        expr = operands[i]
        expr_type = coerce_to_literal(operand_types[i])

        if sum_type_name is not None:
            expr_type = try_expanding_sum_type_to_union(expr_type, sum_type_name)

        # We intentionally use 'conditional_types' directly here instead of
        # 'self.conditional_types_with_intersection': we only compute ad-hoc
        # intersections when working with pure instances.
        types = conditional_types(expr_type, target_type)
        partial_type_maps.append(conditional_types_to_typemaps(expr, *types))

    return reduce_conditional_maps(partial_type_maps)

</t>
<t tx="ekr.20230831011819.344">def refine_away_none_in_comparison(
    self,
    operands: list[Expression],
    operand_types: list[Type],
    chain_indices: list[int],
    narrowable_operand_indices: AbstractSet[int],
) -&gt; tuple[TypeMap, TypeMap]:
    """Produces conditional type maps refining away None in an identity/equality chain.

    For more details about what the different arguments mean, see the
    docstring of 'refine_identity_comparison_expression' up above.
    """
    non_optional_types = []
    for i in chain_indices:
        typ = operand_types[i]
        if not is_overlapping_none(typ):
            non_optional_types.append(typ)

    # Make sure we have a mixture of optional and non-optional types.
    if len(non_optional_types) == 0 or len(non_optional_types) == len(chain_indices):
        return {}, {}

    if_map = {}
    for i in narrowable_operand_indices:
        expr_type = operand_types[i]
        if not is_overlapping_none(expr_type):
            continue
        if any(is_overlapping_erased_types(expr_type, t) for t in non_optional_types):
            if_map[operands[i]] = remove_optional(expr_type)

    return if_map, {}

</t>
<t tx="ekr.20230831011819.345">#
# Helpers
#
@overload
def check_subtype(
    self,
    subtype: Type,
    supertype: Type,
    context: Context,
    msg: str,
    subtype_label: str | None = None,
    supertype_label: str | None = None,
    *,
    notes: list[str] | None = None,
    code: ErrorCode | None = None,
    outer_context: Context | None = None,
) -&gt; bool:
    ...

</t>
<t tx="ekr.20230831011819.346">@overload
def check_subtype(
    self,
    subtype: Type,
    supertype: Type,
    context: Context,
    msg: ErrorMessage,
    subtype_label: str | None = None,
    supertype_label: str | None = None,
    *,
    notes: list[str] | None = None,
    outer_context: Context | None = None,
) -&gt; bool:
    ...

</t>
<t tx="ekr.20230831011819.347">def check_subtype(
    self,
    subtype: Type,
    supertype: Type,
    context: Context,
    msg: str | ErrorMessage,
    subtype_label: str | None = None,
    supertype_label: str | None = None,
    *,
    notes: list[str] | None = None,
    code: ErrorCode | None = None,
    outer_context: Context | None = None,
) -&gt; bool:
    """Generate an error if the subtype is not compatible with supertype."""
    if is_subtype(subtype, supertype, options=self.options):
        return True

    if isinstance(msg, str):
        msg = ErrorMessage(msg, code=code)

    if self.msg.prefer_simple_messages():
        self.fail(msg, context)  # Fast path -- skip all fancy logic
        return False

    orig_subtype = subtype
    subtype = get_proper_type(subtype)
    orig_supertype = supertype
    supertype = get_proper_type(supertype)
    if self.msg.try_report_long_tuple_assignment_error(
        subtype, supertype, context, msg, subtype_label, supertype_label
    ):
        return False
    extra_info: list[str] = []
    note_msg = ""
    notes = notes or []
    if subtype_label is not None or supertype_label is not None:
        subtype_str, supertype_str = format_type_distinctly(
            orig_subtype, orig_supertype, options=self.options
        )
        if subtype_label is not None:
            extra_info.append(subtype_label + " " + subtype_str)
        if supertype_label is not None:
            extra_info.append(supertype_label + " " + supertype_str)
        note_msg = make_inferred_type_note(
            outer_context or context, subtype, supertype, supertype_str
        )
        if isinstance(subtype, Instance) and isinstance(supertype, Instance):
            notes = append_invariance_notes(notes, subtype, supertype)
    if extra_info:
        msg = msg.with_additional_msg(" (" + ", ".join(extra_info) + ")")

    self.fail(msg, context)
    for note in notes:
        self.msg.note(note, context, code=msg.code)
    if note_msg:
        self.note(note_msg, context, code=msg.code)
    self.msg.maybe_note_concatenate_pos_args(subtype, supertype, context, code=msg.code)
    if (
        isinstance(supertype, Instance)
        and supertype.type.is_protocol
        and isinstance(subtype, (CallableType, Instance, TupleType, TypedDictType))
    ):
        self.msg.report_protocol_problems(subtype, supertype, context, code=msg.code)
    if isinstance(supertype, CallableType) and isinstance(subtype, Instance):
        call = find_member("__call__", subtype, subtype, is_operator=True)
        if call:
            self.msg.note_call(subtype, call, context, code=msg.code)
    if isinstance(subtype, (CallableType, Overloaded)) and isinstance(supertype, Instance):
        if supertype.type.is_protocol and "__call__" in supertype.type.protocol_members:
            call = find_member("__call__", supertype, subtype, is_operator=True)
            assert call is not None
            if not is_subtype(subtype, call, options=self.options):
                self.msg.note_call(supertype, call, context, code=msg.code)
    self.check_possible_missing_await(subtype, supertype, context)
    return False

</t>
<t tx="ekr.20230831011819.348">def get_precise_awaitable_type(self, typ: Type, local_errors: ErrorWatcher) -&gt; Type | None:
    """If type implements Awaitable[X] with non-Any X, return X.

    In all other cases return None. This method must be called in context
    of local_errors.
    """
    if isinstance(get_proper_type(typ), PartialType):
        # Partial types are special, ignore them here.
        return None
    try:
        aw_type = self.expr_checker.check_awaitable_expr(
            typ, Context(), "", ignore_binder=True
        )
    except KeyError:
        # This is a hack to speed up tests by not including Awaitable in all typing stubs.
        return None
    if local_errors.has_new_errors():
        return None
    if isinstance(get_proper_type(aw_type), (AnyType, UnboundType)):
        return None
    return aw_type

</t>
<t tx="ekr.20230831011819.349">@contextmanager
def checking_await_set(self) -&gt; Iterator[None]:
    self.checking_missing_await = True
    try:
        yield
    finally:
        self.checking_missing_await = False

</t>
<t tx="ekr.20230831011819.35">def _get(self, key: Key, index: int = -1) -&gt; Type | None:
    if index &lt; 0:
        index += len(self.frames)
    for i in range(index, -1, -1):
        if key in self.frames[i].types:
            return self.frames[i].types[key]
    return None

</t>
<t tx="ekr.20230831011819.350">def check_possible_missing_await(
    self, subtype: Type, supertype: Type, context: Context
) -&gt; None:
    """Check if the given type becomes a subtype when awaited."""
    if self.checking_missing_await:
        # Avoid infinite recursion.
        return
    with self.checking_await_set(), self.msg.filter_errors() as local_errors:
        aw_type = self.get_precise_awaitable_type(subtype, local_errors)
        if aw_type is None:
            return
        if not self.check_subtype(
            aw_type, supertype, context, msg=message_registry.INCOMPATIBLE_TYPES
        ):
            return
    self.msg.possible_missing_await(context)

</t>
<t tx="ekr.20230831011819.351">def contains_none(self, t: Type) -&gt; bool:
    t = get_proper_type(t)
    return (
        isinstance(t, NoneType)
        or (isinstance(t, UnionType) and any(self.contains_none(ut) for ut in t.items))
        or (isinstance(t, TupleType) and any(self.contains_none(tt) for tt in t.items))
        or (
            isinstance(t, Instance)
            and bool(t.args)
            and any(self.contains_none(it) for it in t.args)
        )
    )

</t>
<t tx="ekr.20230831011819.352">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with given name and implicit Any type args.

    For example, named_type('builtins.object') produces the 'object' type.
    """
    # Assume that the name refers to a type.
    sym = self.lookup_qualified(name)
    node = sym.node
    if isinstance(node, TypeAlias):
        assert isinstance(node.target, Instance)  # type: ignore[misc]
        node = node.target.type
    assert isinstance(node, TypeInfo)
    any_type = AnyType(TypeOfAny.from_omitted_generics)
    return Instance(node, [any_type] * len(node.defn.type_vars))

</t>
<t tx="ekr.20230831011819.353">def named_generic_type(self, name: str, args: list[Type]) -&gt; Instance:
    """Return an instance with the given name and type arguments.

    Assume that the number of arguments is correct.  Assume that
    the name refers to a compatible generic type.
    """
    info = self.lookup_typeinfo(name)
    args = [remove_instance_last_known_values(arg) for arg in args]
    # TODO: assert len(args) == len(info.defn.type_vars)
    return Instance(info, args)

</t>
<t tx="ekr.20230831011819.354">def lookup_typeinfo(self, fullname: str) -&gt; TypeInfo:
    # Assume that the name refers to a class.
    sym = self.lookup_qualified(fullname)
    node = sym.node
    assert isinstance(node, TypeInfo)
    return node

</t>
<t tx="ekr.20230831011819.355">def type_type(self) -&gt; Instance:
    """Return instance type 'type'."""
    return self.named_type("builtins.type")

</t>
<t tx="ekr.20230831011819.356">def str_type(self) -&gt; Instance:
    """Return instance type 'str'."""
    return self.named_type("builtins.str")

</t>
<t tx="ekr.20230831011819.357">def store_type(self, node: Expression, typ: Type) -&gt; None:
    """Store the type of a node in the type map."""
    self._type_maps[-1][node] = typ

</t>
<t tx="ekr.20230831011819.358">def has_type(self, node: Expression) -&gt; bool:
    return any(node in m for m in reversed(self._type_maps))

</t>
<t tx="ekr.20230831011819.359">def lookup_type_or_none(self, node: Expression) -&gt; Type | None:
    for m in reversed(self._type_maps):
        if node in m:
            return m[node]
    return None

</t>
<t tx="ekr.20230831011819.36">def put(self, expr: Expression, typ: Type) -&gt; None:
    if not isinstance(expr, (IndexExpr, MemberExpr, NameExpr)):
        return
    if not literal(expr):
        return
    key = literal_hash(expr)
    assert key is not None, "Internal error: binder tried to put non-literal"
    if key not in self.declarations:
        self.declarations[key] = get_declaration(expr)
        self._add_dependencies(key)
    self._put(key, typ)

</t>
<t tx="ekr.20230831011819.360">def lookup_type(self, node: Expression) -&gt; Type:
    for m in reversed(self._type_maps):
        t = m.get(node)
        if t is not None:
            return t
    raise KeyError(node)

</t>
<t tx="ekr.20230831011819.361">def store_types(self, d: dict[Expression, Type]) -&gt; None:
    self._type_maps[-1].update(d)

</t>
<t tx="ekr.20230831011819.362">@contextmanager
def local_type_map(self) -&gt; Iterator[dict[Expression, Type]]:
    """Store inferred types into a temporary type map (returned).

    This can be used to perform type checking "experiments" without
    affecting exported types (which are used by mypyc).
    """
    temp_type_map: dict[Expression, Type] = {}
    self._type_maps.append(temp_type_map)
    yield temp_type_map
    self._type_maps.pop()

</t>
<t tx="ekr.20230831011819.363">def in_checked_function(self) -&gt; bool:
    """Should we type-check the current function?

    - Yes if --check-untyped-defs is set.
    - Yes outside functions.
    - Yes in annotated functions.
    - No otherwise.
    """
    return (
        self.options.check_untyped_defs or not self.dynamic_funcs or not self.dynamic_funcs[-1]
    )

</t>
<t tx="ekr.20230831011819.364">def lookup(self, name: str) -&gt; SymbolTableNode:
    """Look up a definition from the symbol table with the given name."""
    if name in self.globals:
        return self.globals[name]
    else:
        b = self.globals.get("__builtins__", None)
        if b:
            assert isinstance(b.node, MypyFile)
            table = b.node.names
            if name in table:
                return table[name]
        raise KeyError(f"Failed lookup: {name}")

</t>
<t tx="ekr.20230831011819.365">def lookup_qualified(self, name: str) -&gt; SymbolTableNode:
    if "." not in name:
        return self.lookup(name)
    else:
        parts = name.split(".")
        n = self.modules[parts[0]]
        for i in range(1, len(parts) - 1):
            sym = n.names.get(parts[i])
            assert sym is not None, "Internal error: attempted lookup of unknown name"
            assert isinstance(sym.node, MypyFile)
            n = sym.node
        last = parts[-1]
        if last in n.names:
            return n.names[last]
        elif len(parts) == 2 and parts[0] in ("builtins", "typing"):
            fullname = ".".join(parts)
            if fullname in SUGGESTED_TEST_FIXTURES:
                suggestion = ", e.g. add '[{} fixtures/{}]' to your test".format(
                    parts[0], SUGGESTED_TEST_FIXTURES[fullname]
                )
            else:
                suggestion = ""
            raise KeyError(
                "Could not find builtin symbol '{}' (If you are running a "
                "test case, use a fixture that "
                "defines this symbol{})".format(last, suggestion)
            )
        else:
            msg = "Failed qualified lookup: '{}' (fullname = '{}')."
            raise KeyError(msg.format(last, name))

</t>
<t tx="ekr.20230831011819.366">@contextmanager
def enter_partial_types(
    self, *, is_function: bool = False, is_class: bool = False
) -&gt; Iterator[None]:
    """Enter a new scope for collecting partial types.

    Also report errors for (some) variables which still have partial
    types, i.e. we couldn't infer a complete type.
    """
    is_local = (self.partial_types and self.partial_types[-1].is_local) or is_function
    self.partial_types.append(PartialTypeScope({}, is_function, is_local))
    yield

    # Don't complain about not being able to infer partials if it is
    # at the toplevel (with allow_untyped_globals) or if it is in an
    # untyped function being checked with check_untyped_defs.
    permissive = (self.options.allow_untyped_globals and not is_local) or (
        self.options.check_untyped_defs and self.dynamic_funcs and self.dynamic_funcs[-1]
    )

    partial_types, _, _ = self.partial_types.pop()
    if not self.current_node_deferred:
        for var, context in partial_types.items():
            # If we require local partial types, there are a few exceptions where
            # we fall back to inferring just "None" as the type from a None initializer:
            #
            # 1. If all happens within a single function this is acceptable, since only
            #    the topmost function is a separate target in fine-grained incremental mode.
            #    We primarily want to avoid "splitting" partial types across targets.
            #
            # 2. A None initializer in the class body if the attribute is defined in a base
            #    class is fine, since the attribute is already defined and it's currently okay
            #    to vary the type of an attribute covariantly. The None type will still be
            #    checked for compatibility with base classes elsewhere. Without this exception
            #    mypy could require an annotation for an attribute that already has been
            #    declared in a base class, which would be bad.
            allow_none = (
                not self.options.local_partial_types
                or is_function
                or (is_class and self.is_defined_in_base_class(var))
            )
            if (
                allow_none
                and isinstance(var.type, PartialType)
                and var.type.type is None
                and not permissive
            ):
                var.type = NoneType()
            else:
                if var not in self.partial_reported and not permissive:
                    self.msg.need_annotation_for_var(var, context, self.options.python_version)
                    self.partial_reported.add(var)
                if var.type:
                    fixed = fixup_partial_type(var.type)
                    var.invalid_partial_type = fixed != var.type
                    var.type = fixed

</t>
<t tx="ekr.20230831011819.367">def handle_partial_var_type(
    self, typ: PartialType, is_lvalue: bool, node: Var, context: Context
) -&gt; Type:
    """Handle a reference to a partial type through a var.

    (Used by checkexpr and checkmember.)
    """
    in_scope, is_local, partial_types = self.find_partial_types_in_all_scopes(node)
    if typ.type is None and in_scope:
        # 'None' partial type. It has a well-defined type. In an lvalue context
        # we want to preserve the knowledge of it being a partial type.
        if not is_lvalue:
            return NoneType()
        else:
            return typ
    else:
        if partial_types is not None and not self.current_node_deferred:
            if in_scope:
                context = partial_types[node]
                if is_local or not self.options.allow_untyped_globals:
                    self.msg.need_annotation_for_var(
                        node, context, self.options.python_version
                    )
                    self.partial_reported.add(node)
            else:
                # Defer the node -- we might get a better type in the outer scope
                self.handle_cannot_determine_type(node.name, context)
        return fixup_partial_type(typ)

</t>
<t tx="ekr.20230831011819.368">def is_defined_in_base_class(self, var: Var) -&gt; bool:
    if not var.info:
        return False
    return var.info.fallback_to_any or any(
        base.get(var.name) is not None for base in var.info.mro[1:]
    )

</t>
<t tx="ekr.20230831011819.369">def find_partial_types(self, var: Var) -&gt; dict[Var, Context] | None:
    """Look for an active partial type scope containing variable.

    A scope is active if assignments in the current context can refine a partial
    type originally defined in the scope. This is affected by the local_partial_types
    configuration option.
    """
    in_scope, _, partial_types = self.find_partial_types_in_all_scopes(var)
    if in_scope:
        return partial_types
    return None

</t>
<t tx="ekr.20230831011819.37">def unreachable(self) -&gt; None:
    self.frames[-1].unreachable = True

</t>
<t tx="ekr.20230831011819.370">def find_partial_types_in_all_scopes(
    self, var: Var
) -&gt; tuple[bool, bool, dict[Var, Context] | None]:
    """Look for partial type scope containing variable.

    Return tuple (is the scope active, is the scope a local scope, scope).
    """
    for scope in reversed(self.partial_types):
        if var in scope.map:
            # All scopes within the outermost function are active. Scopes out of
            # the outermost function are inactive to allow local reasoning (important
            # for fine-grained incremental mode).
            disallow_other_scopes = self.options.local_partial_types

            if isinstance(var.type, PartialType) and var.type.type is not None and var.info:
                # This is an ugly hack to make partial generic self attributes behave
                # as if --local-partial-types is always on (because it used to be like this).
                disallow_other_scopes = True

            scope_active = (
                not disallow_other_scopes or scope.is_local == self.partial_types[-1].is_local
            )
            return scope_active, scope.is_local, scope.map
    return False, False, None

</t>
<t tx="ekr.20230831011819.371">def temp_node(self, t: Type, context: Context | None = None) -&gt; TempNode:
    """Create a temporary node with the given, fixed type."""
    return TempNode(t, context=context)

</t>
<t tx="ekr.20230831011819.372">def fail(
    self, msg: str | ErrorMessage, context: Context, *, code: ErrorCode | None = None
) -&gt; None:
    """Produce an error message."""
    if isinstance(msg, ErrorMessage):
        self.msg.fail(msg.value, context, code=msg.code)
        return
    self.msg.fail(msg, context, code=code)

</t>
<t tx="ekr.20230831011819.373">def note(
    self,
    msg: str | ErrorMessage,
    context: Context,
    offset: int = 0,
    *,
    code: ErrorCode | None = None,
) -&gt; None:
    """Produce a note."""
    if isinstance(msg, ErrorMessage):
        self.msg.note(msg.value, context, code=msg.code)
        return
    self.msg.note(msg, context, offset=offset, code=code)

</t>
<t tx="ekr.20230831011819.374">def iterable_item_type(
    self, it: Instance | CallableType | TypeType | Overloaded, context: Context
) -&gt; Type:
    if isinstance(it, Instance):
        iterable = map_instance_to_supertype(it, self.lookup_typeinfo("typing.Iterable"))
        item_type = iterable.args[0]
        if not isinstance(get_proper_type(item_type), AnyType):
            # This relies on 'map_instance_to_supertype' returning 'Iterable[Any]'
            # in case there is no explicit base class.
            return item_type
    # Try also structural typing.
    return self.analyze_iterable_item_type_without_expression(it, context)[1]

</t>
<t tx="ekr.20230831011819.375">def function_type(self, func: FuncBase) -&gt; FunctionLike:
    return function_type(func, self.named_type("builtins.function"))

</t>
<t tx="ekr.20230831011819.376">def push_type_map(self, type_map: TypeMap) -&gt; None:
    if type_map is None:
        self.binder.unreachable()
    else:
        for expr, type in type_map.items():
            self.binder.put(expr, type)

</t>
<t tx="ekr.20230831011819.377">def infer_issubclass_maps(self, node: CallExpr, expr: Expression) -&gt; tuple[TypeMap, TypeMap]:
    """Infer type restrictions for an expression in issubclass call."""
    vartype = self.lookup_type(expr)
    type = self.get_isinstance_type(node.args[1])
    if isinstance(vartype, TypeVarType):
        vartype = vartype.upper_bound
    vartype = get_proper_type(vartype)
    if isinstance(vartype, UnionType):
        union_list = []
        for t in get_proper_types(vartype.items):
            if isinstance(t, TypeType):
                union_list.append(t.item)
            else:
                # This is an error that should be reported earlier
                # if we reach here, we refuse to do any type inference.
                return {}, {}
        vartype = UnionType(union_list)
    elif isinstance(vartype, TypeType):
        vartype = vartype.item
    elif isinstance(vartype, Instance) and vartype.type.is_metaclass():
        vartype = self.named_type("builtins.object")
    else:
        # Any other object whose type we don't know precisely
        # for example, Any or a custom metaclass.
        return {}, {}  # unknown type
    yes_type, no_type = self.conditional_types_with_intersection(vartype, type, expr)
    yes_map, no_map = conditional_types_to_typemaps(expr, yes_type, no_type)
    yes_map, no_map = map(convert_to_typetype, (yes_map, no_map))
    return yes_map, no_map

</t>
<t tx="ekr.20230831011819.378">@overload
def conditional_types_with_intersection(
    self,
    expr_type: Type,
    type_ranges: list[TypeRange] | None,
    ctx: Context,
    default: None = None,
) -&gt; tuple[Type | None, Type | None]:
    ...

</t>
<t tx="ekr.20230831011819.379">@overload
def conditional_types_with_intersection(
    self, expr_type: Type, type_ranges: list[TypeRange] | None, ctx: Context, default: Type
) -&gt; tuple[Type, Type]:
    ...

</t>
<t tx="ekr.20230831011819.38">def suppress_unreachable_warnings(self) -&gt; None:
    self.frames[-1].suppress_unreachable_warnings = True

</t>
<t tx="ekr.20230831011819.380">def conditional_types_with_intersection(
    self,
    expr_type: Type,
    type_ranges: list[TypeRange] | None,
    ctx: Context,
    default: Type | None = None,
) -&gt; tuple[Type | None, Type | None]:
    initial_types = conditional_types(expr_type, type_ranges, default)
    # For some reason, doing "yes_map, no_map = conditional_types_to_typemaps(...)"
    # doesn't work: mypyc will decide that 'yes_map' is of type None if we try.
    yes_type: Type | None = initial_types[0]
    no_type: Type | None = initial_types[1]

    if not isinstance(get_proper_type(yes_type), UninhabitedType) or type_ranges is None:
        return yes_type, no_type

    # If conditional_types was unable to successfully narrow the expr_type
    # using the type_ranges and concluded if-branch is unreachable, we try
    # computing it again using a different algorithm that tries to generate
    # an ad-hoc intersection between the expr_type and the type_ranges.
    proper_type = get_proper_type(expr_type)
    if isinstance(proper_type, UnionType):
        possible_expr_types = get_proper_types(proper_type.relevant_items())
    else:
        possible_expr_types = [proper_type]

    possible_target_types = []
    for tr in type_ranges:
        item = get_proper_type(tr.item)
        if not isinstance(item, Instance) or tr.is_upper_bound:
            return yes_type, no_type
        possible_target_types.append(item)

    out = []
    errors: list[tuple[str, str]] = []
    for v in possible_expr_types:
        if not isinstance(v, Instance):
            return yes_type, no_type
        for t in possible_target_types:
            intersection = self.intersect_instances((v, t), errors)
            if intersection is None:
                continue
            out.append(intersection)
    if not out:
        # Only report errors if no element in the union worked.
        if self.should_report_unreachable_issues():
            for types, reason in errors:
                self.msg.impossible_intersection(types, reason, ctx)
        return UninhabitedType(), expr_type
    new_yes_type = make_simplified_union(out)
    return new_yes_type, expr_type

</t>
<t tx="ekr.20230831011819.381">def is_writable_attribute(self, node: Node) -&gt; bool:
    """Check if an attribute is writable"""
    if isinstance(node, Var):
        if node.is_property and not node.is_settable_property:
            return False
        return True
    elif isinstance(node, OverloadedFuncDef) and node.is_property:
        first_item = node.items[0]
        assert isinstance(first_item, Decorator)
        return first_item.var.is_settable_property
    return False

</t>
<t tx="ekr.20230831011819.382">def get_isinstance_type(self, expr: Expression) -&gt; list[TypeRange] | None:
    if isinstance(expr, OpExpr) and expr.op == "|":
        left = self.get_isinstance_type(expr.left)
        right = self.get_isinstance_type(expr.right)
        if left is None or right is None:
            return None
        return left + right
    all_types = get_proper_types(flatten_types(self.lookup_type(expr)))
    types: list[TypeRange] = []
    for typ in all_types:
        if isinstance(typ, FunctionLike) and typ.is_type_obj():
            # Type variables may be present -- erase them, which is the best
            # we can do (outside disallowing them here).
            erased_type = erase_typevars(typ.items[0].ret_type)
            types.append(TypeRange(erased_type, is_upper_bound=False))
        elif isinstance(typ, TypeType):
            # Type[A] means "any type that is a subtype of A" rather than "precisely type A"
            # we indicate this by setting is_upper_bound flag
            types.append(TypeRange(typ.item, is_upper_bound=True))
        elif isinstance(typ, Instance) and typ.type.fullname == "builtins.type":
            object_type = Instance(typ.type.mro[-1], [])
            types.append(TypeRange(object_type, is_upper_bound=True))
        elif isinstance(typ, AnyType):
            types.append(TypeRange(typ, is_upper_bound=False))
        else:  # we didn't see an actual type, but rather a variable with unknown value
            return None
    if not types:
        # this can happen if someone has empty tuple as 2nd argument to isinstance
        # strictly speaking, we should return UninhabitedType but for simplicity we will simply
        # refuse to do any type inference for now
        return None
    return types

</t>
<t tx="ekr.20230831011819.383">def is_literal_enum(self, n: Expression) -&gt; bool:
    """Returns true if this expression (with the given type context) is an Enum literal.

    For example, if we had an enum:

        class Foo(Enum):
            A = 1
            B = 2

    ...and if the expression 'Foo' referred to that enum within the current type context,
    then the expression 'Foo.A' would be a literal enum. However, if we did 'a = Foo.A',
    then the variable 'a' would *not* be a literal enum.

    We occasionally special-case expressions like 'Foo.A' and treat them as a single primitive
    unit for the same reasons we sometimes treat 'True', 'False', or 'None' as a single
    primitive unit.
    """
    if not isinstance(n, MemberExpr) or not isinstance(n.expr, NameExpr):
        return False

    parent_type = self.lookup_type_or_none(n.expr)
    member_type = self.lookup_type_or_none(n)
    if member_type is None or parent_type is None:
        return False

    parent_type = get_proper_type(parent_type)
    member_type = get_proper_type(coerce_to_literal(member_type))
    if not isinstance(parent_type, FunctionLike) or not isinstance(member_type, LiteralType):
        return False

    if not parent_type.is_type_obj():
        return False

    return (
        member_type.is_enum_literal()
        and member_type.fallback.type == parent_type.type_object()
    )

</t>
<t tx="ekr.20230831011819.384">def add_any_attribute_to_type(self, typ: Type, name: str) -&gt; Type:
    """Inject an extra attribute with Any type using fallbacks."""
    orig_typ = typ
    typ = get_proper_type(typ)
    any_type = AnyType(TypeOfAny.unannotated)
    if isinstance(typ, Instance):
        result = typ.copy_with_extra_attr(name, any_type)
        # For instances, we erase the possible module name, so that restrictions
        # become anonymous types.ModuleType instances, allowing hasattr() to
        # have effect on modules.
        assert result.extra_attrs is not None
        result.extra_attrs.mod_name = None
        return result
    if isinstance(typ, TupleType):
        fallback = typ.partial_fallback.copy_with_extra_attr(name, any_type)
        return typ.copy_modified(fallback=fallback)
    if isinstance(typ, CallableType):
        fallback = typ.fallback.copy_with_extra_attr(name, any_type)
        return typ.copy_modified(fallback=fallback)
    if isinstance(typ, TypeType) and isinstance(typ.item, Instance):
        return TypeType.make_normalized(self.add_any_attribute_to_type(typ.item, name))
    if isinstance(typ, TypeVarType):
        return typ.copy_modified(
            upper_bound=self.add_any_attribute_to_type(typ.upper_bound, name),
            values=[self.add_any_attribute_to_type(v, name) for v in typ.values],
        )
    if isinstance(typ, UnionType):
        with_attr, without_attr = self.partition_union_by_attr(typ, name)
        return make_simplified_union(
            with_attr + [self.add_any_attribute_to_type(typ, name) for typ in without_attr]
        )
    return orig_typ

</t>
<t tx="ekr.20230831011819.385">def hasattr_type_maps(
    self, expr: Expression, source_type: Type, name: str
) -&gt; tuple[TypeMap, TypeMap]:
    """Simple support for hasattr() checks.

    Essentially the logic is following:
        * In the if branch, keep types that already has a valid attribute as is,
          for other inject an attribute with `Any` type.
        * In the else branch, remove types that already have a valid attribute,
          while keeping the rest.
    """
    if self.has_valid_attribute(source_type, name):
        return {expr: source_type}, {}

    source_type = get_proper_type(source_type)
    if isinstance(source_type, UnionType):
        _, without_attr = self.partition_union_by_attr(source_type, name)
        yes_map = {expr: self.add_any_attribute_to_type(source_type, name)}
        return yes_map, {expr: make_simplified_union(without_attr)}

    type_with_attr = self.add_any_attribute_to_type(source_type, name)
    if type_with_attr != source_type:
        return {expr: type_with_attr}, {}
    return {}, {}

</t>
<t tx="ekr.20230831011819.386">def partition_union_by_attr(
    self, source_type: UnionType, name: str
) -&gt; tuple[list[Type], list[Type]]:
    with_attr = []
    without_attr = []
    for item in source_type.items:
        if self.has_valid_attribute(item, name):
            with_attr.append(item)
        else:
            without_attr.append(item)
    return with_attr, without_attr

</t>
<t tx="ekr.20230831011819.387">def has_valid_attribute(self, typ: Type, name: str) -&gt; bool:
    p_typ = get_proper_type(typ)
    if isinstance(p_typ, AnyType):
        return False
    if isinstance(p_typ, Instance) and p_typ.extra_attrs and p_typ.extra_attrs.mod_name:
        # Presence of module_symbol_table means this check will skip ModuleType.__getattr__
        module_symbol_table = p_typ.type.names
    else:
        module_symbol_table = None
    with self.msg.filter_errors() as watcher:
        analyze_member_access(
            name,
            typ,
            TempNode(AnyType(TypeOfAny.special_form)),
            False,
            False,
            False,
            self.msg,
            original_type=typ,
            chk=self,
            # This is not a real attribute lookup so don't mess with deferring nodes.
            no_deferral=True,
            module_symbol_table=module_symbol_table,
        )
    return not watcher.has_new_errors()

</t>
<t tx="ekr.20230831011819.388">def get_expression_type(self, node: Expression, type_context: Type | None = None) -&gt; Type:
    return self.expr_checker.accept(node, type_context=type_context)


</t>
<t tx="ekr.20230831011819.389">class CollectArgTypeVarTypes(TypeTraverserVisitor):
    """Collects the non-nested argument types in a set."""

    @others
</t>
<t tx="ekr.20230831011819.39">def get(self, expr: Expression) -&gt; Type | None:
    key = literal_hash(expr)
    assert key is not None, "Internal error: binder tried to get non-literal"
    return self._get(key)

</t>
<t tx="ekr.20230831011819.390">def __init__(self) -&gt; None:
    self.arg_types: set[TypeVarType] = set()

</t>
<t tx="ekr.20230831011819.391">def visit_type_var(self, t: TypeVarType) -&gt; None:
    self.arg_types.add(t)


</t>
<t tx="ekr.20230831011819.392">@overload
def conditional_types(
    current_type: Type, proposed_type_ranges: list[TypeRange] | None, default: None = None
) -&gt; tuple[Type | None, Type | None]:
    ...


</t>
<t tx="ekr.20230831011819.393">@overload
def conditional_types(
    current_type: Type, proposed_type_ranges: list[TypeRange] | None, default: Type
) -&gt; tuple[Type, Type]:
    ...


</t>
<t tx="ekr.20230831011819.394">def conditional_types(
    current_type: Type, proposed_type_ranges: list[TypeRange] | None, default: Type | None = None
) -&gt; tuple[Type | None, Type | None]:
    """Takes in the current type and a proposed type of an expression.

    Returns a 2-tuple: The first element is the proposed type, if the expression
    can be the proposed type. The second element is the type it would hold
    if it was not the proposed type, if any. UninhabitedType means unreachable.
    None means no new information can be inferred. If default is set it is returned
    instead."""
    if proposed_type_ranges:
        if len(proposed_type_ranges) == 1:
            target = proposed_type_ranges[0].item
            target = get_proper_type(target)
            if isinstance(target, LiteralType) and (
                target.is_enum_literal() or isinstance(target.value, bool)
            ):
                enum_name = target.fallback.type.fullname
                current_type = try_expanding_sum_type_to_union(current_type, enum_name)
        proposed_items = [type_range.item for type_range in proposed_type_ranges]
        proposed_type = make_simplified_union(proposed_items)
        if isinstance(proposed_type, AnyType):
            # We don't really know much about the proposed type, so we shouldn't
            # attempt to narrow anything. Instead, we broaden the expr to Any to
            # avoid false positives
            return proposed_type, default
        elif not any(
            type_range.is_upper_bound for type_range in proposed_type_ranges
        ) and is_proper_subtype(current_type, proposed_type, ignore_promotions=True):
            # Expression is always of one of the types in proposed_type_ranges
            return default, UninhabitedType()
        elif not is_overlapping_types(
            current_type, proposed_type, prohibit_none_typevar_overlap=True, ignore_promotions=True
        ):
            # Expression is never of any type in proposed_type_ranges
            return UninhabitedType(), default
        else:
            # we can only restrict when the type is precise, not bounded
            proposed_precise_type = UnionType.make_union(
                [
                    type_range.item
                    for type_range in proposed_type_ranges
                    if not type_range.is_upper_bound
                ]
            )
            remaining_type = restrict_subtype_away(current_type, proposed_precise_type)
            return proposed_type, remaining_type
    else:
        # An isinstance check, but we don't understand the type
        return current_type, default


</t>
<t tx="ekr.20230831011819.395">def conditional_types_to_typemaps(
    expr: Expression, yes_type: Type | None, no_type: Type | None
) -&gt; tuple[TypeMap, TypeMap]:
    expr = collapse_walrus(expr)
    maps: list[TypeMap] = []
    for typ in (yes_type, no_type):
        proper_type = get_proper_type(typ)
        if isinstance(proper_type, UninhabitedType):
            maps.append(None)
        elif proper_type is None:
            maps.append({})
        else:
            assert typ is not None
            maps.append({expr: typ})

    return cast(Tuple[TypeMap, TypeMap], tuple(maps))


</t>
<t tx="ekr.20230831011819.396">def gen_unique_name(base: str, table: SymbolTable) -&gt; str:
    """Generate a name that does not appear in table by appending numbers to base."""
    if base not in table:
        return base
    i = 1
    while base + str(i) in table:
        i += 1
    return base + str(i)


</t>
<t tx="ekr.20230831011819.397">def is_true_literal(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'True' literal/keyword."""
    return refers_to_fullname(n, "builtins.True") or isinstance(n, IntExpr) and n.value != 0


</t>
<t tx="ekr.20230831011819.398">def is_false_literal(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'False' literal/keyword."""
    return refers_to_fullname(n, "builtins.False") or isinstance(n, IntExpr) and n.value == 0


</t>
<t tx="ekr.20230831011819.399">def is_literal_none(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'None' literal/keyword."""
    return isinstance(n, NameExpr) and n.fullname == "builtins.None"


</t>
<t tx="ekr.20230831011819.4">@path mypy
"""Mypy type checker command line tool."""
&lt;&lt; __main__.py: preamble &gt;&gt;
@others


if __name__ == "__main__":
    console_entry()
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.40">def is_unreachable(self) -&gt; bool:
    # TODO: Copy the value of unreachable into new frames to avoid
    # this traversal on every statement?
    return any(f.unreachable for f in self.frames)

</t>
<t tx="ekr.20230831011819.400">def is_literal_not_implemented(n: Expression) -&gt; bool:
    return isinstance(n, NameExpr) and n.fullname == "builtins.NotImplemented"


</t>
<t tx="ekr.20230831011819.401">def _is_empty_generator_function(func: FuncItem) -&gt; bool:
    """
    Checks whether a function's body is 'return; yield' (the yield being added only
    to promote the function into a generator function).
    """
    body = func.body.body
    return (
        len(body) == 2
        and isinstance(ret_stmt := body[0], ReturnStmt)
        and (ret_stmt.expr is None or is_literal_none(ret_stmt.expr))
        and isinstance(expr_stmt := body[1], ExpressionStmt)
        and isinstance(yield_expr := expr_stmt.expr, YieldExpr)
        and (yield_expr.expr is None or is_literal_none(yield_expr.expr))
    )


</t>
<t tx="ekr.20230831011819.402">def builtin_item_type(tp: Type) -&gt; Type | None:
    """Get the item type of a builtin container.

    If 'tp' is not one of the built containers (these includes NamedTuple and TypedDict)
    or if the container is not parameterized (like List or List[Any])
    return None. This function is used to narrow optional types in situations like this:

        x: Optional[int]
        if x in (1, 2, 3):
            x + 42  # OK

    Note: this is only OK for built-in containers, where we know the behavior
    of __contains__.
    """
    tp = get_proper_type(tp)

    if isinstance(tp, Instance):
        if tp.type.fullname in [
            "builtins.list",
            "builtins.tuple",
            "builtins.dict",
            "builtins.set",
            "builtins.frozenset",
            "_collections_abc.dict_keys",
            "typing.KeysView",
        ]:
            if not tp.args:
                # TODO: fix tuple in lib-stub/builtins.pyi (it should be generic).
                return None
            if not isinstance(get_proper_type(tp.args[0]), AnyType):
                return tp.args[0]
    elif isinstance(tp, TupleType) and all(
        not isinstance(it, AnyType) for it in get_proper_types(tp.items)
    ):
        return make_simplified_union(tp.items)  # this type is not externally visible
    elif isinstance(tp, TypedDictType):
        # TypedDict always has non-optional string keys. Find the key type from the Mapping
        # base class.
        for base in tp.fallback.type.mro:
            if base.fullname == "typing.Mapping":
                return map_instance_to_supertype(tp.fallback, base).args[0]
        assert False, "No Mapping base class found for TypedDict fallback"
    return None


</t>
<t tx="ekr.20230831011819.403">def and_conditional_maps(m1: TypeMap, m2: TypeMap) -&gt; TypeMap:
    """Calculate what information we can learn from the truth of (e1 and e2)
    in terms of the information that we can learn from the truth of e1 and
    the truth of e2.
    """

    if m1 is None or m2 is None:
        # One of the conditions can never be true.
        return None
    # Both conditions can be true; combine the information. Anything
    # we learn from either conditions's truth is valid. If the same
    # expression's type is refined by both conditions, we somewhat
    # arbitrarily give precedence to m2. (In the future, we could use
    # an intersection type.)
    result = m2.copy()
    m2_keys = {literal_hash(n2) for n2 in m2}
    for n1 in m1:
        if literal_hash(n1) not in m2_keys:
            result[n1] = m1[n1]
    return result


</t>
<t tx="ekr.20230831011819.404">def or_conditional_maps(m1: TypeMap, m2: TypeMap) -&gt; TypeMap:
    """Calculate what information we can learn from the truth of (e1 or e2)
    in terms of the information that we can learn from the truth of e1 and
    the truth of e2.
    """

    if m1 is None:
        return m2
    if m2 is None:
        return m1
    # Both conditions can be true. Combine information about
    # expressions whose type is refined by both conditions. (We do not
    # learn anything about expressions whose type is refined by only
    # one condition.)
    result: dict[Expression, Type] = {}
    for n1 in m1:
        for n2 in m2:
            if literal_hash(n1) == literal_hash(n2):
                result[n1] = make_simplified_union([m1[n1], m2[n2]])
    return result


</t>
<t tx="ekr.20230831011819.405">def reduce_conditional_maps(type_maps: list[tuple[TypeMap, TypeMap]]) -&gt; tuple[TypeMap, TypeMap]:
    """Reduces a list containing pairs of if/else TypeMaps into a single pair.

    We "and" together all of the if TypeMaps and "or" together the else TypeMaps. So
    for example, if we had the input:

        [
            ({x: TypeIfX, shared: TypeIfShared1}, {x: TypeElseX, shared: TypeElseShared1}),
            ({y: TypeIfY, shared: TypeIfShared2}, {y: TypeElseY, shared: TypeElseShared2}),
        ]

    ...we'd return the output:

        (
            {x: TypeIfX,   y: TypeIfY,   shared: PseudoIntersection[TypeIfShared1, TypeIfShared2]},
            {shared: Union[TypeElseShared1, TypeElseShared2]},
        )

    ...where "PseudoIntersection[X, Y] == Y" because mypy actually doesn't understand intersections
    yet, so we settle for just arbitrarily picking the right expr's type.

    We only retain the shared expression in the 'else' case because we don't actually know
    whether x was refined or y was refined -- only just that one of the two was refined.
    """
    if len(type_maps) == 0:
        return {}, {}
    elif len(type_maps) == 1:
        return type_maps[0]
    else:
        final_if_map, final_else_map = type_maps[0]
        for if_map, else_map in type_maps[1:]:
            final_if_map = and_conditional_maps(final_if_map, if_map)
            final_else_map = or_conditional_maps(final_else_map, else_map)

        return final_if_map, final_else_map


</t>
<t tx="ekr.20230831011819.406">def convert_to_typetype(type_map: TypeMap) -&gt; TypeMap:
    converted_type_map: dict[Expression, Type] = {}
    if type_map is None:
        return None
    for expr, typ in type_map.items():
        t = typ
        if isinstance(t, TypeVarType):
            t = t.upper_bound
        # TODO: should we only allow unions of instances as per PEP 484?
        if not isinstance(get_proper_type(t), (UnionType, Instance)):
            # unknown type; error was likely reported earlier
            return {}
        converted_type_map[expr] = TypeType.make_normalized(typ)
    return converted_type_map


</t>
<t tx="ekr.20230831011819.407">def flatten(t: Expression) -&gt; list[Expression]:
    """Flatten a nested sequence of tuples/lists into one list of nodes."""
    if isinstance(t, (TupleExpr, ListExpr)):
        return [b for a in t.items for b in flatten(a)]
    elif isinstance(t, StarExpr):
        return flatten(t.expr)
    else:
        return [t]


</t>
<t tx="ekr.20230831011819.408">def flatten_types(t: Type) -&gt; list[Type]:
    """Flatten a nested sequence of tuples into one list of nodes."""
    t = get_proper_type(t)
    if isinstance(t, TupleType):
        return [b for a in t.items for b in flatten_types(a)]
    elif is_named_instance(t, "builtins.tuple"):
        return [t.args[0]]
    else:
        return [t]


</t>
<t tx="ekr.20230831011819.409">def expand_func(defn: FuncItem, map: dict[TypeVarId, Type]) -&gt; FuncItem:
    visitor = TypeTransformVisitor(map)
    ret = visitor.node(defn)
    assert isinstance(ret, FuncItem)
    return ret


</t>
<t tx="ekr.20230831011819.41">def is_unreachable_warning_suppressed(self) -&gt; bool:
    return any(f.suppress_unreachable_warnings for f in self.frames)

</t>
<t tx="ekr.20230831011819.410">class TypeTransformVisitor(TransformVisitor):
    @others
</t>
<t tx="ekr.20230831011819.411">def __init__(self, map: dict[TypeVarId, Type]) -&gt; None:
    super().__init__()
    self.map = map

</t>
<t tx="ekr.20230831011819.412">def type(self, type: Type) -&gt; Type:
    return expand_type(type, self.map)


</t>
<t tx="ekr.20230831011819.413">def are_argument_counts_overlapping(t: CallableType, s: CallableType) -&gt; bool:
    """Can a single call match both t and s, based just on positional argument counts?"""
    min_args = max(t.min_args, s.min_args)
    max_args = min(t.max_possible_positional_args(), s.max_possible_positional_args())
    return min_args &lt;= max_args


</t>
<t tx="ekr.20230831011819.414">def is_unsafe_overlapping_overload_signatures(
    signature: CallableType, other: CallableType, class_type_vars: list[TypeVarLikeType]
) -&gt; bool:
    """Check if two overloaded signatures are unsafely overlapping or partially overlapping.

    We consider two functions 's' and 't' to be unsafely overlapping if both
    of the following are true:

    1.  s's parameters are all more precise or partially overlapping with t's
    2.  s's return type is NOT a subtype of t's.

    Assumes that 'signature' appears earlier in the list of overload
    alternatives then 'other' and that their argument counts are overlapping.
    """
    # Try detaching callables from the containing class so that all TypeVars
    # are treated as being free.
    #
    # This lets us identify cases where the two signatures use completely
    # incompatible types -- e.g. see the testOverloadingInferUnionReturnWithMixedTypevars
    # test case.
    signature = detach_callable(signature, class_type_vars)
    other = detach_callable(other, class_type_vars)

    # Note: We repeat this check twice in both directions due to a slight
    # asymmetry in 'is_callable_compatible'. When checking for partial overlaps,
    # we attempt to unify 'signature' and 'other' both against each other.
    #
    # If 'signature' cannot be unified with 'other', we end early. However,
    # if 'other' cannot be modified with 'signature', the function continues
    # using the older version of 'other'.
    #
    # This discrepancy is unfortunately difficult to get rid of, so we repeat the
    # checks twice in both directions for now.
    #
    # Note that we ignore possible overlap between type variables and None. This
    # is technically unsafe, but unsafety is tiny and this prevents some common
    # use cases like:
    #     @overload
    #     def foo(x: None) -&gt; None: ..
    #     @overload
    #     def foo(x: T) -&gt; Foo[T]: ...
    return is_callable_compatible(
        signature,
        other,
        is_compat=is_overlapping_types_no_promote_no_uninhabited_no_none,
        is_compat_return=lambda l, r: not is_subtype_no_promote(l, r),
        ignore_return=False,
        check_args_covariantly=True,
        allow_partial_overlap=True,
        no_unify_none=True,
    ) or is_callable_compatible(
        other,
        signature,
        is_compat=is_overlapping_types_no_promote_no_uninhabited_no_none,
        is_compat_return=lambda l, r: not is_subtype_no_promote(r, l),
        ignore_return=False,
        check_args_covariantly=False,
        allow_partial_overlap=True,
        no_unify_none=True,
    )


</t>
<t tx="ekr.20230831011819.415">def detach_callable(typ: CallableType, class_type_vars: list[TypeVarLikeType]) -&gt; CallableType:
    """Ensures that the callable's type variables are 'detached' and independent of the context.

    A callable normally keeps track of the type variables it uses within its 'variables' field.
    However, if the callable is from a method and that method is using a class type variable,
    the callable will not keep track of that type variable since it belongs to the class.

    This function will traverse the callable and find all used type vars and add them to the
    variables field if it isn't already present.

    The caller can then unify on all type variables whether the callable is originally from
    the class or not."""
    if not class_type_vars:
        # Fast path, nothing to update.
        return typ
    seen_type_vars = set()
    for t in typ.arg_types + [typ.ret_type]:
        seen_type_vars |= set(get_type_vars(t))
    return typ.copy_modified(
        variables=list(typ.variables) + [tv for tv in class_type_vars if tv in seen_type_vars]
    )


</t>
<t tx="ekr.20230831011819.416">def overload_can_never_match(signature: CallableType, other: CallableType) -&gt; bool:
    """Check if the 'other' method can never be matched due to 'signature'.

    This can happen if signature's parameters are all strictly broader then
    other's parameters.

    Assumes that both signatures have overlapping argument counts.
    """
    # The extra erasure is needed to prevent spurious errors
    # in situations where an `Any` overload is used as a fallback
    # for an overload with type variables. The spurious error appears
    # because the type variables turn into `Any` during unification in
    # the below subtype check and (surprisingly?) `is_proper_subtype(Any, Any)`
    # returns `True`.
    # TODO: find a cleaner solution instead of this ad-hoc erasure.
    exp_signature = expand_type(
        signature, {tvar.id: erase_def_to_union_or_bound(tvar) for tvar in signature.variables}
    )
    return is_callable_compatible(
        exp_signature, other, is_compat=is_more_precise, ignore_return=True
    )


</t>
<t tx="ekr.20230831011819.417">def is_more_general_arg_prefix(t: FunctionLike, s: FunctionLike) -&gt; bool:
    """Does t have wider arguments than s?"""
    # TODO should an overload with additional items be allowed to be more
    #      general than one with fewer items (or just one item)?
    if isinstance(t, CallableType):
        if isinstance(s, CallableType):
            return is_callable_compatible(t, s, is_compat=is_proper_subtype, ignore_return=True)
    elif isinstance(t, FunctionLike):
        if isinstance(s, FunctionLike):
            if len(t.items) == len(s.items):
                return all(
                    is_same_arg_prefix(items, itemt) for items, itemt in zip(t.items, s.items)
                )
    return False


</t>
<t tx="ekr.20230831011819.418">def is_same_arg_prefix(t: CallableType, s: CallableType) -&gt; bool:
    return is_callable_compatible(
        t,
        s,
        is_compat=is_same_type,
        ignore_return=True,
        check_args_covariantly=True,
        ignore_pos_arg_names=True,
    )


</t>
<t tx="ekr.20230831011819.419">def infer_operator_assignment_method(typ: Type, operator: str) -&gt; tuple[bool, str]:
    """Determine if operator assignment on given value type is in-place, and the method name.

    For example, if operator is '+', return (True, '__iadd__') or (False, '__add__')
    depending on which method is supported by the type.
    """
    typ = get_proper_type(typ)
    method = operators.op_methods[operator]
    if isinstance(typ, Instance):
        if operator in operators.ops_with_inplace_method:
            inplace_method = "__i" + method[2:]
            if typ.type.has_readable_member(inplace_method):
                return True, inplace_method
    return False, method


</t>
<t tx="ekr.20230831011819.42">def cleanse(self, expr: Expression) -&gt; None:
    """Remove all references to a Node from the binder."""
    key = literal_hash(expr)
    assert key is not None, "Internal error: binder tried cleanse non-literal"
    self._cleanse_key(key)

</t>
<t tx="ekr.20230831011819.420">def is_valid_inferred_type(typ: Type, is_lvalue_final: bool = False) -&gt; bool:
    """Is an inferred type valid and needs no further refinement?

    Examples of invalid types include the None type (when we are not assigning
    None to a final lvalue) or List[&lt;uninhabited&gt;].

    When not doing strict Optional checking, all types containing None are
    invalid.  When doing strict Optional checking, only None and types that are
    incompletely defined (i.e. contain UninhabitedType) are invalid.
    """
    proper_type = get_proper_type(typ)
    if isinstance(proper_type, NoneType):
        # If the lvalue is final, we may immediately infer NoneType when the
        # initializer is None.
        #
        # If not, we want to defer making this decision. The final inferred
        # type could either be NoneType or an Optional type, depending on
        # the context. This resolution happens in leave_partial_types when
        # we pop a partial types scope.
        return is_lvalue_final
    elif isinstance(proper_type, UninhabitedType):
        return False
    return not typ.accept(InvalidInferredTypes())


</t>
<t tx="ekr.20230831011819.421">class InvalidInferredTypes(BoolTypeQuery):
    """Find type components that are not valid for an inferred type.

    These include &lt;Erased&gt; type, and any &lt;nothing&gt; types resulting from failed
    (ambiguous) type inference.
    """

    @others
</t>
<t tx="ekr.20230831011819.422">def __init__(self) -&gt; None:
    super().__init__(ANY_STRATEGY)

</t>
<t tx="ekr.20230831011819.423">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return t.ambiguous

</t>
<t tx="ekr.20230831011819.424">def visit_erased_type(self, t: ErasedType) -&gt; bool:
    # This can happen inside a lambda.
    return True

</t>
<t tx="ekr.20230831011819.425">def visit_type_var(self, t: TypeVarType) -&gt; bool:
    # This is needed to prevent leaking into partial types during
    # multi-step type inference.
    return t.id.is_meta_var()


</t>
<t tx="ekr.20230831011819.426">class SetNothingToAny(TypeTranslator):
    """Replace all ambiguous &lt;nothing&gt; types with Any (to avoid spurious extra errors)."""

    @others
</t>
<t tx="ekr.20230831011819.427">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    if t.ambiguous:
        return AnyType(TypeOfAny.from_error)
    return t

</t>
<t tx="ekr.20230831011819.428">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Target of the alias cannot be an ambiguous &lt;nothing&gt;, so we just
    # replace the arguments.
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20230831011819.429">def is_node_static(node: Node | None) -&gt; bool | None:
    """Find out if a node describes a static function method."""

    if isinstance(node, FuncDef):
        return node.is_static

    if isinstance(node, Var):
        return node.is_staticmethod

    return None


</t>
<t tx="ekr.20230831011819.43">def _cleanse_key(self, key: Key) -&gt; None:
    """Remove all references to a key from the binder."""
    for frame in self.frames:
        if key in frame.types:
            del frame.types[key]

</t>
<t tx="ekr.20230831011819.430">class CheckerScope:
    @others
</t>
<t tx="ekr.20230831011819.431"># We keep two stacks combined, to maintain the relative order
stack: list[TypeInfo | FuncItem | MypyFile]

def __init__(self, module: MypyFile) -&gt; None:
    self.stack = [module]

</t>
<t tx="ekr.20230831011819.432">def top_function(self) -&gt; FuncItem | None:
    for e in reversed(self.stack):
        if isinstance(e, FuncItem):
            return e
    return None

</t>
<t tx="ekr.20230831011819.433">def top_non_lambda_function(self) -&gt; FuncItem | None:
    for e in reversed(self.stack):
        if isinstance(e, FuncItem) and not isinstance(e, LambdaExpr):
            return e
    return None

</t>
<t tx="ekr.20230831011819.434">def active_class(self) -&gt; TypeInfo | None:
    if isinstance(self.stack[-1], TypeInfo):
        return self.stack[-1]
    return None

</t>
<t tx="ekr.20230831011819.435">def enclosing_class(self) -&gt; TypeInfo | None:
    """Is there a class *directly* enclosing this function?"""
    top = self.top_function()
    assert top, "This method must be called from inside a function"
    index = self.stack.index(top)
    assert index, "CheckerScope stack must always start with a module"
    enclosing = self.stack[index - 1]
    if isinstance(enclosing, TypeInfo):
        return enclosing
    return None

</t>
<t tx="ekr.20230831011819.436">def active_self_type(self) -&gt; Instance | TupleType | None:
    """An instance or tuple type representing the current class.

    This returns None unless we are in class body or in a method.
    In particular, inside a function nested in method this returns None.
    """
    info = self.active_class()
    if not info and self.top_function():
        info = self.enclosing_class()
    if info:
        return fill_typevars(info)
    return None

</t>
<t tx="ekr.20230831011819.437">@contextmanager
def push_function(self, item: FuncItem) -&gt; Iterator[None]:
    self.stack.append(item)
    yield
    self.stack.pop()

</t>
<t tx="ekr.20230831011819.438">@contextmanager
def push_class(self, info: TypeInfo) -&gt; Iterator[None]:
    self.stack.append(info)
    yield
    self.stack.pop()


</t>
<t tx="ekr.20230831011819.439">TKey = TypeVar("TKey")
TValue = TypeVar("TValue")


class DisjointDict(Generic[TKey, TValue]):
    """An variation of the union-find algorithm/data structure where instead of keeping
    track of just disjoint sets, we keep track of disjoint dicts -- keep track of multiple
    Set[Key] -&gt; Set[Value] mappings, where each mapping's keys are guaranteed to be disjoint.

    This data structure is currently used exclusively by 'group_comparison_operands' below
    to merge chains of '==' and 'is' comparisons when two or more chains use the same expression
    in best-case O(n), where n is the number of operands.

    Specifically, the `add_mapping()` function and `items()` functions will take on average
    O(k + v) and O(n) respectively, where k and v are the number of keys and values we're adding
    for a given chain. Note that k &lt;= n and v &lt;= n.

    We hit these average/best-case scenarios for most user code: e.g. when the user has just
    a single chain like 'a == b == c == d == ...' or multiple disjoint chains like
    'a==b &lt; c==d &lt; e==f &lt; ...'. (Note that a naive iterative merging would be O(n^2) for
    the latter case).

    In comparison, this data structure will make 'group_comparison_operands' have a worst-case
    runtime of O(n*log(n)): 'add_mapping()' and 'items()' are worst-case O(k*log(n) + v) and
    O(k*log(n)) respectively. This happens only in the rare case where the user keeps repeatedly
    making disjoint mappings before merging them in a way that persistently dodges the path
    compression optimization in '_lookup_root_id', which would end up constructing a single
    tree of height log_2(n). This makes root lookups no longer amoritized constant time when we
    finally call 'items()'.
    """

    @others
</t>
<t tx="ekr.20230831011819.44">def update_from_options(self, frames: list[Frame]) -&gt; bool:
    """Update the frame to reflect that each key will be updated
    as in one of the frames.  Return whether any item changes.

    If a key is declared as AnyType, only update it if all the
    options are the same.
    """

    frames = [f for f in frames if not f.unreachable]
    changed = False
    keys = {key for f in frames for key in f.types}

    for key in keys:
        current_value = self._get(key)
        resulting_values = [f.types.get(key, current_value) for f in frames]
        if any(x is None for x in resulting_values):
            # We didn't know anything about key before
            # (current_value must be None), and we still don't
            # know anything about key in at least one possible frame.
            continue

        type = resulting_values[0]
        assert type is not None
        declaration_type = get_proper_type(self.declarations.get(key))
        if isinstance(declaration_type, AnyType):
            # At this point resulting values can't contain None, see continue above
            if not all(is_same_type(type, cast(Type, t)) for t in resulting_values[1:]):
                type = AnyType(TypeOfAny.from_another_any, source_any=declaration_type)
        else:
            for other in resulting_values[1:]:
                assert other is not None
                type = join_simple(self.declarations[key], type, other)
        if current_value is None or not is_same_type(type, current_value):
            self._put(key, type)
            changed = True

    self.frames[-1].unreachable = not frames

    return changed

</t>
<t tx="ekr.20230831011819.440">def __init__(self) -&gt; None:
    # Each key maps to a unique ID
    self._key_to_id: dict[TKey, int] = {}

    # Each id points to the parent id, forming a forest of upwards-pointing trees. If the
    # current id already is the root, it points to itself. We gradually flatten these trees
    # as we perform root lookups: eventually all nodes point directly to its root.
    self._id_to_parent_id: dict[int, int] = {}

    # Each root id in turn maps to the set of values.
    self._root_id_to_values: dict[int, set[TValue]] = {}

</t>
<t tx="ekr.20230831011819.441">def add_mapping(self, keys: set[TKey], values: set[TValue]) -&gt; None:
    """Adds a 'Set[TKey] -&gt; Set[TValue]' mapping. If there already exists a mapping
    containing one or more of the given keys, we merge the input mapping with the old one.

    Note that the given set of keys must be non-empty -- otherwise, nothing happens.
    """
    if not keys:
        return

    subtree_roots = [self._lookup_or_make_root_id(key) for key in keys]
    new_root = subtree_roots[0]

    root_values = self._root_id_to_values[new_root]
    root_values.update(values)
    for subtree_root in subtree_roots[1:]:
        if subtree_root == new_root or subtree_root not in self._root_id_to_values:
            continue
        self._id_to_parent_id[subtree_root] = new_root
        root_values.update(self._root_id_to_values.pop(subtree_root))

</t>
<t tx="ekr.20230831011819.442">def items(self) -&gt; list[tuple[set[TKey], set[TValue]]]:
    """Returns all disjoint mappings in key-value pairs."""
    root_id_to_keys: dict[int, set[TKey]] = {}
    for key in self._key_to_id:
        root_id = self._lookup_root_id(key)
        if root_id not in root_id_to_keys:
            root_id_to_keys[root_id] = set()
        root_id_to_keys[root_id].add(key)

    output = []
    for root_id, keys in root_id_to_keys.items():
        output.append((keys, self._root_id_to_values[root_id]))

    return output

</t>
<t tx="ekr.20230831011819.443">def _lookup_or_make_root_id(self, key: TKey) -&gt; int:
    if key in self._key_to_id:
        return self._lookup_root_id(key)
    else:
        new_id = len(self._key_to_id)
        self._key_to_id[key] = new_id
        self._id_to_parent_id[new_id] = new_id
        self._root_id_to_values[new_id] = set()
        return new_id

</t>
<t tx="ekr.20230831011819.444">def _lookup_root_id(self, key: TKey) -&gt; int:
    i = self._key_to_id[key]
    while i != self._id_to_parent_id[i]:
        # Optimization: make keys directly point to their grandparents to speed up
        # future traversals. This prevents degenerate trees of height n from forming.
        new_parent = self._id_to_parent_id[self._id_to_parent_id[i]]
        self._id_to_parent_id[i] = new_parent
        i = new_parent
    return i


</t>
<t tx="ekr.20230831011819.445">def group_comparison_operands(
    pairwise_comparisons: Iterable[tuple[str, Expression, Expression]],
    operand_to_literal_hash: Mapping[int, Key],
    operators_to_group: set[str],
) -&gt; list[tuple[str, list[int]]]:
    """Group a series of comparison operands together chained by any operand
    in the 'operators_to_group' set. All other pairwise operands are kept in
    groups of size 2.

    For example, suppose we have the input comparison expression:

        x0 == x1 == x2 &lt; x3 &lt; x4 is x5 is x6 is not x7 is not x8

    If we get these expressions in a pairwise way (e.g. by calling ComparisionExpr's
    'pairwise()' method), we get the following as input:

        [('==', x0, x1), ('==', x1, x2), ('&lt;', x2, x3), ('&lt;', x3, x4),
         ('is', x4, x5), ('is', x5, x6), ('is not', x6, x7), ('is not', x7, x8)]

    If `operators_to_group` is the set {'==', 'is'}, this function will produce
    the following "simplified operator list":

       [("==", [0, 1, 2]), ("&lt;", [2, 3]), ("&lt;", [3, 4]),
        ("is", [4, 5, 6]), ("is not", [6, 7]), ("is not", [7, 8])]

    Note that (a) we yield *indices* to the operands rather then the operand
    expressions themselves and that (b) operands used in a consecutive chain
    of '==' or 'is' are grouped together.

    If two of these chains happen to contain operands with the same underlying
    literal hash (e.g. are assignable and correspond to the same expression),
    we combine those chains together. For example, if we had:

        same == x &lt; y == same

    ...and if 'operand_to_literal_hash' contained the same values for the indices
    0 and 3, we'd produce the following output:

        [("==", [0, 1, 2, 3]), ("&lt;", [1, 2])]

    But if the 'operand_to_literal_hash' did *not* contain an entry, we'd instead
    default to returning:

        [("==", [0, 1]), ("&lt;", [1, 2]), ("==", [2, 3])]

    This function is currently only used to assist with type-narrowing refinements
    and is extracted out to a helper function so we can unit test it.
    """
    groups: dict[str, DisjointDict[Key, int]] = {op: DisjointDict() for op in operators_to_group}

    simplified_operator_list: list[tuple[str, list[int]]] = []
    last_operator: str | None = None
    current_indices: set[int] = set()
    current_hashes: set[Key] = set()
    for i, (operator, left_expr, right_expr) in enumerate(pairwise_comparisons):
        if last_operator is None:
            last_operator = operator

        if current_indices and (operator != last_operator or operator not in operators_to_group):
            # If some of the operands in the chain are assignable, defer adding it: we might
            # end up needing to merge it with other chains that appear later.
            if not current_hashes:
                simplified_operator_list.append((last_operator, sorted(current_indices)))
            else:
                groups[last_operator].add_mapping(current_hashes, current_indices)
            last_operator = operator
            current_indices = set()
            current_hashes = set()

        # Note: 'i' corresponds to the left operand index, so 'i + 1' is the
        # right operand.
        current_indices.add(i)
        current_indices.add(i + 1)

        # We only ever want to combine operands/combine chains for these operators
        if operator in operators_to_group:
            left_hash = operand_to_literal_hash.get(i)
            if left_hash is not None:
                current_hashes.add(left_hash)
            right_hash = operand_to_literal_hash.get(i + 1)
            if right_hash is not None:
                current_hashes.add(right_hash)

    if last_operator is not None:
        if not current_hashes:
            simplified_operator_list.append((last_operator, sorted(current_indices)))
        else:
            groups[last_operator].add_mapping(current_hashes, current_indices)

    # Now that we know which chains happen to contain the same underlying expressions
    # and can be merged together, add in this info back to the output.
    for operator, disjoint_dict in groups.items():
        for keys, indices in disjoint_dict.items():
            simplified_operator_list.append((operator, sorted(indices)))

    # For stability, reorder list by the first operand index to appear
    simplified_operator_list.sort(key=lambda item: item[1][0])
    return simplified_operator_list


</t>
<t tx="ekr.20230831011819.446">def is_typed_callable(c: Type | None) -&gt; bool:
    c = get_proper_type(c)
    if not c or not isinstance(c, CallableType):
        return False
    return not all(
        isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated
        for t in get_proper_types(c.arg_types + [c.ret_type])
    )


</t>
<t tx="ekr.20230831011819.447">def is_untyped_decorator(typ: Type | None) -&gt; bool:
    typ = get_proper_type(typ)
    if not typ:
        return True
    elif isinstance(typ, CallableType):
        return not is_typed_callable(typ)
    elif isinstance(typ, Instance):
        method = typ.type.get_method("__call__")
        if method:
            if isinstance(method, Decorator):
                return is_untyped_decorator(method.func.type) or is_untyped_decorator(
                    method.var.type
                )

            if isinstance(method.type, Overloaded):
                return any(is_untyped_decorator(item) for item in method.type.items)
            else:
                return not is_typed_callable(method.type)
        else:
            return False
    elif isinstance(typ, Overloaded):
        return any(is_untyped_decorator(item) for item in typ.items)
    return True


</t>
<t tx="ekr.20230831011819.448">def is_static(func: FuncBase | Decorator) -&gt; bool:
    if isinstance(func, Decorator):
        return is_static(func.func)
    elif isinstance(func, FuncBase):
        return func.is_static
    assert False, f"Unexpected func type: {type(func)}"


</t>
<t tx="ekr.20230831011819.449">def is_property(defn: SymbolNode) -&gt; bool:
    if isinstance(defn, Decorator):
        return defn.func.is_property
    if isinstance(defn, OverloadedFuncDef):
        if defn.items and isinstance(defn.items[0], Decorator):
            return defn.items[0].func.is_property
    return False


</t>
<t tx="ekr.20230831011819.45">def pop_frame(self, can_skip: bool, fall_through: int) -&gt; Frame:
    """Pop a frame and return it.

    See frame_context() for documentation of fall_through.
    """

    if fall_through &gt; 0:
        self.allow_jump(-fall_through)

    result = self.frames.pop()
    options = self.options_on_return.pop()

    if can_skip:
        options.insert(0, self.frames[-1])

    self.last_pop_changed = self.update_from_options(options)

    return result

</t>
<t tx="ekr.20230831011819.450">def get_property_type(t: ProperType) -&gt; ProperType:
    if isinstance(t, CallableType):
        return get_proper_type(t.ret_type)
    if isinstance(t, Overloaded):
        return get_proper_type(t.items[0].ret_type)
    return t


</t>
<t tx="ekr.20230831011819.451">def is_subtype_no_promote(left: Type, right: Type) -&gt; bool:
    return is_subtype(left, right, ignore_promotions=True)


</t>
<t tx="ekr.20230831011819.452">def is_overlapping_types_no_promote_no_uninhabited_no_none(left: Type, right: Type) -&gt; bool:
    # For the purpose of unsafe overload checks we consider list[&lt;nothing&gt;] and list[int]
    # non-overlapping. This is consistent with how we treat list[int] and list[str] as
    # non-overlapping, despite [] belongs to both. Also this will prevent false positives
    # for failed type inference during unification.
    return is_overlapping_types(
        left,
        right,
        ignore_promotions=True,
        ignore_uninhabited=True,
        prohibit_none_typevar_overlap=True,
    )


</t>
<t tx="ekr.20230831011819.453">def is_private(node_name: str) -&gt; bool:
    """Check if node is private to class definition."""
    return node_name.startswith("__") and not node_name.endswith("__")


</t>
<t tx="ekr.20230831011819.454">def is_string_literal(typ: Type) -&gt; bool:
    strs = try_getting_str_literals_from_type(typ)
    return strs is not None and len(strs) == 1


</t>
<t tx="ekr.20230831011819.455">def has_bool_item(typ: ProperType) -&gt; bool:
    """Return True if type is 'bool' or a union with a 'bool' item."""
    if is_named_instance(typ, "builtins.bool"):
        return True
    if isinstance(typ, UnionType):
        return any(is_named_instance(item, "builtins.bool") for item in typ.items)
    return False


</t>
<t tx="ekr.20230831011819.456">def collapse_walrus(e: Expression) -&gt; Expression:
    """If an expression is an AssignmentExpr, pull out the assignment target.

    We don't make any attempt to pull out all the targets in code like `x := (y := z)`.
    We could support narrowing those if that sort of code turns out to be common.
    """
    if isinstance(e, AssignmentExpr):
        return e.target
    return e


</t>
<t tx="ekr.20230831011819.457">def find_last_var_assignment_line(n: Node, v: Var) -&gt; int:
    """Find the highest line number of a potential assignment to variable within node.

    This supports local and global variables.

    Return -1 if no assignment was found.
    """
    visitor = VarAssignVisitor(v)
    n.accept(visitor)
    return visitor.last_line


</t>
<t tx="ekr.20230831011819.458">class VarAssignVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011819.459">def __init__(self, v: Var) -&gt; None:
    self.last_line = -1
    self.lvalue = False
    self.var_node = v

</t>
<t tx="ekr.20230831011819.46">@contextmanager
def accumulate_type_assignments(self) -&gt; Iterator[Assigns]:
    """Push a new map to collect assigned types in multiassign from union.

    If this map is not None, actual binding is deferred until all items in
    the union are processed (a union of collected items is later bound
    manually by the caller).
    """
    old_assignments = None
    if self.type_assignments is not None:
        old_assignments = self.type_assignments
    self.type_assignments = defaultdict(list)
    yield self.type_assignments
    self.type_assignments = old_assignments

</t>
<t tx="ekr.20230831011819.460">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    self.lvalue = True
    for lv in s.lvalues:
        lv.accept(self)
    self.lvalue = False

</t>
<t tx="ekr.20230831011819.461">def visit_name_expr(self, e: NameExpr) -&gt; None:
    if self.lvalue and e.node is self.var_node:
        self.last_line = max(self.last_line, e.line)

</t>
<t tx="ekr.20230831011819.462">def visit_member_expr(self, e: MemberExpr) -&gt; None:
    old_lvalue = self.lvalue
    self.lvalue = False
    super().visit_member_expr(e)
    self.lvalue = old_lvalue

</t>
<t tx="ekr.20230831011819.463">def visit_index_expr(self, e: IndexExpr) -&gt; None:
    old_lvalue = self.lvalue
    self.lvalue = False
    super().visit_index_expr(e)
    self.lvalue = old_lvalue

</t>
<t tx="ekr.20230831011819.464">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    self.lvalue = True
    for lv in s.target:
        if lv is not None:
            lv.accept(self)
    self.lvalue = False
    s.body.accept(self)

</t>
<t tx="ekr.20230831011819.465">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    self.lvalue = True
    s.index.accept(self)
    self.lvalue = False
    s.body.accept(self)
    if s.else_body:
        s.else_body.accept(self)

</t>
<t tx="ekr.20230831011819.466">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; None:
    self.lvalue = True
    e.target.accept(self)
    self.lvalue = False
    e.value.accept(self)

</t>
<t tx="ekr.20230831011819.467">def visit_as_pattern(self, p: AsPattern) -&gt; None:
    if p.pattern is not None:
        p.pattern.accept(self)
    if p.name is not None:
        self.lvalue = True
        p.name.accept(self)
        self.lvalue = False

</t>
<t tx="ekr.20230831011819.468">def visit_starred_pattern(self, p: StarredPattern) -&gt; None:
    if p.capture is not None:
        self.lvalue = True
        p.capture.accept(self)
        self.lvalue = False
</t>
<t tx="ekr.20230831011819.469">@path mypy
"""Expression type checker. This file is conceptually part of TypeChecker."""
&lt;&lt; checkexpr.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.47">def assign_type(
    self, expr: Expression, type: Type, declared_type: Type | None, restrict_any: bool = False
) -&gt; None:
    # We should erase last known value in binder, because if we are using it,
    # it means that the target is not final, and therefore can't hold a literal.
    type = remove_instance_last_known_values(type)

    if self.type_assignments is not None:
        # We are in a multiassign from union, defer the actual binding,
        # just collect the types.
        self.type_assignments[expr].append((type, declared_type))
        return
    if not isinstance(expr, (IndexExpr, MemberExpr, NameExpr)):
        return None
    if not literal(expr):
        return
    self.invalidate_dependencies(expr)

    if declared_type is None:
        # Not sure why this happens.  It seems to mainly happen in
        # member initialization.
        return
    if not is_subtype(type, declared_type):
        # Pretty sure this is only happens when there's a type error.

        # Ideally this function wouldn't be called if the
        # expression has a type error, though -- do other kinds of
        # errors cause this function to get called at invalid
        # times?
        return

    p_declared = get_proper_type(declared_type)
    p_type = get_proper_type(type)
    enclosing_type = get_proper_type(self.most_recent_enclosing_type(expr, type))
    if isinstance(enclosing_type, AnyType) and not restrict_any:
        # If x is Any and y is int, after x = y we do not infer that x is int.
        # This could be changed.
        # Instead, since we narrowed type from Any in a recent frame (probably an
        # isinstance check), but now it is reassigned, we broaden back
        # to Any (which is the most recent enclosing type)
        self.put(expr, enclosing_type)
    # As a special case, when assigning Any to a variable with a
    # declared Optional type that has been narrowed to None,
    # replace all the Nones in the declared Union type with Any.
    # This overrides the normal behavior of ignoring Any assignments to variables
    # in order to prevent false positives.
    # (See discussion in #3526)
    elif (
        isinstance(p_type, AnyType)
        and isinstance(p_declared, UnionType)
        and any(isinstance(get_proper_type(item), NoneType) for item in p_declared.items)
        and isinstance(
            get_proper_type(self.most_recent_enclosing_type(expr, NoneType())), NoneType
        )
    ):
        # Replace any Nones in the union type with Any
        new_items = [
            type if isinstance(get_proper_type(item), NoneType) else item
            for item in p_declared.items
        ]
        self.put(expr, UnionType(new_items))
    elif isinstance(p_type, AnyType) and not (
        isinstance(p_declared, UnionType)
        and any(isinstance(get_proper_type(item), AnyType) for item in p_declared.items)
    ):
        # Assigning an Any value doesn't affect the type to avoid false negatives, unless
        # there is an Any item in a declared union type.
        self.put(expr, declared_type)
    else:
        self.put(expr, type)

    for i in self.try_frames:
        # XXX This should probably not copy the entire frame, but
        # just copy this variable into a single stored frame.
        self.allow_jump(i)

</t>
<t tx="ekr.20230831011819.470">
from __future__ import annotations

import itertools
import time
from collections import defaultdict
from contextlib import contextmanager
from typing import Callable, ClassVar, Final, Iterable, Iterator, List, Optional, Sequence, cast
from typing_extensions import TypeAlias as _TypeAlias, overload

import mypy.checker
import mypy.errorcodes as codes
from mypy import applytype, erasetype, join, message_registry, nodes, operators, types
from mypy.argmap import ArgTypeExpander, map_actuals_to_formals, map_formals_to_actuals
from mypy.checkmember import analyze_member_access, freeze_all_type_vars, type_object_type
from mypy.checkstrformat import StringFormatterChecker
from mypy.erasetype import erase_type, remove_instance_last_known_values, replace_meta_vars
from mypy.errors import ErrorWatcher, report_internal_error
from mypy.expandtype import (
    expand_type,
    expand_type_by_instance,
    freshen_all_functions_type_vars,
    freshen_function_type_vars,
)
from mypy.infer import ArgumentInferContext, infer_function_type_arguments, infer_type_arguments
from mypy.literals import literal
from mypy.maptype import map_instance_to_supertype
from mypy.meet import is_overlapping_types, narrow_declared_type
from mypy.message_registry import ErrorMessage
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    IMPLICITLY_ABSTRACT,
    LITERAL_TYPE,
    REVEAL_TYPE,
    ArgKind,
    AssertTypeExpr,
    AssignmentExpr,
    AwaitExpr,
    BytesExpr,
    CallExpr,
    CastExpr,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    Context,
    Decorator,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    FloatExpr,
    FuncDef,
    GeneratorExpr,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    OpExpr,
    OverloadedFuncDef,
    ParamSpecExpr,
    PlaceholderNode,
    PromoteExpr,
    RefExpr,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    StrExpr,
    SuperExpr,
    SymbolNode,
    TempNode,
    TupleExpr,
    TypeAlias,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeInfo,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    YieldExpr,
    YieldFromExpr,
)
from mypy.plugin import (
    FunctionContext,
    FunctionSigContext,
    MethodContext,
    MethodSigContext,
    Plugin,
)
from mypy.semanal_enum import ENUM_BASES
from mypy.state import state
from mypy.subtypes import (
    find_member,
    is_equivalent,
    is_same_type,
    is_subtype,
    non_method_protocol_members,
)
from mypy.traverser import has_await_expression
from mypy.type_visitor import TypeTranslator
from mypy.typeanal import (
    check_for_explicit_any,
    has_any_from_unimported_type,
    instantiate_type_alias,
    make_optional_type,
    set_any_tvars,
)
from mypy.typeops import (
    callable_type,
    custom_special_method,
    erase_to_union_or_bound,
    false_only,
    fixup_partial_type,
    function_type,
    get_all_type_vars,
    get_type_vars,
    is_literal_type_like,
    make_simplified_union,
    simple_literal_type,
    true_only,
    try_expanding_sum_type_to_union,
    try_getting_str_literals,
    tuple_fallback,
)
from mypy.types import (
    LITERAL_TYPE_NAMES,
    TUPLE_LIKE_INSTANCE_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    ExtraAttrs,
    FunctionLike,
    Instance,
    LiteralType,
    LiteralValue,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecFlavor,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UninhabitedType,
    UnionType,
    UnpackType,
    find_unpack_in_list,
    flatten_nested_unions,
    get_proper_type,
    get_proper_types,
    has_recursive_types,
    is_named_instance,
    remove_dups,
    split_with_prefix_and_suffix,
)
from mypy.types_utils import (
    is_generic_instance,
    is_overlapping_none,
    is_self_type_like,
    remove_optional,
)
from mypy.typestate import type_state
from mypy.typevars import fill_typevars
from mypy.util import split_module_names
from mypy.visitor import ExpressionVisitor

# Type of callback user for checking individual function arguments. See
# check_args() below for details.
ArgChecker: _TypeAlias = Callable[
    [Type, Type, ArgKind, Type, int, int, CallableType, Optional[Type], Context, Context], None
]

# Maximum nesting level for math union in overloads, setting this to large values
# may cause performance issues. The reason is that although union math algorithm we use
# nicely captures most corner cases, its worst case complexity is exponential,
# see https://github.com/python/mypy/pull/5255#discussion_r196896335 for discussion.
MAX_UNIONS: Final = 5


# Types considered safe for comparisons with --strict-equality due to known behaviour of __eq__.
# NOTE: All these types are subtypes of AbstractSet.
OVERLAPPING_TYPES_ALLOWLIST: Final = [
    "builtins.set",
    "builtins.frozenset",
    "typing.KeysView",
    "typing.ItemsView",
    "builtins._dict_keys",
    "builtins._dict_items",
    "_collections_abc.dict_keys",
    "_collections_abc.dict_items",
]
OVERLAPPING_BYTES_ALLOWLIST: Final = {
    "builtins.bytes",
    "builtins.bytearray",
    "builtins.memoryview",
}


</t>
<t tx="ekr.20230831011819.471">class TooManyUnions(Exception):
    """Indicates that we need to stop splitting unions in an attempt
    to match an overload in order to save performance.
    """


</t>
<t tx="ekr.20230831011819.472">def allow_fast_container_literal(t: Type) -&gt; bool:
    if isinstance(t, TypeAliasType) and t.is_recursive:
        return False
    t = get_proper_type(t)
    return isinstance(t, Instance) or (
        isinstance(t, TupleType) and all(allow_fast_container_literal(it) for it in t.items)
    )


</t>
<t tx="ekr.20230831011819.473">def extract_refexpr_names(expr: RefExpr) -&gt; set[str]:
    """Recursively extracts all module references from a reference expression.

    Note that currently, the only two subclasses of RefExpr are NameExpr and
    MemberExpr."""
    output: set[str] = set()
    while isinstance(expr.node, MypyFile) or expr.fullname:
        if isinstance(expr.node, MypyFile) and expr.fullname:
            # If it's None, something's wrong (perhaps due to an
            # import cycle or a suppressed error).  For now we just
            # skip it.
            output.add(expr.fullname)

        if isinstance(expr, NameExpr):
            is_suppressed_import = isinstance(expr.node, Var) and expr.node.is_suppressed_import
            if isinstance(expr.node, TypeInfo):
                # Reference to a class or a nested class
                output.update(split_module_names(expr.node.module_name))
            elif "." in expr.fullname and not is_suppressed_import:
                # Everything else (that is not a silenced import within a class)
                output.add(expr.fullname.rsplit(".", 1)[0])
            break
        elif isinstance(expr, MemberExpr):
            if isinstance(expr.expr, RefExpr):
                expr = expr.expr
            else:
                break
        else:
            raise AssertionError(f"Unknown RefExpr subclass: {type(expr)}")
    return output


</t>
<t tx="ekr.20230831011819.474">class Finished(Exception):
    """Raised if we can terminate overload argument check early (no match)."""


</t>
<t tx="ekr.20230831011819.475">class ExpressionChecker(ExpressionVisitor[Type]):
    """Expression type checker.

    This class works closely together with checker.TypeChecker.
    """

    @others
</t>
<t tx="ekr.20230831011819.476"># Some services are provided by a TypeChecker instance.
chk: mypy.checker.TypeChecker
# This is shared with TypeChecker, but stored also here for convenience.
msg: MessageBuilder
# Type context for type inference
type_context: list[Type | None]

# cache resolved types in some cases
resolved_type: dict[Expression, ProperType]

strfrm_checker: StringFormatterChecker
plugin: Plugin

def __init__(
    self,
    chk: mypy.checker.TypeChecker,
    msg: MessageBuilder,
    plugin: Plugin,
    per_line_checking_time_ns: dict[int, int],
) -&gt; None:
    """Construct an expression type checker."""
    self.chk = chk
    self.msg = msg
    self.plugin = plugin
    self.per_line_checking_time_ns = per_line_checking_time_ns
    self.collect_line_checking_stats = chk.options.line_checking_stats is not None
    # Are we already visiting some expression? This is used to avoid double counting
    # time for nested expressions.
    self.in_expression = False
    self.type_context = [None]

    # Temporary overrides for expression types. This is currently
    # used by the union math in overloads.
    # TODO: refactor this to use a pattern similar to one in
    # multiassign_from_union, or maybe even combine the two?
    self.type_overrides: dict[Expression, Type] = {}
    self.strfrm_checker = StringFormatterChecker(self, self.chk, self.msg)

    self.resolved_type = {}

    # Callee in a call expression is in some sense both runtime context and
    # type context, because we support things like C[int](...). Store information
    # on whether current expression is a callee, to give better error messages
    # related to type context.
    self.is_callee = False
    type_state.infer_polymorphic = self.chk.options.new_type_inference

</t>
<t tx="ekr.20230831011819.477">def reset(self) -&gt; None:
    self.resolved_type = {}

</t>
<t tx="ekr.20230831011819.478">def visit_name_expr(self, e: NameExpr) -&gt; Type:
    """Type check a name expression.

    It can be of any kind: local, member or global.
    """
    self.chk.module_refs.update(extract_refexpr_names(e))
    result = self.analyze_ref_expr(e)
    return self.narrow_type_from_binder(e, result)

</t>
<t tx="ekr.20230831011819.479">def analyze_ref_expr(self, e: RefExpr, lvalue: bool = False) -&gt; Type:
    result: Type | None = None
    node = e.node

    if isinstance(e, NameExpr) and e.is_special_form:
        # A special form definition, nothing to check here.
        return AnyType(TypeOfAny.special_form)

    if isinstance(node, Var):
        # Variable reference.
        result = self.analyze_var_ref(node, e)
        if isinstance(result, PartialType):
            result = self.chk.handle_partial_var_type(result, lvalue, node, e)
    elif isinstance(node, FuncDef):
        # Reference to a global function.
        result = function_type(node, self.named_type("builtins.function"))
    elif isinstance(node, OverloadedFuncDef):
        if node.type is None:
            if self.chk.in_checked_function() and node.items:
                self.chk.handle_cannot_determine_type(node.name, e)
            result = AnyType(TypeOfAny.from_error)
        else:
            result = node.type
    elif isinstance(node, TypeInfo):
        # Reference to a type object.
        if node.typeddict_type:
            # We special-case TypedDict, because they don't define any constructor.
            result = self.typeddict_callable(node)
        else:
            result = type_object_type(node, self.named_type)
        if isinstance(result, CallableType) and isinstance(  # type: ignore[misc]
            result.ret_type, Instance
        ):
            # We need to set correct line and column
            # TODO: always do this in type_object_type by passing the original context
            result.ret_type.line = e.line
            result.ret_type.column = e.column
        if isinstance(get_proper_type(self.type_context[-1]), TypeType):
            # This is the type in a Type[] expression, so substitute type
            # variables with Any.
            result = erasetype.erase_typevars(result)
    elif isinstance(node, MypyFile):
        # Reference to a module object.
        result = self.module_type(node)
    elif isinstance(node, Decorator):
        result = self.analyze_var_ref(node.var, e)
    elif isinstance(node, TypeAlias):
        # Something that refers to a type alias appears in runtime context.
        # Note that we suppress bogus errors for alias redefinitions,
        # they are already reported in semanal.py.
        result = self.alias_type_in_runtime_context(
            node, ctx=e, alias_definition=e.is_alias_rvalue or lvalue
        )
    elif isinstance(node, (TypeVarExpr, ParamSpecExpr)):
        result = self.object_type()
    else:
        if isinstance(node, PlaceholderNode):
            assert False, f"PlaceholderNode {node.fullname!r} leaked to checker"
        # Unknown reference; use any type implicitly to avoid
        # generating extra type errors.
        result = AnyType(TypeOfAny.from_error)
    assert result is not None
    return result

</t>
<t tx="ekr.20230831011819.48">def invalidate_dependencies(self, expr: BindableExpression) -&gt; None:
    """Invalidate knowledge of types that include expr, but not expr itself.

    For example, when expr is foo.bar, invalidate foo.bar.baz.

    It is overly conservative: it invalidates globally, including
    in code paths unreachable from here.
    """
    key = literal_hash(expr)
    assert key is not None
    for dep in self.dependencies.get(key, set()):
        self._cleanse_key(dep)

</t>
<t tx="ekr.20230831011819.480">def analyze_var_ref(self, var: Var, context: Context) -&gt; Type:
    if var.type:
        var_type = get_proper_type(var.type)
        if isinstance(var_type, Instance):
            if self.is_literal_context() and var_type.last_known_value is not None:
                return var_type.last_known_value
            if var.name in {"True", "False"}:
                return self.infer_literal_expr_type(var.name == "True", "builtins.bool")
        return var.type
    else:
        if not var.is_ready and self.chk.in_checked_function():
            self.chk.handle_cannot_determine_type(var.name, context)
        # Implicit 'Any' type.
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.481">def module_type(self, node: MypyFile) -&gt; Instance:
    try:
        result = self.named_type("types.ModuleType")
    except KeyError:
        # In test cases might 'types' may not be available.
        # Fall back to a dummy 'object' type instead to
        # avoid a crash.
        result = self.named_type("builtins.object")
    module_attrs = {}
    immutable = set()
    for name, n in node.names.items():
        if not n.module_public:
            continue
        if isinstance(n.node, Var) and n.node.is_final:
            immutable.add(name)
        typ = self.chk.determine_type_of_member(n)
        if typ:
            module_attrs[name] = typ
        else:
            # TODO: what to do about nested module references?
            # They are non-trivial because there may be import cycles.
            module_attrs[name] = AnyType(TypeOfAny.special_form)
    result.extra_attrs = ExtraAttrs(module_attrs, immutable, node.fullname)
    return result

</t>
<t tx="ekr.20230831011819.482">def visit_call_expr(self, e: CallExpr, allow_none_return: bool = False) -&gt; Type:
    """Type check a call expression."""
    if e.analyzed:
        if isinstance(e.analyzed, NamedTupleExpr) and not e.analyzed.is_typed:
            # Type check the arguments, but ignore the results. This relies
            # on the typeshed stubs to type check the arguments.
            self.visit_call_expr_inner(e)
        # It's really a special form that only looks like a call.
        return self.accept(e.analyzed, self.type_context[-1])
    return self.visit_call_expr_inner(e, allow_none_return=allow_none_return)

</t>
<t tx="ekr.20230831011819.483">def refers_to_typeddict(self, base: Expression) -&gt; bool:
    if not isinstance(base, RefExpr):
        return False
    if isinstance(base.node, TypeInfo) and base.node.typeddict_type is not None:
        # Direct reference.
        return True
    return isinstance(base.node, TypeAlias) and isinstance(
        get_proper_type(base.node.target), TypedDictType
    )

</t>
<t tx="ekr.20230831011819.484">def visit_call_expr_inner(self, e: CallExpr, allow_none_return: bool = False) -&gt; Type:
    if (
        self.refers_to_typeddict(e.callee)
        or isinstance(e.callee, IndexExpr)
        and self.refers_to_typeddict(e.callee.base)
    ):
        typeddict_callable = get_proper_type(self.accept(e.callee, is_callee=True))
        if isinstance(typeddict_callable, CallableType):
            typeddict_type = get_proper_type(typeddict_callable.ret_type)
            assert isinstance(typeddict_type, TypedDictType)
            return self.check_typeddict_call(
                typeddict_type, e.arg_kinds, e.arg_names, e.args, e, typeddict_callable
            )
    if (
        isinstance(e.callee, NameExpr)
        and e.callee.name in ("isinstance", "issubclass")
        and len(e.args) == 2
    ):
        for typ in mypy.checker.flatten(e.args[1]):
            node = None
            if isinstance(typ, NameExpr):
                try:
                    node = self.chk.lookup_qualified(typ.name)
                except KeyError:
                    # Undefined names should already be reported in semantic analysis.
                    pass
            if is_expr_literal_type(typ):
                self.msg.cannot_use_function_with_type(e.callee.name, "Literal", e)
                continue
            if (
                node
                and isinstance(node.node, TypeAlias)
                and isinstance(get_proper_type(node.node.target), AnyType)
            ):
                self.msg.cannot_use_function_with_type(e.callee.name, "Any", e)
                continue
            if (
                isinstance(typ, IndexExpr)
                and isinstance(typ.analyzed, (TypeApplication, TypeAliasExpr))
            ) or (
                isinstance(typ, NameExpr)
                and node
                and isinstance(node.node, TypeAlias)
                and not node.node.no_args
            ):
                self.msg.type_arguments_not_allowed(e)
            if isinstance(typ, RefExpr) and isinstance(typ.node, TypeInfo):
                if typ.node.typeddict_type:
                    self.msg.cannot_use_function_with_type(e.callee.name, "TypedDict", e)
                elif typ.node.is_newtype:
                    self.msg.cannot_use_function_with_type(e.callee.name, "NewType", e)
    self.try_infer_partial_type(e)
    type_context = None
    if isinstance(e.callee, LambdaExpr):
        formal_to_actual = map_actuals_to_formals(
            e.arg_kinds,
            e.arg_names,
            e.callee.arg_kinds,
            e.callee.arg_names,
            lambda i: self.accept(e.args[i]),
        )

        arg_types = [
            join.join_type_list([self.accept(e.args[j]) for j in formal_to_actual[i]])
            for i in range(len(e.callee.arg_kinds))
        ]
        type_context = CallableType(
            arg_types,
            e.callee.arg_kinds,
            e.callee.arg_names,
            ret_type=self.object_type(),
            fallback=self.named_type("builtins.function"),
        )
    callee_type = get_proper_type(
        self.accept(e.callee, type_context, always_allow_any=True, is_callee=True)
    )

    # Figure out the full name of the callee for plugin lookup.
    object_type = None
    member = None
    fullname = None
    if isinstance(e.callee, RefExpr):
        # There are two special cases where plugins might act:
        # * A "static" reference/alias to a class or function;
        #   get_function_hook() will be invoked for these.
        fullname = e.callee.fullname or None
        if isinstance(e.callee.node, TypeAlias):
            target = get_proper_type(e.callee.node.target)
            if isinstance(target, Instance):
                fullname = target.type.fullname
        # * Call to a method on object that has a full name (see
        #   method_fullname() for details on supported objects);
        #   get_method_hook() and get_method_signature_hook() will
        #   be invoked for these.
        if (
            not fullname
            and isinstance(e.callee, MemberExpr)
            and self.chk.has_type(e.callee.expr)
        ):
            member = e.callee.name
            object_type = self.chk.lookup_type(e.callee.expr)

    if (
        self.chk.options.disallow_untyped_calls
        and self.chk.in_checked_function()
        and isinstance(callee_type, CallableType)
        and callee_type.implicit
    ):
        if fullname is None and member is not None:
            assert object_type is not None
            fullname = self.method_fullname(object_type, member)
        if not fullname or not any(
            fullname == p or fullname.startswith(f"{p}.")
            for p in self.chk.options.untyped_calls_exclude
        ):
            self.msg.untyped_function_call(callee_type, e)

    ret_type = self.check_call_expr_with_callee_type(
        callee_type, e, fullname, object_type, member
    )
    if isinstance(e.callee, RefExpr) and len(e.args) == 2:
        if e.callee.fullname in ("builtins.isinstance", "builtins.issubclass"):
            self.check_runtime_protocol_test(e)
        if e.callee.fullname == "builtins.issubclass":
            self.check_protocol_issubclass(e)
    if isinstance(e.callee, MemberExpr) and e.callee.name == "format":
        self.check_str_format_call(e)
    ret_type = get_proper_type(ret_type)
    if isinstance(ret_type, UnionType):
        ret_type = make_simplified_union(ret_type.items)
    if isinstance(ret_type, UninhabitedType) and not ret_type.ambiguous:
        self.chk.binder.unreachable()
    # Warn on calls to functions that always return None. The check
    # of ret_type is both a common-case optimization and prevents reporting
    # the error in dynamic functions (where it will be Any).
    if (
        not allow_none_return
        and isinstance(ret_type, NoneType)
        and self.always_returns_none(e.callee)
    ):
        self.chk.msg.does_not_return_value(callee_type, e)
        return AnyType(TypeOfAny.from_error)
    return ret_type

</t>
<t tx="ekr.20230831011819.485">def check_str_format_call(self, e: CallExpr) -&gt; None:
    """More precise type checking for str.format() calls on literals."""
    assert isinstance(e.callee, MemberExpr)
    format_value = None
    if isinstance(e.callee.expr, StrExpr):
        format_value = e.callee.expr.value
    elif self.chk.has_type(e.callee.expr):
        base_typ = try_getting_literal(self.chk.lookup_type(e.callee.expr))
        if isinstance(base_typ, LiteralType) and isinstance(base_typ.value, str):
            format_value = base_typ.value
    if format_value is not None:
        self.strfrm_checker.check_str_format_call(e, format_value)

</t>
<t tx="ekr.20230831011819.486">def method_fullname(self, object_type: Type, method_name: str) -&gt; str | None:
    """Convert a method name to a fully qualified name, based on the type of the object that
    it is invoked on. Return `None` if the name of `object_type` cannot be determined.
    """
    object_type = get_proper_type(object_type)

    if isinstance(object_type, CallableType) and object_type.is_type_obj():
        # For class method calls, object_type is a callable representing the class object.
        # We "unwrap" it to a regular type, as the class/instance method difference doesn't
        # affect the fully qualified name.
        object_type = get_proper_type(object_type.ret_type)
    elif isinstance(object_type, TypeType):
        object_type = object_type.item

    type_name = None
    if isinstance(object_type, Instance):
        type_name = object_type.type.fullname
    elif isinstance(object_type, (TypedDictType, LiteralType)):
        info = object_type.fallback.type.get_containing_type_info(method_name)
        type_name = info.fullname if info is not None else None
    elif isinstance(object_type, TupleType):
        type_name = tuple_fallback(object_type).type.fullname

    if type_name:
        return f"{type_name}.{method_name}"
    else:
        return None

</t>
<t tx="ekr.20230831011819.487">def always_returns_none(self, node: Expression) -&gt; bool:
    """Check if `node` refers to something explicitly annotated as only returning None."""
    if isinstance(node, RefExpr):
        if self.defn_returns_none(node.node):
            return True
    if isinstance(node, MemberExpr) and node.node is None:  # instance or class attribute
        typ = get_proper_type(self.chk.lookup_type(node.expr))
        if isinstance(typ, Instance):
            info = typ.type
        elif isinstance(typ, CallableType) and typ.is_type_obj():
            ret_type = get_proper_type(typ.ret_type)
            if isinstance(ret_type, Instance):
                info = ret_type.type
            else:
                return False
        else:
            return False
        sym = info.get(node.name)
        if sym and self.defn_returns_none(sym.node):
            return True
    return False

</t>
<t tx="ekr.20230831011819.488">def defn_returns_none(self, defn: SymbolNode | None) -&gt; bool:
    """Check if `defn` can _only_ return None."""
    if isinstance(defn, FuncDef):
        return isinstance(defn.type, CallableType) and isinstance(
            get_proper_type(defn.type.ret_type), NoneType
        )
    if isinstance(defn, OverloadedFuncDef):
        return all(self.defn_returns_none(item) for item in defn.items)
    if isinstance(defn, Var):
        typ = get_proper_type(defn.type)
        if (
            not defn.is_inferred
            and isinstance(typ, CallableType)
            and isinstance(get_proper_type(typ.ret_type), NoneType)
        ):
            return True
        if isinstance(typ, Instance):
            sym = typ.type.get("__call__")
            if sym and self.defn_returns_none(sym.node):
                return True
    return False

</t>
<t tx="ekr.20230831011819.489">def check_runtime_protocol_test(self, e: CallExpr) -&gt; None:
    for expr in mypy.checker.flatten(e.args[1]):
        tp = get_proper_type(self.chk.lookup_type(expr))
        if (
            isinstance(tp, CallableType)
            and tp.is_type_obj()
            and tp.type_object().is_protocol
            and not tp.type_object().runtime_protocol
        ):
            self.chk.fail(message_registry.RUNTIME_PROTOCOL_EXPECTED, e)

</t>
<t tx="ekr.20230831011819.49">def most_recent_enclosing_type(self, expr: BindableExpression, type: Type) -&gt; Type | None:
    type = get_proper_type(type)
    if isinstance(type, AnyType):
        return get_declaration(expr)
    key = literal_hash(expr)
    assert key is not None
    enclosers = [get_declaration(expr)] + [
        f.types[key] for f in self.frames if key in f.types and is_subtype(type, f.types[key])
    ]
    return enclosers[-1]

</t>
<t tx="ekr.20230831011819.490">def check_protocol_issubclass(self, e: CallExpr) -&gt; None:
    for expr in mypy.checker.flatten(e.args[1]):
        tp = get_proper_type(self.chk.lookup_type(expr))
        if isinstance(tp, CallableType) and tp.is_type_obj() and tp.type_object().is_protocol:
            attr_members = non_method_protocol_members(tp.type_object())
            if attr_members:
                self.chk.msg.report_non_method_protocol(tp.type_object(), attr_members, e)

</t>
<t tx="ekr.20230831011819.491">def check_typeddict_call(
    self,
    callee: TypedDictType,
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None],
    args: list[Expression],
    context: Context,
    orig_callee: Type | None,
) -&gt; Type:
    if args and all([ak in (ARG_NAMED, ARG_STAR2) for ak in arg_kinds]):
        # ex: Point(x=42, y=1337, **extras)
        # This is a bit ugly, but this is a price for supporting all possible syntax
        # variants for TypedDict constructors.
        kwargs = zip([StrExpr(n) if n is not None else None for n in arg_names], args)
        result = self.validate_typeddict_kwargs(kwargs=kwargs, callee=callee)
        if result is not None:
            validated_kwargs, always_present_keys = result
            return self.check_typeddict_call_with_kwargs(
                callee, validated_kwargs, context, orig_callee, always_present_keys
            )
        return AnyType(TypeOfAny.from_error)

    if len(args) == 1 and arg_kinds[0] == ARG_POS:
        unique_arg = args[0]
        if isinstance(unique_arg, DictExpr):
            # ex: Point({'x': 42, 'y': 1337, **extras})
            return self.check_typeddict_call_with_dict(
                callee, unique_arg.items, context, orig_callee
            )
        if isinstance(unique_arg, CallExpr) and isinstance(unique_arg.analyzed, DictExpr):
            # ex: Point(dict(x=42, y=1337, **extras))
            return self.check_typeddict_call_with_dict(
                callee, unique_arg.analyzed.items, context, orig_callee
            )

    if not args:
        # ex: EmptyDict()
        return self.check_typeddict_call_with_kwargs(callee, {}, context, orig_callee, set())

    self.chk.fail(message_registry.INVALID_TYPEDDICT_ARGS, context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011819.492">def validate_typeddict_kwargs(
    self, kwargs: Iterable[tuple[Expression | None, Expression]], callee: TypedDictType
) -&gt; tuple[dict[str, list[Expression]], set[str]] | None:
    # All (actual or mapped from ** unpacks) expressions that can match given key.
    result = defaultdict(list)
    # Keys that are guaranteed to be present no matter what (e.g. for all items of a union)
    always_present_keys = set()
    # Indicates latest encountered ** unpack among items.
    last_star_found = None

    for item_name_expr, item_arg in kwargs:
        if item_name_expr:
            key_type = self.accept(item_name_expr)
            values = try_getting_str_literals(item_name_expr, key_type)
            literal_value = None
            if values and len(values) == 1:
                literal_value = values[0]
            if literal_value is None:
                key_context = item_name_expr or item_arg
                self.chk.fail(
                    message_registry.TYPEDDICT_KEY_MUST_BE_STRING_LITERAL,
                    key_context,
                    code=codes.LITERAL_REQ,
                )
                return None
            else:
                # A directly present key unconditionally shadows all previously found
                # values from ** items.
                # TODO: for duplicate keys, type-check all values.
                result[literal_value] = [item_arg]
                always_present_keys.add(literal_value)
        else:
            last_star_found = item_arg
            if not self.validate_star_typeddict_item(
                item_arg, callee, result, always_present_keys
            ):
                return None
    if self.chk.options.extra_checks and last_star_found is not None:
        absent_keys = []
        for key in callee.items:
            if key not in callee.required_keys and key not in result:
                absent_keys.append(key)
        if absent_keys:
            # Having an optional key not explicitly declared by a ** unpacked
            # TypedDict is unsafe, it may be an (incompatible) subtype at runtime.
            # TODO: catch the cases where a declared key is overridden by a subsequent
            # ** item without it (and not again overriden with complete ** item).
            self.msg.non_required_keys_absent_with_star(absent_keys, last_star_found)
    return result, always_present_keys

</t>
<t tx="ekr.20230831011819.493">def validate_star_typeddict_item(
    self,
    item_arg: Expression,
    callee: TypedDictType,
    result: dict[str, list[Expression]],
    always_present_keys: set[str],
) -&gt; bool:
    """Update keys/expressions from a ** expression in TypedDict constructor.

    Note `result` and `always_present_keys` are updated in place. Return true if the
    expression `item_arg` may valid in `callee` TypedDict context.
    """
    with self.chk.local_type_map(), self.msg.filter_errors():
        inferred = get_proper_type(self.accept(item_arg, type_context=callee))
    possible_tds = []
    if isinstance(inferred, TypedDictType):
        possible_tds = [inferred]
    elif isinstance(inferred, UnionType):
        for item in get_proper_types(inferred.relevant_items()):
            if isinstance(item, TypedDictType):
                possible_tds.append(item)
            elif not self.valid_unpack_fallback_item(item):
                self.msg.unsupported_target_for_star_typeddict(item, item_arg)
                return False
    elif not self.valid_unpack_fallback_item(inferred):
        self.msg.unsupported_target_for_star_typeddict(inferred, item_arg)
        return False
    all_keys: set[str] = set()
    for td in possible_tds:
        all_keys |= td.items.keys()
    for key in all_keys:
        arg = TempNode(
            UnionType.make_union([td.items[key] for td in possible_tds if key in td.items])
        )
        arg.set_line(item_arg)
        if all(key in td.required_keys for td in possible_tds):
            always_present_keys.add(key)
            # Always present keys override previously found values. This is done
            # to support use cases like `Config({**defaults, **overrides})`, where
            # some `overrides` types are narrower that types in `defaults`, and
            # former are too wide for `Config`.
            if result[key]:
                first = result[key][0]
                if not isinstance(first, TempNode):
                    # We must always preserve any non-synthetic values, so that
                    # we will accept them even if they are shadowed.
                    result[key] = [first, arg]
                else:
                    result[key] = [arg]
            else:
                result[key] = [arg]
        else:
            # If this key is not required at least in some item of a union
            # it may not shadow previous item, so we need to type check both.
            result[key].append(arg)
    return True

</t>
<t tx="ekr.20230831011819.494">def valid_unpack_fallback_item(self, typ: ProperType) -&gt; bool:
    if isinstance(typ, AnyType):
        return True
    if not isinstance(typ, Instance) or not typ.type.has_base("typing.Mapping"):
        return False
    mapped = map_instance_to_supertype(typ, self.chk.lookup_typeinfo("typing.Mapping"))
    return all(isinstance(a, AnyType) for a in get_proper_types(mapped.args))

</t>
<t tx="ekr.20230831011819.495">def match_typeddict_call_with_dict(
    self,
    callee: TypedDictType,
    kwargs: list[tuple[Expression | None, Expression]],
    context: Context,
) -&gt; bool:
    result = self.validate_typeddict_kwargs(kwargs=kwargs, callee=callee)
    if result is not None:
        validated_kwargs, _ = result
        return callee.required_keys &lt;= set(validated_kwargs.keys()) &lt;= set(callee.items.keys())
    else:
        return False

</t>
<t tx="ekr.20230831011819.496">def check_typeddict_call_with_dict(
    self,
    callee: TypedDictType,
    kwargs: list[tuple[Expression | None, Expression]],
    context: Context,
    orig_callee: Type | None,
) -&gt; Type:
    result = self.validate_typeddict_kwargs(kwargs=kwargs, callee=callee)
    if result is not None:
        validated_kwargs, always_present_keys = result
        return self.check_typeddict_call_with_kwargs(
            callee,
            kwargs=validated_kwargs,
            context=context,
            orig_callee=orig_callee,
            always_present_keys=always_present_keys,
        )
    else:
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011819.497">def typeddict_callable(self, info: TypeInfo) -&gt; CallableType:
    """Construct a reasonable type for a TypedDict type in runtime context.

    If it appears as a callee, it will be special-cased anyway, e.g. it is
    also allowed to accept a single positional argument if it is a dict literal.

    Note it is not safe to move this to type_object_type() since it will crash
    on plugin-generated TypedDicts, that may not have the special_alias.
    """
    assert info.special_alias is not None
    target = info.special_alias.target
    assert isinstance(target, ProperType) and isinstance(target, TypedDictType)
    expected_types = list(target.items.values())
    kinds = [ArgKind.ARG_NAMED] * len(expected_types)
    names = list(target.items.keys())
    return CallableType(
        expected_types,
        kinds,
        names,
        target,
        self.named_type("builtins.type"),
        variables=info.defn.type_vars,
    )

</t>
<t tx="ekr.20230831011819.498">def typeddict_callable_from_context(self, callee: TypedDictType) -&gt; CallableType:
    return CallableType(
        list(callee.items.values()),
        [ArgKind.ARG_NAMED] * len(callee.items),
        list(callee.items.keys()),
        callee,
        self.named_type("builtins.type"),
    )

</t>
<t tx="ekr.20230831011819.499">def check_typeddict_call_with_kwargs(
    self,
    callee: TypedDictType,
    kwargs: dict[str, list[Expression]],
    context: Context,
    orig_callee: Type | None,
    always_present_keys: set[str],
) -&gt; Type:
    actual_keys = kwargs.keys()
    if not (
        callee.required_keys &lt;= always_present_keys and actual_keys &lt;= callee.items.keys()
    ):
        if not (actual_keys &lt;= callee.items.keys()):
            self.msg.unexpected_typeddict_keys(
                callee,
                expected_keys=[
                    key
                    for key in callee.items.keys()
                    if key in callee.required_keys or key in actual_keys
                ],
                actual_keys=list(actual_keys),
                context=context,
            )
        if not (callee.required_keys &lt;= always_present_keys):
            self.msg.unexpected_typeddict_keys(
                callee,
                expected_keys=[
                    key for key in callee.items.keys() if key in callee.required_keys
                ],
                actual_keys=[
                    key for key in always_present_keys if key in callee.required_keys
                ],
                context=context,
            )
        if callee.required_keys &gt; actual_keys:
            # found_set is a sub-set of the required_keys
            # This means we're missing some keys and as such, we can't
            # properly type the object
            return AnyType(TypeOfAny.from_error)

    orig_callee = get_proper_type(orig_callee)
    if isinstance(orig_callee, CallableType):
        infer_callee = orig_callee
    else:
        # Try reconstructing from type context.
        if callee.fallback.type.special_alias is not None:
            infer_callee = self.typeddict_callable(callee.fallback.type)
        else:
            # Likely a TypedDict type generated by a plugin.
            infer_callee = self.typeddict_callable_from_context(callee)

    # We don't show any errors, just infer types in a generic TypedDict type,
    # a custom error message will be given below, if there are errors.
    with self.msg.filter_errors(), self.chk.local_type_map():
        orig_ret_type, _ = self.check_callable_call(
            infer_callee,
            # We use first expression for each key to infer type variables of a generic
            # TypedDict. This is a bit arbitrary, but in most cases will work better than
            # trying to infer a union or a join.
            [args[0] for args in kwargs.values()],
            [ArgKind.ARG_NAMED] * len(kwargs),
            context,
            list(kwargs.keys()),
            None,
            None,
            None,
        )

    ret_type = get_proper_type(orig_ret_type)
    if not isinstance(ret_type, TypedDictType):
        # If something went really wrong, type-check call with original type,
        # this may give a better error message.
        ret_type = callee

    for item_name, item_expected_type in ret_type.items.items():
        if item_name in kwargs:
            item_values = kwargs[item_name]
            for item_value in item_values:
                self.chk.check_simple_assignment(
                    lvalue_type=item_expected_type,
                    rvalue=item_value,
                    context=item_value,
                    msg=ErrorMessage(
                        message_registry.INCOMPATIBLE_TYPES.value, code=codes.TYPEDDICT_ITEM
                    ),
                    lvalue_name=f'TypedDict item "{item_name}"',
                    rvalue_name="expression",
                )

    return orig_ret_type

</t>
<t tx="ekr.20230831011819.5">
from __future__ import annotations

import os
import sys
import traceback

from mypy.main import main, process_options
from mypy.util import FancyFormatter


</t>
<t tx="ekr.20230831011819.50">def allow_jump(self, index: int) -&gt; None:
    # self.frames and self.options_on_return have different lengths
    # so make sure the index is positive
    if index &lt; 0:
        index += len(self.options_on_return)
    frame = Frame(self._get_id())
    for f in self.frames[index + 1 :]:
        frame.types.update(f.types)
        if f.unreachable:
            frame.unreachable = True
    self.options_on_return[index].append(frame)

</t>
<t tx="ekr.20230831011819.500">def get_partial_self_var(self, expr: MemberExpr) -&gt; Var | None:
    """Get variable node for a partial self attribute.

    If the expression is not a self attribute, or attribute is not variable,
    or variable is not partial, return None.
    """
    if not (
        isinstance(expr.expr, NameExpr)
        and isinstance(expr.expr.node, Var)
        and expr.expr.node.is_self
    ):
        # Not a self.attr expression.
        return None
    info = self.chk.scope.enclosing_class()
    if not info or expr.name not in info.names:
        # Don't mess with partial types in superclasses.
        return None
    sym = info.names[expr.name]
    if isinstance(sym.node, Var) and isinstance(sym.node.type, PartialType):
        return sym.node
    return None

</t>
<t tx="ekr.20230831011819.501"># Types and methods that can be used to infer partial types.
item_args: ClassVar[dict[str, list[str]]] = {
    "builtins.list": ["append"],
    "builtins.set": ["add", "discard"],
}
container_args: ClassVar[dict[str, dict[str, list[str]]]] = {
    "builtins.list": {"extend": ["builtins.list"]},
    "builtins.dict": {"update": ["builtins.dict"]},
    "collections.OrderedDict": {"update": ["builtins.dict"]},
    "builtins.set": {"update": ["builtins.set", "builtins.list"]},
}

def try_infer_partial_type(self, e: CallExpr) -&gt; None:
    """Try to make partial type precise from a call."""
    if not isinstance(e.callee, MemberExpr):
        return
    callee = e.callee
    if isinstance(callee.expr, RefExpr):
        # Call a method with a RefExpr callee, such as 'x.method(...)'.
        ret = self.get_partial_var(callee.expr)
        if ret is None:
            return
        var, partial_types = ret
        typ = self.try_infer_partial_value_type_from_call(e, callee.name, var)
        # Var may be deleted from partial_types in try_infer_partial_value_type_from_call
        if typ is not None and var in partial_types:
            var.type = typ
            del partial_types[var]
    elif isinstance(callee.expr, IndexExpr) and isinstance(callee.expr.base, RefExpr):
        # Call 'x[y].method(...)'; may infer type of 'x' if it's a partial defaultdict.
        if callee.expr.analyzed is not None:
            return  # A special form
        base = callee.expr.base
        index = callee.expr.index
        ret = self.get_partial_var(base)
        if ret is None:
            return
        var, partial_types = ret
        partial_type = get_partial_instance_type(var.type)
        if partial_type is None or partial_type.value_type is None:
            return
        value_type = self.try_infer_partial_value_type_from_call(e, callee.name, var)
        if value_type is not None:
            # Infer key type.
            key_type = self.accept(index)
            if mypy.checker.is_valid_inferred_type(key_type):
                # Store inferred partial type.
                assert partial_type.type is not None
                typename = partial_type.type.fullname
                var.type = self.chk.named_generic_type(typename, [key_type, value_type])
                del partial_types[var]

</t>
<t tx="ekr.20230831011819.502">def get_partial_var(self, ref: RefExpr) -&gt; tuple[Var, dict[Var, Context]] | None:
    var = ref.node
    if var is None and isinstance(ref, MemberExpr):
        var = self.get_partial_self_var(ref)
    if not isinstance(var, Var):
        return None
    partial_types = self.chk.find_partial_types(var)
    if partial_types is None:
        return None
    return var, partial_types

</t>
<t tx="ekr.20230831011819.503">def try_infer_partial_value_type_from_call(
    self, e: CallExpr, methodname: str, var: Var
) -&gt; Instance | None:
    """Try to make partial type precise from a call such as 'x.append(y)'."""
    if self.chk.current_node_deferred:
        return None
    partial_type = get_partial_instance_type(var.type)
    if partial_type is None:
        return None
    if partial_type.value_type:
        typename = partial_type.value_type.type.fullname
    else:
        assert partial_type.type is not None
        typename = partial_type.type.fullname
    # Sometimes we can infer a full type for a partial List, Dict or Set type.
    # TODO: Don't infer argument expression twice.
    if (
        typename in self.item_args
        and methodname in self.item_args[typename]
        and e.arg_kinds == [ARG_POS]
    ):
        item_type = self.accept(e.args[0])
        if mypy.checker.is_valid_inferred_type(item_type):
            return self.chk.named_generic_type(typename, [item_type])
    elif (
        typename in self.container_args
        and methodname in self.container_args[typename]
        and e.arg_kinds == [ARG_POS]
    ):
        arg_type = get_proper_type(self.accept(e.args[0]))
        if isinstance(arg_type, Instance):
            arg_typename = arg_type.type.fullname
            if arg_typename in self.container_args[typename][methodname]:
                if all(
                    mypy.checker.is_valid_inferred_type(item_type)
                    for item_type in arg_type.args
                ):
                    return self.chk.named_generic_type(typename, list(arg_type.args))
        elif isinstance(arg_type, AnyType):
            return self.chk.named_type(typename)

    return None

</t>
<t tx="ekr.20230831011819.504">def apply_function_plugin(
    self,
    callee: CallableType,
    arg_kinds: list[ArgKind],
    arg_types: list[Type],
    arg_names: Sequence[str | None] | None,
    formal_to_actual: list[list[int]],
    args: list[Expression],
    fullname: str,
    object_type: Type | None,
    context: Context,
) -&gt; Type:
    """Use special case logic to infer the return type of a specific named function/method.

    Caller must ensure that a plugin hook exists. There are two different cases:

    - If object_type is None, the caller must ensure that a function hook exists
      for fullname.
    - If object_type is not None, the caller must ensure that a method hook exists
      for fullname.

    Return the inferred return type.
    """
    num_formals = len(callee.arg_types)
    formal_arg_types: list[list[Type]] = [[] for _ in range(num_formals)]
    formal_arg_exprs: list[list[Expression]] = [[] for _ in range(num_formals)]
    formal_arg_names: list[list[str | None]] = [[] for _ in range(num_formals)]
    formal_arg_kinds: list[list[ArgKind]] = [[] for _ in range(num_formals)]
    for formal, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            formal_arg_types[formal].append(arg_types[actual])
            formal_arg_exprs[formal].append(args[actual])
            if arg_names:
                formal_arg_names[formal].append(arg_names[actual])
            formal_arg_kinds[formal].append(arg_kinds[actual])

    if object_type is None:
        # Apply function plugin
        callback = self.plugin.get_function_hook(fullname)
        assert callback is not None  # Assume that caller ensures this
        return callback(
            FunctionContext(
                formal_arg_types,
                formal_arg_kinds,
                callee.arg_names,
                formal_arg_names,
                callee.ret_type,
                formal_arg_exprs,
                context,
                self.chk,
            )
        )
    else:
        # Apply method plugin
        method_callback = self.plugin.get_method_hook(fullname)
        assert method_callback is not None  # Assume that caller ensures this
        object_type = get_proper_type(object_type)
        return method_callback(
            MethodContext(
                object_type,
                formal_arg_types,
                formal_arg_kinds,
                callee.arg_names,
                formal_arg_names,
                callee.ret_type,
                formal_arg_exprs,
                context,
                self.chk,
            )
        )

</t>
<t tx="ekr.20230831011819.505">def apply_signature_hook(
    self,
    callee: FunctionLike,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    hook: Callable[[list[list[Expression]], CallableType], FunctionLike],
) -&gt; FunctionLike:
    """Helper to apply a signature hook for either a function or method"""
    if isinstance(callee, CallableType):
        num_formals = len(callee.arg_kinds)
        formal_to_actual = map_actuals_to_formals(
            arg_kinds,
            arg_names,
            callee.arg_kinds,
            callee.arg_names,
            lambda i: self.accept(args[i]),
        )
        formal_arg_exprs: list[list[Expression]] = [[] for _ in range(num_formals)]
        for formal, actuals in enumerate(formal_to_actual):
            for actual in actuals:
                formal_arg_exprs[formal].append(args[actual])
        return hook(formal_arg_exprs, callee)
    else:
        assert isinstance(callee, Overloaded)
        items = []
        for item in callee.items:
            adjusted = self.apply_signature_hook(item, args, arg_kinds, arg_names, hook)
            assert isinstance(adjusted, CallableType)
            items.append(adjusted)
        return Overloaded(items)

</t>
<t tx="ekr.20230831011819.506">def apply_function_signature_hook(
    self,
    callee: FunctionLike,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None,
    signature_hook: Callable[[FunctionSigContext], FunctionLike],
) -&gt; FunctionLike:
    """Apply a plugin hook that may infer a more precise signature for a function."""
    return self.apply_signature_hook(
        callee,
        args,
        arg_kinds,
        arg_names,
        (lambda args, sig: signature_hook(FunctionSigContext(args, sig, context, self.chk))),
    )

</t>
<t tx="ekr.20230831011819.507">def apply_method_signature_hook(
    self,
    callee: FunctionLike,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None,
    object_type: Type,
    signature_hook: Callable[[MethodSigContext], FunctionLike],
) -&gt; FunctionLike:
    """Apply a plugin hook that may infer a more precise signature for a method."""
    pobject_type = get_proper_type(object_type)
    return self.apply_signature_hook(
        callee,
        args,
        arg_kinds,
        arg_names,
        (
            lambda args, sig: signature_hook(
                MethodSigContext(pobject_type, args, sig, context, self.chk)
            )
        ),
    )

</t>
<t tx="ekr.20230831011819.508">def transform_callee_type(
    self,
    callable_name: str | None,
    callee: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None = None,
    object_type: Type | None = None,
) -&gt; Type:
    """Attempt to determine a more accurate signature for a method call.

    This is done by looking up and applying a method signature hook (if one exists for the
    given method name).

    If no matching method signature hook is found, callee is returned unmodified. The same
    happens if the arguments refer to a non-method callable (this is allowed so that the code
    calling transform_callee_type needs to perform fewer boilerplate checks).

    Note: this method is *not* called automatically as part of check_call, because in some
    cases check_call is called multiple times while checking a single call (for example when
    dealing with overloads). Instead, this method needs to be called explicitly
    (if appropriate) before the signature is passed to check_call.
    """
    callee = get_proper_type(callee)
    if callable_name is not None and isinstance(callee, FunctionLike):
        if object_type is not None:
            method_sig_hook = self.plugin.get_method_signature_hook(callable_name)
            if method_sig_hook:
                return self.apply_method_signature_hook(
                    callee, args, arg_kinds, context, arg_names, object_type, method_sig_hook
                )
        else:
            function_sig_hook = self.plugin.get_function_signature_hook(callable_name)
            if function_sig_hook:
                return self.apply_function_signature_hook(
                    callee, args, arg_kinds, context, arg_names, function_sig_hook
                )

    return callee

</t>
<t tx="ekr.20230831011819.509">def is_generic_decorator_overload_call(
    self, callee_type: CallableType, args: list[Expression]
) -&gt; Overloaded | None:
    """Check if this looks like an application of a generic function to overload argument."""
    assert callee_type.variables
    if len(callee_type.arg_types) != 1 or len(args) != 1:
        # TODO: can we handle more general cases?
        return None
    if not isinstance(get_proper_type(callee_type.arg_types[0]), CallableType):
        return None
    if not isinstance(get_proper_type(callee_type.ret_type), CallableType):
        return None
    with self.chk.local_type_map():
        with self.msg.filter_errors():
            arg_type = get_proper_type(self.accept(args[0], type_context=None))
    if isinstance(arg_type, Overloaded):
        return arg_type
    return None

</t>
<t tx="ekr.20230831011819.51">def handle_break(self) -&gt; None:
    self.allow_jump(self.break_frames[-1])
    self.unreachable()

</t>
<t tx="ekr.20230831011819.510">def handle_decorator_overload_call(
    self, callee_type: CallableType, overloaded: Overloaded, ctx: Context
) -&gt; tuple[Type, Type] | None:
    """Type-check application of a generic callable to an overload.

    We check call on each individual overload item, and then combine results into a new
    overload. This function should be only used if callee_type takes and returns a Callable.
    """
    result = []
    inferred_args = []
    for item in overloaded.items:
        arg = TempNode(typ=item)
        with self.msg.filter_errors() as err:
            item_result, inferred_arg = self.check_call(callee_type, [arg], [ARG_POS], ctx)
        if err.has_new_errors():
            # This overload doesn't match.
            continue
        p_item_result = get_proper_type(item_result)
        if not isinstance(p_item_result, CallableType):
            continue
        p_inferred_arg = get_proper_type(inferred_arg)
        if not isinstance(p_inferred_arg, CallableType):
            continue
        inferred_args.append(p_inferred_arg)
        result.append(p_item_result)
    if not result or not inferred_args:
        # None of the overload matched (or overload was initially malformed).
        return None
    return Overloaded(result), Overloaded(inferred_args)

</t>
<t tx="ekr.20230831011819.511">def check_call_expr_with_callee_type(
    self,
    callee_type: Type,
    e: CallExpr,
    callable_name: str | None,
    object_type: Type | None,
    member: str | None = None,
) -&gt; Type:
    """Type check call expression.

    The callee_type should be used as the type of callee expression. In particular,
    in case of a union type this can be a particular item of the union, so that we can
    apply plugin hooks to each item.

    The 'member', 'callable_name' and 'object_type' are only used to call plugin hooks.
    If 'callable_name' is None but 'member' is not None (member call), try constructing
    'callable_name' using 'object_type' (the base type on which the method is called),
    for example 'typing.Mapping.get'.
    """
    if callable_name is None and member is not None:
        assert object_type is not None
        callable_name = self.method_fullname(object_type, member)
    object_type = get_proper_type(object_type)
    if callable_name:
        # Try to refine the call signature using plugin hooks before checking the call.
        callee_type = self.transform_callee_type(
            callable_name, callee_type, e.args, e.arg_kinds, e, e.arg_names, object_type
        )
    # Unions are special-cased to allow plugins to act on each item in the union.
    elif member is not None and isinstance(object_type, UnionType):
        return self.check_union_call_expr(e, object_type, member)
    ret_type, callee_type = self.check_call(
        callee_type,
        e.args,
        e.arg_kinds,
        e,
        e.arg_names,
        callable_node=e.callee,
        callable_name=callable_name,
        object_type=object_type,
    )
    proper_callee = get_proper_type(callee_type)
    if (
        isinstance(e.callee, RefExpr)
        and isinstance(proper_callee, CallableType)
        and proper_callee.type_guard is not None
    ):
        # Cache it for find_isinstance_check()
        e.callee.type_guard = proper_callee.type_guard
    return ret_type

</t>
<t tx="ekr.20230831011819.512">def check_union_call_expr(self, e: CallExpr, object_type: UnionType, member: str) -&gt; Type:
    """Type check calling a member expression where the base type is a union."""
    res: list[Type] = []
    for typ in object_type.relevant_items():
        # Member access errors are already reported when visiting the member expression.
        with self.msg.filter_errors():
            item = analyze_member_access(
                member,
                typ,
                e,
                False,
                False,
                False,
                self.msg,
                original_type=object_type,
                chk=self.chk,
                in_literal_context=self.is_literal_context(),
                self_type=typ,
            )
        narrowed = self.narrow_type_from_binder(e.callee, item, skip_non_overlapping=True)
        if narrowed is None:
            continue
        callable_name = self.method_fullname(typ, member)
        item_object_type = typ if callable_name else None
        res.append(
            self.check_call_expr_with_callee_type(narrowed, e, callable_name, item_object_type)
        )
    return make_simplified_union(res)

</t>
<t tx="ekr.20230831011819.513">def check_call(
    self,
    callee: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None = None,
    callable_node: Expression | None = None,
    callable_name: str | None = None,
    object_type: Type | None = None,
) -&gt; tuple[Type, Type]:
    """Type check a call.

    Also infer type arguments if the callee is a generic function.

    Return (result type, inferred callee type).

    Arguments:
        callee: type of the called value
        args: actual argument expressions
        arg_kinds: contains nodes.ARG_* constant for each argument in args
             describing whether the argument is positional, *arg, etc.
        context: current expression context, used for inference.
        arg_names: names of arguments (optional)
        callable_node: associate the inferred callable type to this node,
            if specified
        callable_name: Fully-qualified name of the function/method to call,
            or None if unavailable (examples: 'builtins.open', 'typing.Mapping.get')
        object_type: If callable_name refers to a method, the type of the object
            on which the method is being called
    """
    callee = get_proper_type(callee)

    if isinstance(callee, CallableType):
        if callee.variables:
            overloaded = self.is_generic_decorator_overload_call(callee, args)
            if overloaded is not None:
                # Special casing for inline application of generic callables to overloads.
                # Supporting general case would be tricky, but this should cover 95% of cases.
                overloaded_result = self.handle_decorator_overload_call(
                    callee, overloaded, context
                )
                if overloaded_result is not None:
                    return overloaded_result

        return self.check_callable_call(
            callee,
            args,
            arg_kinds,
            context,
            arg_names,
            callable_node,
            callable_name,
            object_type,
        )
    elif isinstance(callee, Overloaded):
        return self.check_overload_call(
            callee, args, arg_kinds, arg_names, callable_name, object_type, context
        )
    elif isinstance(callee, AnyType) or not self.chk.in_checked_function():
        return self.check_any_type_call(args, callee)
    elif isinstance(callee, UnionType):
        return self.check_union_call(callee, args, arg_kinds, arg_names, context)
    elif isinstance(callee, Instance):
        call_function = analyze_member_access(
            "__call__",
            callee,
            context,
            is_lvalue=False,
            is_super=False,
            is_operator=True,
            msg=self.msg,
            original_type=callee,
            chk=self.chk,
            in_literal_context=self.is_literal_context(),
        )
        callable_name = callee.type.fullname + ".__call__"
        # Apply method signature hook, if one exists
        call_function = self.transform_callee_type(
            callable_name, call_function, args, arg_kinds, context, arg_names, callee
        )
        result = self.check_call(
            call_function,
            args,
            arg_kinds,
            context,
            arg_names,
            callable_node,
            callable_name,
            callee,
        )
        if callable_node:
            # check_call() stored "call_function" as the type, which is incorrect.
            # Override the type.
            self.chk.store_type(callable_node, callee)
        return result
    elif isinstance(callee, TypeVarType):
        return self.check_call(
            callee.upper_bound, args, arg_kinds, context, arg_names, callable_node
        )
    elif isinstance(callee, TypeType):
        item = self.analyze_type_type_callee(callee.item, context)
        return self.check_call(item, args, arg_kinds, context, arg_names, callable_node)
    elif isinstance(callee, TupleType):
        return self.check_call(
            tuple_fallback(callee),
            args,
            arg_kinds,
            context,
            arg_names,
            callable_node,
            callable_name,
            object_type,
        )
    else:
        return self.msg.not_callable(callee, context), AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011819.514">def check_callable_call(
    self,
    callee: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None,
    callable_node: Expression | None,
    callable_name: str | None,
    object_type: Type | None,
) -&gt; tuple[Type, Type]:
    """Type check a call that targets a callable value.

    See the docstring of check_call for more information.
    """
    # Always unpack **kwargs before checking a call.
    callee = callee.with_unpacked_kwargs().with_normalized_var_args()
    if callable_name is None and callee.name:
        callable_name = callee.name
    ret_type = get_proper_type(callee.ret_type)
    if callee.is_type_obj() and isinstance(ret_type, Instance):
        callable_name = ret_type.type.fullname
    if isinstance(callable_node, RefExpr) and callable_node.fullname in ENUM_BASES:
        # An Enum() call that failed SemanticAnalyzerPass2.check_enum_call().
        return callee.ret_type, callee

    if (
        callee.is_type_obj()
        and callee.type_object().is_protocol
        # Exception for Type[...]
        and not callee.from_type_type
    ):
        self.chk.fail(
            message_registry.CANNOT_INSTANTIATE_PROTOCOL.format(callee.type_object().name),
            context,
        )
    elif (
        callee.is_type_obj()
        and callee.type_object().is_abstract
        # Exception for Type[...]
        and not callee.from_type_type
        and not callee.type_object().fallback_to_any
    ):
        type = callee.type_object()
        # Determine whether the implicitly abstract attributes are functions with
        # None-compatible return types.
        abstract_attributes: dict[str, bool] = {}
        for attr_name, abstract_status in type.abstract_attributes:
            if abstract_status == IMPLICITLY_ABSTRACT:
                abstract_attributes[attr_name] = self.can_return_none(type, attr_name)
            else:
                abstract_attributes[attr_name] = False
        self.msg.cannot_instantiate_abstract_class(
            callee.type_object().name, abstract_attributes, context
        )

    formal_to_actual = map_actuals_to_formals(
        arg_kinds,
        arg_names,
        callee.arg_kinds,
        callee.arg_names,
        lambda i: self.accept(args[i]),
    )

    # This is tricky: return type may contain its own type variables, like in
    # def [S] (S) -&gt; def [T] (T) -&gt; tuple[S, T], so we need to update their ids
    # to avoid possible id clashes if this call itself appears in a generic
    # function body.
    ret_type = get_proper_type(callee.ret_type)
    if isinstance(ret_type, CallableType) and ret_type.variables:
        fresh_ret_type = freshen_all_functions_type_vars(callee.ret_type)
        freeze_all_type_vars(fresh_ret_type)
        callee = callee.copy_modified(ret_type=fresh_ret_type)

    if callee.is_generic():
        need_refresh = any(
            isinstance(v, (ParamSpecType, TypeVarTupleType)) for v in callee.variables
        )
        callee = freshen_function_type_vars(callee)
        callee = self.infer_function_type_arguments_using_context(callee, context)
        if need_refresh:
            # Argument kinds etc. may have changed due to
            # ParamSpec or TypeVarTuple variables being replaced with an arbitrary
            # number of arguments; recalculate actual-to-formal map
            formal_to_actual = map_actuals_to_formals(
                arg_kinds,
                arg_names,
                callee.arg_kinds,
                callee.arg_names,
                lambda i: self.accept(args[i]),
            )
        callee = self.infer_function_type_arguments(
            callee, args, arg_kinds, arg_names, formal_to_actual, need_refresh, context
        )
        if need_refresh:
            formal_to_actual = map_actuals_to_formals(
                arg_kinds,
                arg_names,
                callee.arg_kinds,
                callee.arg_names,
                lambda i: self.accept(args[i]),
            )

    param_spec = callee.param_spec()
    if param_spec is not None and arg_kinds == [ARG_STAR, ARG_STAR2]:
        arg1 = self.accept(args[0])
        arg2 = self.accept(args[1])
        if (
            isinstance(arg1, ParamSpecType)
            and isinstance(arg2, ParamSpecType)
            and arg1.flavor == ParamSpecFlavor.ARGS
            and arg2.flavor == ParamSpecFlavor.KWARGS
            and arg1.id == arg2.id == param_spec.id
        ):
            return callee.ret_type, callee

    arg_types = self.infer_arg_types_in_context(callee, args, arg_kinds, formal_to_actual)

    self.check_argument_count(
        callee,
        arg_types,
        arg_kinds,
        arg_names,
        formal_to_actual,
        context,
        object_type,
        callable_name,
    )

    self.check_argument_types(
        arg_types, arg_kinds, args, callee, formal_to_actual, context, object_type=object_type
    )

    if (
        callee.is_type_obj()
        and (len(arg_types) == 1)
        and is_equivalent(callee.ret_type, self.named_type("builtins.type"))
    ):
        callee = callee.copy_modified(ret_type=TypeType.make_normalized(arg_types[0]))

    if callable_node:
        # Store the inferred callable type.
        self.chk.store_type(callable_node, callee)

    if callable_name and (
        (object_type is None and self.plugin.get_function_hook(callable_name))
        or (object_type is not None and self.plugin.get_method_hook(callable_name))
    ):
        new_ret_type = self.apply_function_plugin(
            callee,
            arg_kinds,
            arg_types,
            arg_names,
            formal_to_actual,
            args,
            callable_name,
            object_type,
            context,
        )
        callee = callee.copy_modified(ret_type=new_ret_type)
    return callee.ret_type, callee

</t>
<t tx="ekr.20230831011819.515">def can_return_none(self, type: TypeInfo, attr_name: str) -&gt; bool:
    """Is the given attribute a method with a None-compatible return type?

    Overloads are only checked if there is an implementation.
    """
    if not state.strict_optional:
        # If strict-optional is not set, is_subtype(NoneType(), T) is always True.
        # So, we cannot do anything useful here in that case.
        return False
    for base in type.mro:
        symnode = base.names.get(attr_name)
        if symnode is None:
            continue
        node = symnode.node
        if isinstance(node, OverloadedFuncDef):
            node = node.impl
        if isinstance(node, Decorator):
            node = node.func
        if isinstance(node, FuncDef):
            if node.type is not None:
                assert isinstance(node.type, CallableType)
                return is_subtype(NoneType(), node.type.ret_type)
    return False

</t>
<t tx="ekr.20230831011819.516">def analyze_type_type_callee(self, item: ProperType, context: Context) -&gt; Type:
    """Analyze the callee X in X(...) where X is Type[item].

    Return a Y that we can pass to check_call(Y, ...).
    """
    if isinstance(item, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=item)
    if isinstance(item, Instance):
        res = type_object_type(item.type, self.named_type)
        if isinstance(res, CallableType):
            res = res.copy_modified(from_type_type=True)
        expanded = expand_type_by_instance(res, item)
        if isinstance(expanded, CallableType):
            # Callee of the form Type[...] should never be generic, only
            # proper class objects can be.
            expanded = expanded.copy_modified(variables=[])
        return expanded
    if isinstance(item, UnionType):
        return UnionType(
            [
                self.analyze_type_type_callee(get_proper_type(tp), context)
                for tp in item.relevant_items()
            ],
            item.line,
        )
    if isinstance(item, TypeVarType):
        # Pretend we're calling the typevar's upper bound,
        # i.e. its constructor (a poor approximation for reality,
        # but better than AnyType...), but replace the return type
        # with typevar.
        callee = self.analyze_type_type_callee(get_proper_type(item.upper_bound), context)
        callee = get_proper_type(callee)
        if isinstance(callee, CallableType):
            callee = callee.copy_modified(ret_type=item)
        elif isinstance(callee, Overloaded):
            callee = Overloaded([c.copy_modified(ret_type=item) for c in callee.items])
        return callee
    # We support Type of namedtuples but not of tuples in general
    if isinstance(item, TupleType) and tuple_fallback(item).type.fullname != "builtins.tuple":
        return self.analyze_type_type_callee(tuple_fallback(item), context)

    self.msg.unsupported_type_type(item, context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011819.517">def infer_arg_types_in_empty_context(self, args: list[Expression]) -&gt; list[Type]:
    """Infer argument expression types in an empty context.

    In short, we basically recurse on each argument without considering
    in what context the argument was called.
    """
    res: list[Type] = []

    for arg in args:
        arg_type = self.accept(arg)
        if has_erased_component(arg_type):
            res.append(NoneType())
        else:
            res.append(arg_type)
    return res

</t>
<t tx="ekr.20230831011819.518">def infer_more_unions_for_recursive_type(self, type_context: Type) -&gt; bool:
    """Adjust type inference of unions if type context has a recursive type.

    Return the old state. The caller must assign it to type_state.infer_unions
    afterwards.

    This is a hack to better support inference for recursive types.

    Note: This is performance-sensitive and must not be a context manager
    until mypyc supports them better.
    """
    old = type_state.infer_unions
    if has_recursive_types(type_context):
        type_state.infer_unions = True
    return old

</t>
<t tx="ekr.20230831011819.519">def infer_arg_types_in_context(
    self,
    callee: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
) -&gt; list[Type]:
    """Infer argument expression types using a callable type as context.

    For example, if callee argument 2 has type List[int], infer the
    argument expression with List[int] type context.

    Returns the inferred types of *actual arguments*.
    """
    res: list[Type | None] = [None] * len(args)

    for i, actuals in enumerate(formal_to_actual):
        for ai in actuals:
            if not arg_kinds[ai].is_star():
                arg_type = callee.arg_types[i]
                # When the outer context for a function call is known to be recursive,
                # we solve type constraints inferred from arguments using unions instead
                # of joins. This is a bit arbitrary, but in practice it works for most
                # cases. A cleaner alternative would be to switch to single bin type
                # inference, but this is a lot of work.
                old = self.infer_more_unions_for_recursive_type(arg_type)
                res[ai] = self.accept(args[ai], arg_type)
                # We need to manually restore union inference state, ugh.
                type_state.infer_unions = old

    # Fill in the rest of the argument types.
    for i, t in enumerate(res):
        if not t:
            res[i] = self.accept(args[i])
    assert all(tp is not None for tp in res)
    return cast(List[Type], res)

</t>
<t tx="ekr.20230831011819.52">def handle_continue(self) -&gt; None:
    self.allow_jump(self.continue_frames[-1])
    self.unreachable()

</t>
<t tx="ekr.20230831011819.520">def infer_function_type_arguments_using_context(
    self, callable: CallableType, error_context: Context
) -&gt; CallableType:
    """Unify callable return type to type context to infer type vars.

    For example, if the return type is set[t] where 't' is a type variable
    of callable, and if the context is set[int], return callable modified
    by substituting 't' with 'int'.
    """
    ctx = self.type_context[-1]
    if not ctx:
        return callable
    # The return type may have references to type metavariables that
    # we are inferring right now. We must consider them as indeterminate
    # and they are not potential results; thus we replace them with the
    # special ErasedType type. On the other hand, class type variables are
    # valid results.
    erased_ctx = replace_meta_vars(ctx, ErasedType())
    ret_type = callable.ret_type
    if is_overlapping_none(ret_type) and is_overlapping_none(ctx):
        # If both the context and the return type are optional, unwrap the optional,
        # since in 99% cases this is what a user expects. In other words, we replace
        #     Optional[T] &lt;: Optional[int]
        # with
        #     T &lt;: int
        # while the former would infer T &lt;: Optional[int].
        ret_type = remove_optional(ret_type)
        erased_ctx = remove_optional(erased_ctx)
        #
        # TODO: Instead of this hack and the one below, we need to use outer and
        # inner contexts at the same time. This is however not easy because of two
        # reasons:
        #   * We need to support constraints like [1 &lt;: 2, 2 &lt;: X], i.e. with variables
        #     on both sides. (This is not too hard.)
        #   * We need to update all the inference "infrastructure", so that all
        #     variables in an expression are inferred at the same time.
        #     (And this is hard, also we need to be careful with lambdas that require
        #     two passes.)
    if isinstance(ret_type, TypeVarType):
        # Another special case: the return type is a type variable. If it's unrestricted,
        # we could infer a too general type for the type variable if we use context,
        # and this could result in confusing and spurious type errors elsewhere.
        #
        # So we give up and just use function arguments for type inference, with just two
        # exceptions:
        #
        # 1. If the context is a generic instance type, actually use it as context, as
        #    this *seems* to usually be the reasonable thing to do.
        #
        #    See also github issues #462 and #360.
        #
        # 2. If the context is some literal type, we want to "propagate" that information
        #    down so that we infer a more precise type for literal expressions. For example,
        #    the expression `3` normally has an inferred type of `builtins.int`: but if it's
        #    in a literal context like below, we want it to infer `Literal[3]` instead.
        #
        #        def expects_literal(x: Literal[3]) -&gt; None: pass
        #        def identity(x: T) -&gt; T: return x
        #
        #        expects_literal(identity(3))  # Should type-check
        # TODO: we may want to add similar exception if all arguments are lambdas, since
        # in this case external context is almost everything we have.
        if not is_generic_instance(ctx) and not is_literal_type_like(ctx):
            return callable.copy_modified()
    args = infer_type_arguments(callable.variables, ret_type, erased_ctx)
    # Only substitute non-Uninhabited and non-erased types.
    new_args: list[Type | None] = []
    for arg in args:
        if has_uninhabited_component(arg) or has_erased_component(arg):
            new_args.append(None)
        else:
            new_args.append(arg)
    # Don't show errors after we have only used the outer context for inference.
    # We will use argument context to infer more variables.
    return self.apply_generic_arguments(
        callable, new_args, error_context, skip_unsatisfied=True
    )

</t>
<t tx="ekr.20230831011819.521">def infer_function_type_arguments(
    self,
    callee_type: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    formal_to_actual: list[list[int]],
    need_refresh: bool,
    context: Context,
) -&gt; CallableType:
    """Infer the type arguments for a generic callee type.

    Infer based on the types of arguments.

    Return a derived callable type that has the arguments applied.
    """
    if self.chk.in_checked_function():
        # Disable type errors during type inference. There may be errors
        # due to partial available context information at this time, but
        # these errors can be safely ignored as the arguments will be
        # inferred again later.
        with self.msg.filter_errors():
            arg_types = self.infer_arg_types_in_context(
                callee_type, args, arg_kinds, formal_to_actual
            )

        arg_pass_nums = self.get_arg_infer_passes(
            callee_type.arg_types, formal_to_actual, len(args)
        )

        pass1_args: list[Type | None] = []
        for i, arg in enumerate(arg_types):
            if arg_pass_nums[i] &gt; 1:
                pass1_args.append(None)
            else:
                pass1_args.append(arg)

        inferred_args, _ = infer_function_type_arguments(
            callee_type,
            pass1_args,
            arg_kinds,
            formal_to_actual,
            context=self.argument_infer_context(),
            strict=self.chk.in_checked_function(),
        )

        if 2 in arg_pass_nums:
            # Second pass of type inference.
            (callee_type, inferred_args) = self.infer_function_type_arguments_pass2(
                callee_type,
                args,
                arg_kinds,
                arg_names,
                formal_to_actual,
                inferred_args,
                need_refresh,
                context,
            )

        if (
            callee_type.special_sig == "dict"
            and len(inferred_args) == 2
            and (ARG_NAMED in arg_kinds or ARG_STAR2 in arg_kinds)
        ):
            # HACK: Infer str key type for dict(...) with keyword args. The type system
            #       can't represent this so we special case it, as this is a pretty common
            #       thing. This doesn't quite work with all possible subclasses of dict
            #       if they shuffle type variables around, as we assume that there is a 1-1
            #       correspondence with dict type variables. This is a marginal issue and
            #       a little tricky to fix so it's left unfixed for now.
            first_arg = get_proper_type(inferred_args[0])
            if isinstance(first_arg, (NoneType, UninhabitedType)):
                inferred_args[0] = self.named_type("builtins.str")
            elif not first_arg or not is_subtype(self.named_type("builtins.str"), first_arg):
                self.chk.fail(message_registry.KEYWORD_ARGUMENT_REQUIRES_STR_KEY_TYPE, context)

        if self.chk.options.new_type_inference and any(
            a is None
            or isinstance(get_proper_type(a), UninhabitedType)
            or set(get_type_vars(a)) &amp; set(callee_type.variables)
            for a in inferred_args
        ):
            if need_refresh:
                # Technically we need to refresh formal_to_actual after *each* inference pass,
                # since each pass can expand ParamSpec or TypeVarTuple. Although such situations
                # are very rare, not doing this can cause crashes.
                formal_to_actual = map_actuals_to_formals(
                    arg_kinds,
                    arg_names,
                    callee_type.arg_kinds,
                    callee_type.arg_names,
                    lambda a: self.accept(args[a]),
                )
            # If the regular two-phase inference didn't work, try inferring type
            # variables while allowing for polymorphic solutions, i.e. for solutions
            # potentially involving free variables.
            # TODO: support the similar inference for return type context.
            poly_inferred_args, free_vars = infer_function_type_arguments(
                callee_type,
                arg_types,
                arg_kinds,
                formal_to_actual,
                context=self.argument_infer_context(),
                strict=self.chk.in_checked_function(),
                allow_polymorphic=True,
            )
            poly_callee_type = self.apply_generic_arguments(
                callee_type, poly_inferred_args, context
            )
            # Try applying inferred polymorphic type if possible, e.g. Callable[[T], T] can
            # be interpreted as def [T] (T) -&gt; T, but dict[T, T] cannot be expressed.
            applied = apply_poly(poly_callee_type, free_vars)
            if applied is not None and all(
                a is not None and not isinstance(get_proper_type(a), UninhabitedType)
                for a in poly_inferred_args
            ):
                freeze_all_type_vars(applied)
                return applied
            # If it didn't work, erase free variables as &lt;nothing&gt;, to avoid confusing errors.
            unknown = UninhabitedType()
            unknown.ambiguous = True
            inferred_args = [
                expand_type(
                    a, {v.id: unknown for v in list(callee_type.variables) + free_vars}
                )
                if a is not None
                else None
                for a in poly_inferred_args
            ]
    else:
        # In dynamically typed functions use implicit 'Any' types for
        # type variables.
        inferred_args = [AnyType(TypeOfAny.unannotated)] * len(callee_type.variables)
    return self.apply_inferred_arguments(callee_type, inferred_args, context)

</t>
<t tx="ekr.20230831011819.522">def infer_function_type_arguments_pass2(
    self,
    callee_type: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    formal_to_actual: list[list[int]],
    old_inferred_args: Sequence[Type | None],
    need_refresh: bool,
    context: Context,
) -&gt; tuple[CallableType, list[Type | None]]:
    """Perform second pass of generic function type argument inference.

    The second pass is needed for arguments with types such as Callable[[T], S],
    where both T and S are type variables, when the actual argument is a
    lambda with inferred types.  The idea is to infer the type variable T
    in the first pass (based on the types of other arguments).  This lets
    us infer the argument and return type of the lambda expression and
    thus also the type variable S in this second pass.

    Return (the callee with type vars applied, inferred actual arg types).
    """
    # None or erased types in inferred types mean that there was not enough
    # information to infer the argument. Replace them with None values so
    # that they are not applied yet below.
    inferred_args = list(old_inferred_args)
    for i, arg in enumerate(get_proper_types(inferred_args)):
        if isinstance(arg, (NoneType, UninhabitedType)) or has_erased_component(arg):
            inferred_args[i] = None
    callee_type = self.apply_generic_arguments(callee_type, inferred_args, context)
    if need_refresh:
        formal_to_actual = map_actuals_to_formals(
            arg_kinds,
            arg_names,
            callee_type.arg_kinds,
            callee_type.arg_names,
            lambda a: self.accept(args[a]),
        )

    arg_types = self.infer_arg_types_in_context(callee_type, args, arg_kinds, formal_to_actual)

    inferred_args, _ = infer_function_type_arguments(
        callee_type,
        arg_types,
        arg_kinds,
        formal_to_actual,
        context=self.argument_infer_context(),
    )

    return callee_type, inferred_args

</t>
<t tx="ekr.20230831011819.523">def argument_infer_context(self) -&gt; ArgumentInferContext:
    return ArgumentInferContext(
        self.chk.named_type("typing.Mapping"), self.chk.named_type("typing.Iterable")
    )

</t>
<t tx="ekr.20230831011819.524">def get_arg_infer_passes(
    self, arg_types: list[Type], formal_to_actual: list[list[int]], num_actuals: int
) -&gt; list[int]:
    """Return pass numbers for args for two-pass argument type inference.

    For each actual, the pass number is either 1 (first pass) or 2 (second
    pass).

    Two-pass argument type inference primarily lets us infer types of
    lambdas more effectively.
    """
    res = [1] * num_actuals
    for i, arg in enumerate(arg_types):
        if arg.accept(ArgInferSecondPassQuery()):
            for j in formal_to_actual[i]:
                res[j] = 2
    return res

</t>
<t tx="ekr.20230831011819.525">def apply_inferred_arguments(
    self, callee_type: CallableType, inferred_args: Sequence[Type | None], context: Context
) -&gt; CallableType:
    """Apply inferred values of type arguments to a generic function.

    Inferred_args contains the values of function type arguments.
    """
    # Report error if some of the variables could not be solved. In that
    # case assume that all variables have type Any to avoid extra
    # bogus error messages.
    for i, inferred_type in enumerate(inferred_args):
        if not inferred_type or has_erased_component(inferred_type):
            # Could not infer a non-trivial type for a type variable.
            self.msg.could_not_infer_type_arguments(callee_type, i + 1, context)
            inferred_args = [AnyType(TypeOfAny.from_error)] * len(inferred_args)
    # Apply the inferred types to the function type. In this case the
    # return type must be CallableType, since we give the right number of type
    # arguments.
    return self.apply_generic_arguments(callee_type, inferred_args, context)

</t>
<t tx="ekr.20230831011819.526">def check_argument_count(
    self,
    callee: CallableType,
    actual_types: list[Type],
    actual_kinds: list[ArgKind],
    actual_names: Sequence[str | None] | None,
    formal_to_actual: list[list[int]],
    context: Context | None,
    object_type: Type | None = None,
    callable_name: str | None = None,
) -&gt; bool:
    """Check that there is a value for all required arguments to a function.

    Also check that there are no duplicate values for arguments. Report found errors
    using 'messages' if it's not None. If 'messages' is given, 'context' must also be given.

    Return False if there were any errors. Otherwise return True
    """
    if context is None:
        # Avoid "is None" checks
        context = TempNode(AnyType(TypeOfAny.special_form))

    # TODO(jukka): We could return as soon as we find an error if messages is None.

    # Collect dict of all actual arguments matched to formal arguments, with occurrence count
    all_actuals: dict[int, int] = {}
    for actuals in formal_to_actual:
        for a in actuals:
            all_actuals[a] = all_actuals.get(a, 0) + 1

    ok, is_unexpected_arg_error = self.check_for_extra_actual_arguments(
        callee, actual_types, actual_kinds, actual_names, all_actuals, context
    )

    # Check for too many or few values for formals.
    for i, kind in enumerate(callee.arg_kinds):
        if kind.is_required() and not formal_to_actual[i] and not is_unexpected_arg_error:
            # No actual for a mandatory formal
            if kind.is_positional():
                self.msg.too_few_arguments(callee, context, actual_names)
                if object_type and callable_name and "." in callable_name:
                    self.missing_classvar_callable_note(object_type, callable_name, context)
            else:
                argname = callee.arg_names[i] or "?"
                self.msg.missing_named_argument(callee, context, argname)
            ok = False
        elif not kind.is_star() and is_duplicate_mapping(
            formal_to_actual[i], actual_types, actual_kinds
        ):
            if self.chk.in_checked_function() or isinstance(
                get_proper_type(actual_types[formal_to_actual[i][0]]), TupleType
            ):
                self.msg.duplicate_argument_value(callee, i, context)
                ok = False
        elif (
            kind.is_named()
            and formal_to_actual[i]
            and actual_kinds[formal_to_actual[i][0]] not in [nodes.ARG_NAMED, nodes.ARG_STAR2]
        ):
            # Positional argument when expecting a keyword argument.
            self.msg.too_many_positional_arguments(callee, context)
            ok = False
    return ok

</t>
<t tx="ekr.20230831011819.527">def check_for_extra_actual_arguments(
    self,
    callee: CallableType,
    actual_types: list[Type],
    actual_kinds: list[ArgKind],
    actual_names: Sequence[str | None] | None,
    all_actuals: dict[int, int],
    context: Context,
) -&gt; tuple[bool, bool]:
    """Check for extra actual arguments.

    Return tuple (was everything ok,
                  was there an extra keyword argument error [used to avoid duplicate errors]).
    """

    is_unexpected_arg_error = False  # Keep track of errors to avoid duplicate errors
    ok = True  # False if we've found any error

    for i, kind in enumerate(actual_kinds):
        if (
            i not in all_actuals
            and
            # We accept the other iterables than tuple (including Any)
            # as star arguments because they could be empty, resulting no arguments.
            (kind != nodes.ARG_STAR or is_non_empty_tuple(actual_types[i]))
            and
            # Accept all types for double-starred arguments, because they could be empty
            # dictionaries and we can't tell it from their types
            kind != nodes.ARG_STAR2
        ):
            # Extra actual: not matched by a formal argument.
            ok = False
            if kind != nodes.ARG_NAMED:
                self.msg.too_many_arguments(callee, context)
            else:
                assert actual_names, "Internal error: named kinds without names given"
                act_name = actual_names[i]
                assert act_name is not None
                act_type = actual_types[i]
                self.msg.unexpected_keyword_argument(callee, act_name, act_type, context)
                is_unexpected_arg_error = True
        elif (
            kind == nodes.ARG_STAR and nodes.ARG_STAR not in callee.arg_kinds
        ) or kind == nodes.ARG_STAR2:
            actual_type = get_proper_type(actual_types[i])
            if isinstance(actual_type, (TupleType, TypedDictType)):
                if all_actuals.get(i, 0) &lt; len(actual_type.items):
                    # Too many tuple/dict items as some did not match.
                    if kind != nodes.ARG_STAR2 or not isinstance(actual_type, TypedDictType):
                        self.msg.too_many_arguments(callee, context)
                    else:
                        self.msg.too_many_arguments_from_typed_dict(
                            callee, actual_type, context
                        )
                        is_unexpected_arg_error = True
                    ok = False
            # *args/**kwargs can be applied even if the function takes a fixed
            # number of positional arguments. This may succeed at runtime.

    return ok, is_unexpected_arg_error

</t>
<t tx="ekr.20230831011819.528">def missing_classvar_callable_note(
    self, object_type: Type, callable_name: str, context: Context
) -&gt; None:
    if isinstance(object_type, ProperType) and isinstance(object_type, Instance):
        _, var_name = callable_name.rsplit(".", maxsplit=1)
        node = object_type.type.get(var_name)
        if node is not None and isinstance(node.node, Var):
            if not node.node.is_inferred and not node.node.is_classvar:
                self.msg.note(
                    f'"{var_name}" is considered instance variable,'
                    " to make it class variable use ClassVar[...]",
                    context,
                )

</t>
<t tx="ekr.20230831011819.529">def check_argument_types(
    self,
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    args: list[Expression],
    callee: CallableType,
    formal_to_actual: list[list[int]],
    context: Context,
    check_arg: ArgChecker | None = None,
    object_type: Type | None = None,
) -&gt; None:
    """Check argument types against a callable type.

    Report errors if the argument types are not compatible.

    The check_call docstring describes some of the arguments.
    """
    check_arg = check_arg or self.check_arg
    # Keep track of consumed tuple *arg items.
    mapper = ArgTypeExpander(self.argument_infer_context())
    for i, actuals in enumerate(formal_to_actual):
        orig_callee_arg_type = get_proper_type(callee.arg_types[i])

        # Checking the case that we have more than one item but the first argument
        # is an unpack, so this would be something like:
        # [Tuple[Unpack[Ts]], int]
        #
        # In this case we have to check everything together, we do this by re-unifying
        # the suffices to the tuple, e.g. a single actual like
        # Tuple[Unpack[Ts], int]
        expanded_tuple = False
        if len(actuals) &gt; 1:
            first_actual_arg_type = get_proper_type(arg_types[actuals[0]])
            if (
                isinstance(first_actual_arg_type, TupleType)
                and len(first_actual_arg_type.items) == 1
                and isinstance(first_actual_arg_type.items[0], UnpackType)
            ):
                # TODO: use walrus operator
                actual_types = [first_actual_arg_type.items[0]] + [
                    arg_types[a] for a in actuals[1:]
                ]
                actual_kinds = [nodes.ARG_STAR] + [nodes.ARG_POS] * (len(actuals) - 1)

                # TODO: can we really assert this? What if formal is just plain Unpack[Ts]?
                assert isinstance(orig_callee_arg_type, UnpackType)
                assert isinstance(orig_callee_arg_type.type, ProperType) and isinstance(
                    orig_callee_arg_type.type, TupleType
                )
                assert orig_callee_arg_type.type.items
                callee_arg_types = orig_callee_arg_type.type.items
                callee_arg_kinds = [nodes.ARG_STAR] + [nodes.ARG_POS] * (
                    len(orig_callee_arg_type.type.items) - 1
                )
                expanded_tuple = True

        if not expanded_tuple:
            actual_types = [arg_types[a] for a in actuals]
            actual_kinds = [arg_kinds[a] for a in actuals]
            if isinstance(orig_callee_arg_type, UnpackType):
                unpacked_type = get_proper_type(orig_callee_arg_type.type)
                if isinstance(unpacked_type, TupleType):
                    inner_unpack_index = find_unpack_in_list(unpacked_type.items)
                    if inner_unpack_index is None:
                        callee_arg_types = unpacked_type.items
                        callee_arg_kinds = [ARG_POS] * len(actuals)
                    else:
                        inner_unpack = unpacked_type.items[inner_unpack_index]
                        assert isinstance(inner_unpack, UnpackType)
                        inner_unpacked_type = get_proper_type(inner_unpack.type)
                        # We assume heterogenous tuples are desugared earlier
                        assert isinstance(inner_unpacked_type, Instance)
                        assert inner_unpacked_type.type.fullname == "builtins.tuple"
                        callee_arg_types = (
                            unpacked_type.items[:inner_unpack_index]
                            + [inner_unpacked_type.args[0]]
                            * (len(actuals) - len(unpacked_type.items) + 1)
                            + unpacked_type.items[inner_unpack_index + 1 :]
                        )
                        callee_arg_kinds = [ARG_POS] * len(actuals)
                elif isinstance(unpacked_type, TypeVarTupleType):
                    callee_arg_types = [orig_callee_arg_type]
                    callee_arg_kinds = [ARG_STAR]
                else:
                    # TODO: Any and &lt;nothing&gt; can appear in Unpack (as a result of user error),
                    # fail gracefully here and elsewhere (and/or normalize them away).
                    assert isinstance(unpacked_type, Instance)
                    assert unpacked_type.type.fullname == "builtins.tuple"
                    callee_arg_types = [unpacked_type.args[0]] * len(actuals)
                    callee_arg_kinds = [ARG_POS] * len(actuals)
            else:
                callee_arg_types = [orig_callee_arg_type] * len(actuals)
                callee_arg_kinds = [callee.arg_kinds[i]] * len(actuals)

        assert len(actual_types) == len(actuals) == len(actual_kinds)

        if len(callee_arg_types) != len(actual_types):
            # TODO: Improve error message
            self.chk.fail("Invalid number of arguments", context)
            continue

        assert len(callee_arg_types) == len(actual_types)
        assert len(callee_arg_types) == len(callee_arg_kinds)
        for actual, actual_type, actual_kind, callee_arg_type, callee_arg_kind in zip(
            actuals, actual_types, actual_kinds, callee_arg_types, callee_arg_kinds
        ):
            if actual_type is None:
                continue  # Some kind of error was already reported.
            # Check that a *arg is valid as varargs.
            if actual_kind == nodes.ARG_STAR and not self.is_valid_var_arg(actual_type):
                self.msg.invalid_var_arg(actual_type, context)
            if actual_kind == nodes.ARG_STAR2 and not self.is_valid_keyword_var_arg(
                actual_type
            ):
                is_mapping = is_subtype(
                    actual_type, self.chk.named_type("_typeshed.SupportsKeysAndGetItem")
                )
                self.msg.invalid_keyword_var_arg(actual_type, is_mapping, context)
            expanded_actual = mapper.expand_actual_type(
                actual_type, actual_kind, callee.arg_names[i], callee_arg_kind
            )
            check_arg(
                expanded_actual,
                actual_type,
                actual_kind,
                callee_arg_type,
                actual + 1,
                i + 1,
                callee,
                object_type,
                args[actual],
                context,
            )

</t>
<t tx="ekr.20230831011819.53">@contextmanager
def frame_context(
    self,
    *,
    can_skip: bool,
    fall_through: int = 1,
    break_frame: int = 0,
    continue_frame: int = 0,
    conditional_frame: bool = False,
    try_frame: bool = False,
) -&gt; Iterator[Frame]:
    """Return a context manager that pushes/pops frames on enter/exit.

    If can_skip is True, control flow is allowed to bypass the
    newly-created frame.

    If fall_through &gt; 0, then it will allow control flow that
    falls off the end of the frame to escape to its ancestor
    `fall_through` levels higher. Otherwise control flow ends
    at the end of the frame.

    If break_frame &gt; 0, then 'break' statements within this frame
    will jump out to the frame break_frame levels higher than the
    frame created by this call to frame_context. Similarly for
    continue_frame and 'continue' statements.

    If try_frame is true, then execution is allowed to jump at any
    point within the newly created frame (or its descendants) to
    its parent (i.e., to the frame that was on top before this
    call to frame_context).

    After the context manager exits, self.last_pop_changed indicates
    whether any types changed in the newly-topmost frame as a result
    of popping this frame.
    """
    assert len(self.frames) &gt; 1

    if break_frame:
        self.break_frames.append(len(self.frames) - break_frame)
    if continue_frame:
        self.continue_frames.append(len(self.frames) - continue_frame)
    if try_frame:
        self.try_frames.add(len(self.frames) - 1)

    new_frame = self.push_frame(conditional_frame)
    if try_frame:
        # An exception may occur immediately
        self.allow_jump(-1)
    yield new_frame
    self.pop_frame(can_skip, fall_through)

    if break_frame:
        self.break_frames.pop()
    if continue_frame:
        self.continue_frames.pop()
    if try_frame:
        self.try_frames.remove(len(self.frames) - 1)

</t>
<t tx="ekr.20230831011819.530">def check_arg(
    self,
    caller_type: Type,
    original_caller_type: Type,
    caller_kind: ArgKind,
    callee_type: Type,
    n: int,
    m: int,
    callee: CallableType,
    object_type: Type | None,
    context: Context,
    outer_context: Context,
) -&gt; None:
    """Check the type of a single argument in a call."""
    caller_type = get_proper_type(caller_type)
    original_caller_type = get_proper_type(original_caller_type)
    callee_type = get_proper_type(callee_type)

    if isinstance(caller_type, DeletedType):
        self.msg.deleted_as_rvalue(caller_type, context)
    # Only non-abstract non-protocol class can be given where Type[...] is expected...
    elif self.has_abstract_type_part(caller_type, callee_type):
        self.msg.concrete_only_call(callee_type, context)
    elif not is_subtype(caller_type, callee_type, options=self.chk.options):
        code = self.msg.incompatible_argument(
            n,
            m,
            callee,
            original_caller_type,
            caller_kind,
            object_type=object_type,
            context=context,
            outer_context=outer_context,
        )
        self.msg.incompatible_argument_note(
            original_caller_type, callee_type, context, code=code
        )
        if not self.msg.prefer_simple_messages():
            self.chk.check_possible_missing_await(caller_type, callee_type, context)

</t>
<t tx="ekr.20230831011819.531">def check_overload_call(
    self,
    callee: Overloaded,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    callable_name: str | None,
    object_type: Type | None,
    context: Context,
) -&gt; tuple[Type, Type]:
    """Checks a call to an overloaded function."""
    # Normalize unpacked kwargs before checking the call.
    callee = callee.with_unpacked_kwargs()
    arg_types = self.infer_arg_types_in_empty_context(args)
    # Step 1: Filter call targets to remove ones where the argument counts don't match
    plausible_targets = self.plausible_overload_call_targets(
        arg_types, arg_kinds, arg_names, callee
    )

    # Step 2: If the arguments contain a union, we try performing union math first,
    #         instead of picking the first matching overload.
    #         This is because picking the first overload often ends up being too greedy:
    #         for example, when we have a fallback alternative that accepts an unrestricted
    #         typevar. See https://github.com/python/mypy/issues/4063 for related discussion.
    erased_targets: list[CallableType] | None = None
    unioned_result: tuple[Type, Type] | None = None

    # Determine whether we need to encourage union math. This should be generally safe,
    # as union math infers better results in the vast majority of cases, but it is very
    # computationally intensive.
    none_type_var_overlap = self.possible_none_type_var_overlap(arg_types, plausible_targets)
    union_interrupted = False  # did we try all union combinations?
    if any(self.real_union(arg) for arg in arg_types):
        try:
            with self.msg.filter_errors():
                unioned_return = self.union_overload_result(
                    plausible_targets,
                    args,
                    arg_types,
                    arg_kinds,
                    arg_names,
                    callable_name,
                    object_type,
                    none_type_var_overlap,
                    context,
                )
        except TooManyUnions:
            union_interrupted = True
        else:
            # Record if we succeeded. Next we need to see if maybe normal procedure
            # gives a narrower type.
            if unioned_return:
                returns, inferred_types = zip(*unioned_return)
                # Note that we use `combine_function_signatures` instead of just returning
                # a union of inferred callables because for example a call
                # Union[int -&gt; int, str -&gt; str](Union[int, str]) is invalid and
                # we don't want to introduce internal inconsistencies.
                unioned_result = (
                    make_simplified_union(list(returns), context.line, context.column),
                    self.combine_function_signatures(get_proper_types(inferred_types)),
                )

    # Step 3: We try checking each branch one-by-one.
    inferred_result = self.infer_overload_return_type(
        plausible_targets,
        args,
        arg_types,
        arg_kinds,
        arg_names,
        callable_name,
        object_type,
        context,
    )
    # If any of checks succeed, stop early.
    if inferred_result is not None and unioned_result is not None:
        # Both unioned and direct checks succeeded, choose the more precise type.
        if (
            is_subtype(inferred_result[0], unioned_result[0])
            and not isinstance(get_proper_type(inferred_result[0]), AnyType)
            and not none_type_var_overlap
        ):
            return inferred_result
        return unioned_result
    elif unioned_result is not None:
        return unioned_result
    elif inferred_result is not None:
        return inferred_result

    # Step 4: Failure. At this point, we know there is no match. We fall back to trying
    #         to find a somewhat plausible overload target using the erased types
    #         so we can produce a nice error message.
    #
    #         For example, suppose the user passes a value of type 'List[str]' into an
    #         overload with signatures f(x: int) -&gt; int and f(x: List[int]) -&gt; List[int].
    #
    #         Neither alternative matches, but we can guess the user probably wants the
    #         second one.
    erased_targets = self.overload_erased_call_targets(
        plausible_targets, arg_types, arg_kinds, arg_names, args, context
    )

    # Step 5: We try and infer a second-best alternative if possible. If not, fall back
    #         to using 'Any'.
    if len(erased_targets) &gt; 0:
        # Pick the first plausible erased target as the fallback
        # TODO: Adjust the error message here to make it clear there was no match.
        #       In order to do this, we need to find a clean way of associating
        #       a note with whatever error message 'self.check_call' will generate.
        #       In particular, the note's line and column numbers need to be the same
        #       as the error's.
        target: Type = erased_targets[0]
    else:
        # There was no plausible match: give up
        target = AnyType(TypeOfAny.from_error)
        if not is_operator_method(callable_name):
            code = None
        else:
            code = codes.OPERATOR
        self.msg.no_variant_matches_arguments(callee, arg_types, context, code=code)

    result = self.check_call(
        target,
        args,
        arg_kinds,
        context,
        arg_names,
        callable_name=callable_name,
        object_type=object_type,
    )
    # Do not show the extra error if the union math was forced.
    if union_interrupted and not none_type_var_overlap:
        self.chk.fail(message_registry.TOO_MANY_UNION_COMBINATIONS, context)
    return result

</t>
<t tx="ekr.20230831011819.532">def plausible_overload_call_targets(
    self,
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    overload: Overloaded,
) -&gt; list[CallableType]:
    """Returns all overload call targets that having matching argument counts.

    If the given args contains a star-arg (*arg or **kwarg argument), this method
    will ensure all star-arg overloads appear at the start of the list, instead
    of their usual location.

    The only exception is if the starred argument is something like a Tuple or a
    NamedTuple, which has a definitive "shape". If so, we don't move the corresponding
    alternative to the front since we can infer a more precise match using the original
    order."""

    def has_shape(typ: Type) -&gt; bool:
        typ = get_proper_type(typ)
        return isinstance(typ, (TupleType, TypedDictType)) or (
            isinstance(typ, Instance) and typ.type.is_named_tuple
        )

    matches: list[CallableType] = []
    star_matches: list[CallableType] = []

    args_have_var_arg = False
    args_have_kw_arg = False
    for kind, typ in zip(arg_kinds, arg_types):
        if kind == ARG_STAR and not has_shape(typ):
            args_have_var_arg = True
        if kind == ARG_STAR2 and not has_shape(typ):
            args_have_kw_arg = True

    for typ in overload.items:
        formal_to_actual = map_actuals_to_formals(
            arg_kinds, arg_names, typ.arg_kinds, typ.arg_names, lambda i: arg_types[i]
        )

        with self.msg.filter_errors():
            if self.check_argument_count(
                typ, arg_types, arg_kinds, arg_names, formal_to_actual, None
            ):
                if args_have_var_arg and typ.is_var_arg:
                    star_matches.append(typ)
                elif args_have_kw_arg and typ.is_kw_arg:
                    star_matches.append(typ)
                else:
                    matches.append(typ)

    return star_matches + matches

</t>
<t tx="ekr.20230831011819.533">def infer_overload_return_type(
    self,
    plausible_targets: list[CallableType],
    args: list[Expression],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    callable_name: str | None,
    object_type: Type | None,
    context: Context,
) -&gt; tuple[Type, Type] | None:
    """Attempts to find the first matching callable from the given list.

    If a match is found, returns a tuple containing the result type and the inferred
    callee type. (This tuple is meant to be eventually returned by check_call.)
    If multiple targets match due to ambiguous Any parameters, returns (AnyType, AnyType).
    If no targets match, returns None.

    Assumes all of the given targets have argument counts compatible with the caller.
    """

    matches: list[CallableType] = []
    return_types: list[Type] = []
    inferred_types: list[Type] = []
    args_contain_any = any(map(has_any_type, arg_types))
    type_maps: list[dict[Expression, Type]] = []

    for typ in plausible_targets:
        assert self.msg is self.chk.msg
        with self.msg.filter_errors() as w:
            with self.chk.local_type_map() as m:
                ret_type, infer_type = self.check_call(
                    callee=typ,
                    args=args,
                    arg_kinds=arg_kinds,
                    arg_names=arg_names,
                    context=context,
                    callable_name=callable_name,
                    object_type=object_type,
                )
        is_match = not w.has_new_errors()
        if is_match:
            # Return early if possible; otherwise record info so we can
            # check for ambiguity due to 'Any' below.
            if not args_contain_any:
                return ret_type, infer_type
            matches.append(typ)
            return_types.append(ret_type)
            inferred_types.append(infer_type)
            type_maps.append(m)

    if not matches:
        return None
    elif any_causes_overload_ambiguity(matches, return_types, arg_types, arg_kinds, arg_names):
        # An argument of type or containing the type 'Any' caused ambiguity.
        # We try returning a precise type if we can. If not, we give up and just return 'Any'.
        if all_same_types(return_types):
            self.chk.store_types(type_maps[0])
            return return_types[0], inferred_types[0]
        elif all_same_types([erase_type(typ) for typ in return_types]):
            self.chk.store_types(type_maps[0])
            return erase_type(return_types[0]), erase_type(inferred_types[0])
        else:
            return self.check_call(
                callee=AnyType(TypeOfAny.special_form),
                args=args,
                arg_kinds=arg_kinds,
                arg_names=arg_names,
                context=context,
                callable_name=callable_name,
                object_type=object_type,
            )
    else:
        # Success! No ambiguity; return the first match.
        self.chk.store_types(type_maps[0])
        return return_types[0], inferred_types[0]

</t>
<t tx="ekr.20230831011819.534">def overload_erased_call_targets(
    self,
    plausible_targets: list[CallableType],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    args: list[Expression],
    context: Context,
) -&gt; list[CallableType]:
    """Returns a list of all targets that match the caller after erasing types.

    Assumes all of the given targets have argument counts compatible with the caller.
    """
    matches: list[CallableType] = []
    for typ in plausible_targets:
        if self.erased_signature_similarity(
            arg_types, arg_kinds, arg_names, args, typ, context
        ):
            matches.append(typ)
    return matches

</t>
<t tx="ekr.20230831011819.535">def possible_none_type_var_overlap(
    self, arg_types: list[Type], plausible_targets: list[CallableType]
) -&gt; bool:
    """Heuristic to determine whether we need to try forcing union math.

    This is needed to avoid greedy type variable match in situations like this:
        @overload
        def foo(x: None) -&gt; None: ...
        @overload
        def foo(x: T) -&gt; list[T]: ...

        x: int | None
        foo(x)
    we want this call to infer list[int] | None, not list[int | None].
    """
    if not plausible_targets or not arg_types:
        return False
    has_optional_arg = False
    for arg_type in get_proper_types(arg_types):
        if not isinstance(arg_type, UnionType):
            continue
        for item in get_proper_types(arg_type.items):
            if isinstance(item, NoneType):
                has_optional_arg = True
                break
    if not has_optional_arg:
        return False

    min_prefix = min(len(c.arg_types) for c in plausible_targets)
    for i in range(min_prefix):
        if any(
            isinstance(get_proper_type(c.arg_types[i]), NoneType) for c in plausible_targets
        ) and any(
            isinstance(get_proper_type(c.arg_types[i]), TypeVarType) for c in plausible_targets
        ):
            return True
    return False

</t>
<t tx="ekr.20230831011819.536">def union_overload_result(
    self,
    plausible_targets: list[CallableType],
    args: list[Expression],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    callable_name: str | None,
    object_type: Type | None,
    none_type_var_overlap: bool,
    context: Context,
    level: int = 0,
) -&gt; list[tuple[Type, Type]] | None:
    """Accepts a list of overload signatures and attempts to match calls by destructuring
    the first union.

    Return a list of (&lt;return type&gt;, &lt;inferred variant type&gt;) if call succeeds for every
    item of the desctructured union. Returns None if there is no match.
    """
    # Step 1: If we are already too deep, then stop immediately. Otherwise mypy might
    # hang for long time because of a weird overload call. The caller will get
    # the exception and generate an appropriate note message, if needed.
    if level &gt;= MAX_UNIONS:
        raise TooManyUnions

    # Step 2: Find position of the first union in arguments. Return the normal inferred
    # type if no more unions left.
    for idx, typ in enumerate(arg_types):
        if self.real_union(typ):
            break
    else:
        # No unions in args, just fall back to normal inference
        with self.type_overrides_set(args, arg_types):
            res = self.infer_overload_return_type(
                plausible_targets,
                args,
                arg_types,
                arg_kinds,
                arg_names,
                callable_name,
                object_type,
                context,
            )
        if res is not None:
            return [res]
        return None

    # Step 3: Try a direct match before splitting to avoid unnecessary union splits
    # and save performance.
    if not none_type_var_overlap:
        with self.type_overrides_set(args, arg_types):
            direct = self.infer_overload_return_type(
                plausible_targets,
                args,
                arg_types,
                arg_kinds,
                arg_names,
                callable_name,
                object_type,
                context,
            )
        if direct is not None and not isinstance(
            get_proper_type(direct[0]), (UnionType, AnyType)
        ):
            # We only return non-unions soon, to avoid greedy match.
            return [direct]

    # Step 4: Split the first remaining union type in arguments into items and
    # try to match each item individually (recursive).
    first_union = get_proper_type(arg_types[idx])
    assert isinstance(first_union, UnionType)
    res_items = []
    for item in first_union.relevant_items():
        new_arg_types = arg_types.copy()
        new_arg_types[idx] = item
        sub_result = self.union_overload_result(
            plausible_targets,
            args,
            new_arg_types,
            arg_kinds,
            arg_names,
            callable_name,
            object_type,
            none_type_var_overlap,
            context,
            level + 1,
        )
        if sub_result is not None:
            res_items.extend(sub_result)
        else:
            # Some item doesn't match, return soon.
            return None

    # Step 5: If splitting succeeded, then filter out duplicate items before returning.
    seen: set[tuple[Type, Type]] = set()
    result = []
    for pair in res_items:
        if pair not in seen:
            seen.add(pair)
            result.append(pair)
    return result

</t>
<t tx="ekr.20230831011819.537">def real_union(self, typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, UnionType) and len(typ.relevant_items()) &gt; 1

</t>
<t tx="ekr.20230831011819.538">@contextmanager
def type_overrides_set(
    self, exprs: Sequence[Expression], overrides: Sequence[Type]
) -&gt; Iterator[None]:
    """Set _temporary_ type overrides for given expressions."""
    assert len(exprs) == len(overrides)
    for expr, typ in zip(exprs, overrides):
        self.type_overrides[expr] = typ
    try:
        yield
    finally:
        for expr in exprs:
            del self.type_overrides[expr]

</t>
<t tx="ekr.20230831011819.539">def combine_function_signatures(self, types: list[ProperType]) -&gt; AnyType | CallableType:
    """Accepts a list of function signatures and attempts to combine them together into a
    new CallableType consisting of the union of all of the given arguments and return types.

    If there is at least one non-callable type, return Any (this can happen if there is
    an ambiguity because of Any in arguments).
    """
    assert types, "Trying to merge no callables"
    if not all(isinstance(c, CallableType) for c in types):
        return AnyType(TypeOfAny.special_form)
    callables = cast("list[CallableType]", types)
    if len(callables) == 1:
        return callables[0]

    # Note: we are assuming here that if a user uses some TypeVar 'T' in
    # two different functions, they meant for that TypeVar to mean the
    # same thing.
    #
    # This function will make sure that all instances of that TypeVar 'T'
    # refer to the same underlying TypeVarType objects to simplify the union-ing
    # logic below.
    #
    # (If the user did *not* mean for 'T' to be consistently bound to the
    # same type in their overloads, well, their code is probably too
    # confusing and ought to be re-written anyways.)
    callables, variables = merge_typevars_in_callables_by_name(callables)

    new_args: list[list[Type]] = [[] for _ in range(len(callables[0].arg_types))]
    new_kinds = list(callables[0].arg_kinds)
    new_returns: list[Type] = []

    too_complex = False
    for target in callables:
        # We fall back to Callable[..., Union[&lt;returns&gt;]] if the functions do not have
        # the exact same signature. The only exception is if one arg is optional and
        # the other is positional: in that case, we continue unioning (and expect a
        # positional arg).
        # TODO: Enhance the merging logic to handle a wider variety of signatures.
        if len(new_kinds) != len(target.arg_kinds):
            too_complex = True
            break
        for i, (new_kind, target_kind) in enumerate(zip(new_kinds, target.arg_kinds)):
            if new_kind == target_kind:
                continue
            elif new_kind.is_positional() and target_kind.is_positional():
                new_kinds[i] = ARG_POS
            else:
                too_complex = True
                break

        if too_complex:
            break  # outer loop

        for i, arg in enumerate(target.arg_types):
            new_args[i].append(arg)
        new_returns.append(target.ret_type)

    union_return = make_simplified_union(new_returns)
    if too_complex:
        any = AnyType(TypeOfAny.special_form)
        return callables[0].copy_modified(
            arg_types=[any, any],
            arg_kinds=[ARG_STAR, ARG_STAR2],
            arg_names=[None, None],
            ret_type=union_return,
            variables=variables,
            implicit=True,
        )

    final_args = []
    for args_list in new_args:
        new_type = make_simplified_union(args_list)
        final_args.append(new_type)

    return callables[0].copy_modified(
        arg_types=final_args,
        arg_kinds=new_kinds,
        ret_type=union_return,
        variables=variables,
        implicit=True,
    )

</t>
<t tx="ekr.20230831011819.54">@contextmanager
def top_frame_context(self) -&gt; Iterator[Frame]:
    """A variant of frame_context for use at the top level of
    a namespace (module, function, or class).
    """
    assert len(self.frames) == 1
    yield self.push_frame()
    self.pop_frame(True, 0)
    assert len(self.frames) == 1


</t>
<t tx="ekr.20230831011819.540">def erased_signature_similarity(
    self,
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    args: list[Expression],
    callee: CallableType,
    context: Context,
) -&gt; bool:
    """Determine whether arguments could match the signature at runtime, after
    erasing types."""
    formal_to_actual = map_actuals_to_formals(
        arg_kinds, arg_names, callee.arg_kinds, callee.arg_names, lambda i: arg_types[i]
    )

    with self.msg.filter_errors():
        if not self.check_argument_count(
            callee, arg_types, arg_kinds, arg_names, formal_to_actual, None
        ):
            # Too few or many arguments -&gt; no match.
            return False

    def check_arg(
        caller_type: Type,
        original_ccaller_type: Type,
        caller_kind: ArgKind,
        callee_type: Type,
        n: int,
        m: int,
        callee: CallableType,
        object_type: Type | None,
        context: Context,
        outer_context: Context,
    ) -&gt; None:
        if not arg_approximate_similarity(caller_type, callee_type):
            # No match -- exit early since none of the remaining work can change
            # the result.
            raise Finished

    try:
        self.check_argument_types(
            arg_types,
            arg_kinds,
            args,
            callee,
            formal_to_actual,
            context=context,
            check_arg=check_arg,
        )
        return True
    except Finished:
        return False

</t>
<t tx="ekr.20230831011819.541">def apply_generic_arguments(
    self,
    callable: CallableType,
    types: Sequence[Type | None],
    context: Context,
    skip_unsatisfied: bool = False,
) -&gt; CallableType:
    """Simple wrapper around mypy.applytype.apply_generic_arguments."""
    return applytype.apply_generic_arguments(
        callable,
        types,
        self.msg.incompatible_typevar_value,
        context,
        skip_unsatisfied=skip_unsatisfied,
    )

</t>
<t tx="ekr.20230831011819.542">def check_any_type_call(self, args: list[Expression], callee: Type) -&gt; tuple[Type, Type]:
    self.infer_arg_types_in_empty_context(args)
    callee = get_proper_type(callee)
    if isinstance(callee, AnyType):
        return (
            AnyType(TypeOfAny.from_another_any, source_any=callee),
            AnyType(TypeOfAny.from_another_any, source_any=callee),
        )
    else:
        return AnyType(TypeOfAny.special_form), AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.543">def check_union_call(
    self,
    callee: UnionType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    context: Context,
) -&gt; tuple[Type, Type]:
    with self.msg.disable_type_names():
        results = [
            self.check_call(subtype, args, arg_kinds, context, arg_names)
            for subtype in callee.relevant_items()
        ]

    return (make_simplified_union([res[0] for res in results]), callee)

</t>
<t tx="ekr.20230831011819.544">def visit_member_expr(self, e: MemberExpr, is_lvalue: bool = False) -&gt; Type:
    """Visit member expression (of form e.id)."""
    self.chk.module_refs.update(extract_refexpr_names(e))
    result = self.analyze_ordinary_member_access(e, is_lvalue)
    return self.narrow_type_from_binder(e, result)

</t>
<t tx="ekr.20230831011819.545">def analyze_ordinary_member_access(self, e: MemberExpr, is_lvalue: bool) -&gt; Type:
    """Analyse member expression or member lvalue."""
    if e.kind is not None:
        # This is a reference to a module attribute.
        return self.analyze_ref_expr(e)
    else:
        # This is a reference to a non-module attribute.
        original_type = self.accept(e.expr, is_callee=self.is_callee)
        base = e.expr
        module_symbol_table = None

        if isinstance(base, RefExpr) and isinstance(base.node, MypyFile):
            module_symbol_table = base.node.names
        if isinstance(base, RefExpr) and isinstance(base.node, Var):
            is_self = base.node.is_self
        else:
            is_self = False

        member_type = analyze_member_access(
            e.name,
            original_type,
            e,
            is_lvalue,
            False,
            False,
            self.msg,
            original_type=original_type,
            chk=self.chk,
            in_literal_context=self.is_literal_context(),
            module_symbol_table=module_symbol_table,
            is_self=is_self,
        )

        return member_type

</t>
<t tx="ekr.20230831011819.546">def analyze_external_member_access(
    self, member: str, base_type: Type, context: Context
) -&gt; Type:
    """Analyse member access that is external, i.e. it cannot
    refer to private definitions. Return the result type.
    """
    # TODO remove; no private definitions in mypy
    return analyze_member_access(
        member,
        base_type,
        context,
        False,
        False,
        False,
        self.msg,
        original_type=base_type,
        chk=self.chk,
        in_literal_context=self.is_literal_context(),
    )

</t>
<t tx="ekr.20230831011819.547">def is_literal_context(self) -&gt; bool:
    return is_literal_type_like(self.type_context[-1])

</t>
<t tx="ekr.20230831011819.548">def infer_literal_expr_type(self, value: LiteralValue, fallback_name: str) -&gt; Type:
    """Analyzes the given literal expression and determines if we should be
    inferring an Instance type, a Literal[...] type, or an Instance that
    remembers the original literal. We...

    1. ...Infer a normal Instance in most circumstances.

    2. ...Infer a Literal[...] if we're in a literal context. For example, if we
       were analyzing the "3" in "foo(3)" where "foo" has a signature of
       "def foo(Literal[3]) -&gt; None", we'd want to infer that the "3" has a
       type of Literal[3] instead of Instance.

    3. ...Infer an Instance that remembers the original Literal if we're declaring
       a Final variable with an inferred type -- for example, "bar" in "bar: Final = 3"
       would be assigned an Instance that remembers it originated from a '3'. See
       the comments in Instance's constructor for more details.
    """
    typ = self.named_type(fallback_name)
    if self.is_literal_context():
        return LiteralType(value=value, fallback=typ)
    else:
        return typ.copy_modified(
            last_known_value=LiteralType(
                value=value, fallback=typ, line=typ.line, column=typ.column
            )
        )

</t>
<t tx="ekr.20230831011819.549">def concat_tuples(self, left: TupleType, right: TupleType) -&gt; TupleType:
    """Concatenate two fixed length tuples."""
    return TupleType(
        items=left.items + right.items, fallback=self.named_type("builtins.tuple")
    )

</t>
<t tx="ekr.20230831011819.55">def get_declaration(expr: BindableExpression) -&gt; Type | None:
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, Var):
            type = expr.node.type
            if not isinstance(get_proper_type(type), PartialType):
                return type
        elif isinstance(expr.node, TypeInfo):
            return TypeType(fill_typevars_with_any(expr.node))
    return None
</t>
<t tx="ekr.20230831011819.550">def visit_int_expr(self, e: IntExpr) -&gt; Type:
    """Type check an integer literal (trivial)."""
    return self.infer_literal_expr_type(e.value, "builtins.int")

</t>
<t tx="ekr.20230831011819.551">def visit_str_expr(self, e: StrExpr) -&gt; Type:
    """Type check a string literal (trivial)."""
    return self.infer_literal_expr_type(e.value, "builtins.str")

</t>
<t tx="ekr.20230831011819.552">def visit_bytes_expr(self, e: BytesExpr) -&gt; Type:
    """Type check a bytes literal (trivial)."""
    return self.infer_literal_expr_type(e.value, "builtins.bytes")

</t>
<t tx="ekr.20230831011819.553">def visit_float_expr(self, e: FloatExpr) -&gt; Type:
    """Type check a float literal (trivial)."""
    return self.named_type("builtins.float")

</t>
<t tx="ekr.20230831011819.554">def visit_complex_expr(self, e: ComplexExpr) -&gt; Type:
    """Type check a complex literal."""
    return self.named_type("builtins.complex")

</t>
<t tx="ekr.20230831011819.555">def visit_ellipsis(self, e: EllipsisExpr) -&gt; Type:
    """Type check '...'."""
    return self.named_type("builtins.ellipsis")

</t>
<t tx="ekr.20230831011819.556">def visit_op_expr(self, e: OpExpr) -&gt; Type:
    """Type check a binary operator expression."""
    if e.analyzed:
        # It's actually a type expression X | Y.
        return self.accept(e.analyzed)
    if e.op == "and" or e.op == "or":
        return self.check_boolean_op(e, e)
    if e.op == "*" and isinstance(e.left, ListExpr):
        # Expressions of form [...] * e get special type inference.
        return self.check_list_multiply(e)
    if e.op == "%":
        if isinstance(e.left, BytesExpr):
            return self.strfrm_checker.check_str_interpolation(e.left, e.right)
        if isinstance(e.left, StrExpr):
            return self.strfrm_checker.check_str_interpolation(e.left, e.right)
    left_type = self.accept(e.left)

    proper_left_type = get_proper_type(left_type)
    if isinstance(proper_left_type, TupleType) and e.op == "+":
        left_add_method = proper_left_type.partial_fallback.type.get("__add__")
        if left_add_method and left_add_method.fullname == "builtins.tuple.__add__":
            proper_right_type = get_proper_type(self.accept(e.right))
            if isinstance(proper_right_type, TupleType):
                right_radd_method = proper_right_type.partial_fallback.type.get("__radd__")
                if right_radd_method is None:
                    return self.concat_tuples(proper_left_type, proper_right_type)

    if e.op in operators.op_methods:
        method = operators.op_methods[e.op]
        result, method_type = self.check_op(method, left_type, e.right, e, allow_reverse=True)
        e.method_type = method_type
        return result
    else:
        raise RuntimeError(f"Unknown operator {e.op}")

</t>
<t tx="ekr.20230831011819.557">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; Type:
    """Type check a comparison expression.

    Comparison expressions are type checked consecutive-pair-wise
    That is, 'a &lt; b &gt; c == d' is check as 'a &lt; b and b &gt; c and c == d'
    """
    result: Type | None = None
    sub_result: Type

    # Check each consecutive operand pair and their operator
    for left, right, operator in zip(e.operands, e.operands[1:], e.operators):
        left_type = self.accept(left)

        if operator == "in" or operator == "not in":
            # This case covers both iterables and containers, which have different meanings.
            # For a container, the in operator calls the __contains__ method.
            # For an iterable, the in operator iterates over the iterable, and compares each item one-by-one.
            # We allow `in` for a union of containers and iterables as long as at least one of them matches the
            # type of the left operand, as the operation will simply return False if the union's container/iterator
            # type doesn't match the left operand.

            # If the right operand has partial type, look it up without triggering
            # a "Need type annotation ..." message, as it would be noise.
            right_type = self.find_partial_type_ref_fast_path(right)
            if right_type is None:
                right_type = self.accept(right)  # Validate the right operand

            right_type = get_proper_type(right_type)
            item_types: Sequence[Type] = [right_type]
            if isinstance(right_type, UnionType):
                item_types = list(right_type.relevant_items())

            sub_result = self.bool_type()

            container_types: list[Type] = []
            iterable_types: list[Type] = []
            failed_out = False
            encountered_partial_type = False

            for item_type in item_types:
                # Keep track of whether we get type check errors (these won't be reported, they
                # are just to verify whether something is valid typing wise).
                with self.msg.filter_errors(save_filtered_errors=True) as container_errors:
                    _, method_type = self.check_method_call_by_name(
                        method="__contains__",
                        base_type=item_type,
                        args=[left],
                        arg_kinds=[ARG_POS],
                        context=e,
                        original_type=right_type,
                    )
                    # Container item type for strict type overlap checks. Note: we need to only
                    # check for nominal type, because a usual "Unsupported operands for in"
                    # will be reported for types incompatible with __contains__().
                    # See testCustomContainsCheckStrictEquality for an example.
                    cont_type = self.chk.analyze_container_item_type(item_type)

                if isinstance(item_type, PartialType):
                    # We don't really know if this is an error or not, so just shut up.
                    encountered_partial_type = True
                    pass
                elif (
                    container_errors.has_new_errors()
                    and
                    # is_valid_var_arg is True for any Iterable
                    self.is_valid_var_arg(item_type)
                ):
                    # it's not a container, but it is an iterable
                    with self.msg.filter_errors(save_filtered_errors=True) as iterable_errors:
                        _, itertype = self.chk.analyze_iterable_item_type_without_expression(
                            item_type, e
                        )
                    if iterable_errors.has_new_errors():
                        self.msg.add_errors(iterable_errors.filtered_errors())
                        failed_out = True
                    else:
                        method_type = CallableType(
                            [left_type],
                            [nodes.ARG_POS],
                            [None],
                            self.bool_type(),
                            self.named_type("builtins.function"),
                        )
                        e.method_types.append(method_type)
                        iterable_types.append(itertype)
                elif not container_errors.has_new_errors() and cont_type:
                    container_types.append(cont_type)
                    e.method_types.append(method_type)
                else:
                    self.msg.add_errors(container_errors.filtered_errors())
                    failed_out = True

            if not encountered_partial_type and not failed_out:
                iterable_type = UnionType.make_union(iterable_types)
                if not is_subtype(left_type, iterable_type):
                    if not container_types:
                        self.msg.unsupported_operand_types("in", left_type, right_type, e)
                    else:
                        container_type = UnionType.make_union(container_types)
                        if self.dangerous_comparison(
                            left_type,
                            container_type,
                            original_container=right_type,
                            prefer_literal=False,
                        ):
                            self.msg.dangerous_comparison(
                                left_type, container_type, "container", e
                            )

        elif operator in operators.op_methods:
            method = operators.op_methods[operator]

            with ErrorWatcher(self.msg.errors) as w:
                sub_result, method_type = self.check_op(
                    method, left_type, right, e, allow_reverse=True
                )
                e.method_types.append(method_type)

            # Only show dangerous overlap if there are no other errors. See
            # testCustomEqCheckStrictEquality for an example.
            if not w.has_new_errors() and operator in ("==", "!="):
                right_type = self.accept(right)
                if self.dangerous_comparison(left_type, right_type):
                    # Show the most specific literal types possible
                    left_type = try_getting_literal(left_type)
                    right_type = try_getting_literal(right_type)
                    self.msg.dangerous_comparison(left_type, right_type, "equality", e)

        elif operator == "is" or operator == "is not":
            right_type = self.accept(right)  # validate the right operand
            sub_result = self.bool_type()
            if self.dangerous_comparison(left_type, right_type):
                # Show the most specific literal types possible
                left_type = try_getting_literal(left_type)
                right_type = try_getting_literal(right_type)
                self.msg.dangerous_comparison(left_type, right_type, "identity", e)
            e.method_types.append(None)
        else:
            raise RuntimeError(f"Unknown comparison operator {operator}")

        #  Determine type of boolean-and of result and sub_result
        if result is None:
            result = sub_result
        else:
            result = join.join_types(result, sub_result)

    assert result is not None
    return result

</t>
<t tx="ekr.20230831011819.558">def find_partial_type_ref_fast_path(self, expr: Expression) -&gt; Type | None:
    """If expression has a partial generic type, return it without additional checks.

    In particular, this does not generate an error about a missing annotation.

    Otherwise, return None.
    """
    if not isinstance(expr, RefExpr):
        return None
    if isinstance(expr.node, Var):
        result = self.analyze_var_ref(expr.node, expr)
        if isinstance(result, PartialType) and result.type is not None:
            self.chk.store_type(expr, fixup_partial_type(result))
            return result
    return None

</t>
<t tx="ekr.20230831011819.559">def dangerous_comparison(
    self,
    left: Type,
    right: Type,
    original_container: Type | None = None,
    *,
    prefer_literal: bool = True,
) -&gt; bool:
    """Check for dangerous non-overlapping comparisons like 42 == 'no'.

    The original_container is the original container type for 'in' checks
    (and None for equality checks).

    Rules:
        * X and None are overlapping even in strict-optional mode. This is to allow
        'assert x is not None' for x defined as 'x = None  # type: str' in class body
        (otherwise mypy itself would have couple dozen errors because of this).
        * Optional[X] and Optional[Y] are non-overlapping if X and Y are
        non-overlapping, although technically None is overlap, it is most
        likely an error.
        * Any overlaps with everything, i.e. always safe.
        * Special case: b'abc' in b'cde' is safe.
    """
    if not self.chk.options.strict_equality:
        return False

    left, right = get_proper_types((left, right))

    # We suppress the error if there is a custom __eq__() method on either
    # side. User defined (or even standard library) classes can define this
    # to return True for comparisons between non-overlapping types.
    if custom_special_method(left, "__eq__") or custom_special_method(right, "__eq__"):
        return False

    if prefer_literal:
        # Also flag non-overlapping literals in situations like:
        #    x: Literal['a', 'b']
        #    if x == 'c':
        #        ...
        left = try_getting_literal(left)
        right = try_getting_literal(right)

    if self.chk.binder.is_unreachable_warning_suppressed():
        # We are inside a function that contains type variables with value restrictions in
        # its signature. In this case we just suppress all strict-equality checks to avoid
        # false positives for code like:
        #
        #     T = TypeVar('T', str, int)
        #     def f(x: T) -&gt; T:
        #         if x == 0:
        #             ...
        #         return x
        #
        # TODO: find a way of disabling the check only for types resulted from the expansion.
        return False
    if isinstance(left, NoneType) or isinstance(right, NoneType):
        return False
    if isinstance(left, UnionType) and isinstance(right, UnionType):
        left = remove_optional(left)
        right = remove_optional(right)
        left, right = get_proper_types((left, right))
    if (
        original_container
        and has_bytes_component(original_container)
        and has_bytes_component(left)
    ):
        # We need to special case bytes and bytearray, because 97 in b'abc', b'a' in b'abc',
        # b'a' in bytearray(b'abc') etc. all return True (and we want to show the error only
        # if the check can _never_ be True).
        return False
    if isinstance(left, Instance) and isinstance(right, Instance):
        # Special case some builtin implementations of AbstractSet.
        left_name = left.type.fullname
        right_name = right.type.fullname
        if (
            left_name in OVERLAPPING_TYPES_ALLOWLIST
            and right_name in OVERLAPPING_TYPES_ALLOWLIST
        ):
            abstract_set = self.chk.lookup_typeinfo("typing.AbstractSet")
            left = map_instance_to_supertype(left, abstract_set)
            right = map_instance_to_supertype(right, abstract_set)
            return self.dangerous_comparison(left.args[0], right.args[0])
        elif left_name in ("builtins.list", "builtins.tuple") and right_name == left_name:
            return self.dangerous_comparison(left.args[0], right.args[0])
        elif left_name in OVERLAPPING_BYTES_ALLOWLIST and right_name in (
            OVERLAPPING_BYTES_ALLOWLIST
        ):
            return False
    if isinstance(left, LiteralType) and isinstance(right, LiteralType):
        if isinstance(left.value, bool) and isinstance(right.value, bool):
            # Comparing different booleans is not dangerous.
            return False
    return not is_overlapping_types(left, right, ignore_promotions=False)

</t>
<t tx="ekr.20230831011819.56">@path mypy
"""A Bogus[T] type alias for marking when we subvert the type system

We need this for compiling with mypyc, which inserts runtime
typechecks that cause problems when we subvert the type system. So
when compiling with mypyc, we turn those places into Any, while
keeping the types around for normal typechecks.

Since this causes the runtime types to be Any, this is best used
in places where efficient access to properties is not important.
For those cases some other technique should be used.
"""

from __future__ import annotations

from typing import Any, TypeVar

from mypy_extensions import FlexibleAlias

T = TypeVar("T")

# This won't ever be true at runtime, but we consider it true during
# mypyc compilations.
MYPYC = False
if MYPYC:
    Bogus = FlexibleAlias[T, Any]
else:
    Bogus = FlexibleAlias[T, T]
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.560">def check_method_call_by_name(
    self,
    method: str,
    base_type: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    original_type: Type | None = None,
) -&gt; tuple[Type, Type]:
    """Type check a call to a named method on an object.

    Return tuple (result type, inferred method type). The 'original_type'
    is used for error messages.
    """
    original_type = original_type or base_type
    # Unions are special-cased to allow plugins to act on each element of the union.
    base_type = get_proper_type(base_type)
    if isinstance(base_type, UnionType):
        return self.check_union_method_call_by_name(
            method, base_type, args, arg_kinds, context, original_type
        )

    method_type = analyze_member_access(
        method,
        base_type,
        context,
        False,
        False,
        True,
        self.msg,
        original_type=original_type,
        chk=self.chk,
        in_literal_context=self.is_literal_context(),
    )
    return self.check_method_call(method, base_type, method_type, args, arg_kinds, context)

</t>
<t tx="ekr.20230831011819.561">def check_union_method_call_by_name(
    self,
    method: str,
    base_type: UnionType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    original_type: Type | None = None,
) -&gt; tuple[Type, Type]:
    """Type check a call to a named method on an object with union type.

    This essentially checks the call using check_method_call_by_name() for each
    union item and unions the result. We do this to allow plugins to act on
    individual union items.
    """
    res: list[Type] = []
    meth_res: list[Type] = []
    for typ in base_type.relevant_items():
        # Format error messages consistently with
        # mypy.checkmember.analyze_union_member_access().
        with self.msg.disable_type_names():
            item, meth_item = self.check_method_call_by_name(
                method, typ, args, arg_kinds, context, original_type
            )
        res.append(item)
        meth_res.append(meth_item)
    return make_simplified_union(res), make_simplified_union(meth_res)

</t>
<t tx="ekr.20230831011819.562">def check_method_call(
    self,
    method_name: str,
    base_type: Type,
    method_type: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
) -&gt; tuple[Type, Type]:
    """Type check a call to a method with the given name and type on an object.

    Return tuple (result type, inferred method type).
    """
    callable_name = self.method_fullname(base_type, method_name)
    object_type = base_type if callable_name is not None else None

    # Try to refine the method signature using plugin hooks before checking the call.
    method_type = self.transform_callee_type(
        callable_name, method_type, args, arg_kinds, context, object_type=object_type
    )

    return self.check_call(
        method_type,
        args,
        arg_kinds,
        context,
        callable_name=callable_name,
        object_type=base_type,
    )

</t>
<t tx="ekr.20230831011819.563">def check_op_reversible(
    self,
    op_name: str,
    left_type: Type,
    left_expr: Expression,
    right_type: Type,
    right_expr: Expression,
    context: Context,
) -&gt; tuple[Type, Type]:
    def lookup_operator(op_name: str, base_type: Type) -&gt; Type | None:
        """Looks up the given operator and returns the corresponding type,
        if it exists."""

        # This check is an important performance optimization,
        # even though it is mostly a subset of
        # analyze_member_access.
        # TODO: Find a way to remove this call without performance implications.
        if not self.has_member(base_type, op_name):
            return None

        with self.msg.filter_errors() as w:
            member = analyze_member_access(
                name=op_name,
                typ=base_type,
                is_lvalue=False,
                is_super=False,
                is_operator=True,
                original_type=base_type,
                context=context,
                msg=self.msg,
                chk=self.chk,
                in_literal_context=self.is_literal_context(),
            )
            return None if w.has_new_errors() else member

    def lookup_definer(typ: Instance, attr_name: str) -&gt; str | None:
        """Returns the name of the class that contains the actual definition of attr_name.

        So if class A defines foo and class B subclasses A, running
        'get_class_defined_in(B, "foo")` would return the full name of A.

        However, if B were to override and redefine foo, that method call would
        return the full name of B instead.

        If the attr name is not present in the given class or its MRO, returns None.
        """
        for cls in typ.type.mro:
            if cls.names.get(attr_name):
                return cls.fullname
        return None

    left_type = get_proper_type(left_type)
    right_type = get_proper_type(right_type)

    # If either the LHS or the RHS are Any, we can't really concluding anything
    # about the operation since the Any type may or may not define an
    # __op__ or __rop__ method. So, we punt and return Any instead.

    if isinstance(left_type, AnyType):
        any_type = AnyType(TypeOfAny.from_another_any, source_any=left_type)
        return any_type, any_type
    if isinstance(right_type, AnyType):
        any_type = AnyType(TypeOfAny.from_another_any, source_any=right_type)
        return any_type, any_type

    # STEP 1:
    # We start by getting the __op__ and __rop__ methods, if they exist.

    rev_op_name = operators.reverse_op_methods[op_name]

    left_op = lookup_operator(op_name, left_type)
    right_op = lookup_operator(rev_op_name, right_type)

    # STEP 2a:
    # We figure out in which order Python will call the operator methods. As it
    # turns out, it's not as simple as just trying to call __op__ first and
    # __rop__ second.
    #
    # We store the determined order inside the 'variants_raw' variable,
    # which records tuples containing the method, base type, and the argument.

    if op_name in operators.op_methods_that_shortcut and is_same_type(left_type, right_type):
        # When we do "A() + A()", for example, Python will only call the __add__ method,
        # never the __radd__ method.
        #
        # This is the case even if the __add__ method is completely missing and the __radd__
        # method is defined.

        variants_raw = [(left_op, left_type, right_expr)]
    elif (
        is_subtype(right_type, left_type)
        and isinstance(left_type, Instance)
        and isinstance(right_type, Instance)
        and not (
            left_type.type.alt_promote is not None
            and left_type.type.alt_promote.type is right_type.type
        )
        and lookup_definer(left_type, op_name) != lookup_definer(right_type, rev_op_name)
    ):
        # When we do "A() + B()" where B is a subclass of A, we'll actually try calling
        # B's __radd__ method first, but ONLY if B explicitly defines or overrides the
        # __radd__ method.
        #
        # This mechanism lets subclasses "refine" the expected outcome of the operation, even
        # if they're located on the RHS.
        #
        # As a special case, the alt_promote check makes sure that we don't use the
        # __radd__ method of int if the LHS is a native int type.

        variants_raw = [(right_op, right_type, left_expr), (left_op, left_type, right_expr)]
    else:
        # In all other cases, we do the usual thing and call __add__ first and
        # __radd__ second when doing "A() + B()".

        variants_raw = [(left_op, left_type, right_expr), (right_op, right_type, left_expr)]

    # STEP 3:
    # We now filter out all non-existent operators. The 'variants' list contains
    # all operator methods that are actually present, in the order that Python
    # attempts to invoke them.

    variants = [(op, obj, arg) for (op, obj, arg) in variants_raw if op is not None]

    # STEP 4:
    # We now try invoking each one. If an operation succeeds, end early and return
    # the corresponding result. Otherwise, return the result and errors associated
    # with the first entry.

    errors = []
    results = []
    for method, obj, arg in variants:
        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            result = self.check_method_call(op_name, obj, method, [arg], [ARG_POS], context)
        if local_errors.has_new_errors():
            errors.append(local_errors.filtered_errors())
            results.append(result)
        else:
            return result

    # We finish invoking above operators and no early return happens. Therefore,
    # we check if either the LHS or the RHS is Instance and fallbacks to Any,
    # if so, we also return Any
    if (isinstance(left_type, Instance) and left_type.type.fallback_to_any) or (
        isinstance(right_type, Instance) and right_type.type.fallback_to_any
    ):
        any_type = AnyType(TypeOfAny.special_form)
        return any_type, any_type

    # STEP 4b:
    # Sometimes, the variants list is empty. In that case, we fall-back to attempting to
    # call the __op__ method (even though it's missing).

    if not variants:
        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            result = self.check_method_call_by_name(
                op_name, left_type, [right_expr], [ARG_POS], context
            )

        if local_errors.has_new_errors():
            errors.append(local_errors.filtered_errors())
            results.append(result)
        else:
            # In theory, we should never enter this case, but it seems
            # we sometimes do, when dealing with Type[...]? E.g. see
            # check-classes.testTypeTypeComparisonWorks.
            #
            # This is probably related to the TODO in lookup_operator(...)
            # up above.
            #
            # TODO: Remove this extra case
            return result

    self.msg.add_errors(errors[0])
    if len(results) == 1:
        return results[0]
    else:
        error_any = AnyType(TypeOfAny.from_error)
        result = error_any, error_any
        return result

</t>
<t tx="ekr.20230831011819.564">def check_op(
    self,
    method: str,
    base_type: Type,
    arg: Expression,
    context: Context,
    allow_reverse: bool = False,
) -&gt; tuple[Type, Type]:
    """Type check a binary operation which maps to a method call.

    Return tuple (result type, inferred operator method type).
    """

    if allow_reverse:
        left_variants = [base_type]
        base_type = get_proper_type(base_type)
        if isinstance(base_type, UnionType):
            left_variants = [
                item for item in flatten_nested_unions(base_type.relevant_items())
            ]
        right_type = self.accept(arg)

        # Step 1: We first try leaving the right arguments alone and destructure
        # just the left ones. (Mypy can sometimes perform some more precise inference
        # if we leave the right operands a union -- see testOperatorWithEmptyListAndSum.)
        all_results = []
        all_inferred = []

        with self.msg.filter_errors() as local_errors:
            for left_possible_type in left_variants:
                result, inferred = self.check_op_reversible(
                    op_name=method,
                    left_type=left_possible_type,
                    left_expr=TempNode(left_possible_type, context=context),
                    right_type=right_type,
                    right_expr=arg,
                    context=context,
                )
                all_results.append(result)
                all_inferred.append(inferred)

        if not local_errors.has_new_errors():
            results_final = make_simplified_union(all_results)
            inferred_final = make_simplified_union(all_inferred)
            return results_final, inferred_final

        # Step 2: If that fails, we try again but also destructure the right argument.
        # This is also necessary to make certain edge cases work -- see
        # testOperatorDoubleUnionInterwovenUnionAdd, for example.

        # Note: We want to pass in the original 'arg' for 'left_expr' and 'right_expr'
        # whenever possible so that plugins and similar things can introspect on the original
        # node if possible.
        #
        # We don't do the same for the base expression because it could lead to weird
        # type inference errors -- e.g. see 'testOperatorDoubleUnionSum'.
        # TODO: Can we use `type_overrides_set()` here?
        right_variants = [(right_type, arg)]
        right_type = get_proper_type(right_type)
        if isinstance(right_type, UnionType):
            right_variants = [
                (item, TempNode(item, context=context))
                for item in flatten_nested_unions(right_type.relevant_items())
            ]

        all_results = []
        all_inferred = []

        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            for left_possible_type in left_variants:
                for right_possible_type, right_expr in right_variants:
                    result, inferred = self.check_op_reversible(
                        op_name=method,
                        left_type=left_possible_type,
                        left_expr=TempNode(left_possible_type, context=context),
                        right_type=right_possible_type,
                        right_expr=right_expr,
                        context=context,
                    )
                    all_results.append(result)
                    all_inferred.append(inferred)

        if local_errors.has_new_errors():
            self.msg.add_errors(local_errors.filtered_errors())
            # Point any notes to the same location as an existing message.
            err = local_errors.filtered_errors()[-1]
            recent_context = TempNode(NoneType())
            recent_context.line = err.line
            recent_context.column = err.column
            if len(left_variants) &gt;= 2 and len(right_variants) &gt;= 2:
                self.msg.warn_both_operands_are_from_unions(recent_context)
            elif len(left_variants) &gt;= 2:
                self.msg.warn_operand_was_from_union("Left", base_type, context=recent_context)
            elif len(right_variants) &gt;= 2:
                self.msg.warn_operand_was_from_union(
                    "Right", right_type, context=recent_context
                )

        # See the comment in 'check_overload_call' for more details on why
        # we call 'combine_function_signature' instead of just unioning the inferred
        # callable types.
        results_final = make_simplified_union(all_results)
        inferred_final = self.combine_function_signatures(get_proper_types(all_inferred))
        return results_final, inferred_final
    else:
        return self.check_method_call_by_name(
            method=method,
            base_type=base_type,
            args=[arg],
            arg_kinds=[ARG_POS],
            context=context,
        )

</t>
<t tx="ekr.20230831011819.565">def check_boolean_op(self, e: OpExpr, context: Context) -&gt; Type:
    """Type check a boolean operation ('and' or 'or')."""

    # A boolean operation can evaluate to either of the operands.

    # We use the current type context to guide the type inference of of
    # the left operand. We also use the left operand type to guide the type
    # inference of the right operand so that expressions such as
    # '[1] or []' are inferred correctly.
    ctx = self.type_context[-1]
    left_type = self.accept(e.left, ctx)
    expanded_left_type = try_expanding_sum_type_to_union(
        self.accept(e.left, ctx), "builtins.bool"
    )

    assert e.op in ("and", "or")  # Checked by visit_op_expr

    if e.right_always:
        left_map: mypy.checker.TypeMap = None
        right_map: mypy.checker.TypeMap = {}
    elif e.right_unreachable:
        left_map, right_map = {}, None
    elif e.op == "and":
        right_map, left_map = self.chk.find_isinstance_check(e.left)
    elif e.op == "or":
        left_map, right_map = self.chk.find_isinstance_check(e.left)

    # If left_map is None then we know mypy considers the left expression
    # to be redundant.
    if (
        codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes
        and left_map is None
        # don't report an error if it's intentional
        and not e.right_always
    ):
        self.msg.redundant_left_operand(e.op, e.left)

    if (
        self.chk.should_report_unreachable_issues()
        and right_map is None
        # don't report an error if it's intentional
        and not e.right_unreachable
    ):
        self.msg.unreachable_right_operand(e.op, e.right)

    # If right_map is None then we know mypy considers the right branch
    # to be unreachable and therefore any errors found in the right branch
    # should be suppressed.
    with self.msg.filter_errors(filter_errors=right_map is None):
        right_type = self.analyze_cond_branch(right_map, e.right, expanded_left_type)

    if left_map is None and right_map is None:
        return UninhabitedType()

    if right_map is None:
        # The boolean expression is statically known to be the left value
        assert left_map is not None
        return left_type
    if left_map is None:
        # The boolean expression is statically known to be the right value
        assert right_map is not None
        return right_type

    if e.op == "and":
        restricted_left_type = false_only(expanded_left_type)
        result_is_left = not expanded_left_type.can_be_true
    elif e.op == "or":
        restricted_left_type = true_only(expanded_left_type)
        result_is_left = not expanded_left_type.can_be_false

    if isinstance(restricted_left_type, UninhabitedType):
        # The left operand can never be the result
        return right_type
    elif result_is_left:
        # The left operand is always the result
        return left_type
    else:
        return make_simplified_union([restricted_left_type, right_type])

</t>
<t tx="ekr.20230831011819.566">def check_list_multiply(self, e: OpExpr) -&gt; Type:
    """Type check an expression of form '[...] * e'.

    Type inference is special-cased for this common construct.
    """
    right_type = self.accept(e.right)
    if is_subtype(right_type, self.named_type("builtins.int")):
        # Special case: [...] * &lt;int value&gt;. Use the type context of the
        # OpExpr, since the multiplication does not affect the type.
        left_type = self.accept(e.left, type_context=self.type_context[-1])
    else:
        left_type = self.accept(e.left)
    result, method_type = self.check_op("__mul__", left_type, e.right, e)
    e.method_type = method_type
    return result

</t>
<t tx="ekr.20230831011819.567">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; Type:
    value = self.accept(e.value)
    self.chk.check_assignment(e.target, e.value)
    self.chk.check_final(e)
    self.chk.store_type(e.target, value)
    self.find_partial_type_ref_fast_path(e.target)
    return value

</t>
<t tx="ekr.20230831011819.568">def visit_unary_expr(self, e: UnaryExpr) -&gt; Type:
    """Type check an unary operation ('not', '-', '+' or '~')."""
    operand_type = self.accept(e.expr)
    op = e.op
    if op == "not":
        result: Type = self.bool_type()
    else:
        method = operators.unary_op_methods[op]
        result, method_type = self.check_method_call_by_name(method, operand_type, [], [], e)
        e.method_type = method_type
    return result

</t>
<t tx="ekr.20230831011819.569">def visit_index_expr(self, e: IndexExpr) -&gt; Type:
    """Type check an index expression (base[index]).

    It may also represent type application.
    """
    result = self.visit_index_expr_helper(e)
    result = self.narrow_type_from_binder(e, result)
    p_result = get_proper_type(result)
    if (
        self.is_literal_context()
        and isinstance(p_result, Instance)
        and p_result.last_known_value is not None
    ):
        result = p_result.last_known_value
    return result

</t>
<t tx="ekr.20230831011819.57">@path mypy
"""Facilities to analyze entire programs, including imported modules.

Parse and analyze the source files of a program in the correct order
(based on file dependencies), and collect the results.

This module only directs a build, which is performed in multiple passes per
file.  The individual passes are implemented in separate modules.

The function build() is the main interface to this module.
"""
&lt;&lt; build.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.570">def visit_index_expr_helper(self, e: IndexExpr) -&gt; Type:
    if e.analyzed:
        # It's actually a type application.
        return self.accept(e.analyzed)
    left_type = self.accept(e.base)
    return self.visit_index_with_type(left_type, e)

</t>
<t tx="ekr.20230831011819.571">def visit_index_with_type(
    self, left_type: Type, e: IndexExpr, original_type: ProperType | None = None
) -&gt; Type:
    """Analyze type of an index expression for a given type of base expression.

    The 'original_type' is used for error messages (currently used for union types).
    """
    index = e.index
    left_type = get_proper_type(left_type)

    # Visit the index, just to make sure we have a type for it available
    self.accept(index)

    if isinstance(left_type, UnionType):
        original_type = original_type or left_type
        # Don't combine literal types, since we may need them for type narrowing.
        return make_simplified_union(
            [
                self.visit_index_with_type(typ, e, original_type)
                for typ in left_type.relevant_items()
            ],
            contract_literals=False,
        )
    elif isinstance(left_type, TupleType) and self.chk.in_checked_function():
        # Special case for tuples. They return a more specific type when
        # indexed by an integer literal.
        if isinstance(index, SliceExpr):
            return self.visit_tuple_slice_helper(left_type, index)

        ns = self.try_getting_int_literals(index)
        if ns is not None:
            out = []
            for n in ns:
                if n &lt; 0:
                    n += len(left_type.items)
                if 0 &lt;= n &lt; len(left_type.items):
                    out.append(left_type.items[n])
                else:
                    self.chk.fail(message_registry.TUPLE_INDEX_OUT_OF_RANGE, e)
                    return AnyType(TypeOfAny.from_error)
            return make_simplified_union(out)
        else:
            return self.nonliteral_tuple_index_helper(left_type, index)
    elif isinstance(left_type, TypedDictType):
        return self.visit_typeddict_index_expr(left_type, e.index)
    elif (
        isinstance(left_type, CallableType)
        and left_type.is_type_obj()
        and left_type.type_object().is_enum
    ):
        return self.visit_enum_index_expr(left_type.type_object(), e.index, e)
    elif isinstance(left_type, TypeVarType) and not self.has_member(
        left_type.upper_bound, "__getitem__"
    ):
        return self.visit_index_with_type(left_type.upper_bound, e, original_type)
    else:
        result, method_type = self.check_method_call_by_name(
            "__getitem__", left_type, [e.index], [ARG_POS], e, original_type=original_type
        )
        e.method_type = method_type
        return result

</t>
<t tx="ekr.20230831011819.572">def visit_tuple_slice_helper(self, left_type: TupleType, slic: SliceExpr) -&gt; Type:
    begin: Sequence[int | None] = [None]
    end: Sequence[int | None] = [None]
    stride: Sequence[int | None] = [None]

    if slic.begin_index:
        begin_raw = self.try_getting_int_literals(slic.begin_index)
        if begin_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        begin = begin_raw

    if slic.end_index:
        end_raw = self.try_getting_int_literals(slic.end_index)
        if end_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        end = end_raw

    if slic.stride:
        stride_raw = self.try_getting_int_literals(slic.stride)
        if stride_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        stride = stride_raw

    items: list[Type] = []
    for b, e, s in itertools.product(begin, end, stride):
        items.append(left_type.slice(b, e, s))
    return make_simplified_union(items)

</t>
<t tx="ekr.20230831011819.573">def try_getting_int_literals(self, index: Expression) -&gt; list[int] | None:
    """If the given expression or type corresponds to an int literal
    or a union of int literals, returns a list of the underlying ints.
    Otherwise, returns None.

    Specifically, this function is guaranteed to return a list with
    one or more ints if one one the following is true:

    1. 'expr' is a IntExpr or a UnaryExpr backed by an IntExpr
    2. 'typ' is a LiteralType containing an int
    3. 'typ' is a UnionType containing only LiteralType of ints
    """
    if isinstance(index, IntExpr):
        return [index.value]
    elif isinstance(index, UnaryExpr):
        if index.op == "-":
            operand = index.expr
            if isinstance(operand, IntExpr):
                return [-1 * operand.value]
    typ = get_proper_type(self.accept(index))
    if isinstance(typ, Instance) and typ.last_known_value is not None:
        typ = typ.last_known_value
    if isinstance(typ, LiteralType) and isinstance(typ.value, int):
        return [typ.value]
    if isinstance(typ, UnionType):
        out = []
        for item in get_proper_types(typ.items):
            if isinstance(item, LiteralType) and isinstance(item.value, int):
                out.append(item.value)
            else:
                return None
        return out
    return None

</t>
<t tx="ekr.20230831011819.574">def nonliteral_tuple_index_helper(self, left_type: TupleType, index: Expression) -&gt; Type:
    self.check_method_call_by_name("__getitem__", left_type, [index], [ARG_POS], context=index)
    # We could return the return type from above, but unions are often better than the join
    union = make_simplified_union(left_type.items)
    if isinstance(index, SliceExpr):
        return self.chk.named_generic_type("builtins.tuple", [union])
    return union

</t>
<t tx="ekr.20230831011819.575">def visit_typeddict_index_expr(
    self, td_type: TypedDictType, index: Expression, setitem: bool = False
) -&gt; Type:
    if isinstance(index, StrExpr):
        key_names = [index.value]
    else:
        typ = get_proper_type(self.accept(index))
        if isinstance(typ, UnionType):
            key_types: list[Type] = list(typ.items)
        else:
            key_types = [typ]

        key_names = []
        for key_type in get_proper_types(key_types):
            if isinstance(key_type, Instance) and key_type.last_known_value is not None:
                key_type = key_type.last_known_value

            if (
                isinstance(key_type, LiteralType)
                and isinstance(key_type.value, str)
                and key_type.fallback.type.fullname != "builtins.bytes"
            ):
                key_names.append(key_type.value)
            else:
                self.msg.typeddict_key_must_be_string_literal(td_type, index)
                return AnyType(TypeOfAny.from_error)

    value_types = []
    for key_name in key_names:
        value_type = td_type.items.get(key_name)
        if value_type is None:
            self.msg.typeddict_key_not_found(td_type, key_name, index, setitem)
            return AnyType(TypeOfAny.from_error)
        else:
            value_types.append(value_type)
    return make_simplified_union(value_types)

</t>
<t tx="ekr.20230831011819.576">def visit_enum_index_expr(
    self, enum_type: TypeInfo, index: Expression, context: Context
) -&gt; Type:
    string_type: Type = self.named_type("builtins.str")
    self.chk.check_subtype(
        self.accept(index),
        string_type,
        context,
        "Enum index should be a string",
        "actual index type",
    )
    return Instance(enum_type, [])

</t>
<t tx="ekr.20230831011819.577">def visit_cast_expr(self, expr: CastExpr) -&gt; Type:
    """Type check a cast expression."""
    source_type = self.accept(
        expr.expr,
        type_context=AnyType(TypeOfAny.special_form),
        allow_none_return=True,
        always_allow_any=True,
    )
    target_type = expr.type
    options = self.chk.options
    if (
        options.warn_redundant_casts
        and not isinstance(get_proper_type(target_type), AnyType)
        and source_type == target_type
    ):
        self.msg.redundant_cast(target_type, expr)
    if options.disallow_any_unimported and has_any_from_unimported_type(target_type):
        self.msg.unimported_type_becomes_any("Target type of cast", target_type, expr)
    check_for_explicit_any(
        target_type, self.chk.options, self.chk.is_typeshed_stub, self.msg, context=expr
    )
    return target_type

</t>
<t tx="ekr.20230831011819.578">def visit_assert_type_expr(self, expr: AssertTypeExpr) -&gt; Type:
    source_type = self.accept(
        expr.expr,
        type_context=self.type_context[-1],
        allow_none_return=True,
        always_allow_any=True,
    )
    if self.chk.current_node_deferred:
        return source_type

    target_type = expr.type
    proper_source_type = get_proper_type(source_type)
    if (
        isinstance(proper_source_type, mypy.types.Instance)
        and proper_source_type.last_known_value is not None
    ):
        source_type = proper_source_type.last_known_value
    if not is_same_type(source_type, target_type):
        if not self.chk.in_checked_function():
            self.msg.note(
                '"assert_type" expects everything to be "Any" in unchecked functions',
                expr.expr,
            )
        self.msg.assert_type_fail(source_type, target_type, expr)
    return source_type

</t>
<t tx="ekr.20230831011819.579">def visit_reveal_expr(self, expr: RevealExpr) -&gt; Type:
    """Type check a reveal_type expression."""
    if expr.kind == REVEAL_TYPE:
        assert expr.expr is not None
        revealed_type = self.accept(
            expr.expr, type_context=self.type_context[-1], allow_none_return=True
        )
        if not self.chk.current_node_deferred:
            self.msg.reveal_type(revealed_type, expr.expr)
            if not self.chk.in_checked_function():
                self.msg.note(
                    "'reveal_type' always outputs 'Any' in unchecked functions", expr.expr
                )
        return revealed_type
    else:
        # REVEAL_LOCALS
        if not self.chk.current_node_deferred:
            # the RevealExpr contains a local_nodes attribute,
            # calculated at semantic analysis time. Use it to pull out the
            # corresponding subset of variables in self.chk.type_map
            names_to_types = (
                {var_node.name: var_node.type for var_node in expr.local_nodes}
                if expr.local_nodes is not None
                else {}
            )

            self.msg.reveal_locals(names_to_types, expr)
        return NoneType()

</t>
<t tx="ekr.20230831011819.580">def visit_type_application(self, tapp: TypeApplication) -&gt; Type:
    """Type check a type application (expr[type, ...]).

    There are two different options here, depending on whether expr refers
    to a type alias or directly to a generic class. In the first case we need
    to use a dedicated function typeanal.instantiate_type_alias(). This
    is due to slight differences in how type arguments are applied and checked.
    """
    if isinstance(tapp.expr, RefExpr) and isinstance(tapp.expr.node, TypeAlias):
        # Subscription of a (generic) alias in runtime context, expand the alias.
        item = instantiate_type_alias(
            tapp.expr.node,
            tapp.types,
            self.chk.fail,
            tapp.expr.node.no_args,
            tapp,
            self.chk.options,
        )
        item = get_proper_type(item)
        if isinstance(item, Instance):
            tp = type_object_type(item.type, self.named_type)
            return self.apply_type_arguments_to_callable(tp, item.args, tapp)
        elif isinstance(item, TupleType) and item.partial_fallback.type.is_named_tuple:
            tp = type_object_type(item.partial_fallback.type, self.named_type)
            return self.apply_type_arguments_to_callable(tp, item.partial_fallback.args, tapp)
        elif isinstance(item, TypedDictType):
            return self.typeddict_callable_from_context(item)
        else:
            self.chk.fail(message_registry.ONLY_CLASS_APPLICATION, tapp)
            return AnyType(TypeOfAny.from_error)
    # Type application of a normal generic class in runtime context.
    # This is typically used as `x = G[int]()`.
    tp = get_proper_type(self.accept(tapp.expr))
    if isinstance(tp, (CallableType, Overloaded)):
        if not tp.is_type_obj():
            self.chk.fail(message_registry.ONLY_CLASS_APPLICATION, tapp)
        return self.apply_type_arguments_to_callable(tp, tapp.types, tapp)
    if isinstance(tp, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=tp)
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.581">def visit_type_alias_expr(self, alias: TypeAliasExpr) -&gt; Type:
    """Right hand side of a type alias definition.

    It has the same type as if the alias itself was used in a runtime context.
    For example, here:

        A = reveal_type(List[T])
        reveal_type(A)

    both `reveal_type` instances will reveal the same type `def (...) -&gt; builtins.list[Any]`.
    Note that type variables are implicitly substituted with `Any`.
    """
    return self.alias_type_in_runtime_context(alias.node, ctx=alias, alias_definition=True)

</t>
<t tx="ekr.20230831011819.582">def alias_type_in_runtime_context(
    self, alias: TypeAlias, *, ctx: Context, alias_definition: bool = False
) -&gt; Type:
    """Get type of a type alias (could be generic) in a runtime expression.

    Note that this function can be called only if the alias appears _not_
    as a target of type application, which is treated separately in the
    visit_type_application method. Some examples where this method is called are
    casts and instantiation:

        class LongName(Generic[T]): ...
        A = LongName[int]

        x = A()
        y = cast(A, ...)
    """
    if isinstance(alias.target, Instance) and alias.target.invalid:  # type: ignore[misc]
        # An invalid alias, error already has been reported
        return AnyType(TypeOfAny.from_error)
    # If this is a generic alias, we set all variables to `Any`.
    # For example:
    #     A = List[Tuple[T, T]]
    #     x = A() &lt;- same as List[Tuple[Any, Any]], see PEP 484.
    disallow_any = self.chk.options.disallow_any_generics and self.is_callee
    item = get_proper_type(
        set_any_tvars(
            alias,
            ctx.line,
            ctx.column,
            self.chk.options,
            disallow_any=disallow_any,
            fail=self.msg.fail,
        )
    )
    if isinstance(item, Instance):
        # Normally we get a callable type (or overloaded) with .is_type_obj() true
        # representing the class's constructor
        tp = type_object_type(item.type, self.named_type)
        if alias.no_args:
            return tp
        return self.apply_type_arguments_to_callable(tp, item.args, ctx)
    elif (
        isinstance(item, TupleType)
        and
        # Tuple[str, int]() fails at runtime, only named tuples and subclasses work.
        tuple_fallback(item).type.fullname != "builtins.tuple"
    ):
        return type_object_type(tuple_fallback(item).type, self.named_type)
    elif isinstance(item, TypedDictType):
        return self.typeddict_callable_from_context(item)
    elif isinstance(item, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=item)
    else:
        if alias_definition:
            return AnyType(TypeOfAny.special_form)
        # The _SpecialForm type can be used in some runtime contexts (e.g. it may have __or__).
        return self.named_type("typing._SpecialForm")

</t>
<t tx="ekr.20230831011819.583">def split_for_callable(
    self, t: CallableType, args: Sequence[Type], ctx: Context
) -&gt; list[Type]:
    """Handle directly applying type arguments to a variadic Callable.

    This is needed in situations where e.g. variadic class object appears in
    runtime context. For example:
        class C(Generic[T, Unpack[Ts]]): ...
        x = C[int, str]()

    We simply group the arguments that need to go into Ts variable into a TupleType,
    similar to how it is done in other places using split_with_prefix_and_suffix().
    """
    vars = t.variables
    if not vars or not any(isinstance(v, TypeVarTupleType) for v in vars):
        return list(args)

    prefix = next(i for (i, v) in enumerate(vars) if isinstance(v, TypeVarTupleType))
    suffix = len(vars) - prefix - 1
    if len(args) &lt; len(vars) - 1:
        self.msg.incompatible_type_application(len(vars), len(args), ctx)
        return [AnyType(TypeOfAny.from_error)] * len(vars)

    tvt = vars[prefix]
    assert isinstance(tvt, TypeVarTupleType)
    start, middle, end = split_with_prefix_and_suffix(tuple(args), prefix, suffix)
    return list(start) + [TupleType(list(middle), tvt.tuple_fallback)] + list(end)

</t>
<t tx="ekr.20230831011819.584">def apply_type_arguments_to_callable(
    self, tp: Type, args: Sequence[Type], ctx: Context
) -&gt; Type:
    """Apply type arguments to a generic callable type coming from a type object.

    This will first perform type arguments count checks, report the
    error as needed, and return the correct kind of Any. As a special
    case this returns Any for non-callable types, because if type object type
    is not callable, then an error should be already reported.
    """
    tp = get_proper_type(tp)

    if isinstance(tp, CallableType):
        if len(tp.variables) != len(args) and not any(
            isinstance(v, TypeVarTupleType) for v in tp.variables
        ):
            if tp.is_type_obj() and tp.type_object().fullname == "builtins.tuple":
                # TODO: Specialize the callable for the type arguments
                return tp
            self.msg.incompatible_type_application(len(tp.variables), len(args), ctx)
            return AnyType(TypeOfAny.from_error)
        return self.apply_generic_arguments(tp, self.split_for_callable(tp, args, ctx), ctx)
    if isinstance(tp, Overloaded):
        for it in tp.items:
            if len(it.variables) != len(args) and not any(
                isinstance(v, TypeVarTupleType) for v in it.variables
            ):
                self.msg.incompatible_type_application(len(it.variables), len(args), ctx)
                return AnyType(TypeOfAny.from_error)
        return Overloaded(
            [
                self.apply_generic_arguments(it, self.split_for_callable(it, args, ctx), ctx)
                for it in tp.items
            ]
        )
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.585">def visit_list_expr(self, e: ListExpr) -&gt; Type:
    """Type check a list expression [...]."""
    return self.check_lst_expr(e, "builtins.list", "&lt;list&gt;")

</t>
<t tx="ekr.20230831011819.586">def visit_set_expr(self, e: SetExpr) -&gt; Type:
    return self.check_lst_expr(e, "builtins.set", "&lt;set&gt;")

</t>
<t tx="ekr.20230831011819.587">def fast_container_type(
    self, e: ListExpr | SetExpr | TupleExpr, container_fullname: str
) -&gt; Type | None:
    """
    Fast path to determine the type of a list or set literal,
    based on the list of entries. This mostly impacts large
    module-level constant definitions.

    Limitations:
     - no active type context
     - no star expressions
     - the joined type of all entries must be an Instance or Tuple type
    """
    ctx = self.type_context[-1]
    if ctx:
        return None
    rt = self.resolved_type.get(e, None)
    if rt is not None:
        return rt if isinstance(rt, Instance) else None
    values: list[Type] = []
    for item in e.items:
        if isinstance(item, StarExpr):
            # fallback to slow path
            self.resolved_type[e] = NoneType()
            return None
        values.append(self.accept(item))
    vt = join.join_type_list(values)
    if not allow_fast_container_literal(vt):
        self.resolved_type[e] = NoneType()
        return None
    ct = self.chk.named_generic_type(container_fullname, [vt])
    self.resolved_type[e] = ct
    return ct

</t>
<t tx="ekr.20230831011819.588">def check_lst_expr(self, e: ListExpr | SetExpr | TupleExpr, fullname: str, tag: str) -&gt; Type:
    # fast path
    t = self.fast_container_type(e, fullname)
    if t:
        return t

    # Translate into type checking a generic function call.
    # Used for list and set expressions, as well as for tuples
    # containing star expressions that don't refer to a
    # Tuple. (Note: "lst" stands for list-set-tuple. :-)
    tv = TypeVarType(
        "T",
        "T",
        id=-1,
        values=[],
        upper_bound=self.object_type(),
        default=AnyType(TypeOfAny.from_omitted_generics),
    )
    constructor = CallableType(
        [tv],
        [nodes.ARG_STAR],
        [None],
        self.chk.named_generic_type(fullname, [tv]),
        self.named_type("builtins.function"),
        name=tag,
        variables=[tv],
    )
    out = self.check_call(
        constructor,
        [(i.expr if isinstance(i, StarExpr) else i) for i in e.items],
        [(nodes.ARG_STAR if isinstance(i, StarExpr) else nodes.ARG_POS) for i in e.items],
        e,
    )[0]
    return remove_instance_last_known_values(out)

</t>
<t tx="ekr.20230831011819.589">def visit_tuple_expr(self, e: TupleExpr) -&gt; Type:
    """Type check a tuple expression."""
    # Try to determine type context for type inference.
    type_context = get_proper_type(self.type_context[-1])
    type_context_items = None
    if isinstance(type_context, UnionType):
        tuples_in_context = [
            t
            for t in get_proper_types(type_context.items)
            if (isinstance(t, TupleType) and len(t.items) == len(e.items))
            or is_named_instance(t, TUPLE_LIKE_INSTANCE_NAMES)
        ]
        if len(tuples_in_context) == 1:
            type_context = tuples_in_context[0]
        else:
            # There are either no relevant tuples in the Union, or there is
            # more than one.  Either way, we can't decide on a context.
            pass

    if isinstance(type_context, TupleType):
        type_context_items = type_context.items
    elif type_context and is_named_instance(type_context, TUPLE_LIKE_INSTANCE_NAMES):
        assert isinstance(type_context, Instance)
        if type_context.args:
            type_context_items = [type_context.args[0]] * len(e.items)
    # NOTE: it's possible for the context to have a different
    # number of items than e.  In that case we use those context
    # items that match a position in e, and we'll worry about type
    # mismatches later.

    # Infer item types.  Give up if there's a star expression
    # that's not a Tuple.
    items: list[Type] = []
    j = 0  # Index into type_context_items; irrelevant if type_context_items is none
    for i in range(len(e.items)):
        item = e.items[i]
        if isinstance(item, StarExpr):
            # Special handling for star expressions.
            # TODO: If there's a context, and item.expr is a
            # TupleExpr, flatten it, so we can benefit from the
            # context?  Counterargument: Why would anyone write
            # (1, *(2, 3)) instead of (1, 2, 3) except in a test?
            tt = self.accept(item.expr)
            tt = get_proper_type(tt)
            if isinstance(tt, TupleType):
                items.extend(tt.items)
                j += len(tt.items)
            else:
                # A star expression that's not a Tuple.
                # Treat the whole thing as a variable-length tuple.
                return self.check_lst_expr(e, "builtins.tuple", "&lt;tuple&gt;")
        else:
            if not type_context_items or j &gt;= len(type_context_items):
                tt = self.accept(item)
            else:
                tt = self.accept(item, type_context_items[j])
                j += 1
            items.append(tt)
    # This is a partial fallback item type. A precise type will be calculated on demand.
    fallback_item = AnyType(TypeOfAny.special_form)
    return TupleType(items, self.chk.named_generic_type("builtins.tuple", [fallback_item]))

</t>
<t tx="ekr.20230831011819.59"># TODO: More consistent terminology, e.g. path/fnam, module/id, state/file

from __future__ import annotations

import collections
import contextlib
import errno
import gc
import json
import os
import platform
import re
import stat
import sys
import time
import types
from typing import (
    TYPE_CHECKING,
    AbstractSet,
    Any,
    Callable,
    ClassVar,
    Dict,
    Final,
    Iterator,
    Mapping,
    NamedTuple,
    NoReturn,
    Sequence,
    TextIO,
)
from typing_extensions import TypeAlias as _TypeAlias, TypedDict

import mypy.semanal_main
from mypy.checker import TypeChecker
from mypy.errors import CompileError, ErrorInfo, Errors, report_internal_error
from mypy.graph_utils import prepare_sccs, strongly_connected_components, topsort
from mypy.indirection import TypeIndirectionVisitor
from mypy.messages import MessageBuilder
from mypy.nodes import Import, ImportAll, ImportBase, ImportFrom, MypyFile, SymbolTable, TypeInfo
from mypy.partially_defined import PossiblyUndefinedVariableVisitor
from mypy.semanal import SemanticAnalyzer
from mypy.semanal_pass1 import SemanticAnalyzerPreAnalysis
from mypy.util import (
    DecodeError,
    decode_python_encoding,
    get_mypy_comments,
    hash_digest,
    is_stub_package_file,
    is_sub_path,
    is_typeshed_file,
    module_prefix,
    read_py_file,
    time_ref,
    time_spent_us,
)

if TYPE_CHECKING:
    from mypy.report import Reports  # Avoid unconditional slow import

from mypy import errorcodes as codes
from mypy.config_parser import parse_mypy_comments
from mypy.fixup import fixup_module
from mypy.freetree import free_tree
from mypy.fscache import FileSystemCache
from mypy.metastore import FilesystemMetadataStore, MetadataStore, SqliteMetadataStore
from mypy.modulefinder import (
    BuildSource as BuildSource,
    BuildSourceSet as BuildSourceSet,
    FindModuleCache,
    ModuleNotFoundReason,
    ModuleSearchResult,
    SearchPaths,
    compute_search_paths,
)
from mypy.nodes import Expression
from mypy.options import Options
from mypy.parse import parse
from mypy.plugin import ChainedPlugin, Plugin, ReportConfigContext
from mypy.plugins.default import DefaultPlugin
from mypy.renaming import LimitedVariableRenameVisitor, VariableRenameVisitor
from mypy.stats import dump_type_stats
from mypy.stubinfo import legacy_bundled_packages, non_bundled_packages, stub_distribution_name
from mypy.types import Type
from mypy.typestate import reset_global_state, type_state
from mypy.version import __version__

# Switch to True to produce debug output related to fine-grained incremental
# mode only that is useful during development. This produces only a subset of
# output compared to --verbose output. We use a global flag to enable this so
# that it's easy to enable this when running tests.
DEBUG_FINE_GRAINED: Final = False

# These modules are special and should always come from typeshed.
CORE_BUILTIN_MODULES: Final = {
    "builtins",
    "typing",
    "types",
    "typing_extensions",
    "mypy_extensions",
    "_typeshed",
    "_collections_abc",
    "collections",
    "collections.abc",
    "sys",
    "abc",
}


Graph: _TypeAlias = Dict[str, "State"]


# TODO: Get rid of BuildResult.  We might as well return a BuildManager.
</t>
<t tx="ekr.20230831011819.590">def fast_dict_type(self, e: DictExpr) -&gt; Type | None:
    """
    Fast path to determine the type of a dict literal,
    based on the list of entries. This mostly impacts large
    module-level constant definitions.

    Limitations:
     - no active type context
     - only supported star expressions are other dict instances
     - the joined types of all keys and values must be Instance or Tuple types
    """
    ctx = self.type_context[-1]
    if ctx:
        return None
    rt = self.resolved_type.get(e, None)
    if rt is not None:
        return rt if isinstance(rt, Instance) else None
    keys: list[Type] = []
    values: list[Type] = []
    stargs: tuple[Type, Type] | None = None
    for key, value in e.items:
        if key is None:
            st = get_proper_type(self.accept(value))
            if (
                isinstance(st, Instance)
                and st.type.fullname == "builtins.dict"
                and len(st.args) == 2
            ):
                stargs = (st.args[0], st.args[1])
            else:
                self.resolved_type[e] = NoneType()
                return None
        else:
            keys.append(self.accept(key))
            values.append(self.accept(value))
    kt = join.join_type_list(keys)
    vt = join.join_type_list(values)
    if not (allow_fast_container_literal(kt) and allow_fast_container_literal(vt)):
        self.resolved_type[e] = NoneType()
        return None
    if stargs and (stargs[0] != kt or stargs[1] != vt):
        self.resolved_type[e] = NoneType()
        return None
    dt = self.chk.named_generic_type("builtins.dict", [kt, vt])
    self.resolved_type[e] = dt
    return dt

</t>
<t tx="ekr.20230831011819.591">def check_typeddict_literal_in_context(
    self, e: DictExpr, typeddict_context: TypedDictType
) -&gt; Type:
    orig_ret_type = self.check_typeddict_call_with_dict(
        callee=typeddict_context, kwargs=e.items, context=e, orig_callee=None
    )
    ret_type = get_proper_type(orig_ret_type)
    if isinstance(ret_type, TypedDictType):
        return ret_type.copy_modified()
    return typeddict_context.copy_modified()

</t>
<t tx="ekr.20230831011819.592">def visit_dict_expr(self, e: DictExpr) -&gt; Type:
    """Type check a dict expression.

    Translate it into a call to dict(), with provisions for **expr.
    """
    # if the dict literal doesn't match TypedDict, check_typeddict_call_with_dict reports
    # an error, but returns the TypedDict type that matches the literal it found
    # that would cause a second error when that TypedDict type is returned upstream
    # to avoid the second error, we always return TypedDict type that was requested
    typeddict_contexts = self.find_typeddict_context(self.type_context[-1], e)
    if typeddict_contexts:
        if len(typeddict_contexts) == 1:
            return self.check_typeddict_literal_in_context(e, typeddict_contexts[0])
        # Multiple items union, check if at least one of them matches cleanly.
        for typeddict_context in typeddict_contexts:
            with self.msg.filter_errors() as err, self.chk.local_type_map() as tmap:
                ret_type = self.check_typeddict_literal_in_context(e, typeddict_context)
            if err.has_new_errors():
                continue
            self.chk.store_types(tmap)
            return ret_type
        # No item matched without an error, so we can't unambiguously choose the item.
        self.msg.typeddict_context_ambiguous(typeddict_contexts, e)

    # fast path attempt
    dt = self.fast_dict_type(e)
    if dt:
        return dt

    # Define type variables (used in constructors below).
    kt = TypeVarType(
        "KT",
        "KT",
        id=-1,
        values=[],
        upper_bound=self.object_type(),
        default=AnyType(TypeOfAny.from_omitted_generics),
    )
    vt = TypeVarType(
        "VT",
        "VT",
        id=-2,
        values=[],
        upper_bound=self.object_type(),
        default=AnyType(TypeOfAny.from_omitted_generics),
    )

    # Collect function arguments, watching out for **expr.
    args: list[Expression] = []
    expected_types: list[Type] = []
    for key, value in e.items:
        if key is None:
            args.append(value)
            expected_types.append(
                self.chk.named_generic_type("_typeshed.SupportsKeysAndGetItem", [kt, vt])
            )
        else:
            tup = TupleExpr([key, value])
            if key.line &gt;= 0:
                tup.line = key.line
                tup.column = key.column
            else:
                tup.line = value.line
                tup.column = value.column
            tup.end_line = value.end_line
            tup.end_column = value.end_column
            args.append(tup)
            expected_types.append(TupleType([kt, vt], self.named_type("builtins.tuple")))

    # The callable type represents a function like this (except we adjust for **expr):
    #   def &lt;dict&gt;(*v: Tuple[kt, vt]) -&gt; Dict[kt, vt]: ...
    constructor = CallableType(
        expected_types,
        [nodes.ARG_POS] * len(expected_types),
        [None] * len(expected_types),
        self.chk.named_generic_type("builtins.dict", [kt, vt]),
        self.named_type("builtins.function"),
        name="&lt;dict&gt;",
        variables=[kt, vt],
    )
    return self.check_call(constructor, args, [nodes.ARG_POS] * len(args), e)[0]

</t>
<t tx="ekr.20230831011819.593">def find_typeddict_context(
    self, context: Type | None, dict_expr: DictExpr
) -&gt; list[TypedDictType]:
    context = get_proper_type(context)
    if isinstance(context, TypedDictType):
        return [context]
    elif isinstance(context, UnionType):
        items = []
        for item in context.items:
            item_contexts = self.find_typeddict_context(item, dict_expr)
            for item_context in item_contexts:
                if self.match_typeddict_call_with_dict(
                    item_context, dict_expr.items, dict_expr
                ):
                    items.append(item_context)
        return items
    # No TypedDict type in context.
    return []

</t>
<t tx="ekr.20230831011819.594">def visit_lambda_expr(self, e: LambdaExpr) -&gt; Type:
    """Type check lambda expression."""
    self.chk.check_default_args(e, body_is_trivial=False)
    inferred_type, type_override = self.infer_lambda_type_using_context(e)
    if not inferred_type:
        self.chk.return_types.append(AnyType(TypeOfAny.special_form))
        # Type check everything in the body except for the final return
        # statement (it can contain tuple unpacking before return).
        with self.chk.scope.push_function(e):
            # Lambdas can have more than one element in body,
            # when we add "fictional" AssigmentStatement nodes, like in:
            # `lambda (a, b): a`
            for stmt in e.body.body[:-1]:
                stmt.accept(self.chk)
            # Only type check the return expression, not the return statement.
            # This is important as otherwise the following statements would be
            # considered unreachable. There's no useful type context.
            ret_type = self.accept(e.expr(), allow_none_return=True)
        fallback = self.named_type("builtins.function")
        self.chk.return_types.pop()
        return callable_type(e, fallback, ret_type)
    else:
        # Type context available.
        self.chk.return_types.append(inferred_type.ret_type)
        self.chk.check_func_item(e, type_override=type_override)
        if not self.chk.has_type(e.expr()):
            # TODO: return expression must be accepted before exiting function scope.
            self.accept(e.expr(), allow_none_return=True)
        ret_type = self.chk.lookup_type(e.expr())
        self.chk.return_types.pop()
        return replace_callable_return_type(inferred_type, ret_type)

</t>
<t tx="ekr.20230831011819.595">def infer_lambda_type_using_context(
    self, e: LambdaExpr
) -&gt; tuple[CallableType | None, CallableType | None]:
    """Try to infer lambda expression type using context.

    Return None if could not infer type.
    The second item in the return type is the type_override parameter for check_func_item.
    """
    # TODO also accept 'Any' context
    ctx = get_proper_type(self.type_context[-1])

    if isinstance(ctx, UnionType):
        callables = [
            t for t in get_proper_types(ctx.relevant_items()) if isinstance(t, CallableType)
        ]
        if len(callables) == 1:
            ctx = callables[0]

    if not ctx or not isinstance(ctx, CallableType):
        return None, None

    # The context may have function type variables in it. We replace them
    # since these are the type variables we are ultimately trying to infer;
    # they must be considered as indeterminate. We use ErasedType since it
    # does not affect type inference results (it is for purposes like this
    # only).
    if self.chk.options.new_type_inference:
        # With new type inference we can preserve argument types even if they
        # are generic, since new inference algorithm can handle constraints
        # like S &lt;: T (we still erase return type since it's ultimately unknown).
        extra_vars = []
        for arg in ctx.arg_types:
            meta_vars = [tv for tv in get_all_type_vars(arg) if tv.id.is_meta_var()]
            extra_vars.extend([tv for tv in meta_vars if tv not in extra_vars])
        callable_ctx = ctx.copy_modified(
            ret_type=replace_meta_vars(ctx.ret_type, ErasedType()),
            variables=list(ctx.variables) + extra_vars,
        )
    else:
        erased_ctx = replace_meta_vars(ctx, ErasedType())
        assert isinstance(erased_ctx, ProperType) and isinstance(erased_ctx, CallableType)
        callable_ctx = erased_ctx

    # The callable_ctx may have a fallback of builtins.type if the context
    # is a constructor -- but this fallback doesn't make sense for lambdas.
    callable_ctx = callable_ctx.copy_modified(fallback=self.named_type("builtins.function"))

    if callable_ctx.type_guard is not None:
        # Lambda's return type cannot be treated as a `TypeGuard`,
        # because it is implicit. And `TypeGuard`s must be explicit.
        # See https://github.com/python/mypy/issues/9927
        return None, None

    arg_kinds = [arg.kind for arg in e.arguments]

    if callable_ctx.is_ellipsis_args or ctx.param_spec() is not None:
        # Fill in Any arguments to match the arguments of the lambda.
        callable_ctx = callable_ctx.copy_modified(
            is_ellipsis_args=False,
            arg_types=[AnyType(TypeOfAny.special_form)] * len(arg_kinds),
            arg_kinds=arg_kinds,
            arg_names=e.arg_names.copy(),
        )

    if ARG_STAR in arg_kinds or ARG_STAR2 in arg_kinds:
        # TODO treat this case appropriately
        return callable_ctx, None

    if callable_ctx.arg_kinds != arg_kinds:
        # Incompatible context; cannot use it to infer types.
        self.chk.fail(message_registry.CANNOT_INFER_LAMBDA_TYPE, e)
        return None, None

    return callable_ctx, callable_ctx

</t>
<t tx="ekr.20230831011819.596">def visit_super_expr(self, e: SuperExpr) -&gt; Type:
    """Type check a super expression (non-lvalue)."""

    # We have an expression like super(T, var).member

    # First compute the types of T and var
    types = self._super_arg_types(e)
    if isinstance(types, tuple):
        type_type, instance_type = types
    else:
        return types

    # Now get the MRO
    type_info = type_info_from_type(type_type)
    if type_info is None:
        self.chk.fail(message_registry.UNSUPPORTED_ARG_1_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    instance_info = type_info_from_type(instance_type)
    if instance_info is None:
        self.chk.fail(message_registry.UNSUPPORTED_ARG_2_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    mro = instance_info.mro

    # The base is the first MRO entry *after* type_info that has a member
    # with the right name
    index = None
    if type_info in mro:
        index = mro.index(type_info)
    else:
        method = self.chk.scope.top_function()
        # Mypy explicitly allows supertype upper bounds (and no upper bound at all)
        # for annotating self-types. However, if such an annotation is used for
        # checking super() we will still get an error. So to be consistent, we also
        # allow such imprecise annotations for use with super(), where we fall back
        # to the current class MRO instead. This works only from inside a method.
        if method is not None and is_self_type_like(
            instance_type, is_classmethod=method.is_class
        ):
            if e.info and type_info in e.info.mro:
                mro = e.info.mro
                index = mro.index(type_info)
    if index is None:
        if (
            instance_info.is_protocol
            and instance_info != type_info
            and not type_info.is_protocol
        ):
            # A special case for mixins, in this case super() should point
            # directly to the host protocol, this is not safe, since the real MRO
            # is not known yet for mixin, but this feature is more like an escape hatch.
            index = -1
        else:
            self.chk.fail(message_registry.SUPER_ARG_2_NOT_INSTANCE_OF_ARG_1, e)
            return AnyType(TypeOfAny.from_error)

    if len(mro) == index + 1:
        self.chk.fail(message_registry.TARGET_CLASS_HAS_NO_BASE_CLASS, e)
        return AnyType(TypeOfAny.from_error)

    for base in mro[index + 1 :]:
        if e.name in base.names or base == mro[-1]:
            if e.info and e.info.fallback_to_any and base == mro[-1]:
                # There's an undefined base class, and we're at the end of the
                # chain.  That's not an error.
                return AnyType(TypeOfAny.special_form)

            return analyze_member_access(
                name=e.name,
                typ=instance_type,
                is_lvalue=False,
                is_super=True,
                is_operator=False,
                original_type=instance_type,
                override_info=base,
                context=e,
                msg=self.msg,
                chk=self.chk,
                in_literal_context=self.is_literal_context(),
            )

    assert False, "unreachable"

</t>
<t tx="ekr.20230831011819.597">def _super_arg_types(self, e: SuperExpr) -&gt; Type | tuple[Type, Type]:
    """
    Computes the types of the type and instance expressions in super(T, instance), or the
    implicit ones for zero-argument super() expressions.  Returns a single type for the whole
    super expression when possible (for errors, anys), otherwise the pair of computed types.
    """

    if not self.chk.in_checked_function():
        return AnyType(TypeOfAny.unannotated)
    elif len(e.call.args) == 0:
        if not e.info:
            # This has already been reported by the semantic analyzer.
            return AnyType(TypeOfAny.from_error)
        elif self.chk.scope.active_class():
            self.chk.fail(message_registry.SUPER_OUTSIDE_OF_METHOD_NOT_SUPPORTED, e)
            return AnyType(TypeOfAny.from_error)

        # Zero-argument super() is like super(&lt;current class&gt;, &lt;self&gt;)
        current_type = fill_typevars(e.info)
        type_type: ProperType = TypeType(current_type)

        # Use the type of the self argument, in case it was annotated
        method = self.chk.scope.top_function()
        assert method is not None
        if method.arguments:
            instance_type: Type = method.arguments[0].variable.type or current_type
        else:
            self.chk.fail(message_registry.SUPER_ENCLOSING_POSITIONAL_ARGS_REQUIRED, e)
            return AnyType(TypeOfAny.from_error)
    elif ARG_STAR in e.call.arg_kinds:
        self.chk.fail(message_registry.SUPER_VARARGS_NOT_SUPPORTED, e)
        return AnyType(TypeOfAny.from_error)
    elif set(e.call.arg_kinds) != {ARG_POS}:
        self.chk.fail(message_registry.SUPER_POSITIONAL_ARGS_REQUIRED, e)
        return AnyType(TypeOfAny.from_error)
    elif len(e.call.args) == 1:
        self.chk.fail(message_registry.SUPER_WITH_SINGLE_ARG_NOT_SUPPORTED, e)
        return AnyType(TypeOfAny.from_error)
    elif len(e.call.args) == 2:
        type_type = get_proper_type(self.accept(e.call.args[0]))
        instance_type = self.accept(e.call.args[1])
    else:
        self.chk.fail(message_registry.TOO_MANY_ARGS_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    # Imprecisely assume that the type is the current class
    if isinstance(type_type, AnyType):
        if e.info:
            type_type = TypeType(fill_typevars(e.info))
        else:
            return AnyType(TypeOfAny.from_another_any, source_any=type_type)
    elif isinstance(type_type, TypeType):
        type_item = type_type.item
        if isinstance(type_item, AnyType):
            if e.info:
                type_type = TypeType(fill_typevars(e.info))
            else:
                return AnyType(TypeOfAny.from_another_any, source_any=type_item)

    if not isinstance(type_type, TypeType) and not (
        isinstance(type_type, FunctionLike) and type_type.is_type_obj()
    ):
        self.msg.first_argument_for_super_must_be_type(type_type, e)
        return AnyType(TypeOfAny.from_error)

    # Imprecisely assume that the instance is of the current class
    instance_type = get_proper_type(instance_type)
    if isinstance(instance_type, AnyType):
        if e.info:
            instance_type = fill_typevars(e.info)
        else:
            return AnyType(TypeOfAny.from_another_any, source_any=instance_type)
    elif isinstance(instance_type, TypeType):
        instance_item = instance_type.item
        if isinstance(instance_item, AnyType):
            if e.info:
                instance_type = TypeType(fill_typevars(e.info))
            else:
                return AnyType(TypeOfAny.from_another_any, source_any=instance_item)

    return type_type, instance_type

</t>
<t tx="ekr.20230831011819.598">def visit_slice_expr(self, e: SliceExpr) -&gt; Type:
    try:
        supports_index = self.chk.named_type("typing_extensions.SupportsIndex")
    except KeyError:
        supports_index = self.chk.named_type("builtins.int")  # thanks, fixture life
    expected = make_optional_type(supports_index)
    for index in [e.begin_index, e.end_index, e.stride]:
        if index:
            t = self.accept(index)
            self.chk.check_subtype(t, expected, index, message_registry.INVALID_SLICE_INDEX)
    return self.named_type("builtins.slice")

</t>
<t tx="ekr.20230831011819.599">def visit_list_comprehension(self, e: ListComprehension) -&gt; Type:
    return self.check_generator_or_comprehension(
        e.generator, "builtins.list", "&lt;list-comprehension&gt;"
    )

</t>
<t tx="ekr.20230831011819.6">def console_entry() -&gt; None:
    try:
        main()
        sys.stdout.flush()
        sys.stderr.flush()
    except BrokenPipeError:
        # Python flushes standard streams on exit; redirect remaining output
        # to devnull to avoid another BrokenPipeError at shutdown
        devnull = os.open(os.devnull, os.O_WRONLY)
        os.dup2(devnull, sys.stdout.fileno())
        sys.exit(2)
    except KeyboardInterrupt:
        _, options = process_options(args=sys.argv[1:])
        if options.show_traceback:
            sys.stdout.write(traceback.format_exc())
        formatter = FancyFormatter(sys.stdout, sys.stderr, False)
        msg = "Interrupted\n"
        sys.stdout.write(formatter.style(msg, color="red", bold=True))
        sys.stdout.flush()
        sys.stderr.flush()
        sys.exit(2)
</t>
<t tx="ekr.20230831011819.60">class BuildResult:
    """The result of a successful build.

    Attributes:
      manager: The build manager.
      files:   Dictionary from module name to related AST node.
      types:   Dictionary from parse tree node to its inferred type.
      used_cache: Whether the build took advantage of a pre-existing cache
      errors:  List of error messages.
    """

    @others
</t>
<t tx="ekr.20230831011819.600">def visit_set_comprehension(self, e: SetComprehension) -&gt; Type:
    return self.check_generator_or_comprehension(
        e.generator, "builtins.set", "&lt;set-comprehension&gt;"
    )

</t>
<t tx="ekr.20230831011819.601">def visit_generator_expr(self, e: GeneratorExpr) -&gt; Type:
    # If any of the comprehensions use async for, the expression will return an async generator
    # object, or if the left-side expression uses await.
    if any(e.is_async) or has_await_expression(e.left_expr):
        typ = "typing.AsyncGenerator"
        # received type is always None in async generator expressions
        additional_args: list[Type] = [NoneType()]
    else:
        typ = "typing.Generator"
        # received type and returned type are None
        additional_args = [NoneType(), NoneType()]
    return self.check_generator_or_comprehension(
        e, typ, "&lt;generator&gt;", additional_args=additional_args
    )

</t>
<t tx="ekr.20230831011819.602">def check_generator_or_comprehension(
    self,
    gen: GeneratorExpr,
    type_name: str,
    id_for_messages: str,
    additional_args: list[Type] | None = None,
) -&gt; Type:
    """Type check a generator expression or a list comprehension."""
    additional_args = additional_args or []
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        self.check_for_comp(gen)

        # Infer the type of the list comprehension by using a synthetic generic
        # callable type.
        tv = TypeVarType(
            "T",
            "T",
            id=-1,
            values=[],
            upper_bound=self.object_type(),
            default=AnyType(TypeOfAny.from_omitted_generics),
        )
        tv_list: list[Type] = [tv]
        constructor = CallableType(
            tv_list,
            [nodes.ARG_POS],
            [None],
            self.chk.named_generic_type(type_name, tv_list + additional_args),
            self.chk.named_type("builtins.function"),
            name=id_for_messages,
            variables=[tv],
        )
        return self.check_call(constructor, [gen.left_expr], [nodes.ARG_POS], gen)[0]

</t>
<t tx="ekr.20230831011819.603">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; Type:
    """Type check a dictionary comprehension."""
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        self.check_for_comp(e)

        # Infer the type of the list comprehension by using a synthetic generic
        # callable type.
        ktdef = TypeVarType(
            "KT",
            "KT",
            id=-1,
            values=[],
            upper_bound=self.object_type(),
            default=AnyType(TypeOfAny.from_omitted_generics),
        )
        vtdef = TypeVarType(
            "VT",
            "VT",
            id=-2,
            values=[],
            upper_bound=self.object_type(),
            default=AnyType(TypeOfAny.from_omitted_generics),
        )
        constructor = CallableType(
            [ktdef, vtdef],
            [nodes.ARG_POS, nodes.ARG_POS],
            [None, None],
            self.chk.named_generic_type("builtins.dict", [ktdef, vtdef]),
            self.chk.named_type("builtins.function"),
            name="&lt;dictionary-comprehension&gt;",
            variables=[ktdef, vtdef],
        )
        return self.check_call(
            constructor, [e.key, e.value], [nodes.ARG_POS, nodes.ARG_POS], e
        )[0]

</t>
<t tx="ekr.20230831011819.604">def check_for_comp(self, e: GeneratorExpr | DictionaryComprehension) -&gt; None:
    """Check the for_comp part of comprehensions. That is the part from 'for':
    ... for x in y if z

    Note: This adds the type information derived from the condlists to the current binder.
    """
    for index, sequence, conditions, is_async in zip(
        e.indices, e.sequences, e.condlists, e.is_async
    ):
        if is_async:
            _, sequence_type = self.chk.analyze_async_iterable_item_type(sequence)
        else:
            _, sequence_type = self.chk.analyze_iterable_item_type(sequence)
        self.chk.analyze_index_variables(index, sequence_type, True, e)
        for condition in conditions:
            self.accept(condition)

            # values are only part of the comprehension when all conditions are true
            true_map, false_map = self.chk.find_isinstance_check(condition)

            if true_map:
                self.chk.push_type_map(true_map)

            if codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes:
                if true_map is None:
                    self.msg.redundant_condition_in_comprehension(False, condition)
                elif false_map is None:
                    self.msg.redundant_condition_in_comprehension(True, condition)

</t>
<t tx="ekr.20230831011819.605">def visit_conditional_expr(self, e: ConditionalExpr, allow_none_return: bool = False) -&gt; Type:
    self.accept(e.cond)
    ctx = self.type_context[-1]

    # Gain type information from isinstance if it is there
    # but only for the current expression
    if_map, else_map = self.chk.find_isinstance_check(e.cond)
    if codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes:
        if if_map is None:
            self.msg.redundant_condition_in_if(False, e.cond)
        elif else_map is None:
            self.msg.redundant_condition_in_if(True, e.cond)

    if_type = self.analyze_cond_branch(
        if_map, e.if_expr, context=ctx, allow_none_return=allow_none_return
    )

    # we want to keep the narrowest value of if_type for union'ing the branches
    # however, it would be silly to pass a literal as a type context. Pass the
    # underlying fallback type instead.
    if_type_fallback = simple_literal_type(get_proper_type(if_type)) or if_type

    # Analyze the right branch using full type context and store the type
    full_context_else_type = self.analyze_cond_branch(
        else_map, e.else_expr, context=ctx, allow_none_return=allow_none_return
    )

    if not mypy.checker.is_valid_inferred_type(if_type):
        # Analyze the right branch disregarding the left branch.
        else_type = full_context_else_type
        # we want to keep the narrowest value of else_type for union'ing the branches
        # however, it would be silly to pass a literal as a type context. Pass the
        # underlying fallback type instead.
        else_type_fallback = simple_literal_type(get_proper_type(else_type)) or else_type

        # If it would make a difference, re-analyze the left
        # branch using the right branch's type as context.
        if ctx is None or not is_equivalent(else_type_fallback, ctx):
            # TODO: If it's possible that the previous analysis of
            # the left branch produced errors that are avoided
            # using this context, suppress those errors.
            if_type = self.analyze_cond_branch(
                if_map,
                e.if_expr,
                context=else_type_fallback,
                allow_none_return=allow_none_return,
            )

    elif if_type_fallback == ctx:
        # There is no point re-running the analysis if if_type is equal to ctx.
        # That would  be an exact duplicate of the work we just did.
        # This optimization is particularly important to avoid exponential blowup with nested
        # if/else expressions: https://github.com/python/mypy/issues/9591
        # TODO: would checking for is_proper_subtype also work and cover more cases?
        else_type = full_context_else_type
    else:
        # Analyze the right branch in the context of the left
        # branch's type.
        else_type = self.analyze_cond_branch(
            else_map,
            e.else_expr,
            context=if_type_fallback,
            allow_none_return=allow_none_return,
        )

    # Only create a union type if the type context is a union, to be mostly
    # compatible with older mypy versions where we always did a join.
    #
    # TODO: Always create a union or at least in more cases?
    if isinstance(get_proper_type(self.type_context[-1]), UnionType):
        res: Type = make_simplified_union([if_type, full_context_else_type])
    else:
        res = join.join_types(if_type, else_type)

    return res

</t>
<t tx="ekr.20230831011819.606">def analyze_cond_branch(
    self,
    map: dict[Expression, Type] | None,
    node: Expression,
    context: Type | None,
    allow_none_return: bool = False,
) -&gt; Type:
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        if map is None:
            # We still need to type check node, in case we want to
            # process it for isinstance checks later
            self.accept(node, type_context=context, allow_none_return=allow_none_return)
            return UninhabitedType()
        self.chk.push_type_map(map)
        return self.accept(node, type_context=context, allow_none_return=allow_none_return)

</t>
<t tx="ekr.20230831011819.607">#
# Helpers
#

def accept(
    self,
    node: Expression,
    type_context: Type | None = None,
    allow_none_return: bool = False,
    always_allow_any: bool = False,
    is_callee: bool = False,
) -&gt; Type:
    """Type check a node in the given type context.  If allow_none_return
    is True and this expression is a call, allow it to return None.  This
    applies only to this expression and not any subexpressions.
    """
    if node in self.type_overrides:
        # This branch is very fast, there is no point timing it.
        return self.type_overrides[node]
    # We don't use context manager here to get most precise data (and avoid overhead).
    record_time = False
    if self.collect_line_checking_stats and not self.in_expression:
        t0 = time.perf_counter_ns()
        self.in_expression = True
        record_time = True
    self.type_context.append(type_context)
    old_is_callee = self.is_callee
    self.is_callee = is_callee
    try:
        if allow_none_return and isinstance(node, CallExpr):
            typ = self.visit_call_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, YieldFromExpr):
            typ = self.visit_yield_from_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, ConditionalExpr):
            typ = self.visit_conditional_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, AwaitExpr):
            typ = self.visit_await_expr(node, allow_none_return=True)
        else:
            typ = node.accept(self)
    except Exception as err:
        report_internal_error(
            err, self.chk.errors.file, node.line, self.chk.errors, self.chk.options
        )
    self.is_callee = old_is_callee
    self.type_context.pop()
    assert typ is not None
    self.chk.store_type(node, typ)

    if (
        self.chk.options.disallow_any_expr
        and not always_allow_any
        and not self.chk.is_stub
        and self.chk.in_checked_function()
        and has_any_type(typ)
        and not self.chk.current_node_deferred
    ):
        self.msg.disallowed_any_type(typ, node)

    if not self.chk.in_checked_function() or self.chk.current_node_deferred:
        result: Type = AnyType(TypeOfAny.unannotated)
    else:
        result = typ
    if record_time:
        self.per_line_checking_time_ns[node.line] += time.perf_counter_ns() - t0
        self.in_expression = False
    return result

</t>
<t tx="ekr.20230831011819.608">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with type given by the name and no type
    arguments. Alias for TypeChecker.named_type.
    """
    return self.chk.named_type(name)

</t>
<t tx="ekr.20230831011819.609">def is_valid_var_arg(self, typ: Type) -&gt; bool:
    """Is a type valid as a *args argument?"""
    typ = get_proper_type(typ)
    return isinstance(typ, (TupleType, AnyType, ParamSpecType, UnpackType)) or is_subtype(
        typ, self.chk.named_generic_type("typing.Iterable", [AnyType(TypeOfAny.special_form)])
    )

</t>
<t tx="ekr.20230831011819.61">def __init__(self, manager: BuildManager, graph: Graph) -&gt; None:
    self.manager = manager
    self.graph = graph
    self.files = manager.modules
    self.types = manager.all_types  # Non-empty if export_types True in options
    self.used_cache = manager.cache_enabled
    self.errors: list[str] = []  # Filled in by build if desired


</t>
<t tx="ekr.20230831011819.610">def is_valid_keyword_var_arg(self, typ: Type) -&gt; bool:
    """Is a type valid as a **kwargs argument?"""
    return (
        is_subtype(
            typ,
            self.chk.named_generic_type(
                "_typeshed.SupportsKeysAndGetItem",
                [self.named_type("builtins.str"), AnyType(TypeOfAny.special_form)],
            ),
        )
        or is_subtype(
            typ,
            self.chk.named_generic_type(
                "_typeshed.SupportsKeysAndGetItem", [UninhabitedType(), UninhabitedType()]
            ),
        )
        or isinstance(typ, ParamSpecType)
    )

</t>
<t tx="ekr.20230831011819.611">def has_member(self, typ: Type, member: str) -&gt; bool:
    """Does type have member with the given name?"""
    # TODO: refactor this to use checkmember.analyze_member_access, otherwise
    # these two should be carefully kept in sync.
    # This is much faster than analyze_member_access, though, and so using
    # it first as a filter is important for performance.
    typ = get_proper_type(typ)

    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = tuple_fallback(typ)
    if isinstance(typ, LiteralType):
        typ = typ.fallback
    if isinstance(typ, Instance):
        return typ.type.has_readable_member(member)
    if isinstance(typ, FunctionLike) and typ.is_type_obj():
        return typ.fallback.type.has_readable_member(member)
    elif isinstance(typ, AnyType):
        return True
    elif isinstance(typ, UnionType):
        result = all(self.has_member(x, member) for x in typ.relevant_items())
        return result
    elif isinstance(typ, TypeType):
        # Type[Union[X, ...]] is always normalized to Union[Type[X], ...],
        # so we don't need to care about unions here.
        item = typ.item
        if isinstance(item, TypeVarType):
            item = get_proper_type(item.upper_bound)
        if isinstance(item, TupleType):
            item = tuple_fallback(item)
        if isinstance(item, Instance) and item.type.metaclass_type is not None:
            return self.has_member(item.type.metaclass_type, member)
        if isinstance(item, AnyType):
            return True
        return False
    else:
        return False

</t>
<t tx="ekr.20230831011819.612">def not_ready_callback(self, name: str, context: Context) -&gt; None:
    """Called when we can't infer the type of a variable because it's not ready yet.

    Either defer type checking of the enclosing function to the next
    pass or report an error.
    """
    self.chk.handle_cannot_determine_type(name, context)

</t>
<t tx="ekr.20230831011819.613">def visit_yield_expr(self, e: YieldExpr) -&gt; Type:
    return_type = self.chk.return_types[-1]
    expected_item_type = self.chk.get_generator_yield_type(return_type, False)
    if e.expr is None:
        if (
            not isinstance(get_proper_type(expected_item_type), (NoneType, AnyType))
            and self.chk.in_checked_function()
        ):
            self.chk.fail(message_registry.YIELD_VALUE_EXPECTED, e)
    else:
        actual_item_type = self.accept(e.expr, expected_item_type)
        self.chk.check_subtype(
            actual_item_type,
            expected_item_type,
            e,
            message_registry.INCOMPATIBLE_TYPES_IN_YIELD,
            "actual type",
            "expected type",
        )
    return self.chk.get_generator_receive_type(return_type, False)

</t>
<t tx="ekr.20230831011819.614">def visit_await_expr(self, e: AwaitExpr, allow_none_return: bool = False) -&gt; Type:
    expected_type = self.type_context[-1]
    if expected_type is not None:
        expected_type = self.chk.named_generic_type("typing.Awaitable", [expected_type])
    actual_type = get_proper_type(self.accept(e.expr, expected_type))
    if isinstance(actual_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=actual_type)
    ret = self.check_awaitable_expr(
        actual_type, e, message_registry.INCOMPATIBLE_TYPES_IN_AWAIT
    )
    if not allow_none_return and isinstance(get_proper_type(ret), NoneType):
        self.chk.msg.does_not_return_value(None, e)
    return ret

</t>
<t tx="ekr.20230831011819.615">def check_awaitable_expr(
    self, t: Type, ctx: Context, msg: str | ErrorMessage, ignore_binder: bool = False
) -&gt; Type:
    """Check the argument to `await` and extract the type of value.

    Also used by `async for` and `async with`.
    """
    if not self.chk.check_subtype(
        t, self.named_type("typing.Awaitable"), ctx, msg, "actual type", "expected type"
    ):
        return AnyType(TypeOfAny.special_form)
    else:
        generator = self.check_method_call_by_name("__await__", t, [], [], ctx)[0]
        ret_type = self.chk.get_generator_return_type(generator, False)
        ret_type = get_proper_type(ret_type)
        if (
            not ignore_binder
            and isinstance(ret_type, UninhabitedType)
            and not ret_type.ambiguous
        ):
            self.chk.binder.unreachable()
        return ret_type

</t>
<t tx="ekr.20230831011819.616">def visit_yield_from_expr(self, e: YieldFromExpr, allow_none_return: bool = False) -&gt; Type:
    # NOTE: Whether `yield from` accepts an `async def` decorated
    # with `@types.coroutine` (or `@asyncio.coroutine`) depends on
    # whether the generator containing the `yield from` is itself
    # thus decorated.  But it accepts a generator regardless of
    # how it's decorated.
    return_type = self.chk.return_types[-1]
    # TODO: What should the context for the sub-expression be?
    # If the containing function has type Generator[X, Y, ...],
    # the context should be Generator[X, Y, T], where T is the
    # context of the 'yield from' itself (but it isn't known).
    subexpr_type = get_proper_type(self.accept(e.expr))

    # Check that the expr is an instance of Iterable and get the type of the iterator produced
    # by __iter__.
    if isinstance(subexpr_type, AnyType):
        iter_type: Type = AnyType(TypeOfAny.from_another_any, source_any=subexpr_type)
    elif self.chk.type_is_iterable(subexpr_type):
        if is_async_def(subexpr_type) and not has_coroutine_decorator(return_type):
            self.chk.msg.yield_from_invalid_operand_type(subexpr_type, e)

        any_type = AnyType(TypeOfAny.special_form)
        generic_generator_type = self.chk.named_generic_type(
            "typing.Generator", [any_type, any_type, any_type]
        )
        iter_type, _ = self.check_method_call_by_name(
            "__iter__", subexpr_type, [], [], context=generic_generator_type
        )
    else:
        if not (is_async_def(subexpr_type) and has_coroutine_decorator(return_type)):
            self.chk.msg.yield_from_invalid_operand_type(subexpr_type, e)
            iter_type = AnyType(TypeOfAny.from_error)
        else:
            iter_type = self.check_awaitable_expr(
                subexpr_type, e, message_registry.INCOMPATIBLE_TYPES_IN_YIELD_FROM
            )

    # Check that the iterator's item type matches the type yielded by the Generator function
    # containing this `yield from` expression.
    expected_item_type = self.chk.get_generator_yield_type(return_type, False)
    actual_item_type = self.chk.get_generator_yield_type(iter_type, False)

    self.chk.check_subtype(
        actual_item_type,
        expected_item_type,
        e,
        message_registry.INCOMPATIBLE_TYPES_IN_YIELD_FROM,
        "actual type",
        "expected type",
    )

    # Determine the type of the entire yield from expression.
    iter_type = get_proper_type(iter_type)
    if isinstance(iter_type, Instance) and iter_type.type.fullname == "typing.Generator":
        expr_type = self.chk.get_generator_return_type(iter_type, False)
    else:
        # Non-Generators don't return anything from `yield from` expressions.
        # However special-case Any (which might be produced by an error).
        actual_item_type = get_proper_type(actual_item_type)
        if isinstance(actual_item_type, AnyType):
            expr_type = AnyType(TypeOfAny.from_another_any, source_any=actual_item_type)
        else:
            # Treat `Iterator[X]` as a shorthand for `Generator[X, None, Any]`.
            expr_type = NoneType()

    if not allow_none_return and isinstance(get_proper_type(expr_type), NoneType):
        self.chk.msg.does_not_return_value(None, e)
    return expr_type

</t>
<t tx="ekr.20230831011819.617">def visit_temp_node(self, e: TempNode) -&gt; Type:
    return e.type

</t>
<t tx="ekr.20230831011819.618">def visit_type_var_expr(self, e: TypeVarExpr) -&gt; Type:
    p_default = get_proper_type(e.default)
    if not (
        isinstance(p_default, AnyType)
        and p_default.type_of_any == TypeOfAny.from_omitted_generics
    ):
        if not is_subtype(p_default, e.upper_bound):
            self.chk.fail("TypeVar default must be a subtype of the bound type", e)
        if e.values and not any(p_default == value for value in e.values):
            self.chk.fail("TypeVar default must be one of the constraint types", e)
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.619">def visit_paramspec_expr(self, e: ParamSpecExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.62">def build(
    sources: list[BuildSource],
    options: Options,
    alt_lib_path: str | None = None,
    flush_errors: Callable[[list[str], bool], None] | None = None,
    fscache: FileSystemCache | None = None,
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
    extra_plugins: Sequence[Plugin] | None = None,
) -&gt; BuildResult:
    """Analyze a program.

    A single call to build performs parsing, semantic analysis and optionally
    type checking for the program *and* all imported modules, recursively.

    Return BuildResult if successful or only non-blocking errors were found;
    otherwise raise CompileError.

    If a flush_errors callback is provided, all error messages will be
    passed to it and the errors and messages fields of BuildResult and
    CompileError (respectively) will be empty. Otherwise those fields will
    report any error messages.

    Args:
      sources: list of sources to build
      options: build options
      alt_lib_path: an additional directory for looking up library modules
        (takes precedence over other directories)
      flush_errors: optional function to flush errors after a file is processed
      fscache: optionally a file-system cacher

    """
    # If we were not given a flush_errors, we use one that will populate those
    # fields for callers that want the traditional API.
    messages = []

    def default_flush_errors(new_messages: list[str], is_serious: bool) -&gt; None:
        messages.extend(new_messages)

    flush_errors = flush_errors or default_flush_errors
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr
    extra_plugins = extra_plugins or []

    try:
        result = _build(
            sources, options, alt_lib_path, flush_errors, fscache, stdout, stderr, extra_plugins
        )
        result.errors = messages
        return result
    except CompileError as e:
        # CompileErrors raised from an errors object carry all of the
        # messages that have not been reported out by error streaming.
        # Patch it up to contain either none or all none of the messages,
        # depending on whether we are flushing errors.
        serious = not e.use_stdout
        flush_errors(e.messages, serious)
        e.messages = messages
        raise


</t>
<t tx="ekr.20230831011819.620">def visit_type_var_tuple_expr(self, e: TypeVarTupleExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.621">def visit_newtype_expr(self, e: NewTypeExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.622">def visit_namedtuple_expr(self, e: NamedTupleExpr) -&gt; Type:
    tuple_type = e.info.tuple_type
    if tuple_type:
        if self.chk.options.disallow_any_unimported and has_any_from_unimported_type(
            tuple_type
        ):
            self.msg.unimported_type_becomes_any("NamedTuple type", tuple_type, e)
        check_for_explicit_any(
            tuple_type, self.chk.options, self.chk.is_typeshed_stub, self.msg, context=e
        )
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.623">def visit_enum_call_expr(self, e: EnumCallExpr) -&gt; Type:
    for name, value in zip(e.items, e.values):
        if value is not None:
            typ = self.accept(value)
            if not isinstance(get_proper_type(typ), AnyType):
                var = e.info.names[name].node
                if isinstance(var, Var):
                    # Inline TypeChecker.set_inferred_type(),
                    # without the lvalue.  (This doesn't really do
                    # much, since the value attribute is defined
                    # to have type Any in the typeshed stub.)
                    var.type = typ
                    var.is_inferred = True
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.624">def visit_typeddict_expr(self, e: TypedDictExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.625">def visit__promote_expr(self, e: PromoteExpr) -&gt; Type:
    return e.type

</t>
<t tx="ekr.20230831011819.626">def visit_star_expr(self, e: StarExpr) -&gt; Type:
    # TODO: should this ever be called (see e.g. mypyc visitor)?
    return self.accept(e.expr)

</t>
<t tx="ekr.20230831011819.627">def object_type(self) -&gt; Instance:
    """Return instance type 'object'."""
    return self.named_type("builtins.object")

</t>
<t tx="ekr.20230831011819.628">def bool_type(self) -&gt; Instance:
    """Return instance type 'bool'."""
    return self.named_type("builtins.bool")

</t>
<t tx="ekr.20230831011819.629">@overload
def narrow_type_from_binder(self, expr: Expression, known_type: Type) -&gt; Type:
    ...

</t>
<t tx="ekr.20230831011819.63">def _build(
    sources: list[BuildSource],
    options: Options,
    alt_lib_path: str | None,
    flush_errors: Callable[[list[str], bool], None],
    fscache: FileSystemCache | None,
    stdout: TextIO,
    stderr: TextIO,
    extra_plugins: Sequence[Plugin],
) -&gt; BuildResult:
    if platform.python_implementation() == "CPython":
        # This seems the most reasonable place to tune garbage collection.
        gc.set_threshold(150 * 1000)

    data_dir = default_data_dir()
    fscache = fscache or FileSystemCache()

    search_paths = compute_search_paths(sources, options, data_dir, alt_lib_path)

    reports = None
    if options.report_dirs:
        # Import lazily to avoid slowing down startup.
        from mypy.report import Reports

        reports = Reports(data_dir, options.report_dirs)

    source_set = BuildSourceSet(sources)
    cached_read = fscache.read
    errors = Errors(options, read_source=lambda path: read_py_file(path, cached_read))
    plugin, snapshot = load_plugins(options, errors, stdout, extra_plugins)

    # Add catch-all .gitignore to cache dir if we created it
    cache_dir_existed = os.path.isdir(options.cache_dir)

    # Construct a build manager object to hold state during the build.
    #
    # Ignore current directory prefix in error messages.
    manager = BuildManager(
        data_dir,
        search_paths,
        ignore_prefix=os.getcwd(),
        source_set=source_set,
        reports=reports,
        options=options,
        version_id=__version__,
        plugin=plugin,
        plugins_snapshot=snapshot,
        errors=errors,
        flush_errors=flush_errors,
        fscache=fscache,
        stdout=stdout,
        stderr=stderr,
    )
    manager.trace(repr(options))

    reset_global_state()
    try:
        graph = dispatch(sources, manager, stdout)
        if not options.fine_grained_incremental:
            type_state.reset_all_subtype_caches()
        if options.timing_stats is not None:
            dump_timing_stats(options.timing_stats, graph)
        if options.line_checking_stats is not None:
            dump_line_checking_stats(options.line_checking_stats, graph)
        return BuildResult(manager, graph)
    finally:
        t0 = time.time()
        manager.metastore.commit()
        manager.add_stats(cache_commit_time=time.time() - t0)
        manager.log(
            "Build finished in %.3f seconds with %d modules, and %d errors"
            % (
                time.time() - manager.start_time,
                len(manager.modules),
                manager.errors.num_messages(),
            )
        )
        manager.dump_stats()
        if reports is not None:
            # Finish the HTML or XML reports even if CompileError was raised.
            reports.finish()
        if not cache_dir_existed and os.path.isdir(options.cache_dir):
            add_catch_all_gitignore(options.cache_dir)
            exclude_from_backups(options.cache_dir)
        if os.path.isdir(options.cache_dir):
            record_missing_stub_packages(options.cache_dir, manager.missing_stub_packages)


</t>
<t tx="ekr.20230831011819.630">@overload
def narrow_type_from_binder(
    self, expr: Expression, known_type: Type, skip_non_overlapping: bool
) -&gt; Type | None:
    ...

</t>
<t tx="ekr.20230831011819.631">def narrow_type_from_binder(
    self, expr: Expression, known_type: Type, skip_non_overlapping: bool = False
) -&gt; Type | None:
    """Narrow down a known type of expression using information in conditional type binder.

    If 'skip_non_overlapping' is True, return None if the type and restriction are
    non-overlapping.
    """
    if literal(expr) &gt;= LITERAL_TYPE:
        restriction = self.chk.binder.get(expr)
        # If the current node is deferred, some variables may get Any types that they
        # otherwise wouldn't have. We don't want to narrow down these since it may
        # produce invalid inferred Optional[Any] types, at least.
        if restriction and not (
            isinstance(get_proper_type(known_type), AnyType) and self.chk.current_node_deferred
        ):
            # Note: this call should match the one in narrow_declared_type().
            if skip_non_overlapping and not is_overlapping_types(
                known_type, restriction, prohibit_none_typevar_overlap=True
            ):
                return None
            return narrow_declared_type(known_type, restriction)
    return known_type

</t>
<t tx="ekr.20230831011819.632">def has_abstract_type_part(self, caller_type: ProperType, callee_type: ProperType) -&gt; bool:
    # TODO: support other possible types here
    if isinstance(caller_type, TupleType) and isinstance(callee_type, TupleType):
        return any(
            self.has_abstract_type(get_proper_type(caller), get_proper_type(callee))
            for caller, callee in zip(caller_type.items, callee_type.items)
        )
    return self.has_abstract_type(caller_type, callee_type)

</t>
<t tx="ekr.20230831011819.633">def has_abstract_type(self, caller_type: ProperType, callee_type: ProperType) -&gt; bool:
    return (
        isinstance(caller_type, CallableType)
        and isinstance(callee_type, TypeType)
        and caller_type.is_type_obj()
        and (caller_type.type_object().is_abstract or caller_type.type_object().is_protocol)
        and isinstance(callee_type.item, Instance)
        and (callee_type.item.type.is_abstract or callee_type.item.type.is_protocol)
        and not self.chk.allow_abstract_call
    )


</t>
<t tx="ekr.20230831011819.634">def has_any_type(t: Type, ignore_in_type_obj: bool = False) -&gt; bool:
    """Whether t contains an Any type"""
    return t.accept(HasAnyType(ignore_in_type_obj))


</t>
<t tx="ekr.20230831011819.635">class HasAnyType(types.BoolTypeQuery):
    @others
</t>
<t tx="ekr.20230831011819.636">def __init__(self, ignore_in_type_obj: bool) -&gt; None:
    super().__init__(types.ANY_STRATEGY)
    self.ignore_in_type_obj = ignore_in_type_obj

</t>
<t tx="ekr.20230831011819.637">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any != TypeOfAny.special_form  # special forms are not real Any types

</t>
<t tx="ekr.20230831011819.638">def visit_callable_type(self, t: CallableType) -&gt; bool:
    if self.ignore_in_type_obj and t.is_type_obj():
        return False
    return super().visit_callable_type(t)

</t>
<t tx="ekr.20230831011819.639">def visit_type_var(self, t: TypeVarType) -&gt; bool:
    default = [t.default] if t.has_default() else []
    return self.query_types([t.upper_bound, *default] + t.values)

</t>
<t tx="ekr.20230831011819.64">def default_data_dir() -&gt; str:
    """Returns directory containing typeshed directory."""
    return os.path.dirname(__file__)


</t>
<t tx="ekr.20230831011819.640">def visit_param_spec(self, t: ParamSpecType) -&gt; bool:
    default = [t.default] if t.has_default() else []
    return self.query_types([t.upper_bound, *default])

</t>
<t tx="ekr.20230831011819.641">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; bool:
    default = [t.default] if t.has_default() else []
    return self.query_types([t.upper_bound, *default])


</t>
<t tx="ekr.20230831011819.642">def has_coroutine_decorator(t: Type) -&gt; bool:
    """Whether t came from a function decorated with `@coroutine`."""
    t = get_proper_type(t)
    return isinstance(t, Instance) and t.type.fullname == "typing.AwaitableGenerator"


</t>
<t tx="ekr.20230831011819.643">def is_async_def(t: Type) -&gt; bool:
    """Whether t came from a function defined using `async def`."""
    # In check_func_def(), when we see a function decorated with
    # `@typing.coroutine` or `@async.coroutine`, we change the
    # return type to typing.AwaitableGenerator[...], so that its
    # type is compatible with either Generator or Awaitable.
    # But for the check here we need to know whether the original
    # function (before decoration) was an `async def`.  The
    # AwaitableGenerator type conveniently preserves the original
    # type as its 4th parameter (3rd when using 0-origin indexing
    # :-), so that we can recover that information here.
    # (We really need to see whether the original, undecorated
    # function was an `async def`, which is orthogonal to its
    # decorations.)
    t = get_proper_type(t)
    if (
        isinstance(t, Instance)
        and t.type.fullname == "typing.AwaitableGenerator"
        and len(t.args) &gt;= 4
    ):
        t = get_proper_type(t.args[3])
    return isinstance(t, Instance) and t.type.fullname == "typing.Coroutine"


</t>
<t tx="ekr.20230831011819.644">def is_non_empty_tuple(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, TupleType) and bool(t.items)


</t>
<t tx="ekr.20230831011819.645">def is_duplicate_mapping(
    mapping: list[int], actual_types: list[Type], actual_kinds: list[ArgKind]
) -&gt; bool:
    return (
        len(mapping) &gt; 1
        # Multiple actuals can map to the same formal if they both come from
        # varargs (*args and **kwargs); in this case at runtime it is possible
        # that here are no duplicates. We need to allow this, as the convention
        # f(..., *args, **kwargs) is common enough.
        and not (
            len(mapping) == 2
            and actual_kinds[mapping[0]] == nodes.ARG_STAR
            and actual_kinds[mapping[1]] == nodes.ARG_STAR2
        )
        # Multiple actuals can map to the same formal if there are multiple
        # **kwargs which cannot be mapped with certainty (non-TypedDict
        # **kwargs).
        and not all(
            actual_kinds[m] == nodes.ARG_STAR2
            and not isinstance(get_proper_type(actual_types[m]), TypedDictType)
            for m in mapping
        )
    )


</t>
<t tx="ekr.20230831011819.646">def replace_callable_return_type(c: CallableType, new_ret_type: Type) -&gt; CallableType:
    """Return a copy of a callable type with a different return type."""
    return c.copy_modified(ret_type=new_ret_type)


</t>
<t tx="ekr.20230831011819.647">def apply_poly(tp: CallableType, poly_tvars: Sequence[TypeVarLikeType]) -&gt; CallableType | None:
    """Make free type variables generic in the type if possible.

    This will translate the type `tp` while trying to create valid bindings for
    type variables `poly_tvars` while traversing the type. This follows the same rules
    as we do during semantic analysis phase, examples:
      * Callable[Callable[[T], T], T] -&gt; def [T] (def (T) -&gt; T) -&gt; T
      * Callable[[], Callable[[T], T]] -&gt; def () -&gt; def [T] (T -&gt; T)
      * List[T] -&gt; None (not possible)
    """
    try:
        return tp.copy_modified(
            arg_types=[t.accept(PolyTranslator(poly_tvars)) for t in tp.arg_types],
            ret_type=tp.ret_type.accept(PolyTranslator(poly_tvars)),
            variables=[],
        )
    except PolyTranslationError:
        return None


</t>
<t tx="ekr.20230831011819.648">class PolyTranslationError(Exception):
    pass


</t>
<t tx="ekr.20230831011819.649">class PolyTranslator(TypeTranslator):
    """Make free type variables generic in the type if possible.

    See docstring for apply_poly() for details.
    """

    @others
</t>
<t tx="ekr.20230831011819.65">def normpath(path: str, options: Options) -&gt; str:
    """Convert path to absolute; but to relative in bazel mode.

    (Bazel's distributed cache doesn't like filesystem metadata to
    end up in output files.)
    """
    # TODO: Could we always use relpath?  (A worry in non-bazel
    # mode would be that a moved file may change its full module
    # name without changing its size, mtime or hash.)
    if options.bazel:
        return os.path.relpath(path)
    else:
        return os.path.abspath(path)


</t>
<t tx="ekr.20230831011819.650">def __init__(self, poly_tvars: Sequence[TypeVarLikeType]) -&gt; None:
    self.poly_tvars = set(poly_tvars)
    # This is a simplified version of TypeVarScope used during semantic analysis.
    self.bound_tvars: set[TypeVarLikeType] = set()
    self.seen_aliases: set[TypeInfo] = set()

</t>
<t tx="ekr.20230831011819.651">def collect_vars(self, t: CallableType | Parameters) -&gt; list[TypeVarLikeType]:
    found_vars = []
    for arg in t.arg_types:
        for tv in get_all_type_vars(arg):
            if isinstance(tv, ParamSpecType):
                normalized: TypeVarLikeType = tv.copy_modified(
                    flavor=ParamSpecFlavor.BARE, prefix=Parameters([], [], [])
                )
            else:
                normalized = tv
            if normalized in self.poly_tvars and normalized not in self.bound_tvars:
                found_vars.append(normalized)
    return remove_dups(found_vars)

</t>
<t tx="ekr.20230831011819.652">def visit_callable_type(self, t: CallableType) -&gt; Type:
    found_vars = self.collect_vars(t)
    self.bound_tvars |= set(found_vars)
    result = super().visit_callable_type(t)
    self.bound_tvars -= set(found_vars)

    assert isinstance(result, ProperType) and isinstance(result, CallableType)
    result.variables = list(result.variables) + found_vars
    return result

</t>
<t tx="ekr.20230831011819.653">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    if t in self.poly_tvars and t not in self.bound_tvars:
        raise PolyTranslationError()
    return super().visit_type_var(t)

</t>
<t tx="ekr.20230831011819.654">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    if t in self.poly_tvars and t not in self.bound_tvars:
        raise PolyTranslationError()
    return super().visit_param_spec(t)

</t>
<t tx="ekr.20230831011819.655">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    if t in self.poly_tvars and t not in self.bound_tvars:
        raise PolyTranslationError()
    return super().visit_type_var_tuple(t)

</t>
<t tx="ekr.20230831011819.656">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    if not t.args:
        return t.copy_modified()
    if not t.is_recursive:
        return get_proper_type(t).accept(self)
    # We can't handle polymorphic application for recursive generic aliases
    # without risking an infinite recursion, just give up for now.
    raise PolyTranslationError()

</t>
<t tx="ekr.20230831011819.657">def visit_instance(self, t: Instance) -&gt; Type:
    if t.type.has_param_spec_type:
        # We need this special-casing to preserve the possibility to store a
        # generic function in an instance type. Things like
        #     forall T . Foo[[x: T], T]
        # are not really expressible in current type system, but this looks like
        # a useful feature, so let's keep it.
        param_spec_index = next(
            i for (i, tv) in enumerate(t.type.defn.type_vars) if isinstance(tv, ParamSpecType)
        )
        p = get_proper_type(t.args[param_spec_index])
        if isinstance(p, Parameters):
            found_vars = self.collect_vars(p)
            self.bound_tvars |= set(found_vars)
            new_args = [a.accept(self) for a in t.args]
            self.bound_tvars -= set(found_vars)

            repl = new_args[param_spec_index]
            assert isinstance(repl, ProperType) and isinstance(repl, Parameters)
            repl.variables = list(repl.variables) + list(found_vars)
            return t.copy_modified(args=new_args)
    # There is the same problem with callback protocols as with aliases
    # (callback protocols are essentially more flexible aliases to callables).
    if t.args and t.type.is_protocol and t.type.protocol_members == ["__call__"]:
        if t.type in self.seen_aliases:
            raise PolyTranslationError()
        self.seen_aliases.add(t.type)
        call = find_member("__call__", t, t, is_operator=True)
        assert call is not None
        return call.accept(self)
    return super().visit_instance(t)


</t>
<t tx="ekr.20230831011819.658">class ArgInferSecondPassQuery(types.BoolTypeQuery):
    """Query whether an argument type should be inferred in the second pass.

    The result is True if the type has a type variable in a callable return
    type anywhere. For example, the result for Callable[[], T] is True if t is
    a type variable.
    """

    @others
</t>
<t tx="ekr.20230831011819.659">def __init__(self) -&gt; None:
    super().__init__(types.ANY_STRATEGY)

</t>
<t tx="ekr.20230831011819.66">class CacheMeta(NamedTuple):
    id: str
    path: str
    mtime: int
    size: int
    hash: str
    dependencies: list[str]  # names of imported modules
    data_mtime: int  # mtime of data_json
    data_json: str  # path of &lt;id&gt;.data.json
    suppressed: list[str]  # dependencies that weren't imported
    options: dict[str, object] | None  # build options
    # dep_prios and dep_lines are in parallel with dependencies + suppressed
    dep_prios: list[int]
    dep_lines: list[int]
    interface_hash: str  # hash representing the public interface
    version_id: str  # mypy version for cache invalidation
    ignore_all: bool  # if errors were ignored
    plugin_data: Any  # config data from plugins


</t>
<t tx="ekr.20230831011819.660">def visit_callable_type(self, t: CallableType) -&gt; bool:
    return self.query_types(t.arg_types) or t.accept(HasTypeVarQuery())


</t>
<t tx="ekr.20230831011819.661">class HasTypeVarQuery(types.BoolTypeQuery):
    """Visitor for querying whether a type has a type variable component."""

    @others
</t>
<t tx="ekr.20230831011819.662">def __init__(self) -&gt; None:
    super().__init__(types.ANY_STRATEGY)

</t>
<t tx="ekr.20230831011819.663">def visit_type_var(self, t: TypeVarType) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011819.664">def visit_param_spec(self, t: ParamSpecType) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011819.665">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; bool:
    return True


</t>
<t tx="ekr.20230831011819.666">def has_erased_component(t: Type | None) -&gt; bool:
    return t is not None and t.accept(HasErasedComponentsQuery())


</t>
<t tx="ekr.20230831011819.667">class HasErasedComponentsQuery(types.BoolTypeQuery):
    """Visitor for querying whether a type has an erased component."""

    @others
</t>
<t tx="ekr.20230831011819.668">def __init__(self) -&gt; None:
    super().__init__(types.ANY_STRATEGY)

</t>
<t tx="ekr.20230831011819.669">def visit_erased_type(self, t: ErasedType) -&gt; bool:
    return True


</t>
<t tx="ekr.20230831011819.67"># NOTE: dependencies + suppressed == all reachable imports;
# suppressed contains those reachable imports that were prevented by
# silent mode or simply not found.


# Metadata for the fine-grained dependencies file associated with a module.
class FgDepMeta(TypedDict):
    path: str
    mtime: int


</t>
<t tx="ekr.20230831011819.670">def has_uninhabited_component(t: Type | None) -&gt; bool:
    return t is not None and t.accept(HasUninhabitedComponentsQuery())


</t>
<t tx="ekr.20230831011819.671">class HasUninhabitedComponentsQuery(types.BoolTypeQuery):
    """Visitor for querying whether a type has an UninhabitedType component."""

    @others
</t>
<t tx="ekr.20230831011819.672">def __init__(self) -&gt; None:
    super().__init__(types.ANY_STRATEGY)

</t>
<t tx="ekr.20230831011819.673">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return True


</t>
<t tx="ekr.20230831011819.674">def arg_approximate_similarity(actual: Type, formal: Type) -&gt; bool:
    """Return if caller argument (actual) is roughly compatible with signature arg (formal).

    This function is deliberately loose and will report two types are similar
    as long as their "shapes" are plausibly the same.

    This is useful when we're doing error reporting: for example, if we're trying
    to select an overload alternative and there's no exact match, we can use
    this function to help us identify which alternative the user might have
    *meant* to match.
    """
    actual = get_proper_type(actual)
    formal = get_proper_type(formal)

    # Erase typevars: we'll consider them all to have the same "shape".
    if isinstance(actual, TypeVarType):
        actual = erase_to_union_or_bound(actual)
    if isinstance(formal, TypeVarType):
        formal = erase_to_union_or_bound(formal)

    # Callable or Type[...]-ish types
    def is_typetype_like(typ: ProperType) -&gt; bool:
        return (
            isinstance(typ, TypeType)
            or (isinstance(typ, FunctionLike) and typ.is_type_obj())
            or (isinstance(typ, Instance) and typ.type.fullname == "builtins.type")
        )

    if isinstance(formal, CallableType):
        if isinstance(actual, (CallableType, Overloaded, TypeType)):
            return True
    if is_typetype_like(actual) and is_typetype_like(formal):
        return True

    # Unions
    if isinstance(actual, UnionType):
        return any(arg_approximate_similarity(item, formal) for item in actual.relevant_items())
    if isinstance(formal, UnionType):
        return any(arg_approximate_similarity(actual, item) for item in formal.relevant_items())

    # TypedDicts
    if isinstance(actual, TypedDictType):
        if isinstance(formal, TypedDictType):
            return True
        return arg_approximate_similarity(actual.fallback, formal)

    # Instances
    # For instances, we mostly defer to the existing is_subtype check.
    if isinstance(formal, Instance):
        if isinstance(actual, CallableType):
            actual = actual.fallback
        if isinstance(actual, Overloaded):
            actual = actual.items[0].fallback
        if isinstance(actual, TupleType):
            actual = tuple_fallback(actual)
        if isinstance(actual, Instance) and formal.type in actual.type.mro:
            # Try performing a quick check as an optimization
            return True

    # Fall back to a standard subtype check for the remaining kinds of type.
    return is_subtype(erasetype.erase_type(actual), erasetype.erase_type(formal))


</t>
<t tx="ekr.20230831011819.675">def any_causes_overload_ambiguity(
    items: list[CallableType],
    return_types: list[Type],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
) -&gt; bool:
    """May an argument containing 'Any' cause ambiguous result type on call to overloaded function?

    Note that this sometimes returns True even if there is no ambiguity, since a correct
    implementation would be complex (and the call would be imprecisely typed due to Any
    types anyway).

    Args:
        items: Overload items matching the actual arguments
        arg_types: Actual argument types
        arg_kinds: Actual argument kinds
        arg_names: Actual argument names
    """
    if all_same_types(return_types):
        return False

    actual_to_formal = [
        map_formals_to_actuals(
            arg_kinds, arg_names, item.arg_kinds, item.arg_names, lambda i: arg_types[i]
        )
        for item in items
    ]

    for arg_idx, arg_type in enumerate(arg_types):
        # We ignore Anys in type object callables as ambiguity
        # creators, since that can lead to falsely claiming ambiguity
        # for overloads between Type and Callable.
        if has_any_type(arg_type, ignore_in_type_obj=True):
            matching_formals_unfiltered = [
                (item_idx, lookup[arg_idx])
                for item_idx, lookup in enumerate(actual_to_formal)
                if lookup[arg_idx]
            ]

            matching_returns = []
            matching_formals = []
            for item_idx, formals in matching_formals_unfiltered:
                matched_callable = items[item_idx]
                matching_returns.append(matched_callable.ret_type)

                # Note: if an actual maps to multiple formals of differing types within
                # a single callable, then we know at least one of those formals must be
                # a different type then the formal(s) in some other callable.
                # So it's safe to just append everything to the same list.
                for formal in formals:
                    matching_formals.append(matched_callable.arg_types[formal])
            if not all_same_types(matching_formals) and not all_same_types(matching_returns):
                # Any maps to multiple different types, and the return types of these items differ.
                return True
    return False


</t>
<t tx="ekr.20230831011819.676">def all_same_types(types: list[Type]) -&gt; bool:
    if not types:
        return True
    return all(is_same_type(t, types[0]) for t in types[1:])


</t>
<t tx="ekr.20230831011819.677">def merge_typevars_in_callables_by_name(
    callables: Sequence[CallableType],
) -&gt; tuple[list[CallableType], list[TypeVarType]]:
    """Takes all the typevars present in the callables and 'combines' the ones with the same name.

    For example, suppose we have two callables with signatures "f(x: T, y: S) -&gt; T" and
    "f(x: List[Tuple[T, S]]) -&gt; Tuple[T, S]". Both callables use typevars named "T" and
    "S", but we treat them as distinct, unrelated typevars. (E.g. they could both have
    distinct ids.)

    If we pass in both callables into this function, it returns a list containing two
    new callables that are identical in signature, but use the same underlying TypeVarType
    for T and S.

    This is useful if we want to take the output lists and "merge" them into one callable
    in some way -- for example, when unioning together overloads.

    Returns both the new list of callables and a list of all distinct TypeVarType objects used.
    """
    output: list[CallableType] = []
    unique_typevars: dict[str, TypeVarType] = {}
    variables: list[TypeVarType] = []

    for target in callables:
        if target.is_generic():
            target = freshen_function_type_vars(target)

            rename = {}  # Dict[TypeVarId, TypeVar]
            for tv in target.variables:
                name = tv.fullname
                if name not in unique_typevars:
                    # TODO(PEP612): fix for ParamSpecType
                    if isinstance(tv, ParamSpecType):
                        continue
                    assert isinstance(tv, TypeVarType)
                    unique_typevars[name] = tv
                    variables.append(tv)
                rename[tv.id] = unique_typevars[name]

            target = expand_type(target, rename)
        output.append(target)

    return output, variables


</t>
<t tx="ekr.20230831011819.678">def try_getting_literal(typ: Type) -&gt; ProperType:
    """If possible, get a more precise literal type for a given type."""
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and typ.last_known_value is not None:
        return typ.last_known_value
    return typ


</t>
<t tx="ekr.20230831011819.679">def is_expr_literal_type(node: Expression) -&gt; bool:
    """Returns 'true' if the given node is a Literal"""
    if isinstance(node, IndexExpr):
        base = node.base
        return isinstance(base, RefExpr) and base.fullname in LITERAL_TYPE_NAMES
    if isinstance(node, NameExpr):
        underlying = node.node
        return isinstance(underlying, TypeAlias) and isinstance(
            get_proper_type(underlying.target), LiteralType
        )
    return False


</t>
<t tx="ekr.20230831011819.68">def cache_meta_from_dict(meta: dict[str, Any], data_json: str) -&gt; CacheMeta:
    """Build a CacheMeta object from a json metadata dictionary

    Args:
      meta: JSON metadata read from the metadata cache file
      data_json: Path to the .data.json file containing the AST trees
    """
    sentinel: Any = None  # Values to be validated by the caller
    return CacheMeta(
        meta.get("id", sentinel),
        meta.get("path", sentinel),
        int(meta["mtime"]) if "mtime" in meta else sentinel,
        meta.get("size", sentinel),
        meta.get("hash", sentinel),
        meta.get("dependencies", []),
        int(meta["data_mtime"]) if "data_mtime" in meta else sentinel,
        data_json,
        meta.get("suppressed", []),
        meta.get("options"),
        meta.get("dep_prios", []),
        meta.get("dep_lines", []),
        meta.get("interface_hash", ""),
        meta.get("version_id", sentinel),
        meta.get("ignore_all", True),
        meta.get("plugin_data", None),
    )


</t>
<t tx="ekr.20230831011819.680">def has_bytes_component(typ: Type) -&gt; bool:
    """Is this one of builtin byte types, or a union that contains it?"""
    typ = get_proper_type(typ)
    byte_types = {"builtins.bytes", "builtins.bytearray"}
    if isinstance(typ, UnionType):
        return any(has_bytes_component(t) for t in typ.items)
    if isinstance(typ, Instance) and typ.type.fullname in byte_types:
        return True
    return False


</t>
<t tx="ekr.20230831011819.681">def type_info_from_type(typ: Type) -&gt; TypeInfo | None:
    """Gets the TypeInfo for a type, indirecting through things like type variables and tuples."""
    typ = get_proper_type(typ)
    if isinstance(typ, FunctionLike) and typ.is_type_obj():
        return typ.type_object()
    if isinstance(typ, TypeType):
        typ = typ.item
    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = tuple_fallback(typ)
    if isinstance(typ, Instance):
        return typ.type

    # A complicated type. Too tricky, give up.
    # TODO: Do something more clever here.
    return None


</t>
<t tx="ekr.20230831011819.682">def is_operator_method(fullname: str | None) -&gt; bool:
    if not fullname:
        return False
    short_name = fullname.split(".")[-1]
    return (
        short_name in operators.op_methods.values()
        or short_name in operators.reverse_op_methods.values()
        or short_name in operators.unary_op_methods.values()
    )


</t>
<t tx="ekr.20230831011819.683">def get_partial_instance_type(t: Type | None) -&gt; PartialType | None:
    if t is None or not isinstance(t, PartialType) or t.type is None:
        return None
    return t
</t>
<t tx="ekr.20230831011819.684">@path mypy
"""Type checking of attribute access"""
&lt;&lt; checkmember.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.685">
from __future__ import annotations

from typing import TYPE_CHECKING, Callable, Sequence, cast

from mypy import meet, message_registry, subtypes
from mypy.erasetype import erase_typevars
from mypy.expandtype import (
    expand_self_type,
    expand_type_by_instance,
    freshen_all_functions_type_vars,
)
from mypy.maptype import map_instance_to_supertype
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    SYMBOL_FUNCBASE_TYPES,
    Context,
    Decorator,
    FuncBase,
    FuncDef,
    IndexExpr,
    MypyFile,
    OverloadedFuncDef,
    SymbolNode,
    SymbolTable,
    TempNode,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    Var,
    is_final_node,
)
from mypy.plugin import AttributeContext
from mypy.typeops import (
    bind_self,
    class_callable,
    erase_to_bound,
    function_type,
    get_type_vars,
    make_simplified_union,
    supported_self_type,
    tuple_fallback,
    type_object_type_from_function,
)
from mypy.types import (
    ENUM_REMOVED_PROPS,
    AnyType,
    CallableType,
    DeletedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnionType,
    get_proper_type,
)
from mypy.typetraverser import TypeTraverserVisitor

if TYPE_CHECKING:  # import for forward declaration only
    import mypy.checker

from mypy import state


</t>
<t tx="ekr.20230831011819.686">class MemberContext:
    """Information and objects needed to type check attribute access.

    Look at the docstring of analyze_member_access for more information.
    """

    @others
</t>
<t tx="ekr.20230831011819.687">def __init__(
    self,
    is_lvalue: bool,
    is_super: bool,
    is_operator: bool,
    original_type: Type,
    context: Context,
    msg: MessageBuilder,
    chk: mypy.checker.TypeChecker,
    self_type: Type | None,
    module_symbol_table: SymbolTable | None = None,
    no_deferral: bool = False,
    is_self: bool = False,
) -&gt; None:
    self.is_lvalue = is_lvalue
    self.is_super = is_super
    self.is_operator = is_operator
    self.original_type = original_type
    self.self_type = self_type or original_type
    self.context = context  # Error context
    self.msg = msg
    self.chk = chk
    self.module_symbol_table = module_symbol_table
    self.no_deferral = no_deferral
    self.is_self = is_self

</t>
<t tx="ekr.20230831011819.688">def named_type(self, name: str) -&gt; Instance:
    return self.chk.named_type(name)

</t>
<t tx="ekr.20230831011819.689">def not_ready_callback(self, name: str, context: Context) -&gt; None:
    self.chk.handle_cannot_determine_type(name, context)

</t>
<t tx="ekr.20230831011819.69"># Priorities used for imports.  (Here, top-level includes inside a class.)
# These are used to determine a more predictable order in which the
# nodes in an import cycle are processed.
PRI_HIGH: Final = 5  # top-level "from X import blah"
PRI_MED: Final = 10  # top-level "import X"
PRI_LOW: Final = 20  # either form inside a function
PRI_MYPY: Final = 25  # inside "if MYPY" or "if TYPE_CHECKING"
PRI_INDIRECT: Final = 30  # an indirect dependency
PRI_ALL: Final = 99  # include all priorities


def import_priority(imp: ImportBase, toplevel_priority: int) -&gt; int:
    """Compute import priority from an import node."""
    if not imp.is_top_level:
        # Inside a function
        return PRI_LOW
    if imp.is_mypy_only:
        # Inside "if MYPY" or "if typing.TYPE_CHECKING"
        return max(PRI_MYPY, toplevel_priority)
    # A regular import; priority determined by argument.
    return toplevel_priority


</t>
<t tx="ekr.20230831011819.690">def copy_modified(
    self,
    *,
    messages: MessageBuilder | None = None,
    self_type: Type | None = None,
    is_lvalue: bool | None = None,
) -&gt; MemberContext:
    mx = MemberContext(
        self.is_lvalue,
        self.is_super,
        self.is_operator,
        self.original_type,
        self.context,
        self.msg,
        self.chk,
        self.self_type,
        self.module_symbol_table,
        self.no_deferral,
    )
    if messages is not None:
        mx.msg = messages
    if self_type is not None:
        mx.self_type = self_type
    if is_lvalue is not None:
        mx.is_lvalue = is_lvalue
    return mx


</t>
<t tx="ekr.20230831011819.691">def analyze_member_access(
    name: str,
    typ: Type,
    context: Context,
    is_lvalue: bool,
    is_super: bool,
    is_operator: bool,
    msg: MessageBuilder,
    *,
    original_type: Type,
    chk: mypy.checker.TypeChecker,
    override_info: TypeInfo | None = None,
    in_literal_context: bool = False,
    self_type: Type | None = None,
    module_symbol_table: SymbolTable | None = None,
    no_deferral: bool = False,
    is_self: bool = False,
) -&gt; Type:
    """Return the type of attribute 'name' of 'typ'.

    The actual implementation is in '_analyze_member_access' and this docstring
    also applies to it.

    This is a general operation that supports various different variations:

      1. lvalue or non-lvalue access (setter or getter access)
      2. supertype access when using super() (is_super == True and
         'override_info' should refer to the supertype)

    'original_type' is the most precise inferred or declared type of the base object
    that we have available. When looking for an attribute of 'typ', we may perform
    recursive calls targeting the fallback type, and 'typ' may become some supertype
    of 'original_type'. 'original_type' is always preserved as the 'typ' type used in
    the initial, non-recursive call. The 'self_type' is a component of 'original_type'
    to which generic self should be bound (a narrower type that has a fallback to instance).
    Currently this is used only for union types.

    'module_symbol_table' is passed to this function if 'typ' is actually a module
    and we want to keep track of the available attributes of the module (since they
    are not available via the type object directly)
    """
    mx = MemberContext(
        is_lvalue,
        is_super,
        is_operator,
        original_type,
        context,
        msg,
        chk=chk,
        self_type=self_type,
        module_symbol_table=module_symbol_table,
        no_deferral=no_deferral,
        is_self=is_self,
    )
    result = _analyze_member_access(name, typ, mx, override_info)
    possible_literal = get_proper_type(result)
    if (
        in_literal_context
        and isinstance(possible_literal, Instance)
        and possible_literal.last_known_value is not None
    ):
        return possible_literal.last_known_value
    else:
        return result


</t>
<t tx="ekr.20230831011819.692">def _analyze_member_access(
    name: str, typ: Type, mx: MemberContext, override_info: TypeInfo | None = None
) -&gt; Type:
    # TODO: This and following functions share some logic with subtypes.find_member;
    #       consider refactoring.
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return analyze_instance_member_access(name, typ, mx, override_info)
    elif isinstance(typ, AnyType):
        # The base object has dynamic type.
        return AnyType(TypeOfAny.from_another_any, source_any=typ)
    elif isinstance(typ, UnionType):
        return analyze_union_member_access(name, typ, mx)
    elif isinstance(typ, FunctionLike) and typ.is_type_obj():
        return analyze_type_callable_member_access(name, typ, mx)
    elif isinstance(typ, TypeType):
        return analyze_type_type_member_access(name, typ, mx, override_info)
    elif isinstance(typ, TupleType):
        # Actually look up from the fallback instance type.
        return _analyze_member_access(name, tuple_fallback(typ), mx, override_info)
    elif isinstance(typ, (LiteralType, FunctionLike)):
        # Actually look up from the fallback instance type.
        return _analyze_member_access(name, typ.fallback, mx, override_info)
    elif isinstance(typ, TypedDictType):
        return analyze_typeddict_access(name, typ, mx, override_info)
    elif isinstance(typ, NoneType):
        return analyze_none_member_access(name, typ, mx)
    elif isinstance(typ, TypeVarLikeType):
        if isinstance(typ, TypeVarType) and typ.values:
            return _analyze_member_access(
                name, make_simplified_union(typ.values), mx, override_info
            )
        return _analyze_member_access(name, typ.upper_bound, mx, override_info)
    elif isinstance(typ, DeletedType):
        mx.msg.deleted_as_rvalue(typ, mx.context)
        return AnyType(TypeOfAny.from_error)
    return report_missing_attribute(mx.original_type, typ, name, mx)


</t>
<t tx="ekr.20230831011819.693">def may_be_awaitable_attribute(
    name: str, typ: Type, mx: MemberContext, override_info: TypeInfo | None = None
) -&gt; bool:
    """Check if the given type has the attribute when awaited."""
    if mx.chk.checking_missing_await:
        # Avoid infinite recursion.
        return False
    with mx.chk.checking_await_set(), mx.msg.filter_errors() as local_errors:
        aw_type = mx.chk.get_precise_awaitable_type(typ, local_errors)
        if aw_type is None:
            return False
        _ = _analyze_member_access(name, aw_type, mx, override_info)
        return not local_errors.has_new_errors()


</t>
<t tx="ekr.20230831011819.694">def report_missing_attribute(
    original_type: Type,
    typ: Type,
    name: str,
    mx: MemberContext,
    override_info: TypeInfo | None = None,
) -&gt; Type:
    res_type = mx.msg.has_no_attr(original_type, typ, name, mx.context, mx.module_symbol_table)
    if not mx.msg.prefer_simple_messages():
        if may_be_awaitable_attribute(name, typ, mx, override_info):
            mx.msg.possible_missing_await(mx.context)
    return res_type


</t>
<t tx="ekr.20230831011819.695"># The several functions that follow implement analyze_member_access for various
# types and aren't documented individually.


def analyze_instance_member_access(
    name: str, typ: Instance, mx: MemberContext, override_info: TypeInfo | None
) -&gt; Type:
    if name == "__init__" and not mx.is_super:
        # Accessing __init__ in statically typed code would compromise
        # type safety unless used via super().
        mx.msg.fail(message_registry.CANNOT_ACCESS_INIT, mx.context)
        return AnyType(TypeOfAny.from_error)

    # The base object has an instance type.

    info = typ.type
    if override_info:
        info = override_info

    if (
        state.find_occurrences
        and info.name == state.find_occurrences[0]
        and name == state.find_occurrences[1]
    ):
        mx.msg.note("Occurrence of '{}.{}'".format(*state.find_occurrences), mx.context)

    # Look up the member. First look up the method dictionary.
    method = info.get_method(name)
    if method and not isinstance(method, Decorator):
        if mx.is_super:
            validate_super_call(method, mx)

        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            first_item = method.items[0]
            assert isinstance(first_item, Decorator)
            return analyze_var(name, first_item.var, typ, info, mx)
        if mx.is_lvalue:
            mx.msg.cant_assign_to_method(mx.context)
        if not isinstance(method, OverloadedFuncDef):
            signature = function_type(method, mx.named_type("builtins.function"))
        else:
            if method.type is None:
                # Overloads may be not ready if they are decorated. Handle this in same
                # manner as we would handle a regular decorated function: defer if possible.
                if not mx.no_deferral and method.items:
                    mx.not_ready_callback(method.name, mx.context)
                return AnyType(TypeOfAny.special_form)
            assert isinstance(method.type, Overloaded)
            signature = method.type
        signature = freshen_all_functions_type_vars(signature)
        if not method.is_static:
            if name != "__call__":
                # TODO: use proper treatment of special methods on unions instead
                #       of this hack here and below (i.e. mx.self_type).
                dispatched_type = meet.meet_types(mx.original_type, typ)
                signature = check_self_arg(
                    signature, dispatched_type, method.is_class, mx.context, name, mx.msg
                )
            signature = bind_self(signature, mx.self_type, is_classmethod=method.is_class)
        # TODO: should we skip these steps for static methods as well?
        # Since generic static methods should not be allowed.
        typ = map_instance_to_supertype(typ, method.info)
        member_type = expand_type_by_instance(signature, typ)
        freeze_all_type_vars(member_type)
        return member_type
    else:
        # Not a method.
        return analyze_member_var_access(name, typ, info, mx)


</t>
<t tx="ekr.20230831011819.696">def validate_super_call(node: FuncBase, mx: MemberContext) -&gt; None:
    unsafe_super = False
    if isinstance(node, FuncDef) and node.is_trivial_body:
        unsafe_super = True
        impl = node
    elif isinstance(node, OverloadedFuncDef):
        if node.impl:
            impl = node.impl if isinstance(node.impl, FuncDef) else node.impl.func
            unsafe_super = impl.is_trivial_body
    if unsafe_super:
        ret_type = (
            impl.type.ret_type
            if isinstance(impl.type, CallableType)
            else AnyType(TypeOfAny.unannotated)
        )
        if not subtypes.is_subtype(NoneType(), ret_type):
            mx.msg.unsafe_super(node.name, node.info.name, mx.context)


</t>
<t tx="ekr.20230831011819.697">def analyze_type_callable_member_access(name: str, typ: FunctionLike, mx: MemberContext) -&gt; Type:
    # Class attribute.
    # TODO super?
    ret_type = typ.items[0].ret_type
    assert isinstance(ret_type, ProperType)
    if isinstance(ret_type, TupleType):
        ret_type = tuple_fallback(ret_type)
    if isinstance(ret_type, TypedDictType):
        ret_type = ret_type.fallback
    if isinstance(ret_type, Instance):
        if not mx.is_operator:
            # When Python sees an operator (eg `3 == 4`), it automatically translates that
            # into something like `int.__eq__(3, 4)` instead of `(3).__eq__(4)` as an
            # optimization.
            #
            # While it normally it doesn't matter which of the two versions are used, it
            # does cause inconsistencies when working with classes. For example, translating
            # `int == int` to `int.__eq__(int)` would not work since `int.__eq__` is meant to
            # compare two int _instances_. What we really want is `type(int).__eq__`, which
            # is meant to compare two types or classes.
            #
            # This check makes sure that when we encounter an operator, we skip looking up
            # the corresponding method in the current instance to avoid this edge case.
            # See https://github.com/python/mypy/pull/1787 for more info.
            # TODO: do not rely on same type variables being present in all constructor overloads.
            result = analyze_class_attribute_access(
                ret_type, name, mx, original_vars=typ.items[0].variables, mcs_fallback=typ.fallback
            )
            if result:
                return result
        # Look up from the 'type' type.
        return _analyze_member_access(name, typ.fallback, mx)
    else:
        assert False, f"Unexpected type {ret_type!r}"


</t>
<t tx="ekr.20230831011819.698">def analyze_type_type_member_access(
    name: str, typ: TypeType, mx: MemberContext, override_info: TypeInfo | None
) -&gt; Type:
    # Similar to analyze_type_callable_attribute_access.
    item = None
    fallback = mx.named_type("builtins.type")
    if isinstance(typ.item, Instance):
        item = typ.item
    elif isinstance(typ.item, AnyType):
        with mx.msg.filter_errors():
            return _analyze_member_access(name, fallback, mx, override_info)
    elif isinstance(typ.item, TypeVarType):
        upper_bound = get_proper_type(typ.item.upper_bound)
        if isinstance(upper_bound, Instance):
            item = upper_bound
        elif isinstance(upper_bound, UnionType):
            return _analyze_member_access(
                name,
                TypeType.make_normalized(upper_bound, line=typ.line, column=typ.column),
                mx,
                override_info,
            )
        elif isinstance(upper_bound, TupleType):
            item = tuple_fallback(upper_bound)
        elif isinstance(upper_bound, AnyType):
            with mx.msg.filter_errors():
                return _analyze_member_access(name, fallback, mx, override_info)
    elif isinstance(typ.item, TupleType):
        item = tuple_fallback(typ.item)
    elif isinstance(typ.item, FunctionLike) and typ.item.is_type_obj():
        item = typ.item.fallback
    elif isinstance(typ.item, TypeType):
        # Access member on metaclass object via Type[Type[C]]
        if isinstance(typ.item.item, Instance):
            item = typ.item.item.type.metaclass_type
    ignore_messages = False

    if item is not None:
        fallback = item.type.metaclass_type or fallback

    if item and not mx.is_operator:
        # See comment above for why operators are skipped
        result = analyze_class_attribute_access(
            item, name, mx, mcs_fallback=fallback, override_info=override_info
        )
        if result:
            if not (isinstance(get_proper_type(result), AnyType) and item.type.fallback_to_any):
                return result
            else:
                # We don't want errors on metaclass lookup for classes with Any fallback
                ignore_messages = True

    with mx.msg.filter_errors(filter_errors=ignore_messages):
        return _analyze_member_access(name, fallback, mx, override_info)


</t>
<t tx="ekr.20230831011819.699">def analyze_union_member_access(name: str, typ: UnionType, mx: MemberContext) -&gt; Type:
    with mx.msg.disable_type_names():
        results = []
        for subtype in typ.relevant_items():
            # Self types should be bound to every individual item of a union.
            item_mx = mx.copy_modified(self_type=subtype)
            results.append(_analyze_member_access(name, subtype, item_mx))
    return make_simplified_union(results)


</t>
<t tx="ekr.20230831011819.7">@path mypy
&lt;&lt; api.py: docstring &gt;&gt;
&lt;&lt; api.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.70">def load_plugins_from_config(
    options: Options, errors: Errors, stdout: TextIO
) -&gt; tuple[list[Plugin], dict[str, str]]:
    """Load all configured plugins.

    Return a list of all the loaded plugins from the config file.
    The second return value is a snapshot of versions/hashes of loaded user
    plugins (for cache validation).
    """
    import importlib

    snapshot: dict[str, str] = {}

    if not options.config_file:
        return [], snapshot

    line = find_config_file_line_number(options.config_file, "mypy", "plugins")
    if line == -1:
        line = 1  # We need to pick some line number that doesn't look too confusing

    def plugin_error(message: str) -&gt; NoReturn:
        errors.report(line, 0, message)
        errors.raise_error(use_stdout=False)

    custom_plugins: list[Plugin] = []
    errors.set_file(options.config_file, None, options)
    for plugin_path in options.plugins:
        func_name = "plugin"
        plugin_dir: str | None = None
        if ":" in os.path.basename(plugin_path):
            plugin_path, func_name = plugin_path.rsplit(":", 1)
        if plugin_path.endswith(".py"):
            # Plugin paths can be relative to the config file location.
            plugin_path = os.path.join(os.path.dirname(options.config_file), plugin_path)
            if not os.path.isfile(plugin_path):
                plugin_error(f'Can\'t find plugin "{plugin_path}"')
            # Use an absolute path to avoid populating the cache entry
            # for 'tmp' during tests, since it will be different in
            # different tests.
            plugin_dir = os.path.abspath(os.path.dirname(plugin_path))
            fnam = os.path.basename(plugin_path)
            module_name = fnam[:-3]
            sys.path.insert(0, plugin_dir)
        elif re.search(r"[\\/]", plugin_path):
            fnam = os.path.basename(plugin_path)
            plugin_error(f'Plugin "{fnam}" does not have a .py extension')
        else:
            module_name = plugin_path

        try:
            module = importlib.import_module(module_name)
        except Exception as exc:
            plugin_error(f'Error importing plugin "{plugin_path}": {exc}')
        finally:
            if plugin_dir is not None:
                assert sys.path[0] == plugin_dir
                del sys.path[0]

        if not hasattr(module, func_name):
            plugin_error(
                'Plugin "{}" does not define entry point function "{}"'.format(
                    plugin_path, func_name
                )
            )

        try:
            plugin_type = getattr(module, func_name)(__version__)
        except Exception:
            print(f"Error calling the plugin(version) entry point of {plugin_path}\n", file=stdout)
            raise  # Propagate to display traceback

        if not isinstance(plugin_type, type):
            plugin_error(
                'Type object expected as the return value of "plugin"; got {!r} (in {})'.format(
                    plugin_type, plugin_path
                )
            )
        if not issubclass(plugin_type, Plugin):
            plugin_error(
                'Return value of "plugin" must be a subclass of "mypy.plugin.Plugin" '
                "(in {})".format(plugin_path)
            )
        try:
            custom_plugins.append(plugin_type(options))
            snapshot[module_name] = take_module_snapshot(module)
        except Exception:
            print(f"Error constructing plugin instance of {plugin_type.__name__}\n", file=stdout)
            raise  # Propagate to display traceback

    return custom_plugins, snapshot


</t>
<t tx="ekr.20230831011819.700">def analyze_none_member_access(name: str, typ: NoneType, mx: MemberContext) -&gt; Type:
    if name == "__bool__":
        literal_false = LiteralType(False, fallback=mx.named_type("builtins.bool"))
        return CallableType(
            arg_types=[],
            arg_kinds=[],
            arg_names=[],
            ret_type=literal_false,
            fallback=mx.named_type("builtins.function"),
        )
    else:
        return _analyze_member_access(name, mx.named_type("builtins.object"), mx)


</t>
<t tx="ekr.20230831011819.701">def analyze_member_var_access(
    name: str, itype: Instance, info: TypeInfo, mx: MemberContext
) -&gt; Type:
    """Analyse attribute access that does not target a method.

    This is logically part of analyze_member_access and the arguments are similar.

    original_type is the type of E in the expression E.var
    """
    # It was not a method. Try looking up a variable.
    v = lookup_member_var_or_accessor(info, name, mx.is_lvalue)

    vv = v
    if isinstance(vv, Decorator):
        # The associated Var node of a decorator contains the type.
        v = vv.var
        if mx.is_super:
            validate_super_call(vv.func, mx)

    if isinstance(vv, TypeInfo):
        # If the associated variable is a TypeInfo synthesize a Var node for
        # the purposes of type checking.  This enables us to type check things
        # like accessing class attributes on an inner class.
        v = Var(name, type=type_object_type(vv, mx.named_type))
        v.info = info

    if isinstance(vv, TypeAlias):
        # Similar to the above TypeInfo case, we allow using
        # qualified type aliases in runtime context if it refers to an
        # instance type. For example:
        #     class C:
        #         A = List[int]
        #     x = C.A() &lt;- this is OK
        typ = mx.chk.expr_checker.alias_type_in_runtime_context(
            vv, ctx=mx.context, alias_definition=mx.is_lvalue
        )
        v = Var(name, type=typ)
        v.info = info

    if isinstance(v, Var):
        implicit = info[name].implicit

        # An assignment to final attribute is always an error,
        # independently of types.
        if mx.is_lvalue and not mx.chk.get_final_context():
            check_final_member(name, info, mx.msg, mx.context)

        return analyze_var(name, v, itype, info, mx, implicit=implicit)
    elif isinstance(v, FuncDef):
        assert False, "Did not expect a function"
    elif isinstance(v, MypyFile):
        mx.chk.module_refs.add(v.fullname)
        return mx.chk.expr_checker.module_type(v)
    elif (
        not v
        and name not in ["__getattr__", "__setattr__", "__getattribute__"]
        and not mx.is_operator
        and mx.module_symbol_table is None
    ):
        # Above we skip ModuleType.__getattr__ etc. if we have a
        # module symbol table, since the symbol table allows precise
        # checking.
        if not mx.is_lvalue:
            for method_name in ("__getattribute__", "__getattr__"):
                method = info.get_method(method_name)

                # __getattribute__ is defined on builtins.object and returns Any, so without
                # the guard this search will always find object.__getattribute__ and conclude
                # that the attribute exists
                if method and method.info.fullname != "builtins.object":
                    bound_method = analyze_decorator_or_funcbase_access(
                        defn=method,
                        itype=itype,
                        info=info,
                        self_type=mx.self_type,
                        name=method_name,
                        mx=mx,
                    )
                    typ = map_instance_to_supertype(itype, method.info)
                    getattr_type = get_proper_type(expand_type_by_instance(bound_method, typ))
                    if isinstance(getattr_type, CallableType):
                        result = getattr_type.ret_type
                    else:
                        result = getattr_type

                    # Call the attribute hook before returning.
                    fullname = f"{method.info.fullname}.{name}"
                    hook = mx.chk.plugin.get_attribute_hook(fullname)
                    if hook:
                        result = hook(
                            AttributeContext(
                                get_proper_type(mx.original_type), result, mx.context, mx.chk
                            )
                        )
                    return result
        else:
            setattr_meth = info.get_method("__setattr__")
            if setattr_meth and setattr_meth.info.fullname != "builtins.object":
                bound_type = analyze_decorator_or_funcbase_access(
                    defn=setattr_meth,
                    itype=itype,
                    info=info,
                    self_type=mx.self_type,
                    name=name,
                    mx=mx.copy_modified(is_lvalue=False),
                )
                typ = map_instance_to_supertype(itype, setattr_meth.info)
                setattr_type = get_proper_type(expand_type_by_instance(bound_type, typ))
                if isinstance(setattr_type, CallableType) and len(setattr_type.arg_types) &gt; 0:
                    return setattr_type.arg_types[-1]

    if itype.type.fallback_to_any:
        return AnyType(TypeOfAny.special_form)

    # Could not find the member.
    if itype.extra_attrs and name in itype.extra_attrs.attrs:
        # For modules use direct symbol table lookup.
        if not itype.extra_attrs.mod_name:
            return itype.extra_attrs.attrs[name]

    if mx.is_super:
        mx.msg.undefined_in_superclass(name, mx.context)
        return AnyType(TypeOfAny.from_error)
    else:
        return report_missing_attribute(mx.original_type, itype, name, mx)


</t>
<t tx="ekr.20230831011819.702">def check_final_member(name: str, info: TypeInfo, msg: MessageBuilder, ctx: Context) -&gt; None:
    """Give an error if the name being assigned was declared as final."""
    for base in info.mro:
        sym = base.names.get(name)
        if sym and is_final_node(sym.node):
            msg.cant_assign_to_final(name, attr_assign=True, ctx=ctx)


</t>
<t tx="ekr.20230831011819.703">def analyze_descriptor_access(descriptor_type: Type, mx: MemberContext) -&gt; Type:
    """Type check descriptor access.

    Arguments:
        descriptor_type: The type of the descriptor attribute being accessed
            (the type of ``f`` in ``a.f`` when ``f`` is a descriptor).
        mx: The current member access context.
    Return:
        The return type of the appropriate ``__get__`` overload for the descriptor.
    """
    instance_type = get_proper_type(mx.original_type)
    orig_descriptor_type = descriptor_type
    descriptor_type = get_proper_type(descriptor_type)

    if isinstance(descriptor_type, UnionType):
        # Map the access over union types
        return make_simplified_union(
            [analyze_descriptor_access(typ, mx) for typ in descriptor_type.items]
        )
    elif not isinstance(descriptor_type, Instance):
        return orig_descriptor_type

    if not descriptor_type.type.has_readable_member("__get__"):
        return orig_descriptor_type

    dunder_get = descriptor_type.type.get_method("__get__")
    if dunder_get is None:
        mx.msg.fail(
            message_registry.DESCRIPTOR_GET_NOT_CALLABLE.format(
                descriptor_type.str_with_options(mx.msg.options)
            ),
            mx.context,
        )
        return AnyType(TypeOfAny.from_error)

    bound_method = analyze_decorator_or_funcbase_access(
        defn=dunder_get,
        itype=descriptor_type,
        info=descriptor_type.type,
        self_type=descriptor_type,
        name="__get__",
        mx=mx,
    )

    typ = map_instance_to_supertype(descriptor_type, dunder_get.info)
    dunder_get_type = expand_type_by_instance(bound_method, typ)

    if isinstance(instance_type, FunctionLike) and instance_type.is_type_obj():
        owner_type = instance_type.items[0].ret_type
        instance_type = NoneType()
    elif isinstance(instance_type, TypeType):
        owner_type = instance_type.item
        instance_type = NoneType()
    else:
        owner_type = instance_type

    callable_name = mx.chk.expr_checker.method_fullname(descriptor_type, "__get__")
    dunder_get_type = mx.chk.expr_checker.transform_callee_type(
        callable_name,
        dunder_get_type,
        [
            TempNode(instance_type, context=mx.context),
            TempNode(TypeType.make_normalized(owner_type), context=mx.context),
        ],
        [ARG_POS, ARG_POS],
        mx.context,
        object_type=descriptor_type,
    )

    _, inferred_dunder_get_type = mx.chk.expr_checker.check_call(
        dunder_get_type,
        [
            TempNode(instance_type, context=mx.context),
            TempNode(TypeType.make_normalized(owner_type), context=mx.context),
        ],
        [ARG_POS, ARG_POS],
        mx.context,
        object_type=descriptor_type,
        callable_name=callable_name,
    )

    inferred_dunder_get_type = get_proper_type(inferred_dunder_get_type)
    if isinstance(inferred_dunder_get_type, AnyType):
        # check_call failed, and will have reported an error
        return inferred_dunder_get_type

    if not isinstance(inferred_dunder_get_type, CallableType):
        mx.msg.fail(
            message_registry.DESCRIPTOR_GET_NOT_CALLABLE.format(
                descriptor_type.str_with_options(mx.msg.options)
            ),
            mx.context,
        )
        return AnyType(TypeOfAny.from_error)

    return inferred_dunder_get_type.ret_type


</t>
<t tx="ekr.20230831011819.704">def is_instance_var(var: Var) -&gt; bool:
    """Return if var is an instance variable according to PEP 526."""
    return (
        # check the type_info node is the var (not a decorated function, etc.)
        var.name in var.info.names
        and var.info.names[var.name].node is var
        and not var.is_classvar
        # variables without annotations are treated as classvar
        and not var.is_inferred
    )


</t>
<t tx="ekr.20230831011819.705">def analyze_var(
    name: str,
    var: Var,
    itype: Instance,
    info: TypeInfo,
    mx: MemberContext,
    *,
    implicit: bool = False,
) -&gt; Type:
    """Analyze access to an attribute via a Var node.

    This is conceptually part of analyze_member_access and the arguments are similar.
    itype is the instance type in which attribute should be looked up
    original_type is the type of E in the expression E.var
    if implicit is True, the original Var was created as an assignment to self
    """
    # Found a member variable.
    original_itype = itype
    itype = map_instance_to_supertype(itype, var.info)
    typ = var.type
    if typ:
        if isinstance(typ, PartialType):
            return mx.chk.handle_partial_var_type(typ, mx.is_lvalue, var, mx.context)
        if mx.is_lvalue and var.is_property and not var.is_settable_property:
            # TODO allow setting attributes in subclass (although it is probably an error)
            mx.msg.read_only_property(name, itype.type, mx.context)
        if mx.is_lvalue and var.is_classvar:
            mx.msg.cant_assign_to_classvar(name, mx.context)
        t = freshen_all_functions_type_vars(typ)
        if not (mx.is_self or mx.is_super) or supported_self_type(
            get_proper_type(mx.original_type)
        ):
            t = expand_self_type(var, t, mx.original_type)
        elif (
            mx.is_self
            and original_itype.type != var.info
            # If an attribute with Self-type was defined in a supertype, we need to
            # rebind the Self type variable to Self type variable of current class...
            and original_itype.type.self_type is not None
            # ...unless `self` has an explicit non-trivial annotation.
            and original_itype == mx.chk.scope.active_self_type()
        ):
            t = expand_self_type(var, t, original_itype.type.self_type)
        t = get_proper_type(expand_type_by_instance(t, itype))
        freeze_all_type_vars(t)
        result: Type = t
        typ = get_proper_type(typ)
        if (
            var.is_initialized_in_class
            and (not is_instance_var(var) or mx.is_operator)
            and isinstance(typ, FunctionLike)
            and not typ.is_type_obj()
        ):
            if mx.is_lvalue:
                if var.is_property:
                    if not var.is_settable_property:
                        mx.msg.read_only_property(name, itype.type, mx.context)
                else:
                    mx.msg.cant_assign_to_method(mx.context)

            if not var.is_staticmethod:
                # Class-level function objects and classmethods become bound methods:
                # the former to the instance, the latter to the class.
                functype = typ
                # Use meet to narrow original_type to the dispatched type.
                # For example, assume
                # * A.f: Callable[[A1], None] where A1 &lt;: A (maybe A1 == A)
                # * B.f: Callable[[B1], None] where B1 &lt;: B (maybe B1 == B)
                # * x: Union[A1, B1]
                # In `x.f`, when checking `x` against A1 we assume x is compatible with A
                # and similarly for B1 when checking against B
                dispatched_type = meet.meet_types(mx.original_type, itype)
                signature = freshen_all_functions_type_vars(functype)
                bound = get_proper_type(expand_self_type(var, signature, mx.original_type))
                assert isinstance(bound, FunctionLike)
                signature = bound
                signature = check_self_arg(
                    signature, dispatched_type, var.is_classmethod, mx.context, name, mx.msg
                )
                signature = bind_self(signature, mx.self_type, var.is_classmethod)
                expanded_signature = expand_type_by_instance(signature, itype)
                freeze_all_type_vars(expanded_signature)
                if var.is_property:
                    # A property cannot have an overloaded type =&gt; the cast is fine.
                    assert isinstance(expanded_signature, CallableType)
                    result = expanded_signature.ret_type
                else:
                    result = expanded_signature
    else:
        if not var.is_ready and not mx.no_deferral:
            mx.not_ready_callback(var.name, mx.context)
        # Implicit 'Any' type.
        result = AnyType(TypeOfAny.special_form)
    fullname = f"{var.info.fullname}.{name}"
    hook = mx.chk.plugin.get_attribute_hook(fullname)
    if result and not mx.is_lvalue and not implicit:
        result = analyze_descriptor_access(result, mx)
    if hook:
        result = hook(
            AttributeContext(get_proper_type(mx.original_type), result, mx.context, mx.chk)
        )
    return result


</t>
<t tx="ekr.20230831011819.706">def freeze_all_type_vars(member_type: Type) -&gt; None:
    member_type.accept(FreezeTypeVarsVisitor())


</t>
<t tx="ekr.20230831011819.707">class FreezeTypeVarsVisitor(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011819.708">def visit_callable_type(self, t: CallableType) -&gt; None:
    for v in t.variables:
        v.id.meta_level = 0
    super().visit_callable_type(t)


</t>
<t tx="ekr.20230831011819.709">def lookup_member_var_or_accessor(info: TypeInfo, name: str, is_lvalue: bool) -&gt; SymbolNode | None:
    """Find the attribute/accessor node that refers to a member of a type."""
    # TODO handle lvalues
    node = info.get(name)
    if node:
        return node.node
    else:
        return None


</t>
<t tx="ekr.20230831011819.71">def load_plugins(
    options: Options, errors: Errors, stdout: TextIO, extra_plugins: Sequence[Plugin]
) -&gt; tuple[Plugin, dict[str, str]]:
    """Load all configured plugins.

    Return a plugin that encapsulates all plugins chained together. Always
    at least include the default plugin (it's last in the chain).
    The second return value is a snapshot of versions/hashes of loaded user
    plugins (for cache validation).
    """
    custom_plugins, snapshot = load_plugins_from_config(options, errors, stdout)

    custom_plugins += extra_plugins

    default_plugin: Plugin = DefaultPlugin(options)
    if not custom_plugins:
        return default_plugin, snapshot

    # Custom plugins take precedence over the default plugin.
    return ChainedPlugin(options, custom_plugins + [default_plugin]), snapshot


</t>
<t tx="ekr.20230831011819.710">def check_self_arg(
    functype: FunctionLike,
    dispatched_arg_type: Type,
    is_classmethod: bool,
    context: Context,
    name: str,
    msg: MessageBuilder,
) -&gt; FunctionLike:
    """Check that an instance has a valid type for a method with annotated 'self'.

    For example if the method is defined as:
        class A:
            def f(self: S) -&gt; T: ...
    then for 'x.f' we check that meet(type(x), A) &lt;: S. If the method is overloaded, we
    select only overloads items that satisfy this requirement. If there are no matching
    overloads, an error is generated.

    Note: dispatched_arg_type uses a meet to select a relevant item in case if the
    original type of 'x' is a union. This is done because several special methods
    treat union types in ad-hoc manner, so we can't use MemberContext.self_type yet.
    """
    items = functype.items
    if not items:
        return functype
    new_items = []
    if is_classmethod:
        dispatched_arg_type = TypeType.make_normalized(dispatched_arg_type)

    for item in items:
        if not item.arg_types or item.arg_kinds[0] not in (ARG_POS, ARG_STAR):
            # No positional first (self) argument (*args is okay).
            msg.no_formal_self(name, item, context)
            # This is pretty bad, so just return the original signature if
            # there is at least one such error.
            return functype
        else:
            selfarg = get_proper_type(item.arg_types[0])
            if subtypes.is_subtype(dispatched_arg_type, erase_typevars(erase_to_bound(selfarg))):
                new_items.append(item)
            elif isinstance(selfarg, ParamSpecType):
                # TODO: This is not always right. What's the most reasonable thing to do here?
                new_items.append(item)
            elif isinstance(selfarg, TypeVarTupleType):
                raise NotImplementedError
    if not new_items:
        # Choose first item for the message (it may be not very helpful for overloads).
        msg.incompatible_self_argument(
            name, dispatched_arg_type, items[0], is_classmethod, context
        )
        return functype
    if len(new_items) == 1:
        return new_items[0]
    return Overloaded(new_items)


</t>
<t tx="ekr.20230831011819.711">def analyze_class_attribute_access(
    itype: Instance,
    name: str,
    mx: MemberContext,
    *,
    mcs_fallback: Instance,
    override_info: TypeInfo | None = None,
    original_vars: Sequence[TypeVarLikeType] | None = None,
) -&gt; Type | None:
    """Analyze access to an attribute on a class object.

    itype is the return type of the class object callable, original_type is the type
    of E in the expression E.var, original_vars are type variables of the class callable
    (for generic classes).
    """
    info = itype.type
    if override_info:
        info = override_info

    fullname = f"{info.fullname}.{name}"
    hook = mx.chk.plugin.get_class_attribute_hook(fullname)

    node = info.get(name)
    if not node:
        if itype.extra_attrs and name in itype.extra_attrs.attrs:
            # For modules use direct symbol table lookup.
            if not itype.extra_attrs.mod_name:
                return itype.extra_attrs.attrs[name]
        if info.fallback_to_any or info.meta_fallback_to_any:
            return apply_class_attr_hook(mx, hook, AnyType(TypeOfAny.special_form))
        return None

    if (
        isinstance(node.node, Var)
        and not node.node.is_classvar
        and not hook
        and mcs_fallback.type.get(name)
    ):
        # If the same attribute is declared on the metaclass and the class but with different types,
        # and the attribute on the class is not a ClassVar,
        # the type of the attribute on the metaclass should take priority
        # over the type of the attribute on the class,
        # when the attribute is being accessed from the class object itself.
        #
        # Return `None` here to signify that the name should be looked up
        # on the class object itself rather than the instance.
        return None

    is_decorated = isinstance(node.node, Decorator)
    is_method = is_decorated or isinstance(node.node, FuncBase)
    if mx.is_lvalue:
        if is_method:
            mx.msg.cant_assign_to_method(mx.context)
        if isinstance(node.node, TypeInfo):
            mx.msg.fail(message_registry.CANNOT_ASSIGN_TO_TYPE, mx.context)

    # Refuse class attribute access if slot defined
    if info.slots and name in info.slots:
        mx.msg.fail(message_registry.CLASS_VAR_CONFLICTS_SLOTS.format(name), mx.context)

    # If a final attribute was declared on `self` in `__init__`, then it
    # can't be accessed on the class object.
    if node.implicit and isinstance(node.node, Var) and node.node.is_final:
        mx.msg.fail(
            message_registry.CANNOT_ACCESS_FINAL_INSTANCE_ATTR.format(node.node.name), mx.context
        )

    # An assignment to final attribute on class object is also always an error,
    # independently of types.
    if mx.is_lvalue and not mx.chk.get_final_context():
        check_final_member(name, info, mx.msg, mx.context)

    if info.is_enum and not (mx.is_lvalue or is_decorated or is_method):
        enum_class_attribute_type = analyze_enum_class_attribute_access(itype, name, mx)
        if enum_class_attribute_type:
            return apply_class_attr_hook(mx, hook, enum_class_attribute_type)

    t = node.type
    if t:
        if isinstance(t, PartialType):
            symnode = node.node
            assert isinstance(symnode, Var)
            return apply_class_attr_hook(
                mx, hook, mx.chk.handle_partial_var_type(t, mx.is_lvalue, symnode, mx.context)
            )

        # Find the class where method/variable was defined.
        if isinstance(node.node, Decorator):
            super_info: TypeInfo | None = node.node.var.info
        elif isinstance(node.node, (Var, SYMBOL_FUNCBASE_TYPES)):
            super_info = node.node.info
        else:
            super_info = None

        # Map the type to how it would look as a defining class. For example:
        #     class C(Generic[T]): ...
        #     class D(C[Tuple[T, S]]): ...
        #     D[int, str].method()
        # Here itype is D[int, str], isuper is C[Tuple[int, str]].
        if not super_info:
            isuper = None
        else:
            isuper = map_instance_to_supertype(itype, super_info)

        if isinstance(node.node, Var):
            assert isuper is not None
            # Check if original variable type has type variables. For example:
            #     class C(Generic[T]):
            #         x: T
            #     C.x  # Error, ambiguous access
            #     C[int].x  # Also an error, since C[int] is same as C at runtime
            # Exception is Self type wrapped in ClassVar, that is safe.
            def_vars = set(node.node.info.defn.type_vars)
            if not node.node.is_classvar and node.node.info.self_type:
                def_vars.add(node.node.info.self_type)
            typ_vars = set(get_type_vars(t))
            if def_vars &amp; typ_vars:
                # Exception: access on Type[...], including first argument of class methods is OK.
                if not isinstance(get_proper_type(mx.original_type), TypeType) or node.implicit:
                    if node.node.is_classvar:
                        message = message_registry.GENERIC_CLASS_VAR_ACCESS
                    else:
                        message = message_registry.GENERIC_INSTANCE_VAR_CLASS_ACCESS
                    mx.msg.fail(message, mx.context)

            # Erase non-mapped variables, but keep mapped ones, even if there is an error.
            # In the above example this means that we infer following types:
            #     C.x -&gt; Any
            #     C[int].x -&gt; int
            t = get_proper_type(expand_self_type(node.node, t, itype))
            t = erase_typevars(expand_type_by_instance(t, isuper), {tv.id for tv in def_vars})

        is_classmethod = (is_decorated and cast(Decorator, node.node).func.is_class) or (
            isinstance(node.node, FuncBase) and node.node.is_class
        )
        t = get_proper_type(t)
        if isinstance(t, FunctionLike) and is_classmethod:
            t = check_self_arg(t, mx.self_type, False, mx.context, name, mx.msg)
        result = add_class_tvars(
            t, isuper, is_classmethod, mx.self_type, original_vars=original_vars
        )
        if not mx.is_lvalue:
            result = analyze_descriptor_access(result, mx)

        return apply_class_attr_hook(mx, hook, result)
    elif isinstance(node.node, Var):
        mx.not_ready_callback(name, mx.context)
        return AnyType(TypeOfAny.special_form)

    if isinstance(node.node, TypeVarExpr):
        mx.msg.fail(
            message_registry.CANNOT_USE_TYPEVAR_AS_EXPRESSION.format(info.name, name), mx.context
        )
        return AnyType(TypeOfAny.from_error)

    if isinstance(node.node, TypeInfo):
        return type_object_type(node.node, mx.named_type)

    if isinstance(node.node, MypyFile):
        # Reference to a module object.
        return mx.named_type("types.ModuleType")

    if isinstance(node.node, TypeAlias):
        return mx.chk.expr_checker.alias_type_in_runtime_context(
            node.node, ctx=mx.context, alias_definition=mx.is_lvalue
        )

    if is_decorated:
        assert isinstance(node.node, Decorator)
        if node.node.type:
            return apply_class_attr_hook(mx, hook, node.node.type)
        else:
            mx.not_ready_callback(name, mx.context)
            return AnyType(TypeOfAny.from_error)
    else:
        assert isinstance(node.node, FuncBase)
        typ = function_type(node.node, mx.named_type("builtins.function"))
        # Note: if we are accessing class method on class object, the cls argument is bound.
        # Annotated and/or explicit class methods go through other code paths above, for
        # unannotated implicit class methods we do this here.
        if node.node.is_class:
            typ = bind_self(typ, is_classmethod=True)
        return apply_class_attr_hook(mx, hook, typ)


</t>
<t tx="ekr.20230831011819.712">def apply_class_attr_hook(
    mx: MemberContext, hook: Callable[[AttributeContext], Type] | None, result: Type
) -&gt; Type | None:
    if hook:
        result = hook(
            AttributeContext(get_proper_type(mx.original_type), result, mx.context, mx.chk)
        )
    return result


</t>
<t tx="ekr.20230831011819.713">def analyze_enum_class_attribute_access(
    itype: Instance, name: str, mx: MemberContext
) -&gt; Type | None:
    # Skip these since Enum will remove it
    if name in ENUM_REMOVED_PROPS:
        return report_missing_attribute(mx.original_type, itype, name, mx)
    # For other names surrendered by underscores, we don't make them Enum members
    if name.startswith("__") and name.endswith("__") and name.replace("_", "") != "":
        return None

    enum_literal = LiteralType(name, fallback=itype)
    return itype.copy_modified(last_known_value=enum_literal)


</t>
<t tx="ekr.20230831011819.714">def analyze_typeddict_access(
    name: str, typ: TypedDictType, mx: MemberContext, override_info: TypeInfo | None
) -&gt; Type:
    if name == "__setitem__":
        if isinstance(mx.context, IndexExpr):
            # Since we can get this during `a['key'] = ...`
            # it is safe to assume that the context is `IndexExpr`.
            item_type = mx.chk.expr_checker.visit_typeddict_index_expr(
                typ, mx.context.index, setitem=True
            )
        else:
            # It can also be `a.__setitem__(...)` direct call.
            # In this case `item_type` can be `Any`,
            # because we don't have args available yet.
            # TODO: check in `default` plugin that `__setitem__` is correct.
            item_type = AnyType(TypeOfAny.implementation_artifact)
        return CallableType(
            arg_types=[mx.chk.named_type("builtins.str"), item_type],
            arg_kinds=[ARG_POS, ARG_POS],
            arg_names=[None, None],
            ret_type=NoneType(),
            fallback=mx.chk.named_type("builtins.function"),
            name=name,
        )
    elif name == "__delitem__":
        return CallableType(
            arg_types=[mx.chk.named_type("builtins.str")],
            arg_kinds=[ARG_POS],
            arg_names=[None],
            ret_type=NoneType(),
            fallback=mx.chk.named_type("builtins.function"),
            name=name,
        )
    return _analyze_member_access(name, typ.fallback, mx, override_info)


</t>
<t tx="ekr.20230831011819.715">def add_class_tvars(
    t: ProperType,
    isuper: Instance | None,
    is_classmethod: bool,
    original_type: Type,
    original_vars: Sequence[TypeVarLikeType] | None = None,
) -&gt; Type:
    """Instantiate type variables during analyze_class_attribute_access,
    e.g T and Q in the following:

    class A(Generic[T]):
        @classmethod
        def foo(cls: Type[Q]) -&gt; Tuple[T, Q]: ...

    class B(A[str]): pass
    B.foo()

    Args:
        t: Declared type of the method (or property)
        isuper: Current instance mapped to the superclass where method was defined, this
            is usually done by map_instance_to_supertype()
        is_classmethod: True if this method is decorated with @classmethod
        original_type: The value of the type B in the expression B.foo() or the corresponding
            component in case of a union (this is used to bind the self-types)
        original_vars: Type variables of the class callable on which the method was accessed
    Returns:
        Expanded method type with added type variables (when needed).
    """
    # TODO: verify consistency between Q and T

    # We add class type variables if the class method is accessed on class object
    # without applied type arguments, this matches the behavior of __init__().
    # For example (continuing the example in docstring):
    #     A       # The type of callable is def [T] () -&gt; A[T], _not_ def () -&gt; A[Any]
    #     A[int]  # The type of callable is def () -&gt; A[int]
    # and
    #     A.foo       # The type is generic def [T] () -&gt; Tuple[T, A[T]]
    #     A[int].foo  # The type is non-generic def () -&gt; Tuple[int, A[int]]
    #
    # This behaviour is useful for defining alternative constructors for generic classes.
    # To achieve such behaviour, we add the class type variables that are still free
    # (i.e. appear in the return type of the class object on which the method was accessed).
    if isinstance(t, CallableType):
        tvars = original_vars if original_vars is not None else []
        t = freshen_all_functions_type_vars(t)
        if is_classmethod:
            t = bind_self(t, original_type, is_classmethod=True)
            assert isuper is not None
            t = expand_type_by_instance(t, isuper)
        freeze_all_type_vars(t)
        return t.copy_modified(variables=list(tvars) + list(t.variables))
    elif isinstance(t, Overloaded):
        return Overloaded(
            [
                cast(
                    CallableType,
                    add_class_tvars(
                        item, isuper, is_classmethod, original_type, original_vars=original_vars
                    ),
                )
                for item in t.items
            ]
        )
    if isuper is not None:
        t = expand_type_by_instance(t, isuper)
    return t


</t>
<t tx="ekr.20230831011819.716">def type_object_type(info: TypeInfo, named_type: Callable[[str], Instance]) -&gt; ProperType:
    """Return the type of a type object.

    For a generic type G with type variables T and S the type is generally of form

      Callable[..., G[T, S]]

    where ... are argument types for the __init__/__new__ method (without the self
    argument). Also, the fallback type will be 'type' instead of 'function'.
    """

    # We take the type from whichever of __init__ and __new__ is first
    # in the MRO, preferring __init__ if there is a tie.
    init_method = info.get("__init__")
    new_method = info.get("__new__")
    if not init_method or not is_valid_constructor(init_method.node):
        # Must be an invalid class definition.
        return AnyType(TypeOfAny.from_error)
    # There *should* always be a __new__ method except the test stubs
    # lack it, so just copy init_method in that situation
    new_method = new_method or init_method
    if not is_valid_constructor(new_method.node):
        # Must be an invalid class definition.
        return AnyType(TypeOfAny.from_error)

    # The two is_valid_constructor() checks ensure this.
    assert isinstance(new_method.node, (SYMBOL_FUNCBASE_TYPES, Decorator))
    assert isinstance(init_method.node, (SYMBOL_FUNCBASE_TYPES, Decorator))

    init_index = info.mro.index(init_method.node.info)
    new_index = info.mro.index(new_method.node.info)

    fallback = info.metaclass_type or named_type("builtins.type")
    if init_index &lt; new_index:
        method: FuncBase | Decorator = init_method.node
        is_new = False
    elif init_index &gt; new_index:
        method = new_method.node
        is_new = True
    else:
        if init_method.node.info.fullname == "builtins.object":
            # Both are defined by object.  But if we've got a bogus
            # base class, we can't know for sure, so check for that.
            if info.fallback_to_any:
                # Construct a universal callable as the prototype.
                any_type = AnyType(TypeOfAny.special_form)
                sig = CallableType(
                    arg_types=[any_type, any_type],
                    arg_kinds=[ARG_STAR, ARG_STAR2],
                    arg_names=["_args", "_kwds"],
                    ret_type=any_type,
                    fallback=named_type("builtins.function"),
                )
                return class_callable(sig, info, fallback, None, is_new=False)

        # Otherwise prefer __init__ in a tie. It isn't clear that this
        # is the right thing, but __new__ caused problems with
        # typeshed (#5647).
        method = init_method.node
        is_new = False
    # Construct callable type based on signature of __init__. Adjust
    # return type and insert type arguments.
    if isinstance(method, FuncBase):
        t = function_type(method, fallback)
    else:
        assert isinstance(method.type, ProperType)
        assert isinstance(method.type, FunctionLike)  # is_valid_constructor() ensures this
        t = method.type
    return type_object_type_from_function(t, info, method.info, fallback, is_new)


</t>
<t tx="ekr.20230831011819.717">def analyze_decorator_or_funcbase_access(
    defn: Decorator | FuncBase,
    itype: Instance,
    info: TypeInfo,
    self_type: Type | None,
    name: str,
    mx: MemberContext,
) -&gt; Type:
    """Analyzes the type behind method access.

    The function itself can possibly be decorated.
    See: https://github.com/python/mypy/issues/10409
    """
    if isinstance(defn, Decorator):
        return analyze_var(name, defn.var, itype, info, mx)
    return bind_self(
        function_type(defn, mx.chk.named_type("builtins.function")), original_type=self_type
    )


</t>
<t tx="ekr.20230831011819.718">def is_valid_constructor(n: SymbolNode | None) -&gt; bool:
    """Does this node represents a valid constructor method?

    This includes normal functions, overloaded functions, and decorators
    that return a callable type.
    """
    if isinstance(n, FuncBase):
        return True
    if isinstance(n, Decorator):
        return isinstance(get_proper_type(n.type), FunctionLike)
    return False
</t>
<t tx="ekr.20230831011819.719">@path mypy
"""Pattern checker. This file is conceptually part of TypeChecker."""
&lt;&lt; checkpattern.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.72">def take_module_snapshot(module: types.ModuleType) -&gt; str:
    """Take plugin module snapshot by recording its version and hash.

    We record _both_ hash and the version to detect more possible changes
    (e.g. if there is a change in modules imported by a plugin).
    """
    if hasattr(module, "__file__"):
        assert module.__file__ is not None
        with open(module.__file__, "rb") as f:
            digest = hash_digest(f.read())
    else:
        digest = "unknown"
    ver = getattr(module, "__version__", "none")
    return f"{ver}:{digest}"


</t>
<t tx="ekr.20230831011819.720">
from __future__ import annotations

from collections import defaultdict
from typing import Final, NamedTuple

import mypy.checker
from mypy import message_registry
from mypy.checkmember import analyze_member_access
from mypy.expandtype import expand_type_by_instance
from mypy.join import join_types
from mypy.literals import literal_hash
from mypy.maptype import map_instance_to_supertype
from mypy.meet import narrow_declared_type
from mypy.messages import MessageBuilder
from mypy.nodes import ARG_POS, Context, Expression, NameExpr, TypeAlias, TypeInfo, Var
from mypy.options import Options
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    Pattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.plugin import Plugin
from mypy.subtypes import is_subtype
from mypy.typeops import (
    coerce_to_literal,
    make_simplified_union,
    try_getting_str_literals_from_type,
    tuple_fallback,
)
from mypy.types import (
    AnyType,
    Instance,
    LiteralType,
    NoneType,
    ProperType,
    TupleType,
    Type,
    TypedDictType,
    TypeOfAny,
    UninhabitedType,
    UnionType,
    get_proper_type,
)
from mypy.typevars import fill_typevars
from mypy.visitor import PatternVisitor

self_match_type_names: Final = [
    "builtins.bool",
    "builtins.bytearray",
    "builtins.bytes",
    "builtins.dict",
    "builtins.float",
    "builtins.frozenset",
    "builtins.int",
    "builtins.list",
    "builtins.set",
    "builtins.str",
    "builtins.tuple",
]

non_sequence_match_type_names: Final = ["builtins.str", "builtins.bytes", "builtins.bytearray"]


# For every Pattern a PatternType can be calculated. This requires recursively calculating
# the PatternTypes of the sub-patterns first.
# Using the data in the PatternType the match subject and captured names can be narrowed/inferred.
</t>
<t tx="ekr.20230831011819.721">class PatternType(NamedTuple):
    type: Type  # The type the match subject can be narrowed to
    rest_type: Type  # The remaining type if the pattern didn't match
    captures: dict[Expression, Type]  # The variables captured by the pattern


</t>
<t tx="ekr.20230831011819.722">class PatternChecker(PatternVisitor[PatternType]):
    """Pattern checker.

    This class checks if a pattern can match a type, what the type can be narrowed to, and what
    type capture patterns should be inferred as.
    """

    @others
</t>
<t tx="ekr.20230831011819.723"># Some services are provided by a TypeChecker instance.
chk: mypy.checker.TypeChecker
# This is shared with TypeChecker, but stored also here for convenience.
msg: MessageBuilder
# Currently unused
plugin: Plugin
# The expression being matched against the pattern
subject: Expression

subject_type: Type
# Type of the subject to check the (sub)pattern against
type_context: list[Type]
# Types that match against self instead of their __match_args__ if used as a class pattern
# Filled in from self_match_type_names
self_match_types: list[Type]
# Types that are sequences, but don't match sequence patterns. Filled in from
# non_sequence_match_type_names
non_sequence_match_types: list[Type]

options: Options

def __init__(
    self, chk: mypy.checker.TypeChecker, msg: MessageBuilder, plugin: Plugin, options: Options
) -&gt; None:
    self.chk = chk
    self.msg = msg
    self.plugin = plugin

    self.type_context = []
    self.self_match_types = self.generate_types_from_names(self_match_type_names)
    self.non_sequence_match_types = self.generate_types_from_names(
        non_sequence_match_type_names
    )
    self.options = options

</t>
<t tx="ekr.20230831011819.724">def accept(self, o: Pattern, type_context: Type) -&gt; PatternType:
    self.type_context.append(type_context)
    result = o.accept(self)
    self.type_context.pop()

    return result

</t>
<t tx="ekr.20230831011819.725">def visit_as_pattern(self, o: AsPattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    if o.pattern is not None:
        pattern_type = self.accept(o.pattern, current_type)
        typ, rest_type, type_map = pattern_type
    else:
        typ, rest_type, type_map = current_type, UninhabitedType(), {}

    if not is_uninhabited(typ) and o.name is not None:
        typ, _ = self.chk.conditional_types_with_intersection(
            current_type, [get_type_range(typ)], o, default=current_type
        )
        if not is_uninhabited(typ):
            type_map[o.name] = typ

    return PatternType(typ, rest_type, type_map)

</t>
<t tx="ekr.20230831011819.726">def visit_or_pattern(self, o: OrPattern) -&gt; PatternType:
    current_type = self.type_context[-1]

    #
    # Check all the subpatterns
    #
    pattern_types = []
    for pattern in o.patterns:
        pattern_type = self.accept(pattern, current_type)
        pattern_types.append(pattern_type)
        current_type = pattern_type.rest_type

    #
    # Collect the final type
    #
    types = []
    for pattern_type in pattern_types:
        if not is_uninhabited(pattern_type.type):
            types.append(pattern_type.type)

    #
    # Check the capture types
    #
    capture_types: dict[Var, list[tuple[Expression, Type]]] = defaultdict(list)
    # Collect captures from the first subpattern
    for expr, typ in pattern_types[0].captures.items():
        node = get_var(expr)
        capture_types[node].append((expr, typ))

    # Check if other subpatterns capture the same names
    for i, pattern_type in enumerate(pattern_types[1:]):
        vars = {get_var(expr) for expr, _ in pattern_type.captures.items()}
        if capture_types.keys() != vars:
            self.msg.fail(message_registry.OR_PATTERN_ALTERNATIVE_NAMES, o.patterns[i])
        for expr, typ in pattern_type.captures.items():
            node = get_var(expr)
            capture_types[node].append((expr, typ))

    captures: dict[Expression, Type] = {}
    for var, capture_list in capture_types.items():
        typ = UninhabitedType()
        for _, other in capture_list:
            typ = join_types(typ, other)

        captures[capture_list[0][0]] = typ

    union_type = make_simplified_union(types)
    return PatternType(union_type, current_type, captures)

</t>
<t tx="ekr.20230831011819.727">def visit_value_pattern(self, o: ValuePattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    typ = self.chk.expr_checker.accept(o.expr)
    typ = coerce_to_literal(typ)
    narrowed_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type, [get_type_range(typ)], o, default=current_type
    )
    if not isinstance(get_proper_type(narrowed_type), (LiteralType, UninhabitedType)):
        return PatternType(narrowed_type, UnionType.make_union([narrowed_type, rest_type]), {})
    return PatternType(narrowed_type, rest_type, {})

</t>
<t tx="ekr.20230831011819.728">def visit_singleton_pattern(self, o: SingletonPattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    value: bool | None = o.value
    if isinstance(value, bool):
        typ = self.chk.expr_checker.infer_literal_expr_type(value, "builtins.bool")
    elif value is None:
        typ = NoneType()
    else:
        assert False

    narrowed_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type, [get_type_range(typ)], o, default=current_type
    )
    return PatternType(narrowed_type, rest_type, {})

</t>
<t tx="ekr.20230831011819.729">def visit_sequence_pattern(self, o: SequencePattern) -&gt; PatternType:
    #
    # check for existence of a starred pattern
    #
    current_type = get_proper_type(self.type_context[-1])
    if not self.can_match_sequence(current_type):
        return self.early_non_match()
    star_positions = [i for i, p in enumerate(o.patterns) if isinstance(p, StarredPattern)]
    star_position: int | None = None
    if len(star_positions) == 1:
        star_position = star_positions[0]
    elif len(star_positions) &gt;= 2:
        assert False, "Parser should prevent multiple starred patterns"
    required_patterns = len(o.patterns)
    if star_position is not None:
        required_patterns -= 1

    #
    # get inner types of original type
    #
    if isinstance(current_type, TupleType):
        inner_types = current_type.items
        size_diff = len(inner_types) - required_patterns
        if size_diff &lt; 0:
            return self.early_non_match()
        elif size_diff &gt; 0 and star_position is None:
            return self.early_non_match()
    else:
        inner_type = self.get_sequence_type(current_type, o)
        if inner_type is None:
            inner_type = self.chk.named_type("builtins.object")
        inner_types = [inner_type] * len(o.patterns)

    #
    # match inner patterns
    #
    contracted_new_inner_types: list[Type] = []
    contracted_rest_inner_types: list[Type] = []
    captures: dict[Expression, Type] = {}

    contracted_inner_types = self.contract_starred_pattern_types(
        inner_types, star_position, required_patterns
    )
    for p, t in zip(o.patterns, contracted_inner_types):
        pattern_type = self.accept(p, t)
        typ, rest, type_map = pattern_type
        contracted_new_inner_types.append(typ)
        contracted_rest_inner_types.append(rest)
        self.update_type_map(captures, type_map)

    new_inner_types = self.expand_starred_pattern_types(
        contracted_new_inner_types, star_position, len(inner_types)
    )
    rest_inner_types = self.expand_starred_pattern_types(
        contracted_rest_inner_types, star_position, len(inner_types)
    )

    #
    # Calculate new type
    #
    new_type: Type
    rest_type: Type = current_type
    if isinstance(current_type, TupleType):
        narrowed_inner_types = []
        inner_rest_types = []
        for inner_type, new_inner_type in zip(inner_types, new_inner_types):
            (
                narrowed_inner_type,
                inner_rest_type,
            ) = self.chk.conditional_types_with_intersection(
                new_inner_type, [get_type_range(inner_type)], o, default=new_inner_type
            )
            narrowed_inner_types.append(narrowed_inner_type)
            inner_rest_types.append(inner_rest_type)
        if all(not is_uninhabited(typ) for typ in narrowed_inner_types):
            new_type = TupleType(narrowed_inner_types, current_type.partial_fallback)
        else:
            new_type = UninhabitedType()

        if all(is_uninhabited(typ) for typ in inner_rest_types):
            # All subpatterns always match, so we can apply negative narrowing
            rest_type = TupleType(rest_inner_types, current_type.partial_fallback)
    else:
        new_inner_type = UninhabitedType()
        for typ in new_inner_types:
            new_inner_type = join_types(new_inner_type, typ)
        new_type = self.construct_sequence_child(current_type, new_inner_type)
        if is_subtype(new_type, current_type):
            new_type, _ = self.chk.conditional_types_with_intersection(
                current_type, [get_type_range(new_type)], o, default=current_type
            )
        else:
            new_type = current_type
    return PatternType(new_type, rest_type, captures)

</t>
<t tx="ekr.20230831011819.73">def find_config_file_line_number(path: str, section: str, setting_name: str) -&gt; int:
    """Return the approximate location of setting_name within mypy config file.

    Return -1 if can't determine the line unambiguously.
    """
    in_desired_section = False
    try:
        results = []
        with open(path, encoding="UTF-8") as f:
            for i, line in enumerate(f):
                line = line.strip()
                if line.startswith("[") and line.endswith("]"):
                    current_section = line[1:-1].strip()
                    in_desired_section = current_section == section
                elif in_desired_section and re.match(rf"{setting_name}\s*=", line):
                    results.append(i + 1)
        if len(results) == 1:
            return results[0]
    except OSError:
        pass
    return -1


</t>
<t tx="ekr.20230831011819.730">def get_sequence_type(self, t: Type, context: Context) -&gt; Type | None:
    t = get_proper_type(t)
    if isinstance(t, AnyType):
        return AnyType(TypeOfAny.from_another_any, t)
    if isinstance(t, UnionType):
        items = [self.get_sequence_type(item, context) for item in t.items]
        not_none_items = [item for item in items if item is not None]
        if not_none_items:
            return make_simplified_union(not_none_items)
        else:
            return None

    if self.chk.type_is_iterable(t) and isinstance(t, (Instance, TupleType)):
        if isinstance(t, TupleType):
            t = tuple_fallback(t)
        return self.chk.iterable_item_type(t, context)
    else:
        return None

</t>
<t tx="ekr.20230831011819.731">def contract_starred_pattern_types(
    self, types: list[Type], star_pos: int | None, num_patterns: int
) -&gt; list[Type]:
    """
    Contracts a list of types in a sequence pattern depending on the position of a starred
    capture pattern.

    For example if the sequence pattern [a, *b, c] is matched against types [bool, int, str,
    bytes] the contracted types are [bool, Union[int, str], bytes].

    If star_pos in None the types are returned unchanged.
    """
    if star_pos is None:
        return types
    new_types = types[:star_pos]
    star_length = len(types) - num_patterns
    new_types.append(make_simplified_union(types[star_pos : star_pos + star_length]))
    new_types += types[star_pos + star_length :]

    return new_types

</t>
<t tx="ekr.20230831011819.732">def expand_starred_pattern_types(
    self, types: list[Type], star_pos: int | None, num_types: int
) -&gt; list[Type]:
    """Undoes the contraction done by contract_starred_pattern_types.

    For example if the sequence pattern is [a, *b, c] and types [bool, int, str] are extended
    to length 4 the result is [bool, int, int, str].
    """
    if star_pos is None:
        return types
    new_types = types[:star_pos]
    star_length = num_types - len(types) + 1
    new_types += [types[star_pos]] * star_length
    new_types += types[star_pos + 1 :]

    return new_types

</t>
<t tx="ekr.20230831011819.733">def visit_starred_pattern(self, o: StarredPattern) -&gt; PatternType:
    captures: dict[Expression, Type] = {}
    if o.capture is not None:
        list_type = self.chk.named_generic_type("builtins.list", [self.type_context[-1]])
        captures[o.capture] = list_type
    return PatternType(self.type_context[-1], UninhabitedType(), captures)

</t>
<t tx="ekr.20230831011819.734">def visit_mapping_pattern(self, o: MappingPattern) -&gt; PatternType:
    current_type = get_proper_type(self.type_context[-1])
    can_match = True
    captures: dict[Expression, Type] = {}
    for key, value in zip(o.keys, o.values):
        inner_type = self.get_mapping_item_type(o, current_type, key)
        if inner_type is None:
            can_match = False
            inner_type = self.chk.named_type("builtins.object")
        pattern_type = self.accept(value, inner_type)
        if is_uninhabited(pattern_type.type):
            can_match = False
        else:
            self.update_type_map(captures, pattern_type.captures)

    if o.rest is not None:
        mapping = self.chk.named_type("typing.Mapping")
        if is_subtype(current_type, mapping) and isinstance(current_type, Instance):
            mapping_inst = map_instance_to_supertype(current_type, mapping.type)
            dict_typeinfo = self.chk.lookup_typeinfo("builtins.dict")
            rest_type = Instance(dict_typeinfo, mapping_inst.args)
        else:
            object_type = self.chk.named_type("builtins.object")
            rest_type = self.chk.named_generic_type(
                "builtins.dict", [object_type, object_type]
            )

        captures[o.rest] = rest_type

    if can_match:
        # We can't narrow the type here, as Mapping key is invariant.
        new_type = self.type_context[-1]
    else:
        new_type = UninhabitedType()
    return PatternType(new_type, current_type, captures)

</t>
<t tx="ekr.20230831011819.735">def get_mapping_item_type(
    self, pattern: MappingPattern, mapping_type: Type, key: Expression
) -&gt; Type | None:
    mapping_type = get_proper_type(mapping_type)
    if isinstance(mapping_type, TypedDictType):
        with self.msg.filter_errors() as local_errors:
            result: Type | None = self.chk.expr_checker.visit_typeddict_index_expr(
                mapping_type, key
            )
            has_local_errors = local_errors.has_new_errors()
        # If we can't determine the type statically fall back to treating it as a normal
        # mapping
        if has_local_errors:
            with self.msg.filter_errors() as local_errors:
                result = self.get_simple_mapping_item_type(pattern, mapping_type, key)

                if local_errors.has_new_errors():
                    result = None
    else:
        with self.msg.filter_errors():
            result = self.get_simple_mapping_item_type(pattern, mapping_type, key)
    return result

</t>
<t tx="ekr.20230831011819.736">def get_simple_mapping_item_type(
    self, pattern: MappingPattern, mapping_type: Type, key: Expression
) -&gt; Type:
    result, _ = self.chk.expr_checker.check_method_call_by_name(
        "__getitem__", mapping_type, [key], [ARG_POS], pattern
    )
    return result

</t>
<t tx="ekr.20230831011819.737">def visit_class_pattern(self, o: ClassPattern) -&gt; PatternType:
    current_type = get_proper_type(self.type_context[-1])

    #
    # Check class type
    #
    type_info = o.class_ref.node
    if type_info is None:
        return PatternType(AnyType(TypeOfAny.from_error), AnyType(TypeOfAny.from_error), {})
    if isinstance(type_info, TypeAlias) and not type_info.no_args:
        self.msg.fail(message_registry.CLASS_PATTERN_GENERIC_TYPE_ALIAS, o)
        return self.early_non_match()
    if isinstance(type_info, TypeInfo):
        any_type = AnyType(TypeOfAny.implementation_artifact)
        typ: Type = Instance(type_info, [any_type] * len(type_info.defn.type_vars))
    elif isinstance(type_info, TypeAlias):
        typ = type_info.target
    else:
        if isinstance(type_info, Var) and type_info.type is not None:
            name = type_info.type.str_with_options(self.options)
        else:
            name = type_info.name
        self.msg.fail(message_registry.CLASS_PATTERN_TYPE_REQUIRED.format(name), o)
        return self.early_non_match()

    new_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type, [get_type_range(typ)], o, default=current_type
    )
    if is_uninhabited(new_type):
        return self.early_non_match()
    # TODO: Do I need this?
    narrowed_type = narrow_declared_type(current_type, new_type)

    #
    # Convert positional to keyword patterns
    #
    keyword_pairs: list[tuple[str | None, Pattern]] = []
    match_arg_set: set[str] = set()

    captures: dict[Expression, Type] = {}

    if len(o.positionals) != 0:
        if self.should_self_match(typ):
            if len(o.positionals) &gt; 1:
                self.msg.fail(message_registry.CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS, o)
            pattern_type = self.accept(o.positionals[0], narrowed_type)
            if not is_uninhabited(pattern_type.type):
                return PatternType(
                    pattern_type.type,
                    join_types(rest_type, pattern_type.rest_type),
                    pattern_type.captures,
                )
            captures = pattern_type.captures
        else:
            with self.msg.filter_errors() as local_errors:
                match_args_type = analyze_member_access(
                    "__match_args__",
                    typ,
                    o,
                    False,
                    False,
                    False,
                    self.msg,
                    original_type=typ,
                    chk=self.chk,
                )
                has_local_errors = local_errors.has_new_errors()
            if has_local_errors:
                self.msg.fail(
                    message_registry.MISSING_MATCH_ARGS.format(
                        typ.str_with_options(self.options)
                    ),
                    o,
                )
                return self.early_non_match()

            proper_match_args_type = get_proper_type(match_args_type)
            if isinstance(proper_match_args_type, TupleType):
                match_arg_names = get_match_arg_names(proper_match_args_type)

                if len(o.positionals) &gt; len(match_arg_names):
                    self.msg.fail(message_registry.CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS, o)
                    return self.early_non_match()
            else:
                match_arg_names = [None] * len(o.positionals)

            for arg_name, pos in zip(match_arg_names, o.positionals):
                keyword_pairs.append((arg_name, pos))
                if arg_name is not None:
                    match_arg_set.add(arg_name)

    #
    # Check for duplicate patterns
    #
    keyword_arg_set = set()
    has_duplicates = False
    for key, value in zip(o.keyword_keys, o.keyword_values):
        keyword_pairs.append((key, value))
        if key in match_arg_set:
            self.msg.fail(
                message_registry.CLASS_PATTERN_KEYWORD_MATCHES_POSITIONAL.format(key), value
            )
            has_duplicates = True
        elif key in keyword_arg_set:
            self.msg.fail(
                message_registry.CLASS_PATTERN_DUPLICATE_KEYWORD_PATTERN.format(key), value
            )
            has_duplicates = True
        keyword_arg_set.add(key)

    if has_duplicates:
        return self.early_non_match()

    #
    # Check keyword patterns
    #
    can_match = True
    for keyword, pattern in keyword_pairs:
        key_type: Type | None = None
        with self.msg.filter_errors() as local_errors:
            if keyword is not None:
                key_type = analyze_member_access(
                    keyword,
                    narrowed_type,
                    pattern,
                    False,
                    False,
                    False,
                    self.msg,
                    original_type=new_type,
                    chk=self.chk,
                )
            else:
                key_type = AnyType(TypeOfAny.from_error)
            has_local_errors = local_errors.has_new_errors()
        if has_local_errors or key_type is None:
            key_type = AnyType(TypeOfAny.from_error)
            self.msg.fail(
                message_registry.CLASS_PATTERN_UNKNOWN_KEYWORD.format(
                    typ.str_with_options(self.options), keyword
                ),
                pattern,
            )

        inner_type, inner_rest_type, inner_captures = self.accept(pattern, key_type)
        if is_uninhabited(inner_type):
            can_match = False
        else:
            self.update_type_map(captures, inner_captures)
            if not is_uninhabited(inner_rest_type):
                rest_type = current_type

    if not can_match:
        new_type = UninhabitedType()
    return PatternType(new_type, rest_type, captures)

</t>
<t tx="ekr.20230831011819.738">def should_self_match(self, typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and typ.type.is_named_tuple:
        return False
    for other in self.self_match_types:
        if is_subtype(typ, other):
            return True
    return False

</t>
<t tx="ekr.20230831011819.739">def can_match_sequence(self, typ: ProperType) -&gt; bool:
    if isinstance(typ, UnionType):
        return any(self.can_match_sequence(get_proper_type(item)) for item in typ.items)
    for other in self.non_sequence_match_types:
        # We have to ignore promotions, as memoryview should match, but bytes,
        # which it can be promoted to, shouldn't
        if is_subtype(typ, other, ignore_promotions=True):
            return False
    sequence = self.chk.named_type("typing.Sequence")
    # If the static type is more general than sequence the actual type could still match
    return is_subtype(typ, sequence) or is_subtype(sequence, typ)

</t>
<t tx="ekr.20230831011819.74">class BuildManager:
    """This class holds shared state for building a mypy program.

    It is used to coordinate parsing, import processing, semantic
    analysis and type checking.  The actual build steps are carried
    out by dispatch().

    Attributes:
      data_dir:        Mypy data directory (contains stubs)
      search_paths:    SearchPaths instance indicating where to look for modules
      modules:         Mapping of module ID to MypyFile (shared by the passes)
      semantic_analyzer:
                       Semantic analyzer, pass 2
      all_types:       Map {Expression: Type} from all modules (enabled by export_types)
      options:         Build options
      missing_modules: Set of modules that could not be imported encountered so far
      stale_modules:   Set of modules that needed to be rechecked (only used by tests)
      fg_deps_meta:    Metadata for fine-grained dependencies caches associated with modules
      fg_deps:         A fine-grained dependency map
      version_id:      The current mypy version (based on commit id when possible)
      plugin:          Active mypy plugin(s)
      plugins_snapshot:
                       Snapshot of currently active user plugins (versions and hashes)
      old_plugins_snapshot:
                       Plugins snapshot from previous incremental run (or None in
                       non-incremental mode and if cache was not found)
      errors:          Used for reporting all errors
      flush_errors:    A function for processing errors after each SCC
      cache_enabled:   Whether cache is being read. This is set based on options,
                       but is disabled if fine-grained cache loading fails
                       and after an initial fine-grained load. This doesn't
                       determine whether we write cache files or not.
      quickstart_state:
                       A cache of filename -&gt; mtime/size/hash info used to avoid
                       needing to hash source files when using a cache with mismatching mtimes
      stats:           Dict with various instrumentation numbers, it is used
                       not only for debugging, but also required for correctness,
                       in particular to check consistency of the fine-grained dependency cache.
      fscache:         A file system cacher
      ast_cache:       AST cache to speed up mypy daemon
    """

    @others
</t>
<t tx="ekr.20230831011819.740">def generate_types_from_names(self, type_names: list[str]) -&gt; list[Type]:
    types: list[Type] = []
    for name in type_names:
        try:
            types.append(self.chk.named_type(name))
        except KeyError as e:
            # Some built in types are not defined in all test cases
            if not name.startswith("builtins."):
                raise e
    return types

</t>
<t tx="ekr.20230831011819.741">def update_type_map(
    self, original_type_map: dict[Expression, Type], extra_type_map: dict[Expression, Type]
) -&gt; None:
    # Calculating this would not be needed if TypeMap directly used literal hashes instead of
    # expressions, as suggested in the TODO above it's definition
    already_captured = {literal_hash(expr) for expr in original_type_map}
    for expr, typ in extra_type_map.items():
        if literal_hash(expr) in already_captured:
            node = get_var(expr)
            self.msg.fail(
                message_registry.MULTIPLE_ASSIGNMENTS_IN_PATTERN.format(node.name), expr
            )
        else:
            original_type_map[expr] = typ

</t>
<t tx="ekr.20230831011819.742">def construct_sequence_child(self, outer_type: Type, inner_type: Type) -&gt; Type:
    """
    If outer_type is a child class of typing.Sequence returns a new instance of
    outer_type, that is a Sequence of inner_type. If outer_type is not a child class of
    typing.Sequence just returns a Sequence of inner_type

    For example:
    construct_sequence_child(List[int], str) = List[str]

    TODO: this doesn't make sense. For example if one has class S(Sequence[int], Generic[T])
    or class T(Sequence[Tuple[T, T]]), there is no way any of those can map to Sequence[str].
    """
    proper_type = get_proper_type(outer_type)
    if isinstance(proper_type, UnionType):
        types = [
            self.construct_sequence_child(item, inner_type)
            for item in proper_type.items
            if self.can_match_sequence(get_proper_type(item))
        ]
        return make_simplified_union(types)
    sequence = self.chk.named_generic_type("typing.Sequence", [inner_type])
    if is_subtype(outer_type, self.chk.named_type("typing.Sequence")):
        proper_type = get_proper_type(outer_type)
        if isinstance(proper_type, TupleType):
            proper_type = tuple_fallback(proper_type)
        assert isinstance(proper_type, Instance)
        empty_type = fill_typevars(proper_type.type)
        partial_type = expand_type_by_instance(empty_type, sequence)
        return expand_type_by_instance(partial_type, proper_type)
    else:
        return sequence

</t>
<t tx="ekr.20230831011819.743">def early_non_match(self) -&gt; PatternType:
    return PatternType(UninhabitedType(), self.type_context[-1], {})


</t>
<t tx="ekr.20230831011819.744">def get_match_arg_names(typ: TupleType) -&gt; list[str | None]:
    args: list[str | None] = []
    for item in typ.items:
        values = try_getting_str_literals_from_type(item)
        if values is None or len(values) != 1:
            args.append(None)
        else:
            args.append(values[0])
    return args


</t>
<t tx="ekr.20230831011819.745">def get_var(expr: Expression) -&gt; Var:
    """
    Warning: this in only true for expressions captured by a match statement.
    Don't call it from anywhere else
    """
    assert isinstance(expr, NameExpr)
    node = expr.node
    assert isinstance(node, Var)
    return node


</t>
<t tx="ekr.20230831011819.746">def get_type_range(typ: Type) -&gt; mypy.checker.TypeRange:
    typ = get_proper_type(typ)
    if (
        isinstance(typ, Instance)
        and typ.last_known_value
        and isinstance(typ.last_known_value.value, bool)
    ):
        typ = typ.last_known_value
    return mypy.checker.TypeRange(typ, is_upper_bound=False)


</t>
<t tx="ekr.20230831011819.747">def is_uninhabited(typ: Type) -&gt; bool:
    return isinstance(get_proper_type(typ), UninhabitedType)
</t>
<t tx="ekr.20230831011819.748">@path mypy
&lt;&lt; checkstrformat.py: docstring &gt;&gt;
&lt;&lt; checkstrformat.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.75">def __init__(
    self,
    data_dir: str,
    search_paths: SearchPaths,
    ignore_prefix: str,
    source_set: BuildSourceSet,
    reports: Reports | None,
    options: Options,
    version_id: str,
    plugin: Plugin,
    plugins_snapshot: dict[str, str],
    errors: Errors,
    flush_errors: Callable[[list[str], bool], None],
    fscache: FileSystemCache,
    stdout: TextIO,
    stderr: TextIO,
) -&gt; None:
    self.stats: dict[str, Any] = {}  # Values are ints or floats
    self.stdout = stdout
    self.stderr = stderr
    self.start_time = time.time()
    self.data_dir = data_dir
    self.errors = errors
    self.errors.set_ignore_prefix(ignore_prefix)
    self.search_paths = search_paths
    self.source_set = source_set
    self.reports = reports
    self.options = options
    self.version_id = version_id
    self.modules: dict[str, MypyFile] = {}
    self.missing_modules: set[str] = set()
    self.fg_deps_meta: dict[str, FgDepMeta] = {}
    # fg_deps holds the dependencies of every module that has been
    # processed. We store this in BuildManager so that we can compute
    # dependencies as we go, which allows us to free ASTs and type information,
    # saving a ton of memory on net.
    self.fg_deps: dict[str, set[str]] = {}
    # Always convert the plugin to a ChainedPlugin so that it can be manipulated if needed
    if not isinstance(plugin, ChainedPlugin):
        plugin = ChainedPlugin(options, [plugin])
    self.plugin = plugin
    # Set of namespaces (module or class) that are being populated during semantic
    # analysis and may have missing definitions.
    self.incomplete_namespaces: set[str] = set()
    self.semantic_analyzer = SemanticAnalyzer(
        self.modules,
        self.missing_modules,
        self.incomplete_namespaces,
        self.errors,
        self.plugin,
    )
    self.all_types: dict[Expression, Type] = {}  # Enabled by export_types
    self.indirection_detector = TypeIndirectionVisitor()
    self.stale_modules: set[str] = set()
    self.rechecked_modules: set[str] = set()
    self.flush_errors = flush_errors
    has_reporters = reports is not None and reports.reporters
    self.cache_enabled = (
        options.incremental
        and (not options.fine_grained_incremental or options.use_fine_grained_cache)
        and not has_reporters
    )
    self.fscache = fscache
    self.find_module_cache = FindModuleCache(
        self.search_paths, self.fscache, self.options, source_set=self.source_set
    )
    for module in CORE_BUILTIN_MODULES:
        if options.use_builtins_fixtures:
            continue
        path = self.find_module_cache.find_module(module)
        if not isinstance(path, str):
            raise CompileError(
                [f"Failed to find builtin module {module}, perhaps typeshed is broken?"]
            )
        if is_typeshed_file(options.abs_custom_typeshed_dir, path) or is_stub_package_file(
            path
        ):
            continue

        raise CompileError(
            [
                f'mypy: "{os.path.relpath(path)}" shadows library module "{module}"',
                f'note: A user-defined top-level module with name "{module}" is not supported',
            ]
        )

    self.metastore = create_metastore(options)

    # a mapping from source files to their corresponding shadow files
    # for efficient lookup
    self.shadow_map: dict[str, str] = {}
    if self.options.shadow_file is not None:
        self.shadow_map = {
            source_file: shadow_file for (source_file, shadow_file) in self.options.shadow_file
        }
    # a mapping from each file being typechecked to its possible shadow file
    self.shadow_equivalence_map: dict[str, str | None] = {}
    self.plugin = plugin
    self.plugins_snapshot = plugins_snapshot
    self.old_plugins_snapshot = read_plugins_snapshot(self)
    self.quickstart_state = read_quickstart_file(options, self.stdout)
    # Fine grained targets (module top levels and top level functions) processed by
    # the semantic analyzer, used only for testing. Currently used only by the new
    # semantic analyzer. Tuple of module and target name.
    self.processed_targets: list[tuple[str, str]] = []
    # Missing stub packages encountered.
    self.missing_stub_packages: set[str] = set()
    # Cache for mypy ASTs that have completed semantic analysis
    # pass 1. When multiple files are added to the build in a
    # single daemon increment, only one of the files gets added
    # per step and the others are discarded. This gets repeated
    # until all the files have been added. This means that a
    # new file can be processed O(n**2) times. This cache
    # avoids most of this redundant work.
    self.ast_cache: dict[str, tuple[MypyFile, list[ErrorInfo]]] = {}

</t>
<t tx="ekr.20230831011819.750">from __future__ import annotations

import re
from typing import TYPE_CHECKING, Callable, Dict, Final, Match, Pattern, Tuple, Union, cast
from typing_extensions import TypeAlias as _TypeAlias

import mypy.errorcodes as codes
from mypy.errors import Errors
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    BytesExpr,
    CallExpr,
    Context,
    DictExpr,
    Expression,
    ExpressionStmt,
    IndexExpr,
    IntExpr,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    StarExpr,
    StrExpr,
    TempNode,
    TupleExpr,
)
from mypy.types import (
    AnyType,
    Instance,
    LiteralType,
    TupleType,
    Type,
    TypeOfAny,
    TypeVarType,
    UnionType,
    get_proper_type,
    get_proper_types,
)

if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.checker
    import mypy.checkexpr

from mypy import message_registry
from mypy.maptype import map_instance_to_supertype
from mypy.messages import MessageBuilder
from mypy.parse import parse
from mypy.subtypes import is_subtype
from mypy.typeops import custom_special_method

FormatStringExpr: _TypeAlias = Union[StrExpr, BytesExpr]
Checkers: _TypeAlias = Tuple[Callable[[Expression], None], Callable[[Type], bool]]
MatchMap: _TypeAlias = Dict[Tuple[int, int], Match[str]]  # span -&gt; match


</t>
<t tx="ekr.20230831011819.751">def compile_format_re() -&gt; Pattern[str]:
    """Construct regexp to match format conversion specifiers in % interpolation.

    See https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting
    The regexp is intentionally a bit wider to report better errors.
    """
    key_re = r"(\((?P&lt;key&gt;[^)]*)\))?"  # (optional) parenthesised sequence of characters.
    flags_re = r"(?P&lt;flags&gt;[#0\-+ ]*)"  # (optional) sequence of flags.
    width_re = r"(?P&lt;width&gt;[1-9][0-9]*|\*)?"  # (optional) minimum field width (* or numbers).
    precision_re = r"(?:\.(?P&lt;precision&gt;\*|[0-9]+)?)?"  # (optional) . followed by * of numbers.
    length_mod_re = r"[hlL]?"  # (optional) length modifier (unused).
    type_re = r"(?P&lt;type&gt;.)?"  # conversion type.
    format_re = "%" + key_re + flags_re + width_re + precision_re + length_mod_re + type_re
    return re.compile(format_re)


</t>
<t tx="ekr.20230831011819.752">def compile_new_format_re(custom_spec: bool) -&gt; Pattern[str]:
    """Construct regexps to match format conversion specifiers in str.format() calls.

    See After https://docs.python.org/3/library/string.html#formatspec for
    specifications. The regexps are intentionally wider, to report better errors,
    instead of just not matching.
    """

    # Field (optional) is an integer/identifier possibly followed by several .attr and [index].
    field = r"(?P&lt;field&gt;(?P&lt;key&gt;[^.[!:]*)([^:!]+)?)"

    # Conversion (optional) is ! followed by one of letters for forced repr(), str(), or ascii().
    conversion = r"(?P&lt;conversion&gt;![^:])?"

    # Format specification (optional) follows its own mini-language:
    if not custom_spec:
        # Fill and align is valid for all builtin types.
        fill_align = r"(?P&lt;fill_align&gt;.?[&lt;&gt;=^])?"
        # Number formatting options are only valid for int, float, complex, and Decimal,
        # except if only width is given (it is valid for all types).
        # This contains sign, flags (sign, # and/or 0), width, grouping (_ or ,) and precision.
        num_spec = r"(?P&lt;flags&gt;[+\- ]?#?0?)(?P&lt;width&gt;\d+)?[_,]?(?P&lt;precision&gt;\.\d+)?"
        # The last element is type.
        conv_type = r"(?P&lt;type&gt;.)?"  # only some are supported, but we want to give a better error
        format_spec = r"(?P&lt;format_spec&gt;:" + fill_align + num_spec + conv_type + r")?"
    else:
        # Custom types can define their own form_spec using __format__().
        format_spec = r"(?P&lt;format_spec&gt;:.*)?"

    return re.compile(field + conversion + format_spec)


</t>
<t tx="ekr.20230831011819.753">FORMAT_RE: Final = compile_format_re()
FORMAT_RE_NEW: Final = compile_new_format_re(False)
FORMAT_RE_NEW_CUSTOM: Final = compile_new_format_re(True)
DUMMY_FIELD_NAME: Final = "__dummy_name__"

# Types that require either int or float.
NUMERIC_TYPES_OLD: Final = {"d", "i", "o", "u", "x", "X", "e", "E", "f", "F", "g", "G"}
NUMERIC_TYPES_NEW: Final = {"b", "d", "o", "e", "E", "f", "F", "g", "G", "n", "x", "X", "%"}

# These types accept _only_ int.
REQUIRE_INT_OLD: Final = {"o", "x", "X"}
REQUIRE_INT_NEW: Final = {"b", "d", "o", "x", "X"}

# These types fall back to SupportsFloat with % (other fall back to SupportsInt)
FLOAT_TYPES: Final = {"e", "E", "f", "F", "g", "G"}


class ConversionSpecifier:
    @others
</t>
<t tx="ekr.20230831011819.754">def __init__(
    self, match: Match[str], start_pos: int = -1, non_standard_format_spec: bool = False
) -&gt; None:
    self.whole_seq = match.group()
    self.start_pos = start_pos

    m_dict = match.groupdict()
    self.key = m_dict.get("key")

    # Replace unmatched optional groups with empty matches (for convenience).
    self.conv_type = m_dict.get("type", "")
    self.flags = m_dict.get("flags", "")
    self.width = m_dict.get("width", "")
    self.precision = m_dict.get("precision", "")

    # Used only for str.format() calls (it may be custom for types with __format__()).
    self.format_spec = m_dict.get("format_spec")
    self.non_standard_format_spec = non_standard_format_spec
    # Used only for str.format() calls.
    self.conversion = m_dict.get("conversion")
    # Full formatted expression (i.e. key plus following attributes and/or indexes).
    # Used only for str.format() calls.
    self.field = m_dict.get("field")

</t>
<t tx="ekr.20230831011819.755">def has_key(self) -&gt; bool:
    return self.key is not None

</t>
<t tx="ekr.20230831011819.756">def has_star(self) -&gt; bool:
    return self.width == "*" or self.precision == "*"


</t>
<t tx="ekr.20230831011819.757">def parse_conversion_specifiers(format_str: str) -&gt; list[ConversionSpecifier]:
    """Parse c-printf-style format string into list of conversion specifiers."""
    specifiers: list[ConversionSpecifier] = []
    for m in re.finditer(FORMAT_RE, format_str):
        specifiers.append(ConversionSpecifier(m, start_pos=m.start()))
    return specifiers


</t>
<t tx="ekr.20230831011819.758">def parse_format_value(
    format_value: str, ctx: Context, msg: MessageBuilder, nested: bool = False
) -&gt; list[ConversionSpecifier] | None:
    """Parse format string into list of conversion specifiers.

    The specifiers may be nested (two levels maximum), in this case they are ordered as
    '{0:{1}}, {2:{3}{4}}'. Return None in case of an error.
    """
    top_targets = find_non_escaped_targets(format_value, ctx, msg)
    if top_targets is None:
        return None

    result: list[ConversionSpecifier] = []
    for target, start_pos in top_targets:
        match = FORMAT_RE_NEW.fullmatch(target)
        if match:
            conv_spec = ConversionSpecifier(match, start_pos=start_pos)
        else:
            custom_match = FORMAT_RE_NEW_CUSTOM.fullmatch(target)
            if custom_match:
                conv_spec = ConversionSpecifier(
                    custom_match, start_pos=start_pos, non_standard_format_spec=True
                )
            else:
                msg.fail(
                    "Invalid conversion specifier in format string",
                    ctx,
                    code=codes.STRING_FORMATTING,
                )
                return None

        if conv_spec.key and ("{" in conv_spec.key or "}" in conv_spec.key):
            msg.fail("Conversion value must not contain { or }", ctx, code=codes.STRING_FORMATTING)
            return None
        result.append(conv_spec)

        # Parse nested conversions that are allowed in format specifier.
        if (
            conv_spec.format_spec
            and conv_spec.non_standard_format_spec
            and ("{" in conv_spec.format_spec or "}" in conv_spec.format_spec)
        ):
            if nested:
                msg.fail(
                    "Formatting nesting must be at most two levels deep",
                    ctx,
                    code=codes.STRING_FORMATTING,
                )
                return None
            sub_conv_specs = parse_format_value(conv_spec.format_spec, ctx, msg, nested=True)
            if sub_conv_specs is None:
                return None
            result.extend(sub_conv_specs)
    return result


</t>
<t tx="ekr.20230831011819.759">def find_non_escaped_targets(
    format_value: str, ctx: Context, msg: MessageBuilder
) -&gt; list[tuple[str, int]] | None:
    """Return list of raw (un-parsed) format specifiers in format string.

    Format specifiers don't include enclosing braces. We don't use regexp for
    this because they don't work well with nested/repeated patterns
    (both greedy and non-greedy), and these are heavily used internally for
    representation of f-strings.

    Return None in case of an error.
    """
    result = []
    next_spec = ""
    pos = 0
    nesting = 0
    while pos &lt; len(format_value):
        c = format_value[pos]
        if not nesting:
            # Skip any paired '{{' and '}}', enter nesting on '{', report error on '}'.
            if c == "{":
                if pos &lt; len(format_value) - 1 and format_value[pos + 1] == "{":
                    pos += 1
                else:
                    nesting = 1
            if c == "}":
                if pos &lt; len(format_value) - 1 and format_value[pos + 1] == "}":
                    pos += 1
                else:
                    msg.fail(
                        "Invalid conversion specifier in format string: unexpected }",
                        ctx,
                        code=codes.STRING_FORMATTING,
                    )
                    return None
        else:
            # Adjust nesting level, then either continue adding chars or move on.
            if c == "{":
                nesting += 1
            if c == "}":
                nesting -= 1
            if nesting:
                next_spec += c
            else:
                result.append((next_spec, pos - len(next_spec)))
                next_spec = ""
        pos += 1
    if nesting:
        msg.fail(
            "Invalid conversion specifier in format string: unmatched {",
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return None
    return result


</t>
<t tx="ekr.20230831011819.76">def dump_stats(self) -&gt; None:
    if self.options.dump_build_stats:
        print("Stats:")
        for key, value in sorted(self.stats_summary().items()):
            print(f"{key + ':':24}{value}")

</t>
<t tx="ekr.20230831011819.760">class StringFormatterChecker:
    """String interpolation/formatter type checker.

    This class works closely together with checker.ExpressionChecker.
    """

    @others
</t>
<t tx="ekr.20230831011819.761"># Some services are provided by a TypeChecker instance.
chk: mypy.checker.TypeChecker
# This is shared with TypeChecker, but stored also here for convenience.
msg: MessageBuilder
# Some services are provided by a ExpressionChecker instance.
exprchk: mypy.checkexpr.ExpressionChecker

def __init__(
    self,
    exprchk: mypy.checkexpr.ExpressionChecker,
    chk: mypy.checker.TypeChecker,
    msg: MessageBuilder,
) -&gt; None:
    """Construct an expression type checker."""
    self.chk = chk
    self.exprchk = exprchk
    self.msg = msg

</t>
<t tx="ekr.20230831011819.762">def check_str_format_call(self, call: CallExpr, format_value: str) -&gt; None:
    """Perform more precise checks for str.format() calls when possible.

    Currently the checks are performed for:
      * Actual string literals
      * Literal types with string values
      * Final names with string values

    The checks that we currently perform:
      * Check generic validity (e.g. unmatched { or }, and {} in invalid positions)
      * Check consistency of specifiers' auto-numbering
      * Verify that replacements can be found for all conversion specifiers,
        and all arguments were used
      * Non-standard format specs are only allowed for types with custom __format__
      * Type check replacements with accessors applied (if any).
      * Verify that specifier type is known and matches replacement type
      * Perform special checks for some specifier types:
        - 'c' requires a single character string
        - 's' must not accept bytes
        - non-empty flags are only allowed for numeric types
    """
    conv_specs = parse_format_value(format_value, call, self.msg)
    if conv_specs is None:
        return
    if not self.auto_generate_keys(conv_specs, call):
        return
    self.check_specs_in_format_call(call, conv_specs, format_value)

</t>
<t tx="ekr.20230831011819.763">def check_specs_in_format_call(
    self, call: CallExpr, specs: list[ConversionSpecifier], format_value: str
) -&gt; None:
    """Perform pairwise checks for conversion specifiers vs their replacements.

    The core logic for format checking is implemented in this method.
    """
    assert all(s.key for s in specs), "Keys must be auto-generated first!"
    replacements = self.find_replacements_in_call(call, [cast(str, s.key) for s in specs])
    assert len(replacements) == len(specs)
    for spec, repl in zip(specs, replacements):
        repl = self.apply_field_accessors(spec, repl, ctx=call)
        actual_type = repl.type if isinstance(repl, TempNode) else self.chk.lookup_type(repl)
        assert actual_type is not None

        # Special case custom formatting.
        if (
            spec.format_spec
            and spec.non_standard_format_spec
            and
            # Exclude "dynamic" specifiers (i.e. containing nested formatting).
            not ("{" in spec.format_spec or "}" in spec.format_spec)
        ):
            if (
                not custom_special_method(actual_type, "__format__", check_all=True)
                or spec.conversion
            ):
                # TODO: add support for some custom specs like datetime?
                self.msg.fail(
                    "Unrecognized format" ' specification "{}"'.format(spec.format_spec[1:]),
                    call,
                    code=codes.STRING_FORMATTING,
                )
                continue
        # Adjust expected and actual types.
        if not spec.conv_type:
            expected_type: Type | None = AnyType(TypeOfAny.special_form)
        else:
            assert isinstance(call.callee, MemberExpr)
            if isinstance(call.callee.expr, StrExpr):
                format_str = call.callee.expr
            else:
                format_str = StrExpr(format_value)
            expected_type = self.conversion_type(
                spec.conv_type, call, format_str, format_call=True
            )
        if spec.conversion is not None:
            # If the explicit conversion is given, then explicit conversion is called _first_.
            if spec.conversion[1] not in "rsa":
                self.msg.fail(
                    'Invalid conversion type "{}",'
                    ' must be one of "r", "s" or "a"'.format(spec.conversion[1]),
                    call,
                    code=codes.STRING_FORMATTING,
                )
            actual_type = self.named_type("builtins.str")

        # Perform the checks for given types.
        if expected_type is None:
            continue

        a_type = get_proper_type(actual_type)
        actual_items = (
            get_proper_types(a_type.items) if isinstance(a_type, UnionType) else [a_type]
        )
        for a_type in actual_items:
            if custom_special_method(a_type, "__format__"):
                continue
            self.check_placeholder_type(a_type, expected_type, call)
            self.perform_special_format_checks(spec, call, repl, a_type, expected_type)

</t>
<t tx="ekr.20230831011819.764">def perform_special_format_checks(
    self,
    spec: ConversionSpecifier,
    call: CallExpr,
    repl: Expression,
    actual_type: Type,
    expected_type: Type,
) -&gt; None:
    # TODO: try refactoring to combine this logic with % formatting.
    if spec.conv_type == "c":
        if isinstance(repl, (StrExpr, BytesExpr)) and len(repl.value) != 1:
            self.msg.requires_int_or_char(call, format_call=True)
        c_typ = get_proper_type(self.chk.lookup_type(repl))
        if isinstance(c_typ, Instance) and c_typ.last_known_value:
            c_typ = c_typ.last_known_value
        if isinstance(c_typ, LiteralType) and isinstance(c_typ.value, str):
            if len(c_typ.value) != 1:
                self.msg.requires_int_or_char(call, format_call=True)
    if (not spec.conv_type or spec.conv_type == "s") and not spec.conversion:
        if has_type_component(actual_type, "builtins.bytes") and not custom_special_method(
            actual_type, "__str__"
        ):
            self.msg.fail(
                'If x = b\'abc\' then f"{x}" or "{}".format(x) produces "b\'abc\'", '
                'not "abc". If this is desired behavior, use f"{x!r}" or "{!r}".format(x). '
                "Otherwise, decode the bytes",
                call,
                code=codes.STR_BYTES_PY3,
            )
    if spec.flags:
        numeric_types = UnionType(
            [self.named_type("builtins.int"), self.named_type("builtins.float")]
        )
        if (
            spec.conv_type
            and spec.conv_type not in NUMERIC_TYPES_NEW
            or not spec.conv_type
            and not is_subtype(actual_type, numeric_types)
            and not custom_special_method(actual_type, "__format__")
        ):
            self.msg.fail(
                "Numeric flags are only allowed for numeric types",
                call,
                code=codes.STRING_FORMATTING,
            )

</t>
<t tx="ekr.20230831011819.765">def find_replacements_in_call(self, call: CallExpr, keys: list[str]) -&gt; list[Expression]:
    """Find replacement expression for every specifier in str.format() call.

    In case of an error use TempNode(AnyType).
    """
    result: list[Expression] = []
    used: set[Expression] = set()
    for key in keys:
        if key.isdecimal():
            expr = self.get_expr_by_position(int(key), call)
            if not expr:
                self.msg.fail(
                    "Cannot find replacement for positional"
                    " format specifier {}".format(key),
                    call,
                    code=codes.STRING_FORMATTING,
                )
                expr = TempNode(AnyType(TypeOfAny.from_error))
        else:
            expr = self.get_expr_by_name(key, call)
            if not expr:
                self.msg.fail(
                    "Cannot find replacement for named" ' format specifier "{}"'.format(key),
                    call,
                    code=codes.STRING_FORMATTING,
                )
                expr = TempNode(AnyType(TypeOfAny.from_error))
        result.append(expr)
        if not isinstance(expr, TempNode):
            used.add(expr)
    # Strictly speaking not using all replacements is not a type error, but most likely
    # a typo in user code, so we show an error like we do for % formatting.
    total_explicit = len([kind for kind in call.arg_kinds if kind in (ARG_POS, ARG_NAMED)])
    if len(used) &lt; total_explicit:
        self.msg.too_many_string_formatting_arguments(call)
    return result

</t>
<t tx="ekr.20230831011819.766">def get_expr_by_position(self, pos: int, call: CallExpr) -&gt; Expression | None:
    """Get positional replacement expression from '{0}, {1}'.format(x, y, ...) call.

    If the type is from *args, return TempNode(&lt;item type&gt;). Return None in case of
    an error.
    """
    pos_args = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_POS]
    if pos &lt; len(pos_args):
        return pos_args[pos]
    star_args = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_STAR]
    if not star_args:
        return None

    # Fall back to *args when present in call.
    star_arg = star_args[0]
    varargs_type = get_proper_type(self.chk.lookup_type(star_arg))
    if not isinstance(varargs_type, Instance) or not varargs_type.type.has_base(
        "typing.Sequence"
    ):
        # Error should be already reported.
        return TempNode(AnyType(TypeOfAny.special_form))
    iter_info = self.chk.named_generic_type(
        "typing.Sequence", [AnyType(TypeOfAny.special_form)]
    ).type
    return TempNode(map_instance_to_supertype(varargs_type, iter_info).args[0])

</t>
<t tx="ekr.20230831011819.767">def get_expr_by_name(self, key: str, call: CallExpr) -&gt; Expression | None:
    """Get named replacement expression from '{name}'.format(name=...) call.

    If the type is from **kwargs, return TempNode(&lt;item type&gt;). Return None in case of
    an error.
    """
    named_args = [
        arg
        for arg, kind, name in zip(call.args, call.arg_kinds, call.arg_names)
        if kind == ARG_NAMED and name == key
    ]
    if named_args:
        return named_args[0]
    star_args_2 = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_STAR2]
    if not star_args_2:
        return None
    star_arg_2 = star_args_2[0]
    kwargs_type = get_proper_type(self.chk.lookup_type(star_arg_2))
    if not isinstance(kwargs_type, Instance) or not kwargs_type.type.has_base(
        "typing.Mapping"
    ):
        # Error should be already reported.
        return TempNode(AnyType(TypeOfAny.special_form))
    any_type = AnyType(TypeOfAny.special_form)
    mapping_info = self.chk.named_generic_type("typing.Mapping", [any_type, any_type]).type
    return TempNode(map_instance_to_supertype(kwargs_type, mapping_info).args[1])

</t>
<t tx="ekr.20230831011819.768">def auto_generate_keys(self, all_specs: list[ConversionSpecifier], ctx: Context) -&gt; bool:
    """Translate '{} {name} {}' to '{0} {name} {1}'.

    Return True if generation was successful, otherwise report an error and return false.
    """
    some_defined = any(s.key and s.key.isdecimal() for s in all_specs)
    all_defined = all(bool(s.key) for s in all_specs)
    if some_defined and not all_defined:
        self.msg.fail(
            "Cannot combine automatic field numbering and manual field specification",
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return False
    if all_defined:
        return True
    next_index = 0
    for spec in all_specs:
        if not spec.key:
            str_index = str(next_index)
            spec.key = str_index
            # Update also the full field (i.e. turn {.x} into {0.x}).
            if not spec.field:
                spec.field = str_index
            else:
                spec.field = str_index + spec.field
            next_index += 1
    return True

</t>
<t tx="ekr.20230831011819.769">def apply_field_accessors(
    self, spec: ConversionSpecifier, repl: Expression, ctx: Context
) -&gt; Expression:
    """Transform and validate expr in '{.attr[item]}'.format(expr) into expr.attr['item'].

    If validation fails, return TempNode(AnyType).
    """
    assert spec.key, "Keys must be auto-generated first!"
    if spec.field == spec.key:
        return repl
    assert spec.field

    temp_errors = Errors(self.chk.options)
    dummy = DUMMY_FIELD_NAME + spec.field[len(spec.key) :]
    temp_ast: Node = parse(
        dummy, fnam="&lt;format&gt;", module=None, options=self.chk.options, errors=temp_errors
    )
    if temp_errors.is_errors():
        self.msg.fail(
            f'Syntax error in format specifier "{spec.field}"',
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return TempNode(AnyType(TypeOfAny.from_error))

    # These asserts are guaranteed by the original regexp.
    assert isinstance(temp_ast, MypyFile)
    temp_ast = temp_ast.defs[0]
    assert isinstance(temp_ast, ExpressionStmt)
    temp_ast = temp_ast.expr
    if not self.validate_and_transform_accessors(temp_ast, repl, spec, ctx=ctx):
        return TempNode(AnyType(TypeOfAny.from_error))

    # Check if there are any other errors (like missing members).
    # TODO: fix column to point to actual start of the format specifier _within_ string.
    temp_ast.line = ctx.line
    temp_ast.column = ctx.column
    self.exprchk.accept(temp_ast)
    return temp_ast

</t>
<t tx="ekr.20230831011819.77">def use_fine_grained_cache(self) -&gt; bool:
    return self.cache_enabled and self.options.use_fine_grained_cache

</t>
<t tx="ekr.20230831011819.770">def validate_and_transform_accessors(
    self,
    temp_ast: Expression,
    original_repl: Expression,
    spec: ConversionSpecifier,
    ctx: Context,
) -&gt; bool:
    """Validate and transform (in-place) format field accessors.

    On error, report it and return False. The transformations include replacing the dummy
    variable with actual replacement expression and translating any name expressions in an
    index into strings, so that this will work:

        class User(TypedDict):
            name: str
            id: int
        u: User
        '{[id]:d} -&gt; {[name]}'.format(u)
    """
    if not isinstance(temp_ast, (MemberExpr, IndexExpr)):
        self.msg.fail(
            "Only index and member expressions are allowed in"
            ' format field accessors; got "{}"'.format(spec.field),
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return False
    if isinstance(temp_ast, MemberExpr):
        node = temp_ast.expr
    else:
        node = temp_ast.base
        if not isinstance(temp_ast.index, (NameExpr, IntExpr)):
            assert spec.key, "Call this method only after auto-generating keys!"
            assert spec.field
            self.msg.fail(
                "Invalid index expression in format field"
                ' accessor "{}"'.format(spec.field[len(spec.key) :]),
                ctx,
                code=codes.STRING_FORMATTING,
            )
            return False
        if isinstance(temp_ast.index, NameExpr):
            temp_ast.index = StrExpr(temp_ast.index.name)
    if isinstance(node, NameExpr) and node.name == DUMMY_FIELD_NAME:
        # Replace it with the actual replacement expression.
        assert isinstance(temp_ast, (IndexExpr, MemberExpr))  # XXX: this is redundant
        if isinstance(temp_ast, IndexExpr):
            temp_ast.base = original_repl
        else:
            temp_ast.expr = original_repl
        return True
    node.line = ctx.line
    node.column = ctx.column
    return self.validate_and_transform_accessors(
        node, original_repl=original_repl, spec=spec, ctx=ctx
    )

</t>
<t tx="ekr.20230831011819.771"># TODO: In Python 3, the bytes formatting has a more restricted set of options
#       compared to string formatting.
def check_str_interpolation(self, expr: FormatStringExpr, replacements: Expression) -&gt; Type:
    """Check the types of the 'replacements' in a string interpolation
    expression: str % replacements.
    """
    self.exprchk.accept(expr)
    specifiers = parse_conversion_specifiers(expr.value)
    has_mapping_keys = self.analyze_conversion_specifiers(specifiers, expr)
    if has_mapping_keys is None:
        pass  # Error was reported
    elif has_mapping_keys:
        self.check_mapping_str_interpolation(specifiers, replacements, expr)
    else:
        self.check_simple_str_interpolation(specifiers, replacements, expr)

    if isinstance(expr, BytesExpr):
        return self.named_type("builtins.bytes")
    elif isinstance(expr, StrExpr):
        return self.named_type("builtins.str")
    else:
        assert False

</t>
<t tx="ekr.20230831011819.772">def analyze_conversion_specifiers(
    self, specifiers: list[ConversionSpecifier], context: Context
) -&gt; bool | None:
    has_star = any(specifier.has_star() for specifier in specifiers)
    has_key = any(specifier.has_key() for specifier in specifiers)
    all_have_keys = all(
        specifier.has_key() or specifier.conv_type == "%" for specifier in specifiers
    )

    if has_key and has_star:
        self.msg.string_interpolation_with_star_and_key(context)
        return None
    if has_key and not all_have_keys:
        self.msg.string_interpolation_mixing_key_and_non_keys(context)
        return None
    return has_key

</t>
<t tx="ekr.20230831011819.773">def check_simple_str_interpolation(
    self,
    specifiers: list[ConversionSpecifier],
    replacements: Expression,
    expr: FormatStringExpr,
) -&gt; None:
    """Check % string interpolation with positional specifiers '%s, %d' % ('yes, 42')."""
    checkers = self.build_replacement_checkers(specifiers, replacements, expr)
    if checkers is None:
        return

    rhs_type = get_proper_type(self.accept(replacements))
    rep_types: list[Type] = []
    if isinstance(rhs_type, TupleType):
        rep_types = rhs_type.items
    elif isinstance(rhs_type, AnyType):
        return
    elif isinstance(rhs_type, Instance) and rhs_type.type.fullname == "builtins.tuple":
        # Assume that an arbitrary-length tuple has the right number of items.
        rep_types = [rhs_type.args[0]] * len(checkers)
    elif isinstance(rhs_type, UnionType):
        for typ in rhs_type.relevant_items():
            temp_node = TempNode(typ)
            temp_node.line = replacements.line
            self.check_simple_str_interpolation(specifiers, temp_node, expr)
        return
    else:
        rep_types = [rhs_type]

    if len(checkers) &gt; len(rep_types):
        # Only check the fix-length Tuple type. Other Iterable types would skip.
        if is_subtype(rhs_type, self.chk.named_type("typing.Iterable")) and not isinstance(
            rhs_type, TupleType
        ):
            return
        else:
            self.msg.too_few_string_formatting_arguments(replacements)
    elif len(checkers) &lt; len(rep_types):
        self.msg.too_many_string_formatting_arguments(replacements)
    else:
        if len(checkers) == 1:
            check_node, check_type = checkers[0]
            if isinstance(rhs_type, TupleType) and len(rhs_type.items) == 1:
                check_type(rhs_type.items[0])
            else:
                check_node(replacements)
        elif isinstance(replacements, TupleExpr) and not any(
            isinstance(item, StarExpr) for item in replacements.items
        ):
            for checks, rep_node in zip(checkers, replacements.items):
                check_node, check_type = checks
                check_node(rep_node)
        else:
            for checks, rep_type in zip(checkers, rep_types):
                check_node, check_type = checks
                check_type(rep_type)

</t>
<t tx="ekr.20230831011819.774">def check_mapping_str_interpolation(
    self,
    specifiers: list[ConversionSpecifier],
    replacements: Expression,
    expr: FormatStringExpr,
) -&gt; None:
    """Check % string interpolation with names specifiers '%(name)s' % {'name': 'John'}."""
    if isinstance(replacements, DictExpr) and all(
        isinstance(k, (StrExpr, BytesExpr)) for k, v in replacements.items
    ):
        mapping: dict[str, Type] = {}
        for k, v in replacements.items:
            if isinstance(expr, BytesExpr):
                # Special case: for bytes formatting keys must be bytes.
                if not isinstance(k, BytesExpr):
                    self.msg.fail(
                        "Dictionary keys in bytes formatting must be bytes, not strings",
                        expr,
                        code=codes.STRING_FORMATTING,
                    )
            key_str = cast(FormatStringExpr, k).value
            mapping[key_str] = self.accept(v)

        for specifier in specifiers:
            if specifier.conv_type == "%":
                # %% is allowed in mappings, no checking is required
                continue
            assert specifier.key is not None
            if specifier.key not in mapping:
                self.msg.key_not_in_mapping(specifier.key, replacements)
                return
            rep_type = mapping[specifier.key]
            assert specifier.conv_type is not None
            expected_type = self.conversion_type(specifier.conv_type, replacements, expr)
            if expected_type is None:
                return
            self.chk.check_subtype(
                rep_type,
                expected_type,
                replacements,
                message_registry.INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION,
                "expression has type",
                f"placeholder with key '{specifier.key}' has type",
                code=codes.STRING_FORMATTING,
            )
            if specifier.conv_type == "s":
                self.check_s_special_cases(expr, rep_type, expr)
    else:
        rep_type = self.accept(replacements)
        dict_type = self.build_dict_type(expr)
        self.chk.check_subtype(
            rep_type,
            dict_type,
            replacements,
            message_registry.FORMAT_REQUIRES_MAPPING,
            "expression has type",
            "expected type for mapping is",
            code=codes.STRING_FORMATTING,
        )

</t>
<t tx="ekr.20230831011819.775">def build_dict_type(self, expr: FormatStringExpr) -&gt; Type:
    """Build expected mapping type for right operand in % formatting."""
    any_type = AnyType(TypeOfAny.special_form)
    if isinstance(expr, BytesExpr):
        bytes_type = self.chk.named_generic_type("builtins.bytes", [])
        return self.chk.named_generic_type(
            "_typeshed.SupportsKeysAndGetItem", [bytes_type, any_type]
        )
    elif isinstance(expr, StrExpr):
        str_type = self.chk.named_generic_type("builtins.str", [])
        return self.chk.named_generic_type(
            "_typeshed.SupportsKeysAndGetItem", [str_type, any_type]
        )
    else:
        assert False, "Unreachable"

</t>
<t tx="ekr.20230831011819.776">def build_replacement_checkers(
    self, specifiers: list[ConversionSpecifier], context: Context, expr: FormatStringExpr
) -&gt; list[Checkers] | None:
    checkers: list[Checkers] = []
    for specifier in specifiers:
        checker = self.replacement_checkers(specifier, context, expr)
        if checker is None:
            return None
        checkers.extend(checker)
    return checkers

</t>
<t tx="ekr.20230831011819.777">def replacement_checkers(
    self, specifier: ConversionSpecifier, context: Context, expr: FormatStringExpr
) -&gt; list[Checkers] | None:
    """Returns a list of tuples of two functions that check whether a replacement is
    of the right type for the specifier. The first function takes a node and checks
    its type in the right type context. The second function just checks a type.
    """
    checkers: list[Checkers] = []

    if specifier.width == "*":
        checkers.append(self.checkers_for_star(context))
    if specifier.precision == "*":
        checkers.append(self.checkers_for_star(context))

    if specifier.conv_type == "c":
        c = self.checkers_for_c_type(specifier.conv_type, context, expr)
        if c is None:
            return None
        checkers.append(c)
    elif specifier.conv_type is not None and specifier.conv_type != "%":
        c = self.checkers_for_regular_type(specifier.conv_type, context, expr)
        if c is None:
            return None
        checkers.append(c)
    return checkers

</t>
<t tx="ekr.20230831011819.778">def checkers_for_star(self, context: Context) -&gt; Checkers:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with a star in a conversion specifier.
    """
    expected = self.named_type("builtins.int")

    def check_type(type: Type) -&gt; bool:
        expected = self.named_type("builtins.int")
        return self.chk.check_subtype(
            type, expected, context, "* wants int", code=codes.STRING_FORMATTING
        )

    def check_expr(expr: Expression) -&gt; None:
        type = self.accept(expr, expected)
        check_type(type)

    return check_expr, check_type

</t>
<t tx="ekr.20230831011819.779">def check_placeholder_type(self, typ: Type, expected_type: Type, context: Context) -&gt; bool:
    return self.chk.check_subtype(
        typ,
        expected_type,
        context,
        message_registry.INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION,
        "expression has type",
        "placeholder has type",
        code=codes.STRING_FORMATTING,
    )

</t>
<t tx="ekr.20230831011819.78">def maybe_swap_for_shadow_path(self, path: str) -&gt; str:
    if not self.shadow_map:
        return path

    path = normpath(path, self.options)

    previously_checked = path in self.shadow_equivalence_map
    if not previously_checked:
        for source, shadow in self.shadow_map.items():
            if self.fscache.samefile(path, source):
                self.shadow_equivalence_map[path] = shadow
                break
            else:
                self.shadow_equivalence_map[path] = None

    shadow_file = self.shadow_equivalence_map.get(path)
    return shadow_file if shadow_file else path

</t>
<t tx="ekr.20230831011819.780">def checkers_for_regular_type(
    self, conv_type: str, context: Context, expr: FormatStringExpr
) -&gt; Checkers | None:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with 'type'. Return None in case of an error.
    """
    expected_type = self.conversion_type(conv_type, context, expr)
    if expected_type is None:
        return None

    def check_type(typ: Type) -&gt; bool:
        assert expected_type is not None
        ret = self.check_placeholder_type(typ, expected_type, context)
        if ret and conv_type == "s":
            ret = self.check_s_special_cases(expr, typ, context)
        return ret

    def check_expr(expr: Expression) -&gt; None:
        type = self.accept(expr, expected_type)
        check_type(type)

    return check_expr, check_type

</t>
<t tx="ekr.20230831011819.781">def check_s_special_cases(self, expr: FormatStringExpr, typ: Type, context: Context) -&gt; bool:
    """Additional special cases for %s in bytes vs string context."""
    if isinstance(expr, StrExpr):
        # Couple special cases for string formatting.
        if has_type_component(typ, "builtins.bytes"):
            self.msg.fail(
                'If x = b\'abc\' then "%s" % x produces "b\'abc\'", not "abc". '
                'If this is desired behavior use "%r" % x. Otherwise, decode the bytes',
                context,
                code=codes.STR_BYTES_PY3,
            )
            return False
    if isinstance(expr, BytesExpr):
        # A special case for bytes formatting: b'%s' actually requires bytes on Python 3.
        if has_type_component(typ, "builtins.str"):
            self.msg.fail(
                "On Python 3 b'%s' requires bytes, not string",
                context,
                code=codes.STRING_FORMATTING,
            )
            return False
    return True

</t>
<t tx="ekr.20230831011819.782">def checkers_for_c_type(
    self, type: str, context: Context, format_expr: FormatStringExpr
) -&gt; Checkers | None:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with 'type' that is a character type.
    """
    expected_type = self.conversion_type(type, context, format_expr)
    if expected_type is None:
        return None

    def check_type(type: Type) -&gt; bool:
        assert expected_type is not None
        if isinstance(format_expr, BytesExpr):
            err_msg = '"%c" requires an integer in range(256) or a single byte'
        else:
            err_msg = '"%c" requires int or char'
        return self.chk.check_subtype(
            type,
            expected_type,
            context,
            err_msg,
            "expression has type",
            code=codes.STRING_FORMATTING,
        )

    def check_expr(expr: Expression) -&gt; None:
        """int, or str with length 1"""
        type = self.accept(expr, expected_type)
        # We need further check with expr to make sure that
        # it has exact one char or one single byte.
        if check_type(type):
            # Python 3 doesn't support b'%c' % str
            if (
                isinstance(format_expr, BytesExpr)
                and isinstance(expr, BytesExpr)
                and len(expr.value) != 1
            ):
                self.msg.requires_int_or_single_byte(context)
            elif isinstance(expr, (StrExpr, BytesExpr)) and len(expr.value) != 1:
                self.msg.requires_int_or_char(context)

    return check_expr, check_type

</t>
<t tx="ekr.20230831011819.783">def conversion_type(
    self, p: str, context: Context, expr: FormatStringExpr, format_call: bool = False
) -&gt; Type | None:
    """Return the type that is accepted for a string interpolation conversion specifier type.

    Note that both Python's float (e.g. %f) and integer (e.g. %d)
    specifier types accept both float and integers.

    The 'format_call' argument indicates whether this type came from % interpolation or from
    a str.format() call, the meaning of few formatting types are different.
    """
    NUMERIC_TYPES = NUMERIC_TYPES_NEW if format_call else NUMERIC_TYPES_OLD
    INT_TYPES = REQUIRE_INT_NEW if format_call else REQUIRE_INT_OLD
    if p == "b" and not format_call:
        if not isinstance(expr, BytesExpr):
            self.msg.fail(
                'Format character "b" is only supported on bytes patterns',
                context,
                code=codes.STRING_FORMATTING,
            )
            return None
        return self.named_type("builtins.bytes")
    elif p == "a":
        # TODO: return type object?
        return AnyType(TypeOfAny.special_form)
    elif p in ["s", "r"]:
        return AnyType(TypeOfAny.special_form)
    elif p in NUMERIC_TYPES:
        if p in INT_TYPES:
            numeric_types = [self.named_type("builtins.int")]
        else:
            numeric_types = [
                self.named_type("builtins.int"),
                self.named_type("builtins.float"),
            ]
            if not format_call:
                if p in FLOAT_TYPES:
                    numeric_types.append(self.named_type("typing.SupportsFloat"))
                else:
                    numeric_types.append(self.named_type("typing.SupportsInt"))
        return UnionType.make_union(numeric_types)
    elif p in ["c"]:
        if isinstance(expr, BytesExpr):
            return UnionType(
                [self.named_type("builtins.int"), self.named_type("builtins.bytes")]
            )
        else:
            return UnionType(
                [self.named_type("builtins.int"), self.named_type("builtins.str")]
            )
    else:
        self.msg.unsupported_placeholder(p, context)
        return None

</t>
<t tx="ekr.20230831011819.784">#
# Helpers
#

def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with type given by the name and no type
    arguments. Alias for TypeChecker.named_type.
    """
    return self.chk.named_type(name)

</t>
<t tx="ekr.20230831011819.785">def accept(self, expr: Expression, context: Type | None = None) -&gt; Type:
    """Type check a node. Alias for TypeChecker.accept."""
    return self.chk.expr_checker.accept(expr, context)


</t>
<t tx="ekr.20230831011819.786">def has_type_component(typ: Type, fullname: str) -&gt; bool:
    """Is this a specific instance type, or a union that contains it?

    We use this ad-hoc function instead of a proper visitor or subtype check
    because some str vs bytes errors are strictly speaking not runtime errors,
    but rather highly counter-intuitive behavior. This is similar to what is used for
    --strict-equality.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return typ.type.has_base(fullname)
    elif isinstance(typ, TypeVarType):
        return has_type_component(typ.upper_bound, fullname) or any(
            has_type_component(v, fullname) for v in typ.values
        )
    elif isinstance(typ, UnionType):
        return any(has_type_component(t, fullname) for t in typ.relevant_items())
    return False
</t>
<t tx="ekr.20230831011819.787">@path mypy
&lt;&lt; config_parser.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.788">from __future__ import annotations

import argparse
import configparser
import glob as fileglob
import os
import re
import sys
from io import StringIO

from mypy.errorcodes import error_codes

if sys.version_info &gt;= (3, 11):
    import tomllib
else:
    import tomli as tomllib

from typing import (
    Any,
    Callable,
    Dict,
    Final,
    Iterable,
    List,
    Mapping,
    MutableMapping,
    Sequence,
    TextIO,
    Tuple,
    Union,
)
from typing_extensions import TypeAlias as _TypeAlias

from mypy import defaults
from mypy.options import PER_MODULE_OPTIONS, Options

_CONFIG_VALUE_TYPES: _TypeAlias = Union[
    str, bool, int, float, Dict[str, str], List[str], Tuple[int, int]
]
_INI_PARSER_CALLABLE: _TypeAlias = Callable[[Any], _CONFIG_VALUE_TYPES]


</t>
<t tx="ekr.20230831011819.789">def parse_version(v: str | float) -&gt; tuple[int, int]:
    m = re.match(r"\A(\d)\.(\d+)\Z", str(v))
    if not m:
        raise argparse.ArgumentTypeError(f"Invalid python version '{v}' (expected format: 'x.y')")
    major, minor = int(m.group(1)), int(m.group(2))
    if major == 2 and minor == 7:
        pass  # Error raised elsewhere
    elif major == 3:
        if minor &lt; defaults.PYTHON3_VERSION_MIN[1]:
            msg = "Python 3.{} is not supported (must be {}.{} or higher)".format(
                minor, *defaults.PYTHON3_VERSION_MIN
            )

            if isinstance(v, float):
                msg += ". You may need to put quotes around your Python version"

            raise argparse.ArgumentTypeError(msg)
    else:
        raise argparse.ArgumentTypeError(
            f"Python major version '{major}' out of range (must be 3)"
        )
    return major, minor


</t>
<t tx="ekr.20230831011819.79">def get_stat(self, path: str) -&gt; os.stat_result:
    return self.fscache.stat(self.maybe_swap_for_shadow_path(path))

</t>
<t tx="ekr.20230831011819.790">def try_split(v: str | Sequence[str], split_regex: str = "[,]") -&gt; list[str]:
    """Split and trim a str or list of str into a list of str"""
    if isinstance(v, str):
        return [p.strip() for p in re.split(split_regex, v)]

    return [p.strip() for p in v]


</t>
<t tx="ekr.20230831011819.791">def validate_codes(codes: list[str]) -&gt; list[str]:
    invalid_codes = set(codes) - set(error_codes.keys())
    if invalid_codes:
        raise argparse.ArgumentTypeError(
            f"Invalid error code(s): {', '.join(sorted(invalid_codes))}"
        )
    return codes


</t>
<t tx="ekr.20230831011819.792">def validate_package_allow_list(allow_list: list[str]) -&gt; list[str]:
    for p in allow_list:
        msg = f"Invalid allow list entry: {p}"
        if "*" in p:
            raise argparse.ArgumentTypeError(
                f"{msg} (entries are already prefixes so must not contain *)"
            )
        if "\\" in p or "/" in p:
            raise argparse.ArgumentTypeError(
                f"{msg} (entries must be packages like foo.bar not directories or files)"
            )
    return allow_list


</t>
<t tx="ekr.20230831011819.793">def expand_path(path: str) -&gt; str:
    """Expand the user home directory and any environment variables contained within
    the provided path.
    """

    return os.path.expandvars(os.path.expanduser(path))


</t>
<t tx="ekr.20230831011819.794">def str_or_array_as_list(v: str | Sequence[str]) -&gt; list[str]:
    if isinstance(v, str):
        return [v.strip()] if v.strip() else []
    return [p.strip() for p in v if p.strip()]


</t>
<t tx="ekr.20230831011819.795">def split_and_match_files_list(paths: Sequence[str]) -&gt; list[str]:
    """Take a list of files/directories (with support for globbing through the glob library).

    Where a path/glob matches no file, we still include the raw path in the resulting list.

    Returns a list of file paths
    """
    expanded_paths = []

    for path in paths:
        path = expand_path(path.strip())
        globbed_files = fileglob.glob(path, recursive=True)
        if globbed_files:
            expanded_paths.extend(globbed_files)
        else:
            expanded_paths.append(path)

    return expanded_paths


</t>
<t tx="ekr.20230831011819.796">def split_and_match_files(paths: str) -&gt; list[str]:
    """Take a string representing a list of files/directories (with support for globbing
    through the glob library).

    Where a path/glob matches no file, we still include the raw path in the resulting list.

    Returns a list of file paths
    """

    return split_and_match_files_list(paths.split(","))


</t>
<t tx="ekr.20230831011819.797">def check_follow_imports(choice: str) -&gt; str:
    choices = ["normal", "silent", "skip", "error"]
    if choice not in choices:
        raise argparse.ArgumentTypeError(
            "invalid choice '{}' (choose from {})".format(
                choice, ", ".join(f"'{x}'" for x in choices)
            )
        )
    return choice


</t>
<t tx="ekr.20230831011819.798">def split_commas(value: str) -&gt; list[str]:
    # Uses a bit smarter technique to allow last trailing comma
    # and to remove last `""` item from the split.
    items = value.split(",")
    if items and items[-1] == "":
        items.pop(-1)
    return items


</t>
<t tx="ekr.20230831011819.799"># For most options, the type of the default value set in options.py is
# sufficient, and we don't have to do anything here.  This table
# exists to specify types for values initialized to None or container
# types.
ini_config_types: Final[dict[str, _INI_PARSER_CALLABLE]] = {
    "python_version": parse_version,
    "custom_typing_module": str,
    "custom_typeshed_dir": expand_path,
    "mypy_path": lambda s: [expand_path(p.strip()) for p in re.split("[,:]", s)],
    "files": split_and_match_files,
    "quickstart_file": expand_path,
    "junit_xml": expand_path,
    "follow_imports": check_follow_imports,
    "no_site_packages": bool,
    "plugins": lambda s: [p.strip() for p in split_commas(s)],
    "always_true": lambda s: [p.strip() for p in split_commas(s)],
    "always_false": lambda s: [p.strip() for p in split_commas(s)],
    "untyped_calls_exclude": lambda s: validate_package_allow_list(
        [p.strip() for p in split_commas(s)]
    ),
    "enable_incomplete_feature": lambda s: [p.strip() for p in split_commas(s)],
    "disable_error_code": lambda s: validate_codes([p.strip() for p in split_commas(s)]),
    "enable_error_code": lambda s: validate_codes([p.strip() for p in split_commas(s)]),
    "package_root": lambda s: [p.strip() for p in split_commas(s)],
    "cache_dir": expand_path,
    "python_executable": expand_path,
    "strict": bool,
    "exclude": lambda s: [s.strip()],
    "packages": try_split,
    "modules": try_split,
}

# Reuse the ini_config_types and overwrite the diff
toml_config_types: Final[dict[str, _INI_PARSER_CALLABLE]] = ini_config_types.copy()
toml_config_types.update(
    {
        "python_version": parse_version,
        "mypy_path": lambda s: [expand_path(p) for p in try_split(s, "[,:]")],
        "files": lambda s: split_and_match_files_list(try_split(s)),
        "follow_imports": lambda s: check_follow_imports(str(s)),
        "plugins": try_split,
        "always_true": try_split,
        "always_false": try_split,
        "untyped_calls_exclude": lambda s: validate_package_allow_list(try_split(s)),
        "enable_incomplete_feature": try_split,
        "disable_error_code": lambda s: validate_codes(try_split(s)),
        "enable_error_code": lambda s: validate_codes(try_split(s)),
        "package_root": try_split,
        "exclude": str_or_array_as_list,
        "packages": try_split,
        "modules": try_split,
    }
)


def parse_config_file(
    options: Options,
    set_strict_flags: Callable[[], None],
    filename: str | None,
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
) -&gt; None:
    """Parse a config file into an Options object.

    Errors are written to stderr but are not fatal.

    If filename is None, fall back to default config files.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr

    if filename is not None:
        config_files: tuple[str, ...] = (filename,)
    else:
        config_files_iter: Iterable[str] = map(os.path.expanduser, defaults.CONFIG_FILES)
        config_files = tuple(config_files_iter)

    config_parser = configparser.RawConfigParser()

    for config_file in config_files:
        if not os.path.exists(config_file):
            continue
        try:
            if is_toml(config_file):
                with open(config_file, "rb") as f:
                    toml_data = tomllib.load(f)
                # Filter down to just mypy relevant toml keys
                toml_data = toml_data.get("tool", {})
                if "mypy" not in toml_data:
                    continue
                toml_data = {"mypy": toml_data["mypy"]}
                parser: MutableMapping[str, Any] = destructure_overrides(toml_data)
                config_types = toml_config_types
            else:
                config_parser.read(config_file)
                parser = config_parser
                config_types = ini_config_types
        except (tomllib.TOMLDecodeError, configparser.Error, ConfigTOMLValueError) as err:
            print(f"{config_file}: {err}", file=stderr)
        else:
            if config_file in defaults.SHARED_CONFIG_FILES and "mypy" not in parser:
                continue
            file_read = config_file
            options.config_file = file_read
            break
    else:
        return

    os.environ["MYPY_CONFIG_FILE_DIR"] = os.path.dirname(os.path.abspath(config_file))

    if "mypy" not in parser:
        if filename or file_read not in defaults.SHARED_CONFIG_FILES:
            print(f"{file_read}: No [mypy] section in config file", file=stderr)
    else:
        section = parser["mypy"]
        prefix = f"{file_read}: [mypy]: "
        updates, report_dirs = parse_section(
            prefix, options, set_strict_flags, section, config_types, stderr
        )
        for k, v in updates.items():
            setattr(options, k, v)
        options.report_dirs.update(report_dirs)

    for name, section in parser.items():
        if name.startswith("mypy-"):
            prefix = get_prefix(file_read, name)
            updates, report_dirs = parse_section(
                prefix, options, set_strict_flags, section, config_types, stderr
            )
            if report_dirs:
                print(
                    "%sPer-module sections should not specify reports (%s)"
                    % (prefix, ", ".join(s + "_report" for s in sorted(report_dirs))),
                    file=stderr,
                )
            if set(updates) - PER_MODULE_OPTIONS:
                print(
                    "%sPer-module sections should only specify per-module flags (%s)"
                    % (prefix, ", ".join(sorted(set(updates) - PER_MODULE_OPTIONS))),
                    file=stderr,
                )
                updates = {k: v for k, v in updates.items() if k in PER_MODULE_OPTIONS}

            globs = name[5:]
            for glob in globs.split(","):
                # For backwards compatibility, replace (back)slashes with dots.
                glob = glob.replace(os.sep, ".")
                if os.altsep:
                    glob = glob.replace(os.altsep, ".")

                if any(c in glob for c in "?[]!") or any(
                    "*" in x and x != "*" for x in glob.split(".")
                ):
                    print(
                        "%sPatterns must be fully-qualified module names, optionally "
                        "with '*' in some components (e.g spam.*.eggs.*)" % prefix,
                        file=stderr,
                    )
                else:
                    options.per_module_options[glob] = updates


</t>
<t tx="ekr.20230831011819.80">def getmtime(self, path: str) -&gt; int:
    """Return a file's mtime; but 0 in bazel mode.

    (Bazel's distributed cache doesn't like filesystem metadata to
    end up in output files.)
    """
    if self.options.bazel:
        return 0
    else:
        return int(self.metastore.getmtime(path))

</t>
<t tx="ekr.20230831011819.800">def get_prefix(file_read: str, name: str) -&gt; str:
    if is_toml(file_read):
        module_name_str = 'module = "%s"' % "-".join(name.split("-")[1:])
    else:
        module_name_str = name

    return f"{file_read}: [{module_name_str}]: "


</t>
<t tx="ekr.20230831011819.801">def is_toml(filename: str) -&gt; bool:
    return filename.lower().endswith(".toml")


</t>
<t tx="ekr.20230831011819.802">def destructure_overrides(toml_data: dict[str, Any]) -&gt; dict[str, Any]:
    """Take the new [[tool.mypy.overrides]] section array in the pyproject.toml file,
    and convert it back to a flatter structure that the existing config_parser can handle.

    E.g. the following pyproject.toml file:

        [[tool.mypy.overrides]]
        module = [
            "a.b",
            "b.*"
        ]
        disallow_untyped_defs = true

        [[tool.mypy.overrides]]
        module = 'c'
        disallow_untyped_defs = false

    Would map to the following config dict that it would have gotten from parsing an equivalent
    ini file:

        {
            "mypy-a.b": {
                disallow_untyped_defs = true,
            },
            "mypy-b.*": {
                disallow_untyped_defs = true,
            },
            "mypy-c": {
                disallow_untyped_defs: false,
            },
        }
    """
    if "overrides" not in toml_data["mypy"]:
        return toml_data

    if not isinstance(toml_data["mypy"]["overrides"], list):
        raise ConfigTOMLValueError(
            "tool.mypy.overrides sections must be an array. Please make "
            "sure you are using double brackets like so: [[tool.mypy.overrides]]"
        )

    result = toml_data.copy()
    for override in result["mypy"]["overrides"]:
        if "module" not in override:
            raise ConfigTOMLValueError(
                "toml config file contains a [[tool.mypy.overrides]] "
                "section, but no module to override was specified."
            )

        if isinstance(override["module"], str):
            modules = [override["module"]]
        elif isinstance(override["module"], list):
            modules = override["module"]
        else:
            raise ConfigTOMLValueError(
                "toml config file contains a [[tool.mypy.overrides]] "
                "section with a module value that is not a string or a list of "
                "strings"
            )

        for module in modules:
            module_overrides = override.copy()
            del module_overrides["module"]
            old_config_name = f"mypy-{module}"
            if old_config_name not in result:
                result[old_config_name] = module_overrides
            else:
                for new_key, new_value in module_overrides.items():
                    if (
                        new_key in result[old_config_name]
                        and result[old_config_name][new_key] != new_value
                    ):
                        raise ConfigTOMLValueError(
                            "toml config file contains "
                            "[[tool.mypy.overrides]] sections with conflicting "
                            "values. Module '%s' has two different values for '%s'"
                            % (module, new_key)
                        )
                    result[old_config_name][new_key] = new_value

    del result["mypy"]["overrides"]
    return result


</t>
<t tx="ekr.20230831011819.803">def parse_section(
    prefix: str,
    template: Options,
    set_strict_flags: Callable[[], None],
    section: Mapping[str, Any],
    config_types: dict[str, Any],
    stderr: TextIO = sys.stderr,
) -&gt; tuple[dict[str, object], dict[str, str]]:
    """Parse one section of a config file.

    Returns a dict of option values encountered, and a dict of report directories.
    """
    results: dict[str, object] = {}
    report_dirs: dict[str, str] = {}
    for key in section:
        invert = False
        options_key = key
        if key in config_types:
            ct = config_types[key]
        else:
            dv = None
            # We have to keep new_semantic_analyzer in Options
            # for plugin compatibility but it is not a valid option anymore.
            assert hasattr(template, "new_semantic_analyzer")
            if key != "new_semantic_analyzer":
                dv = getattr(template, key, None)
            if dv is None:
                if key.endswith("_report"):
                    report_type = key[:-7].replace("_", "-")
                    if report_type in defaults.REPORTER_NAMES:
                        report_dirs[report_type] = str(section[key])
                    else:
                        print(f"{prefix}Unrecognized report type: {key}", file=stderr)
                    continue
                if key.startswith("x_"):
                    pass  # Don't complain about `x_blah` flags
                elif key.startswith("no_") and hasattr(template, key[3:]):
                    options_key = key[3:]
                    invert = True
                elif key.startswith("allow") and hasattr(template, "dis" + key):
                    options_key = "dis" + key
                    invert = True
                elif key.startswith("disallow") and hasattr(template, key[3:]):
                    options_key = key[3:]
                    invert = True
                elif key.startswith("show_") and hasattr(template, "hide_" + key[5:]):
                    options_key = "hide_" + key[5:]
                    invert = True
                elif key == "strict":
                    pass  # Special handling below
                else:
                    print(f"{prefix}Unrecognized option: {key} = {section[key]}", file=stderr)
                if invert:
                    dv = getattr(template, options_key, None)
                else:
                    continue
            ct = type(dv)
        v: Any = None
        try:
            if ct is bool:
                if isinstance(section, dict):
                    v = convert_to_boolean(section.get(key))
                else:
                    v = section.getboolean(key)  # type: ignore[attr-defined]  # Until better stub
                if invert:
                    v = not v
            elif callable(ct):
                if invert:
                    print(f"{prefix}Can not invert non-boolean key {options_key}", file=stderr)
                    continue
                try:
                    v = ct(section.get(key))
                except argparse.ArgumentTypeError as err:
                    print(f"{prefix}{key}: {err}", file=stderr)
                    continue
            else:
                print(f"{prefix}Don't know what type {key} should have", file=stderr)
                continue
        except ValueError as err:
            print(f"{prefix}{key}: {err}", file=stderr)
            continue
        if key == "strict":
            if v:
                set_strict_flags()
            continue
        results[options_key] = v

    # These two flags act as per-module overrides, so store the empty defaults.
    if "disable_error_code" not in results:
        results["disable_error_code"] = []
    if "enable_error_code" not in results:
        results["enable_error_code"] = []

    return results, report_dirs


</t>
<t tx="ekr.20230831011819.804">def convert_to_boolean(value: Any | None) -&gt; bool:
    """Return a boolean value translating from other types if necessary."""
    if isinstance(value, bool):
        return value
    if not isinstance(value, str):
        value = str(value)
    if value.lower() not in configparser.RawConfigParser.BOOLEAN_STATES:
        raise ValueError(f"Not a boolean: {value}")
    return configparser.RawConfigParser.BOOLEAN_STATES[value.lower()]


</t>
<t tx="ekr.20230831011819.805">def split_directive(s: str) -&gt; tuple[list[str], list[str]]:
    """Split s on commas, except during quoted sections.

    Returns the parts and a list of error messages."""
    parts = []
    cur: list[str] = []
    errors = []
    i = 0
    while i &lt; len(s):
        if s[i] == ",":
            parts.append("".join(cur).strip())
            cur = []
        elif s[i] == '"':
            i += 1
            while i &lt; len(s) and s[i] != '"':
                cur.append(s[i])
                i += 1
            if i == len(s):
                errors.append("Unterminated quote in configuration comment")
                cur.clear()
        else:
            cur.append(s[i])
        i += 1
    if cur:
        parts.append("".join(cur).strip())

    return parts, errors


</t>
<t tx="ekr.20230831011819.806">def mypy_comments_to_config_map(line: str, template: Options) -&gt; tuple[dict[str, str], list[str]]:
    """Rewrite the mypy comment syntax into ini file syntax."""
    options = {}
    entries, errors = split_directive(line)
    for entry in entries:
        if "=" not in entry:
            name = entry
            value = None
        else:
            name, value = (x.strip() for x in entry.split("=", 1))

        name = name.replace("-", "_")
        if value is None:
            value = "True"
        options[name] = value

    return options, errors


</t>
<t tx="ekr.20230831011819.807">def parse_mypy_comments(
    args: list[tuple[int, str]], template: Options
) -&gt; tuple[dict[str, object], list[tuple[int, str]]]:
    """Parse a collection of inline mypy: configuration comments.

    Returns a dictionary of options to be applied and a list of error messages
    generated.
    """

    errors: list[tuple[int, str]] = []
    sections = {}

    for lineno, line in args:
        # In order to easily match the behavior for bools, we abuse configparser.
        # Oddly, the only way to get the SectionProxy object with the getboolean
        # method is to create a config parser.
        parser = configparser.RawConfigParser()
        options, parse_errors = mypy_comments_to_config_map(line, template)
        parser["dummy"] = options
        errors.extend((lineno, x) for x in parse_errors)

        stderr = StringIO()
        strict_found = False

        def set_strict_flags() -&gt; None:
            nonlocal strict_found
            strict_found = True

        new_sections, reports = parse_section(
            "", template, set_strict_flags, parser["dummy"], ini_config_types, stderr=stderr
        )
        errors.extend((lineno, x) for x in stderr.getvalue().strip().split("\n") if x)
        if reports:
            errors.append((lineno, "Reports not supported in inline configuration"))
        if strict_found:
            errors.append(
                (
                    lineno,
                    'Setting "strict" not supported in inline configuration: specify it in '
                    "a configuration file instead, or set individual inline flags "
                    '(see "mypy -h" for the list of flags enabled in strict mode)',
                )
            )

        sections.update(new_sections)

    return sections, errors


</t>
<t tx="ekr.20230831011819.808">def get_config_module_names(filename: str | None, modules: list[str]) -&gt; str:
    if not filename or not modules:
        return ""

    if not is_toml(filename):
        return ", ".join(f"[mypy-{module}]" for module in modules)

    return "module = ['%s']" % ("', '".join(sorted(modules)))


</t>
<t tx="ekr.20230831011819.809">class ConfigTOMLValueError(ValueError):
    pass
</t>
<t tx="ekr.20230831011819.81">def all_imported_modules_in_file(self, file: MypyFile) -&gt; list[tuple[int, str, int]]:
    """Find all reachable import statements in a file.

    Return list of tuples (priority, module id, import line number)
    for all modules imported in file; lower numbers == higher priority.

    Can generate blocking errors on bogus relative imports.
    """

    def correct_rel_imp(imp: ImportFrom | ImportAll) -&gt; str:
        """Function to correct for relative imports."""
        file_id = file.fullname
        rel = imp.relative
        if rel == 0:
            return imp.id
        if os.path.basename(file.path).startswith("__init__."):
            rel -= 1
        if rel != 0:
            file_id = ".".join(file_id.split(".")[:-rel])
        new_id = file_id + "." + imp.id if imp.id else file_id

        if not new_id:
            self.errors.set_file(file.path, file.name, self.options)
            self.errors.report(
                imp.line, 0, "No parent module -- cannot perform relative import", blocker=True
            )

        return new_id

    res: list[tuple[int, str, int]] = []
    for imp in file.imports:
        if not imp.is_unreachable:
            if isinstance(imp, Import):
                pri = import_priority(imp, PRI_MED)
                ancestor_pri = import_priority(imp, PRI_LOW)
                for id, _ in imp.ids:
                    res.append((pri, id, imp.line))
                    ancestor_parts = id.split(".")[:-1]
                    ancestors = []
                    for part in ancestor_parts:
                        ancestors.append(part)
                        res.append((ancestor_pri, ".".join(ancestors), imp.line))
            elif isinstance(imp, ImportFrom):
                cur_id = correct_rel_imp(imp)
                all_are_submodules = True
                # Also add any imported names that are submodules.
                pri = import_priority(imp, PRI_MED)
                for name, __ in imp.names:
                    sub_id = cur_id + "." + name
                    if self.is_module(sub_id):
                        res.append((pri, sub_id, imp.line))
                    else:
                        all_are_submodules = False
                # Add cur_id as a dependency, even if all of the
                # imports are submodules. Processing import from will try
                # to look through cur_id, so we should depend on it.
                # As a workaround for for some bugs in cycle handling (#4498),
                # if all of the imports are submodules, do the import at a lower
                # priority.
                pri = import_priority(imp, PRI_HIGH if not all_are_submodules else PRI_LOW)
                res.append((pri, cur_id, imp.line))
            elif isinstance(imp, ImportAll):
                pri = import_priority(imp, PRI_HIGH)
                res.append((pri, correct_rel_imp(imp), imp.line))

    # Sort such that module (e.g. foo.bar.baz) comes before its ancestors (e.g. foo
    # and foo.bar) so that, if FindModuleCache finds the target module in a
    # package marked with py.typed underneath a namespace package installed in
    # site-packages, (gasp), that cache's knowledge of the ancestors
    # (aka FindModuleCache.ns_ancestors) can be primed when it is asked to find
    # the parent.
    res.sort(key=lambda x: -x[1].count("."))
    return res

</t>
<t tx="ekr.20230831011819.810">@path mypy
"""Constant folding of expressions.

For example, 3 + 5 can be constant folded into 8.
"""

&lt;&lt; constant_fold.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.812">from __future__ import annotations

from typing import Final, Union

from mypy.nodes import (
    ComplexExpr,
    Expression,
    FloatExpr,
    IntExpr,
    NameExpr,
    OpExpr,
    StrExpr,
    UnaryExpr,
    Var,
)

# All possible result types of constant folding
ConstantValue = Union[int, bool, float, complex, str]
CONST_TYPES: Final = (int, bool, float, complex, str)


</t>
<t tx="ekr.20230831011819.813">def constant_fold_expr(expr: Expression, cur_mod_id: str) -&gt; ConstantValue | None:
    """Return the constant value of an expression for supported operations.

    Among other things, support int arithmetic and string
    concatenation. For example, the expression 3 + 5 has the constant
    value 8.

    Also bind simple references to final constants defined in the
    current module (cur_mod_id). Binding to references is best effort
    -- we don't bind references to other modules. Mypyc trusts these
    to be correct in compiled modules, so that it can replace a
    constant expression (or a reference to one) with the statically
    computed value. We don't want to infer constant values based on
    stubs, in particular, as these might not match the implementation
    (due to version skew, for example).

    Return None if unsuccessful.
    """
    if isinstance(expr, IntExpr):
        return expr.value
    if isinstance(expr, StrExpr):
        return expr.value
    if isinstance(expr, FloatExpr):
        return expr.value
    if isinstance(expr, ComplexExpr):
        return expr.value
    elif isinstance(expr, NameExpr):
        if expr.name == "True":
            return True
        elif expr.name == "False":
            return False
        node = expr.node
        if (
            isinstance(node, Var)
            and node.is_final
            and node.fullname.rsplit(".", 1)[0] == cur_mod_id
        ):
            value = node.final_value
            if isinstance(value, (CONST_TYPES)):
                return value
    elif isinstance(expr, OpExpr):
        left = constant_fold_expr(expr.left, cur_mod_id)
        right = constant_fold_expr(expr.right, cur_mod_id)
        if left is not None and right is not None:
            return constant_fold_binary_op(expr.op, left, right)
    elif isinstance(expr, UnaryExpr):
        value = constant_fold_expr(expr.expr, cur_mod_id)
        if value is not None:
            return constant_fold_unary_op(expr.op, value)
    return None


</t>
<t tx="ekr.20230831011819.814">def constant_fold_binary_op(
    op: str, left: ConstantValue, right: ConstantValue
) -&gt; ConstantValue | None:
    if isinstance(left, int) and isinstance(right, int):
        return constant_fold_binary_int_op(op, left, right)

    # Float and mixed int/float arithmetic.
    if isinstance(left, float) and isinstance(right, float):
        return constant_fold_binary_float_op(op, left, right)
    elif isinstance(left, float) and isinstance(right, int):
        return constant_fold_binary_float_op(op, left, right)
    elif isinstance(left, int) and isinstance(right, float):
        return constant_fold_binary_float_op(op, left, right)

    # String concatenation and multiplication.
    if op == "+" and isinstance(left, str) and isinstance(right, str):
        return left + right
    elif op == "*" and isinstance(left, str) and isinstance(right, int):
        return left * right
    elif op == "*" and isinstance(left, int) and isinstance(right, str):
        return left * right

    # Complex construction.
    if op == "+" and isinstance(left, (int, float)) and isinstance(right, complex):
        return left + right
    elif op == "+" and isinstance(left, complex) and isinstance(right, (int, float)):
        return left + right
    elif op == "-" and isinstance(left, (int, float)) and isinstance(right, complex):
        return left - right
    elif op == "-" and isinstance(left, complex) and isinstance(right, (int, float)):
        return left - right

    return None


</t>
<t tx="ekr.20230831011819.815">def constant_fold_binary_int_op(op: str, left: int, right: int) -&gt; int | float | None:
    if op == "+":
        return left + right
    if op == "-":
        return left - right
    elif op == "*":
        return left * right
    elif op == "/":
        if right != 0:
            return left / right
    elif op == "//":
        if right != 0:
            return left // right
    elif op == "%":
        if right != 0:
            return left % right
    elif op == "&amp;":
        return left &amp; right
    elif op == "|":
        return left | right
    elif op == "^":
        return left ^ right
    elif op == "&lt;&lt;":
        if right &gt;= 0:
            return left &lt;&lt; right
    elif op == "&gt;&gt;":
        if right &gt;= 0:
            return left &gt;&gt; right
    elif op == "**":
        if right &gt;= 0:
            ret = left**right
            assert isinstance(ret, int)
            return ret
    return None


</t>
<t tx="ekr.20230831011819.816">def constant_fold_binary_float_op(op: str, left: int | float, right: int | float) -&gt; float | None:
    assert not (isinstance(left, int) and isinstance(right, int)), (op, left, right)
    if op == "+":
        return left + right
    elif op == "-":
        return left - right
    elif op == "*":
        return left * right
    elif op == "/":
        if right != 0:
            return left / right
    elif op == "//":
        if right != 0:
            return left // right
    elif op == "%":
        if right != 0:
            return left % right
    elif op == "**":
        if (left &lt; 0 and isinstance(right, int)) or left &gt; 0:
            try:
                ret = left**right
            except OverflowError:
                return None
            else:
                assert isinstance(ret, float), ret
                return ret

    return None


</t>
<t tx="ekr.20230831011819.817">def constant_fold_unary_op(op: str, value: ConstantValue) -&gt; int | float | None:
    if op == "-" and isinstance(value, (int, float)):
        return -value
    elif op == "~" and isinstance(value, int):
        return ~value
    elif op == "+" and isinstance(value, (int, float)):
        return value
    return None
</t>
<t tx="ekr.20230831011819.818">@path mypy
"""Type inference constraints."""
&lt;&lt; constraints.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.819">
from __future__ import annotations

from typing import TYPE_CHECKING, Final, Iterable, List, Sequence, cast

import mypy.subtypes
import mypy.typeops
from mypy.argmap import ArgTypeExpander
from mypy.erasetype import erase_typevars
from mypy.maptype import map_instance_to_supertype
from mypy.nodes import (
    ARG_OPT,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    CONTRAVARIANT,
    COVARIANT,
    ArgKind,
    TypeInfo,
)
from mypy.types import (
    TUPLE_LIKE_INSTANCE_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeQuery,
    TypeType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    find_unpack_in_list,
    get_proper_type,
    has_recursive_types,
    has_type_vars,
    is_named_instance,
    split_with_prefix_and_suffix,
)
from mypy.types_utils import is_union_with_any
from mypy.typestate import type_state
from mypy.typevartuples import extract_unpack, split_with_mapped_and_template

if TYPE_CHECKING:
    from mypy.infer import ArgumentInferContext

SUBTYPE_OF: Final = 0
SUPERTYPE_OF: Final = 1


</t>
<t tx="ekr.20230831011819.82">def is_module(self, id: str) -&gt; bool:
    """Is there a file in the file system corresponding to module id?"""
    return find_module_simple(id, self) is not None

</t>
<t tx="ekr.20230831011819.820">class Constraint:
    """A representation of a type constraint.

    It can be either T &lt;: type or T :&gt; type (T is a type variable).
    """

    @others
</t>
<t tx="ekr.20230831011819.821">type_var: TypeVarId
op = 0  # SUBTYPE_OF or SUPERTYPE_OF
target: Type

def __init__(self, type_var: TypeVarLikeType, op: int, target: Type) -&gt; None:
    self.type_var = type_var.id
    self.op = op
    # TODO: should we add "assert not isinstance(target, UnpackType)"?
    # UnpackType is a synthetic type, and is never valid as a constraint target.
    self.target = target
    self.origin_type_var = type_var
    # These are additional type variables that should be solved for together with type_var.
    # TODO: A cleaner solution may be to modify the return type of infer_constraints()
    # to include these instead, but this is a rather big refactoring.
    self.extra_tvars: list[TypeVarLikeType] = []

</t>
<t tx="ekr.20230831011819.822">def __repr__(self) -&gt; str:
    op_str = "&lt;:"
    if self.op == SUPERTYPE_OF:
        op_str = ":&gt;"
    return f"{self.type_var} {op_str} {self.target}"

</t>
<t tx="ekr.20230831011819.823">def __hash__(self) -&gt; int:
    return hash((self.type_var, self.op, self.target))

</t>
<t tx="ekr.20230831011819.824">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Constraint):
        return False
    return (self.type_var, self.op, self.target) == (other.type_var, other.op, other.target)


</t>
<t tx="ekr.20230831011819.825">def infer_constraints_for_callable(
    callee: CallableType,
    arg_types: Sequence[Type | None],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
    context: ArgumentInferContext,
) -&gt; list[Constraint]:
    """Infer type variable constraints for a callable and actual arguments.

    Return a list of constraints.
    """
    constraints: list[Constraint] = []
    mapper = ArgTypeExpander(context)

    for i, actuals in enumerate(formal_to_actual):
        if isinstance(callee.arg_types[i], UnpackType):
            unpack_type = callee.arg_types[i]
            assert isinstance(unpack_type, UnpackType)

            # In this case we are binding all of the actuals to *args
            # and we want a constraint that the typevar tuple being unpacked
            # is equal to a type list of all the actuals.
            actual_types = []
            for actual in actuals:
                actual_arg_type = arg_types[actual]
                if actual_arg_type is None:
                    continue

                actual_types.append(
                    mapper.expand_actual_type(
                        actual_arg_type,
                        arg_kinds[actual],
                        callee.arg_names[i],
                        callee.arg_kinds[i],
                    )
                )

            unpacked_type = get_proper_type(unpack_type.type)
            if isinstance(unpacked_type, TypeVarTupleType):
                constraints.append(
                    Constraint(
                        unpacked_type,
                        SUPERTYPE_OF,
                        TupleType(actual_types, unpacked_type.tuple_fallback),
                    )
                )
            elif isinstance(unpacked_type, TupleType):
                # Prefixes get converted to positional args, so technically the only case we
                # should have here is like Tuple[Unpack[Ts], Y1, Y2, Y3]. If this turns out
                # not to hold we can always handle the prefixes too.
                inner_unpack = unpacked_type.items[0]
                assert isinstance(inner_unpack, UnpackType)
                inner_unpacked_type = get_proper_type(inner_unpack.type)
                suffix_len = len(unpacked_type.items) - 1
                if isinstance(inner_unpacked_type, TypeVarTupleType):
                    # Variadic item can be either *Ts...
                    constraints.append(
                        Constraint(
                            inner_unpacked_type,
                            SUPERTYPE_OF,
                            TupleType(
                                actual_types[:-suffix_len], inner_unpacked_type.tuple_fallback
                            ),
                        )
                    )
                else:
                    # ...or it can be a homogeneous tuple.
                    assert (
                        isinstance(inner_unpacked_type, Instance)
                        and inner_unpacked_type.type.fullname == "builtins.tuple"
                    )
                    for at in actual_types[:-suffix_len]:
                        constraints.extend(
                            infer_constraints(inner_unpacked_type.args[0], at, SUPERTYPE_OF)
                        )
                # Now handle the suffix (if any).
                if suffix_len:
                    for tt, at in zip(unpacked_type.items[1:], actual_types[-suffix_len:]):
                        constraints.extend(infer_constraints(tt, at, SUPERTYPE_OF))
            else:
                assert False, "mypy bug: unhandled constraint inference case"
        else:
            for actual in actuals:
                actual_arg_type = arg_types[actual]
                if actual_arg_type is None:
                    continue

                actual_type = mapper.expand_actual_type(
                    actual_arg_type, arg_kinds[actual], callee.arg_names[i], callee.arg_kinds[i]
                )
                # TODO: if callee has ParamSpec, we need to collect all actuals that map to star
                # args and create single constraint between P and resulting Parameters instead.
                c = infer_constraints(callee.arg_types[i], actual_type, SUPERTYPE_OF)
                constraints.extend(c)

    return constraints


</t>
<t tx="ekr.20230831011819.826">def infer_constraints(
    template: Type, actual: Type, direction: int, skip_neg_op: bool = False
) -&gt; list[Constraint]:
    """Infer type constraints.

    Match a template type, which may contain type variable references,
    recursively against a type which does not contain (the same) type
    variable references. The result is a list of type constrains of
    form 'T is a supertype/subtype of x', where T is a type variable
    present in the template and x is a type without reference to type
    variables present in the template.

    Assume T and S are type variables. Now the following results can be
    calculated (read as '(template, actual) --&gt; result'):

      (T, X)            --&gt;  T :&gt; X
      (X[T], X[Y])      --&gt;  T &lt;: Y and T :&gt; Y
      ((T, T), (X, Y))  --&gt;  T :&gt; X and T :&gt; Y
      ((T, S), (X, Y))  --&gt;  T :&gt; X and S :&gt; Y
      (X[T], Any)       --&gt;  T &lt;: Any and T :&gt; Any

    The constraints are represented as Constraint objects. If skip_neg_op == True,
    then skip adding reverse (polymorphic) constraints (since this is already a call
    to infer such constraints).
    """
    if any(
        get_proper_type(template) == get_proper_type(t)
        and get_proper_type(actual) == get_proper_type(a)
        for (t, a) in reversed(type_state.inferring)
    ):
        return []
    if has_recursive_types(template) or isinstance(get_proper_type(template), Instance):
        # This case requires special care because it may cause infinite recursion.
        # Note that we include Instances because the may be recursive as str(Sequence[str]).
        if not has_type_vars(template):
            # Return early on an empty branch.
            return []
        type_state.inferring.append((template, actual))
        res = _infer_constraints(template, actual, direction, skip_neg_op)
        type_state.inferring.pop()
        return res
    return _infer_constraints(template, actual, direction, skip_neg_op)


</t>
<t tx="ekr.20230831011819.827">def _infer_constraints(
    template: Type, actual: Type, direction: int, skip_neg_op: bool
) -&gt; list[Constraint]:
    orig_template = template
    template = get_proper_type(template)
    actual = get_proper_type(actual)

    # Type inference shouldn't be affected by whether union types have been simplified.
    # We however keep any ErasedType items, so that the caller will see it when using
    # checkexpr.has_erased_component().
    if isinstance(template, UnionType):
        template = mypy.typeops.make_simplified_union(template.items, keep_erased=True)
    if isinstance(actual, UnionType):
        actual = mypy.typeops.make_simplified_union(actual.items, keep_erased=True)

    # Ignore Any types from the type suggestion engine to avoid them
    # causing us to infer Any in situations where a better job could
    # be done otherwise. (This can produce false positives but that
    # doesn't really matter because it is all heuristic anyway.)
    if isinstance(actual, AnyType) and actual.type_of_any == TypeOfAny.suggestion_engine:
        return []

    # If the template is simply a type variable, emit a Constraint directly.
    # We need to handle this case before handling Unions for two reasons:
    #  1. "T &lt;: Union[U1, U2]" is not equivalent to "T &lt;: U1 or T &lt;: U2",
    #     because T can itself be a union (notably, Union[U1, U2] itself).
    #  2. "T :&gt; Union[U1, U2]" is logically equivalent to "T :&gt; U1 and
    #     T :&gt; U2", but they are not equivalent to the constraint solver,
    #     which never introduces new Union types (it uses join() instead).
    if isinstance(template, TypeVarType):
        return [Constraint(template, direction, actual)]

    # Now handle the case of either template or actual being a Union.
    # For a Union to be a subtype of another type, every item of the Union
    # must be a subtype of that type, so concatenate the constraints.
    if direction == SUBTYPE_OF and isinstance(template, UnionType):
        res = []
        for t_item in template.items:
            res.extend(infer_constraints(t_item, actual, direction))
        return res
    if direction == SUPERTYPE_OF and isinstance(actual, UnionType):
        res = []
        for a_item in actual.items:
            res.extend(infer_constraints(orig_template, a_item, direction))
        return res

    # Now the potential subtype is known not to be a Union or a type
    # variable that we are solving for. In that case, for a Union to
    # be a supertype of the potential subtype, some item of the Union
    # must be a supertype of it.
    if direction == SUBTYPE_OF and isinstance(actual, UnionType):
        # If some of items is not a complete type, disregard that.
        items = simplify_away_incomplete_types(actual.items)
        # We infer constraints eagerly -- try to find constraints for a type
        # variable if possible. This seems to help with some real-world
        # use cases.
        return any_constraints(
            [infer_constraints_if_possible(template, a_item, direction) for a_item in items],
            eager=True,
        )
    if direction == SUPERTYPE_OF and isinstance(template, UnionType):
        # When the template is a union, we are okay with leaving some
        # type variables indeterminate. This helps with some special
        # cases, though this isn't very principled.
        result = any_constraints(
            [
                infer_constraints_if_possible(t_item, actual, direction)
                for t_item in template.items
            ],
            eager=False,
        )
        if result:
            return result
        elif has_recursive_types(template) and not has_recursive_types(actual):
            return handle_recursive_union(template, actual, direction)
        return []

    # Remaining cases are handled by ConstraintBuilderVisitor.
    return template.accept(ConstraintBuilderVisitor(actual, direction, skip_neg_op))


</t>
<t tx="ekr.20230831011819.828">def infer_constraints_if_possible(
    template: Type, actual: Type, direction: int
) -&gt; list[Constraint] | None:
    """Like infer_constraints, but return None if the input relation is
    known to be unsatisfiable, for example if template=List[T] and actual=int.
    (In this case infer_constraints would return [], just like it would for
    an automatically satisfied relation like template=List[T] and actual=object.)
    """
    if direction == SUBTYPE_OF and not mypy.subtypes.is_subtype(erase_typevars(template), actual):
        return None
    if direction == SUPERTYPE_OF and not mypy.subtypes.is_subtype(
        actual, erase_typevars(template)
    ):
        return None
    if (
        direction == SUPERTYPE_OF
        and isinstance(template, TypeVarType)
        and not mypy.subtypes.is_subtype(actual, erase_typevars(template.upper_bound))
    ):
        # This is not caught by the above branch because of the erase_typevars() call,
        # that would return 'Any' for a type variable.
        return None
    return infer_constraints(template, actual, direction)


</t>
<t tx="ekr.20230831011819.829">def select_trivial(options: Sequence[list[Constraint] | None]) -&gt; list[list[Constraint]]:
    """Select only those lists where each item is a constraint against Any."""
    res = []
    for option in options:
        if option is None:
            continue
        if all(isinstance(get_proper_type(c.target), AnyType) for c in option):
            res.append(option)
    return res


</t>
<t tx="ekr.20230831011819.83">def parse_file(
    self, id: str, path: str, source: str, ignore_errors: bool, options: Options
) -&gt; MypyFile:
    """Parse the source of a file with the given name.

    Raise CompileError if there is a parse error.
    """
    t0 = time.time()
    if ignore_errors:
        self.errors.ignored_files.add(path)
    tree = parse(source, path, id, self.errors, options=options)
    tree._fullname = id
    self.add_stats(
        files_parsed=1,
        modules_parsed=int(not tree.is_stub),
        stubs_parsed=int(tree.is_stub),
        parse_time=time.time() - t0,
    )

    if self.errors.is_blockers():
        self.log("Bailing due to parse errors")
        self.errors.raise_error()

    self.errors.set_file_ignored_lines(path, tree.ignored_lines, ignore_errors)
    return tree

</t>
<t tx="ekr.20230831011819.830">def merge_with_any(constraint: Constraint) -&gt; Constraint:
    """Transform a constraint target into a union with given Any type."""
    target = constraint.target
    if is_union_with_any(target):
        # Do not produce redundant unions.
        return constraint
    # TODO: if we will support multiple sources Any, use this here instead.
    any_type = AnyType(TypeOfAny.implementation_artifact)
    return Constraint(
        constraint.origin_type_var,
        constraint.op,
        UnionType.make_union([target, any_type], target.line, target.column),
    )


</t>
<t tx="ekr.20230831011819.831">def handle_recursive_union(template: UnionType, actual: Type, direction: int) -&gt; list[Constraint]:
    # This is a hack to special-case things like Union[T, Inst[T]] in recursive types. Although
    # it is quite arbitrary, it is a relatively common pattern, so we should handle it well.
    # This function may be called when inferring against such union resulted in different
    # constraints for each item. Normally we give up in such case, but here we instead split
    # the union in two parts, and try inferring sequentially.
    non_type_var_items = [t for t in template.items if not isinstance(t, TypeVarType)]
    type_var_items = [t for t in template.items if isinstance(t, TypeVarType)]
    return infer_constraints(
        UnionType.make_union(non_type_var_items), actual, direction
    ) or infer_constraints(UnionType.make_union(type_var_items), actual, direction)


</t>
<t tx="ekr.20230831011819.832">def any_constraints(options: list[list[Constraint] | None], eager: bool) -&gt; list[Constraint]:
    """Deduce what we can from a collection of constraint lists.

    It's a given that at least one of the lists must be satisfied. A
    None element in the list of options represents an unsatisfiable
    constraint and is ignored.  Ignore empty constraint lists if eager
    is true -- they are always trivially satisfiable.
    """
    if eager:
        valid_options = [option for option in options if option]
    else:
        valid_options = [option for option in options if option is not None]

    if not valid_options:
        return []

    if len(valid_options) == 1:
        return valid_options[0]

    if all(is_same_constraints(valid_options[0], c) for c in valid_options[1:]):
        # Multiple sets of constraints that are all the same. Just pick any one of them.
        return valid_options[0]

    if all(is_similar_constraints(valid_options[0], c) for c in valid_options[1:]):
        # All options have same structure. In this case we can merge-in trivial
        # options (i.e. those that only have Any) and try again.
        # TODO: More generally, if a given (variable, direction) pair appears in
        # every option, combine the bounds with meet/join always, not just for Any.
        trivial_options = select_trivial(valid_options)
        if trivial_options and len(trivial_options) &lt; len(valid_options):
            merged_options = []
            for option in valid_options:
                if option in trivial_options:
                    continue
                if option is not None:
                    merged_option: list[Constraint] | None = [merge_with_any(c) for c in option]
                else:
                    merged_option = None
                merged_options.append(merged_option)
            return any_constraints(list(merged_options), eager)

    # If normal logic didn't work, try excluding trivially unsatisfiable constraint (due to
    # upper bounds) from each option, and comparing them again.
    filtered_options = [filter_satisfiable(o) for o in options]
    if filtered_options != options:
        return any_constraints(filtered_options, eager=eager)

    # Otherwise, there are either no valid options or multiple, inconsistent valid
    # options. Give up and deduce nothing.
    return []


</t>
<t tx="ekr.20230831011819.833">def filter_satisfiable(option: list[Constraint] | None) -&gt; list[Constraint] | None:
    """Keep only constraints that can possibly be satisfied.

    Currently, we filter out constraints where target is not a subtype of the upper bound.
    Since those can be never satisfied. We may add more cases in future if it improves type
    inference.
    """
    if not option:
        return option
    satisfiable = []
    for c in option:
        if isinstance(c.origin_type_var, TypeVarType) and c.origin_type_var.values:
            if any(
                mypy.subtypes.is_subtype(c.target, value) for value in c.origin_type_var.values
            ):
                satisfiable.append(c)
        elif mypy.subtypes.is_subtype(c.target, c.origin_type_var.upper_bound):
            satisfiable.append(c)
    if not satisfiable:
        return None
    return satisfiable


</t>
<t tx="ekr.20230831011819.834">def is_same_constraints(x: list[Constraint], y: list[Constraint]) -&gt; bool:
    for c1 in x:
        if not any(is_same_constraint(c1, c2) for c2 in y):
            return False
    for c1 in y:
        if not any(is_same_constraint(c1, c2) for c2 in x):
            return False
    return True


</t>
<t tx="ekr.20230831011819.835">def is_same_constraint(c1: Constraint, c2: Constraint) -&gt; bool:
    # Ignore direction when comparing constraints against Any.
    skip_op_check = isinstance(get_proper_type(c1.target), AnyType) and isinstance(
        get_proper_type(c2.target), AnyType
    )
    return (
        c1.type_var == c2.type_var
        and (c1.op == c2.op or skip_op_check)
        and mypy.subtypes.is_same_type(c1.target, c2.target)
    )


</t>
<t tx="ekr.20230831011819.836">def is_similar_constraints(x: list[Constraint], y: list[Constraint]) -&gt; bool:
    """Check that two lists of constraints have similar structure.

    This means that each list has same type variable plus direction pairs (i.e we
    ignore the target). Except for constraints where target is Any type, there
    we ignore direction as well.
    """
    return _is_similar_constraints(x, y) and _is_similar_constraints(y, x)


</t>
<t tx="ekr.20230831011819.837">def _is_similar_constraints(x: list[Constraint], y: list[Constraint]) -&gt; bool:
    """Check that every constraint in the first list has a similar one in the second.

    See docstring above for definition of similarity.
    """
    for c1 in x:
        has_similar = False
        for c2 in y:
            # Ignore direction when either constraint is against Any.
            skip_op_check = isinstance(get_proper_type(c1.target), AnyType) or isinstance(
                get_proper_type(c2.target), AnyType
            )
            if c1.type_var == c2.type_var and (c1.op == c2.op or skip_op_check):
                has_similar = True
                break
        if not has_similar:
            return False
    return True


</t>
<t tx="ekr.20230831011819.838">def simplify_away_incomplete_types(types: Iterable[Type]) -&gt; list[Type]:
    complete = [typ for typ in types if is_complete_type(typ)]
    if complete:
        return complete
    else:
        return list(types)


</t>
<t tx="ekr.20230831011819.839">def is_complete_type(typ: Type) -&gt; bool:
    """Is a type complete?

    A complete doesn't have uninhabited type components or (when not in strict
    optional mode) None components.
    """
    return typ.accept(CompleteTypeVisitor())


</t>
<t tx="ekr.20230831011819.84">def load_fine_grained_deps(self, id: str) -&gt; dict[str, set[str]]:
    t0 = time.time()
    if id in self.fg_deps_meta:
        # TODO: Assert deps file wasn't changed.
        deps = json.loads(self.metastore.read(self.fg_deps_meta[id]["path"]))
    else:
        deps = {}
    val = {k: set(v) for k, v in deps.items()}
    self.add_stats(load_fg_deps_time=time.time() - t0)
    return val

</t>
<t tx="ekr.20230831011819.840">class CompleteTypeVisitor(TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20230831011819.841">def __init__(self) -&gt; None:
    super().__init__(all)

</t>
<t tx="ekr.20230831011819.842">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return False


</t>
<t tx="ekr.20230831011819.843">class ConstraintBuilderVisitor(TypeVisitor[List[Constraint]]):
    """Visitor class for inferring type constraints."""

    @others
</t>
<t tx="ekr.20230831011819.844"># The type that is compared against a template
# TODO: The value may be None. Is that actually correct?
actual: ProperType

def __init__(self, actual: ProperType, direction: int, skip_neg_op: bool) -&gt; None:
    # Direction must be SUBTYPE_OF or SUPERTYPE_OF.
    self.actual = actual
    self.direction = direction
    # Whether to skip polymorphic inference (involves inference in opposite direction)
    # this is used to prevent infinite recursion when both template and actual are
    # generic callables.
    self.skip_neg_op = skip_neg_op

</t>
<t tx="ekr.20230831011819.845"># Trivial leaf types

def visit_unbound_type(self, template: UnboundType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20230831011819.846">def visit_any(self, template: AnyType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20230831011819.847">def visit_none_type(self, template: NoneType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20230831011819.848">def visit_uninhabited_type(self, template: UninhabitedType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20230831011819.849">def visit_erased_type(self, template: ErasedType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20230831011819.85">def report_file(
    self, file: MypyFile, type_map: dict[Expression, Type], options: Options
) -&gt; None:
    if self.reports is not None and self.source_set.is_source(file):
        self.reports.file(file, self.modules, type_map, options)

</t>
<t tx="ekr.20230831011819.850">def visit_deleted_type(self, template: DeletedType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20230831011819.851">def visit_literal_type(self, template: LiteralType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20230831011819.852"># Errors

def visit_partial_type(self, template: PartialType) -&gt; list[Constraint]:
    # We can't do anything useful with a partial type here.
    assert False, "Internal error"

</t>
<t tx="ekr.20230831011819.853"># Non-trivial leaf type

def visit_type_var(self, template: TypeVarType) -&gt; list[Constraint]:
    assert False, (
        "Unexpected TypeVarType in ConstraintBuilderVisitor"
        " (should have been handled in infer_constraints)"
    )

</t>
<t tx="ekr.20230831011819.854">def visit_param_spec(self, template: ParamSpecType) -&gt; list[Constraint]:
    # Can't infer ParamSpecs from component values (only via Callable[P, T]).
    return []

</t>
<t tx="ekr.20230831011819.855">def visit_type_var_tuple(self, template: TypeVarTupleType) -&gt; list[Constraint]:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011819.856">def visit_unpack_type(self, template: UnpackType) -&gt; list[Constraint]:
    raise RuntimeError("Mypy bug: unpack should be handled at a higher level.")

</t>
<t tx="ekr.20230831011819.857">def visit_parameters(self, template: Parameters) -&gt; list[Constraint]:
    # Constraining Any against C[P] turns into infer_against_any([P], Any)
    # ... which seems like the only case this can happen. Better to fail loudly otherwise.
    if isinstance(self.actual, AnyType):
        return self.infer_against_any(template.arg_types, self.actual)
    if type_state.infer_polymorphic and isinstance(self.actual, Parameters):
        # For polymorphic inference we need to be able to infer secondary constraints
        # in situations like [x: T] &lt;: P &lt;: [x: int]. Note we invert direction, since
        # this function expects direction between callables.
        return infer_callable_arguments_constraints(
            template, self.actual, neg_op(self.direction)
        )
    raise RuntimeError("Parameters cannot be constrained to")

</t>
<t tx="ekr.20230831011819.858"># Non-leaf types

def visit_instance(self, template: Instance) -&gt; list[Constraint]:
    original_actual = actual = self.actual
    res: list[Constraint] = []
    if isinstance(actual, (CallableType, Overloaded)) and template.type.is_protocol:
        if "__call__" in template.type.protocol_members:
            # Special case: a generic callback protocol
            if not any(template == t for t in template.type.inferring):
                template.type.inferring.append(template)
                call = mypy.subtypes.find_member(
                    "__call__", template, actual, is_operator=True
                )
                assert call is not None
                if mypy.subtypes.is_subtype(actual, erase_typevars(call)):
                    subres = infer_constraints(call, actual, self.direction)
                    res.extend(subres)
                template.type.inferring.pop()
    if isinstance(actual, CallableType) and actual.fallback is not None:
        if actual.is_type_obj() and template.type.is_protocol:
            ret_type = get_proper_type(actual.ret_type)
            if isinstance(ret_type, TupleType):
                ret_type = mypy.typeops.tuple_fallback(ret_type)
            if isinstance(ret_type, Instance):
                if self.direction == SUBTYPE_OF:
                    subtype = template
                else:
                    subtype = ret_type
                res.extend(
                    self.infer_constraints_from_protocol_members(
                        ret_type, template, subtype, template, class_obj=True
                    )
                )
        actual = actual.fallback
    if isinstance(actual, TypeType) and template.type.is_protocol:
        if isinstance(actual.item, Instance):
            if self.direction == SUBTYPE_OF:
                subtype = template
            else:
                subtype = actual.item
            res.extend(
                self.infer_constraints_from_protocol_members(
                    actual.item, template, subtype, template, class_obj=True
                )
            )
        if self.direction == SUPERTYPE_OF:
            # Infer constraints for Type[T] via metaclass of T when it makes sense.
            a_item = actual.item
            if isinstance(a_item, TypeVarType):
                a_item = get_proper_type(a_item.upper_bound)
            if isinstance(a_item, Instance) and a_item.type.metaclass_type:
                res.extend(
                    self.infer_constraints_from_protocol_members(
                        a_item.type.metaclass_type, template, actual, template
                    )
                )

    if isinstance(actual, Overloaded) and actual.fallback is not None:
        actual = actual.fallback
    if isinstance(actual, TypedDictType):
        actual = actual.as_anonymous().fallback
    if isinstance(actual, LiteralType):
        actual = actual.fallback
    if isinstance(actual, Instance):
        instance = actual
        erased = erase_typevars(template)
        assert isinstance(erased, Instance)  # type: ignore[misc]
        # We always try nominal inference if possible,
        # it is much faster than the structural one.
        if self.direction == SUBTYPE_OF and template.type.has_base(instance.type.fullname):
            mapped = map_instance_to_supertype(template, instance.type)
            tvars = mapped.type.defn.type_vars

            if instance.type.has_type_var_tuple_type:
                assert instance.type.type_var_tuple_prefix is not None
                assert instance.type.type_var_tuple_suffix is not None
                assert mapped.type.type_var_tuple_prefix is not None
                assert mapped.type.type_var_tuple_suffix is not None

                unpack_constraints, instance_args, mapped_args = build_constraints_for_unpack(
                    instance.args,
                    instance.type.type_var_tuple_prefix,
                    instance.type.type_var_tuple_suffix,
                    mapped.args,
                    mapped.type.type_var_tuple_prefix,
                    mapped.type.type_var_tuple_suffix,
                    self.direction,
                )
                res.extend(unpack_constraints)

                tvars_prefix, _, tvars_suffix = split_with_prefix_and_suffix(
                    tuple(tvars),
                    instance.type.type_var_tuple_prefix,
                    instance.type.type_var_tuple_suffix,
                )
                tvars = cast("list[TypeVarLikeType]", list(tvars_prefix + tvars_suffix))
            else:
                mapped_args = mapped.args
                instance_args = instance.args

            # N.B: We use zip instead of indexing because the lengths might have
            # mismatches during daemon reprocessing.
            for tvar, mapped_arg, instance_arg in zip(tvars, mapped_args, instance_args):
                if isinstance(tvar, TypeVarType):
                    # The constraints for generic type parameters depend on variance.
                    # Include constraints from both directions if invariant.
                    if tvar.variance != CONTRAVARIANT:
                        res.extend(infer_constraints(mapped_arg, instance_arg, self.direction))
                    if tvar.variance != COVARIANT:
                        res.extend(
                            infer_constraints(mapped_arg, instance_arg, neg_op(self.direction))
                        )
                elif isinstance(tvar, ParamSpecType) and isinstance(mapped_arg, ParamSpecType):
                    prefix = mapped_arg.prefix
                    if isinstance(instance_arg, Parameters):
                        # No such thing as variance for ParamSpecs, consider them invariant
                        # TODO: constraints between prefixes using
                        # infer_callable_arguments_constraints()
                        suffix: Type = instance_arg.copy_modified(
                            instance_arg.arg_types[len(prefix.arg_types) :],
                            instance_arg.arg_kinds[len(prefix.arg_kinds) :],
                            instance_arg.arg_names[len(prefix.arg_names) :],
                        )
                        res.append(Constraint(mapped_arg, SUBTYPE_OF, suffix))
                        res.append(Constraint(mapped_arg, SUPERTYPE_OF, suffix))
                    elif isinstance(instance_arg, ParamSpecType):
                        suffix = instance_arg.copy_modified(
                            prefix=Parameters(
                                instance_arg.prefix.arg_types[len(prefix.arg_types) :],
                                instance_arg.prefix.arg_kinds[len(prefix.arg_kinds) :],
                                instance_arg.prefix.arg_names[len(prefix.arg_names) :],
                            )
                        )
                        res.append(Constraint(mapped_arg, SUBTYPE_OF, suffix))
                        res.append(Constraint(mapped_arg, SUPERTYPE_OF, suffix))
                else:
                    # This case should have been handled above.
                    assert not isinstance(tvar, TypeVarTupleType)

            return res
        elif self.direction == SUPERTYPE_OF and instance.type.has_base(template.type.fullname):
            mapped = map_instance_to_supertype(instance, template.type)
            tvars = template.type.defn.type_vars
            if template.type.has_type_var_tuple_type:
                assert mapped.type.type_var_tuple_prefix is not None
                assert mapped.type.type_var_tuple_suffix is not None
                assert template.type.type_var_tuple_prefix is not None
                assert template.type.type_var_tuple_suffix is not None

                unpack_constraints, mapped_args, template_args = build_constraints_for_unpack(
                    mapped.args,
                    mapped.type.type_var_tuple_prefix,
                    mapped.type.type_var_tuple_suffix,
                    template.args,
                    template.type.type_var_tuple_prefix,
                    template.type.type_var_tuple_suffix,
                    self.direction,
                )
                res.extend(unpack_constraints)

                tvars_prefix, _, tvars_suffix = split_with_prefix_and_suffix(
                    tuple(tvars),
                    template.type.type_var_tuple_prefix,
                    template.type.type_var_tuple_suffix,
                )
                tvars = cast("list[TypeVarLikeType]", list(tvars_prefix + tvars_suffix))
            else:
                mapped_args = mapped.args
                template_args = template.args
            # N.B: We use zip instead of indexing because the lengths might have
            # mismatches during daemon reprocessing.
            for tvar, mapped_arg, template_arg in zip(tvars, mapped_args, template_args):
                assert not isinstance(tvar, TypeVarTupleType)
                if isinstance(tvar, TypeVarType):
                    # The constraints for generic type parameters depend on variance.
                    # Include constraints from both directions if invariant.
                    if tvar.variance != CONTRAVARIANT:
                        res.extend(infer_constraints(template_arg, mapped_arg, self.direction))
                    if tvar.variance != COVARIANT:
                        res.extend(
                            infer_constraints(template_arg, mapped_arg, neg_op(self.direction))
                        )
                elif isinstance(tvar, ParamSpecType) and isinstance(
                    template_arg, ParamSpecType
                ):
                    prefix = template_arg.prefix
                    if isinstance(mapped_arg, Parameters):
                        # No such thing as variance for ParamSpecs, consider them invariant
                        # TODO: constraints between prefixes using
                        # infer_callable_arguments_constraints()
                        suffix = mapped_arg.copy_modified(
                            mapped_arg.arg_types[len(prefix.arg_types) :],
                            mapped_arg.arg_kinds[len(prefix.arg_kinds) :],
                            mapped_arg.arg_names[len(prefix.arg_names) :],
                        )
                        res.append(Constraint(template_arg, SUBTYPE_OF, suffix))
                        res.append(Constraint(template_arg, SUPERTYPE_OF, suffix))
                    elif isinstance(mapped_arg, ParamSpecType):
                        suffix = mapped_arg.copy_modified(
                            prefix=Parameters(
                                mapped_arg.prefix.arg_types[len(prefix.arg_types) :],
                                mapped_arg.prefix.arg_kinds[len(prefix.arg_kinds) :],
                                mapped_arg.prefix.arg_names[len(prefix.arg_names) :],
                            )
                        )
                        res.append(Constraint(template_arg, SUBTYPE_OF, suffix))
                        res.append(Constraint(template_arg, SUPERTYPE_OF, suffix))
                else:
                    # This case should have been handled above.
                    assert not isinstance(tvar, TypeVarTupleType)
            return res
        if (
            template.type.is_protocol
            and self.direction == SUPERTYPE_OF
            and
            # We avoid infinite recursion for structural subtypes by checking
            # whether this type already appeared in the inference chain.
            # This is a conservative way to break the inference cycles.
            # It never produces any "false" constraints but gives up soon
            # on purely structural inference cycles, see #3829.
            # Note that we use is_protocol_implementation instead of is_subtype
            # because some type may be considered a subtype of a protocol
            # due to _promote, but still not implement the protocol.
            not any(template == t for t in reversed(template.type.inferring))
            and mypy.subtypes.is_protocol_implementation(instance, erased, skip=["__call__"])
        ):
            template.type.inferring.append(template)
            res.extend(
                self.infer_constraints_from_protocol_members(
                    instance, template, original_actual, template
                )
            )
            template.type.inferring.pop()
            return res
        elif (
            instance.type.is_protocol
            and self.direction == SUBTYPE_OF
            and
            # We avoid infinite recursion for structural subtypes also here.
            not any(instance == i for i in reversed(instance.type.inferring))
            and mypy.subtypes.is_protocol_implementation(erased, instance, skip=["__call__"])
        ):
            instance.type.inferring.append(instance)
            res.extend(
                self.infer_constraints_from_protocol_members(
                    instance, template, template, instance
                )
            )
            instance.type.inferring.pop()
            return res
    if res:
        return res

    if isinstance(actual, AnyType):
        return self.infer_against_any(template.args, actual)
    if (
        isinstance(actual, TupleType)
        and is_named_instance(template, TUPLE_LIKE_INSTANCE_NAMES)
        and self.direction == SUPERTYPE_OF
    ):
        for item in actual.items:
            if isinstance(item, UnpackType):
                unpacked = get_proper_type(item.type)
                if isinstance(unpacked, TypeVarType):
                    # Cannot infer anything for T from [T, ...] &lt;: *Ts
                    continue
                assert (
                    isinstance(unpacked, Instance)
                    and unpacked.type.fullname == "builtins.tuple"
                )
                item = unpacked.args[0]
            cb = infer_constraints(template.args[0], item, SUPERTYPE_OF)
            res.extend(cb)
        return res
    elif isinstance(actual, TupleType) and self.direction == SUPERTYPE_OF:
        return infer_constraints(template, mypy.typeops.tuple_fallback(actual), self.direction)
    elif isinstance(actual, TypeVarType):
        if not actual.values and not actual.id.is_meta_var():
            return infer_constraints(template, actual.upper_bound, self.direction)
        return []
    elif isinstance(actual, ParamSpecType):
        return infer_constraints(template, actual.upper_bound, self.direction)
    elif isinstance(actual, TypeVarTupleType):
        raise NotImplementedError
    else:
        return []

</t>
<t tx="ekr.20230831011819.859">def infer_constraints_from_protocol_members(
    self,
    instance: Instance,
    template: Instance,
    subtype: Type,
    protocol: Instance,
    class_obj: bool = False,
) -&gt; list[Constraint]:
    """Infer constraints for situations where either 'template' or 'instance' is a protocol.

    The 'protocol' is the one of two that is an instance of protocol type, 'subtype'
    is the type used to bind self during inference. Currently, we just infer constrains for
    every protocol member type (both ways for settable members).
    """
    res = []
    for member in protocol.type.protocol_members:
        inst = mypy.subtypes.find_member(member, instance, subtype, class_obj=class_obj)
        temp = mypy.subtypes.find_member(member, template, subtype)
        if inst is None or temp is None:
            if member == "__call__":
                continue
            return []  # See #11020
        # The above is safe since at this point we know that 'instance' is a subtype
        # of (erased) 'template', therefore it defines all protocol members
        res.extend(infer_constraints(temp, inst, self.direction))
        if mypy.subtypes.IS_SETTABLE in mypy.subtypes.get_member_flags(member, protocol):
            # Settable members are invariant, add opposite constraints
            res.extend(infer_constraints(temp, inst, neg_op(self.direction)))
    return res

</t>
<t tx="ekr.20230831011819.86">def verbosity(self) -&gt; int:
    return self.options.verbosity

</t>
<t tx="ekr.20230831011819.860">def visit_callable_type(self, template: CallableType) -&gt; list[Constraint]:
    # Normalize callables before matching against each other.
    # Note that non-normalized callables can be created in annotations
    # using e.g. callback protocols.
    # TODO: check that callables match? Ideally we should not infer constraints
    # callables that can never be subtypes of one another in given direction.
    template = template.with_unpacked_kwargs()
    extra_tvars = False
    if isinstance(self.actual, CallableType):
        res: list[Constraint] = []
        cactual = self.actual.with_unpacked_kwargs()
        param_spec = template.param_spec()
        if param_spec is None:
            # TODO: Erase template variables if it is generic?
            if (
                type_state.infer_polymorphic
                and cactual.variables
                and not self.skip_neg_op
                # Technically, the correct inferred type for application of e.g.
                # Callable[..., T] -&gt; Callable[..., T] (with literal ellipsis), to a generic
                # like U -&gt; U, should be Callable[..., Any], but if U is a self-type, we can
                # allow it to leak, to be later bound to self. A bunch of existing code
                # depends on this old behaviour.
                and not any(tv.id.raw_id == 0 for tv in cactual.variables)
            ):
                # If the actual callable is generic, infer constraints in the opposite
                # direction, and indicate to the solver there are extra type variables
                # to solve for (see more details in mypy/solve.py).
                res.extend(
                    infer_constraints(
                        cactual, template, neg_op(self.direction), skip_neg_op=True
                    )
                )
                extra_tvars = True

            # We can't infer constraints from arguments if the template is Callable[..., T]
            # (with literal '...').
            if not template.is_ellipsis_args:
                unpack_present = find_unpack_in_list(template.arg_types)
                if unpack_present is not None:
                    # We need to re-normalize args to the form they appear in tuples,
                    # for callables we always pack the suffix inside another tuple.
                    unpack = template.arg_types[unpack_present]
                    assert isinstance(unpack, UnpackType)
                    tuple_type = get_tuple_fallback_from_unpack(unpack)
                    template_types = repack_callable_args(template, tuple_type)
                    actual_types = repack_callable_args(cactual, tuple_type)
                    # Now we can use the same general helper as for tuple types.
                    unpack_constraints = build_constraints_for_simple_unpack(
                        template_types, actual_types, neg_op(self.direction)
                    )
                    res.extend(unpack_constraints)
                else:
                    # Negate direction due to function argument type contravariance.
                    res.extend(
                        infer_callable_arguments_constraints(template, cactual, self.direction)
                    )
        else:
            prefix = param_spec.prefix
            prefix_len = len(prefix.arg_types)
            cactual_ps = cactual.param_spec()

            if type_state.infer_polymorphic and cactual.variables and not self.skip_neg_op:
                # Similar logic to the branch above.
                res.extend(
                    infer_constraints(
                        cactual, template, neg_op(self.direction), skip_neg_op=True
                    )
                )
                extra_tvars = True

            if not cactual_ps:
                max_prefix_len = len([k for k in cactual.arg_kinds if k in (ARG_POS, ARG_OPT)])
                prefix_len = min(prefix_len, max_prefix_len)
                res.append(
                    Constraint(
                        param_spec,
                        neg_op(self.direction),
                        Parameters(
                            arg_types=cactual.arg_types[prefix_len:],
                            arg_kinds=cactual.arg_kinds[prefix_len:],
                            arg_names=cactual.arg_names[prefix_len:],
                            variables=cactual.variables
                            if not type_state.infer_polymorphic
                            else [],
                        ),
                    )
                )
            else:
                if len(param_spec.prefix.arg_types) &lt;= len(cactual_ps.prefix.arg_types):
                    cactual_ps = cactual_ps.copy_modified(
                        prefix=Parameters(
                            arg_types=cactual_ps.prefix.arg_types[prefix_len:],
                            arg_kinds=cactual_ps.prefix.arg_kinds[prefix_len:],
                            arg_names=cactual_ps.prefix.arg_names[prefix_len:],
                        )
                    )
                    res.append(Constraint(param_spec, neg_op(self.direction), cactual_ps))

            # Compare prefixes as well
            cactual_prefix = cactual.copy_modified(
                arg_types=cactual.arg_types[:prefix_len],
                arg_kinds=cactual.arg_kinds[:prefix_len],
                arg_names=cactual.arg_names[:prefix_len],
            )
            res.extend(
                infer_callable_arguments_constraints(prefix, cactual_prefix, self.direction)
            )

        template_ret_type, cactual_ret_type = template.ret_type, cactual.ret_type
        if template.type_guard is not None:
            template_ret_type = template.type_guard
        if cactual.type_guard is not None:
            cactual_ret_type = cactual.type_guard

        res.extend(infer_constraints(template_ret_type, cactual_ret_type, self.direction))
        if extra_tvars:
            for c in res:
                c.extra_tvars += cactual.variables
        return res
    elif isinstance(self.actual, AnyType):
        param_spec = template.param_spec()
        any_type = AnyType(TypeOfAny.from_another_any, source_any=self.actual)
        if param_spec is None:
            # FIX what if generic
            res = self.infer_against_any(template.arg_types, self.actual)
        else:
            res = [
                Constraint(
                    param_spec,
                    SUBTYPE_OF,
                    Parameters([any_type, any_type], [ARG_STAR, ARG_STAR2], [None, None]),
                )
            ]
        res.extend(infer_constraints(template.ret_type, any_type, self.direction))
        return res
    elif isinstance(self.actual, Overloaded):
        return self.infer_against_overloaded(self.actual, template)
    elif isinstance(self.actual, TypeType):
        return infer_constraints(template.ret_type, self.actual.item, self.direction)
    elif isinstance(self.actual, Instance):
        # Instances with __call__ method defined are considered structural
        # subtypes of Callable with a compatible signature.
        call = mypy.subtypes.find_member(
            "__call__", self.actual, self.actual, is_operator=True
        )
        if call:
            return infer_constraints(template, call, self.direction)
        else:
            return []
    else:
        return []

</t>
<t tx="ekr.20230831011819.861">def infer_against_overloaded(
    self, overloaded: Overloaded, template: CallableType
) -&gt; list[Constraint]:
    # Create constraints by matching an overloaded type against a template.
    # This is tricky to do in general. We cheat by only matching against
    # the first overload item that is callable compatible. This
    # seems to work somewhat well, but we should really use a more
    # reliable technique.
    item = find_matching_overload_item(overloaded, template)
    return infer_constraints(template, item, self.direction)

</t>
<t tx="ekr.20230831011819.862">def visit_tuple_type(self, template: TupleType) -&gt; list[Constraint]:
    actual = self.actual
    unpack_index = find_unpack_in_list(template.items)
    is_varlength_tuple = (
        isinstance(actual, Instance) and actual.type.fullname == "builtins.tuple"
    )

    if isinstance(actual, TupleType) or is_varlength_tuple:
        res: list[Constraint] = []
        if unpack_index is not None:
            if is_varlength_tuple:
                unpack_type = template.items[unpack_index]
                assert isinstance(unpack_type, UnpackType)
                unpacked_type = unpack_type.type
                assert isinstance(unpacked_type, TypeVarTupleType)
                return [Constraint(type_var=unpacked_type, op=self.direction, target=actual)]
            else:
                assert isinstance(actual, TupleType)
                unpack_constraints = build_constraints_for_simple_unpack(
                    template.items, actual.items, self.direction
                )
                actual_items: tuple[Type, ...] = ()
                template_items: tuple[Type, ...] = ()
                res.extend(unpack_constraints)
        elif isinstance(actual, TupleType):
            actual_items = tuple(actual.items)
            template_items = tuple(template.items)
        else:
            return res

        # Cases above will return if actual wasn't a TupleType.
        assert isinstance(actual, TupleType)
        if len(actual_items) == len(template_items):
            if (
                actual.partial_fallback.type.is_named_tuple
                and template.partial_fallback.type.is_named_tuple
            ):
                # For named tuples using just the fallbacks usually gives better results.
                return res + infer_constraints(
                    template.partial_fallback, actual.partial_fallback, self.direction
                )
            for i in range(len(template_items)):
                res.extend(
                    infer_constraints(template_items[i], actual_items[i], self.direction)
                )
        return res
    elif isinstance(actual, AnyType):
        return self.infer_against_any(template.items, actual)
    else:
        return []

</t>
<t tx="ekr.20230831011819.863">def visit_typeddict_type(self, template: TypedDictType) -&gt; list[Constraint]:
    actual = self.actual
    if isinstance(actual, TypedDictType):
        res: list[Constraint] = []
        # NOTE: Non-matching keys are ignored. Compatibility is checked
        #       elsewhere so this shouldn't be unsafe.
        for item_name, template_item_type, actual_item_type in template.zip(actual):
            res.extend(infer_constraints(template_item_type, actual_item_type, self.direction))
        return res
    elif isinstance(actual, AnyType):
        return self.infer_against_any(template.items.values(), actual)
    else:
        return []

</t>
<t tx="ekr.20230831011819.864">def visit_union_type(self, template: UnionType) -&gt; list[Constraint]:
    assert False, (
        "Unexpected UnionType in ConstraintBuilderVisitor"
        " (should have been handled in infer_constraints)"
    )

</t>
<t tx="ekr.20230831011819.865">def visit_type_alias_type(self, template: TypeAliasType) -&gt; list[Constraint]:
    assert False, f"This should be never called, got {template}"

</t>
<t tx="ekr.20230831011819.866">def infer_against_any(self, types: Iterable[Type], any_type: AnyType) -&gt; list[Constraint]:
    res: list[Constraint] = []
    for t in types:
        if isinstance(t, UnpackType) and isinstance(t.type, TypeVarTupleType):
            res.append(Constraint(t.type, self.direction, any_type))
        else:
            # Note that we ignore variance and simply always use the
            # original direction. This is because for Any targets direction is
            # irrelevant in most cases, see e.g. is_same_constraint().
            res.extend(infer_constraints(t, any_type, self.direction))
    return res

</t>
<t tx="ekr.20230831011819.867">def visit_overloaded(self, template: Overloaded) -&gt; list[Constraint]:
    if isinstance(self.actual, CallableType):
        items = find_matching_overload_items(template, self.actual)
    else:
        items = template.items
    res: list[Constraint] = []
    for t in items:
        res.extend(infer_constraints(t, self.actual, self.direction))
    return res

</t>
<t tx="ekr.20230831011819.868">def visit_type_type(self, template: TypeType) -&gt; list[Constraint]:
    if isinstance(self.actual, CallableType):
        return infer_constraints(template.item, self.actual.ret_type, self.direction)
    elif isinstance(self.actual, Overloaded):
        return infer_constraints(template.item, self.actual.items[0].ret_type, self.direction)
    elif isinstance(self.actual, TypeType):
        return infer_constraints(template.item, self.actual.item, self.direction)
    elif isinstance(self.actual, AnyType):
        return infer_constraints(template.item, self.actual, self.direction)
    else:
        return []


</t>
<t tx="ekr.20230831011819.869">def neg_op(op: int) -&gt; int:
    """Map SubtypeOf to SupertypeOf and vice versa."""

    if op == SUBTYPE_OF:
        return SUPERTYPE_OF
    elif op == SUPERTYPE_OF:
        return SUBTYPE_OF
    else:
        raise ValueError(f"Invalid operator {op}")


</t>
<t tx="ekr.20230831011819.87">def log(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 1:
        if message:
            print("LOG: ", *message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20230831011819.870">def find_matching_overload_item(overloaded: Overloaded, template: CallableType) -&gt; CallableType:
    """Disambiguate overload item against a template."""
    items = overloaded.items
    for item in items:
        # Return type may be indeterminate in the template, so ignore it when performing a
        # subtype check.
        if mypy.subtypes.is_callable_compatible(
            item, template, is_compat=mypy.subtypes.is_subtype, ignore_return=True
        ):
            return item
    # Fall back to the first item if we can't find a match. This is totally arbitrary --
    # maybe we should just bail out at this point.
    return items[0]


</t>
<t tx="ekr.20230831011819.871">def find_matching_overload_items(
    overloaded: Overloaded, template: CallableType
) -&gt; list[CallableType]:
    """Like find_matching_overload_item, but return all matches, not just the first."""
    items = overloaded.items
    res = []
    for item in items:
        # Return type may be indeterminate in the template, so ignore it when performing a
        # subtype check.
        if mypy.subtypes.is_callable_compatible(
            item, template, is_compat=mypy.subtypes.is_subtype, ignore_return=True
        ):
            res.append(item)
    if not res:
        # Falling back to all items if we can't find a match is pretty arbitrary, but
        # it maintains backward compatibility.
        res = items.copy()
    return res


</t>
<t tx="ekr.20230831011819.872">def get_tuple_fallback_from_unpack(unpack: UnpackType) -&gt; TypeInfo | None:
    """Get builtins.tuple type from available types to construct homogeneous tuples."""
    tp = get_proper_type(unpack.type)
    if isinstance(tp, Instance) and tp.type.fullname == "builtins.tuple":
        return tp.type
    if isinstance(tp, TypeVarTupleType):
        return tp.tuple_fallback.type
    if isinstance(tp, TupleType):
        for base in tp.partial_fallback.type.mro:
            if base.fullname == "builtins.tuple":
                return base
    return None


</t>
<t tx="ekr.20230831011819.873">def repack_callable_args(callable: CallableType, tuple_type: TypeInfo | None) -&gt; list[Type]:
    """Present callable with star unpack in a normalized form.

    Since positional arguments cannot follow star argument, they are packed in a suffix,
    while prefix is represented as individual positional args. We want to put all in a single
    list with unpack in the middle, and prefix/suffix on the sides (as they would appear
    in e.g. a TupleType).
    """
    if ARG_STAR not in callable.arg_kinds:
        return callable.arg_types
    star_index = callable.arg_kinds.index(ARG_STAR)
    arg_types = callable.arg_types[:star_index]
    star_type = callable.arg_types[star_index]
    suffix_types = []
    if not isinstance(star_type, UnpackType):
        if tuple_type is not None:
            # Re-normalize *args: X -&gt; *args: *tuple[X, ...]
            star_type = UnpackType(Instance(tuple_type, [star_type]))
        else:
            # This is unfortunate, something like tuple[Any, ...] would be better.
            star_type = UnpackType(AnyType(TypeOfAny.from_error))
    else:
        tp = get_proper_type(star_type.type)
        if isinstance(tp, TupleType):
            assert isinstance(tp.items[0], UnpackType)
            star_type = tp.items[0]
            suffix_types = tp.items[1:]
    return arg_types + [star_type] + suffix_types


</t>
<t tx="ekr.20230831011819.874">def build_constraints_for_simple_unpack(
    template_args: list[Type], actual_args: list[Type], direction: int
) -&gt; list[Constraint]:
    """Infer constraints between two lists of types with variadic items.

    This function is only supposed to be called when a variadic item is present in templates.
    If there is no variadic item the actuals, we simply use split_with_prefix_and_suffix()
    and infer prefix &lt;: prefix, suffix &lt;: suffix, variadic &lt;: middle. If there is a variadic
    item in the actuals we need to be more careful, only common prefix/suffix can generate
    constraints, also we can only infer constraints for variadic template item, if template
    prefix/suffix are shorter that actual ones, otherwise there may be partial overlap
    between variadic items, for example if template prefix is longer:

        templates: T1, T2, Ts, Ts, Ts, ...
        actuals:   A1, As, As, As, ...

    Note: this function can only be called for builtin variadic constructors: Tuple and Callable,
    for Instances variance depends on position, and a much more complex function
    build_constraints_for_unpack() should be used.
    """
    template_unpack = find_unpack_in_list(template_args)
    assert template_unpack is not None
    template_prefix = template_unpack
    template_suffix = len(template_args) - template_prefix - 1

    t_unpack = None
    res = []

    actual_unpack = find_unpack_in_list(actual_args)
    if actual_unpack is None:
        t_unpack = template_args[template_unpack]
        if template_prefix + template_suffix &gt; len(actual_args):
            # These can't be subtypes of each-other, return fast.
            assert isinstance(t_unpack, UnpackType)
            if isinstance(t_unpack.type, TypeVarTupleType):
                # Set TypeVarTuple to empty to improve error messages.
                return [
                    Constraint(
                        t_unpack.type, direction, TupleType([], t_unpack.type.tuple_fallback)
                    )
                ]
            else:
                return []
        common_prefix = template_prefix
        common_suffix = template_suffix
    else:
        actual_prefix = actual_unpack
        actual_suffix = len(actual_args) - actual_prefix - 1
        common_prefix = min(template_prefix, actual_prefix)
        common_suffix = min(template_suffix, actual_suffix)
        if actual_prefix &gt;= template_prefix and actual_suffix &gt;= template_suffix:
            # This is the only case where we can guarantee there will be no partial overlap.
            t_unpack = template_args[template_unpack]

    # Handle constraints from prefixes/suffixes first.
    start, middle, end = split_with_prefix_and_suffix(
        tuple(actual_args), common_prefix, common_suffix
    )
    for t, a in zip(template_args[:common_prefix], start):
        res.extend(infer_constraints(t, a, direction))
    if common_suffix:
        for t, a in zip(template_args[-common_suffix:], end):
            res.extend(infer_constraints(t, a, direction))

    if t_unpack is not None:
        # Add constraint(s) for variadic item when possible.
        assert isinstance(t_unpack, UnpackType)
        tp = get_proper_type(t_unpack.type)
        if isinstance(tp, Instance) and tp.type.fullname == "builtins.tuple":
            # Homogeneous case *tuple[T, ...] &lt;: [X, Y, Z, ...].
            for a in middle:
                # TODO: should we use union instead of join here?
                if not isinstance(a, UnpackType):
                    res.extend(infer_constraints(tp.args[0], a, direction))
                else:
                    a_tp = get_proper_type(a.type)
                    # This is the case *tuple[T, ...] &lt;: *tuple[A, ...].
                    if isinstance(a_tp, Instance) and a_tp.type.fullname == "builtins.tuple":
                        res.extend(infer_constraints(tp.args[0], a_tp.args[0], direction))
        elif isinstance(tp, TypeVarTupleType):
            res.append(Constraint(tp, direction, TupleType(list(middle), tp.tuple_fallback)))
    return res


</t>
<t tx="ekr.20230831011819.875">def build_constraints_for_unpack(
    # TODO: this naming is misleading, these should be "actual", not "mapped"
    # both template and actual can be mapped before, depending on direction.
    # Also the convention is to put template related args first.
    mapped: tuple[Type, ...],
    mapped_prefix_len: int | None,
    mapped_suffix_len: int | None,
    template: tuple[Type, ...],
    template_prefix_len: int,
    template_suffix_len: int,
    direction: int,
) -&gt; tuple[list[Constraint], tuple[Type, ...], tuple[Type, ...]]:
    # TODO: this function looks broken:
    # a) it should take into account variances, but it doesn't
    # b) it looks like both call sites always pass identical values to args (2, 3) and (5, 6)
    # because after map_instance_to_supertype() both template and actual have same TypeInfo.
    if mapped_prefix_len is None:
        mapped_prefix_len = template_prefix_len
    if mapped_suffix_len is None:
        mapped_suffix_len = template_suffix_len

    split_result = split_with_mapped_and_template(
        mapped,
        mapped_prefix_len,
        mapped_suffix_len,
        template,
        template_prefix_len,
        template_suffix_len,
    )
    assert split_result is not None
    (
        mapped_prefix,
        mapped_middle,
        mapped_suffix,
        template_prefix,
        template_middle,
        template_suffix,
    ) = split_result

    template_unpack = extract_unpack(template_middle)
    res = []

    if template_unpack is not None:
        if isinstance(template_unpack, TypeVarTupleType):
            res.append(
                Constraint(
                    template_unpack,
                    direction,
                    TupleType(list(mapped_middle), template_unpack.tuple_fallback),
                )
            )
        elif (
            isinstance(template_unpack, Instance)
            and template_unpack.type.fullname == "builtins.tuple"
        ):
            for item in mapped_middle:
                res.extend(infer_constraints(template_unpack.args[0], item, direction))

        elif isinstance(template_unpack, TupleType):
            if len(template_unpack.items) == len(mapped_middle):
                for template_arg, item in zip(template_unpack.items, mapped_middle):
                    res.extend(infer_constraints(template_arg, item, direction))
    return res, mapped_prefix + mapped_suffix, template_prefix + template_suffix


</t>
<t tx="ekr.20230831011819.876">def infer_directed_arg_constraints(left: Type, right: Type, direction: int) -&gt; list[Constraint]:
    """Infer constraints between two arguments using direction between original callables."""
    if isinstance(left, (ParamSpecType, UnpackType)) or isinstance(
        right, (ParamSpecType, UnpackType)
    ):
        # This avoids bogus constraints like T &lt;: P.args
        # TODO: can we infer something useful for *T vs P?
        return []
    if direction == SUBTYPE_OF:
        # We invert direction to account for argument contravariance.
        return infer_constraints(left, right, neg_op(direction))
    else:
        return infer_constraints(right, left, neg_op(direction))


</t>
<t tx="ekr.20230831011819.877">def infer_callable_arguments_constraints(
    template: CallableType | Parameters, actual: CallableType | Parameters, direction: int
) -&gt; list[Constraint]:
    """Infer constraints between argument types of two callables.

    This function essentially extracts four steps from are_parameters_compatible() in
    subtypes.py that involve subtype checks between argument types. We keep the argument
    matching logic, but ignore various strictness flags present there, and checks that
    do not involve subtyping. Then in place of every subtype check we put an infer_constraints()
    call for the same types.
    """
    res = []
    if direction == SUBTYPE_OF:
        left, right = template, actual
    else:
        left, right = actual, template
    left_star = left.var_arg()
    left_star2 = left.kw_arg()
    right_star = right.var_arg()
    right_star2 = right.kw_arg()

    # Numbering of steps below matches the one in are_parameters_compatible() for convenience.
    # Phase 1a: compare star vs star arguments.
    if left_star is not None and right_star is not None:
        res.extend(infer_directed_arg_constraints(left_star.typ, right_star.typ, direction))
    if left_star2 is not None and right_star2 is not None:
        res.extend(infer_directed_arg_constraints(left_star2.typ, right_star2.typ, direction))

    # Phase 1b: compare left args with corresponding non-star right arguments.
    for right_arg in right.formal_arguments():
        left_arg = mypy.typeops.callable_corresponding_argument(left, right_arg)
        if left_arg is None:
            continue
        res.extend(infer_directed_arg_constraints(left_arg.typ, right_arg.typ, direction))

    # Phase 1c: compare left args with right *args.
    if right_star is not None:
        right_by_position = right.try_synthesizing_arg_from_vararg(None)
        assert right_by_position is not None
        i = right_star.pos
        assert i is not None
        while i &lt; len(left.arg_kinds) and left.arg_kinds[i].is_positional():
            left_by_position = left.argument_by_position(i)
            assert left_by_position is not None
            res.extend(
                infer_directed_arg_constraints(
                    left_by_position.typ, right_by_position.typ, direction
                )
            )
            i += 1

    # Phase 1d: compare left args with right **kwargs.
    if right_star2 is not None:
        right_names = {name for name in right.arg_names if name is not None}
        left_only_names = set()
        for name, kind in zip(left.arg_names, left.arg_kinds):
            if name is None or kind.is_star() or name in right_names:
                continue
            left_only_names.add(name)

        right_by_name = right.try_synthesizing_arg_from_kwarg(None)
        assert right_by_name is not None
        for name in left_only_names:
            left_by_name = left.argument_by_name(name)
            assert left_by_name is not None
            res.extend(
                infer_directed_arg_constraints(left_by_name.typ, right_by_name.typ, direction)
            )
    return res
</t>
<t tx="ekr.20230831011819.878">@path mypy
&lt;&lt; copytype.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.879">from __future__ import annotations

from typing import Any, cast

from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    TypeAliasType,
    TypedDictType,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
)

# type_visitor needs to be imported after types
from mypy.type_visitor import TypeVisitor  # ruff: isort: skip


</t>
<t tx="ekr.20230831011819.88">def log_fine_grained(self, *message: str) -&gt; None:
    import mypy.build

    if self.verbosity() &gt;= 1:
        self.log("fine-grained:", *message)
    elif mypy.build.DEBUG_FINE_GRAINED:
        # Output log in a simplified format that is quick to browse.
        if message:
            print(*message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20230831011819.880">def copy_type(t: ProperType) -&gt; ProperType:
    """Create a shallow copy of a type.

    This can be used to mutate the copy with truthiness information.

    Classes compiled with mypyc don't support copy.copy(), so we need
    a custom implementation.
    """
    return t.accept(TypeShallowCopier())


</t>
<t tx="ekr.20230831011819.881">class TypeShallowCopier(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20230831011819.882">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011819.883">def visit_any(self, t: AnyType) -&gt; ProperType:
    return self.copy_common(t, AnyType(t.type_of_any, t.source_any, t.missing_import_name))

</t>
<t tx="ekr.20230831011819.884">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    return self.copy_common(t, NoneType())

</t>
<t tx="ekr.20230831011819.885">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    dup = UninhabitedType(t.is_noreturn)
    dup.ambiguous = t.ambiguous
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20230831011819.886">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.copy_common(t, ErasedType())

</t>
<t tx="ekr.20230831011819.887">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return self.copy_common(t, DeletedType(t.source))

</t>
<t tx="ekr.20230831011819.888">def visit_instance(self, t: Instance) -&gt; ProperType:
    dup = Instance(t.type, t.args, last_known_value=t.last_known_value)
    dup.invalid = t.invalid
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20230831011819.889">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    return self.copy_common(t, t.copy_modified())

</t>
<t tx="ekr.20230831011819.89">def trace(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 2:
        print("TRACE:", *message, file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20230831011819.890">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    dup = ParamSpecType(
        t.name, t.fullname, t.id, t.flavor, t.upper_bound, t.default, prefix=t.prefix
    )
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20230831011819.891">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    dup = Parameters(
        t.arg_types,
        t.arg_kinds,
        t.arg_names,
        variables=t.variables,
        is_ellipsis_args=t.is_ellipsis_args,
    )
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20230831011819.892">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    dup = TypeVarTupleType(
        t.name, t.fullname, t.id, t.upper_bound, t.tuple_fallback, t.default
    )
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20230831011819.893">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    dup = UnpackType(t.type)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20230831011819.894">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    return self.copy_common(t, PartialType(t.type, t.var, t.value_type))

</t>
<t tx="ekr.20230831011819.895">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    return self.copy_common(t, t.copy_modified())

</t>
<t tx="ekr.20230831011819.896">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    return self.copy_common(t, TupleType(t.items, t.partial_fallback, implicit=t.implicit))

</t>
<t tx="ekr.20230831011819.897">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    return self.copy_common(t, TypedDictType(t.items, t.required_keys, t.fallback))

</t>
<t tx="ekr.20230831011819.898">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    return self.copy_common(t, LiteralType(value=t.value, fallback=t.fallback))

</t>
<t tx="ekr.20230831011819.899">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    return self.copy_common(t, UnionType(t.items))

</t>
<t tx="ekr.20230831011819.9">from __future__ import annotations

import sys
from io import StringIO
from typing import Callable, TextIO


</t>
<t tx="ekr.20230831011819.90">def add_stats(self, **kwds: Any) -&gt; None:
    for key, value in kwds.items():
        if key in self.stats:
            self.stats[key] += value
        else:
            self.stats[key] = value

</t>
<t tx="ekr.20230831011819.900">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    return self.copy_common(t, Overloaded(items=t.items))

</t>
<t tx="ekr.20230831011819.901">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    # Use cast since the type annotations in TypeType are imprecise.
    return self.copy_common(t, TypeType(cast(Any, t.item)))

</t>
<t tx="ekr.20230831011819.902">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, "only ProperTypes supported"

</t>
<t tx="ekr.20230831011819.903">def copy_common(self, t: ProperType, t2: ProperType) -&gt; ProperType:
    t2.line = t.line
    t2.column = t.column
    t2.can_be_false = t.can_be_false
    t2.can_be_true = t.can_be_true
    return t2
</t>
<t tx="ekr.20230831011819.904">@path mypy
from __future__ import annotations

import os
from typing import Final

# Earliest fully supported Python 3.x version. Used as the default Python
# version in tests. Mypy wheels should be built starting with this version,
# and CI tests should be run on this version (and later versions).
PYTHON3_VERSION: Final = (3, 8)

# Earliest Python 3.x version supported via --python-version 3.x. To run
# mypy, at least version PYTHON3_VERSION is needed.
PYTHON3_VERSION_MIN: Final = (3, 7)  # Keep in sync with typeshed's python support

CACHE_DIR: Final = ".mypy_cache"
CONFIG_FILE: Final = ["mypy.ini", ".mypy.ini"]
PYPROJECT_CONFIG_FILES: Final = ["pyproject.toml"]
SHARED_CONFIG_FILES: Final = ["setup.cfg"]
USER_CONFIG_FILES: Final = ["~/.config/mypy/config", "~/.mypy.ini"]
if os.environ.get("XDG_CONFIG_HOME"):
    USER_CONFIG_FILES.insert(0, os.path.join(os.environ["XDG_CONFIG_HOME"], "mypy/config"))

CONFIG_FILES: Final = (
    CONFIG_FILE + PYPROJECT_CONFIG_FILES + SHARED_CONFIG_FILES + USER_CONFIG_FILES
)

# This must include all reporters defined in mypy.report. This is defined here
# to make reporter names available without importing mypy.report -- this speeds
# up startup.
REPORTER_NAMES: Final = [
    "linecount",
    "any-exprs",
    "linecoverage",
    "memory-xml",
    "cobertura-xml",
    "xml",
    "xslt-html",
    "xslt-txt",
    "html",
    "txt",
    "lineprecision",
]

# Threshold after which we sometimes filter out most errors to avoid very
# verbose output. The default is to show all errors.
MANY_ERRORS_THRESHOLD: Final = -1
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.905">@path mypy
&lt;&lt; dmypy_os.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.906">from __future__ import annotations

import sys
from typing import Any, Callable

if sys.platform == "win32":
    import ctypes
    import subprocess
    from ctypes.wintypes import DWORD, HANDLE

    PROCESS_QUERY_LIMITED_INFORMATION = ctypes.c_ulong(0x1000)

    kernel32 = ctypes.windll.kernel32
    OpenProcess: Callable[[DWORD, int, int], HANDLE] = kernel32.OpenProcess
    GetExitCodeProcess: Callable[[HANDLE, Any], int] = kernel32.GetExitCodeProcess
else:
    import os
    import signal


</t>
<t tx="ekr.20230831011819.907">def alive(pid: int) -&gt; bool:
    """Is the process alive?"""
    if sys.platform == "win32":
        # why can't anything be easy...
        status = DWORD()
        handle = OpenProcess(PROCESS_QUERY_LIMITED_INFORMATION, 0, pid)
        GetExitCodeProcess(handle, ctypes.byref(status))
        return status.value == 259  # STILL_ACTIVE
    else:
        try:
            os.kill(pid, 0)
        except OSError:
            return False
        return True


</t>
<t tx="ekr.20230831011819.908">def kill(pid: int) -&gt; None:
    """Kill the process."""
    if sys.platform == "win32":
        subprocess.check_output(f"taskkill /pid {pid} /f /t")
    else:
        os.kill(pid, signal.SIGKILL)
</t>
<t tx="ekr.20230831011819.909">@path mypy
"""Server for mypy daemon mode.

This implements a daemon process which keeps useful state in memory
to enable fine-grained incremental reprocessing of changes.
"""

&lt;&lt; dmypy_server.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.91">def stats_summary(self) -&gt; Mapping[str, object]:
    return self.stats


</t>
<t tx="ekr.20230831011819.911">from __future__ import annotations

import argparse
import base64
import io
import json
import os
import pickle
import subprocess
import sys
import time
import traceback
from contextlib import redirect_stderr, redirect_stdout
from typing import AbstractSet, Any, Callable, Final, List, Sequence, Tuple
from typing_extensions import TypeAlias as _TypeAlias

import mypy.build
import mypy.errors
import mypy.main
from mypy.dmypy_util import receive
from mypy.find_sources import InvalidSourceList, create_source_list
from mypy.fscache import FileSystemCache
from mypy.fswatcher import FileData, FileSystemWatcher
from mypy.inspections import InspectionEngine
from mypy.ipc import IPCServer
from mypy.modulefinder import BuildSource, FindModuleCache, SearchPaths, compute_search_paths
from mypy.options import Options
from mypy.server.update import FineGrainedBuildManager, refresh_suppressed_submodules
from mypy.suggestions import SuggestionEngine, SuggestionFailure
from mypy.typestate import reset_global_state
from mypy.util import FancyFormatter, count_stats
from mypy.version import __version__

MEM_PROFILE: Final = False  # If True, dump memory profile after initialization

if sys.platform == "win32":
    from subprocess import STARTUPINFO

</t>
<t tx="ekr.20230831011819.912">    def daemonize(
        options: Options, status_file: str, timeout: int | None = None, log_file: str | None = None
    ) -&gt; int:
        """Create the daemon process via "dmypy daemon" and pass options via command line

        When creating the daemon grandchild, we create it in a new console, which is
        started hidden. We cannot use DETACHED_PROCESS since it will cause console windows
        to pop up when starting. See
        https://github.com/python/cpython/pull/4150#issuecomment-340215696
        for more on why we can't have nice things.

        It also pickles the options to be unpickled by mypy.
        """
        command = [sys.executable, "-m", "mypy.dmypy", "--status-file", status_file, "daemon"]
        pickled_options = pickle.dumps(options.snapshot())
        command.append(f'--options-data="{base64.b64encode(pickled_options).decode()}"')
        if timeout:
            command.append(f"--timeout={timeout}")
        if log_file:
            command.append(f"--log-file={log_file}")
        info = STARTUPINFO()
        info.dwFlags = 0x1  # STARTF_USESHOWWINDOW aka use wShowWindow's value
        info.wShowWindow = 0  # SW_HIDE aka make the window invisible
        try:
            subprocess.Popen(command, creationflags=0x10, startupinfo=info)  # CREATE_NEW_CONSOLE
            return 0
        except subprocess.CalledProcessError as e:
            return e.returncode

</t>
<t tx="ekr.20230831011819.913">else:

    def _daemonize_cb(func: Callable[[], None], log_file: str | None = None) -&gt; int:
        """Arrange to call func() in a grandchild of the current process.

        Return 0 for success, exit status for failure, negative if
        subprocess killed by signal.
        """
        # See https://stackoverflow.com/questions/473620/how-do-you-create-a-daemon-in-python
        sys.stdout.flush()
        sys.stderr.flush()
        pid = os.fork()
        if pid:
            # Parent process: wait for child in case things go bad there.
            npid, sts = os.waitpid(pid, 0)
            sig = sts &amp; 0xFF
            if sig:
                print("Child killed by signal", sig)
                return -sig
            sts = sts &gt;&gt; 8
            if sts:
                print("Child exit status", sts)
            return sts
        # Child process: do a bunch of UNIX stuff and then fork a grandchild.
        try:
            os.setsid()  # Detach controlling terminal
            os.umask(0o27)
            devnull = os.open("/dev/null", os.O_RDWR)
            os.dup2(devnull, 0)
            os.dup2(devnull, 1)
            os.dup2(devnull, 2)
            os.close(devnull)
            pid = os.fork()
            if pid:
                # Child is done, exit to parent.
                os._exit(0)
            # Grandchild: run the server.
            if log_file:
                sys.stdout = sys.stderr = open(log_file, "a", buffering=1)
                fd = sys.stdout.fileno()
                os.dup2(fd, 2)
                os.dup2(fd, 1)
            func()
        finally:
            # Make sure we never get back into the caller.
            os._exit(1)

</t>
<t tx="ekr.20230831011819.914">    def daemonize(
        options: Options, status_file: str, timeout: int | None = None, log_file: str | None = None
    ) -&gt; int:
        """Run the mypy daemon in a grandchild of the current process

        Return 0 for success, exit status for failure, negative if
        subprocess killed by signal.
        """
        return _daemonize_cb(Server(options, status_file, timeout).serve, log_file)


</t>
<t tx="ekr.20230831011819.915"># Server code.

CONNECTION_NAME: Final = "dmypy"


def process_start_options(flags: list[str], allow_sources: bool) -&gt; Options:
    _, options = mypy.main.process_options(
        ["-i"] + flags, require_targets=False, server_options=True
    )
    if options.report_dirs:
        print("dmypy: Ignoring report generation settings. Start/restart cannot generate reports.")
    if options.junit_xml:
        print(
            "dmypy: Ignoring report generation settings. "
            "Start/restart does not support --junit-xml. Pass it to check/recheck instead"
        )
        options.junit_xml = None
    if not options.incremental:
        sys.exit("dmypy: start/restart should not disable incremental mode")
    if options.follow_imports not in ("skip", "error", "normal"):
        sys.exit("dmypy: follow-imports=silent not supported")
    return options


</t>
<t tx="ekr.20230831011819.916">def ignore_suppressed_imports(module: str) -&gt; bool:
    """Can we skip looking for newly unsuppressed imports to module?"""
    # Various submodules of 'encodings' can be suppressed, since it
    # uses module-level '__getattr__'. Skip them since there are many
    # of them, and following imports to them is kind of pointless.
    return module.startswith("encodings.")


</t>
<t tx="ekr.20230831011819.917">ModulePathPair: _TypeAlias = Tuple[str, str]
ModulePathPairs: _TypeAlias = List[ModulePathPair]
ChangesAndRemovals: _TypeAlias = Tuple[ModulePathPairs, ModulePathPairs]


class Server:
    @others
</t>
<t tx="ekr.20230831011819.918"># NOTE: the instance is constructed in the parent process but
# serve() is called in the grandchild (by daemonize()).

def __init__(self, options: Options, status_file: str, timeout: int | None = None) -&gt; None:
    """Initialize the server with the desired mypy flags."""
    self.options = options
    # Snapshot the options info before we muck with it, to detect changes
    self.options_snapshot = options.snapshot()
    self.timeout = timeout
    self.fine_grained_manager: FineGrainedBuildManager | None = None

    if os.path.isfile(status_file):
        os.unlink(status_file)

    self.fscache = FileSystemCache()

    options.raise_exceptions = True
    options.incremental = True
    options.fine_grained_incremental = True
    options.show_traceback = True
    if options.use_fine_grained_cache:
        # Using fine_grained_cache implies generating and caring
        # about the fine grained cache
        options.cache_fine_grained = True
    else:
        options.cache_dir = os.devnull
    # Fine-grained incremental doesn't support general partial types
    # (details in https://github.com/python/mypy/issues/4492)
    options.local_partial_types = True
    self.status_file = status_file

    # Since the object is created in the parent process we can check
    # the output terminal options here.
    self.formatter = FancyFormatter(sys.stdout, sys.stderr, options.hide_error_codes)

</t>
<t tx="ekr.20230831011819.919">def _response_metadata(self) -&gt; dict[str, str]:
    py_version = f"{self.options.python_version[0]}_{self.options.python_version[1]}"
    return {"platform": self.options.platform, "python_version": py_version}

</t>
<t tx="ekr.20230831011819.92">def deps_to_json(x: dict[str, set[str]]) -&gt; str:
    return json.dumps({k: list(v) for k, v in x.items()}, separators=(",", ":"))


</t>
<t tx="ekr.20230831011819.920">def serve(self) -&gt; None:
    """Serve requests, synchronously (no thread or fork)."""
    command = None
    server = IPCServer(CONNECTION_NAME, self.timeout)
    try:
        with open(self.status_file, "w") as f:
            json.dump({"pid": os.getpid(), "connection_name": server.connection_name}, f)
            f.write("\n")  # I like my JSON with a trailing newline
        while True:
            with server:
                data = receive(server)
                debug_stdout = io.StringIO()
                debug_stderr = io.StringIO()
                sys.stdout = debug_stdout
                sys.stderr = debug_stderr
                resp: dict[str, Any] = {}
                if "command" not in data:
                    resp = {"error": "No command found in request"}
                else:
                    command = data["command"]
                    if not isinstance(command, str):
                        resp = {"error": "Command is not a string"}
                    else:
                        command = data.pop("command")
                        try:
                            resp = self.run_command(command, data)
                        except Exception:
                            # If we are crashing, report the crash to the client
                            tb = traceback.format_exception(*sys.exc_info())
                            resp = {"error": "Daemon crashed!\n" + "".join(tb)}
                            resp.update(self._response_metadata())
                            resp["stdout"] = debug_stdout.getvalue()
                            resp["stderr"] = debug_stderr.getvalue()
                            server.write(json.dumps(resp).encode("utf8"))
                            raise
                resp["stdout"] = debug_stdout.getvalue()
                resp["stderr"] = debug_stderr.getvalue()
                try:
                    resp.update(self._response_metadata())
                    server.write(json.dumps(resp).encode("utf8"))
                except OSError:
                    pass  # Maybe the client hung up
                if command == "stop":
                    reset_global_state()
                    sys.exit(0)
    finally:
        # If the final command is something other than a clean
        # stop, remove the status file. (We can't just
        # simplify the logic and always remove the file, since
        # that could cause us to remove a future server's
        # status file.)
        if command != "stop":
            os.unlink(self.status_file)
        try:
            server.cleanup()  # try to remove the socket dir on Linux
        except OSError:
            pass
        exc_info = sys.exc_info()
        if exc_info[0] and exc_info[0] is not SystemExit:
            traceback.print_exception(*exc_info)

</t>
<t tx="ekr.20230831011819.921">def run_command(self, command: str, data: dict[str, object]) -&gt; dict[str, object]:
    """Run a specific command from the registry."""
    key = "cmd_" + command
    method = getattr(self.__class__, key, None)
    if method is None:
        return {"error": f"Unrecognized command '{command}'"}
    else:
        if command not in {"check", "recheck", "run"}:
            # Only the above commands use some error formatting.
            del data["is_tty"]
            del data["terminal_width"]
        ret = method(self, **data)
        assert isinstance(ret, dict)
        return ret

</t>
<t tx="ekr.20230831011819.922"># Command functions (run in the server via RPC).

def cmd_status(self, fswatcher_dump_file: str | None = None) -&gt; dict[str, object]:
    """Return daemon status."""
    res: dict[str, object] = {}
    res.update(get_meminfo())
    if fswatcher_dump_file:
        data = self.fswatcher.dump_file_data() if hasattr(self, "fswatcher") else {}
        # Using .dumps and then writing was noticeably faster than using dump
        s = json.dumps(data)
        with open(fswatcher_dump_file, "w") as f:
            f.write(s)
    return res

</t>
<t tx="ekr.20230831011819.923">def cmd_stop(self) -&gt; dict[str, object]:
    """Stop daemon."""
    # We need to remove the status file *before* we complete the
    # RPC. Otherwise a race condition exists where a subsequent
    # command can see a status file from a dying server and think
    # it is a live one.
    os.unlink(self.status_file)
    return {}

</t>
<t tx="ekr.20230831011819.924">def cmd_run(
    self,
    version: str,
    args: Sequence[str],
    export_types: bool,
    is_tty: bool,
    terminal_width: int,
) -&gt; dict[str, object]:
    """Check a list of files, triggering a restart if needed."""
    stderr = io.StringIO()
    stdout = io.StringIO()
    try:
        # Process options can exit on improper arguments, so we need to catch that and
        # capture stderr so the client can report it
        with redirect_stderr(stderr):
            with redirect_stdout(stdout):
                sources, options = mypy.main.process_options(
                    ["-i"] + list(args),
                    require_targets=True,
                    server_options=True,
                    fscache=self.fscache,
                    program="mypy-daemon",
                    header=argparse.SUPPRESS,
                )
        # Signal that we need to restart if the options have changed
        if not options.compare_stable(self.options_snapshot):
            return {"restart": "configuration changed"}
        if __version__ != version:
            return {"restart": "mypy version changed"}
        if self.fine_grained_manager:
            manager = self.fine_grained_manager.manager
            start_plugins_snapshot = manager.plugins_snapshot
            _, current_plugins_snapshot = mypy.build.load_plugins(
                options, manager.errors, sys.stdout, extra_plugins=()
            )
            if current_plugins_snapshot != start_plugins_snapshot:
                return {"restart": "plugins changed"}
    except InvalidSourceList as err:
        return {"out": "", "err": str(err), "status": 2}
    except SystemExit as e:
        return {"out": stdout.getvalue(), "err": stderr.getvalue(), "status": e.code}
    return self.check(sources, export_types, is_tty, terminal_width)

</t>
<t tx="ekr.20230831011819.925">def cmd_check(
    self, files: Sequence[str], export_types: bool, is_tty: bool, terminal_width: int
) -&gt; dict[str, object]:
    """Check a list of files."""
    try:
        sources = create_source_list(files, self.options, self.fscache)
    except InvalidSourceList as err:
        return {"out": "", "err": str(err), "status": 2}
    return self.check(sources, export_types, is_tty, terminal_width)

</t>
<t tx="ekr.20230831011819.926">def cmd_recheck(
    self,
    is_tty: bool,
    terminal_width: int,
    export_types: bool,
    remove: list[str] | None = None,
    update: list[str] | None = None,
) -&gt; dict[str, object]:
    """Check the same list of files we checked most recently.

    If remove/update is given, they modify the previous list;
    if all are None, stat() is called for each file in the previous list.
    """
    t0 = time.time()
    if not self.fine_grained_manager:
        return {"error": "Command 'recheck' is only valid after a 'check' command"}
    sources = self.previous_sources
    if remove:
        removals = set(remove)
        sources = [s for s in sources if s.path and s.path not in removals]
    if update:
        known = {s.path for s in sources if s.path}
        added = [p for p in update if p not in known]
        try:
            added_sources = create_source_list(added, self.options, self.fscache)
        except InvalidSourceList as err:
            return {"out": "", "err": str(err), "status": 2}
        sources = sources + added_sources  # Make a copy!
    t1 = time.time()
    manager = self.fine_grained_manager.manager
    manager.log(f"fine-grained increment: cmd_recheck: {t1 - t0:.3f}s")
    self.options.export_types = export_types
    if not self.following_imports():
        messages = self.fine_grained_increment(sources, remove, update)
    else:
        assert remove is None and update is None
        messages = self.fine_grained_increment_follow_imports(sources)
    res = self.increment_output(messages, sources, is_tty, terminal_width)
    self.flush_caches()
    self.update_stats(res)
    return res

</t>
<t tx="ekr.20230831011819.927">def check(
    self, sources: list[BuildSource], export_types: bool, is_tty: bool, terminal_width: int
) -&gt; dict[str, Any]:
    """Check using fine-grained incremental mode.

    If is_tty is True format the output nicely with colors and summary line
    (unless disabled in self.options). Also pass the terminal_width to formatter.
    """
    self.options.export_types = export_types
    if not self.fine_grained_manager:
        res = self.initialize_fine_grained(sources, is_tty, terminal_width)
    else:
        if not self.following_imports():
            messages = self.fine_grained_increment(sources)
        else:
            messages = self.fine_grained_increment_follow_imports(sources)
        res = self.increment_output(messages, sources, is_tty, terminal_width)
    self.flush_caches()
    self.update_stats(res)
    return res

</t>
<t tx="ekr.20230831011819.928">def flush_caches(self) -&gt; None:
    self.fscache.flush()
    if self.fine_grained_manager:
        self.fine_grained_manager.flush_cache()

</t>
<t tx="ekr.20230831011819.929">def update_stats(self, res: dict[str, Any]) -&gt; None:
    if self.fine_grained_manager:
        manager = self.fine_grained_manager.manager
        manager.dump_stats()
        res["stats"] = manager.stats
        manager.stats = {}

</t>
<t tx="ekr.20230831011819.93"># File for storing metadata about all the fine-grained dependency caches
DEPS_META_FILE: Final = "@deps.meta.json"
# File for storing fine-grained dependencies that didn't a parent in the build
DEPS_ROOT_FILE: Final = "@root.deps.json"

# The name of the fake module used to store fine-grained dependencies that
# have no other place to go.
FAKE_ROOT_MODULE: Final = "@root"


def write_deps_cache(
    rdeps: dict[str, dict[str, set[str]]], manager: BuildManager, graph: Graph
) -&gt; None:
    """Write cache files for fine-grained dependencies.

    Serialize fine-grained dependencies map for fine grained mode.

    Dependencies on some module 'm' is stored in the dependency cache
    file m.deps.json.  This entails some spooky action at a distance:
    if module 'n' depends on 'm', that produces entries in m.deps.json.
    When there is a dependency on a module that does not exist in the
    build, it is stored with its first existing parent module. If no
    such module exists, it is stored with the fake module FAKE_ROOT_MODULE.

    This means that the validity of the fine-grained dependency caches
    are a global property, so we store validity checking information for
    fine-grained dependencies in a global cache file:
     * We take a snapshot of current sources to later check consistency
       between the fine-grained dependency cache and module cache metadata
     * We store the mtime of all of the dependency files to verify they
       haven't changed
    """
    metastore = manager.metastore

    error = False

    fg_deps_meta = manager.fg_deps_meta.copy()

    for id in rdeps:
        if id != FAKE_ROOT_MODULE:
            _, _, deps_json = get_cache_names(id, graph[id].xpath, manager.options)
        else:
            deps_json = DEPS_ROOT_FILE
        assert deps_json
        manager.log("Writing deps cache", deps_json)
        if not manager.metastore.write(deps_json, deps_to_json(rdeps[id])):
            manager.log(f"Error writing fine-grained deps JSON file {deps_json}")
            error = True
        else:
            fg_deps_meta[id] = {"path": deps_json, "mtime": manager.getmtime(deps_json)}

    meta_snapshot: dict[str, str] = {}
    for id, st in graph.items():
        # If we didn't parse a file (so it doesn't have a
        # source_hash), then it must be a module with a fresh cache,
        # so use the hash from that.
        if st.source_hash:
            hash = st.source_hash
        else:
            assert st.meta, "Module must be either parsed or cached"
            hash = st.meta.hash
        meta_snapshot[id] = hash

    meta = {"snapshot": meta_snapshot, "deps_meta": fg_deps_meta}

    if not metastore.write(DEPS_META_FILE, json.dumps(meta, separators=(",", ":"))):
        manager.log(f"Error writing fine-grained deps meta JSON file {DEPS_META_FILE}")
        error = True

    if error:
        manager.errors.set_file(_cache_dir_prefix(manager.options), None, manager.options)
        manager.errors.report(0, 0, "Error writing fine-grained dependencies cache", blocker=True)


</t>
<t tx="ekr.20230831011819.930">def following_imports(self) -&gt; bool:
    """Are we following imports?"""
    # TODO: What about silent?
    return self.options.follow_imports == "normal"

</t>
<t tx="ekr.20230831011819.931">def initialize_fine_grained(
    self, sources: list[BuildSource], is_tty: bool, terminal_width: int
) -&gt; dict[str, Any]:
    self.fswatcher = FileSystemWatcher(self.fscache)
    t0 = time.time()
    self.update_sources(sources)
    t1 = time.time()
    try:
        result = mypy.build.build(sources=sources, options=self.options, fscache=self.fscache)
    except mypy.errors.CompileError as e:
        output = "".join(s + "\n" for s in e.messages)
        if e.use_stdout:
            out, err = output, ""
        else:
            out, err = "", output
        return {"out": out, "err": err, "status": 2}
    messages = result.errors
    self.fine_grained_manager = FineGrainedBuildManager(result)

    if self.following_imports():
        sources = find_all_sources_in_build(self.fine_grained_manager.graph, sources)
        self.update_sources(sources)

    self.previous_sources = sources

    # If we are using the fine-grained cache, build hasn't actually done
    # the typechecking on the updated files yet.
    # Run a fine-grained update starting from the cached data
    if result.used_cache:
        t2 = time.time()
        # Pull times and hashes out of the saved_cache and stick them into
        # the fswatcher, so we pick up the changes.
        for state in self.fine_grained_manager.graph.values():
            meta = state.meta
            if meta is None:
                continue
            assert state.path is not None
            self.fswatcher.set_file_data(
                state.path,
                FileData(st_mtime=float(meta.mtime), st_size=meta.size, hash=meta.hash),
            )

        changed, removed = self.find_changed(sources)
        changed += self.find_added_suppressed(
            self.fine_grained_manager.graph,
            set(),
            self.fine_grained_manager.manager.search_paths,
        )

        # Find anything that has had its dependency list change
        for state in self.fine_grained_manager.graph.values():
            if not state.is_fresh():
                assert state.path is not None
                changed.append((state.id, state.path))

        t3 = time.time()
        # Run an update
        messages = self.fine_grained_manager.update(changed, removed)

        if self.following_imports():
            # We need to do another update to any new files found by following imports.
            messages = self.fine_grained_increment_follow_imports(sources)

        t4 = time.time()
        self.fine_grained_manager.manager.add_stats(
            update_sources_time=t1 - t0,
            build_time=t2 - t1,
            find_changes_time=t3 - t2,
            fg_update_time=t4 - t3,
            files_changed=len(removed) + len(changed),
        )

    else:
        # Stores the initial state of sources as a side effect.
        self.fswatcher.find_changed()

    if MEM_PROFILE:
        from mypy.memprofile import print_memory_profile

        print_memory_profile(run_gc=False)

    __, n_notes, __ = count_stats(messages)
    status = 1 if messages and n_notes &lt; len(messages) else 0
    messages = self.pretty_messages(messages, len(sources), is_tty, terminal_width)
    return {"out": "".join(s + "\n" for s in messages), "err": "", "status": status}

</t>
<t tx="ekr.20230831011819.932">def fine_grained_increment(
    self,
    sources: list[BuildSource],
    remove: list[str] | None = None,
    update: list[str] | None = None,
) -&gt; list[str]:
    """Perform a fine-grained type checking increment.

    If remove and update are None, determine changed paths by using
    fswatcher. Otherwise, assume that only these files have changes.

    Args:
        sources: sources passed on the command line
        remove: paths of files that have been removed
        update: paths of files that have been changed or created
    """
    assert self.fine_grained_manager is not None
    manager = self.fine_grained_manager.manager

    t0 = time.time()
    if remove is None and update is None:
        # Use the fswatcher to determine which files were changed
        # (updated or added) or removed.
        self.update_sources(sources)
        changed, removed = self.find_changed(sources)
    else:
        # Use the remove/update lists to update fswatcher.
        # This avoids calling stat() for unchanged files.
        changed, removed = self.update_changed(sources, remove or [], update or [])
    changed += self.find_added_suppressed(
        self.fine_grained_manager.graph, set(), manager.search_paths
    )
    manager.search_paths = compute_search_paths(sources, manager.options, manager.data_dir)
    t1 = time.time()
    manager.log(f"fine-grained increment: find_changed: {t1 - t0:.3f}s")
    messages = self.fine_grained_manager.update(changed, removed)
    t2 = time.time()
    manager.log(f"fine-grained increment: update: {t2 - t1:.3f}s")
    manager.add_stats(
        find_changes_time=t1 - t0,
        fg_update_time=t2 - t1,
        files_changed=len(removed) + len(changed),
    )

    self.previous_sources = sources
    return messages

</t>
<t tx="ekr.20230831011819.933">def fine_grained_increment_follow_imports(self, sources: list[BuildSource]) -&gt; list[str]:
    """Like fine_grained_increment, but follow imports."""
    t0 = time.time()

    # TODO: Support file events

    assert self.fine_grained_manager is not None
    fine_grained_manager = self.fine_grained_manager
    graph = fine_grained_manager.graph
    manager = fine_grained_manager.manager

    orig_modules = list(graph.keys())

    self.update_sources(sources)
    changed_paths = self.fswatcher.find_changed()
    manager.search_paths = compute_search_paths(sources, manager.options, manager.data_dir)

    t1 = time.time()
    manager.log(f"fine-grained increment: find_changed: {t1 - t0:.3f}s")

    seen = {source.module for source in sources}

    # Find changed modules reachable from roots (or in roots) already in graph.
    changed, new_files = self.find_reachable_changed_modules(
        sources, graph, seen, changed_paths
    )
    sources.extend(new_files)

    # Process changes directly reachable from roots.
    messages = fine_grained_manager.update(changed, [], followed=True)

    # Follow deps from changed modules (still within graph).
    worklist = changed.copy()
    while worklist:
        module = worklist.pop()
        if module[0] not in graph:
            continue
        sources2 = self.direct_imports(module, graph)
        # Filter anything already seen before. This prevents
        # infinite looping if there are any self edges. (Self
        # edges are maybe a bug, but...)
        sources2 = [source for source in sources2 if source.module not in seen]
        changed, new_files = self.find_reachable_changed_modules(
            sources2, graph, seen, changed_paths
        )
        self.update_sources(new_files)
        messages = fine_grained_manager.update(changed, [], followed=True)
        worklist.extend(changed)

    t2 = time.time()

    def refresh_file(module: str, path: str) -&gt; list[str]:
        return fine_grained_manager.update([(module, path)], [], followed=True)

    for module_id, state in list(graph.items()):
        new_messages = refresh_suppressed_submodules(
            module_id, state.path, fine_grained_manager.deps, graph, self.fscache, refresh_file
        )
        if new_messages is not None:
            messages = new_messages

    t3 = time.time()

    # There may be new files that became available, currently treated as
    # suppressed imports. Process them.
    while True:
        new_unsuppressed = self.find_added_suppressed(graph, seen, manager.search_paths)
        if not new_unsuppressed:
            break
        new_files = [BuildSource(mod[1], mod[0], followed=True) for mod in new_unsuppressed]
        sources.extend(new_files)
        self.update_sources(new_files)
        messages = fine_grained_manager.update(new_unsuppressed, [], followed=True)

        for module_id, path in new_unsuppressed:
            new_messages = refresh_suppressed_submodules(
                module_id, path, fine_grained_manager.deps, graph, self.fscache, refresh_file
            )
            if new_messages is not None:
                messages = new_messages

    t4 = time.time()

    # Find all original modules in graph that were not reached -- they are deleted.
    to_delete = []
    for module_id in orig_modules:
        if module_id not in graph:
            continue
        if module_id not in seen:
            module_path = graph[module_id].path
            assert module_path is not None
            to_delete.append((module_id, module_path))
    if to_delete:
        messages = fine_grained_manager.update([], to_delete)

    fix_module_deps(graph)

    self.previous_sources = find_all_sources_in_build(graph)
    self.update_sources(self.previous_sources)

    # Store current file state as side effect
    self.fswatcher.find_changed()

    t5 = time.time()

    manager.log(f"fine-grained increment: update: {t5 - t1:.3f}s")
    manager.add_stats(
        find_changes_time=t1 - t0,
        fg_update_time=t2 - t1,
        refresh_suppressed_time=t3 - t2,
        find_added_supressed_time=t4 - t3,
        cleanup_time=t5 - t4,
    )

    return messages

</t>
<t tx="ekr.20230831011819.934">def find_reachable_changed_modules(
    self,
    roots: list[BuildSource],
    graph: mypy.build.Graph,
    seen: set[str],
    changed_paths: AbstractSet[str],
) -&gt; tuple[list[tuple[str, str]], list[BuildSource]]:
    """Follow imports within graph from given sources until hitting changed modules.

    If we find a changed module, we can't continue following imports as the imports
    may have changed.

    Args:
        roots: modules where to start search from
        graph: module graph to use for the search
        seen: modules we've seen before that won't be visited (mutated here!!)
        changed_paths: which paths have changed (stop search here and return any found)

    Return (encountered reachable changed modules,
            unchanged files not in sources_set traversed).
    """
    changed = []
    new_files = []
    worklist = roots.copy()
    seen.update(source.module for source in worklist)
    while worklist:
        nxt = worklist.pop()
        if nxt.module not in seen:
            seen.add(nxt.module)
            new_files.append(nxt)
        if nxt.path in changed_paths:
            assert nxt.path is not None  # TODO
            changed.append((nxt.module, nxt.path))
        elif nxt.module in graph:
            state = graph[nxt.module]
            for dep in state.dependencies:
                if dep not in seen:
                    seen.add(dep)
                    worklist.append(BuildSource(graph[dep].path, graph[dep].id, followed=True))
    return changed, new_files

</t>
<t tx="ekr.20230831011819.935">def direct_imports(
    self, module: tuple[str, str], graph: mypy.build.Graph
) -&gt; list[BuildSource]:
    """Return the direct imports of module not included in seen."""
    state = graph[module[0]]
    return [BuildSource(graph[dep].path, dep, followed=True) for dep in state.dependencies]

</t>
<t tx="ekr.20230831011819.936">def find_added_suppressed(
    self, graph: mypy.build.Graph, seen: set[str], search_paths: SearchPaths
) -&gt; list[tuple[str, str]]:
    """Find suppressed modules that have been added (and not included in seen).

    Args:
        seen: reachable modules we've seen before (mutated here!!)

    Return suppressed, added modules.
    """
    all_suppressed = set()
    for state in graph.values():
        all_suppressed |= state.suppressed_set

    # Filter out things that shouldn't actually be considered suppressed.
    #
    # TODO: Figure out why these are treated as suppressed
    all_suppressed = {
        module
        for module in all_suppressed
        if module not in graph and not ignore_suppressed_imports(module)
    }

    # Optimization: skip top-level packages that are obviously not
    # there, to avoid calling the relatively slow find_module()
    # below too many times.
    packages = {module.split(".", 1)[0] for module in all_suppressed}
    packages = filter_out_missing_top_level_packages(packages, search_paths, self.fscache)

    # TODO: Namespace packages

    finder = FindModuleCache(search_paths, self.fscache, self.options)

    found = []

    for module in all_suppressed:
        top_level_pkg = module.split(".", 1)[0]
        if top_level_pkg not in packages:
            # Fast path: non-existent top-level package
            continue
        result = finder.find_module(module, fast_path=True)
        if isinstance(result, str) and module not in seen:
            # When not following imports, we only follow imports to .pyi files.
            if not self.following_imports() and not result.endswith(".pyi"):
                continue
            found.append((module, result))
            seen.add(module)

    return found

</t>
<t tx="ekr.20230831011819.937">def increment_output(
    self, messages: list[str], sources: list[BuildSource], is_tty: bool, terminal_width: int
) -&gt; dict[str, Any]:
    status = 1 if messages else 0
    messages = self.pretty_messages(messages, len(sources), is_tty, terminal_width)
    return {"out": "".join(s + "\n" for s in messages), "err": "", "status": status}

</t>
<t tx="ekr.20230831011819.938">def pretty_messages(
    self,
    messages: list[str],
    n_sources: int,
    is_tty: bool = False,
    terminal_width: int | None = None,
) -&gt; list[str]:
    use_color = self.options.color_output and is_tty
    fit_width = self.options.pretty and is_tty
    if fit_width:
        messages = self.formatter.fit_in_terminal(
            messages, fixed_terminal_width=terminal_width
        )
    if self.options.error_summary:
        summary: str | None = None
        n_errors, n_notes, n_files = count_stats(messages)
        if n_errors:
            summary = self.formatter.format_error(
                n_errors, n_files, n_sources, use_color=use_color
            )
        elif not messages or n_notes == len(messages):
            summary = self.formatter.format_success(n_sources, use_color)
        if summary:
            # Create new list to avoid appending multiple summaries on successive runs.
            messages = messages + [summary]
    if use_color:
        messages = [self.formatter.colorize(m) for m in messages]
    return messages

</t>
<t tx="ekr.20230831011819.939">def update_sources(self, sources: list[BuildSource]) -&gt; None:
    paths = [source.path for source in sources if source.path is not None]
    if self.following_imports():
        # Filter out directories (used for namespace packages).
        paths = [path for path in paths if self.fscache.isfile(path)]
    self.fswatcher.add_watched_paths(paths)

</t>
<t tx="ekr.20230831011819.94">def invert_deps(deps: dict[str, set[str]], graph: Graph) -&gt; dict[str, dict[str, set[str]]]:
    """Splits fine-grained dependencies based on the module of the trigger.

    Returns a dictionary from module ids to all dependencies on that
    module. Dependencies not associated with a module in the build will be
    associated with the nearest parent module that is in the build, or the
    fake module FAKE_ROOT_MODULE if none are.
    """
    # Lazy import to speed up startup
    from mypy.server.target import trigger_to_target

    # Prepopulate the map for all the modules that have been processed,
    # so that we always generate files for processed modules (even if
    # there aren't any dependencies to them.)
    rdeps: dict[str, dict[str, set[str]]] = {id: {} for id, st in graph.items() if st.tree}
    for trigger, targets in deps.items():
        module = module_prefix(graph, trigger_to_target(trigger))
        if not module or not graph[module].tree:
            module = FAKE_ROOT_MODULE

        mod_rdeps = rdeps.setdefault(module, {})
        mod_rdeps.setdefault(trigger, set()).update(targets)

    return rdeps


</t>
<t tx="ekr.20230831011819.940">def update_changed(
    self, sources: list[BuildSource], remove: list[str], update: list[str]
) -&gt; ChangesAndRemovals:
    changed_paths = self.fswatcher.update_changed(remove, update)
    return self._find_changed(sources, changed_paths)

</t>
<t tx="ekr.20230831011819.941">def find_changed(self, sources: list[BuildSource]) -&gt; ChangesAndRemovals:
    changed_paths = self.fswatcher.find_changed()
    return self._find_changed(sources, changed_paths)

</t>
<t tx="ekr.20230831011819.942">def _find_changed(
    self, sources: list[BuildSource], changed_paths: AbstractSet[str]
) -&gt; ChangesAndRemovals:
    # Find anything that has been added or modified
    changed = [
        (source.module, source.path)
        for source in sources
        if source.path and source.path in changed_paths
    ]

    # Now find anything that has been removed from the build
    modules = {source.module for source in sources}
    omitted = [source for source in self.previous_sources if source.module not in modules]
    removed = []
    for source in omitted:
        path = source.path
        assert path
        removed.append((source.module, path))

    # Always add modules that were (re-)added, since they may be detected as not changed by
    # fswatcher (if they were actually not changed), but they may still need to be checked
    # in case they had errors before they were deleted from sources on previous runs.
    previous_modules = {source.module for source in self.previous_sources}
    changed_set = set(changed)
    changed.extend(
        [
            (source.module, source.path)
            for source in sources
            if source.path
            and source.module not in previous_modules
            and (source.module, source.path) not in changed_set
        ]
    )

    # Find anything that has had its module path change because of added or removed __init__s
    last = {s.path: s.module for s in self.previous_sources}
    for s in sources:
        assert s.path
        if s.path in last and last[s.path] != s.module:
            # Mark it as removed from its old name and changed at its new name
            removed.append((last[s.path], s.path))
            changed.append((s.module, s.path))

    return changed, removed

</t>
<t tx="ekr.20230831011819.943">def cmd_inspect(
    self,
    show: str,
    location: str,
    verbosity: int = 0,
    limit: int = 0,
    include_span: bool = False,
    include_kind: bool = False,
    include_object_attrs: bool = False,
    union_attrs: bool = False,
    force_reload: bool = False,
) -&gt; dict[str, object]:
    """Locate and inspect expression(s)."""
    if not self.fine_grained_manager:
        return {
            "error": 'Command "inspect" is only valid after a "check" command'
            " (that produces no parse errors)"
        }
    engine = InspectionEngine(
        self.fine_grained_manager,
        verbosity=verbosity,
        limit=limit,
        include_span=include_span,
        include_kind=include_kind,
        include_object_attrs=include_object_attrs,
        union_attrs=union_attrs,
        force_reload=force_reload,
    )
    old_inspections = self.options.inspections
    self.options.inspections = True
    try:
        if show == "type":
            result = engine.get_type(location)
        elif show == "attrs":
            result = engine.get_attrs(location)
        elif show == "definition":
            result = engine.get_definition(location)
        else:
            assert False, "Unknown inspection kind"
    finally:
        self.options.inspections = old_inspections
    if "out" in result:
        assert isinstance(result["out"], str)
        result["out"] += "\n"
    return result

</t>
<t tx="ekr.20230831011819.944">def cmd_suggest(self, function: str, callsites: bool, **kwargs: Any) -&gt; dict[str, object]:
    """Suggest a signature for a function."""
    if not self.fine_grained_manager:
        return {
            "error": "Command 'suggest' is only valid after a 'check' command"
            " (that produces no parse errors)"
        }
    engine = SuggestionEngine(self.fine_grained_manager, **kwargs)
    try:
        if callsites:
            out = engine.suggest_callsites(function)
        else:
            out = engine.suggest(function)
    except SuggestionFailure as err:
        return {"error": str(err)}
    else:
        if not out:
            out = "No suggestions\n"
        elif not out.endswith("\n"):
            out += "\n"
        return {"out": out, "err": "", "status": 0}
    finally:
        self.flush_caches()

</t>
<t tx="ekr.20230831011819.945">def cmd_hang(self) -&gt; dict[str, object]:
    """Hang for 100 seconds, as a debug hack."""
    time.sleep(100)
    return {}


</t>
<t tx="ekr.20230831011819.946"># Misc utilities.


MiB: Final = 2**20


def get_meminfo() -&gt; dict[str, Any]:
    res: dict[str, Any] = {}
    try:
        import psutil
    except ImportError:
        res["memory_psutil_missing"] = (
            "psutil not found, run pip install mypy[dmypy] "
            "to install the needed components for dmypy"
        )
    else:
        process = psutil.Process()
        meminfo = process.memory_info()
        res["memory_rss_mib"] = meminfo.rss / MiB
        res["memory_vms_mib"] = meminfo.vms / MiB
        if sys.platform == "win32":
            res["memory_maxrss_mib"] = meminfo.peak_wset / MiB
        else:
            # See https://stackoverflow.com/questions/938733/total-memory-used-by-python-process
            import resource  # Since it doesn't exist on Windows.

            rusage = resource.getrusage(resource.RUSAGE_SELF)
            if sys.platform == "darwin":
                factor = 1
            else:
                factor = 1024  # Linux
            res["memory_maxrss_mib"] = rusage.ru_maxrss * factor / MiB
    return res


</t>
<t tx="ekr.20230831011819.947">def find_all_sources_in_build(
    graph: mypy.build.Graph, extra: Sequence[BuildSource] = ()
</t>
<t tx="ekr.20230831011819.948">) -&gt; list[BuildSource]:
    result = list(extra)
    seen = {source.module for source in result}
    for module, state in graph.items():
        if module not in seen:
            result.append(BuildSource(state.path, module))
    return result


def fix_module_deps(graph: mypy.build.Graph) -&gt; None:
    """After an incremental update, update module dependencies to reflect the new state.

    This can make some suppressed dependencies non-suppressed, and vice versa (if modules
    have been added to or removed from the build).
    """
    for module, state in graph.items():
        new_suppressed = []
        new_dependencies = []
        for dep in state.dependencies + state.suppressed:
            if dep in graph:
                new_dependencies.append(dep)
            else:
                new_suppressed.append(dep)
        state.dependencies = new_dependencies
        state.dependencies_set = set(new_dependencies)
        state.suppressed = new_suppressed
        state.suppressed_set = set(new_suppressed)


</t>
<t tx="ekr.20230831011819.949">def filter_out_missing_top_level_packages(
    packages: set[str], search_paths: SearchPaths, fscache: FileSystemCache
) -&gt; set[str]:
    """Quickly filter out obviously missing top-level packages.

    Return packages with entries that can't be found removed.

    This is approximate: some packages that aren't actually valid may be
    included. However, all potentially valid packages must be returned.
    """
    # Start with a empty set and add all potential top-level packages.
    found = set()
    paths = (
        search_paths.python_path
        + search_paths.mypy_path
        + search_paths.package_path
        + search_paths.typeshed_path
    )
    for p in paths:
        try:
            entries = fscache.listdir(p)
        except Exception:
            entries = []
        for entry in entries:
            # The code is hand-optimized for mypyc since this may be somewhat
            # performance-critical.
            if entry.endswith(".py"):
                entry = entry[:-3]
            elif entry.endswith(".pyi"):
                entry = entry[:-4]
            elif entry.endswith("-stubs"):
                # Possible PEP 561 stub package
                entry = entry[:-6]
            if entry in packages:
                found.add(entry)
    return found
</t>
<t tx="ekr.20230831011819.95">def generate_deps_for_cache(manager: BuildManager, graph: Graph) -&gt; dict[str, dict[str, set[str]]]:
    """Generate fine-grained dependencies into a form suitable for serializing.

    This does a couple things:
    1. Splits fine-grained deps based on the module of the trigger
    2. For each module we generated fine-grained deps for, load any previous
       deps and merge them in.

    Returns a dictionary from module ids to all dependencies on that
    module. Dependencies not associated with a module in the build will be
    associated with the nearest parent module that is in the build, or the
    fake module FAKE_ROOT_MODULE if none are.
    """
    from mypy.server.deps import merge_dependencies  # Lazy import to speed up startup

    # Split the dependencies out into based on the module that is depended on.
    rdeps = invert_deps(manager.fg_deps, graph)

    # We can't just clobber existing dependency information, so we
    # load the deps for every module we've generated new dependencies
    # to and merge the new deps into them.
    for module, mdeps in rdeps.items():
        old_deps = manager.load_fine_grained_deps(module)
        merge_dependencies(old_deps, mdeps)

    return rdeps


</t>
<t tx="ekr.20230831011819.950">@path mypy
"""Shared code between dmypy.py and dmypy_server.py.

This should be pretty lightweight and not depend on other mypy code (other than ipc).
"""

&lt;&lt; dmypy_util.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.952">from __future__ import annotations

import json
from typing import Any, Final

from mypy.ipc import IPCBase

DEFAULT_STATUS_FILE: Final = ".dmypy.json"


</t>
<t tx="ekr.20230831011819.953">def receive(connection: IPCBase) -&gt; Any:
    """Receive JSON data from a connection until EOF.

    Raise OSError if the data received is not valid JSON or if it is
    not a dict.
    """
    bdata = connection.read()
    if not bdata:
        raise OSError("No data received")
    try:
        data = json.loads(bdata.decode("utf8"))
    except Exception as e:
        raise OSError("Data received is not valid JSON") from e
    if not isinstance(data, dict):
        raise OSError(f"Data received is not a dict ({type(data)})")
    return data
</t>
<t tx="ekr.20230831011819.954">@path mypy
&lt;&lt; erasetype.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.955">from __future__ import annotations

from typing import Callable, Container, cast

from mypy.nodes import ARG_STAR, ARG_STAR2
from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeTranslator,
    TypeType,
    TypeVarId,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)


</t>
<t tx="ekr.20230831011819.956">def erase_type(typ: Type) -&gt; ProperType:
    """Erase any type variables from a type.

    Also replace tuple types with the corresponding concrete types.

    Examples:
      A -&gt; A
      B[X] -&gt; B[Any]
      Tuple[A, B] -&gt; tuple
      Callable[[A1, A2, ...], R] -&gt; Callable[..., Any]
      Type[X] -&gt; Type[Any]
    """
    typ = get_proper_type(typ)
    return typ.accept(EraseTypeVisitor())


</t>
<t tx="ekr.20230831011819.957">class EraseTypeVisitor(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20230831011819.958">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    # TODO: replace with an assert after UnboundType can't leak from semantic analysis.
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011819.959">def visit_any(self, t: AnyType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011819.96">PLUGIN_SNAPSHOT_FILE: Final = "@plugins_snapshot.json"


def write_plugins_snapshot(manager: BuildManager) -&gt; None:
    """Write snapshot of versions and hashes of currently active plugins."""
    snapshot = json.dumps(manager.plugins_snapshot, separators=(",", ":"))
    if not manager.metastore.write(PLUGIN_SNAPSHOT_FILE, snapshot):
        manager.errors.set_file(_cache_dir_prefix(manager.options), None, manager.options)
        manager.errors.report(0, 0, "Error writing plugins snapshot", blocker=True)


</t>
<t tx="ekr.20230831011819.960">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011819.961">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011819.962">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011819.963">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # Should not get here.
    raise RuntimeError("Cannot erase partial types")

</t>
<t tx="ekr.20230831011819.964">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011819.965">def visit_instance(self, t: Instance) -&gt; ProperType:
    return Instance(t.type, [AnyType(TypeOfAny.special_form)] * len(t.args), t.line)

</t>
<t tx="ekr.20230831011819.966">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.967">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.968">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    raise RuntimeError("Parameters should have been bound to a class")

</t>
<t tx="ekr.20230831011819.969">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.97">def read_plugins_snapshot(manager: BuildManager) -&gt; dict[str, str] | None:
    """Read cached snapshot of versions and hashes of plugins from previous run."""
    snapshot = _load_json_file(
        PLUGIN_SNAPSHOT_FILE,
        manager,
        log_success="Plugins snapshot ",
        log_error="Could not load plugins snapshot: ",
    )
    if snapshot is None:
        return None
    if not isinstance(snapshot, dict):
        manager.log(f"Could not load plugins snapshot: cache is not a dict: {type(snapshot)}")
        return None
    return snapshot


</t>
<t tx="ekr.20230831011819.970">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011819.971">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    # We must preserve the fallback type for overload resolution to work.
    any_type = AnyType(TypeOfAny.special_form)
    return CallableType(
        arg_types=[any_type, any_type],
        arg_kinds=[ARG_STAR, ARG_STAR2],
        arg_names=[None, None],
        ret_type=any_type,
        fallback=t.fallback,
        is_ellipsis_args=True,
        implicit=True,
    )

</t>
<t tx="ekr.20230831011819.972">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20230831011819.973">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    return t.partial_fallback.accept(self)

</t>
<t tx="ekr.20230831011819.974">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20230831011819.975">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    # The fallback for literal types should always be either
    # something like int or str, or an enum class -- types that
    # don't contain any TypeVars. So there's no need to visit it.
    return t

</t>
<t tx="ekr.20230831011819.976">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    erased_items = [erase_type(item) for item in t.items]
    from mypy.typeops import make_simplified_union

    return make_simplified_union(erased_items)

</t>
<t tx="ekr.20230831011819.977">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    return TypeType.make_normalized(t.item.accept(self), line=t.line)

</t>
<t tx="ekr.20230831011819.978">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    raise RuntimeError("Type aliases should be expanded before accepting this visitor")


</t>
<t tx="ekr.20230831011819.979">def erase_typevars(t: Type, ids_to_erase: Container[TypeVarId] | None = None) -&gt; Type:
    """Replace all type variables in a type with any,
    or just the ones in the provided collection.
    """

    def erase_id(id: TypeVarId) -&gt; bool:
        if ids_to_erase is None:
            return True
        return id in ids_to_erase

    return t.accept(TypeVarEraser(erase_id, AnyType(TypeOfAny.special_form)))


</t>
<t tx="ekr.20230831011819.98">def read_quickstart_file(
    options: Options, stdout: TextIO
) -&gt; dict[str, tuple[float, int, str]] | None:
    quickstart: dict[str, tuple[float, int, str]] | None = None
    if options.quickstart_file:
        # This is very "best effort". If the file is missing or malformed,
        # just ignore it.
        raw_quickstart: dict[str, Any] = {}
        try:
            with open(options.quickstart_file) as f:
                raw_quickstart = json.load(f)

            quickstart = {}
            for file, (x, y, z) in raw_quickstart.items():
                quickstart[file] = (x, y, z)
        except Exception as e:
            print(f"Warning: Failed to load quickstart file: {str(e)}\n", file=stdout)
    return quickstart


</t>
<t tx="ekr.20230831011819.980">def replace_meta_vars(t: Type, target_type: Type) -&gt; Type:
    """Replace unification variables in a type with the target type."""
    return t.accept(TypeVarEraser(lambda id: id.is_meta_var(), target_type))


</t>
<t tx="ekr.20230831011819.981">class TypeVarEraser(TypeTranslator):
    """Implementation of type erasure"""

    @others
</t>
<t tx="ekr.20230831011819.982">def __init__(self, erase_id: Callable[[TypeVarId], bool], replacement: Type) -&gt; None:
    self.erase_id = erase_id
    self.replacement = replacement

</t>
<t tx="ekr.20230831011819.983">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20230831011819.984">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20230831011819.985">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20230831011819.986">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Type alias target can't contain bound type variables (not bound by the type
    # alias itself), so it is safe to just erase the arguments.
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20230831011819.987">def remove_instance_last_known_values(t: Type) -&gt; Type:
    return t.accept(LastKnownValueEraser())


</t>
<t tx="ekr.20230831011819.988">class LastKnownValueEraser(TypeTranslator):
    """Removes the Literal[...] type that may be associated with any
    Instance types."""

    @others
</t>
<t tx="ekr.20230831011819.989">def visit_instance(self, t: Instance) -&gt; Type:
    if not t.last_known_value and not t.args:
        return t
    return t.copy_modified(args=[a.accept(self) for a in t.args], last_known_value=None)

</t>
<t tx="ekr.20230831011819.99">def read_deps_cache(manager: BuildManager, graph: Graph) -&gt; dict[str, FgDepMeta] | None:
    """Read and validate the fine-grained dependencies cache.

    See the write_deps_cache documentation for more information on
    the details of the cache.

    Returns None if the cache was invalid in some way.
    """
    deps_meta = _load_json_file(
        DEPS_META_FILE,
        manager,
        log_success="Deps meta ",
        log_error="Could not load fine-grained dependency metadata: ",
    )
    if deps_meta is None:
        return None
    meta_snapshot = deps_meta["snapshot"]
    # Take a snapshot of the source hashes from all of the metas we found.
    # (Including the ones we rejected because they were out of date.)
    # We use this to verify that they match up with the proto_deps.
    current_meta_snapshot = {
        id: st.meta_source_hash for id, st in graph.items() if st.meta_source_hash is not None
    }

    common = set(meta_snapshot.keys()) &amp; set(current_meta_snapshot.keys())
    if any(meta_snapshot[id] != current_meta_snapshot[id] for id in common):
        # TODO: invalidate also if options changed (like --strict-optional)?
        manager.log("Fine-grained dependencies cache inconsistent, ignoring")
        return None

    module_deps_metas = deps_meta["deps_meta"]
    assert isinstance(module_deps_metas, dict)
    if not manager.options.skip_cache_mtime_checks:
        for id, meta in module_deps_metas.items():
            try:
                matched = manager.getmtime(meta["path"]) == meta["mtime"]
            except FileNotFoundError:
                matched = False
            if not matched:
                manager.log(f"Invalid or missing fine-grained deps cache: {meta['path']}")
                return None

    return module_deps_metas


</t>
<t tx="ekr.20230831011819.990">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Type aliases can't contain literal values, because they are
    # always constructed as explicit types.
    return t

</t>
<t tx="ekr.20230831011819.991">def visit_union_type(self, t: UnionType) -&gt; Type:
    new = cast(UnionType, super().visit_union_type(t))
    # Erasure can result in many duplicate items; merge them.
    # Call make_simplified_union only on lists of instance types
    # that all have the same fullname, to avoid simplifying too
    # much.
    instances = [item for item in new.items if isinstance(get_proper_type(item), Instance)]
    # Avoid merge in simple cases such as optional types.
    if len(instances) &gt; 1:
        instances_by_name: dict[str, list[Instance]] = {}
        p_new_items = get_proper_types(new.items)
        for p_item in p_new_items:
            if isinstance(p_item, Instance) and not p_item.args:
                instances_by_name.setdefault(p_item.type.fullname, []).append(p_item)
        merged: list[Type] = []
        for item in new.items:
            orig_item = item
            item = get_proper_type(item)
            if isinstance(item, Instance) and not item.args:
                types = instances_by_name.get(item.type.fullname)
                if types is not None:
                    if len(types) == 1:
                        merged.append(item)
                    else:
                        from mypy.typeops import make_simplified_union

                        merged.append(make_simplified_union(types))
                        del instances_by_name[item.type.fullname]
            else:
                merged.append(orig_item)
        return UnionType.make_union(merged)
    return new
</t>
<t tx="ekr.20230831011819.992">@path mypy
"""Classification of possible errors mypy can detect.

These can be used for filtering specific errors.
"""

&lt;&lt; errorcodes.py: declarations &gt;&gt;
@others


ATTR_DEFINED: Final = ErrorCode("attr-defined", "Check that attribute exists", "General")
NAME_DEFINED: Final = ErrorCode("name-defined", "Check that name is defined", "General")
CALL_ARG: Final[ErrorCode] = ErrorCode(
    "call-arg", "Check number, names and kinds of arguments in calls", "General"
)
ARG_TYPE: Final = ErrorCode("arg-type", "Check argument types in calls", "General")
CALL_OVERLOAD: Final = ErrorCode(
    "call-overload", "Check that an overload variant matches arguments", "General"
)
VALID_TYPE: Final[ErrorCode] = ErrorCode(
    "valid-type", "Check that type (annotation) is valid", "General"
)
VAR_ANNOTATED: Final = ErrorCode(
    "var-annotated", "Require variable annotation if type can't be inferred", "General"
)
OVERRIDE: Final = ErrorCode(
    "override", "Check that method override is compatible with base class", "General"
)
RETURN: Final[ErrorCode] = ErrorCode(
    "return", "Check that function always returns a value", "General"
)
RETURN_VALUE: Final[ErrorCode] = ErrorCode(
    "return-value", "Check that return value is compatible with signature", "General"
)
ASSIGNMENT: Final[ErrorCode] = ErrorCode(
    "assignment", "Check that assigned value is compatible with target", "General"
)
METHOD_ASSIGN: Final[ErrorCode] = ErrorCode(
    "method-assign",
    "Check that assignment target is not a method",
    "General",
    sub_code_of=ASSIGNMENT,
)
TYPE_ARG: Final = ErrorCode("type-arg", "Check that generic type arguments are present", "General")
TYPE_VAR: Final = ErrorCode("type-var", "Check that type variable values are valid", "General")
UNION_ATTR: Final = ErrorCode(
    "union-attr", "Check that attribute exists in each item of a union", "General"
)
INDEX: Final = ErrorCode("index", "Check indexing operations", "General")
OPERATOR: Final = ErrorCode("operator", "Check that operator is valid for operands", "General")
LIST_ITEM: Final = ErrorCode(
    "list-item", "Check list items in a list expression [item, ...]", "General"
)
DICT_ITEM: Final = ErrorCode(
    "dict-item", "Check dict items in a dict expression {key: value, ...}", "General"
)
TYPEDDICT_ITEM: Final = ErrorCode(
    "typeddict-item", "Check items when constructing TypedDict", "General"
)
TYPEDDICT_UNKNOWN_KEY: Final = ErrorCode(
    "typeddict-unknown-key",
    "Check unknown keys when constructing TypedDict",
    "General",
    sub_code_of=TYPEDDICT_ITEM,
)
HAS_TYPE: Final = ErrorCode(
    "has-type", "Check that type of reference can be determined", "General"
)
IMPORT: Final = ErrorCode(
    "import", "Require that imported module can be found or has stubs", "General"
)
IMPORT_NOT_FOUND: Final = ErrorCode(
    "import-not-found", "Require that imported module can be found", "General", sub_code_of=IMPORT
)
IMPORT_UNTYPED: Final = ErrorCode(
    "import-untyped", "Require that imported module has stubs", "General", sub_code_of=IMPORT
)
NO_REDEF: Final = ErrorCode("no-redef", "Check that each name is defined once", "General")
FUNC_RETURNS_VALUE: Final = ErrorCode(
    "func-returns-value", "Check that called function returns a value in value context", "General"
)
ABSTRACT: Final = ErrorCode(
    "abstract", "Prevent instantiation of classes with abstract attributes", "General"
)
TYPE_ABSTRACT: Final = ErrorCode(
    "type-abstract", "Require only concrete classes where Type[...] is expected", "General"
)
VALID_NEWTYPE: Final = ErrorCode(
    "valid-newtype", "Check that argument 2 to NewType is valid", "General"
)
STRING_FORMATTING: Final = ErrorCode(
    "str-format", "Check that string formatting/interpolation is type-safe", "General"
)
STR_BYTES_PY3: Final = ErrorCode(
    "str-bytes-safe", "Warn about implicit coercions related to bytes and string types", "General"
)
EXIT_RETURN: Final = ErrorCode(
    "exit-return", "Warn about too general return type for '__exit__'", "General"
)
LITERAL_REQ: Final = ErrorCode("literal-required", "Check that value is a literal", "General")
UNUSED_COROUTINE: Final = ErrorCode(
    "unused-coroutine", "Ensure that all coroutines are used", "General"
)
# TODO: why do we need the explicit type here? Without it mypyc CI builds fail with
# mypy/message_registry.py:37: error: Cannot determine type of "EMPTY_BODY"  [has-type]
EMPTY_BODY: Final[ErrorCode] = ErrorCode(
    "empty-body",
    "A dedicated error code to opt out return errors for empty/trivial bodies",
    "General",
)
SAFE_SUPER: Final = ErrorCode(
    "safe-super", "Warn about calls to abstract methods with empty/trivial bodies", "General"
)
TOP_LEVEL_AWAIT: Final = ErrorCode(
    "top-level-await", "Warn about top level await expressions", "General"
)
AWAIT_NOT_ASYNC: Final = ErrorCode(
    "await-not-async", 'Warn about "await" outside coroutine ("async def")', "General"
)
# These error codes aren't enabled by default.
NO_UNTYPED_DEF: Final[ErrorCode] = ErrorCode(
    "no-untyped-def", "Check that every function has an annotation", "General"
)
NO_UNTYPED_CALL: Final = ErrorCode(
    "no-untyped-call",
    "Disallow calling functions without type annotations from annotated functions",
    "General",
)
REDUNDANT_CAST: Final = ErrorCode(
    "redundant-cast", "Check that cast changes type of expression", "General"
)
ASSERT_TYPE: Final = ErrorCode("assert-type", "Check that assert_type() call succeeds", "General")
COMPARISON_OVERLAP: Final = ErrorCode(
    "comparison-overlap", "Check that types in comparisons and 'in' expressions overlap", "General"
)
NO_ANY_UNIMPORTED: Final = ErrorCode(
    "no-any-unimported", 'Reject "Any" types from unfollowed imports', "General"
)
NO_ANY_RETURN: Final = ErrorCode(
    "no-any-return",
    'Reject returning value with "Any" type if return type is not "Any"',
    "General",
)
UNREACHABLE: Final = ErrorCode(
    "unreachable", "Warn about unreachable statements or expressions", "General"
)
ANNOTATION_UNCHECKED = ErrorCode(
    "annotation-unchecked", "Notify about type annotations in unchecked functions", "General"
)
POSSIBLY_UNDEFINED: Final[ErrorCode] = ErrorCode(
    "possibly-undefined",
    "Warn about variables that are defined only in some execution paths",
    "General",
    default_enabled=False,
)
REDUNDANT_EXPR: Final = ErrorCode(
    "redundant-expr", "Warn about redundant expressions", "General", default_enabled=False
)
TRUTHY_BOOL: Final[ErrorCode] = ErrorCode(
    "truthy-bool",
    "Warn about expressions that could always evaluate to true in boolean contexts",
    "General",
    default_enabled=False,
)
TRUTHY_FUNCTION: Final[ErrorCode] = ErrorCode(
    "truthy-function",
    "Warn about function that always evaluate to true in boolean contexts",
    "General",
)
TRUTHY_ITERABLE: Final[ErrorCode] = ErrorCode(
    "truthy-iterable",
    "Warn about Iterable expressions that could always evaluate to true in boolean contexts",
    "General",
    default_enabled=False,
)
NAME_MATCH: Final = ErrorCode(
    "name-match", "Check that type definition has consistent naming", "General"
)
NO_OVERLOAD_IMPL: Final = ErrorCode(
    "no-overload-impl",
    "Check that overloaded functions outside stub files have an implementation",
    "General",
)
IGNORE_WITHOUT_CODE: Final = ErrorCode(
    "ignore-without-code",
    "Warn about '# type: ignore' comments which do not have error codes",
    "General",
    default_enabled=False,
)
UNUSED_AWAITABLE: Final = ErrorCode(
    "unused-awaitable",
    "Ensure that all awaitable values are used",
    "General",
    default_enabled=False,
)
REDUNDANT_SELF_TYPE = ErrorCode(
    "redundant-self",
    "Warn about redundant Self type annotations on method first argument",
    "General",
    default_enabled=False,
)
USED_BEFORE_DEF: Final[ErrorCode] = ErrorCode(
    "used-before-def", "Warn about variables that are used before they are defined", "General"
)
UNUSED_IGNORE: Final = ErrorCode(
    "unused-ignore", "Ensure that all type ignores are used", "General", default_enabled=False
)
EXPLICIT_OVERRIDE_REQUIRED: Final = ErrorCode(
    "explicit-override",
    "Require @override decorator if method is overriding a base class method",
    "General",
    default_enabled=False,
)


# Syntax errors are often blocking.
SYNTAX: Final[ErrorCode] = ErrorCode("syntax", "Report syntax errors", "General")

# This is an internal marker code for a whole-file ignore. It is not intended to
# be user-visible.
FILE: Final = ErrorCode("file", "Internal marker for a whole file being ignored", "General")
del error_codes[FILE.code]

# This is a catch-all for remaining uncategorized errors.
MISC: Final = ErrorCode("misc", "Miscellaneous other checks", "General")
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011819.994">from __future__ import annotations

from collections import defaultdict
from typing import Final

from mypy_extensions import mypyc_attr

error_codes: dict[str, ErrorCode] = {}
sub_code_map: dict[str, set[str]] = defaultdict(set)


@mypyc_attr(allow_interpreted_subclasses=True)
</t>
<t tx="ekr.20230831011819.995">class ErrorCode:
    @others
</t>
<t tx="ekr.20230831011819.996">def __init__(
    self,
    code: str,
    description: str,
    category: str,
    default_enabled: bool = True,
    sub_code_of: ErrorCode | None = None,
) -&gt; None:
    self.code = code
    self.description = description
    self.category = category
    self.default_enabled = default_enabled
    self.sub_code_of = sub_code_of
    if sub_code_of is not None:
        assert sub_code_of.sub_code_of is None, "Nested subcategories are not supported"
        sub_code_map[sub_code_of.code].add(code)
    error_codes[code] = self

</t>
<t tx="ekr.20230831011819.997">def __str__(self) -&gt; str:
    return f"&lt;ErrorCode {self.code}&gt;"

</t>
<t tx="ekr.20230831011819.998">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, ErrorCode):
        return False
    return self.code == other.code

</t>
<t tx="ekr.20230831011819.999">def __hash__(self) -&gt; int:
    return hash((self.code,))
</t>
<t tx="ekr.20230831011820.1">def process_cache_map(
    parser: argparse.ArgumentParser, special_opts: argparse.Namespace, options: Options
) -&gt; None:
    """Validate cache_map and copy into options.cache_map."""
    n = len(special_opts.cache_map)
    if n % 3 != 0:
        parser.error("--cache-map requires one or more triples (see source)")
    for i in range(0, n, 3):
        source, meta_file, data_file = special_opts.cache_map[i : i + 3]
        if source in options.cache_map:
            parser.error(f"Duplicate --cache-map source {source})")
        if not source.endswith(".py") and not source.endswith(".pyi"):
            parser.error(f"Invalid --cache-map source {source} (triple[0] must be *.py[i])")
        if not meta_file.endswith(".meta.json"):
            parser.error(
                "Invalid --cache-map meta_file %s (triple[1] must be *.meta.json)" % meta_file
            )
        if not data_file.endswith(".data.json"):
            parser.error(
                "Invalid --cache-map data_file %s (triple[2] must be *.data.json)" % data_file
            )
        options.cache_map[source] = (meta_file, data_file)


</t>
<t tx="ekr.20230831011820.10">def class_derivation_paths(typ: TypeInfo, supertype: TypeInfo) -&gt; list[list[TypeInfo]]:
    """Return an array of non-empty paths of direct base classes from
    type to supertype.  Return [] if no such path could be found.

      InterfaceImplementationPaths(A, B) == [[B]] if A inherits B
      InterfaceImplementationPaths(A, C) == [[B, C]] if A inherits B and
                                                        B inherits C
    """
    # FIX: Currently we might only ever have a single path, so this could be
    #      simplified
    result: list[list[TypeInfo]] = []

    for base in typ.bases:
        btype = base.type
        if btype == supertype:
            result.append([btype])
        else:
            # Try constructing a longer path via the base class.
            for path in class_derivation_paths(btype, supertype):
                result.append([btype] + path)

    return result


</t>
<t tx="ekr.20230831011820.100">def deleted_as_rvalue(self, typ: DeletedType, context: Context) -&gt; None:
    """Report an error about using an deleted type as an rvalue."""
    if typ.source is None:
        s = ""
    else:
        s = f' "{typ.source}"'
    self.fail(f"Trying to read deleted variable{s}", context)

</t>
<t tx="ekr.20230831011820.1000">class LimitedVariableRenameVisitor(TraverserVisitor):
    """Perform some limited variable renaming in with statements.

    This allows reusing a variable in multiple with statements with
    different types. For example, the two instances of 'x' can have
    incompatible types:

       with C() as x:
           f(x)
       with D() as x:
           g(x)

    The above code gets renamed conceptually into this (not valid Python!):

       with C() as x':
           f(x')
       with D() as x:
           g(x)

    If there's a reference to a variable defined in 'with' outside the
    statement, or if there's any trickiness around variable visibility
    (e.g. function definitions), we give up and won't perform renaming.

    The main use case is to allow binding both readable and writable
    binary files into the same variable. These have different types:

        with open(fnam, 'rb') as f: ...
        with open(fnam, 'wb') as f: ...
    """

    @others
</t>
<t tx="ekr.20230831011820.1001">def __init__(self) -&gt; None:
    # Short names of variables bound in with statements using "as"
    # in a surrounding scope
    self.bound_vars: list[str] = []
    # Stack of names that can't be safely renamed, per scope ('*' means that
    # no names can be renamed)
    self.skipped: list[set[str]] = []
    # References to variables that we may need to rename. Stack of
    # scopes; each scope is a mapping from name to list of collections
    # of names that refer to the same logical variable.
    self.refs: list[dict[str, list[list[NameExpr]]]] = []

</t>
<t tx="ekr.20230831011820.1002">def visit_mypy_file(self, file_node: MypyFile) -&gt; None:
    """Rename variables within a file.

    This is the main entry point to this class.
    """
    with self.enter_scope():
        for d in file_node.defs:
            d.accept(self)

</t>
<t tx="ekr.20230831011820.1003">def visit_func_def(self, fdef: FuncDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope():
        for arg in fdef.arguments:
            self.record_skipped(arg.variable.name)
        super().visit_func_def(fdef)

</t>
<t tx="ekr.20230831011820.1004">def visit_class_def(self, cdef: ClassDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope():
        super().visit_class_def(cdef)

</t>
<t tx="ekr.20230831011820.1005">def visit_with_stmt(self, stmt: WithStmt) -&gt; None:
    for expr in stmt.expr:
        expr.accept(self)
    old_len = len(self.bound_vars)
    for target in stmt.target:
        if target is not None:
            self.analyze_lvalue(target)
    for target in stmt.target:
        if target:
            target.accept(self)
    stmt.body.accept(self)

    while len(self.bound_vars) &gt; old_len:
        self.bound_vars.pop()

</t>
<t tx="ekr.20230831011820.1006">def analyze_lvalue(self, lvalue: Lvalue) -&gt; None:
    if isinstance(lvalue, NameExpr):
        name = lvalue.name
        if name in self.bound_vars:
            # Name bound in a surrounding with statement, so it can be renamed
            self.visit_name_expr(lvalue)
        else:
            var_info = self.refs[-1]
            if name not in var_info:
                var_info[name] = []
            var_info[name].append([])
            self.bound_vars.append(name)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.analyze_lvalue(item)
    elif isinstance(lvalue, MemberExpr):
        lvalue.expr.accept(self)
    elif isinstance(lvalue, IndexExpr):
        lvalue.base.accept(self)
        lvalue.index.accept(self)
    elif isinstance(lvalue, StarExpr):
        self.analyze_lvalue(lvalue.expr)

</t>
<t tx="ekr.20230831011820.1007">def visit_import(self, imp: Import) -&gt; None:
    # We don't support renaming imports
    for id, as_id in imp.ids:
        self.record_skipped(as_id or id)

</t>
<t tx="ekr.20230831011820.1008">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    # We don't support renaming imports
    for id, as_id in imp.names:
        self.record_skipped(as_id or id)

</t>
<t tx="ekr.20230831011820.1009">def visit_import_all(self, imp: ImportAll) -&gt; None:
    # Give up, since we don't know all imported names yet
    self.reject_redefinition_of_vars_in_scope()

</t>
<t tx="ekr.20230831011820.101">def deleted_as_lvalue(self, typ: DeletedType, context: Context) -&gt; None:
    """Report an error about using an deleted type as an lvalue.

    Currently, this only occurs when trying to assign to an
    exception variable outside the local except: blocks.
    """
    if typ.source is None:
        s = ""
    else:
        s = f' "{typ.source}"'
    self.fail(f"Assignment to variable{s} outside except: block", context)

</t>
<t tx="ekr.20230831011820.1010">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    name = expr.name
    if name in self.bound_vars:
        # Record reference so that it can be renamed later
        for scope in reversed(self.refs):
            if name in scope:
                scope[name][-1].append(expr)
    else:
        self.record_skipped(name)

</t>
<t tx="ekr.20230831011820.1011">@contextmanager
def enter_scope(self) -&gt; Iterator[None]:
    self.skipped.append(set())
    self.refs.append({})
    yield None
    self.flush_refs()

</t>
<t tx="ekr.20230831011820.1012">def reject_redefinition_of_vars_in_scope(self) -&gt; None:
    self.record_skipped("*")

</t>
<t tx="ekr.20230831011820.1013">def record_skipped(self, name: str) -&gt; None:
    self.skipped[-1].add(name)

</t>
<t tx="ekr.20230831011820.1014">def flush_refs(self) -&gt; None:
    ref_dict = self.refs.pop()
    skipped = self.skipped.pop()
    if "*" not in skipped:
        for name, refs in ref_dict.items():
            if len(refs) &lt;= 1 or name in skipped:
                continue
            # At module top level we must not rename the final definition,
            # as it may be publicly visible
            to_rename = refs[:-1]
            for i, item in enumerate(to_rename):
                rename_refs(item, i)


</t>
<t tx="ekr.20230831011820.1015">def rename_refs(names: list[NameExpr], index: int) -&gt; None:
    name = names[0].name
    new_name = name + "'" * (index + 1)
    for expr in names:
        expr.name = new_name
</t>
<t tx="ekr.20230831011820.1016">@path mypy
"""Classes for producing HTML reports about imprecision."""
&lt;&lt; report.py: preamble &gt;&gt;
@others


register_reporter("lineprecision", LinePrecisionReporter)


# Reporter class names are defined twice to speed up mypy startup, as this
# module is slow to import. Ensure that the two definitions match.
assert set(reporter_classes) == set(REPORTER_NAMES)
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1017">
from __future__ import annotations

import collections
import itertools
import json
import os
import shutil
import sys
import time
import tokenize
from abc import ABCMeta, abstractmethod
from operator import attrgetter
from typing import Any, Callable, Dict, Final, Iterator, Tuple
from typing_extensions import TypeAlias as _TypeAlias
from urllib.request import pathname2url

from mypy import stats
from mypy.defaults import REPORTER_NAMES
from mypy.nodes import Expression, FuncDef, MypyFile
from mypy.options import Options
from mypy.traverser import TraverserVisitor
from mypy.types import Type, TypeOfAny
from mypy.version import __version__

try:
    from lxml import etree  # type: ignore[import]

    LXML_INSTALLED = True
except ImportError:
    LXML_INSTALLED = False

type_of_any_name_map: Final[collections.OrderedDict[int, str]] = collections.OrderedDict(
    [
        (TypeOfAny.unannotated, "Unannotated"),
        (TypeOfAny.explicit, "Explicit"),
        (TypeOfAny.from_unimported_type, "Unimported"),
        (TypeOfAny.from_omitted_generics, "Omitted Generics"),
        (TypeOfAny.from_error, "Error"),
        (TypeOfAny.special_form, "Special Form"),
        (TypeOfAny.implementation_artifact, "Implementation Artifact"),
    ]
)

ReporterClasses: _TypeAlias = Dict[
    str, Tuple[Callable[["Reports", str], "AbstractReporter"], bool]
]

reporter_classes: Final[ReporterClasses] = {}


</t>
<t tx="ekr.20230831011820.1018">class Reports:
    @others
</t>
<t tx="ekr.20230831011820.1019">def __init__(self, data_dir: str, report_dirs: dict[str, str]) -&gt; None:
    self.data_dir = data_dir
    self.reporters: list[AbstractReporter] = []
    self.named_reporters: dict[str, AbstractReporter] = {}

    for report_type, report_dir in sorted(report_dirs.items()):
        self.add_report(report_type, report_dir)

</t>
<t tx="ekr.20230831011820.102">def no_variant_matches_arguments(
    self,
    overload: Overloaded,
    arg_types: list[Type],
    context: Context,
    *,
    code: ErrorCode | None = None,
) -&gt; None:
    code = code or codes.CALL_OVERLOAD
    name = callable_name(overload)
    if name:
        name_str = f" of {name}"
    else:
        name_str = ""
    arg_types_str = ", ".join(format_type(arg, self.options) for arg in arg_types)
    num_args = len(arg_types)
    if num_args == 0:
        self.fail(
            f"All overload variants{name_str} require at least one argument",
            context,
            code=code,
        )
    elif num_args == 1:
        self.fail(
            f"No overload variant{name_str} matches argument type {arg_types_str}",
            context,
            code=code,
        )
    else:
        self.fail(
            f"No overload variant{name_str} matches argument types {arg_types_str}",
            context,
            code=code,
        )

    self.note(f"Possible overload variant{plural_s(len(overload.items))}:", context, code=code)
    for item in overload.items:
        self.note(pretty_callable(item, self.options), context, offset=4, code=code)

</t>
<t tx="ekr.20230831011820.1020">def add_report(self, report_type: str, report_dir: str) -&gt; AbstractReporter:
    try:
        return self.named_reporters[report_type]
    except KeyError:
        pass
    reporter_cls, needs_lxml = reporter_classes[report_type]
    if needs_lxml and not LXML_INSTALLED:
        print(
            (
                "You must install the lxml package before you can run mypy"
                " with `--{}-report`.\n"
                "You can do this with `python3 -m pip install lxml`."
            ).format(report_type),
            file=sys.stderr,
        )
        raise ImportError
    reporter = reporter_cls(self, report_dir)
    self.reporters.append(reporter)
    self.named_reporters[report_type] = reporter
    return reporter

</t>
<t tx="ekr.20230831011820.1021">def file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    for reporter in self.reporters:
        reporter.on_file(tree, modules, type_map, options)

</t>
<t tx="ekr.20230831011820.1022">def finish(self) -&gt; None:
    for reporter in self.reporters:
        reporter.on_finish()


</t>
<t tx="ekr.20230831011820.1023">class AbstractReporter(metaclass=ABCMeta):
    @others
</t>
<t tx="ekr.20230831011820.1024">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    self.output_dir = output_dir
    if output_dir != "&lt;memory&gt;":
        stats.ensure_dir_exists(output_dir)

</t>
<t tx="ekr.20230831011820.1025">@abstractmethod
def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011820.1026">@abstractmethod
def on_finish(self) -&gt; None:
    pass


</t>
<t tx="ekr.20230831011820.1027">def register_reporter(
    report_name: str,
    reporter: Callable[[Reports, str], AbstractReporter],
    needs_lxml: bool = False,
) -&gt; None:
    reporter_classes[report_name] = (reporter, needs_lxml)


</t>
<t tx="ekr.20230831011820.1028">def alias_reporter(source_reporter: str, target_reporter: str) -&gt; None:
    reporter_classes[target_reporter] = reporter_classes[source_reporter]


</t>
<t tx="ekr.20230831011820.1029">def should_skip_path(path: str) -&gt; bool:
    if stats.is_special_module(path):
        return True
    if path.startswith(".."):
        return True
    if "stubs" in path.split("/") or "stubs" in path.split(os.sep):
        return True
    return False


</t>
<t tx="ekr.20230831011820.103">def wrong_number_values_to_unpack(
    self, provided: int, expected: int, context: Context
) -&gt; None:
    if provided &lt; expected:
        if provided == 1:
            self.fail(f"Need more than 1 value to unpack ({expected} expected)", context)
        else:
            self.fail(
                f"Need more than {provided} values to unpack ({expected} expected)", context
            )
    elif provided &gt; expected:
        self.fail(
            f"Too many values to unpack ({expected} expected, {provided} provided)", context
        )

</t>
<t tx="ekr.20230831011820.1030">def iterate_python_lines(path: str) -&gt; Iterator[tuple[int, str]]:
    """Return an iterator over (line number, line text) from a Python file."""
    try:
        with tokenize.open(path) as input_file:
            yield from enumerate(input_file, 1)
    except IsADirectoryError:
        # can happen with namespace packages
        pass


</t>
<t tx="ekr.20230831011820.1031">class FuncCounterVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011820.1032">def __init__(self) -&gt; None:
    super().__init__()
    self.counts = [0, 0]

</t>
<t tx="ekr.20230831011820.1033">def visit_func_def(self, defn: FuncDef) -&gt; None:
    self.counts[defn.type is not None] += 1


</t>
<t tx="ekr.20230831011820.1034">class LineCountReporter(AbstractReporter):
    @others
</t>
<t tx="ekr.20230831011820.1035">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.counts: dict[str, tuple[int, int, int, int]] = {}

</t>
<t tx="ekr.20230831011820.1036">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    # Count physical lines.  This assumes the file's encoding is a
    # superset of ASCII (or at least uses \n in its line endings).
    with open(tree.path, "rb") as f:
        physical_lines = len(f.readlines())

    func_counter = FuncCounterVisitor()
    tree.accept(func_counter)
    unannotated_funcs, annotated_funcs = func_counter.counts
    total_funcs = annotated_funcs + unannotated_funcs

    # Don't count lines or functions as annotated if they have their errors ignored.
    if options.ignore_errors:
        annotated_funcs = 0

    imputed_annotated_lines = (
        physical_lines * annotated_funcs // total_funcs if total_funcs else physical_lines
    )

    self.counts[tree._fullname] = (
        imputed_annotated_lines,
        physical_lines,
        annotated_funcs,
        total_funcs,
    )

</t>
<t tx="ekr.20230831011820.1037">def on_finish(self) -&gt; None:
    counts: list[tuple[tuple[int, int, int, int], str]] = sorted(
        ((c, p) for p, c in self.counts.items()), reverse=True
    )
    total_counts = tuple(sum(c[i] for c, p in counts) for i in range(4))
    with open(os.path.join(self.output_dir, "linecount.txt"), "w") as f:
        f.write("{:7} {:7} {:6} {:6} total\n".format(*total_counts))
        for c, p in counts:
            f.write(f"{c[0]:7} {c[1]:7} {c[2]:6} {c[3]:6} {p}\n")


</t>
<t tx="ekr.20230831011820.1038">register_reporter("linecount", LineCountReporter)


class AnyExpressionsReporter(AbstractReporter):
    """Report frequencies of different kinds of Any types."""

    @others
</t>
<t tx="ekr.20230831011820.1039">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.counts: dict[str, tuple[int, int]] = {}
    self.any_types_counter: dict[str, collections.Counter[int]] = {}

</t>
<t tx="ekr.20230831011820.104">def unpacking_strings_disallowed(self, context: Context) -&gt; None:
    self.fail("Unpacking a string is disallowed", context)

</t>
<t tx="ekr.20230831011820.1040">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
        visit_untyped_defs=False,
    )
    tree.accept(visitor)
    self.any_types_counter[tree.fullname] = visitor.type_of_any_counter
    num_unanalyzed_lines = list(visitor.line_map.values()).count(stats.TYPE_UNANALYZED)
    # count each line of dead code as one expression of type "Any"
    num_any = visitor.num_any_exprs + num_unanalyzed_lines
    num_total = visitor.num_imprecise_exprs + visitor.num_precise_exprs + num_any
    if num_total &gt; 0:
        self.counts[tree.fullname] = (num_any, num_total)

</t>
<t tx="ekr.20230831011820.1041">def on_finish(self) -&gt; None:
    self._report_any_exprs()
    self._report_types_of_anys()

</t>
<t tx="ekr.20230831011820.1042">def _write_out_report(
    self, filename: str, header: list[str], rows: list[list[str]], footer: list[str]
) -&gt; None:
    row_len = len(header)
    assert all(len(row) == row_len for row in rows + [header, footer])
    min_column_distance = 3  # minimum distance between numbers in two columns
    widths = [-1] * row_len
    for row in rows + [header, footer]:
        for i, value in enumerate(row):
            widths[i] = max(widths[i], len(value))
    for i, w in enumerate(widths):
        # Do not add min_column_distance to the first column.
        if i &gt; 0:
            widths[i] = w + min_column_distance
    with open(os.path.join(self.output_dir, filename), "w") as f:
        header_str = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(header, widths)))
        separator = "-" * len(header_str)
        f.write(header_str + "\n")
        f.write(separator + "\n")
        for row_values in rows:
            r = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(row_values, widths)))
            f.write(r + "\n")
        f.write(separator + "\n")
        footer_str = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(footer, widths)))
        f.write(footer_str + "\n")

</t>
<t tx="ekr.20230831011820.1043">def _report_any_exprs(self) -&gt; None:
    total_any = sum(num_any for num_any, _ in self.counts.values())
    total_expr = sum(total for _, total in self.counts.values())
    total_coverage = 100.0
    if total_expr &gt; 0:
        total_coverage = (float(total_expr - total_any) / float(total_expr)) * 100

    column_names = ["Name", "Anys", "Exprs", "Coverage"]
    rows: list[list[str]] = []
    for filename in sorted(self.counts):
        (num_any, num_total) = self.counts[filename]
        coverage = (float(num_total - num_any) / float(num_total)) * 100
        coverage_str = f"{coverage:.2f}%"
        rows.append([filename, str(num_any), str(num_total), coverage_str])
    rows.sort(key=lambda x: x[0])
    total_row = ["Total", str(total_any), str(total_expr), f"{total_coverage:.2f}%"]
    self._write_out_report("any-exprs.txt", column_names, rows, total_row)

</t>
<t tx="ekr.20230831011820.1044">def _report_types_of_anys(self) -&gt; None:
    total_counter: collections.Counter[int] = collections.Counter()
    for counter in self.any_types_counter.values():
        for any_type, value in counter.items():
            total_counter[any_type] += value
    file_column_name = "Name"
    total_row_name = "Total"
    column_names = [file_column_name] + list(type_of_any_name_map.values())
    rows: list[list[str]] = []
    for filename, counter in self.any_types_counter.items():
        rows.append([filename] + [str(counter[typ]) for typ in type_of_any_name_map])
    rows.sort(key=lambda x: x[0])
    total_row = [total_row_name] + [str(total_counter[typ]) for typ in type_of_any_name_map]
    self._write_out_report("types-of-anys.txt", column_names, rows, total_row)


</t>
<t tx="ekr.20230831011820.1045">register_reporter("any-exprs", AnyExpressionsReporter)


class LineCoverageVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011820.1046">def __init__(self, source: list[str]) -&gt; None:
    self.source = source

    # For each line of source, we maintain a pair of
    #  * the indentation level of the surrounding function
    #    (-1 if not inside a function), and
    #  * whether the surrounding function is typed.
    # Initially, everything is covered at indentation level -1.
    self.lines_covered = [(-1, True) for l in source]

</t>
<t tx="ekr.20230831011820.1047"># The Python AST has position information for the starts of
# elements, but not for their ends. Fortunately the
# indentation-based syntax makes it pretty easy to find where a
# block ends without doing any real parsing.

# TODO: Handle line continuations (explicit and implicit) and
# multi-line string literals. (But at least line continuations
# are normally more indented than their surrounding block anyways,
# by PEP 8.)

def indentation_level(self, line_number: int) -&gt; int | None:
    """Return the indentation of a line of the source (specified by
    zero-indexed line number). Returns None for blank lines or comments."""
    line = self.source[line_number]
    indent = 0
    for char in list(line):
        if char == " ":
            indent += 1
        elif char == "\t":
            indent = 8 * ((indent + 8) // 8)
        elif char == "#":
            # Line is a comment; ignore it
            return None
        elif char == "\n":
            # Line is entirely whitespace; ignore it
            return None
        # TODO line continuation (\)
        else:
            # Found a non-whitespace character
            return indent
    # Line is entirely whitespace, and at end of file
    # with no trailing newline; ignore it
    return None

</t>
<t tx="ekr.20230831011820.1048">def visit_func_def(self, defn: FuncDef) -&gt; None:
    start_line = defn.line - 1
    start_indent = None
    # When a function is decorated, sometimes the start line will point to
    # whitespace or comments between the decorator and the function, so
    # we have to look for the start.
    while start_line &lt; len(self.source):
        start_indent = self.indentation_level(start_line)
        if start_indent is not None:
            break
        start_line += 1
    # If we can't find the function give up and don't annotate anything.
    # Our line numbers are not reliable enough to be asserting on.
    if start_indent is None:
        return

    cur_line = start_line + 1
    end_line = cur_line
    # After this loop, function body will be lines [start_line, end_line)
    while cur_line &lt; len(self.source):
        cur_indent = self.indentation_level(cur_line)
        if cur_indent is None:
            # Consume the line, but don't mark it as belonging to the function yet.
            cur_line += 1
        elif cur_indent &gt; start_indent:
            # A non-blank line that belongs to the function.
            cur_line += 1
            end_line = cur_line
        else:
            # We reached a line outside the function definition.
            break

    is_typed = defn.type is not None
    for line in range(start_line, end_line):
        old_indent, _ = self.lines_covered[line]
        # If there was an old indent level for this line, and the new
        # level isn't increasing the indentation, ignore it.
        # This is to be defensive against funniness in our line numbers,
        # which are not always reliable.
        if old_indent &lt;= start_indent:
            self.lines_covered[line] = (start_indent, is_typed)

    # Visit the body, in case there are nested functions
    super().visit_func_def(defn)


</t>
<t tx="ekr.20230831011820.1049">class LineCoverageReporter(AbstractReporter):
    """Exact line coverage reporter.

    This reporter writes a JSON dictionary with one field 'lines' to
    the file 'coverage.json' in the specified report directory. The
    value of that field is a dictionary which associates to each
    source file's absolute pathname the list of line numbers that
    belong to typed functions in that file.
    """

    @others
</t>
<t tx="ekr.20230831011820.105">def type_not_iterable(self, type: Type, context: Context) -&gt; None:
    self.fail(f"{format_type(type, self.options)} object is not iterable", context)

</t>
<t tx="ekr.20230831011820.1050">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.lines_covered: dict[str, list[int]] = {}

</t>
<t tx="ekr.20230831011820.1051">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    with open(tree.path) as f:
        tree_source = f.readlines()

    coverage_visitor = LineCoverageVisitor(tree_source)
    tree.accept(coverage_visitor)

    covered_lines = []
    for line_number, (_, typed) in enumerate(coverage_visitor.lines_covered):
        if typed:
            covered_lines.append(line_number + 1)

    self.lines_covered[os.path.abspath(tree.path)] = covered_lines

</t>
<t tx="ekr.20230831011820.1052">def on_finish(self) -&gt; None:
    with open(os.path.join(self.output_dir, "coverage.json"), "w") as f:
        json.dump({"lines": self.lines_covered}, f)


</t>
<t tx="ekr.20230831011820.1053">register_reporter("linecoverage", LineCoverageReporter)


class FileInfo:
    @others
</t>
<t tx="ekr.20230831011820.1054">def __init__(self, name: str, module: str) -&gt; None:
    self.name = name
    self.module = module
    self.counts = [0] * len(stats.precision_names)

</t>
<t tx="ekr.20230831011820.1055">def total(self) -&gt; int:
    return sum(self.counts)

</t>
<t tx="ekr.20230831011820.1056">def attrib(self) -&gt; dict[str, str]:
    return {name: str(val) for name, val in sorted(zip(stats.precision_names, self.counts))}


</t>
<t tx="ekr.20230831011820.1057">class MemoryXmlReporter(AbstractReporter):
    """Internal reporter that generates XML in memory.

    This is used by all other XML-based reporters to avoid duplication.
    """

    @others
</t>
<t tx="ekr.20230831011820.1058">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_html_path = os.path.join(reports.data_dir, "xml", "mypy-html.xslt")
    self.xslt_txt_path = os.path.join(reports.data_dir, "xml", "mypy-txt.xslt")
    self.css_html_path = os.path.join(reports.data_dir, "xml", "mypy-html.css")
    xsd_path = os.path.join(reports.data_dir, "xml", "mypy.xsd")
    self.schema = etree.XMLSchema(etree.parse(xsd_path))
    self.last_xml: Any | None = None
    self.files: list[FileInfo] = []

</t>
<t tx="ekr.20230831011820.1059"># XML doesn't like control characters, but they are sometimes
# legal in source code (e.g. comments, string literals).
# Tabs (#x09) are allowed in XML content.
control_fixer: Final = str.maketrans("".join(chr(i) for i in range(32) if i != 9), "?" * 31)

def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    self.last_xml = None

    try:
        path = os.path.relpath(tree.path)
    except ValueError:
        return

    if should_skip_path(path) or os.path.isdir(path):
        return  # `path` can sometimes be a directory, see #11334

    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
    )
    tree.accept(visitor)

    root = etree.Element("mypy-report-file", name=path, module=tree._fullname)
    doc = etree.ElementTree(root)
    file_info = FileInfo(path, tree._fullname)

    for lineno, line_text in iterate_python_lines(path):
        status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
        file_info.counts[status] += 1
        etree.SubElement(
            root,
            "line",
            any_info=self._get_any_info_for_line(visitor, lineno),
            content=line_text.rstrip("\n").translate(self.control_fixer),
            number=str(lineno),
            precision=stats.precision_names[status],
        )
    # Assumes a layout similar to what XmlReporter uses.
    xslt_path = os.path.relpath("mypy-html.xslt", path)
    transform_pi = etree.ProcessingInstruction(
        "xml-stylesheet", f'type="text/xsl" href="{pathname2url(xslt_path)}"'
    )
    root.addprevious(transform_pi)
    self.schema.assertValid(doc)

    self.last_xml = doc
    self.files.append(file_info)

</t>
<t tx="ekr.20230831011820.106">def possible_missing_await(self, context: Context) -&gt; None:
    self.note('Maybe you forgot to use "await"?', context)

</t>
<t tx="ekr.20230831011820.1060">@staticmethod
def _get_any_info_for_line(visitor: stats.StatisticsVisitor, lineno: int) -&gt; str:
    if lineno in visitor.any_line_map:
        result = "Any Types on this line: "
        counter: collections.Counter[int] = collections.Counter()
        for typ in visitor.any_line_map[lineno]:
            counter[typ.type_of_any] += 1
        for any_type, occurrences in counter.items():
            result += f"\n{type_of_any_name_map[any_type]} (x{occurrences})"
        return result
    else:
        return "No Anys on this line!"

</t>
<t tx="ekr.20230831011820.1061">def on_finish(self) -&gt; None:
    self.last_xml = None
    # index_path = os.path.join(self.output_dir, 'index.xml')
    output_files = sorted(self.files, key=lambda x: x.module)

    root = etree.Element("mypy-report-index", name="index")
    doc = etree.ElementTree(root)

    for file_info in output_files:
        etree.SubElement(
            root,
            "file",
            file_info.attrib(),
            module=file_info.module,
            name=pathname2url(file_info.name),
            total=str(file_info.total()),
        )
    xslt_path = os.path.relpath("mypy-html.xslt", ".")
    transform_pi = etree.ProcessingInstruction(
        "xml-stylesheet", f'type="text/xsl" href="{pathname2url(xslt_path)}"'
    )
    root.addprevious(transform_pi)
    self.schema.assertValid(doc)

    self.last_xml = doc


</t>
<t tx="ekr.20230831011820.1062">register_reporter("memory-xml", MemoryXmlReporter, needs_lxml=True)


def get_line_rate(covered_lines: int, total_lines: int) -&gt; str:
    if total_lines == 0:
        return str(1.0)
    else:
        return f"{covered_lines / total_lines:.4f}"


</t>
<t tx="ekr.20230831011820.1063">class CoberturaPackage:
    """Container for XML and statistics mapping python modules to Cobertura package."""

    @others
</t>
<t tx="ekr.20230831011820.1064">def __init__(self, name: str) -&gt; None:
    self.name = name
    self.classes: dict[str, Any] = {}
    self.packages: dict[str, CoberturaPackage] = {}
    self.total_lines = 0
    self.covered_lines = 0

</t>
<t tx="ekr.20230831011820.1065">def as_xml(self) -&gt; Any:
    package_element = etree.Element("package", complexity="1.0", name=self.name)
    package_element.attrib["branch-rate"] = "0"
    package_element.attrib["line-rate"] = get_line_rate(self.covered_lines, self.total_lines)
    classes_element = etree.SubElement(package_element, "classes")
    for class_name in sorted(self.classes):
        classes_element.append(self.classes[class_name])
    self.add_packages(package_element)
    return package_element

</t>
<t tx="ekr.20230831011820.1066">def add_packages(self, parent_element: Any) -&gt; None:
    if self.packages:
        packages_element = etree.SubElement(parent_element, "packages")
        for package in sorted(self.packages.values(), key=attrgetter("name")):
            packages_element.append(package.as_xml())


</t>
<t tx="ekr.20230831011820.1067">class CoberturaXmlReporter(AbstractReporter):
    """Reporter for generating Cobertura compliant XML."""

    @others
</t>
<t tx="ekr.20230831011820.1068">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.root = etree.Element("coverage", timestamp=str(int(time.time())), version=__version__)
    self.doc = etree.ElementTree(self.root)
    self.root_package = CoberturaPackage(".")

</t>
<t tx="ekr.20230831011820.1069">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    path = os.path.relpath(tree.path)
    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
    )
    tree.accept(visitor)

    class_name = os.path.basename(path)
    file_info = FileInfo(path, tree._fullname)
    class_element = etree.Element("class", complexity="1.0", filename=path, name=class_name)
    etree.SubElement(class_element, "methods")
    lines_element = etree.SubElement(class_element, "lines")

    class_lines_covered = 0
    class_total_lines = 0
    for lineno, _ in iterate_python_lines(path):
        status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
        hits = 0
        branch = False
        if status == stats.TYPE_EMPTY:
            continue
        class_total_lines += 1
        if status != stats.TYPE_ANY:
            class_lines_covered += 1
            hits = 1
        if status == stats.TYPE_IMPRECISE:
            branch = True
        file_info.counts[status] += 1
        line_element = etree.SubElement(
            lines_element,
            "line",
            branch=str(branch).lower(),
            hits=str(hits),
            number=str(lineno),
            precision=stats.precision_names[status],
        )
        if branch:
            line_element.attrib["condition-coverage"] = "50% (1/2)"
    class_element.attrib["branch-rate"] = "0"
    class_element.attrib["line-rate"] = get_line_rate(class_lines_covered, class_total_lines)
    # parent_module is set to whichever module contains this file.  For most files, we want
    # to simply strip the last element off of the module.  But for __init__.py files,
    # the module == the parent module.
    parent_module = file_info.module.rsplit(".", 1)[0]
    if file_info.name.endswith("__init__.py"):
        parent_module = file_info.module

    if parent_module not in self.root_package.packages:
        self.root_package.packages[parent_module] = CoberturaPackage(parent_module)
    current_package = self.root_package.packages[parent_module]
    packages_to_update = [self.root_package, current_package]
    for package in packages_to_update:
        package.total_lines += class_total_lines
        package.covered_lines += class_lines_covered
    current_package.classes[class_name] = class_element

</t>
<t tx="ekr.20230831011820.107">def incompatible_operator_assignment(self, op: str, context: Context) -&gt; None:
    self.fail(f"Result type of {op} incompatible in assignment", context)

</t>
<t tx="ekr.20230831011820.1070">def on_finish(self) -&gt; None:
    self.root.attrib["line-rate"] = get_line_rate(
        self.root_package.covered_lines, self.root_package.total_lines
    )
    self.root.attrib["branch-rate"] = "0"
    sources = etree.SubElement(self.root, "sources")
    source_element = etree.SubElement(sources, "source")
    source_element.text = os.getcwd()
    self.root_package.add_packages(self.root)
    out_path = os.path.join(self.output_dir, "cobertura.xml")
    self.doc.write(out_path, encoding="utf-8", pretty_print=True)
    print("Generated Cobertura report:", os.path.abspath(out_path))


</t>
<t tx="ekr.20230831011820.1071">register_reporter("cobertura-xml", CoberturaXmlReporter, needs_lxml=True)


class AbstractXmlReporter(AbstractReporter):
    """Internal abstract class for reporters that work via XML."""

    @others
</t>
<t tx="ekr.20230831011820.1072">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    memory_reporter = reports.add_report("memory-xml", "&lt;memory&gt;")
    assert isinstance(memory_reporter, MemoryXmlReporter)
    # The dependency will be called first.
    self.memory_xml = memory_reporter


</t>
<t tx="ekr.20230831011820.1073">class XmlReporter(AbstractXmlReporter):
    """Public reporter that exports XML.

    The produced XML files contain a reference to the absolute path
    of the html transform, so they will be locally viewable in a browser.

    However, there is a bug in Chrome and all other WebKit-based browsers
    that makes it fail from file:// URLs but work on http:// URLs.
    """

    @others
</t>
<t tx="ekr.20230831011820.1074">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    last_xml = self.memory_xml.last_xml
    if last_xml is None:
        return
    path = os.path.relpath(tree.path)
    if path.startswith(".."):
        return
    out_path = os.path.join(self.output_dir, "xml", path + ".xml")
    stats.ensure_dir_exists(os.path.dirname(out_path))
    last_xml.write(out_path, encoding="utf-8")

</t>
<t tx="ekr.20230831011820.1075">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, "index.xml")
    out_xslt = os.path.join(self.output_dir, "mypy-html.xslt")
    out_css = os.path.join(self.output_dir, "mypy-html.css")
    last_xml.write(out_path, encoding="utf-8")
    shutil.copyfile(self.memory_xml.xslt_html_path, out_xslt)
    shutil.copyfile(self.memory_xml.css_html_path, out_css)
    print("Generated XML report:", os.path.abspath(out_path))


</t>
<t tx="ekr.20230831011820.1076">register_reporter("xml", XmlReporter, needs_lxml=True)


class XsltHtmlReporter(AbstractXmlReporter):
    """Public reporter that exports HTML via XSLT.

    This is slightly different than running `xsltproc` on the .xml files,
    because it passes a parameter to rewrite the links.
    """

    @others
</t>
<t tx="ekr.20230831011820.1077">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_html = etree.XSLT(etree.parse(self.memory_xml.xslt_html_path))
    self.param_html = etree.XSLT.strparam("html")

</t>
<t tx="ekr.20230831011820.1078">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    last_xml = self.memory_xml.last_xml
    if last_xml is None:
        return
    path = os.path.relpath(tree.path)
    if path.startswith(".."):
        return
    out_path = os.path.join(self.output_dir, "html", path + ".html")
    stats.ensure_dir_exists(os.path.dirname(out_path))
    transformed_html = bytes(self.xslt_html(last_xml, ext=self.param_html))
    with open(out_path, "wb") as out_file:
        out_file.write(transformed_html)

</t>
<t tx="ekr.20230831011820.1079">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, "index.html")
    out_css = os.path.join(self.output_dir, "mypy-html.css")
    transformed_html = bytes(self.xslt_html(last_xml, ext=self.param_html))
    with open(out_path, "wb") as out_file:
        out_file.write(transformed_html)
    shutil.copyfile(self.memory_xml.css_html_path, out_css)
    print("Generated HTML report (via XSLT):", os.path.abspath(out_path))


</t>
<t tx="ekr.20230831011820.108">def overload_signature_incompatible_with_supertype(
    self, name: str, name_in_super: str, supertype: str, context: Context
) -&gt; None:
    target = self.override_target(name, name_in_super, supertype)
    self.fail(
        f'Signature of "{name}" incompatible with {target}', context, code=codes.OVERRIDE
    )

    note_template = 'Overload variants must be defined in the same order as they are in "{}"'
    self.note(note_template.format(supertype), context, code=codes.OVERRIDE)

</t>
<t tx="ekr.20230831011820.1080">register_reporter("xslt-html", XsltHtmlReporter, needs_lxml=True)


class XsltTxtReporter(AbstractXmlReporter):
    """Public reporter that exports TXT via XSLT.

    Currently this only does the summary, not the individual reports.
    """

    @others
</t>
<t tx="ekr.20230831011820.1081">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_txt = etree.XSLT(etree.parse(self.memory_xml.xslt_txt_path))

</t>
<t tx="ekr.20230831011820.1082">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011820.1083">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, "index.txt")
    transformed_txt = bytes(self.xslt_txt(last_xml))
    with open(out_path, "wb") as out_file:
        out_file.write(transformed_txt)
    print("Generated TXT report (via XSLT):", os.path.abspath(out_path))


</t>
<t tx="ekr.20230831011820.1084">register_reporter("xslt-txt", XsltTxtReporter, needs_lxml=True)

alias_reporter("xslt-html", "html")
alias_reporter("xslt-txt", "txt")


class LinePrecisionReporter(AbstractReporter):
    """Report per-module line counts for typing precision.

    Each line is classified into one of these categories:

    * precise (fully type checked)
    * imprecise (Any types in a type component, such as List[Any])
    * any (something with an Any type, implicit or explicit)
    * empty (empty line, comment or docstring)
    * unanalyzed (mypy considers line unreachable)

    The meaning of these categories varies slightly depending on
    context.
    """

    @others
</t>
<t tx="ekr.20230831011820.1085">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.files: list[FileInfo] = []

</t>
<t tx="ekr.20230831011820.1086">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    try:
        path = os.path.relpath(tree.path)
    except ValueError:
        return

    if should_skip_path(path):
        return

    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
    )
    tree.accept(visitor)

    file_info = FileInfo(path, tree._fullname)
    for lineno, _ in iterate_python_lines(path):
        status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
        file_info.counts[status] += 1

    self.files.append(file_info)

</t>
<t tx="ekr.20230831011820.1087">def on_finish(self) -&gt; None:
    if not self.files:
        # Nothing to do.
        return
    output_files = sorted(self.files, key=lambda x: x.module)
    report_file = os.path.join(self.output_dir, "lineprecision.txt")
    width = max(4, max(len(info.module) for info in output_files))
    titles = ("Lines", "Precise", "Imprecise", "Any", "Empty", "Unanalyzed")
    widths = (width,) + tuple(len(t) for t in titles)
    fmt = "{:%d}  {:%d}  {:%d}  {:%d}  {:%d}  {:%d}  {:%d}\n" % widths
    with open(report_file, "w") as f:
        f.write(fmt.format("Name", *titles))
        f.write("-" * (width + 51) + "\n")
        for file_info in output_files:
            counts = file_info.counts
            f.write(
                fmt.format(
                    file_info.module.ljust(width),
                    file_info.total(),
                    counts[stats.TYPE_PRECISE],
                    counts[stats.TYPE_IMPRECISE],
                    counts[stats.TYPE_ANY],
                    counts[stats.TYPE_EMPTY],
                    counts[stats.TYPE_UNANALYZED],
                )
            )
</t>
<t tx="ekr.20230831011820.1088">@path mypy
"""Track current scope to easily calculate the corresponding fine-grained target.

TODO: Use everywhere where we track targets, including in mypy.errors.
"""

&lt;&lt; scope.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.109">def signature_incompatible_with_supertype(
    self,
    name: str,
    name_in_super: str,
    supertype: str,
    context: Context,
    *,
    original: ProperType,
    override: ProperType,
) -&gt; None:
    code = codes.OVERRIDE
    target = self.override_target(name, name_in_super, supertype)
    self.fail(f'Signature of "{name}" incompatible with {target}', context, code=code)

    original_str, override_str = format_type_distinctly(
        original, override, options=self.options, bare=True
    )

    INCLUDE_DECORATOR = True  # Include @classmethod and @staticmethod decorators, if any
    ALLOW_DUPS = True  # Allow duplicate notes, needed when signatures are duplicates
    ALIGN_OFFSET = 1  # One space, to account for the difference between error and note
    OFFSET = 4  # Four spaces, so that notes will look like this:
    # error: Signature of "f" incompatible with supertype "A"
    # note:      Superclass:
    # note:          def f(self) -&gt; str
    # note:      Subclass:
    # note:          def f(self, x: str) -&gt; None
    self.note(
        "Superclass:", context, offset=ALIGN_OFFSET + OFFSET, allow_dups=ALLOW_DUPS, code=code
    )
    if isinstance(original, (CallableType, Overloaded)):
        self.pretty_callable_or_overload(
            original,
            context,
            offset=ALIGN_OFFSET + 2 * OFFSET,
            add_class_or_static_decorator=INCLUDE_DECORATOR,
            allow_dups=ALLOW_DUPS,
            code=code,
        )
    else:
        self.note(
            original_str,
            context,
            offset=ALIGN_OFFSET + 2 * OFFSET,
            allow_dups=ALLOW_DUPS,
            code=code,
        )

    self.note(
        "Subclass:", context, offset=ALIGN_OFFSET + OFFSET, allow_dups=ALLOW_DUPS, code=code
    )
    if isinstance(override, (CallableType, Overloaded)):
        self.pretty_callable_or_overload(
            override,
            context,
            offset=ALIGN_OFFSET + 2 * OFFSET,
            add_class_or_static_decorator=INCLUDE_DECORATOR,
            allow_dups=ALLOW_DUPS,
            code=code,
        )
    else:
        self.note(
            override_str,
            context,
            offset=ALIGN_OFFSET + 2 * OFFSET,
            allow_dups=ALLOW_DUPS,
            code=code,
        )

</t>
<t tx="ekr.20230831011820.1090">from __future__ import annotations

from contextlib import contextmanager, nullcontext
from typing import Iterator, Optional, Tuple
from typing_extensions import TypeAlias as _TypeAlias

from mypy.nodes import FuncBase, TypeInfo

SavedScope: _TypeAlias = Tuple[str, Optional[TypeInfo], Optional[FuncBase]]


</t>
<t tx="ekr.20230831011820.1091">class Scope:
    """Track which target we are processing at any given time."""

    @others
</t>
<t tx="ekr.20230831011820.1092">def __init__(self) -&gt; None:
    self.module: str | None = None
    self.classes: list[TypeInfo] = []
    self.function: FuncBase | None = None
    self.functions: list[FuncBase] = []
    # Number of nested scopes ignored (that don't get their own separate targets)
    self.ignored = 0

</t>
<t tx="ekr.20230831011820.1093">def current_module_id(self) -&gt; str:
    assert self.module
    return self.module

</t>
<t tx="ekr.20230831011820.1094">def current_target(self) -&gt; str:
    """Return the current target (non-class; for a class return enclosing module)."""
    assert self.module
    if self.function:
        fullname = self.function.fullname
        return fullname or ""
    return self.module

</t>
<t tx="ekr.20230831011820.1095">def current_full_target(self) -&gt; str:
    """Return the current target (may be a class)."""
    assert self.module
    if self.function:
        return self.function.fullname
    if self.classes:
        return self.classes[-1].fullname
    return self.module

</t>
<t tx="ekr.20230831011820.1096">def current_type_name(self) -&gt; str | None:
    """Return the current type's short name if it exists"""
    return self.classes[-1].name if self.classes else None

</t>
<t tx="ekr.20230831011820.1097">def current_function_name(self) -&gt; str | None:
    """Return the current function's short name if it exists"""
    return self.function.name if self.function else None

</t>
<t tx="ekr.20230831011820.1098">@contextmanager
def module_scope(self, prefix: str) -&gt; Iterator[None]:
    self.module = prefix
    self.classes = []
    self.function = None
    self.ignored = 0
    yield
    assert self.module
    self.module = None

</t>
<t tx="ekr.20230831011820.1099">@contextmanager
def function_scope(self, fdef: FuncBase) -&gt; Iterator[None]:
    self.functions.append(fdef)
    if not self.function:
        self.function = fdef
    else:
        # Nested functions are part of the topmost function target.
        self.ignored += 1
    yield
    self.functions.pop()
    if self.ignored:
        # Leave a scope that's included in the enclosing target.
        self.ignored -= 1
    else:
        assert self.function
        self.function = None

</t>
<t tx="ekr.20230831011820.11">def map_instance_to_direct_supertypes(instance: Instance, supertype: TypeInfo) -&gt; list[Instance]:
    # FIX: There should only be one supertypes, always.
    typ = instance.type
    result: list[Instance] = []

    for b in typ.bases:
        if b.type == supertype:
            env = instance_to_type_environment(instance)
            t = expand_type(b, env)
            assert isinstance(t, Instance)
            result.append(t)

    if result:
        return result
    else:
        # Relationship with the supertype not specified explicitly. Use dynamic
        # type arguments implicitly.
        any_type = AnyType(TypeOfAny.unannotated)
        return [Instance(supertype, [any_type] * len(supertype.type_vars))]


</t>
<t tx="ekr.20230831011820.110">def pretty_callable_or_overload(
    self,
    tp: CallableType | Overloaded,
    context: Context,
    *,
    offset: int = 0,
    add_class_or_static_decorator: bool = False,
    allow_dups: bool = False,
    code: ErrorCode | None = None,
) -&gt; None:
    if isinstance(tp, CallableType):
        if add_class_or_static_decorator:
            decorator = pretty_class_or_static_decorator(tp)
            if decorator is not None:
                self.note(decorator, context, offset=offset, allow_dups=allow_dups, code=code)
        self.note(
            pretty_callable(tp, self.options),
            context,
            offset=offset,
            allow_dups=allow_dups,
            code=code,
        )
    elif isinstance(tp, Overloaded):
        self.pretty_overload(
            tp,
            context,
            offset,
            add_class_or_static_decorator=add_class_or_static_decorator,
            allow_dups=allow_dups,
            code=code,
        )

</t>
<t tx="ekr.20230831011820.1100">def outer_functions(self) -&gt; list[FuncBase]:
    return self.functions[:-1]

</t>
<t tx="ekr.20230831011820.1101">def enter_class(self, info: TypeInfo) -&gt; None:
    """Enter a class target scope."""
    if not self.function:
        self.classes.append(info)
    else:
        # Classes within functions are part of the enclosing function target.
        self.ignored += 1

</t>
<t tx="ekr.20230831011820.1102">def leave_class(self) -&gt; None:
    """Leave a class target scope."""
    if self.ignored:
        # Leave a scope that's included in the enclosing target.
        self.ignored -= 1
    else:
        assert self.classes
        # Leave the innermost class.
        self.classes.pop()

</t>
<t tx="ekr.20230831011820.1103">@contextmanager
def class_scope(self, info: TypeInfo) -&gt; Iterator[None]:
    self.enter_class(info)
    yield
    self.leave_class()

</t>
<t tx="ekr.20230831011820.1104">def save(self) -&gt; SavedScope:
    """Produce a saved scope that can be entered with saved_scope()"""
    assert self.module
    # We only save the innermost class, which is sufficient since
    # the rest are only needed for when classes are left.
    cls = self.classes[-1] if self.classes else None
    return self.module, cls, self.function

</t>
<t tx="ekr.20230831011820.1105">@contextmanager
def saved_scope(self, saved: SavedScope) -&gt; Iterator[None]:
    module, info, function = saved
    with self.module_scope(module):
        with self.class_scope(info) if info else nullcontext():
            with self.function_scope(function) if function else nullcontext():
                yield
</t>
<t tx="ekr.20230831011820.1106">@path mypy
&lt;&lt; semanal.py: docstring &gt;&gt;

&lt;&lt; semanal.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1108">from __future__ import annotations

from contextlib import contextmanager
from typing import Any, Callable, Collection, Final, Iterable, Iterator, List, TypeVar, cast
from typing_extensions import TypeAlias as _TypeAlias

from mypy import errorcodes as codes, message_registry
from mypy.constant_fold import constant_fold_expr
from mypy.errorcodes import ErrorCode
from mypy.errors import Errors, report_internal_error
from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.messages import (
    SUGGESTED_TEST_FIXTURES,
    TYPES_FOR_UNIMPORTED_HINTS,
    MessageBuilder,
    best_matches,
    pretty_seq,
)
from mypy.mro import MroError, calculate_mro
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    CONTRAVARIANT,
    COVARIANT,
    GDEF,
    IMPLICITLY_ABSTRACT,
    INVARIANT,
    IS_ABSTRACT,
    LDEF,
    MDEF,
    NOT_ABSTRACT,
    REVEAL_LOCALS,
    REVEAL_TYPE,
    RUNTIME_PROTOCOL_DECOS,
    ArgKind,
    AssertStmt,
    AssertTypeExpr,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    CallExpr,
    CastExpr,
    ClassDef,
    ComparisonExpr,
    ConditionalExpr,
    Context,
    ContinueStmt,
    DataclassTransformSpec,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    ExpressionStmt,
    FakeExpression,
    ForStmt,
    FuncBase,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportBase,
    ImportFrom,
    IndexExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    Lvalue,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    OverloadPart,
    ParamSpecExpr,
    PassStmt,
    PlaceholderNode,
    PromoteExpr,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    Statement,
    StrExpr,
    SuperExpr,
    SymbolNode,
    SymbolTable,
    SymbolTableNode,
    TempNode,
    TryStmt,
    TupleExpr,
    TypeAlias,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeInfo,
    TypeVarExpr,
    TypeVarLikeExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
    get_member_expr_fullname,
    get_nongen_builtins,
    implicit_module_attrs,
    is_final_node,
    type_aliases,
    type_aliases_source_versions,
    typing_extensions_aliases,
)
from mypy.options import TYPE_VAR_TUPLE, Options
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    SequencePattern,
    StarredPattern,
    ValuePattern,
)
from mypy.plugin import (
    ClassDefContext,
    DynamicClassDefContext,
    Plugin,
    SemanticAnalyzerPluginInterface,
)
from mypy.plugins import dataclasses as dataclasses_plugin
from mypy.reachability import (
    ALWAYS_FALSE,
    ALWAYS_TRUE,
    MYPY_FALSE,
    MYPY_TRUE,
    infer_condition_value,
    infer_reachability_of_if_statement,
    infer_reachability_of_match_statement,
)
from mypy.scope import Scope
from mypy.semanal_enum import EnumCallAnalyzer
from mypy.semanal_namedtuple import NamedTupleAnalyzer
from mypy.semanal_newtype import NewTypeAnalyzer
from mypy.semanal_shared import (
    ALLOW_INCOMPATIBLE_OVERRIDE,
    PRIORITY_FALLBACKS,
    SemanticAnalyzerInterface,
    calculate_tuple_fallback,
    find_dataclass_transform_spec,
    has_placeholder,
    parse_bool,
    require_bool_literal_argument,
    set_callable_name as set_callable_name,
)
from mypy.semanal_typeddict import TypedDictAnalyzer
from mypy.tvar_scope import TypeVarLikeScope
from mypy.typeanal import (
    SELF_TYPE_NAMES,
    TypeAnalyser,
    TypeVarLikeList,
    TypeVarLikeQuery,
    analyze_type_alias,
    check_for_explicit_any,
    detect_diverging_alias,
    find_self_type,
    fix_instance_types,
    has_any_from_unimported_type,
    no_subscript_builtin_alias,
    type_constructors,
)
from mypy.typeops import function_type, get_type_vars, try_getting_str_literals_from_type
from mypy.types import (
    ASSERT_TYPE_NAMES,
    DATACLASS_TRANSFORM_NAMES,
    FINAL_DECORATOR_NAMES,
    FINAL_TYPE_NAMES,
    NEVER_NAMES,
    OVERLOAD_NAMES,
    OVERRIDE_DECORATOR_NAMES,
    PROTOCOL_NAMES,
    REVEAL_TYPE_NAMES,
    TPDICT_NAMES,
    TYPE_ALIAS_NAMES,
    TYPED_NAMEDTUPLE_NAMES,
    AnyType,
    CallableType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PlaceholderType,
    ProperType,
    TrivialSyntheticTypeTranslator,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UnpackType,
    get_proper_type,
    get_proper_types,
    is_named_instance,
    remove_dups,
)
from mypy.types_utils import is_invalid_recursive_alias, store_argument_type
from mypy.typevars import fill_typevars
from mypy.util import (
    correct_relative_import,
    is_dunder,
    is_typeshed_file,
    module_prefix,
    unmangle,
    unnamed_function,
)
from mypy.visitor import NodeVisitor

T = TypeVar("T")


FUTURE_IMPORTS: Final = {
    "__future__.nested_scopes": "nested_scopes",
    "__future__.generators": "generators",
    "__future__.division": "division",
    "__future__.absolute_import": "absolute_import",
    "__future__.with_statement": "with_statement",
    "__future__.print_function": "print_function",
    "__future__.unicode_literals": "unicode_literals",
    "__future__.barry_as_FLUFL": "barry_as_FLUFL",
    "__future__.generator_stop": "generator_stop",
    "__future__.annotations": "annotations",
}


# Special cased built-in classes that are needed for basic functionality and need to be
# available very early on.
CORE_BUILTIN_CLASSES: Final = ["object", "bool", "function"]


# Used for tracking incomplete references
Tag: _TypeAlias = int


</t>
<t tx="ekr.20230831011820.1109">class SemanticAnalyzer(
    NodeVisitor[None], SemanticAnalyzerInterface, SemanticAnalyzerPluginInterface
</t>
<t tx="ekr.20230831011820.111">def argument_incompatible_with_supertype(
    self,
    arg_num: int,
    name: str,
    type_name: str | None,
    name_in_supertype: str,
    arg_type_in_supertype: Type,
    supertype: str,
    context: Context,
    secondary_context: Context,
) -&gt; None:
    target = self.override_target(name, name_in_supertype, supertype)
    arg_type_in_supertype_f = format_type_bare(arg_type_in_supertype, self.options)
    self.fail(
        'Argument {} of "{}" is incompatible with {}; '
        'supertype defines the argument type as "{}"'.format(
            arg_num, name, target, arg_type_in_supertype_f
        ),
        context,
        code=codes.OVERRIDE,
        secondary_context=secondary_context,
    )
    if name != "__post_init__":
        # `__post_init__` is special, it can be incompatible by design.
        # So, this note is misleading.
        self.note(
            "This violates the Liskov substitution principle",
            context,
            code=codes.OVERRIDE,
            secondary_context=secondary_context,
        )
        self.note(
            "See https://mypy.readthedocs.io/en/stable/common_issues.html#incompatible-overrides",
            context,
            code=codes.OVERRIDE,
            secondary_context=secondary_context,
        )

    if name == "__eq__" and type_name:
        multiline_msg = self.comparison_method_example_msg(class_name=type_name)
        self.note_multiline(
            multiline_msg, context, code=codes.OVERRIDE, secondary_context=secondary_context
        )

</t>
<t tx="ekr.20230831011820.1110">):
    """Semantically analyze parsed mypy files.

    The analyzer binds names and does various consistency checks for an
    AST. Note that type checking is performed as a separate pass.
    """

    __deletable__ = ["patches", "options", "cur_mod_node"]

    # Module name space
    modules: dict[str, MypyFile]
    # Global name space for current module
    globals: SymbolTable
    # Names declared using "global" (separate set for each scope)
    global_decls: list[set[str]]
    # Names declared using "nonlocal" (separate set for each scope)
    nonlocal_decls: list[set[str]]
    # Local names of function scopes; None for non-function scopes.
    locals: list[SymbolTable | None]
    # Whether each scope is a comprehension scope.
    is_comprehension_stack: list[bool]
    # Nested block depths of scopes
    block_depth: list[int]
    # TypeInfo of directly enclosing class (or None)
    _type: TypeInfo | None = None
    # Stack of outer classes (the second tuple item contains tvars).
    type_stack: list[TypeInfo | None]
    # Type variables bound by the current scope, be it class or function
    tvar_scope: TypeVarLikeScope
    # Per-module options
    options: Options

    # Stack of functions being analyzed
    function_stack: list[FuncItem]

    # Set to True if semantic analysis defines a name, or replaces a
    # placeholder definition. If some iteration makes no progress,
    # there can be at most one additional final iteration (see below).
    progress = False
    deferred = False  # Set to true if another analysis pass is needed
    incomplete = False  # Set to true if current module namespace is missing things
    # Is this the final iteration of semantic analysis (where we report
    # unbound names due to cyclic definitions and should not defer)?
    _final_iteration = False
    # These names couldn't be added to the symbol table due to incomplete deps.
    # Note that missing names are per module, _not_ per namespace. This means that e.g.
    # a missing name at global scope will block adding same name at a class scope.
    # This should not affect correctness and is purely a performance issue,
    # since it can cause unnecessary deferrals. These are represented as
    # PlaceholderNodes in the symbol table. We use this to ensure that the first
    # definition takes precedence even if it's incomplete.
    #
    # Note that a star import adds a special name '*' to the set, this blocks
    # adding _any_ names in the current file.
    missing_names: list[set[str]]
    # Callbacks that will be called after semantic analysis to tweak things.
    patches: list[tuple[int, Callable[[], None]]]
    loop_depth: list[int]  # Depth of breakable loops
    cur_mod_id = ""  # Current module id (or None) (phase 2)
    _is_stub_file = False  # Are we analyzing a stub file?
    _is_typeshed_stub_file = False  # Are we analyzing a typeshed stub file?
    imports: set[str]  # Imported modules (during phase 2 analysis)
    # Note: some imports (and therefore dependencies) might
    # not be found in phase 1, for example due to * imports.
    errors: Errors  # Keeps track of generated errors
    plugin: Plugin  # Mypy plugin for special casing of library features
    statement: Statement | None = None  # Statement/definition being analyzed

    # Mapping from 'async def' function definitions to their return type wrapped as a
    # 'Coroutine[Any, Any, T]'. Used to keep track of whether a function definition's
    # return type has already been wrapped, by checking if the function definition's
    # type is stored in this mapping and that it still matches.
    wrapped_coro_return_types: dict[FuncDef, Type] = {}

    def __init__(
        self,
        modules: dict[str, MypyFile],
        missing_modules: set[str],
        incomplete_namespaces: set[str],
        errors: Errors,
        plugin: Plugin,
    ) -&gt; None:
        """Construct semantic analyzer.

        We reuse the same semantic analyzer instance across multiple modules.

        Args:
            modules: Global modules dictionary
            missing_modules: Modules that could not be imported encountered so far
            incomplete_namespaces: Namespaces that are being populated during semantic analysis
                (can contain modules and classes within the current SCC; mutated by the caller)
            errors: Report analysis errors using this instance
        """
        self.locals = [None]
        self.is_comprehension_stack = [False]
        # Saved namespaces from previous iteration. Every top-level function/method body is
        # analyzed in several iterations until all names are resolved. We need to save
        # the local namespaces for the top level function and all nested functions between
        # these iterations. See also semanal_main.process_top_level_function().
        self.saved_locals: dict[
            FuncItem | GeneratorExpr | DictionaryComprehension, SymbolTable
        ] = {}
        self.imports = set()
        self._type = None
        self.type_stack = []
        # Are the namespaces of classes being processed complete?
        self.incomplete_type_stack: list[bool] = []
        self.tvar_scope = TypeVarLikeScope()
        self.function_stack = []
        self.block_depth = [0]
        self.loop_depth = [0]
        self.errors = errors
        self.modules = modules
        self.msg = MessageBuilder(errors, modules)
        self.missing_modules = missing_modules
        self.missing_names = [set()]
        # These namespaces are still in process of being populated. If we encounter a
        # missing name in these namespaces, we need to defer the current analysis target,
        # since it's possible that the name will be there once the namespace is complete.
        self.incomplete_namespaces = incomplete_namespaces
        self.all_exports: list[str] = []
        # Map from module id to list of explicitly exported names (i.e. names in __all__).
        self.export_map: dict[str, list[str]] = {}
        self.plugin = plugin
        # If True, process function definitions. If False, don't. This is used
        # for processing module top levels in fine-grained incremental mode.
        self.recurse_into_functions = True
        self.scope = Scope()

        # Trace line numbers for every file where deferral happened during analysis of
        # current SCC or top-level function.
        self.deferral_debug_context: list[tuple[str, int]] = []

        # This is needed to properly support recursive type aliases. The problem is that
        # Foo[Bar] could mean three things depending on context: a target for type alias,
        # a normal index expression (including enum index), or a type application.
        # The latter is particularly problematic as it can falsely create incomplete
        # refs while analysing rvalues of type aliases. To avoid this we first analyse
        # rvalues while temporarily setting this to True.
        self.basic_type_applications = False

        # Used to temporarily enable unbound type variables in some contexts. Namely,
        # in base class expressions, and in right hand sides of type aliases. Do not add
        # new uses of this, as this may cause leaking `UnboundType`s to type checking.
        self.allow_unbound_tvars = False

</t>
<t tx="ekr.20230831011820.1111">    # mypyc doesn't properly handle implementing an abstractproperty
    # with a regular attribute so we make them properties
    @property
    def type(self) -&gt; TypeInfo | None:
        return self._type

</t>
<t tx="ekr.20230831011820.1112">    @property
    def is_stub_file(self) -&gt; bool:
        return self._is_stub_file

</t>
<t tx="ekr.20230831011820.1113">    @property
    def is_typeshed_stub_file(self) -&gt; bool:
        return self._is_typeshed_stub_file

</t>
<t tx="ekr.20230831011820.1114">    @property
    def final_iteration(self) -&gt; bool:
        return self._final_iteration

</t>
<t tx="ekr.20230831011820.1115">    @contextmanager
    def allow_unbound_tvars_set(self) -&gt; Iterator[None]:
        old = self.allow_unbound_tvars
        self.allow_unbound_tvars = True
        try:
            yield
        finally:
            self.allow_unbound_tvars = old

</t>
<t tx="ekr.20230831011820.1116">    #
    # Preparing module (performed before semantic analysis)
    #

    def prepare_file(self, file_node: MypyFile) -&gt; None:
        """Prepare a freshly parsed file for semantic analysis."""
        if "builtins" in self.modules:
            file_node.names["__builtins__"] = SymbolTableNode(GDEF, self.modules["builtins"])
        if file_node.fullname == "builtins":
            self.prepare_builtins_namespace(file_node)
        if file_node.fullname == "typing":
            self.prepare_typing_namespace(file_node, type_aliases)
        if file_node.fullname == "typing_extensions":
            self.prepare_typing_namespace(file_node, typing_extensions_aliases)

</t>
<t tx="ekr.20230831011820.1117">    def prepare_typing_namespace(self, file_node: MypyFile, aliases: dict[str, str]) -&gt; None:
        """Remove dummy alias definitions such as List = TypeAlias(object) from typing.

        They will be replaced with real aliases when corresponding targets are ready.
        """

        # This is all pretty unfortunate. typeshed now has a
        # sys.version_info check for OrderedDict, and we shouldn't
        # take it out, because it is correct and a typechecker should
        # use that as a source of truth. But instead we rummage
        # through IfStmts to remove the info first.  (I tried to
        # remove this whole machinery and ran into issues with the
        # builtins/typing import cycle.)
        def helper(defs: list[Statement]) -&gt; None:
            for stmt in defs.copy():
                if isinstance(stmt, IfStmt):
                    for body in stmt.body:
                        helper(body.body)
                    if stmt.else_body:
                        helper(stmt.else_body.body)
                if (
                    isinstance(stmt, AssignmentStmt)
                    and len(stmt.lvalues) == 1
                    and isinstance(stmt.lvalues[0], NameExpr)
                ):
                    # Assignment to a simple name, remove it if it is a dummy alias.
                    if f"{file_node.fullname}.{stmt.lvalues[0].name}" in aliases:
                        defs.remove(stmt)

        helper(file_node.defs)

</t>
<t tx="ekr.20230831011820.1118">    def prepare_builtins_namespace(self, file_node: MypyFile) -&gt; None:
        """Add certain special-cased definitions to the builtins module.

        Some definitions are too special or fundamental to be processed
        normally from the AST.
        """
        names = file_node.names

        # Add empty definition for core built-in classes, since they are required for basic
        # operation. These will be completed later on.
        for name in CORE_BUILTIN_CLASSES:
            cdef = ClassDef(name, Block([]))  # Dummy ClassDef, will be replaced later
            info = TypeInfo(SymbolTable(), cdef, "builtins")
            info._fullname = f"builtins.{name}"
            names[name] = SymbolTableNode(GDEF, info)

        bool_info = names["bool"].node
        assert isinstance(bool_info, TypeInfo)
        bool_type = Instance(bool_info, [])

        special_var_types: list[tuple[str, Type]] = [
            ("None", NoneType()),
            # reveal_type is a mypy-only function that gives an error with
            # the type of its arg.
            ("reveal_type", AnyType(TypeOfAny.special_form)),
            # reveal_locals is a mypy-only function that gives an error with the types of
            # locals
            ("reveal_locals", AnyType(TypeOfAny.special_form)),
            ("True", bool_type),
            ("False", bool_type),
            ("__debug__", bool_type),
        ]

        for name, typ in special_var_types:
            v = Var(name, typ)
            v._fullname = f"builtins.{name}"
            file_node.names[name] = SymbolTableNode(GDEF, v)

</t>
<t tx="ekr.20230831011820.1119">    #
    # Analyzing a target
    #

    def refresh_partial(
        self,
        node: MypyFile | FuncDef | OverloadedFuncDef,
        patches: list[tuple[int, Callable[[], None]]],
        final_iteration: bool,
        file_node: MypyFile,
        options: Options,
        active_type: TypeInfo | None = None,
    ) -&gt; None:
        """Refresh a stale target in fine-grained incremental mode."""
        self.patches = patches
        self.deferred = False
        self.incomplete = False
        self._final_iteration = final_iteration
        self.missing_names[-1] = set()

        with self.file_context(file_node, options, active_type):
            if isinstance(node, MypyFile):
                self.refresh_top_level(node)
            else:
                self.recurse_into_functions = True
                self.accept(node)
        del self.patches

</t>
<t tx="ekr.20230831011820.112">def comparison_method_example_msg(self, class_name: str) -&gt; str:
    return dedent(
</t>
<t tx="ekr.20230831011820.1120">    def refresh_top_level(self, file_node: MypyFile) -&gt; None:
        """Reanalyze a stale module top-level in fine-grained incremental mode."""
        self.recurse_into_functions = False
        self.add_implicit_module_attrs(file_node)
        for d in file_node.defs:
            self.accept(d)
        if file_node.fullname == "typing":
            self.add_builtin_aliases(file_node)
        if file_node.fullname == "typing_extensions":
            self.add_typing_extension_aliases(file_node)
        self.adjust_public_exports()
        self.export_map[self.cur_mod_id] = self.all_exports
        self.all_exports = []

</t>
<t tx="ekr.20230831011820.1121">    def add_implicit_module_attrs(self, file_node: MypyFile) -&gt; None:
        """Manually add implicit definitions of module '__name__' etc."""
        str_type: Type | None = self.named_type_or_none("builtins.str")
        if str_type is None:
            str_type = UnboundType("builtins.str")
        for name, t in implicit_module_attrs.items():
            if name == "__doc__":
                typ: Type = str_type
            elif name == "__path__":
                if not file_node.is_package_init_file():
                    continue
                # Need to construct the type ourselves, to avoid issues with __builtins__.list
                # not being subscriptable or typing.List not getting bound
                inst = self.named_type_or_none("builtins.list", [str_type])
                if inst is None:
                    assert not self.final_iteration, "Cannot find builtins.list to add __path__"
                    self.defer()
                    return
                typ = inst
            elif name == "__annotations__":
                inst = self.named_type_or_none(
                    "builtins.dict", [str_type, AnyType(TypeOfAny.special_form)]
                )
                if inst is None:
                    assert (
                        not self.final_iteration
                    ), "Cannot find builtins.dict to add __annotations__"
                    self.defer()
                    return
                typ = inst
            else:
                assert t is not None, f"type should be specified for {name}"
                typ = UnboundType(t)

            existing = file_node.names.get(name)
            if existing is not None and not isinstance(existing.node, PlaceholderNode):
                # Already exists.
                continue

            an_type = self.anal_type(typ)
            if an_type:
                var = Var(name, an_type)
                var._fullname = self.qualified_name(name)
                var.is_ready = True
                self.add_symbol(name, var, dummy_context())
            else:
                self.add_symbol(
                    name,
                    PlaceholderNode(self.qualified_name(name), file_node, -1),
                    dummy_context(),
                )

</t>
<t tx="ekr.20230831011820.1122">    def add_builtin_aliases(self, tree: MypyFile) -&gt; None:
        """Add builtin type aliases to typing module.

        For historical reasons, the aliases like `List = list` are not defined
        in typeshed stubs for typing module. Instead we need to manually add the
        corresponding nodes on the fly. We explicitly mark these aliases as normalized,
        so that a user can write `typing.List[int]`.
        """
        assert tree.fullname == "typing"
        for alias, target_name in type_aliases.items():
            if type_aliases_source_versions[alias] &gt; self.options.python_version:
                # This alias is not available on this Python version.
                continue
            name = alias.split(".")[-1]
            if name in tree.names and not isinstance(tree.names[name].node, PlaceholderNode):
                continue
            self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20230831011820.1123">    def add_typing_extension_aliases(self, tree: MypyFile) -&gt; None:
        """Typing extensions module does contain some type aliases.

        We need to analyze them as such, because in typeshed
        they are just defined as `_Alias()` call.
        Which is not supported natively.
        """
        assert tree.fullname == "typing_extensions"

        for alias, target_name in typing_extensions_aliases.items():
            name = alias.split(".")[-1]
            if name in tree.names and isinstance(tree.names[name].node, TypeAlias):
                continue  # Do not reset TypeAliases on the second pass.

            # We need to remove any node that is there at the moment. It is invalid.
            tree.names.pop(name, None)

            # Now, create a new alias.
            self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20230831011820.1124">    def create_alias(self, tree: MypyFile, target_name: str, alias: str, name: str) -&gt; None:
        tag = self.track_incomplete_refs()
        n = self.lookup_fully_qualified_or_none(target_name)
        if n:
            if isinstance(n.node, PlaceholderNode):
                self.mark_incomplete(name, tree)
            else:
                # Found built-in class target. Create alias.
                target = self.named_type_or_none(target_name, [])
                assert target is not None
                # Transform List to List[Any], etc.
                fix_instance_types(target, self.fail, self.note, self.options)
                alias_node = TypeAlias(
                    target,
                    alias,
                    line=-1,
                    column=-1,  # there is no context
                    no_args=True,
                    normalized=True,
                )
                self.add_symbol(name, alias_node, tree)
        elif self.found_incomplete_ref(tag):
            # Built-in class target may not ready yet -- defer.
            self.mark_incomplete(name, tree)
        else:
            # Test fixtures may be missing some builtin classes, which is okay.
            # Kill the placeholder if there is one.
            if name in tree.names:
                assert isinstance(tree.names[name].node, PlaceholderNode)
                del tree.names[name]

</t>
<t tx="ekr.20230831011820.1125">    def adjust_public_exports(self) -&gt; None:
        """Adjust the module visibility of globals due to __all__."""
        if "__all__" in self.globals:
            for name, g in self.globals.items():
                # Being included in __all__ explicitly exports and makes public.
                if name in self.all_exports:
                    g.module_public = True
                    g.module_hidden = False
                # But when __all__ is defined, and a symbol is not included in it,
                # it cannot be public.
                else:
                    g.module_public = False

</t>
<t tx="ekr.20230831011820.1126">    @contextmanager
    def file_context(
        self, file_node: MypyFile, options: Options, active_type: TypeInfo | None = None
    ) -&gt; Iterator[None]:
        """Configure analyzer for analyzing targets within a file/class.

        Args:
            file_node: target file
            options: options specific to the file
            active_type: must be the surrounding class to analyze method targets
        """
        scope = self.scope
        self.options = options
        self.errors.set_file(file_node.path, file_node.fullname, scope=scope, options=options)
        self.cur_mod_node = file_node
        self.cur_mod_id = file_node.fullname
        with scope.module_scope(self.cur_mod_id):
            self._is_stub_file = file_node.path.lower().endswith(".pyi")
            self._is_typeshed_stub_file = is_typeshed_file(
                options.abs_custom_typeshed_dir, file_node.path
            )
            self.globals = file_node.names
            self.tvar_scope = TypeVarLikeScope()

            self.named_tuple_analyzer = NamedTupleAnalyzer(options, self)
            self.typed_dict_analyzer = TypedDictAnalyzer(options, self, self.msg)
            self.enum_call_analyzer = EnumCallAnalyzer(options, self)
            self.newtype_analyzer = NewTypeAnalyzer(options, self, self.msg)

            # Counter that keeps track of references to undefined things potentially caused by
            # incomplete namespaces.
            self.num_incomplete_refs = 0

            if active_type:
                self.incomplete_type_stack.append(False)
                scope.enter_class(active_type)
                self.enter_class(active_type.defn.info)
                for tvar in active_type.defn.type_vars:
                    self.tvar_scope.bind_existing(tvar)

            yield

            if active_type:
                scope.leave_class()
                self.leave_class()
                self._type = None
                self.incomplete_type_stack.pop()
        del self.options

</t>
<t tx="ekr.20230831011820.1127">    #
    # Functions
    #

    def visit_func_def(self, defn: FuncDef) -&gt; None:
        self.statement = defn

        # Visit default values because they may contain assignment expressions.
        for arg in defn.arguments:
            if arg.initializer:
                arg.initializer.accept(self)

        defn.is_conditional = self.block_depth[-1] &gt; 0

        # Set full names even for those definitions that aren't added
        # to a symbol table. For example, for overload items.
        defn._fullname = self.qualified_name(defn.name)

        # We don't add module top-level functions to symbol tables
        # when we analyze their bodies in the second phase on analysis,
        # since they were added in the first phase. Nested functions
        # get always added, since they aren't separate targets.
        if not self.recurse_into_functions or len(self.function_stack) &gt; 0:
            if not defn.is_decorated and not defn.is_overload:
                self.add_function_to_symbol_table(defn)

        if not self.recurse_into_functions:
            return

        with self.scope.function_scope(defn):
            self.analyze_func_def(defn)

</t>
<t tx="ekr.20230831011820.1128">    def analyze_func_def(self, defn: FuncDef) -&gt; None:
        self.function_stack.append(defn)

        if defn.type:
            assert isinstance(defn.type, CallableType)
            has_self_type = self.update_function_type_variables(defn.type, defn)
        else:
            has_self_type = False

        self.function_stack.pop()

        if self.is_class_scope():
            # Method definition
            assert self.type is not None
            defn.info = self.type
            if defn.type is not None and defn.name in ("__init__", "__init_subclass__"):
                assert isinstance(defn.type, CallableType)
                if isinstance(get_proper_type(defn.type.ret_type), AnyType):
                    defn.type = defn.type.copy_modified(ret_type=NoneType())
            self.prepare_method_signature(defn, self.type, has_self_type)

        # Analyze function signature
        with self.tvar_scope_frame(self.tvar_scope.method_frame()):
            if defn.type:
                self.check_classvar_in_signature(defn.type)
                assert isinstance(defn.type, CallableType)
                # Signature must be analyzed in the surrounding scope so that
                # class-level imported names and type variables are in scope.
                analyzer = self.type_analyzer()
                tag = self.track_incomplete_refs()
                result = analyzer.visit_callable_type(defn.type, nested=False)
                # Don't store not ready types (including placeholders).
                if self.found_incomplete_ref(tag) or has_placeholder(result):
                    self.defer(defn)
                    return
                assert isinstance(result, ProperType)
                if isinstance(result, CallableType):
                    # type guards need to have a positional argument, to spec
                    skip_self = self.is_class_scope() and not defn.is_static
                    if result.type_guard and ARG_POS not in result.arg_kinds[skip_self:]:
                        self.fail(
                            "TypeGuard functions must have a positional argument",
                            result,
                            code=codes.VALID_TYPE,
                        )
                        # in this case, we just kind of just ... remove the type guard.
                        result = result.copy_modified(type_guard=None)

                    result = self.remove_unpack_kwargs(defn, result)
                    if has_self_type and self.type is not None:
                        info = self.type
                        if info.self_type is not None:
                            result.variables = [info.self_type] + list(result.variables)
                defn.type = result
                self.add_type_alias_deps(analyzer.aliases_used)
                self.check_function_signature(defn)
                self.check_paramspec_definition(defn)
                if isinstance(defn, FuncDef):
                    assert isinstance(defn.type, CallableType)
                    defn.type = set_callable_name(defn.type, defn)

        self.analyze_arg_initializers(defn)
        self.analyze_function_body(defn)

        if self.is_class_scope():
            assert self.type is not None
            # Mark protocol methods with empty bodies as implicitly abstract.
            # This makes explicit protocol subclassing type-safe.
            if (
                self.type.is_protocol
                and not self.is_stub_file  # Bodies in stub files are always empty.
                and (not isinstance(self.scope.function, OverloadedFuncDef) or defn.is_property)
                and defn.abstract_status != IS_ABSTRACT
                and is_trivial_body(defn.body)
            ):
                defn.abstract_status = IMPLICITLY_ABSTRACT
            if (
                is_trivial_body(defn.body)
                and not self.is_stub_file
                and defn.abstract_status != NOT_ABSTRACT
            ):
                defn.is_trivial_body = True

        if (
            defn.is_coroutine
            and isinstance(defn.type, CallableType)
            and self.wrapped_coro_return_types.get(defn) != defn.type
        ):
            if defn.is_async_generator:
                # Async generator types are handled elsewhere
                pass
            else:
                # A coroutine defined as `async def foo(...) -&gt; T: ...`
                # has external return type `Coroutine[Any, Any, T]`.
                any_type = AnyType(TypeOfAny.special_form)
                ret_type = self.named_type_or_none(
                    "typing.Coroutine", [any_type, any_type, defn.type.ret_type]
                )
                assert ret_type is not None, "Internal error: typing.Coroutine not found"
                defn.type = defn.type.copy_modified(ret_type=ret_type)
                self.wrapped_coro_return_types[defn] = defn.type

</t>
<t tx="ekr.20230831011820.1129">    def remove_unpack_kwargs(self, defn: FuncDef, typ: CallableType) -&gt; CallableType:
        if not typ.arg_kinds or typ.arg_kinds[-1] is not ArgKind.ARG_STAR2:
            return typ
        last_type = typ.arg_types[-1]
        if not isinstance(last_type, UnpackType):
            return typ
        last_type = get_proper_type(last_type.type)
        if not isinstance(last_type, TypedDictType):
            self.fail("Unpack item in ** argument must be a TypedDict", defn)
            new_arg_types = typ.arg_types[:-1] + [AnyType(TypeOfAny.from_error)]
            return typ.copy_modified(arg_types=new_arg_types)
        overlap = set(typ.arg_names) &amp; set(last_type.items)
        # It is OK for TypedDict to have a key named 'kwargs'.
        overlap.discard(typ.arg_names[-1])
        if overlap:
            overlapped = ", ".join([f'"{name}"' for name in overlap])
            self.fail(f"Overlap between argument names and ** TypedDict items: {overlapped}", defn)
            new_arg_types = typ.arg_types[:-1] + [AnyType(TypeOfAny.from_error)]
            return typ.copy_modified(arg_types=new_arg_types)
        # OK, everything looks right now, mark the callable type as using unpack.
        new_arg_types = typ.arg_types[:-1] + [last_type]
        return typ.copy_modified(arg_types=new_arg_types, unpack_kwargs=True)

</t>
<t tx="ekr.20230831011820.113">            """\
        It is recommended for "__eq__" to work with arbitrary objects, for example:
            def __eq__(self, other: object) -&gt; bool:
                if not isinstance(other, {class_name}):
                    return NotImplemented
                return &lt;logic to compare two {class_name} instances&gt;
        """.format(
                class_name=class_name
            )
        )

    def return_type_incompatible_with_supertype(
        self,
        name: str,
        name_in_supertype: str,
        supertype: str,
        original: Type,
        override: Type,
        context: Context,
    ) -&gt; None:
        target = self.override_target(name, name_in_supertype, supertype)
        override_str, original_str = format_type_distinctly(
            override, original, options=self.options
        )
        self.fail(
            'Return type {} of "{}" incompatible with return type {} in {}'.format(
                override_str, name, original_str, target
            ),
            context,
            code=codes.OVERRIDE,
        )

</t>
<t tx="ekr.20230831011820.1130">    def prepare_method_signature(self, func: FuncDef, info: TypeInfo, has_self_type: bool) -&gt; None:
        """Check basic signature validity and tweak annotation of self/cls argument."""
        # Only non-static methods are special, as well as __new__.
        functype = func.type
        if func.name == "__new__":
            func.is_static = True
        if not func.is_static or func.name == "__new__":
            if func.name in ["__init_subclass__", "__class_getitem__"]:
                func.is_class = True
            if not func.arguments:
                self.fail(
                    'Method must have at least one argument. Did you forget the "self" argument?',
                    func,
                )
            elif isinstance(functype, CallableType):
                self_type = get_proper_type(functype.arg_types[0])
                if isinstance(self_type, AnyType):
                    if has_self_type:
                        assert self.type is not None and self.type.self_type is not None
                        leading_type: Type = self.type.self_type
                    else:
                        leading_type = fill_typevars(info)
                    if func.is_class or func.name == "__new__":
                        leading_type = self.class_type(leading_type)
                    func.type = replace_implicit_first_type(functype, leading_type)
                elif has_self_type and isinstance(func.unanalyzed_type, CallableType):
                    if not isinstance(get_proper_type(func.unanalyzed_type.arg_types[0]), AnyType):
                        if self.is_expected_self_type(
                            self_type, func.is_class or func.name == "__new__"
                        ):
                            # This error is off by default, since it is explicitly allowed
                            # by the PEP 673.
                            self.fail(
                                'Redundant "Self" annotation for the first method argument',
                                func,
                                code=codes.REDUNDANT_SELF_TYPE,
                            )
                        else:
                            self.fail(
                                "Method cannot have explicit self annotation and Self type", func
                            )
        elif has_self_type:
            self.fail("Static methods cannot use Self type", func)

</t>
<t tx="ekr.20230831011820.1131">    def is_expected_self_type(self, typ: Type, is_classmethod: bool) -&gt; bool:
        """Does this (analyzed or not) type represent the expected Self type for a method?"""
        assert self.type is not None
        typ = get_proper_type(typ)
        if is_classmethod:
            if isinstance(typ, TypeType):
                return self.is_expected_self_type(typ.item, is_classmethod=False)
            if isinstance(typ, UnboundType):
                sym = self.lookup_qualified(typ.name, typ, suppress_errors=True)
                if (
                    sym is not None
                    and (
                        sym.fullname == "typing.Type"
                        or (
                            sym.fullname == "builtins.type"
                            and (
                                self.is_stub_file
                                or self.is_future_flag_set("annotations")
                                or self.options.python_version &gt;= (3, 9)
                            )
                        )
                    )
                    and typ.args
                ):
                    return self.is_expected_self_type(typ.args[0], is_classmethod=False)
            return False
        if isinstance(typ, TypeVarType):
            return typ == self.type.self_type
        if isinstance(typ, UnboundType):
            sym = self.lookup_qualified(typ.name, typ, suppress_errors=True)
            return sym is not None and sym.fullname in SELF_TYPE_NAMES
        return False

</t>
<t tx="ekr.20230831011820.1132">    def set_original_def(self, previous: Node | None, new: FuncDef | Decorator) -&gt; bool:
        """If 'new' conditionally redefine 'previous', set 'previous' as original

        We reject straight redefinitions of functions, as they are usually
        a programming error. For example:

          def f(): ...
          def f(): ...  # Error: 'f' redefined
        """
        if isinstance(new, Decorator):
            new = new.func
        if (
            isinstance(previous, (FuncDef, Decorator))
            and unnamed_function(new.name)
            and unnamed_function(previous.name)
        ):
            return True
        if isinstance(previous, (FuncDef, Var, Decorator)) and new.is_conditional:
            new.original_def = previous
            return True
        else:
            return False

</t>
<t tx="ekr.20230831011820.1133">    def update_function_type_variables(self, fun_type: CallableType, defn: FuncItem) -&gt; bool:
        """Make any type variables in the signature of defn explicit.

        Update the signature of defn to contain type variable definitions
        if defn is generic. Return True, if the signature contains typing.Self
        type, or False otherwise.
        """
        with self.tvar_scope_frame(self.tvar_scope.method_frame()):
            a = self.type_analyzer()
            fun_type.variables, has_self_type = a.bind_function_type_variables(fun_type, defn)
            if has_self_type and self.type is not None:
                self.setup_self_type()
            return has_self_type

</t>
<t tx="ekr.20230831011820.1134">    def setup_self_type(self) -&gt; None:
        """Setup a (shared) Self type variable for current class.

        We intentionally don't add it to the class symbol table,
        so it can be accessed only by mypy and will not cause
        clashes with user defined names.
        """
        assert self.type is not None
        info = self.type
        if info.self_type is not None:
            if has_placeholder(info.self_type.upper_bound):
                # Similar to regular (user defined) type variables.
                self.process_placeholder(
                    None,
                    "Self upper bound",
                    info,
                    force_progress=info.self_type.upper_bound != fill_typevars(info),
                )
            else:
                return
        info.self_type = TypeVarType(
            "Self",
            f"{info.fullname}.Self",
            id=0,
            values=[],
            upper_bound=fill_typevars(info),
            default=AnyType(TypeOfAny.from_omitted_generics),
        )

</t>
<t tx="ekr.20230831011820.1135">    def visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
        self.statement = defn
        self.add_function_to_symbol_table(defn)

        if not self.recurse_into_functions:
            return

        # NB: Since _visit_overloaded_func_def will call accept on the
        # underlying FuncDefs, the function might get entered twice.
        # This is fine, though, because only the outermost function is
        # used to compute targets.
        with self.scope.function_scope(defn):
            self.analyze_overloaded_func_def(defn)

</t>
<t tx="ekr.20230831011820.1136">    def analyze_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
        # OverloadedFuncDef refers to any legitimate situation where you have
        # more than one declaration for the same function in a row.  This occurs
        # with a @property with a setter or a deleter, and for a classic
        # @overload.

        defn._fullname = self.qualified_name(defn.name)
        # TODO: avoid modifying items.
        defn.items = defn.unanalyzed_items.copy()

        first_item = defn.items[0]
        first_item.is_overload = True
        first_item.accept(self)

        if isinstance(first_item, Decorator) and first_item.func.is_property:
            # This is a property.
            first_item.func.is_overload = True
            self.analyze_property_with_multi_part_definition(defn)
            typ = function_type(first_item.func, self.named_type("builtins.function"))
            assert isinstance(typ, CallableType)
            types = [typ]
        else:
            # This is an a normal overload. Find the item signatures, the
            # implementation (if outside a stub), and any missing @overload
            # decorators.
            types, impl, non_overload_indexes = self.analyze_overload_sigs_and_impl(defn)
            defn.impl = impl
            if non_overload_indexes:
                self.handle_missing_overload_decorators(
                    defn, non_overload_indexes, some_overload_decorators=len(types) &gt; 0
                )
            # If we found an implementation, remove it from the overload item list,
            # as it's special.
            if impl is not None:
                assert impl is defn.items[-1]
                defn.items = defn.items[:-1]
            elif not non_overload_indexes:
                self.handle_missing_overload_implementation(defn)

        if types and not any(
            # If some overload items are decorated with other decorators, then
            # the overload type will be determined during type checking.
            isinstance(it, Decorator) and len(it.decorators) &gt; 1
            for it in defn.items
        ):
            # TODO: should we enforce decorated overloads consistency somehow?
            # Some existing code uses both styles:
            #   * Put decorator only on implementation, use "effective" types in overloads
            #   * Put decorator everywhere, use "bare" types in overloads.
            defn.type = Overloaded(types)
            defn.type.line = defn.line

        if not defn.items:
            # It was not a real overload after all, but function redefinition. We've
            # visited the redefinition(s) already.
            if not defn.impl:
                # For really broken overloads with no items and no implementation we need to keep
                # at least one item to hold basic information like function name.
                defn.impl = defn.unanalyzed_items[-1]
            return

        # We know this is an overload def. Infer properties and perform some checks.
        self.process_final_in_overload(defn)
        self.process_static_or_class_method_in_overload(defn)
        self.process_overload_impl(defn)

</t>
<t tx="ekr.20230831011820.1137">    def process_overload_impl(self, defn: OverloadedFuncDef) -&gt; None:
        """Set flags for an overload implementation.

        Currently, this checks for a trivial body in protocols classes,
        where it makes the method implicitly abstract.
        """
        if defn.impl is None:
            return
        impl = defn.impl if isinstance(defn.impl, FuncDef) else defn.impl.func
        if is_trivial_body(impl.body) and self.is_class_scope() and not self.is_stub_file:
            assert self.type is not None
            if self.type.is_protocol:
                impl.abstract_status = IMPLICITLY_ABSTRACT
            if impl.abstract_status != NOT_ABSTRACT:
                impl.is_trivial_body = True

</t>
<t tx="ekr.20230831011820.1138">    def analyze_overload_sigs_and_impl(
        self, defn: OverloadedFuncDef
    ) -&gt; tuple[list[CallableType], OverloadPart | None, list[int]]:
        """Find overload signatures, the implementation, and items with missing @overload.

        Assume that the first was already analyzed. As a side effect:
        analyzes remaining items and updates 'is_overload' flags.
        """
        types = []
        non_overload_indexes = []
        impl: OverloadPart | None = None
        for i, item in enumerate(defn.items):
            if i != 0:
                # Assume that the first item was already visited
                item.is_overload = True
                item.accept(self)
            # TODO: support decorated overloaded functions properly
            if isinstance(item, Decorator):
                callable = function_type(item.func, self.named_type("builtins.function"))
                assert isinstance(callable, CallableType)
                if not any(refers_to_fullname(dec, OVERLOAD_NAMES) for dec in item.decorators):
                    if i == len(defn.items) - 1 and not self.is_stub_file:
                        # Last item outside a stub is impl
                        impl = item
                    else:
                        # Oops it wasn't an overload after all. A clear error
                        # will vary based on where in the list it is, record
                        # that.
                        non_overload_indexes.append(i)
                else:
                    item.func.is_overload = True
                    types.append(callable)
                    if item.var.is_property:
                        self.fail("An overload can not be a property", item)
                # If any item was decorated with `@override`, the whole overload
                # becomes an explicit override.
                defn.is_explicit_override |= item.func.is_explicit_override
            elif isinstance(item, FuncDef):
                if i == len(defn.items) - 1 and not self.is_stub_file:
                    impl = item
                else:
                    non_overload_indexes.append(i)
        return types, impl, non_overload_indexes

</t>
<t tx="ekr.20230831011820.1139">    def handle_missing_overload_decorators(
        self,
        defn: OverloadedFuncDef,
        non_overload_indexes: list[int],
        some_overload_decorators: bool,
    ) -&gt; None:
        """Generate errors for overload items without @overload.

        Side effect: remote non-overload items.
        """
        if some_overload_decorators:
            # Some of them were overloads, but not all.
            for idx in non_overload_indexes:
                if self.is_stub_file:
                    self.fail(
                        "An implementation for an overloaded function "
                        "is not allowed in a stub file",
                        defn.items[idx],
                    )
                else:
                    self.fail(
                        "The implementation for an overloaded function must come last",
                        defn.items[idx],
                    )
        else:
            for idx in non_overload_indexes[1:]:
                self.name_already_defined(defn.name, defn.items[idx], defn.items[0])
            if defn.impl:
                self.name_already_defined(defn.name, defn.impl, defn.items[0])
        # Remove the non-overloads
        for idx in reversed(non_overload_indexes):
            del defn.items[idx]

</t>
<t tx="ekr.20230831011820.114">    def override_target(self, name: str, name_in_super: str, supertype: str) -&gt; str:
        target = f'supertype "{supertype}"'
        if name_in_super != name:
            target = f'"{name_in_super}" of {target}'
        return target

</t>
<t tx="ekr.20230831011820.1140">    def handle_missing_overload_implementation(self, defn: OverloadedFuncDef) -&gt; None:
        """Generate error about missing overload implementation (only if needed)."""
        if not self.is_stub_file:
            if self.type and self.type.is_protocol and not self.is_func_scope():
                # An overloaded protocol method doesn't need an implementation,
                # but if it doesn't have one, then it is considered abstract.
                for item in defn.items:
                    if isinstance(item, Decorator):
                        item.func.abstract_status = IS_ABSTRACT
                    else:
                        item.abstract_status = IS_ABSTRACT
            else:
                # TODO: also allow omitting an implementation for abstract methods in ABCs?
                self.fail(
                    "An overloaded function outside a stub file must have an implementation",
                    defn,
                    code=codes.NO_OVERLOAD_IMPL,
                )

</t>
<t tx="ekr.20230831011820.1141">    def process_final_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
        """Detect the @final status of an overloaded function (and perform checks)."""
        # If the implementation is marked as @final (or the first overload in
        # stubs), then the whole overloaded definition if @final.
        if any(item.is_final for item in defn.items):
            # We anyway mark it as final because it was probably the intention.
            defn.is_final = True
            # Only show the error once per overload
            bad_final = next(ov for ov in defn.items if ov.is_final)
            if not self.is_stub_file:
                self.fail("@final should be applied only to overload implementation", bad_final)
            elif any(item.is_final for item in defn.items[1:]):
                bad_final = next(ov for ov in defn.items[1:] if ov.is_final)
                self.fail(
                    "In a stub file @final must be applied only to the first overload", bad_final
                )
        if defn.impl is not None and defn.impl.is_final:
            defn.is_final = True

</t>
<t tx="ekr.20230831011820.1142">    def process_static_or_class_method_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
        class_status = []
        static_status = []
        for item in defn.items:
            if isinstance(item, Decorator):
                inner = item.func
            elif isinstance(item, FuncDef):
                inner = item
            else:
                assert False, f"The 'item' variable is an unexpected type: {type(item)}"
            class_status.append(inner.is_class)
            static_status.append(inner.is_static)

        if defn.impl is not None:
            if isinstance(defn.impl, Decorator):
                inner = defn.impl.func
            elif isinstance(defn.impl, FuncDef):
                inner = defn.impl
            else:
                assert False, f"Unexpected impl type: {type(defn.impl)}"
            class_status.append(inner.is_class)
            static_status.append(inner.is_static)

        if len(set(class_status)) != 1:
            self.msg.overload_inconsistently_applies_decorator("classmethod", defn)
        elif len(set(static_status)) != 1:
            self.msg.overload_inconsistently_applies_decorator("staticmethod", defn)
        else:
            defn.is_class = class_status[0]
            defn.is_static = static_status[0]

</t>
<t tx="ekr.20230831011820.1143">    def analyze_property_with_multi_part_definition(self, defn: OverloadedFuncDef) -&gt; None:
        """Analyze a property defined using multiple methods (e.g., using @x.setter).

        Assume that the first method (@property) has already been analyzed.
        """
        defn.is_property = True
        items = defn.items
        first_item = defn.items[0]
        assert isinstance(first_item, Decorator)
        deleted_items = []
        for i, item in enumerate(items[1:]):
            if isinstance(item, Decorator):
                if len(item.decorators) &gt;= 1:
                    node = item.decorators[0]
                    if isinstance(node, MemberExpr):
                        if node.name == "setter":
                            # The first item represents the entire property.
                            first_item.var.is_settable_property = True
                            # Get abstractness from the original definition.
                            item.func.abstract_status = first_item.func.abstract_status
                        if node.name == "deleter":
                            item.func.abstract_status = first_item.func.abstract_status
                    else:
                        self.fail(
                            f"Only supported top decorator is @{first_item.func.name}.setter", item
                        )
                item.func.accept(self)
            else:
                self.fail(f'Unexpected definition for property "{first_item.func.name}"', item)
                deleted_items.append(i + 1)
        for i in reversed(deleted_items):
            del items[i]

</t>
<t tx="ekr.20230831011820.1144">    def add_function_to_symbol_table(self, func: FuncDef | OverloadedFuncDef) -&gt; None:
        if self.is_class_scope():
            assert self.type is not None
            func.info = self.type
        func._fullname = self.qualified_name(func.name)
        self.add_symbol(func.name, func, func)

</t>
<t tx="ekr.20230831011820.1145">    def analyze_arg_initializers(self, defn: FuncItem) -&gt; None:
        with self.tvar_scope_frame(self.tvar_scope.method_frame()):
            # Analyze default arguments
            for arg in defn.arguments:
                if arg.initializer:
                    arg.initializer.accept(self)

</t>
<t tx="ekr.20230831011820.1146">    def analyze_function_body(self, defn: FuncItem) -&gt; None:
        is_method = self.is_class_scope()
        with self.tvar_scope_frame(self.tvar_scope.method_frame()):
            # Bind the type variables again to visit the body.
            if defn.type:
                a = self.type_analyzer()
                typ = defn.type
                assert isinstance(typ, CallableType)
                a.bind_function_type_variables(typ, defn)
                for i in range(len(typ.arg_types)):
                    store_argument_type(defn, i, typ, self.named_type)
            self.function_stack.append(defn)
            with self.enter(defn):
                for arg in defn.arguments:
                    self.add_local(arg.variable, defn)

                # The first argument of a non-static, non-class method is like 'self'
                # (though the name could be different), having the enclosing class's
                # instance type.
                if is_method and (not defn.is_static or defn.name == "__new__") and defn.arguments:
                    if not defn.is_class:
                        defn.arguments[0].variable.is_self = True
                    else:
                        defn.arguments[0].variable.is_cls = True

                defn.body.accept(self)
            self.function_stack.pop()

</t>
<t tx="ekr.20230831011820.1147">    def check_classvar_in_signature(self, typ: ProperType) -&gt; None:
        t: ProperType
        if isinstance(typ, Overloaded):
            for t in typ.items:
                self.check_classvar_in_signature(t)
            return
        if not isinstance(typ, CallableType):
            return
        for t in get_proper_types(typ.arg_types) + [get_proper_type(typ.ret_type)]:
            if self.is_classvar(t):
                self.fail_invalid_classvar(t)
                # Show only one error per signature
                break

</t>
<t tx="ekr.20230831011820.1148">    def check_function_signature(self, fdef: FuncItem) -&gt; None:
        sig = fdef.type
        assert isinstance(sig, CallableType)
        if len(sig.arg_types) &lt; len(fdef.arguments):
            self.fail("Type signature has too few arguments", fdef)
            # Add dummy Any arguments to prevent crashes later.
            num_extra_anys = len(fdef.arguments) - len(sig.arg_types)
            extra_anys = [AnyType(TypeOfAny.from_error)] * num_extra_anys
            sig.arg_types.extend(extra_anys)
        elif len(sig.arg_types) &gt; len(fdef.arguments):
            self.fail("Type signature has too many arguments", fdef, blocker=True)

</t>
<t tx="ekr.20230831011820.1149">    def check_paramspec_definition(self, defn: FuncDef) -&gt; None:
        func = defn.type
        assert isinstance(func, CallableType)

        if not any(isinstance(var, ParamSpecType) for var in func.variables):
            return  # Function does not have param spec variables

        args = func.var_arg()
        kwargs = func.kw_arg()
        if args is None and kwargs is None:
            return  # Looks like this function does not have starred args

        args_defn_type = None
        kwargs_defn_type = None
        for arg_def, arg_kind in zip(defn.arguments, defn.arg_kinds):
            if arg_kind == ARG_STAR:
                args_defn_type = arg_def.type_annotation
            elif arg_kind == ARG_STAR2:
                kwargs_defn_type = arg_def.type_annotation

        # This may happen on invalid `ParamSpec` args / kwargs definition,
        # type analyzer sets types of arguments to `Any`, but keeps
        # definition types as `UnboundType` for now.
        if not (
            (isinstance(args_defn_type, UnboundType) and args_defn_type.name.endswith(".args"))
            or (
                isinstance(kwargs_defn_type, UnboundType)
                and kwargs_defn_type.name.endswith(".kwargs")
            )
        ):
            # Looks like both `*args` and `**kwargs` are not `ParamSpec`
            # It might be something else, skipping.
            return

        args_type = args.typ if args is not None else None
        kwargs_type = kwargs.typ if kwargs is not None else None

        if (
            not isinstance(args_type, ParamSpecType)
            or not isinstance(kwargs_type, ParamSpecType)
            or args_type.name != kwargs_type.name
        ):
            if isinstance(args_defn_type, UnboundType) and args_defn_type.name.endswith(".args"):
                param_name = args_defn_type.name.split(".")[0]
            elif isinstance(kwargs_defn_type, UnboundType) and kwargs_defn_type.name.endswith(
                ".kwargs"
            ):
                param_name = kwargs_defn_type.name.split(".")[0]
            else:
                # Fallback for cases that probably should not ever happen:
                param_name = "P"

            self.fail(
                f'ParamSpec must have "*args" typed as "{param_name}.args" and "**kwargs" typed as "{param_name}.kwargs"',
                func,
                code=codes.VALID_TYPE,
            )

</t>
<t tx="ekr.20230831011820.115">    def incompatible_type_application(
        self, expected_arg_count: int, actual_arg_count: int, context: Context
    ) -&gt; None:
        if expected_arg_count == 0:
            self.fail("Type application targets a non-generic function or class", context)
        elif actual_arg_count &gt; expected_arg_count:
            self.fail(
                f"Type application has too many types ({expected_arg_count} expected)", context
            )
        else:
            self.fail(
                f"Type application has too few types ({expected_arg_count} expected)", context
            )

</t>
<t tx="ekr.20230831011820.1150">    def visit_decorator(self, dec: Decorator) -&gt; None:
        self.statement = dec
        # TODO: better don't modify them at all.
        dec.decorators = dec.original_decorators.copy()
        dec.func.is_conditional = self.block_depth[-1] &gt; 0
        if not dec.is_overload:
            self.add_symbol(dec.name, dec, dec)
        dec.func._fullname = self.qualified_name(dec.name)
        dec.var._fullname = self.qualified_name(dec.name)
        for d in dec.decorators:
            d.accept(self)
        removed: list[int] = []
        no_type_check = False
        could_be_decorated_property = False
        for i, d in enumerate(dec.decorators):
            # A bunch of decorators are special cased here.
            if refers_to_fullname(d, "abc.abstractmethod"):
                removed.append(i)
                dec.func.abstract_status = IS_ABSTRACT
                self.check_decorated_function_is_method("abstractmethod", dec)
            elif refers_to_fullname(d, ("asyncio.coroutines.coroutine", "types.coroutine")):
                removed.append(i)
                dec.func.is_awaitable_coroutine = True
            elif refers_to_fullname(d, "builtins.staticmethod"):
                removed.append(i)
                dec.func.is_static = True
                dec.var.is_staticmethod = True
                self.check_decorated_function_is_method("staticmethod", dec)
            elif refers_to_fullname(d, "builtins.classmethod"):
                removed.append(i)
                dec.func.is_class = True
                dec.var.is_classmethod = True
                self.check_decorated_function_is_method("classmethod", dec)
            elif refers_to_fullname(d, OVERRIDE_DECORATOR_NAMES):
                removed.append(i)
                dec.func.is_explicit_override = True
                self.check_decorated_function_is_method("override", dec)
            elif refers_to_fullname(
                d,
                (
                    "builtins.property",
                    "abc.abstractproperty",
                    "functools.cached_property",
                    "enum.property",
                ),
            ):
                removed.append(i)
                dec.func.is_property = True
                dec.var.is_property = True
                if refers_to_fullname(d, "abc.abstractproperty"):
                    dec.func.abstract_status = IS_ABSTRACT
                elif refers_to_fullname(d, "functools.cached_property"):
                    dec.var.is_settable_property = True
                self.check_decorated_function_is_method("property", dec)
            elif refers_to_fullname(d, "typing.no_type_check"):
                dec.var.type = AnyType(TypeOfAny.special_form)
                no_type_check = True
            elif refers_to_fullname(d, FINAL_DECORATOR_NAMES):
                if self.is_class_scope():
                    assert self.type is not None, "No type set at class scope"
                    if self.type.is_protocol:
                        self.msg.protocol_members_cant_be_final(d)
                    else:
                        dec.func.is_final = True
                        dec.var.is_final = True
                    removed.append(i)
                else:
                    self.fail("@final cannot be used with non-method functions", d)
            elif isinstance(d, CallExpr) and refers_to_fullname(
                d.callee, DATACLASS_TRANSFORM_NAMES
            ):
                dec.func.dataclass_transform_spec = self.parse_dataclass_transform_spec(d)
            elif not dec.var.is_property:
                # We have seen a "non-trivial" decorator before seeing @property, if
                # we will see a @property later, give an error, as we don't support this.
                could_be_decorated_property = True
        for i in reversed(removed):
            del dec.decorators[i]
        if (not dec.is_overload or dec.var.is_property) and self.type:
            dec.var.info = self.type
            dec.var.is_initialized_in_class = True
        if not no_type_check and self.recurse_into_functions:
            dec.func.accept(self)
        if could_be_decorated_property and dec.decorators and dec.var.is_property:
            self.fail("Decorators on top of @property are not supported", dec)
        if (dec.func.is_static or dec.func.is_class) and dec.var.is_property:
            self.fail("Only instance methods can be decorated with @property", dec)
        if dec.func.abstract_status == IS_ABSTRACT and dec.func.is_final:
            self.fail(f"Method {dec.func.name} is both abstract and final", dec)
        if dec.func.is_static and dec.func.is_class:
            self.fail(message_registry.CLASS_PATTERN_CLASS_OR_STATIC_METHOD, dec)

</t>
<t tx="ekr.20230831011820.1151">    def check_decorated_function_is_method(self, decorator: str, context: Context) -&gt; None:
        if not self.type or self.is_func_scope():
            self.fail(f'"{decorator}" used with a non-method', context)

</t>
<t tx="ekr.20230831011820.1152">    #
    # Classes
    #

    def visit_class_def(self, defn: ClassDef) -&gt; None:
        self.statement = defn
        self.incomplete_type_stack.append(not defn.info)
        namespace = self.qualified_name(defn.name)
        with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
            self.analyze_class(defn)
        self.incomplete_type_stack.pop()

</t>
<t tx="ekr.20230831011820.1153">    def analyze_class(self, defn: ClassDef) -&gt; None:
        fullname = self.qualified_name(defn.name)
        if not defn.info and not self.is_core_builtin_class(defn):
            # Add placeholder so that self-references in base classes can be
            # resolved.  We don't want this to cause a deferral, since if there
            # are no incomplete references, we'll replace this with a TypeInfo
            # before returning.
            placeholder = PlaceholderNode(fullname, defn, defn.line, becomes_typeinfo=True)
            self.add_symbol(defn.name, placeholder, defn, can_defer=False)

        tag = self.track_incomplete_refs()

        # Restore base classes after previous iteration (things like Generic[T] might be removed).
        defn.base_type_exprs.extend(defn.removed_base_type_exprs)
        defn.removed_base_type_exprs.clear()

        self.infer_metaclass_and_bases_from_compat_helpers(defn)

        bases = defn.base_type_exprs
        bases, tvar_defs, is_protocol = self.clean_up_bases_and_infer_type_variables(
            defn, bases, context=defn
        )

        for tvd in tvar_defs:
            if isinstance(tvd, TypeVarType) and any(
                has_placeholder(t) for t in [tvd.upper_bound] + tvd.values
            ):
                # Some type variable bounds or values are not ready, we need
                # to re-analyze this class.
                self.defer()
            if has_placeholder(tvd.default):
                # Placeholder values in TypeVarLikeTypes may get substituted in.
                # Defer current target until they are ready.
                self.mark_incomplete(defn.name, defn)
                return

        self.analyze_class_keywords(defn)
        bases_result = self.analyze_base_classes(bases)
        if bases_result is None or self.found_incomplete_ref(tag):
            # Something was incomplete. Defer current target.
            self.mark_incomplete(defn.name, defn)
            return

        base_types, base_error = bases_result
        if any(isinstance(base, PlaceholderType) for base, _ in base_types):
            # We need to know the TypeInfo of each base to construct the MRO. Placeholder types
            # are okay in nested positions, since they can't affect the MRO.
            self.mark_incomplete(defn.name, defn)
            return

        declared_metaclass, should_defer, any_meta = self.get_declared_metaclass(
            defn.name, defn.metaclass
        )
        if should_defer or self.found_incomplete_ref(tag):
            # Metaclass was not ready. Defer current target.
            self.mark_incomplete(defn.name, defn)
            return

        if self.analyze_typeddict_classdef(defn):
            if defn.info:
                self.setup_type_vars(defn, tvar_defs)
                self.setup_alias_type_vars(defn)
            return

        if self.analyze_namedtuple_classdef(defn, tvar_defs):
            return

        # Create TypeInfo for class now that base classes and the MRO can be calculated.
        self.prepare_class_def(defn)
        self.setup_type_vars(defn, tvar_defs)
        if base_error:
            defn.info.fallback_to_any = True
        if any_meta:
            defn.info.meta_fallback_to_any = True

        with self.scope.class_scope(defn.info):
            self.configure_base_classes(defn, base_types)
            defn.info.is_protocol = is_protocol
            self.recalculate_metaclass(defn, declared_metaclass)
            defn.info.runtime_protocol = False
            for decorator in defn.decorators:
                self.analyze_class_decorator(defn, decorator)
            self.analyze_class_body_common(defn)

</t>
<t tx="ekr.20230831011820.1154">    def setup_type_vars(self, defn: ClassDef, tvar_defs: list[TypeVarLikeType]) -&gt; None:
        defn.type_vars = tvar_defs
        defn.info.type_vars = []
        # we want to make sure any additional logic in add_type_vars gets run
        defn.info.add_type_vars()

</t>
<t tx="ekr.20230831011820.1155">    def setup_alias_type_vars(self, defn: ClassDef) -&gt; None:
        assert defn.info.special_alias is not None
        defn.info.special_alias.alias_tvars = list(defn.type_vars)
        target = defn.info.special_alias.target
        assert isinstance(target, ProperType)
        if isinstance(target, TypedDictType):
            target.fallback.args = tuple(defn.type_vars)
        elif isinstance(target, TupleType):
            target.partial_fallback.args = tuple(defn.type_vars)
        else:
            assert False, f"Unexpected special alias type: {type(target)}"

</t>
<t tx="ekr.20230831011820.1156">    def is_core_builtin_class(self, defn: ClassDef) -&gt; bool:
        return self.cur_mod_id == "builtins" and defn.name in CORE_BUILTIN_CLASSES

</t>
<t tx="ekr.20230831011820.1157">    def analyze_class_body_common(self, defn: ClassDef) -&gt; None:
        """Parts of class body analysis that are common to all kinds of class defs."""
        self.enter_class(defn.info)
        if any(b.self_type is not None for b in defn.info.mro):
            self.setup_self_type()
        defn.defs.accept(self)
        self.apply_class_plugin_hooks(defn)
        self.leave_class()

</t>
<t tx="ekr.20230831011820.1158">    def analyze_typeddict_classdef(self, defn: ClassDef) -&gt; bool:
        if (
            defn.info
            and defn.info.typeddict_type
            and not has_placeholder(defn.info.typeddict_type)
        ):
            # This is a valid TypedDict, and it is fully analyzed.
            return True
        is_typeddict, info = self.typed_dict_analyzer.analyze_typeddict_classdef(defn)
        if is_typeddict:
            for decorator in defn.decorators:
                decorator.accept(self)
                if isinstance(decorator, RefExpr):
                    if decorator.fullname in FINAL_DECORATOR_NAMES and info is not None:
                        info.is_final = True
            if info is None:
                self.mark_incomplete(defn.name, defn)
            else:
                self.prepare_class_def(defn, info)
            return True
        return False

</t>
<t tx="ekr.20230831011820.1159">    def analyze_namedtuple_classdef(
        self, defn: ClassDef, tvar_defs: list[TypeVarLikeType]
    ) -&gt; bool:
        """Check if this class can define a named tuple."""
        if (
            defn.info
            and defn.info.is_named_tuple
            and defn.info.tuple_type
            and not has_placeholder(defn.info.tuple_type)
        ):
            # Don't reprocess everything. We just need to process methods defined
            # in the named tuple class body.
            is_named_tuple = True
            info: TypeInfo | None = defn.info
        else:
            is_named_tuple, info = self.named_tuple_analyzer.analyze_namedtuple_classdef(
                defn, self.is_stub_file, self.is_func_scope()
            )
        if is_named_tuple:
            if info is None:
                self.mark_incomplete(defn.name, defn)
            else:
                self.prepare_class_def(defn, info, custom_names=True)
                self.setup_type_vars(defn, tvar_defs)
                self.setup_alias_type_vars(defn)
                with self.scope.class_scope(defn.info):
                    for deco in defn.decorators:
                        deco.accept(self)
                        if isinstance(deco, RefExpr) and deco.fullname in FINAL_DECORATOR_NAMES:
                            info.is_final = True
                    with self.named_tuple_analyzer.save_namedtuple_body(info):
                        self.analyze_class_body_common(defn)
            return True
        return False

</t>
<t tx="ekr.20230831011820.116">    def could_not_infer_type_arguments(
        self, callee_type: CallableType, n: int, context: Context
    ) -&gt; None:
        callee_name = callable_name(callee_type)
        if callee_name is not None and n &gt; 0:
            self.fail(f"Cannot infer type argument {n} of {callee_name}", context)
            if callee_name == "&lt;dict&gt;":
                # Invariance in key type causes more of these errors than we would want.
                self.note(
                    "Try assigning the literal to a variable annotated as dict[&lt;key&gt;, &lt;val&gt;]",
                    context,
                )
        else:
            self.fail("Cannot infer function type argument", context)

</t>
<t tx="ekr.20230831011820.1160">    def apply_class_plugin_hooks(self, defn: ClassDef) -&gt; None:
        """Apply a plugin hook that may infer a more precise definition for a class."""

        for decorator in defn.decorators:
            decorator_name = self.get_fullname_for_hook(decorator)
            if decorator_name:
                hook = self.plugin.get_class_decorator_hook(decorator_name)
                # Special case: if the decorator is itself decorated with
                # typing.dataclass_transform, apply the hook for the dataclasses plugin
                # TODO: remove special casing here
                if hook is None and find_dataclass_transform_spec(decorator):
                    hook = dataclasses_plugin.dataclass_tag_callback
                if hook:
                    hook(ClassDefContext(defn, decorator, self))

        if defn.metaclass:
            metaclass_name = self.get_fullname_for_hook(defn.metaclass)
            if metaclass_name:
                hook = self.plugin.get_metaclass_hook(metaclass_name)
                if hook:
                    hook(ClassDefContext(defn, defn.metaclass, self))

        for base_expr in defn.base_type_exprs:
            base_name = self.get_fullname_for_hook(base_expr)
            if base_name:
                hook = self.plugin.get_base_class_hook(base_name)
                if hook:
                    hook(ClassDefContext(defn, base_expr, self))

        # Check if the class definition itself triggers a dataclass transform (via a parent class/
        # metaclass)
        spec = find_dataclass_transform_spec(defn)
        if spec is not None:
            dataclasses_plugin.add_dataclass_tag(defn.info)

</t>
<t tx="ekr.20230831011820.1161">    def get_fullname_for_hook(self, expr: Expression) -&gt; str | None:
        if isinstance(expr, CallExpr):
            return self.get_fullname_for_hook(expr.callee)
        elif isinstance(expr, IndexExpr):
            return self.get_fullname_for_hook(expr.base)
        elif isinstance(expr, RefExpr):
            if expr.fullname:
                return expr.fullname
            # If we don't have a fullname look it up. This happens because base classes are
            # analyzed in a different manner (see exprtotype.py) and therefore those AST
            # nodes will not have full names.
            sym = self.lookup_type_node(expr)
            if sym:
                return sym.fullname
        return None

</t>
<t tx="ekr.20230831011820.1162">    def analyze_class_keywords(self, defn: ClassDef) -&gt; None:
        for value in defn.keywords.values():
            value.accept(self)

</t>
<t tx="ekr.20230831011820.1163">    def enter_class(self, info: TypeInfo) -&gt; None:
        # Remember previous active class
        self.type_stack.append(self.type)
        self.locals.append(None)  # Add class scope
        self.is_comprehension_stack.append(False)
        self.block_depth.append(-1)  # The class body increments this to 0
        self.loop_depth.append(0)
        self._type = info
        self.missing_names.append(set())

</t>
<t tx="ekr.20230831011820.1164">    def leave_class(self) -&gt; None:
        """Restore analyzer state."""
        self.block_depth.pop()
        self.loop_depth.pop()
        self.locals.pop()
        self.is_comprehension_stack.pop()
        self._type = self.type_stack.pop()
        self.missing_names.pop()

</t>
<t tx="ekr.20230831011820.1165">    def analyze_class_decorator(self, defn: ClassDef, decorator: Expression) -&gt; None:
        decorator.accept(self)
        if isinstance(decorator, RefExpr):
            if decorator.fullname in RUNTIME_PROTOCOL_DECOS:
                if defn.info.is_protocol:
                    defn.info.runtime_protocol = True
                else:
                    self.fail("@runtime_checkable can only be used with protocol classes", defn)
            elif decorator.fullname in FINAL_DECORATOR_NAMES:
                defn.info.is_final = True
        elif isinstance(decorator, CallExpr) and refers_to_fullname(
            decorator.callee, DATACLASS_TRANSFORM_NAMES
        ):
            defn.info.dataclass_transform_spec = self.parse_dataclass_transform_spec(decorator)

</t>
<t tx="ekr.20230831011820.1166">    def clean_up_bases_and_infer_type_variables(
        self, defn: ClassDef, base_type_exprs: list[Expression], context: Context
    ) -&gt; tuple[list[Expression], list[TypeVarLikeType], bool]:
        """Remove extra base classes such as Generic and infer type vars.

        For example, consider this class:

          class Foo(Bar, Generic[T]): ...

        Now we will remove Generic[T] from bases of Foo and infer that the
        type variable 'T' is a type argument of Foo.

        Note that this is performed *before* semantic analysis.

        Returns (remaining base expressions, inferred type variables, is protocol).
        """
        removed: list[int] = []
        declared_tvars: TypeVarLikeList = []
        is_protocol = False
        for i, base_expr in enumerate(base_type_exprs):
            if isinstance(base_expr, StarExpr):
                base_expr.valid = True
            self.analyze_type_expr(base_expr)

            try:
                base = self.expr_to_unanalyzed_type(base_expr)
            except TypeTranslationError:
                # This error will be caught later.
                continue
            result = self.analyze_class_typevar_declaration(base)
            if result is not None:
                if declared_tvars:
                    self.fail("Only single Generic[...] or Protocol[...] can be in bases", context)
                removed.append(i)
                tvars = result[0]
                is_protocol |= result[1]
                declared_tvars.extend(tvars)
            if isinstance(base, UnboundType):
                sym = self.lookup_qualified(base.name, base)
                if sym is not None and sym.node is not None:
                    if sym.node.fullname in PROTOCOL_NAMES and i not in removed:
                        # also remove bare 'Protocol' bases
                        removed.append(i)
                        is_protocol = True

        all_tvars = self.get_all_bases_tvars(base_type_exprs, removed)
        if declared_tvars:
            if len(remove_dups(declared_tvars)) &lt; len(declared_tvars):
                self.fail("Duplicate type variables in Generic[...] or Protocol[...]", context)
            declared_tvars = remove_dups(declared_tvars)
            if not set(all_tvars).issubset(set(declared_tvars)):
                self.fail(
                    "If Generic[...] or Protocol[...] is present"
                    " it should list all type variables",
                    context,
                )
                # In case of error, Generic tvars will go first
                declared_tvars = remove_dups(declared_tvars + all_tvars)
        else:
            declared_tvars = all_tvars
        for i in reversed(removed):
            # We need to actually remove the base class expressions like Generic[T],
            # mostly because otherwise they will create spurious dependencies in fine
            # grained incremental mode.
            defn.removed_base_type_exprs.append(defn.base_type_exprs[i])
            del base_type_exprs[i]
        tvar_defs: list[TypeVarLikeType] = []
        for name, tvar_expr in declared_tvars:
            tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
            tvar_defs.append(tvar_def)
        return base_type_exprs, tvar_defs, is_protocol

</t>
<t tx="ekr.20230831011820.1167">    def analyze_class_typevar_declaration(self, base: Type) -&gt; tuple[TypeVarLikeList, bool] | None:
        """Analyze type variables declared using Generic[...] or Protocol[...].

        Args:
            base: Non-analyzed base class

        Return None if the base class does not declare type variables. Otherwise,
        return the type variables.
        """
        if not isinstance(base, UnboundType):
            return None
        unbound = base
        sym = self.lookup_qualified(unbound.name, unbound)
        if sym is None or sym.node is None:
            return None
        if (
            sym.node.fullname == "typing.Generic"
            or sym.node.fullname in PROTOCOL_NAMES
            and base.args
        ):
            is_proto = sym.node.fullname != "typing.Generic"
            tvars: TypeVarLikeList = []
            have_type_var_tuple = False
            for arg in unbound.args:
                tag = self.track_incomplete_refs()
                tvar = self.analyze_unbound_tvar(arg)
                if tvar:
                    if isinstance(tvar[1], TypeVarTupleExpr):
                        if have_type_var_tuple:
                            self.fail("Can only use one type var tuple in a class def", base)
                            continue
                        have_type_var_tuple = True
                    tvars.append(tvar)
                elif not self.found_incomplete_ref(tag):
                    self.fail("Free type variable expected in %s[...]" % sym.node.name, base)
            return tvars, is_proto
        return None

</t>
<t tx="ekr.20230831011820.1168">    def analyze_unbound_tvar(self, t: Type) -&gt; tuple[str, TypeVarLikeExpr] | None:
        if not isinstance(t, UnboundType):
            return None
        unbound = t
        sym = self.lookup_qualified(unbound.name, unbound)
        if sym and isinstance(sym.node, PlaceholderNode):
            self.record_incomplete_ref()
        if sym and isinstance(sym.node, ParamSpecExpr):
            if sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
                # It's bound by our type variable scope
                return None
            return unbound.name, sym.node
        if sym and sym.fullname in ("typing.Unpack", "typing_extensions.Unpack"):
            inner_t = unbound.args[0]
            if not isinstance(inner_t, UnboundType):
                return None
            inner_unbound = inner_t
            inner_sym = self.lookup_qualified(inner_unbound.name, inner_unbound)
            if inner_sym and isinstance(inner_sym.node, PlaceholderNode):
                self.record_incomplete_ref()
            if inner_sym and isinstance(inner_sym.node, TypeVarTupleExpr):
                if inner_sym.fullname and not self.tvar_scope.allow_binding(inner_sym.fullname):
                    # It's bound by our type variable scope
                    return None
                return inner_unbound.name, inner_sym.node
        if sym is None or not isinstance(sym.node, TypeVarExpr):
            return None
        elif sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
            # It's bound by our type variable scope
            return None
        else:
            assert isinstance(sym.node, TypeVarExpr)
            return unbound.name, sym.node

</t>
<t tx="ekr.20230831011820.1169">    def get_all_bases_tvars(
        self, base_type_exprs: list[Expression], removed: list[int]
    ) -&gt; TypeVarLikeList:
        """Return all type variable references in bases."""
        tvars: TypeVarLikeList = []
        for i, base_expr in enumerate(base_type_exprs):
            if i not in removed:
                try:
                    base = self.expr_to_unanalyzed_type(base_expr)
                except TypeTranslationError:
                    # This error will be caught later.
                    continue
                base_tvars = base.accept(TypeVarLikeQuery(self, self.tvar_scope))
                tvars.extend(base_tvars)
        return remove_dups(tvars)

</t>
<t tx="ekr.20230831011820.117">    def invalid_var_arg(self, typ: Type, context: Context) -&gt; None:
        self.fail("List or tuple expected as variadic arguments", context)

</t>
<t tx="ekr.20230831011820.1170">    def get_and_bind_all_tvars(self, type_exprs: list[Expression]) -&gt; list[TypeVarLikeType]:
        """Return all type variable references in item type expressions.

        This is a helper for generic TypedDicts and NamedTuples. Essentially it is
        a simplified version of the logic we use for ClassDef bases. We duplicate
        some amount of code, because it is hard to refactor common pieces.
        """
        tvars = []
        for base_expr in type_exprs:
            try:
                base = self.expr_to_unanalyzed_type(base_expr)
            except TypeTranslationError:
                # This error will be caught later.
                continue
            base_tvars = base.accept(TypeVarLikeQuery(self, self.tvar_scope))
            tvars.extend(base_tvars)
        tvars = remove_dups(tvars)  # Variables are defined in order of textual appearance.
        tvar_defs = []
        for name, tvar_expr in tvars:
            tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
            tvar_defs.append(tvar_def)
        return tvar_defs

</t>
<t tx="ekr.20230831011820.1171">    def prepare_class_def(
        self, defn: ClassDef, info: TypeInfo | None = None, custom_names: bool = False
    ) -&gt; None:
        """Prepare for the analysis of a class definition.

        Create an empty TypeInfo and store it in a symbol table, or if the 'info'
        argument is provided, store it instead (used for magic type definitions).
        """
        if not defn.info:
            defn.fullname = self.qualified_name(defn.name)
            # TODO: Nested classes
            info = info or self.make_empty_type_info(defn)
            defn.info = info
            info.defn = defn
            if not custom_names:
                # Some special classes (in particular NamedTuples) use custom fullname logic.
                # Don't override it here (also see comment below, this needs cleanup).
                if not self.is_func_scope():
                    info._fullname = self.qualified_name(defn.name)
                else:
                    info._fullname = info.name
        local_name = defn.name
        if "@" in local_name:
            local_name = local_name.split("@")[0]
        self.add_symbol(local_name, defn.info, defn)
        if self.is_nested_within_func_scope():
            # We need to preserve local classes, let's store them
            # in globals under mangled unique names
            #
            # TODO: Putting local classes into globals breaks assumptions in fine-grained
            #       incremental mode and we should avoid it. In general, this logic is too
            #       ad-hoc and needs to be removed/refactored.
            if "@" not in defn.info._fullname:
                global_name = defn.info.name + "@" + str(defn.line)
                defn.info._fullname = self.cur_mod_id + "." + global_name
            else:
                # Preserve name from previous fine-grained incremental run.
                global_name = defn.info.name
            defn.fullname = defn.info._fullname
            if defn.info.is_named_tuple:
                # Named tuple nested within a class is stored in the class symbol table.
                self.add_symbol_skip_local(global_name, defn.info)
            else:
                self.globals[global_name] = SymbolTableNode(GDEF, defn.info)

</t>
<t tx="ekr.20230831011820.1172">    def make_empty_type_info(self, defn: ClassDef) -&gt; TypeInfo:
        if (
            self.is_module_scope()
            and self.cur_mod_id == "builtins"
            and defn.name in CORE_BUILTIN_CLASSES
        ):
            # Special case core built-in classes. A TypeInfo was already
            # created for it before semantic analysis, but with a dummy
            # ClassDef. Patch the real ClassDef object.
            info = self.globals[defn.name].node
            assert isinstance(info, TypeInfo)
        else:
            info = TypeInfo(SymbolTable(), defn, self.cur_mod_id)
            info.set_line(defn)
        return info

</t>
<t tx="ekr.20230831011820.1173">    def get_name_repr_of_expr(self, expr: Expression) -&gt; str | None:
        """Try finding a short simplified textual representation of a base class expression."""
        if isinstance(expr, NameExpr):
            return expr.name
        if isinstance(expr, MemberExpr):
            return get_member_expr_fullname(expr)
        if isinstance(expr, IndexExpr):
            return self.get_name_repr_of_expr(expr.base)
        if isinstance(expr, CallExpr):
            return self.get_name_repr_of_expr(expr.callee)
        return None

</t>
<t tx="ekr.20230831011820.1174">    def analyze_base_classes(
        self, base_type_exprs: list[Expression]
    ) -&gt; tuple[list[tuple[ProperType, Expression]], bool] | None:
        """Analyze base class types.

        Return None if some definition was incomplete. Otherwise, return a tuple
        with these items:

         * List of (analyzed type, original expression) tuples
         * Boolean indicating whether one of the bases had a semantic analysis error
        """
        is_error = False
        bases = []
        for base_expr in base_type_exprs:
            if (
                isinstance(base_expr, RefExpr)
                and base_expr.fullname in TYPED_NAMEDTUPLE_NAMES + TPDICT_NAMES
            ):
                # Ignore magic bases for now.
                continue

            try:
                base = self.expr_to_analyzed_type(
                    base_expr, allow_placeholder=True, allow_type_any=True
                )
            except TypeTranslationError:
                name = self.get_name_repr_of_expr(base_expr)
                if isinstance(base_expr, CallExpr):
                    msg = "Unsupported dynamic base class"
                else:
                    msg = "Invalid base class"
                if name:
                    msg += f' "{name}"'
                self.fail(msg, base_expr)
                is_error = True
                continue
            if base is None:
                return None
            base = get_proper_type(base)
            bases.append((base, base_expr))
        return bases, is_error

</t>
<t tx="ekr.20230831011820.1175">    def configure_base_classes(
        self, defn: ClassDef, bases: list[tuple[ProperType, Expression]]
    ) -&gt; None:
        """Set up base classes.

        This computes several attributes on the corresponding TypeInfo defn.info
        related to the base classes: defn.info.bases, defn.info.mro, and
        miscellaneous others (at least tuple_type, fallback_to_any, and is_enum.)
        """
        base_types: list[Instance] = []
        info = defn.info

        for base, base_expr in bases:
            if isinstance(base, TupleType):
                actual_base = self.configure_tuple_base_class(defn, base)
                base_types.append(actual_base)
            elif isinstance(base, Instance):
                if base.type.is_newtype:
                    self.fail('Cannot subclass "NewType"', defn)
                base_types.append(base)
            elif isinstance(base, AnyType):
                if self.options.disallow_subclassing_any:
                    if isinstance(base_expr, (NameExpr, MemberExpr)):
                        msg = f'Class cannot subclass "{base_expr.name}" (has type "Any")'
                    else:
                        msg = 'Class cannot subclass value of type "Any"'
                    self.fail(msg, base_expr)
                info.fallback_to_any = True
            elif isinstance(base, TypedDictType):
                base_types.append(base.fallback)
            else:
                msg = "Invalid base class"
                name = self.get_name_repr_of_expr(base_expr)
                if name:
                    msg += f' "{name}"'
                self.fail(msg, base_expr)
                info.fallback_to_any = True
            if self.options.disallow_any_unimported and has_any_from_unimported_type(base):
                if isinstance(base_expr, (NameExpr, MemberExpr)):
                    prefix = f"Base type {base_expr.name}"
                else:
                    prefix = "Base type"
                self.msg.unimported_type_becomes_any(prefix, base, base_expr)
            check_for_explicit_any(
                base, self.options, self.is_typeshed_stub_file, self.msg, context=base_expr
            )

        # Add 'object' as implicit base if there is no other base class.
        if not base_types and defn.fullname != "builtins.object":
            base_types.append(self.object_type())

        info.bases = base_types

        # Calculate the MRO.
        if not self.verify_base_classes(defn):
            self.set_dummy_mro(defn.info)
            return
        if not self.verify_duplicate_base_classes(defn):
            # We don't want to block the typechecking process,
            # so, we just insert `Any` as the base class and show an error.
            self.set_any_mro(defn.info)
        self.calculate_class_mro(defn, self.object_type)

</t>
<t tx="ekr.20230831011820.1176">    def configure_tuple_base_class(self, defn: ClassDef, base: TupleType) -&gt; Instance:
        info = defn.info

        # There may be an existing valid tuple type from previous semanal iterations.
        # Use equality to check if it is the case.
        if info.tuple_type and info.tuple_type != base and not has_placeholder(info.tuple_type):
            self.fail("Class has two incompatible bases derived from tuple", defn)
            defn.has_incompatible_baseclass = True
        if info.special_alias and has_placeholder(info.special_alias.target):
            self.process_placeholder(
                None, "tuple base", defn, force_progress=base != info.tuple_type
            )
        info.update_tuple_type(base)
        self.setup_alias_type_vars(defn)

        if base.partial_fallback.type.fullname == "builtins.tuple" and not has_placeholder(base):
            # Fallback can only be safely calculated after semantic analysis, since base
            # classes may be incomplete. Postpone the calculation.
            self.schedule_patch(PRIORITY_FALLBACKS, lambda: calculate_tuple_fallback(base))

        return base.partial_fallback

</t>
<t tx="ekr.20230831011820.1177">    def set_dummy_mro(self, info: TypeInfo) -&gt; None:
        # Give it an MRO consisting of just the class itself and object.
        info.mro = [info, self.object_type().type]
        info.bad_mro = True

</t>
<t tx="ekr.20230831011820.1178">    def set_any_mro(self, info: TypeInfo) -&gt; None:
        # Give it an MRO consisting direct `Any` subclass.
        info.fallback_to_any = True
        info.mro = [info, self.object_type().type]

</t>
<t tx="ekr.20230831011820.1179">    def calculate_class_mro(
        self, defn: ClassDef, obj_type: Callable[[], Instance] | None = None
    ) -&gt; None:
        """Calculate method resolution order for a class.

        `obj_type` exists just to fill in empty base class list in case of an error.
        """
        try:
            calculate_mro(defn.info, obj_type)
        except MroError:
            self.fail(
                "Cannot determine consistent method resolution "
                'order (MRO) for "%s"' % defn.name,
                defn,
            )
            self.set_dummy_mro(defn.info)
        # Allow plugins to alter the MRO to handle the fact that `def mro()`
        # on metaclasses permits MRO rewriting.
        if defn.fullname:
            hook = self.plugin.get_customize_class_mro_hook(defn.fullname)
            if hook:
                hook(ClassDefContext(defn, FakeExpression(), self))

</t>
<t tx="ekr.20230831011820.118">    def invalid_keyword_var_arg(self, typ: Type, is_mapping: bool, context: Context) -&gt; None:
        typ = get_proper_type(typ)
        if isinstance(typ, Instance) and is_mapping:
            self.fail("Keywords must be strings", context)
        else:
            self.fail(
                f"Argument after ** must be a mapping, not {format_type(typ, self.options)}",
                context,
                code=codes.ARG_TYPE,
            )

</t>
<t tx="ekr.20230831011820.1180">    def infer_metaclass_and_bases_from_compat_helpers(self, defn: ClassDef) -&gt; None:
        """Lookup for special metaclass declarations, and update defn fields accordingly.

        * six.with_metaclass(M, B1, B2, ...)
        * @six.add_metaclass(M)
        * future.utils.with_metaclass(M, B1, B2, ...)
        * past.utils.with_metaclass(M, B1, B2, ...)
        """

        # Look for six.with_metaclass(M, B1, B2, ...)
        with_meta_expr: Expression | None = None
        if len(defn.base_type_exprs) == 1:
            base_expr = defn.base_type_exprs[0]
            if isinstance(base_expr, CallExpr) and isinstance(base_expr.callee, RefExpr):
                self.analyze_type_expr(base_expr)
                if (
                    base_expr.callee.fullname
                    in {
                        "six.with_metaclass",
                        "future.utils.with_metaclass",
                        "past.utils.with_metaclass",
                    }
                    and len(base_expr.args) &gt;= 1
                    and all(kind == ARG_POS for kind in base_expr.arg_kinds)
                ):
                    with_meta_expr = base_expr.args[0]
                    defn.base_type_exprs = base_expr.args[1:]

        # Look for @six.add_metaclass(M)
        add_meta_expr: Expression | None = None
        for dec_expr in defn.decorators:
            if isinstance(dec_expr, CallExpr) and isinstance(dec_expr.callee, RefExpr):
                dec_expr.callee.accept(self)
                if (
                    dec_expr.callee.fullname == "six.add_metaclass"
                    and len(dec_expr.args) == 1
                    and dec_expr.arg_kinds[0] == ARG_POS
                ):
                    add_meta_expr = dec_expr.args[0]
                    break

        metas = {defn.metaclass, with_meta_expr, add_meta_expr} - {None}
        if len(metas) == 0:
            return
        if len(metas) &gt; 1:
            self.fail("Multiple metaclass definitions", defn)
            return
        defn.metaclass = metas.pop()

</t>
<t tx="ekr.20230831011820.1181">    def verify_base_classes(self, defn: ClassDef) -&gt; bool:
        info = defn.info
        cycle = False
        for base in info.bases:
            baseinfo = base.type
            if self.is_base_class(info, baseinfo):
                self.fail("Cycle in inheritance hierarchy", defn)
                cycle = True
        return not cycle

</t>
<t tx="ekr.20230831011820.1182">    def verify_duplicate_base_classes(self, defn: ClassDef) -&gt; bool:
        dup = find_duplicate(defn.info.direct_base_classes())
        if dup:
            self.fail(f'Duplicate base class "{dup.name}"', defn)
        return not dup

</t>
<t tx="ekr.20230831011820.1183">    def is_base_class(self, t: TypeInfo, s: TypeInfo) -&gt; bool:
        """Determine if t is a base class of s (but do not use mro)."""
        # Search the base class graph for t, starting from s.
        worklist = [s]
        visited = {s}
        while worklist:
            nxt = worklist.pop()
            if nxt == t:
                return True
            for base in nxt.bases:
                if base.type not in visited:
                    worklist.append(base.type)
                    visited.add(base.type)
        return False

</t>
<t tx="ekr.20230831011820.1184">    def get_declared_metaclass(
        self, name: str, metaclass_expr: Expression | None
    ) -&gt; tuple[Instance | None, bool, bool]:
        """Get declared metaclass from metaclass expression.

        Returns a tuple of three values:
          * A metaclass instance or None
          * A boolean indicating whether we should defer
          * A boolean indicating whether we should set metaclass Any fallback
            (either for Any metaclass or invalid/dynamic metaclass).

        The two boolean flags can only be True if instance is None.
        """
        declared_metaclass = None
        if metaclass_expr:
            metaclass_name = None
            if isinstance(metaclass_expr, NameExpr):
                metaclass_name = metaclass_expr.name
            elif isinstance(metaclass_expr, MemberExpr):
                metaclass_name = get_member_expr_fullname(metaclass_expr)
            if metaclass_name is None:
                self.fail(f'Dynamic metaclass not supported for "{name}"', metaclass_expr)
                return None, False, True
            sym = self.lookup_qualified(metaclass_name, metaclass_expr)
            if sym is None:
                # Probably a name error - it is already handled elsewhere
                return None, False, True
            if isinstance(sym.node, Var) and isinstance(get_proper_type(sym.node.type), AnyType):
                if self.options.disallow_subclassing_any:
                    self.fail(
                        f'Class cannot use "{sym.node.name}" as a metaclass (has type "Any")',
                        metaclass_expr,
                    )
                return None, False, True
            if isinstance(sym.node, PlaceholderNode):
                return None, True, False  # defer later in the caller

            # Support type aliases, like `_Meta: TypeAlias = type`
            if (
                isinstance(sym.node, TypeAlias)
                and sym.node.no_args
                and isinstance(sym.node.target, ProperType)
                and isinstance(sym.node.target, Instance)
            ):
                metaclass_info: Node | None = sym.node.target.type
            else:
                metaclass_info = sym.node

            if not isinstance(metaclass_info, TypeInfo) or metaclass_info.tuple_type is not None:
                self.fail(f'Invalid metaclass "{metaclass_name}"', metaclass_expr)
                return None, False, False
            if not metaclass_info.is_metaclass():
                self.fail(
                    'Metaclasses not inheriting from "type" are not supported', metaclass_expr
                )
                return None, False, False
            inst = fill_typevars(metaclass_info)
            assert isinstance(inst, Instance)
            declared_metaclass = inst
        return declared_metaclass, False, False

</t>
<t tx="ekr.20230831011820.1185">    def recalculate_metaclass(self, defn: ClassDef, declared_metaclass: Instance | None) -&gt; None:
        defn.info.declared_metaclass = declared_metaclass
        defn.info.metaclass_type = defn.info.calculate_metaclass_type()
        if any(info.is_protocol for info in defn.info.mro):
            if (
                not defn.info.metaclass_type
                or defn.info.metaclass_type.type.fullname == "builtins.type"
            ):
                # All protocols and their subclasses have ABCMeta metaclass by default.
                # TODO: add a metaclass conflict check if there is another metaclass.
                abc_meta = self.named_type_or_none("abc.ABCMeta", [])
                if abc_meta is not None:  # May be None in tests with incomplete lib-stub.
                    defn.info.metaclass_type = abc_meta
        if defn.info.metaclass_type and defn.info.metaclass_type.type.has_base("enum.EnumMeta"):
            defn.info.is_enum = True
            if defn.type_vars:
                self.fail("Enum class cannot be generic", defn)

</t>
<t tx="ekr.20230831011820.1186">    #
    # Imports
    #

    def visit_import(self, i: Import) -&gt; None:
        self.statement = i
        for id, as_id in i.ids:
            # Modules imported in a stub file without using 'import X as X' won't get exported
            # When implicit re-exporting is disabled, we have the same behavior as stubs.
            use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
            if as_id is not None:
                base_id = id
                imported_id = as_id
                module_public = use_implicit_reexport or id.split(".")[-1] == as_id
            else:
                base_id = id.split(".")[0]
                imported_id = base_id
                module_public = use_implicit_reexport

            if base_id in self.modules:
                node = self.modules[base_id]
                if self.is_func_scope():
                    kind = LDEF
                elif self.type is not None:
                    kind = MDEF
                else:
                    kind = GDEF
                symbol = SymbolTableNode(
                    kind, node, module_public=module_public, module_hidden=not module_public
                )
                self.add_imported_symbol(
                    imported_id,
                    symbol,
                    context=i,
                    module_public=module_public,
                    module_hidden=not module_public,
                )
            else:
                self.add_unknown_imported_symbol(
                    imported_id,
                    context=i,
                    target_name=base_id,
                    module_public=module_public,
                    module_hidden=not module_public,
                )

</t>
<t tx="ekr.20230831011820.1187">    def visit_import_from(self, imp: ImportFrom) -&gt; None:
        self.statement = imp
        module_id = self.correct_relative_import(imp)
        module = self.modules.get(module_id)
        for id, as_id in imp.names:
            fullname = module_id + "." + id
            self.set_future_import_flags(fullname)
            if module is None:
                node = None
            elif module_id == self.cur_mod_id and fullname in self.modules:
                # Submodule takes precedence over definition in surround package, for
                # compatibility with runtime semantics in typical use cases. This
                # could more precisely model runtime semantics by taking into account
                # the line number beyond which the local definition should take
                # precedence, but doesn't seem to be important in most use cases.
                node = SymbolTableNode(GDEF, self.modules[fullname])
            else:
                if id == as_id == "__all__" and module_id in self.export_map:
                    self.all_exports[:] = self.export_map[module_id]
                node = module.names.get(id)

            missing_submodule = False
            imported_id = as_id or id

            # Modules imported in a stub file without using 'from Y import X as X' will
            # not get exported.
            # When implicit re-exporting is disabled, we have the same behavior as stubs.
            use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
            module_public = use_implicit_reexport or (as_id is not None and id == as_id)

            # If the module does not contain a symbol with the name 'id',
            # try checking if it's a module instead.
            if not node:
                mod = self.modules.get(fullname)
                if mod is not None:
                    kind = self.current_symbol_kind()
                    node = SymbolTableNode(kind, mod)
                elif fullname in self.missing_modules:
                    missing_submodule = True
            # If it is still not resolved, check for a module level __getattr__
            if module and not node and "__getattr__" in module.names:
                # We store the fullname of the original definition so that we can
                # detect whether two imported names refer to the same thing.
                fullname = module_id + "." + id
                gvar = self.create_getattr_var(module.names["__getattr__"], imported_id, fullname)
                if gvar:
                    self.add_symbol(
                        imported_id,
                        gvar,
                        imp,
                        module_public=module_public,
                        module_hidden=not module_public,
                    )
                    continue

            if node:
                self.process_imported_symbol(
                    node, module_id, id, imported_id, fullname, module_public, context=imp
                )
                if node.module_hidden:
                    self.report_missing_module_attribute(
                        module_id,
                        id,
                        imported_id,
                        module_public=module_public,
                        module_hidden=not module_public,
                        context=imp,
                        add_unknown_imported_symbol=False,
                    )
            elif module and not missing_submodule:
                # Target module exists but the imported name is missing or hidden.
                self.report_missing_module_attribute(
                    module_id,
                    id,
                    imported_id,
                    module_public=module_public,
                    module_hidden=not module_public,
                    context=imp,
                )
            else:
                # Import of a missing (sub)module.
                self.add_unknown_imported_symbol(
                    imported_id,
                    imp,
                    target_name=fullname,
                    module_public=module_public,
                    module_hidden=not module_public,
                )

</t>
<t tx="ekr.20230831011820.1188">    def process_imported_symbol(
        self,
        node: SymbolTableNode,
        module_id: str,
        id: str,
        imported_id: str,
        fullname: str,
        module_public: bool,
        context: ImportBase,
    ) -&gt; None:
        module_hidden = not module_public and (
            # `from package import submodule` should work regardless of whether package
            # re-exports submodule, so we shouldn't hide it
            not isinstance(node.node, MypyFile)
            or fullname not in self.modules
            # but given `from somewhere import random_unrelated_module` we should hide
            # random_unrelated_module
            or not fullname.startswith(self.cur_mod_id + ".")
        )

        if isinstance(node.node, PlaceholderNode):
            if self.final_iteration:
                self.report_missing_module_attribute(
                    module_id,
                    id,
                    imported_id,
                    module_public=module_public,
                    module_hidden=module_hidden,
                    context=context,
                )
                return
            else:
                # This might become a type.
                self.mark_incomplete(
                    imported_id,
                    node.node,
                    module_public=module_public,
                    module_hidden=module_hidden,
                    becomes_typeinfo=True,
                )
        # NOTE: we take the original node even for final `Var`s. This is to support
        # a common pattern when constants are re-exported (same applies to import *).
        self.add_imported_symbol(
            imported_id, node, context, module_public=module_public, module_hidden=module_hidden
        )

</t>
<t tx="ekr.20230831011820.1189">    def report_missing_module_attribute(
        self,
        import_id: str,
        source_id: str,
        imported_id: str,
        module_public: bool,
        module_hidden: bool,
        context: Node,
        add_unknown_imported_symbol: bool = True,
    ) -&gt; None:
        # Missing attribute.
        if self.is_incomplete_namespace(import_id):
            # We don't know whether the name will be there, since the namespace
            # is incomplete. Defer the current target.
            self.mark_incomplete(
                imported_id, context, module_public=module_public, module_hidden=module_hidden
            )
            return
        message = f'Module "{import_id}" has no attribute "{source_id}"'
        # Suggest alternatives, if any match is found.
        module = self.modules.get(import_id)
        if module:
            if source_id in module.names.keys() and not module.names[source_id].module_public:
                message = (
                    f'Module "{import_id}" does not explicitly export attribute "{source_id}"'
                )
            else:
                alternatives = set(module.names.keys()).difference({source_id})
                matches = best_matches(source_id, alternatives, n=3)
                if matches:
                    suggestion = f"; maybe {pretty_seq(matches, 'or')}?"
                    message += f"{suggestion}"
        self.fail(message, context, code=codes.ATTR_DEFINED)
        if add_unknown_imported_symbol:
            self.add_unknown_imported_symbol(
                imported_id,
                context,
                target_name=None,
                module_public=module_public,
                module_hidden=not module_public,
            )

        if import_id == "typing":
            # The user probably has a missing definition in a test fixture. Let's verify.
            fullname = f"builtins.{source_id.lower()}"
            if (
                self.lookup_fully_qualified_or_none(fullname) is None
                and fullname in SUGGESTED_TEST_FIXTURES
            ):
                # Yes. Generate a helpful note.
                self.msg.add_fixture_note(fullname, context)
            else:
                typing_extensions = self.modules.get("typing_extensions")
                if typing_extensions and source_id in typing_extensions.names:
                    self.msg.note(
                        f"Use `from typing_extensions import {source_id}` instead",
                        context,
                        code=codes.ATTR_DEFINED,
                    )
                    self.msg.note(
                        "See https://mypy.readthedocs.io/en/stable/runtime_troubles.html#using-new-additions-to-the-typing-module",
                        context,
                        code=codes.ATTR_DEFINED,
                    )

</t>
<t tx="ekr.20230831011820.119">    def undefined_in_superclass(self, member: str, context: Context) -&gt; None:
        self.fail(f'"{member}" undefined in superclass', context)

</t>
<t tx="ekr.20230831011820.1190">    def process_import_over_existing_name(
        self,
        imported_id: str,
        existing_symbol: SymbolTableNode,
        module_symbol: SymbolTableNode,
        import_node: ImportBase,
    ) -&gt; bool:
        if existing_symbol.node is module_symbol.node:
            # We added this symbol on previous iteration.
            return False
        if existing_symbol.kind in (LDEF, GDEF, MDEF) and isinstance(
            existing_symbol.node, (Var, FuncDef, TypeInfo, Decorator, TypeAlias)
        ):
            # This is a valid import over an existing definition in the file. Construct a dummy
            # assignment that we'll use to type check the import.
            lvalue = NameExpr(imported_id)
            lvalue.kind = existing_symbol.kind
            lvalue.node = existing_symbol.node
            rvalue = NameExpr(imported_id)
            rvalue.kind = module_symbol.kind
            rvalue.node = module_symbol.node
            if isinstance(rvalue.node, TypeAlias):
                # Suppress bogus errors from the dummy assignment if rvalue is an alias.
                # Otherwise mypy may complain that alias is invalid in runtime context.
                rvalue.is_alias_rvalue = True
            assignment = AssignmentStmt([lvalue], rvalue)
            for node in assignment, lvalue, rvalue:
                node.set_line(import_node)
            import_node.assignments.append(assignment)
            return True
        return False

</t>
<t tx="ekr.20230831011820.1191">    def correct_relative_import(self, node: ImportFrom | ImportAll) -&gt; str:
        import_id, ok = correct_relative_import(
            self.cur_mod_id, node.relative, node.id, self.cur_mod_node.is_package_init_file()
        )
        if not ok:
            self.fail("Relative import climbs too many namespaces", node)
        return import_id

</t>
<t tx="ekr.20230831011820.1192">    def visit_import_all(self, i: ImportAll) -&gt; None:
        i_id = self.correct_relative_import(i)
        if i_id in self.modules:
            m = self.modules[i_id]
            if self.is_incomplete_namespace(i_id):
                # Any names could be missing from the current namespace if the target module
                # namespace is incomplete.
                self.mark_incomplete("*", i)
            for name, node in m.names.items():
                fullname = i_id + "." + name
                self.set_future_import_flags(fullname)
                if node is None:
                    continue
                # if '__all__' exists, all nodes not included have had module_public set to
                # False, and we can skip checking '_' because it's been explicitly included.
                if node.module_public and (not name.startswith("_") or "__all__" in m.names):
                    if isinstance(node.node, MypyFile):
                        # Star import of submodule from a package, add it as a dependency.
                        self.imports.add(node.node.fullname)
                    # `from x import *` always reexports symbols
                    self.add_imported_symbol(
                        name, node, context=i, module_public=True, module_hidden=False
                    )

        else:
            # Don't add any dummy symbols for 'from x import *' if 'x' is unknown.
            pass

</t>
<t tx="ekr.20230831011820.1193">    #
    # Assignment
    #

    def visit_assignment_expr(self, s: AssignmentExpr) -&gt; None:
        s.value.accept(self)
        if self.is_func_scope():
            if not self.check_valid_comprehension(s):
                return
        self.analyze_lvalue(s.target, escape_comprehensions=True, has_explicit_value=True)

</t>
<t tx="ekr.20230831011820.1194">    def check_valid_comprehension(self, s: AssignmentExpr) -&gt; bool:
        """Check that assignment expression is not nested within comprehension at class scope.

        class C:
            [(j := i) for i in [1, 2, 3]]
        is a syntax error that is not enforced by Python parser, but at later steps.
        """
        for i, is_comprehension in enumerate(reversed(self.is_comprehension_stack)):
            if not is_comprehension and i &lt; len(self.locals) - 1:
                if self.locals[-1 - i] is None:
                    self.fail(
                        "Assignment expression within a comprehension"
                        " cannot be used in a class body",
                        s,
                        code=codes.SYNTAX,
                        serious=True,
                        blocker=True,
                    )
                    return False
                break
        return True

</t>
<t tx="ekr.20230831011820.1195">    def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
        self.statement = s

        # Special case assignment like X = X.
        if self.analyze_identity_global_assignment(s):
            return

        tag = self.track_incomplete_refs()

        # Here we have a chicken and egg problem: at this stage we can't call
        # can_be_type_alias(), because we have not enough information about rvalue.
        # But we can't use a full visit because it may emit extra incomplete refs (namely
        # when analysing any type applications there) thus preventing the further analysis.
        # To break the tie, we first analyse rvalue partially, if it can be a type alias.
        if self.can_possibly_be_type_form(s):
            old_basic_type_applications = self.basic_type_applications
            self.basic_type_applications = True
            with self.allow_unbound_tvars_set():
                s.rvalue.accept(self)
            self.basic_type_applications = old_basic_type_applications
        else:
            s.rvalue.accept(self)

        if self.found_incomplete_ref(tag) or self.should_wait_rhs(s.rvalue):
            # Initializer couldn't be fully analyzed. Defer the current node and give up.
            # Make sure that if we skip the definition of some local names, they can't be
            # added later in this scope, since an earlier definition should take precedence.
            for expr in names_modified_by_assignment(s):
                self.mark_incomplete(expr.name, expr)
            return
        if self.can_possibly_be_type_form(s):
            # Now re-visit those rvalues that were we skipped type applications above.
            # This should be safe as generally semantic analyzer is idempotent.
            with self.allow_unbound_tvars_set():
                s.rvalue.accept(self)

        # The r.h.s. is now ready to be classified, first check if it is a special form:
        special_form = False
        # * type alias
        if self.check_and_set_up_type_alias(s):
            s.is_alias_def = True
            special_form = True
        # * type variable definition
        elif self.process_typevar_declaration(s):
            special_form = True
        elif self.process_paramspec_declaration(s):
            special_form = True
        elif self.process_typevartuple_declaration(s):
            special_form = True
        # * type constructors
        elif self.analyze_namedtuple_assign(s):
            special_form = True
        elif self.analyze_typeddict_assign(s):
            special_form = True
        elif self.newtype_analyzer.process_newtype_declaration(s):
            special_form = True
        elif self.analyze_enum_assign(s):
            special_form = True

        if special_form:
            self.record_special_form_lvalue(s)
            return
        # Clear the alias flag if assignment turns out not a special form after all. It
        # may be set to True while there were still placeholders due to forward refs.
        s.is_alias_def = False

        # OK, this is a regular assignment, perform the necessary analysis steps.
        s.is_final_def = self.unwrap_final(s)
        self.analyze_lvalues(s)
        self.check_final_implicit_def(s)
        self.store_final_status(s)
        self.check_classvar(s)
        self.process_type_annotation(s)
        self.apply_dynamic_class_hook(s)
        if not s.type:
            self.process_module_assignment(s.lvalues, s.rvalue, s)
        self.process__all__(s)
        self.process__deletable__(s)
        self.process__slots__(s)

</t>
<t tx="ekr.20230831011820.1196">    def analyze_identity_global_assignment(self, s: AssignmentStmt) -&gt; bool:
        """Special case 'X = X' in global scope.

        This allows supporting some important use cases.

        Return true if special casing was applied.
        """
        if not isinstance(s.rvalue, NameExpr) or len(s.lvalues) != 1:
            # Not of form 'X = X'
            return False
        lvalue = s.lvalues[0]
        if not isinstance(lvalue, NameExpr) or s.rvalue.name != lvalue.name:
            # Not of form 'X = X'
            return False
        if self.type is not None or self.is_func_scope():
            # Not in global scope
            return False
        # It's an assignment like 'X = X' in the global scope.
        name = lvalue.name
        sym = self.lookup(name, s)
        if sym is None:
            if self.final_iteration:
                # Fall back to normal assignment analysis.
                return False
            else:
                self.defer()
                return True
        else:
            if sym.node is None:
                # Something special -- fall back to normal assignment analysis.
                return False
            if name not in self.globals:
                # The name is from builtins. Add an alias to the current module.
                self.add_symbol(name, sym.node, s)
            if not isinstance(sym.node, PlaceholderNode):
                for node in s.rvalue, lvalue:
                    node.node = sym.node
                    node.kind = GDEF
                    node.fullname = sym.node.fullname
            return True

</t>
<t tx="ekr.20230831011820.1197">    def should_wait_rhs(self, rv: Expression) -&gt; bool:
        """Can we already classify this r.h.s. of an assignment or should we wait?

        This returns True if we don't have enough information to decide whether
        an assignment is just a normal variable definition or a special form.
        Always return False if this is a final iteration. This will typically cause
        the lvalue to be classified as a variable plus emit an error.
        """
        if self.final_iteration:
            # No chance, nothing has changed.
            return False
        if isinstance(rv, NameExpr):
            n = self.lookup(rv.name, rv)
            if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
                return True
        elif isinstance(rv, MemberExpr):
            fname = get_member_expr_fullname(rv)
            if fname:
                n = self.lookup_qualified(fname, rv, suppress_errors=True)
                if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
                    return True
        elif isinstance(rv, IndexExpr) and isinstance(rv.base, RefExpr):
            return self.should_wait_rhs(rv.base)
        elif isinstance(rv, CallExpr) and isinstance(rv.callee, RefExpr):
            # This is only relevant for builtin SCC where things like 'TypeVar'
            # may be not ready.
            return self.should_wait_rhs(rv.callee)
        return False

</t>
<t tx="ekr.20230831011820.1198">    def can_be_type_alias(self, rv: Expression, allow_none: bool = False) -&gt; bool:
        """Is this a valid r.h.s. for an alias definition?

        Note: this function should be only called for expressions where self.should_wait_rhs()
        returns False.
        """
        if isinstance(rv, RefExpr) and self.is_type_ref(rv, bare=True):
            return True
        if isinstance(rv, IndexExpr) and self.is_type_ref(rv.base, bare=False):
            return True
        if self.is_none_alias(rv):
            return True
        if allow_none and isinstance(rv, NameExpr) and rv.fullname == "builtins.None":
            return True
        if isinstance(rv, OpExpr) and rv.op == "|":
            if self.is_stub_file:
                return True
            if self.can_be_type_alias(rv.left, allow_none=True) and self.can_be_type_alias(
                rv.right, allow_none=True
            ):
                return True
        return False

</t>
<t tx="ekr.20230831011820.1199">    def can_possibly_be_type_form(self, s: AssignmentStmt) -&gt; bool:
        """Like can_be_type_alias(), but simpler and doesn't require fully analyzed rvalue.

        Instead, use lvalues/annotations structure to figure out whether this can potentially be
        a type alias definition, NamedTuple, or TypedDict. Another difference from above function
        is that we are only interested IndexExpr, CallExpr and OpExpr rvalues, since only those
        can be potentially recursive (things like `A = A` are never valid).
        """
        if len(s.lvalues) &gt; 1:
            return False
        if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.callee, RefExpr):
            ref = s.rvalue.callee.fullname
            return ref in TPDICT_NAMES or ref in TYPED_NAMEDTUPLE_NAMES
        if not isinstance(s.lvalues[0], NameExpr):
            return False
        if s.unanalyzed_type is not None and not self.is_pep_613(s):
            return False
        if not isinstance(s.rvalue, (IndexExpr, OpExpr)):
            return False
        # Something that looks like Foo = Bar[Baz, ...]
        return True

</t>
<t tx="ekr.20230831011820.12">def instance_to_type_environment(instance: Instance) -&gt; dict[TypeVarId, Type]:
    """Given an Instance, produce the resulting type environment for type
    variables bound by the Instance's class definition.

    An Instance is a type application of a class (a TypeInfo) to its
    required number of type arguments.  So this environment consists
    of the class's type variables mapped to the Instance's actual
    arguments.  The type variables are mapped by their `id`.

    """
    return {binder.id: arg for binder, arg in zip(instance.type.defn.type_vars, instance.args)}
</t>
<t tx="ekr.20230831011820.120">    def variable_may_be_undefined(self, name: str, context: Context) -&gt; None:
        self.fail(f'Name "{name}" may be undefined', context, code=codes.POSSIBLY_UNDEFINED)

</t>
<t tx="ekr.20230831011820.1200">    def is_type_ref(self, rv: Expression, bare: bool = False) -&gt; bool:
        """Does this expression refer to a type?

        This includes:
          * Special forms, like Any or Union
          * Classes (except subscripted enums)
          * Other type aliases
          * PlaceholderNodes with becomes_typeinfo=True (these can be not ready class
            definitions, and not ready aliases).

        If bare is True, this is not a base of an index expression, so some special
        forms are not valid (like a bare Union).

        Note: This method should be only used in context of a type alias definition.
        This method can only return True for RefExprs, to check if C[int] is a valid
        target for type alias call this method on expr.base (i.e. on C in C[int]).
        See also can_be_type_alias().
        """
        if not isinstance(rv, RefExpr):
            return False
        if isinstance(rv.node, TypeVarLikeExpr):
            self.fail(f'Type variable "{rv.fullname}" is invalid as target for type alias', rv)
            return False

        if bare:
            # These three are valid even if bare, for example
            # A = Tuple is just equivalent to A = Tuple[Any, ...].
            valid_refs = {"typing.Any", "typing.Tuple", "typing.Callable"}
        else:
            valid_refs = type_constructors

        if isinstance(rv.node, TypeAlias) or rv.fullname in valid_refs:
            return True
        if isinstance(rv.node, TypeInfo):
            if bare:
                return True
            # Assignment color = Color['RED'] defines a variable, not an alias.
            return not rv.node.is_enum
        if isinstance(rv.node, Var):
            return rv.node.fullname in NEVER_NAMES

        if isinstance(rv, NameExpr):
            n = self.lookup(rv.name, rv)
            if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
                return True
        elif isinstance(rv, MemberExpr):
            fname = get_member_expr_fullname(rv)
            if fname:
                # The r.h.s. for variable definitions may not be a type reference but just
                # an instance attribute, so suppress the errors.
                n = self.lookup_qualified(fname, rv, suppress_errors=True)
                if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
                    return True
        return False

</t>
<t tx="ekr.20230831011820.1201">    def is_none_alias(self, node: Expression) -&gt; bool:
        """Is this a r.h.s. for a None alias?

        We special case the assignments like Void = type(None), to allow using
        Void in type annotations.
        """
        if isinstance(node, CallExpr):
            if (
                isinstance(node.callee, NameExpr)
                and len(node.args) == 1
                and isinstance(node.args[0], NameExpr)
            ):
                call = self.lookup_qualified(node.callee.name, node.callee)
                arg = self.lookup_qualified(node.args[0].name, node.args[0])
                if (
                    call is not None
                    and call.node
                    and call.node.fullname == "builtins.type"
                    and arg is not None
                    and arg.node
                    and arg.node.fullname == "builtins.None"
                ):
                    return True
        return False

</t>
<t tx="ekr.20230831011820.1202">    def record_special_form_lvalue(self, s: AssignmentStmt) -&gt; None:
        """Record minimal necessary information about l.h.s. of a special form.

        This exists mostly for compatibility with the old semantic analyzer.
        """
        lvalue = s.lvalues[0]
        assert isinstance(lvalue, NameExpr)
        lvalue.is_special_form = True
        if self.current_symbol_kind() == GDEF:
            lvalue.fullname = self.qualified_name(lvalue.name)
        lvalue.kind = self.current_symbol_kind()

</t>
<t tx="ekr.20230831011820.1203">    def analyze_enum_assign(self, s: AssignmentStmt) -&gt; bool:
        """Check if s defines an Enum."""
        if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, EnumCallExpr):
            # Already analyzed enum -- nothing to do here.
            return True
        return self.enum_call_analyzer.process_enum_call(s, self.is_func_scope())

</t>
<t tx="ekr.20230831011820.1204">    def analyze_namedtuple_assign(self, s: AssignmentStmt) -&gt; bool:
        """Check if s defines a namedtuple."""
        if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, NamedTupleExpr):
            if s.rvalue.analyzed.info.tuple_type and not has_placeholder(
                s.rvalue.analyzed.info.tuple_type
            ):
                return True  # This is a valid and analyzed named tuple definition, nothing to do here.
        if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
            return False
        lvalue = s.lvalues[0]
        if isinstance(lvalue, MemberExpr):
            if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.callee, RefExpr):
                fullname = s.rvalue.callee.fullname
                if fullname == "collections.namedtuple" or fullname in TYPED_NAMEDTUPLE_NAMES:
                    self.fail("NamedTuple type as an attribute is not supported", lvalue)
            return False
        name = lvalue.name
        namespace = self.qualified_name(name)
        with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
            internal_name, info, tvar_defs = self.named_tuple_analyzer.check_namedtuple(
                s.rvalue, name, self.is_func_scope()
            )
            if internal_name is None:
                return False
            if internal_name != name:
                self.fail(
                    'First argument to namedtuple() should be "{}", not "{}"'.format(
                        name, internal_name
                    ),
                    s.rvalue,
                    code=codes.NAME_MATCH,
                )
                return True
            # Yes, it's a valid namedtuple, but defer if it is not ready.
            if not info:
                self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
            else:
                self.setup_type_vars(info.defn, tvar_defs)
                self.setup_alias_type_vars(info.defn)
            return True

</t>
<t tx="ekr.20230831011820.1205">    def analyze_typeddict_assign(self, s: AssignmentStmt) -&gt; bool:
        """Check if s defines a typed dict."""
        if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, TypedDictExpr):
            if s.rvalue.analyzed.info.typeddict_type and not has_placeholder(
                s.rvalue.analyzed.info.typeddict_type
            ):
                # This is a valid and analyzed typed dict definition, nothing to do here.
                return True
        if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
            return False
        lvalue = s.lvalues[0]
        name = lvalue.name
        namespace = self.qualified_name(name)
        with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
            is_typed_dict, info, tvar_defs = self.typed_dict_analyzer.check_typeddict(
                s.rvalue, name, self.is_func_scope()
            )
            if not is_typed_dict:
                return False
            if isinstance(lvalue, MemberExpr):
                self.fail("TypedDict type as attribute is not supported", lvalue)
                return False
            # Yes, it's a valid typed dict, but defer if it is not ready.
            if not info:
                self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
            else:
                defn = info.defn
                self.setup_type_vars(defn, tvar_defs)
                self.setup_alias_type_vars(defn)
            return True

</t>
<t tx="ekr.20230831011820.1206">    def analyze_lvalues(self, s: AssignmentStmt) -&gt; None:
        # We cannot use s.type, because analyze_simple_literal_type() will set it.
        explicit = s.unanalyzed_type is not None
        if self.is_final_type(s.unanalyzed_type):
            # We need to exclude bare Final.
            assert isinstance(s.unanalyzed_type, UnboundType)
            if not s.unanalyzed_type.args:
                explicit = False

        if s.rvalue:
            if isinstance(s.rvalue, TempNode):
                has_explicit_value = not s.rvalue.no_rhs
            else:
                has_explicit_value = True
        else:
            has_explicit_value = False

        for lval in s.lvalues:
            self.analyze_lvalue(
                lval,
                explicit_type=explicit,
                is_final=s.is_final_def,
                has_explicit_value=has_explicit_value,
            )

</t>
<t tx="ekr.20230831011820.1207">    def apply_dynamic_class_hook(self, s: AssignmentStmt) -&gt; None:
        if not isinstance(s.rvalue, CallExpr):
            return
        fname = ""
        call = s.rvalue
        while True:
            if isinstance(call.callee, RefExpr):
                fname = call.callee.fullname
            # check if method call
            if not fname and isinstance(call.callee, MemberExpr):
                callee_expr = call.callee.expr
                if isinstance(callee_expr, RefExpr) and callee_expr.fullname:
                    method_name = call.callee.name
                    fname = callee_expr.fullname + "." + method_name
                elif isinstance(callee_expr, CallExpr):
                    # check if chain call
                    call = callee_expr
                    continue
            break
        if not fname:
            return
        hook = self.plugin.get_dynamic_class_hook(fname)
        if not hook:
            return
        for lval in s.lvalues:
            if not isinstance(lval, NameExpr):
                continue
            hook(DynamicClassDefContext(call, lval.name, self))

</t>
<t tx="ekr.20230831011820.1208">    def unwrap_final(self, s: AssignmentStmt) -&gt; bool:
        """Strip Final[...] if present in an assignment.

        This is done to invoke type inference during type checking phase for this
        assignment. Also, Final[...] doesn't affect type in any way -- it is rather an
        access qualifier for given `Var`.

        Also perform various consistency checks.

        Returns True if Final[...] was present.
        """
        if not s.unanalyzed_type or not self.is_final_type(s.unanalyzed_type):
            return False
        assert isinstance(s.unanalyzed_type, UnboundType)
        if len(s.unanalyzed_type.args) &gt; 1:
            self.fail("Final[...] takes at most one type argument", s.unanalyzed_type)
        invalid_bare_final = False
        if not s.unanalyzed_type.args:
            s.type = None
            if isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs:
                invalid_bare_final = True
                self.fail("Type in Final[...] can only be omitted if there is an initializer", s)
        else:
            s.type = s.unanalyzed_type.args[0]

        if s.type is not None and self.is_classvar(s.type):
            self.fail("Variable should not be annotated with both ClassVar and Final", s)
            return False

        if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], RefExpr):
            self.fail("Invalid final declaration", s)
            return False
        lval = s.lvalues[0]
        assert isinstance(lval, RefExpr)

        # Reset inferred status if it was set due to simple literal rvalue on previous iteration.
        # TODO: this is a best-effort quick fix, we should avoid the need to manually sync this,
        # see https://github.com/python/mypy/issues/6458.
        if lval.is_new_def:
            lval.is_inferred_def = s.type is None

        if self.loop_depth[-1] &gt; 0:
            self.fail("Cannot use Final inside a loop", s)
        if self.type and self.type.is_protocol:
            self.msg.protocol_members_cant_be_final(s)
        if (
            isinstance(s.rvalue, TempNode)
            and s.rvalue.no_rhs
            and not self.is_stub_file
            and not self.is_class_scope()
        ):
            if not invalid_bare_final:  # Skip extra error messages.
                self.msg.final_without_value(s)
        return True

</t>
<t tx="ekr.20230831011820.1209">    def check_final_implicit_def(self, s: AssignmentStmt) -&gt; None:
        """Do basic checks for final declaration on self in __init__.

        Additional re-definition checks are performed by `analyze_lvalue`.
        """
        if not s.is_final_def:
            return
        lval = s.lvalues[0]
        assert isinstance(lval, RefExpr)
        if isinstance(lval, MemberExpr):
            if not self.is_self_member_ref(lval):
                self.fail("Final can be only applied to a name or an attribute on self", s)
                s.is_final_def = False
                return
            else:
                assert self.function_stack
                if self.function_stack[-1].name != "__init__":
                    self.fail("Can only declare a final attribute in class body or __init__", s)
                    s.is_final_def = False
                    return

</t>
<t tx="ekr.20230831011820.121">    def var_used_before_def(self, name: str, context: Context) -&gt; None:
        self.fail(f'Name "{name}" is used before definition', context, code=codes.USED_BEFORE_DEF)

</t>
<t tx="ekr.20230831011820.1210">    def store_final_status(self, s: AssignmentStmt) -&gt; None:
        """If this is a locally valid final declaration, set the corresponding flag on `Var`."""
        if s.is_final_def:
            if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
                node = s.lvalues[0].node
                if isinstance(node, Var):
                    node.is_final = True
                    if s.type:
                        node.final_value = constant_fold_expr(s.rvalue, self.cur_mod_id)
                    if self.is_class_scope() and (
                        isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs
                    ):
                        node.final_unset_in_class = True
        else:
            for lval in self.flatten_lvalues(s.lvalues):
                # Special case: we are working with an `Enum`:
                #
                #   class MyEnum(Enum):
                #       key = 'some value'
                #
                # Here `key` is implicitly final. In runtime, code like
                #
                #     MyEnum.key = 'modified'
                #
                # will fail with `AttributeError: Cannot reassign members.`
                # That's why we need to replicate this.
                if (
                    isinstance(lval, NameExpr)
                    and isinstance(self.type, TypeInfo)
                    and self.type.is_enum
                ):
                    cur_node = self.type.names.get(lval.name, None)
                    if (
                        cur_node
                        and isinstance(cur_node.node, Var)
                        and not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)
                    ):
                        # Double underscored members are writable on an `Enum`.
                        # (Except read-only `__members__` but that is handled in type checker)
                        cur_node.node.is_final = s.is_final_def = not is_dunder(cur_node.node.name)

                # Special case: deferred initialization of a final attribute in __init__.
                # In this case we just pretend this is a valid final definition to suppress
                # errors about assigning to final attribute.
                if isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                    assert self.type, "Self member outside a class"
                    cur_node = self.type.names.get(lval.name, None)
                    if cur_node and isinstance(cur_node.node, Var) and cur_node.node.is_final:
                        assert self.function_stack
                        top_function = self.function_stack[-1]
                        if (
                            top_function.name == "__init__"
                            and cur_node.node.final_unset_in_class
                            and not cur_node.node.final_set_in_init
                            and not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)
                        ):
                            cur_node.node.final_set_in_init = True
                            s.is_final_def = True

</t>
<t tx="ekr.20230831011820.1211">    def flatten_lvalues(self, lvalues: list[Expression]) -&gt; list[Expression]:
        res: list[Expression] = []
        for lv in lvalues:
            if isinstance(lv, (TupleExpr, ListExpr)):
                res.extend(self.flatten_lvalues(lv.items))
            else:
                res.append(lv)
        return res

</t>
<t tx="ekr.20230831011820.1212">    def process_type_annotation(self, s: AssignmentStmt) -&gt; None:
        """Analyze type annotation or infer simple literal type."""
        if s.type:
            lvalue = s.lvalues[-1]
            allow_tuple_literal = isinstance(lvalue, TupleExpr)
            analyzed = self.anal_type(s.type, allow_tuple_literal=allow_tuple_literal)
            # Don't store not ready types (including placeholders).
            if analyzed is None or has_placeholder(analyzed):
                self.defer(s)
                return
            s.type = analyzed
            if (
                self.type
                and self.type.is_protocol
                and isinstance(lvalue, NameExpr)
                and isinstance(s.rvalue, TempNode)
                and s.rvalue.no_rhs
            ):
                if isinstance(lvalue.node, Var):
                    lvalue.node.is_abstract_var = True
        else:
            if (
                self.type
                and self.type.is_protocol
                and self.is_annotated_protocol_member(s)
                and not self.is_func_scope()
            ):
                self.fail("All protocol members must have explicitly declared types", s)
            # Set the type if the rvalue is a simple literal (even if the above error occurred).
            if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
                ref_expr = s.lvalues[0]
                safe_literal_inference = True
                if self.type and isinstance(ref_expr, NameExpr) and len(self.type.mro) &gt; 1:
                    # Check if there is a definition in supertype. If yes, we can't safely
                    # decide here what to infer: int or Literal[42].
                    safe_literal_inference = self.type.mro[1].get(ref_expr.name) is None
                if safe_literal_inference and ref_expr.is_inferred_def:
                    s.type = self.analyze_simple_literal_type(s.rvalue, s.is_final_def)
        if s.type:
            # Store type into nodes.
            for lvalue in s.lvalues:
                self.store_declared_types(lvalue, s.type)

</t>
<t tx="ekr.20230831011820.1213">    def is_annotated_protocol_member(self, s: AssignmentStmt) -&gt; bool:
        """Check whether a protocol member is annotated.

        There are some exceptions that can be left unannotated, like ``__slots__``."""
        return any(
            (isinstance(lv, NameExpr) and lv.name != "__slots__" and lv.is_inferred_def)
            for lv in s.lvalues
        )

</t>
<t tx="ekr.20230831011820.1214">    def analyze_simple_literal_type(self, rvalue: Expression, is_final: bool) -&gt; Type | None:
        """Return builtins.int if rvalue is an int literal, etc.

        If this is a 'Final' context, we return "Literal[...]" instead.
        """
        if self.function_stack:
            # Skip inside a function; this is to avoid confusing
            # the code that handles dead code due to isinstance()
            # inside type variables with value restrictions (like
            # AnyStr).
            return None

        value = constant_fold_expr(rvalue, self.cur_mod_id)
        if value is None or isinstance(value, complex):
            return None

        if isinstance(value, bool):
            type_name = "builtins.bool"
        elif isinstance(value, int):
            type_name = "builtins.int"
        elif isinstance(value, str):
            type_name = "builtins.str"
        elif isinstance(value, float):
            type_name = "builtins.float"

        typ = self.named_type_or_none(type_name)
        if typ and is_final:
            return typ.copy_modified(last_known_value=LiteralType(value=value, fallback=typ))
        return typ

</t>
<t tx="ekr.20230831011820.1215">    def analyze_alias(
        self, name: str, rvalue: Expression, allow_placeholder: bool = False
    ) -&gt; tuple[Type | None, list[TypeVarLikeType], set[str], list[str]]:
        """Check if 'rvalue' is a valid type allowed for aliasing (e.g. not a type variable).

        If yes, return the corresponding type, a list of
        qualified type variable names for generic aliases, a set of names the alias depends on,
        and a list of type variables if the alias is generic.
        A schematic example for the dependencies:
            A = int
            B = str
            analyze_alias(Dict[A, B])[2] == {'__main__.A', '__main__.B'}
        """
        dynamic = bool(self.function_stack and self.function_stack[-1].is_dynamic())
        global_scope = not self.type and not self.function_stack
        try:
            typ = expr_to_unanalyzed_type(rvalue, self.options, self.is_stub_file)
        except TypeTranslationError:
            self.fail(
                "Invalid type alias: expression is not a valid type", rvalue, code=codes.VALID_TYPE
            )
            return None, [], set(), []

        found_type_vars = typ.accept(TypeVarLikeQuery(self, self.tvar_scope))
        tvar_defs: list[TypeVarLikeType] = []
        namespace = self.qualified_name(name)
        with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
            for name, tvar_expr in found_type_vars:
                tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
                tvar_defs.append(tvar_def)

            analyzed, depends_on = analyze_type_alias(
                typ,
                self,
                self.tvar_scope,
                self.plugin,
                self.options,
                self.is_typeshed_stub_file,
                allow_placeholder=allow_placeholder,
                in_dynamic_func=dynamic,
                global_scope=global_scope,
                allowed_alias_tvars=tvar_defs,
            )

        # There can be only one variadic variable at most, the error is reported elsewhere.
        new_tvar_defs = []
        variadic = False
        for td in tvar_defs:
            if isinstance(td, TypeVarTupleType):
                if variadic:
                    continue
                variadic = True
            new_tvar_defs.append(td)

        qualified_tvars = [node.fullname for _name, node in found_type_vars]
        return analyzed, new_tvar_defs, depends_on, qualified_tvars

</t>
<t tx="ekr.20230831011820.1216">    def is_pep_613(self, s: AssignmentStmt) -&gt; bool:
        if s.unanalyzed_type is not None and isinstance(s.unanalyzed_type, UnboundType):
            lookup = self.lookup_qualified(s.unanalyzed_type.name, s, suppress_errors=True)
            if lookup and lookup.fullname in TYPE_ALIAS_NAMES:
                return True
        return False

</t>
<t tx="ekr.20230831011820.1217">    def check_and_set_up_type_alias(self, s: AssignmentStmt) -&gt; bool:
        """Check if assignment creates a type alias and set it up as needed.

        Return True if it is a type alias (even if the target is not ready),
        or False otherwise.

        Note: the resulting types for subscripted (including generic) aliases
        are also stored in rvalue.analyzed.
        """
        if s.invalid_recursive_alias:
            return True
        lvalue = s.lvalues[0]
        if len(s.lvalues) &gt; 1 or not isinstance(lvalue, NameExpr):
            # First rule: Only simple assignments like Alias = ... create aliases.
            return False

        pep_613 = self.is_pep_613(s)
        if not pep_613 and s.unanalyzed_type is not None:
            # Second rule: Explicit type (cls: Type[A] = A) always creates variable, not alias.
            # unless using PEP 613 `cls: TypeAlias = A`
            return False

        if isinstance(s.rvalue, CallExpr) and s.rvalue.analyzed:
            return False

        existing = self.current_symbol_table().get(lvalue.name)
        # Third rule: type aliases can't be re-defined. For example:
        #     A: Type[float] = int
        #     A = float  # OK, but this doesn't define an alias
        #     B = int
        #     B = float  # Error!
        # Don't create an alias in these cases:
        if existing and (
            isinstance(existing.node, Var)  # existing variable
            or (isinstance(existing.node, TypeAlias) and not s.is_alias_def)  # existing alias
            or (isinstance(existing.node, PlaceholderNode) and existing.node.node.line &lt; s.line)
        ):  # previous incomplete definition
            # TODO: find a more robust way to track the order of definitions.
            # Note: if is_alias_def=True, this is just a node from previous iteration.
            if isinstance(existing.node, TypeAlias) and not s.is_alias_def:
                self.fail(
                    'Cannot assign multiple types to name "{}"'
                    ' without an explicit "Type[...]" annotation'.format(lvalue.name),
                    lvalue,
                )
            return False

        non_global_scope = self.type or self.is_func_scope()
        if not pep_613 and isinstance(s.rvalue, RefExpr) and non_global_scope:
            # Fourth rule (special case): Non-subscripted right hand side creates a variable
            # at class and function scopes. For example:
            #
            #   class Model:
            #       ...
            #   class C:
            #       model = Model # this is automatically a variable with type 'Type[Model]'
            #
            # without this rule, this typical use case will require a lot of explicit
            # annotations (see the second rule).
            return False
        rvalue = s.rvalue
        if not pep_613 and not self.can_be_type_alias(rvalue):
            return False

        if existing and not isinstance(existing.node, (PlaceholderNode, TypeAlias)):
            # Cannot redefine existing node as type alias.
            return False

        res: Type | None = None
        if self.is_none_alias(rvalue):
            res = NoneType()
            alias_tvars: list[TypeVarLikeType] = []
            depends_on: set[str] = set()
            qualified_tvars: list[str] = []
        else:
            tag = self.track_incomplete_refs()
            res, alias_tvars, depends_on, qualified_tvars = self.analyze_alias(
                lvalue.name, rvalue, allow_placeholder=True
            )
            if not res:
                return False
            if not self.options.disable_recursive_aliases and not self.is_func_scope():
                # Only marking incomplete for top-level placeholders makes recursive aliases like
                # `A = Sequence[str | A]` valid here, similar to how we treat base classes in class
                # definitions, allowing `class str(Sequence[str]): ...`
                incomplete_target = isinstance(res, ProperType) and isinstance(
                    res, PlaceholderType
                )
            else:
                incomplete_target = has_placeholder(res)
            if self.found_incomplete_ref(tag) or incomplete_target:
                # Since we have got here, we know this must be a type alias (incomplete refs
                # may appear in nested positions), therefore use becomes_typeinfo=True.
                self.mark_incomplete(lvalue.name, rvalue, becomes_typeinfo=True)
                return True
        self.add_type_alias_deps(depends_on)
        # In addition to the aliases used, we add deps on unbound
        # type variables, since they are erased from target type.
        self.add_type_alias_deps(qualified_tvars)
        # The above are only direct deps on other aliases.
        # For subscripted aliases, type deps from expansion are added in deps.py
        # (because the type is stored).
        check_for_explicit_any(res, self.options, self.is_typeshed_stub_file, self.msg, context=s)
        # When this type alias gets "inlined", the Any is not explicit anymore,
        # so we need to replace it with non-explicit Anys.
        res = make_any_non_explicit(res)
        # Note: with the new (lazy) type alias representation we only need to set no_args to True
        # if the expected number of arguments is non-zero, so that aliases like A = List work.
        # However, eagerly expanding aliases like Text = str is a nice performance optimization.
        no_args = isinstance(res, Instance) and not res.args  # type: ignore[misc]
        fix_instance_types(res, self.fail, self.note, self.options)
        # Aliases defined within functions can't be accessed outside
        # the function, since the symbol table will no longer
        # exist. Work around by expanding them eagerly when used.
        eager = self.is_func_scope()
        alias_node = TypeAlias(
            res,
            self.qualified_name(lvalue.name),
            s.line,
            s.column,
            alias_tvars=alias_tvars,
            no_args=no_args,
            eager=eager,
        )
        if isinstance(s.rvalue, (IndexExpr, CallExpr, OpExpr)) and (
            not isinstance(rvalue, OpExpr)
            or (self.options.python_version &gt;= (3, 10) or self.is_stub_file)
        ):
            # Note: CallExpr is for "void = type(None)" and OpExpr is for "X | Y" union syntax.
            s.rvalue.analyzed = TypeAliasExpr(alias_node)
            s.rvalue.analyzed.line = s.line
            # we use the column from resulting target, to get better location for errors
            s.rvalue.analyzed.column = res.column
        elif isinstance(s.rvalue, RefExpr):
            s.rvalue.is_alias_rvalue = True

        if existing:
            # An alias gets updated.
            updated = False
            if isinstance(existing.node, TypeAlias):
                if existing.node.target != res:
                    # Copy expansion to the existing alias, this matches how we update base classes
                    # for a TypeInfo _in place_ if there are nested placeholders.
                    existing.node.target = res
                    existing.node.alias_tvars = alias_tvars
                    existing.node.no_args = no_args
                    updated = True
            else:
                # Otherwise just replace existing placeholder with type alias.
                existing.node = alias_node
                updated = True
            if updated:
                if self.final_iteration:
                    self.cannot_resolve_name(lvalue.name, "name", s)
                    return True
                else:
                    # We need to defer so that this change can get propagated to base classes.
                    self.defer(s, force_progress=True)
        else:
            self.add_symbol(lvalue.name, alias_node, s)
        if isinstance(rvalue, RefExpr) and isinstance(rvalue.node, TypeAlias):
            alias_node.normalized = rvalue.node.normalized
        current_node = existing.node if existing else alias_node
        assert isinstance(current_node, TypeAlias)
        self.disable_invalid_recursive_aliases(s, current_node)
        if self.is_class_scope():
            assert self.type is not None
            if self.type.is_protocol:
                self.fail("Type aliases are prohibited in protocol bodies", s)
                if not lvalue.name[0].isupper():
                    self.note("Use variable annotation syntax to define protocol members", s)
        return True

</t>
<t tx="ekr.20230831011820.1218">    def disable_invalid_recursive_aliases(
        self, s: AssignmentStmt, current_node: TypeAlias
    ) -&gt; None:
        """Prohibit and fix recursive type aliases that are invalid/unsupported."""
        messages = []
        if is_invalid_recursive_alias({current_node}, current_node.target):
            target = (
                "tuple" if isinstance(get_proper_type(current_node.target), TupleType) else "union"
            )
            messages.append(f"Invalid recursive alias: a {target} item of itself")
        if detect_diverging_alias(
            current_node, current_node.target, self.lookup_qualified, self.tvar_scope
        ):
            messages.append("Invalid recursive alias: type variable nesting on right hand side")
        if messages:
            current_node.target = AnyType(TypeOfAny.from_error)
            s.invalid_recursive_alias = True
        for msg in messages:
            self.fail(msg, s.rvalue)

</t>
<t tx="ekr.20230831011820.1219">    def analyze_lvalue(
        self,
        lval: Lvalue,
        nested: bool = False,
        explicit_type: bool = False,
        is_final: bool = False,
        escape_comprehensions: bool = False,
        has_explicit_value: bool = False,
    ) -&gt; None:
        """Analyze an lvalue or assignment target.

        Args:
            lval: The target lvalue
            nested: If true, the lvalue is within a tuple or list lvalue expression
            explicit_type: Assignment has type annotation
            escape_comprehensions: If we are inside a comprehension, set the variable
                in the enclosing scope instead. This implements
                https://www.python.org/dev/peps/pep-0572/#scope-of-the-target
        """
        if escape_comprehensions:
            assert isinstance(lval, NameExpr), "assignment expression target must be NameExpr"
        if isinstance(lval, NameExpr):
            self.analyze_name_lvalue(
                lval,
                explicit_type,
                is_final,
                escape_comprehensions,
                has_explicit_value=has_explicit_value,
            )
        elif isinstance(lval, MemberExpr):
            self.analyze_member_lvalue(lval, explicit_type, is_final, has_explicit_value)
            if explicit_type and not self.is_self_member_ref(lval):
                self.fail("Type cannot be declared in assignment to non-self attribute", lval)
        elif isinstance(lval, IndexExpr):
            if explicit_type:
                self.fail("Unexpected type declaration", lval)
            lval.accept(self)
        elif isinstance(lval, TupleExpr):
            self.analyze_tuple_or_list_lvalue(lval, explicit_type)
        elif isinstance(lval, StarExpr):
            if nested:
                self.analyze_lvalue(lval.expr, nested, explicit_type)
            else:
                self.fail("Starred assignment target must be in a list or tuple", lval)
        else:
            self.fail("Invalid assignment target", lval)

</t>
<t tx="ekr.20230831011820.122">    def first_argument_for_super_must_be_type(self, actual: Type, context: Context) -&gt; None:
        actual = get_proper_type(actual)
        if isinstance(actual, Instance):
            # Don't include type of instance, because it can look confusingly like a type
            # object.
            type_str = "a non-type instance"
        else:
            type_str = format_type(actual, self.options)
        self.fail(
            f'Argument 1 for "super" must be a type object; got {type_str}',
            context,
            code=codes.ARG_TYPE,
        )

</t>
<t tx="ekr.20230831011820.1220">    def analyze_name_lvalue(
        self,
        lvalue: NameExpr,
        explicit_type: bool,
        is_final: bool,
        escape_comprehensions: bool,
        has_explicit_value: bool,
    ) -&gt; None:
        """Analyze an lvalue that targets a name expression.

        Arguments are similar to "analyze_lvalue".
        """
        if lvalue.node:
            # This has been bound already in a previous iteration.
            return

        name = lvalue.name
        if self.is_alias_for_final_name(name):
            if is_final:
                self.fail("Cannot redefine an existing name as final", lvalue)
            else:
                self.msg.cant_assign_to_final(name, self.type is not None, lvalue)

        kind = self.current_symbol_kind()
        names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
        existing = names.get(name)

        outer = self.is_global_or_nonlocal(name)
        if kind == MDEF and isinstance(self.type, TypeInfo) and self.type.is_enum:
            # Special case: we need to be sure that `Enum` keys are unique.
            if existing is not None and not isinstance(existing.node, PlaceholderNode):
                self.fail(
                    'Attempted to reuse member name "{}" in Enum definition "{}"'.format(
                        name, self.type.name
                    ),
                    lvalue,
                )

        if (not existing or isinstance(existing.node, PlaceholderNode)) and not outer:
            # Define new variable.
            var = self.make_name_lvalue_var(lvalue, kind, not explicit_type, has_explicit_value)
            added = self.add_symbol(name, var, lvalue, escape_comprehensions=escape_comprehensions)
            # Only bind expression if we successfully added name to symbol table.
            if added:
                lvalue.is_new_def = True
                lvalue.is_inferred_def = True
                lvalue.kind = kind
                lvalue.node = var
                if kind == GDEF:
                    lvalue.fullname = var._fullname
                else:
                    lvalue.fullname = lvalue.name
                if self.is_func_scope():
                    if unmangle(name) == "_":
                        # Special case for assignment to local named '_': always infer 'Any'.
                        typ = AnyType(TypeOfAny.special_form)
                        self.store_declared_types(lvalue, typ)
            if is_final and self.is_final_redefinition(kind, name):
                self.fail("Cannot redefine an existing name as final", lvalue)
        else:
            self.make_name_lvalue_point_to_existing_def(lvalue, explicit_type, is_final)

</t>
<t tx="ekr.20230831011820.1221">    def is_final_redefinition(self, kind: int, name: str) -&gt; bool:
        if kind == GDEF:
            return self.is_mangled_global(name) and not self.is_initial_mangled_global(name)
        elif kind == MDEF and self.type:
            return unmangle(name) + "'" in self.type.names
        return False

</t>
<t tx="ekr.20230831011820.1222">    def is_alias_for_final_name(self, name: str) -&gt; bool:
        if self.is_func_scope():
            if not name.endswith("'"):
                # Not a mangled name -- can't be an alias
                return False
            name = unmangle(name)
            assert self.locals[-1] is not None, "No locals at function scope"
            existing = self.locals[-1].get(name)
            return existing is not None and is_final_node(existing.node)
        elif self.type is not None:
            orig_name = unmangle(name) + "'"
            if name == orig_name:
                return False
            existing = self.type.names.get(orig_name)
            return existing is not None and is_final_node(existing.node)
        else:
            orig_name = unmangle(name) + "'"
            if name == orig_name:
                return False
            existing = self.globals.get(orig_name)
            return existing is not None and is_final_node(existing.node)

</t>
<t tx="ekr.20230831011820.1223">    def make_name_lvalue_var(
        self, lvalue: NameExpr, kind: int, inferred: bool, has_explicit_value: bool
    ) -&gt; Var:
        """Return a Var node for an lvalue that is a name expression."""
        name = lvalue.name
        v = Var(name)
        v.set_line(lvalue)
        v.is_inferred = inferred
        if kind == MDEF:
            assert self.type is not None
            v.info = self.type
            v.is_initialized_in_class = True
            v.allow_incompatible_override = name in ALLOW_INCOMPATIBLE_OVERRIDE
        if kind != LDEF:
            v._fullname = self.qualified_name(name)
        else:
            # fullanme should never stay None
            v._fullname = name
        v.is_ready = False  # Type not inferred yet
        v.has_explicit_value = has_explicit_value
        return v

</t>
<t tx="ekr.20230831011820.1224">    def make_name_lvalue_point_to_existing_def(
        self, lval: NameExpr, explicit_type: bool, is_final: bool
    ) -&gt; None:
        """Update an lvalue to point to existing definition in the same scope.

        Arguments are similar to "analyze_lvalue".

        Assume that an existing name exists.
        """
        if is_final:
            # Redefining an existing name with final is always an error.
            self.fail("Cannot redefine an existing name as final", lval)
        original_def = self.lookup(lval.name, lval, suppress_errors=True)
        if original_def is None and self.type and not self.is_func_scope():
            # Workaround to allow "x, x = ..." in class body.
            original_def = self.type.get(lval.name)
        if explicit_type:
            # Don't re-bind if there is a type annotation.
            self.name_already_defined(lval.name, lval, original_def)
        else:
            # Bind to an existing name.
            if original_def:
                self.bind_name_expr(lval, original_def)
            else:
                self.name_not_defined(lval.name, lval)
            self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20230831011820.1225">    def analyze_tuple_or_list_lvalue(self, lval: TupleExpr, explicit_type: bool = False) -&gt; None:
        """Analyze an lvalue or assignment target that is a list or tuple."""
        items = lval.items
        star_exprs = [item for item in items if isinstance(item, StarExpr)]

        if len(star_exprs) &gt; 1:
            self.fail("Two starred expressions in assignment", lval)
        else:
            if len(star_exprs) == 1:
                star_exprs[0].valid = True
            for i in items:
                self.analyze_lvalue(
                    lval=i,
                    nested=True,
                    explicit_type=explicit_type,
                    # Lists and tuples always have explicit values defined:
                    # `a, b, c = value`
                    has_explicit_value=True,
                )

</t>
<t tx="ekr.20230831011820.1226">    def analyze_member_lvalue(
        self, lval: MemberExpr, explicit_type: bool, is_final: bool, has_explicit_value: bool
    ) -&gt; None:
        """Analyze lvalue that is a member expression.

        Arguments:
            lval: The target lvalue
            explicit_type: Assignment has type annotation
            is_final: Is the target final
        """
        if lval.node:
            # This has been bound already in a previous iteration.
            return
        lval.accept(self)
        if self.is_self_member_ref(lval):
            assert self.type, "Self member outside a class"
            cur_node = self.type.names.get(lval.name)
            node = self.type.get(lval.name)
            if cur_node and is_final:
                # Overrides will be checked in type checker.
                self.fail("Cannot redefine an existing name as final", lval)
            # On first encounter with this definition, if this attribute was defined before
            # with an inferred type and it's marked with an explicit type now, give an error.
            if (
                not lval.node
                and cur_node
                and isinstance(cur_node.node, Var)
                and cur_node.node.is_inferred
                and explicit_type
            ):
                self.attribute_already_defined(lval.name, lval, cur_node)
            if self.type.is_protocol and has_explicit_value and cur_node is not None:
                # Make this variable non-abstract, it would be safer to do this only if we
                # are inside __init__, but we do this always to preserve historical behaviour.
                if isinstance(cur_node.node, Var):
                    cur_node.node.is_abstract_var = False
            if (
                # If the attribute of self is not defined, create a new Var, ...
                node is None
                # ... or if it is defined as abstract in a *superclass*.
                or (cur_node is None and isinstance(node.node, Var) and node.node.is_abstract_var)
                # ... also an explicit declaration on self also creates a new Var.
                # Note that `explicit_type` might have been erased for bare `Final`,
                # so we also check if `is_final` is passed.
                or (cur_node is None and (explicit_type or is_final))
            ):
                if self.type.is_protocol and node is None:
                    self.fail("Protocol members cannot be defined via assignment to self", lval)
                else:
                    # Implicit attribute definition in __init__.
                    lval.is_new_def = True
                    lval.is_inferred_def = True
                    v = Var(lval.name)
                    v.set_line(lval)
                    v._fullname = self.qualified_name(lval.name)
                    v.info = self.type
                    v.is_ready = False
                    v.explicit_self_type = explicit_type or is_final
                    lval.def_var = v
                    lval.node = v
                    # TODO: should we also set lval.kind = MDEF?
                    self.type.names[lval.name] = SymbolTableNode(MDEF, v, implicit=True)
        self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20230831011820.1227">    def is_self_member_ref(self, memberexpr: MemberExpr) -&gt; bool:
        """Does memberexpr to refer to an attribute of self?"""
        if not isinstance(memberexpr.expr, NameExpr):
            return False
        node = memberexpr.expr.node
        return isinstance(node, Var) and node.is_self

</t>
<t tx="ekr.20230831011820.1228">    def check_lvalue_validity(self, node: Expression | SymbolNode | None, ctx: Context) -&gt; None:
        if isinstance(node, TypeVarExpr):
            self.fail("Invalid assignment target", ctx)
        elif isinstance(node, TypeInfo):
            self.fail(message_registry.CANNOT_ASSIGN_TO_TYPE, ctx)

</t>
<t tx="ekr.20230831011820.1229">    def store_declared_types(self, lvalue: Lvalue, typ: Type) -&gt; None:
        if isinstance(lvalue, RefExpr):
            lvalue.is_inferred_def = False
            if isinstance(lvalue.node, Var):
                var = lvalue.node
                var.type = typ
                var.is_ready = True
                typ = get_proper_type(typ)
                if (
                    var.is_final
                    and isinstance(typ, Instance)
                    and typ.last_known_value
                    and (not self.type or not self.type.is_enum)
                ):
                    var.final_value = typ.last_known_value.value
            # If node is not a variable, we'll catch it elsewhere.
        elif isinstance(lvalue, TupleExpr):
            typ = get_proper_type(typ)
            if isinstance(typ, TupleType):
                if len(lvalue.items) != len(typ.items):
                    self.fail("Incompatible number of tuple items", lvalue)
                    return
                for item, itemtype in zip(lvalue.items, typ.items):
                    self.store_declared_types(item, itemtype)
            else:
                self.fail("Tuple type expected for multiple variables", lvalue)
        elif isinstance(lvalue, StarExpr):
            # Historical behavior for the old parser
            self.store_declared_types(lvalue.expr, typ)
        else:
            # This has been flagged elsewhere as an error, so just ignore here.
            pass

</t>
<t tx="ekr.20230831011820.123">    def unsafe_super(self, method: str, cls: str, ctx: Context) -&gt; None:
        self.fail(
            'Call to abstract method "{}" of "{}" with trivial body'
            " via super() is unsafe".format(method, cls),
            ctx,
            code=codes.SAFE_SUPER,
        )

</t>
<t tx="ekr.20230831011820.1230">    def process_typevar_declaration(self, s: AssignmentStmt) -&gt; bool:
        """Check if s declares a TypeVar; it yes, store it in symbol table.

        Return True if this looks like a type variable declaration (but maybe
        with errors), otherwise return False.
        """
        call = self.get_typevarlike_declaration(s, ("typing.TypeVar", "typing_extensions.TypeVar"))
        if not call:
            return False

        name = self.extract_typevarlike_name(s, call)
        if name is None:
            return False

        # Constraining types
        n_values = call.arg_kinds[1:].count(ARG_POS)
        values = self.analyze_value_types(call.args[1 : 1 + n_values])

        res = self.process_typevar_parameters(
            call.args[1 + n_values :],
            call.arg_names[1 + n_values :],
            call.arg_kinds[1 + n_values :],
            n_values,
            s,
        )
        if res is None:
            return False
        variance, upper_bound, default = res

        existing = self.current_symbol_table().get(name)
        if existing and not (
            isinstance(existing.node, PlaceholderNode)
            or
            # Also give error for another type variable with the same name.
            (isinstance(existing.node, TypeVarExpr) and existing.node is call.analyzed)
        ):
            self.fail(f'Cannot redefine "{name}" as a type variable', s)
            return False

        if self.options.disallow_any_unimported:
            for idx, constraint in enumerate(values, start=1):
                if has_any_from_unimported_type(constraint):
                    prefix = f"Constraint {idx}"
                    self.msg.unimported_type_becomes_any(prefix, constraint, s)

            if has_any_from_unimported_type(upper_bound):
                prefix = "Upper bound of type variable"
                self.msg.unimported_type_becomes_any(prefix, upper_bound, s)

        for t in values + [upper_bound, default]:
            check_for_explicit_any(
                t, self.options, self.is_typeshed_stub_file, self.msg, context=s
            )

        # mypyc suppresses making copies of a function to check each
        # possible type, so set the upper bound to Any to prevent that
        # from causing errors.
        if values and self.options.mypyc:
            upper_bound = AnyType(TypeOfAny.implementation_artifact)

        # Yes, it's a valid type variable definition! Add it to the symbol table.
        if not call.analyzed:
            type_var = TypeVarExpr(
                name, self.qualified_name(name), values, upper_bound, default, variance
            )
            type_var.line = call.line
            call.analyzed = type_var
            updated = True
        else:
            assert isinstance(call.analyzed, TypeVarExpr)
            updated = (
                values != call.analyzed.values
                or upper_bound != call.analyzed.upper_bound
                or default != call.analyzed.default
            )
            call.analyzed.upper_bound = upper_bound
            call.analyzed.values = values
            call.analyzed.default = default
        if any(has_placeholder(v) for v in values):
            self.process_placeholder(None, "TypeVar values", s, force_progress=updated)
        elif has_placeholder(upper_bound):
            self.process_placeholder(None, "TypeVar upper bound", s, force_progress=updated)
        elif has_placeholder(default):
            self.process_placeholder(None, "TypeVar default", s, force_progress=updated)

        self.add_symbol(name, call.analyzed, s)
        return True

</t>
<t tx="ekr.20230831011820.1231">    def check_typevarlike_name(self, call: CallExpr, name: str, context: Context) -&gt; bool:
        """Checks that the name of a TypeVar or ParamSpec matches its variable."""
        name = unmangle(name)
        assert isinstance(call.callee, RefExpr)
        typevarlike_type = (
            call.callee.name if isinstance(call.callee, NameExpr) else call.callee.fullname
        )
        if len(call.args) &lt; 1:
            self.fail(f"Too few arguments for {typevarlike_type}()", context)
            return False
        if not isinstance(call.args[0], StrExpr) or not call.arg_kinds[0] == ARG_POS:
            self.fail(f"{typevarlike_type}() expects a string literal as first argument", context)
            return False
        elif call.args[0].value != name:
            msg = 'String argument 1 "{}" to {}(...) does not match variable name "{}"'
            self.fail(msg.format(call.args[0].value, typevarlike_type, name), context)
            return False
        return True

</t>
<t tx="ekr.20230831011820.1232">    def get_typevarlike_declaration(
        self, s: AssignmentStmt, typevarlike_types: tuple[str, ...]
    ) -&gt; CallExpr | None:
        """Returns the call expression if `s` is a declaration of `typevarlike_type`
        (TypeVar or ParamSpec), or None otherwise.
        """
        if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], NameExpr):
            return None
        if not isinstance(s.rvalue, CallExpr):
            return None
        call = s.rvalue
        callee = call.callee
        if not isinstance(callee, RefExpr):
            return None
        if callee.fullname not in typevarlike_types:
            return None
        return call

</t>
<t tx="ekr.20230831011820.1233">    def process_typevar_parameters(
        self,
        args: list[Expression],
        names: list[str | None],
        kinds: list[ArgKind],
        num_values: int,
        context: Context,
    ) -&gt; tuple[int, Type, Type] | None:
        has_values = num_values &gt; 0
        covariant = False
        contravariant = False
        upper_bound: Type = self.object_type()
        default: Type = AnyType(TypeOfAny.from_omitted_generics)
        for param_value, param_name, param_kind in zip(args, names, kinds):
            if not param_kind.is_named():
                self.fail(message_registry.TYPEVAR_UNEXPECTED_ARGUMENT, context)
                return None
            if param_name == "covariant":
                if isinstance(param_value, NameExpr) and param_value.name in ("True", "False"):
                    covariant = param_value.name == "True"
                else:
                    self.fail(message_registry.TYPEVAR_VARIANCE_DEF.format("covariant"), context)
                    return None
            elif param_name == "contravariant":
                if isinstance(param_value, NameExpr) and param_value.name in ("True", "False"):
                    contravariant = param_value.name == "True"
                else:
                    self.fail(
                        message_registry.TYPEVAR_VARIANCE_DEF.format("contravariant"), context
                    )
                    return None
            elif param_name == "bound":
                if has_values:
                    self.fail("TypeVar cannot have both values and an upper bound", context)
                    return None
                tv_arg = self.get_typevarlike_argument("TypeVar", param_name, param_value, context)
                if tv_arg is None:
                    return None
                upper_bound = tv_arg
            elif param_name == "default":
                tv_arg = self.get_typevarlike_argument(
                    "TypeVar", param_name, param_value, context, allow_unbound_tvars=True
                )
                default = tv_arg or AnyType(TypeOfAny.from_error)
            elif param_name == "values":
                # Probably using obsolete syntax with values=(...). Explain the current syntax.
                self.fail('TypeVar "values" argument not supported', context)
                self.fail(
                    "Use TypeVar('T', t, ...) instead of TypeVar('T', values=(t, ...))", context
                )
                return None
            else:
                self.fail(
                    f'{message_registry.TYPEVAR_UNEXPECTED_ARGUMENT}: "{param_name}"', context
                )
                return None

        if covariant and contravariant:
            self.fail("TypeVar cannot be both covariant and contravariant", context)
            return None
        elif num_values == 1:
            self.fail("TypeVar cannot have only a single constraint", context)
            return None
        elif covariant:
            variance = COVARIANT
        elif contravariant:
            variance = CONTRAVARIANT
        else:
            variance = INVARIANT
        return variance, upper_bound, default

</t>
<t tx="ekr.20230831011820.1234">    def get_typevarlike_argument(
        self,
        typevarlike_name: str,
        param_name: str,
        param_value: Expression,
        context: Context,
        *,
        allow_unbound_tvars: bool = False,
        allow_param_spec_literals: bool = False,
        allow_unpack: bool = False,
        report_invalid_typevar_arg: bool = True,
    ) -&gt; ProperType | None:
        try:
            # We want to use our custom error message below, so we suppress
            # the default error message for invalid types here.
            analyzed = self.expr_to_analyzed_type(
                param_value,
                allow_placeholder=True,
                report_invalid_types=False,
                allow_unbound_tvars=allow_unbound_tvars,
                allow_param_spec_literals=allow_param_spec_literals,
                allow_unpack=allow_unpack,
            )
            if analyzed is None:
                # Type variables are special: we need to place them in the symbol table
                # soon, even if upper bound is not ready yet. Otherwise avoiding
                # a "deadlock" in this common pattern would be tricky:
                #     T = TypeVar('T', bound=Custom[Any])
                #     class Custom(Generic[T]):
                #         ...
                analyzed = PlaceholderType(None, [], context.line)
            typ = get_proper_type(analyzed)
            if report_invalid_typevar_arg and isinstance(typ, AnyType) and typ.is_from_error:
                self.fail(
                    message_registry.TYPEVAR_ARG_MUST_BE_TYPE.format(typevarlike_name, param_name),
                    param_value,
                )
                # Note: we do not return 'None' here -- we want to continue
                # using the AnyType.
            return typ
        except TypeTranslationError:
            if report_invalid_typevar_arg:
                self.fail(
                    message_registry.TYPEVAR_ARG_MUST_BE_TYPE.format(typevarlike_name, param_name),
                    param_value,
                )
            return None

</t>
<t tx="ekr.20230831011820.1235">    def extract_typevarlike_name(self, s: AssignmentStmt, call: CallExpr) -&gt; str | None:
        if not call:
            return None

        lvalue = s.lvalues[0]
        assert isinstance(lvalue, NameExpr)
        if s.type:
            self.fail("Cannot declare the type of a TypeVar or similar construct", s)
            return None

        if not self.check_typevarlike_name(call, lvalue.name, s):
            return None
        return lvalue.name

</t>
<t tx="ekr.20230831011820.1236">    def process_paramspec_declaration(self, s: AssignmentStmt) -&gt; bool:
        """Checks if s declares a ParamSpec; if yes, store it in symbol table.

        Return True if this looks like a ParamSpec (maybe with errors), otherwise return False.

        In the future, ParamSpec may accept bounds and variance arguments, in which
        case more aggressive sharing of code with process_typevar_declaration should be pursued.
        """
        call = self.get_typevarlike_declaration(
            s, ("typing_extensions.ParamSpec", "typing.ParamSpec")
        )
        if not call:
            return False

        name = self.extract_typevarlike_name(s, call)
        if name is None:
            return False

        n_values = call.arg_kinds[1:].count(ARG_POS)
        if n_values != 0:
            self.fail('Too many positional arguments for "ParamSpec"', s)

        default: Type = AnyType(TypeOfAny.from_omitted_generics)
        for param_value, param_name in zip(
            call.args[1 + n_values :], call.arg_names[1 + n_values :]
        ):
            if param_name == "default":
                tv_arg = self.get_typevarlike_argument(
                    "ParamSpec",
                    param_name,
                    param_value,
                    s,
                    allow_unbound_tvars=True,
                    allow_param_spec_literals=True,
                    report_invalid_typevar_arg=False,
                )
                default = tv_arg or AnyType(TypeOfAny.from_error)
                if isinstance(tv_arg, Parameters):
                    for i, arg_type in enumerate(tv_arg.arg_types):
                        typ = get_proper_type(arg_type)
                        if isinstance(typ, AnyType) and typ.is_from_error:
                            self.fail(
                                f"Argument {i} of ParamSpec default must be a type", param_value
                            )
                elif (
                    isinstance(default, AnyType)
                    and default.is_from_error
                    or not isinstance(default, (AnyType, UnboundType))
                ):
                    self.fail(
                        "The default argument to ParamSpec must be a list expression, ellipsis, or a ParamSpec",
                        param_value,
                    )
                    default = AnyType(TypeOfAny.from_error)
            else:
                # ParamSpec is different from a regular TypeVar:
                # arguments are not semantically valid. But, allowed in runtime.
                # So, we need to warn users about possible invalid usage.
                self.fail(
                    "The variance and bound arguments to ParamSpec do not have defined semantics yet",
                    s,
                )

        # PEP 612 reserves the right to define bound, covariant and contravariant arguments to
        # ParamSpec in a later PEP. If and when that happens, we should do something
        # on the lines of process_typevar_parameters

        if not call.analyzed:
            paramspec_var = ParamSpecExpr(
                name, self.qualified_name(name), self.object_type(), default, INVARIANT
            )
            paramspec_var.line = call.line
            call.analyzed = paramspec_var
            updated = True
        else:
            assert isinstance(call.analyzed, ParamSpecExpr)
            updated = default != call.analyzed.default
            call.analyzed.default = default
        if has_placeholder(default):
            self.process_placeholder(None, "ParamSpec default", s, force_progress=updated)

        self.add_symbol(name, call.analyzed, s)
        return True

</t>
<t tx="ekr.20230831011820.1237">    def process_typevartuple_declaration(self, s: AssignmentStmt) -&gt; bool:
        """Checks if s declares a TypeVarTuple; if yes, store it in symbol table.

        Return True if this looks like a TypeVarTuple (maybe with errors), otherwise return False.
        """
        call = self.get_typevarlike_declaration(
            s, ("typing_extensions.TypeVarTuple", "typing.TypeVarTuple")
        )
        if not call:
            return False

        n_values = call.arg_kinds[1:].count(ARG_POS)
        if n_values != 0:
            self.fail('Too many positional arguments for "TypeVarTuple"', s)

        default: Type = AnyType(TypeOfAny.from_omitted_generics)
        for param_value, param_name in zip(
            call.args[1 + n_values :], call.arg_names[1 + n_values :]
        ):
            if param_name == "default":
                tv_arg = self.get_typevarlike_argument(
                    "TypeVarTuple",
                    param_name,
                    param_value,
                    s,
                    allow_unbound_tvars=True,
                    report_invalid_typevar_arg=False,
                    allow_unpack=True,
                )
                default = tv_arg or AnyType(TypeOfAny.from_error)
                if not isinstance(default, UnpackType):
                    self.fail(
                        "The default argument to TypeVarTuple must be an Unpacked tuple",
                        param_value,
                    )
                    default = AnyType(TypeOfAny.from_error)
            else:
                self.fail(f'Unexpected keyword argument "{param_name}" for "TypeVarTuple"', s)

        if not self.incomplete_feature_enabled(TYPE_VAR_TUPLE, s):
            return False

        name = self.extract_typevarlike_name(s, call)
        if name is None:
            return False

        # PEP 646 does not specify the behavior of variance, constraints, or bounds.
        if not call.analyzed:
            tuple_fallback = self.named_type("builtins.tuple", [self.object_type()])
            typevartuple_var = TypeVarTupleExpr(
                name,
                self.qualified_name(name),
                self.object_type(),
                tuple_fallback,
                default,
                INVARIANT,
            )
            typevartuple_var.line = call.line
            call.analyzed = typevartuple_var
            updated = True
        else:
            assert isinstance(call.analyzed, TypeVarTupleExpr)
            updated = default != call.analyzed.default
            call.analyzed.default = default
        if has_placeholder(default):
            self.process_placeholder(None, "TypeVarTuple default", s, force_progress=updated)

        self.add_symbol(name, call.analyzed, s)
        return True

</t>
<t tx="ekr.20230831011820.1238">    def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
        if self.is_func_scope() and not self.type and "@" not in name:
            name += "@" + str(line)
        class_def = ClassDef(name, Block([]))
        if self.is_func_scope() and not self.type:
            # Full names of generated classes should always be prefixed with the module names
            # even if they are nested in a function, since these classes will be (de-)serialized.
            # (Note that the caller should append @line to the name to avoid collisions.)
            # TODO: clean this up, see #6422.
            class_def.fullname = self.cur_mod_id + "." + self.qualified_name(name)
        else:
            class_def.fullname = self.qualified_name(name)

        info = TypeInfo(SymbolTable(), class_def, self.cur_mod_id)
        class_def.info = info
        mro = basetype_or_fallback.type.mro
        if not mro:
            # Probably an error, we should not crash so generate something meaningful.
            mro = [basetype_or_fallback.type, self.object_type().type]
        info.mro = [info] + mro
        info.bases = [basetype_or_fallback]
        return info

</t>
<t tx="ekr.20230831011820.1239">    def analyze_value_types(self, items: list[Expression]) -&gt; list[Type]:
        """Analyze types from values expressions in type variable definition."""
        result: list[Type] = []
        for node in items:
            try:
                analyzed = self.anal_type(
                    self.expr_to_unanalyzed_type(node), allow_placeholder=True
                )
                if analyzed is None:
                    # Type variables are special: we need to place them in the symbol table
                    # soon, even if some value is not ready yet, see process_typevar_parameters()
                    # for an example.
                    analyzed = PlaceholderType(None, [], node.line)
                result.append(analyzed)
            except TypeTranslationError:
                self.fail("Type expected", node)
                result.append(AnyType(TypeOfAny.from_error))
        return result

</t>
<t tx="ekr.20230831011820.124">    def too_few_string_formatting_arguments(self, context: Context) -&gt; None:
        self.fail("Not enough arguments for format string", context, code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20230831011820.1240">    def check_classvar(self, s: AssignmentStmt) -&gt; None:
        """Check if assignment defines a class variable."""
        lvalue = s.lvalues[0]
        if len(s.lvalues) != 1 or not isinstance(lvalue, RefExpr):
            return
        if not s.type or not self.is_classvar(s.type):
            return
        if self.is_class_scope() and isinstance(lvalue, NameExpr):
            node = lvalue.node
            if isinstance(node, Var):
                node.is_classvar = True
            analyzed = self.anal_type(s.type)
            assert self.type is not None
            if analyzed is not None and set(get_type_vars(analyzed)) &amp; set(
                self.type.defn.type_vars
            ):
                # This means that we have a type var defined inside of a ClassVar.
                # This is not allowed by PEP526.
                # See https://github.com/python/mypy/issues/11538

                self.fail(message_registry.CLASS_VAR_WITH_TYPEVARS, s)
            if (
                analyzed is not None
                and self.type.self_type in get_type_vars(analyzed)
                and self.type.defn.type_vars
            ):
                self.fail(message_registry.CLASS_VAR_WITH_GENERIC_SELF, s)
        elif not isinstance(lvalue, MemberExpr) or self.is_self_member_ref(lvalue):
            # In case of member access, report error only when assigning to self
            # Other kinds of member assignments should be already reported
            self.fail_invalid_classvar(lvalue)

</t>
<t tx="ekr.20230831011820.1241">    def is_classvar(self, typ: Type) -&gt; bool:
        if not isinstance(typ, UnboundType):
            return False
        sym = self.lookup_qualified(typ.name, typ)
        if not sym or not sym.node:
            return False
        return sym.node.fullname == "typing.ClassVar"

</t>
<t tx="ekr.20230831011820.1242">    def is_final_type(self, typ: Type | None) -&gt; bool:
        if not isinstance(typ, UnboundType):
            return False
        sym = self.lookup_qualified(typ.name, typ)
        if not sym or not sym.node:
            return False
        return sym.node.fullname in FINAL_TYPE_NAMES

</t>
<t tx="ekr.20230831011820.1243">    def fail_invalid_classvar(self, context: Context) -&gt; None:
        self.fail(message_registry.CLASS_VAR_OUTSIDE_OF_CLASS, context)

</t>
<t tx="ekr.20230831011820.1244">    def process_module_assignment(
        self, lvals: list[Lvalue], rval: Expression, ctx: AssignmentStmt
    ) -&gt; None:
        """Propagate module references across assignments.

        Recursively handles the simple form of iterable unpacking; doesn't
        handle advanced unpacking with *rest, dictionary unpacking, etc.

        In an expression like x = y = z, z is the rval and lvals will be [x,
        y].

        """
        if isinstance(rval, (TupleExpr, ListExpr)) and all(
            isinstance(v, TupleExpr) for v in lvals
        ):
            # rval and all lvals are either list or tuple, so we are dealing
            # with unpacking assignment like `x, y = a, b`. Mypy didn't
            # understand our all(isinstance(...)), so cast them as TupleExpr
            # so mypy knows it is safe to access their .items attribute.
            seq_lvals = cast(List[TupleExpr], lvals)
            # given an assignment like:
            #     (x, y) = (m, n) = (a, b)
            # we now have:
            #     seq_lvals = [(x, y), (m, n)]
            #     seq_rval = (a, b)
            # We now zip this into:
            #     elementwise_assignments = [(a, x, m), (b, y, n)]
            # where each elementwise assignment includes one element of rval and the
            # corresponding element of each lval. Basically we unpack
            #     (x, y) = (m, n) = (a, b)
            # into elementwise assignments
            #     x = m = a
            #     y = n = b
            # and then we recursively call this method for each of those assignments.
            # If the rval and all lvals are not all of the same length, zip will just ignore
            # extra elements, so no error will be raised here; mypy will later complain
            # about the length mismatch in type-checking.
            elementwise_assignments = zip(rval.items, *[v.items for v in seq_lvals])
            for rv, *lvs in elementwise_assignments:
                self.process_module_assignment(lvs, rv, ctx)
        elif isinstance(rval, RefExpr):
            rnode = self.lookup_type_node(rval)
            if rnode and isinstance(rnode.node, MypyFile):
                for lval in lvals:
                    if not isinstance(lval, RefExpr):
                        continue
                    # respect explicitly annotated type
                    if isinstance(lval.node, Var) and lval.node.type is not None:
                        continue

                    # We can handle these assignments to locals and to self
                    if isinstance(lval, NameExpr):
                        lnode = self.current_symbol_table().get(lval.name)
                    elif isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                        assert self.type is not None
                        lnode = self.type.names.get(lval.name)
                    else:
                        continue

                    if lnode:
                        if isinstance(lnode.node, MypyFile) and lnode.node is not rnode.node:
                            assert isinstance(lval, (NameExpr, MemberExpr))
                            self.fail(
                                'Cannot assign multiple modules to name "{}" '
                                'without explicit "types.ModuleType" annotation'.format(lval.name),
                                ctx,
                            )
                        # never create module alias except on initial var definition
                        elif lval.is_inferred_def:
                            assert rnode.node is not None
                            lnode.node = rnode.node

</t>
<t tx="ekr.20230831011820.1245">    def process__all__(self, s: AssignmentStmt) -&gt; None:
        """Export names if argument is a __all__ assignment."""
        if (
            len(s.lvalues) == 1
            and isinstance(s.lvalues[0], NameExpr)
            and s.lvalues[0].name == "__all__"
            and s.lvalues[0].kind == GDEF
            and isinstance(s.rvalue, (ListExpr, TupleExpr))
        ):
            self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20230831011820.1246">    def process__deletable__(self, s: AssignmentStmt) -&gt; None:
        if not self.options.mypyc:
            return
        if (
            len(s.lvalues) == 1
            and isinstance(s.lvalues[0], NameExpr)
            and s.lvalues[0].name == "__deletable__"
            and s.lvalues[0].kind == MDEF
        ):
            rvalue = s.rvalue
            if not isinstance(rvalue, (ListExpr, TupleExpr)):
                self.fail('"__deletable__" must be initialized with a list or tuple expression', s)
                return
            items = rvalue.items
            attrs = []
            for item in items:
                if not isinstance(item, StrExpr):
                    self.fail('Invalid "__deletable__" item; string literal expected', item)
                else:
                    attrs.append(item.value)
            assert self.type
            self.type.deletable_attributes = attrs

</t>
<t tx="ekr.20230831011820.1247">    def process__slots__(self, s: AssignmentStmt) -&gt; None:
        """
        Processing ``__slots__`` if defined in type.

        See: https://docs.python.org/3/reference/datamodel.html#slots
        """
        # Later we can support `__slots__` defined as `__slots__ = other = ('a', 'b')`
        if (
            isinstance(self.type, TypeInfo)
            and len(s.lvalues) == 1
            and isinstance(s.lvalues[0], NameExpr)
            and s.lvalues[0].name == "__slots__"
            and s.lvalues[0].kind == MDEF
        ):
            # We understand `__slots__` defined as string, tuple, list, set, and dict:
            if not isinstance(s.rvalue, (StrExpr, ListExpr, TupleExpr, SetExpr, DictExpr)):
                # For example, `__slots__` can be defined as a variable,
                # we don't support it for now.
                return

            if any(p.slots is None for p in self.type.mro[1:-1]):
                # At least one type in mro (excluding `self` and `object`)
                # does not have concrete `__slots__` defined. Ignoring.
                return

            concrete_slots = True
            rvalue: list[Expression] = []
            if isinstance(s.rvalue, StrExpr):
                rvalue.append(s.rvalue)
            elif isinstance(s.rvalue, (ListExpr, TupleExpr, SetExpr)):
                rvalue.extend(s.rvalue.items)
            else:
                # We have a special treatment of `dict` with possible `{**kwargs}` usage.
                # In this case we consider all `__slots__` to be non-concrete.
                for key, _ in s.rvalue.items:
                    if concrete_slots and key is not None:
                        rvalue.append(key)
                    else:
                        concrete_slots = False

            slots = []
            for item in rvalue:
                # Special case for `'__dict__'` value:
                # when specified it will still allow any attribute assignment.
                if isinstance(item, StrExpr) and item.value != "__dict__":
                    slots.append(item.value)
                else:
                    concrete_slots = False
            if not concrete_slots:
                # Some slot items are dynamic, we don't want any false positives,
                # so, we just pretend that this type does not have any slots at all.
                return

            # We need to copy all slots from super types:
            for super_type in self.type.mro[1:-1]:
                assert super_type.slots is not None
                slots.extend(super_type.slots)
            self.type.slots = set(slots)

</t>
<t tx="ekr.20230831011820.1248">    #
    # Misc statements
    #

    def visit_block(self, b: Block) -&gt; None:
        if b.is_unreachable:
            return
        self.block_depth[-1] += 1
        for s in b.body:
            self.accept(s)
        self.block_depth[-1] -= 1

</t>
<t tx="ekr.20230831011820.1249">    def visit_block_maybe(self, b: Block | None) -&gt; None:
        if b:
            self.visit_block(b)

</t>
<t tx="ekr.20230831011820.125">    def too_many_string_formatting_arguments(self, context: Context) -&gt; None:
        self.fail(
            "Not all arguments converted during string formatting",
            context,
            code=codes.STRING_FORMATTING,
        )

</t>
<t tx="ekr.20230831011820.1250">    def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
        self.statement = s
        s.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1251">    def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
        self.statement = s
        if not self.is_func_scope():
            self.fail('"return" outside function', s)
        if s.expr:
            s.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1252">    def visit_raise_stmt(self, s: RaiseStmt) -&gt; None:
        self.statement = s
        if s.expr:
            s.expr.accept(self)
        if s.from_expr:
            s.from_expr.accept(self)

</t>
<t tx="ekr.20230831011820.1253">    def visit_assert_stmt(self, s: AssertStmt) -&gt; None:
        self.statement = s
        if s.expr:
            s.expr.accept(self)
        if s.msg:
            s.msg.accept(self)

</t>
<t tx="ekr.20230831011820.1254">    def visit_operator_assignment_stmt(self, s: OperatorAssignmentStmt) -&gt; None:
        self.statement = s
        s.lvalue.accept(self)
        s.rvalue.accept(self)
        if (
            isinstance(s.lvalue, NameExpr)
            and s.lvalue.name == "__all__"
            and s.lvalue.kind == GDEF
            and isinstance(s.rvalue, (ListExpr, TupleExpr))
        ):
            self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20230831011820.1255">    def visit_while_stmt(self, s: WhileStmt) -&gt; None:
        self.statement = s
        s.expr.accept(self)
        self.loop_depth[-1] += 1
        s.body.accept(self)
        self.loop_depth[-1] -= 1
        self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20230831011820.1256">    def visit_for_stmt(self, s: ForStmt) -&gt; None:
        if s.is_async:
            if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
                self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

        self.statement = s
        s.expr.accept(self)

        # Bind index variables and check if they define new names.
        self.analyze_lvalue(s.index, explicit_type=s.index_type is not None)
        if s.index_type:
            if self.is_classvar(s.index_type):
                self.fail_invalid_classvar(s.index)
            allow_tuple_literal = isinstance(s.index, TupleExpr)
            analyzed = self.anal_type(s.index_type, allow_tuple_literal=allow_tuple_literal)
            if analyzed is not None:
                self.store_declared_types(s.index, analyzed)
                s.index_type = analyzed

        self.loop_depth[-1] += 1
        self.visit_block(s.body)
        self.loop_depth[-1] -= 1

        self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20230831011820.1257">    def visit_break_stmt(self, s: BreakStmt) -&gt; None:
        self.statement = s
        if self.loop_depth[-1] == 0:
            self.fail('"break" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20230831011820.1258">    def visit_continue_stmt(self, s: ContinueStmt) -&gt; None:
        self.statement = s
        if self.loop_depth[-1] == 0:
            self.fail('"continue" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20230831011820.1259">    def visit_if_stmt(self, s: IfStmt) -&gt; None:
        self.statement = s
        infer_reachability_of_if_statement(s, self.options)
        for i in range(len(s.expr)):
            s.expr[i].accept(self)
            self.visit_block(s.body[i])
        self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20230831011820.126">    def unsupported_placeholder(self, placeholder: str, context: Context) -&gt; None:
        self.fail(
            f'Unsupported format character "{placeholder}"', context, code=codes.STRING_FORMATTING
        )

</t>
<t tx="ekr.20230831011820.1260">    def visit_try_stmt(self, s: TryStmt) -&gt; None:
        self.statement = s
        self.analyze_try_stmt(s, self)

</t>
<t tx="ekr.20230831011820.1261">    def analyze_try_stmt(self, s: TryStmt, visitor: NodeVisitor[None]) -&gt; None:
        s.body.accept(visitor)
        for type, var, handler in zip(s.types, s.vars, s.handlers):
            if type:
                type.accept(visitor)
            if var:
                self.analyze_lvalue(var)
            handler.accept(visitor)
        if s.else_body:
            s.else_body.accept(visitor)
        if s.finally_body:
            s.finally_body.accept(visitor)

</t>
<t tx="ekr.20230831011820.1262">    def visit_with_stmt(self, s: WithStmt) -&gt; None:
        self.statement = s
        types: list[Type] = []

        if s.is_async:
            if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
                self.fail(message_registry.ASYNC_WITH_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

        if s.unanalyzed_type:
            assert isinstance(s.unanalyzed_type, ProperType)
            actual_targets = [t for t in s.target if t is not None]
            if len(actual_targets) == 0:
                # We have a type for no targets
                self.fail('Invalid type comment: "with" statement has no targets', s)
            elif len(actual_targets) == 1:
                # We have one target and one type
                types = [s.unanalyzed_type]
            elif isinstance(s.unanalyzed_type, TupleType):
                # We have multiple targets and multiple types
                if len(actual_targets) == len(s.unanalyzed_type.items):
                    types = s.unanalyzed_type.items.copy()
                else:
                    # But it's the wrong number of items
                    self.fail('Incompatible number of types for "with" targets', s)
            else:
                # We have multiple targets and one type
                self.fail('Multiple types expected for multiple "with" targets', s)

        new_types: list[Type] = []
        for e, n in zip(s.expr, s.target):
            e.accept(self)
            if n:
                self.analyze_lvalue(n, explicit_type=s.unanalyzed_type is not None)

                # Since we have a target, pop the next type from types
                if types:
                    t = types.pop(0)
                    if self.is_classvar(t):
                        self.fail_invalid_classvar(n)
                    allow_tuple_literal = isinstance(n, TupleExpr)
                    analyzed = self.anal_type(t, allow_tuple_literal=allow_tuple_literal)
                    if analyzed is not None:
                        # TODO: Deal with this better
                        new_types.append(analyzed)
                        self.store_declared_types(n, analyzed)

        s.analyzed_types = new_types

        self.visit_block(s.body)

</t>
<t tx="ekr.20230831011820.1263">    def visit_del_stmt(self, s: DelStmt) -&gt; None:
        self.statement = s
        s.expr.accept(self)
        if not self.is_valid_del_target(s.expr):
            self.fail("Invalid delete target", s)

</t>
<t tx="ekr.20230831011820.1264">    def is_valid_del_target(self, s: Expression) -&gt; bool:
        if isinstance(s, (IndexExpr, NameExpr, MemberExpr)):
            return True
        elif isinstance(s, (TupleExpr, ListExpr)):
            return all(self.is_valid_del_target(item) for item in s.items)
        else:
            return False

</t>
<t tx="ekr.20230831011820.1265">    def visit_global_decl(self, g: GlobalDecl) -&gt; None:
        self.statement = g
        for name in g.names:
            if name in self.nonlocal_decls[-1]:
                self.fail(f'Name "{name}" is nonlocal and global', g)
            self.global_decls[-1].add(name)

</t>
<t tx="ekr.20230831011820.1266">    def visit_nonlocal_decl(self, d: NonlocalDecl) -&gt; None:
        self.statement = d
        if self.is_module_scope():
            self.fail("nonlocal declaration not allowed at module level", d)
        else:
            for name in d.names:
                for table in reversed(self.locals[:-1]):
                    if table is not None and name in table:
                        break
                else:
                    self.fail(f'No binding for nonlocal "{name}" found', d)

                if self.locals[-1] is not None and name in self.locals[-1]:
                    self.fail(
                        'Name "{}" is already defined in local '
                        "scope before nonlocal declaration".format(name),
                        d,
                    )

                if name in self.global_decls[-1]:
                    self.fail(f'Name "{name}" is nonlocal and global', d)
                self.nonlocal_decls[-1].add(name)

</t>
<t tx="ekr.20230831011820.1267">    def visit_match_stmt(self, s: MatchStmt) -&gt; None:
        self.statement = s
        infer_reachability_of_match_statement(s, self.options)
        s.subject.accept(self)
        for i in range(len(s.patterns)):
            s.patterns[i].accept(self)
            guard = s.guards[i]
            if guard is not None:
                guard.accept(self)
            self.visit_block(s.bodies[i])

</t>
<t tx="ekr.20230831011820.1268">    #
    # Expressions
    #

    def visit_name_expr(self, expr: NameExpr) -&gt; None:
        n = self.lookup(expr.name, expr)
        if n:
            self.bind_name_expr(expr, n)

</t>
<t tx="ekr.20230831011820.1269">    def bind_name_expr(self, expr: NameExpr, sym: SymbolTableNode) -&gt; None:
        """Bind name expression to a symbol table node."""
        if isinstance(sym.node, TypeVarExpr) and self.tvar_scope.get_binding(sym):
            self.fail(
                '"{}" is a type variable and only valid in type ' "context".format(expr.name), expr
            )
        elif isinstance(sym.node, PlaceholderNode):
            self.process_placeholder(expr.name, "name", expr)
        else:
            expr.kind = sym.kind
            expr.node = sym.node
            expr.fullname = sym.fullname or ""

</t>
<t tx="ekr.20230831011820.127">    def string_interpolation_with_star_and_key(self, context: Context) -&gt; None:
        self.fail(
            "String interpolation contains both stars and mapping keys",
            context,
            code=codes.STRING_FORMATTING,
        )

</t>
<t tx="ekr.20230831011820.1270">    def visit_super_expr(self, expr: SuperExpr) -&gt; None:
        if not self.type and not expr.call.args:
            self.fail('"super" used outside class', expr)
            return
        expr.info = self.type
        for arg in expr.call.args:
            arg.accept(self)

</t>
<t tx="ekr.20230831011820.1271">    def visit_tuple_expr(self, expr: TupleExpr) -&gt; None:
        for item in expr.items:
            if isinstance(item, StarExpr):
                item.valid = True
            item.accept(self)

</t>
<t tx="ekr.20230831011820.1272">    def visit_list_expr(self, expr: ListExpr) -&gt; None:
        for item in expr.items:
            if isinstance(item, StarExpr):
                item.valid = True
            item.accept(self)

</t>
<t tx="ekr.20230831011820.1273">    def visit_set_expr(self, expr: SetExpr) -&gt; None:
        for item in expr.items:
            if isinstance(item, StarExpr):
                item.valid = True
            item.accept(self)

</t>
<t tx="ekr.20230831011820.1274">    def visit_dict_expr(self, expr: DictExpr) -&gt; None:
        for key, value in expr.items:
            if key is not None:
                key.accept(self)
            value.accept(self)

</t>
<t tx="ekr.20230831011820.1275">    def visit_star_expr(self, expr: StarExpr) -&gt; None:
        if not expr.valid:
            self.fail("Can use starred expression only as assignment target", expr, blocker=True)
        else:
            expr.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1276">    def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
        if not self.is_func_scope():
            self.fail('"yield from" outside function', e, serious=True, blocker=True)
        elif self.is_comprehension_stack[-1]:
            self.fail(
                '"yield from" inside comprehension or generator expression',
                e,
                serious=True,
                blocker=True,
            )
        elif self.function_stack[-1].is_coroutine:
            self.fail('"yield from" in async function', e, serious=True, blocker=True)
        else:
            self.function_stack[-1].is_generator = True
        if e.expr:
            e.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1277">    def visit_call_expr(self, expr: CallExpr) -&gt; None:
        """Analyze a call expression.

        Some call expressions are recognized as special forms, including
        cast(...).
        """
        expr.callee.accept(self)
        if refers_to_fullname(expr.callee, "typing.cast"):
            # Special form cast(...).
            if not self.check_fixed_args(expr, 2, "cast"):
                return
            # Translate first argument to an unanalyzed type.
            try:
                target = self.expr_to_unanalyzed_type(expr.args[0])
            except TypeTranslationError:
                self.fail("Cast target is not a type", expr)
                return
            # Piggyback CastExpr object to the CallExpr object; it takes
            # precedence over the CallExpr semantics.
            expr.analyzed = CastExpr(expr.args[1], target)
            expr.analyzed.line = expr.line
            expr.analyzed.column = expr.column
            expr.analyzed.accept(self)
        elif refers_to_fullname(expr.callee, ASSERT_TYPE_NAMES):
            if not self.check_fixed_args(expr, 2, "assert_type"):
                return
            # Translate second argument to an unanalyzed type.
            try:
                target = self.expr_to_unanalyzed_type(expr.args[1])
            except TypeTranslationError:
                self.fail("assert_type() type is not a type", expr)
                return
            expr.analyzed = AssertTypeExpr(expr.args[0], target)
            expr.analyzed.line = expr.line
            expr.analyzed.column = expr.column
            expr.analyzed.accept(self)
        elif refers_to_fullname(expr.callee, REVEAL_TYPE_NAMES):
            if not self.check_fixed_args(expr, 1, "reveal_type"):
                return
            expr.analyzed = RevealExpr(kind=REVEAL_TYPE, expr=expr.args[0])
            expr.analyzed.line = expr.line
            expr.analyzed.column = expr.column
            expr.analyzed.accept(self)
        elif refers_to_fullname(expr.callee, "builtins.reveal_locals"):
            # Store the local variable names into the RevealExpr for use in the
            # type checking pass
            local_nodes: list[Var] = []
            if self.is_module_scope():
                # try to determine just the variable declarations in module scope
                # self.globals.values() contains SymbolTableNode's
                # Each SymbolTableNode has an attribute node that is nodes.Var
                # look for variable nodes that marked as is_inferred
                # Each symboltable node has a Var node as .node
                local_nodes = [
                    n.node
                    for name, n in self.globals.items()
                    if getattr(n.node, "is_inferred", False) and isinstance(n.node, Var)
                ]
            elif self.is_class_scope():
                # type = None  # type: Optional[TypeInfo]
                if self.type is not None:
                    local_nodes = [
                        st.node for st in self.type.names.values() if isinstance(st.node, Var)
                    ]
            elif self.is_func_scope():
                # locals = None  # type: List[Optional[SymbolTable]]
                if self.locals is not None:
                    symbol_table = self.locals[-1]
                    if symbol_table is not None:
                        local_nodes = [
                            st.node for st in symbol_table.values() if isinstance(st.node, Var)
                        ]
            expr.analyzed = RevealExpr(kind=REVEAL_LOCALS, local_nodes=local_nodes)
            expr.analyzed.line = expr.line
            expr.analyzed.column = expr.column
            expr.analyzed.accept(self)
        elif refers_to_fullname(expr.callee, "typing.Any"):
            # Special form Any(...) no longer supported.
            self.fail("Any(...) is no longer supported. Use cast(Any, ...) instead", expr)
        elif refers_to_fullname(expr.callee, "typing._promote"):
            # Special form _promote(...).
            if not self.check_fixed_args(expr, 1, "_promote"):
                return
            # Translate first argument to an unanalyzed type.
            try:
                target = self.expr_to_unanalyzed_type(expr.args[0])
            except TypeTranslationError:
                self.fail("Argument 1 to _promote is not a type", expr)
                return
            expr.analyzed = PromoteExpr(target)
            expr.analyzed.line = expr.line
            expr.analyzed.accept(self)
        elif refers_to_fullname(expr.callee, "builtins.dict"):
            expr.analyzed = self.translate_dict_call(expr)
        elif refers_to_fullname(expr.callee, "builtins.divmod"):
            if not self.check_fixed_args(expr, 2, "divmod"):
                return
            expr.analyzed = OpExpr("divmod", expr.args[0], expr.args[1])
            expr.analyzed.line = expr.line
            expr.analyzed.accept(self)
        else:
            # Normal call expression.
            for a in expr.args:
                a.accept(self)

            if (
                isinstance(expr.callee, MemberExpr)
                and isinstance(expr.callee.expr, NameExpr)
                and expr.callee.expr.name == "__all__"
                and expr.callee.expr.kind == GDEF
                and expr.callee.name in ("append", "extend", "remove")
            ):
                if expr.callee.name == "append" and expr.args:
                    self.add_exports(expr.args[0])
                elif (
                    expr.callee.name == "extend"
                    and expr.args
                    and isinstance(expr.args[0], (ListExpr, TupleExpr))
                ):
                    self.add_exports(expr.args[0].items)
                elif (
                    expr.callee.name == "remove"
                    and expr.args
                    and isinstance(expr.args[0], StrExpr)
                ):
                    self.all_exports = [n for n in self.all_exports if n != expr.args[0].value]

</t>
<t tx="ekr.20230831011820.1278">    def translate_dict_call(self, call: CallExpr) -&gt; DictExpr | None:
        """Translate 'dict(x=y, ...)' to {'x': y, ...} and 'dict()' to {}.

        For other variants of dict(...), return None.
        """
        if not all(kind in (ARG_NAMED, ARG_STAR2) for kind in call.arg_kinds):
            # Must still accept those args.
            for a in call.args:
                a.accept(self)
            return None
        expr = DictExpr(
            [
                (StrExpr(key) if key is not None else None, value)
                for key, value in zip(call.arg_names, call.args)
            ]
        )
        expr.set_line(call)
        expr.accept(self)
        return expr

</t>
<t tx="ekr.20230831011820.1279">    def check_fixed_args(self, expr: CallExpr, numargs: int, name: str) -&gt; bool:
        """Verify that expr has specified number of positional args.

        Return True if the arguments are valid.
        """
        s = "s"
        if numargs == 1:
            s = ""
        if len(expr.args) != numargs:
            self.fail('"%s" expects %d argument%s' % (name, numargs, s), expr)
            return False
        if expr.arg_kinds != [ARG_POS] * numargs:
            self.fail(f'"{name}" must be called with {numargs} positional argument{s}', expr)
            return False
        return True

</t>
<t tx="ekr.20230831011820.128">    def requires_int_or_single_byte(self, context: Context, format_call: bool = False) -&gt; None:
        self.fail(
            '"{}c" requires an integer in range(256) or a single byte'.format(
                ":" if format_call else "%"
            ),
            context,
            code=codes.STRING_FORMATTING,
        )

</t>
<t tx="ekr.20230831011820.1280">    def visit_member_expr(self, expr: MemberExpr) -&gt; None:
        base = expr.expr
        base.accept(self)
        if isinstance(base, RefExpr) and isinstance(base.node, MypyFile):
            # Handle module attribute.
            sym = self.get_module_symbol(base.node, expr.name)
            if sym:
                if isinstance(sym.node, PlaceholderNode):
                    self.process_placeholder(expr.name, "attribute", expr)
                    return
                expr.kind = sym.kind
                expr.fullname = sym.fullname or ""
                expr.node = sym.node
        elif isinstance(base, RefExpr):
            # This branch handles the case C.bar (or cls.bar or self.bar inside
            # a classmethod/method), where C is a class and bar is a type
            # definition or a module resulting from `import bar` (or a module
            # assignment) inside class C. We look up bar in the class' TypeInfo
            # namespace.  This is done only when bar is a module or a type;
            # other things (e.g. methods) are handled by other code in
            # checkmember.
            type_info = None
            if isinstance(base.node, TypeInfo):
                # C.bar where C is a class
                type_info = base.node
            elif isinstance(base.node, Var) and self.type and self.function_stack:
                # check for self.bar or cls.bar in method/classmethod
                func_def = self.function_stack[-1]
                if not func_def.is_static and isinstance(func_def.type, CallableType):
                    formal_arg = func_def.type.argument_by_name(base.node.name)
                    if formal_arg and formal_arg.pos == 0:
                        type_info = self.type
            elif isinstance(base.node, TypeAlias) and base.node.no_args:
                assert isinstance(base.node.target, ProperType)
                if isinstance(base.node.target, Instance):
                    type_info = base.node.target.type

            if type_info:
                n = type_info.names.get(expr.name)
                if n is not None and isinstance(n.node, (MypyFile, TypeInfo, TypeAlias)):
                    if not n:
                        return
                    expr.kind = n.kind
                    expr.fullname = n.fullname or ""
                    expr.node = n.node

</t>
<t tx="ekr.20230831011820.1281">    def visit_op_expr(self, expr: OpExpr) -&gt; None:
        expr.left.accept(self)

        if expr.op in ("and", "or"):
            inferred = infer_condition_value(expr.left, self.options)
            if (inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "and") or (
                inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "or"
            ):
                expr.right_unreachable = True
                return
            elif (inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "and") or (
                inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "or"
            ):
                expr.right_always = True

        expr.right.accept(self)

</t>
<t tx="ekr.20230831011820.1282">    def visit_comparison_expr(self, expr: ComparisonExpr) -&gt; None:
        for operand in expr.operands:
            operand.accept(self)

</t>
<t tx="ekr.20230831011820.1283">    def visit_unary_expr(self, expr: UnaryExpr) -&gt; None:
        expr.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1284">    def visit_index_expr(self, expr: IndexExpr) -&gt; None:
        base = expr.base
        base.accept(self)
        if (
            isinstance(base, RefExpr)
            and isinstance(base.node, TypeInfo)
            and not base.node.is_generic()
        ):
            expr.index.accept(self)
        elif (
            isinstance(base, RefExpr) and isinstance(base.node, TypeAlias)
        ) or refers_to_class_or_function(base):
            # We need to do full processing on every iteration, since some type
            # arguments may contain placeholder types.
            self.analyze_type_application(expr)
        else:
            expr.index.accept(self)

</t>
<t tx="ekr.20230831011820.1285">    def analyze_type_application(self, expr: IndexExpr) -&gt; None:
        """Analyze special form -- type application (either direct or via type aliasing)."""
        types = self.analyze_type_application_args(expr)
        if types is None:
            return
        base = expr.base
        expr.analyzed = TypeApplication(base, types)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        # Types list, dict, set are not subscriptable, prohibit this if
        # subscripted either via type alias...
        if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
            alias = base.node
            target = get_proper_type(alias.target)
            if isinstance(target, Instance):
                name = target.type.fullname
                if (
                    alias.no_args
                    and name  # this avoids bogus errors for already reported aliases
                    in get_nongen_builtins(self.options.python_version)
                    and not self.is_stub_file
                    and not alias.normalized
                ):
                    self.fail(no_subscript_builtin_alias(name, propose_alt=False), expr)
        # ...or directly.
        else:
            n = self.lookup_type_node(base)
            if (
                n
                and n.fullname in get_nongen_builtins(self.options.python_version)
                and not self.is_stub_file
            ):
                self.fail(no_subscript_builtin_alias(n.fullname, propose_alt=False), expr)

</t>
<t tx="ekr.20230831011820.1286">    def analyze_type_application_args(self, expr: IndexExpr) -&gt; list[Type] | None:
        """Analyze type arguments (index) in a type application.

        Return None if anything was incomplete.
        """
        index = expr.index
        tag = self.track_incomplete_refs()
        self.analyze_type_expr(index)
        if self.found_incomplete_ref(tag):
            return None
        if self.basic_type_applications:
            # Postpone the rest until we have more information (for r.h.s. of an assignment)
            return None
        types: list[Type] = []
        if isinstance(index, TupleExpr):
            items = index.items
            is_tuple = isinstance(expr.base, RefExpr) and expr.base.fullname == "builtins.tuple"
            if is_tuple and len(items) == 2 and isinstance(items[-1], EllipsisExpr):
                items = items[:-1]
        else:
            items = [index]

        # TODO: this needs a clean-up.
        # Probably always allow Parameters literals, and validate in semanal_typeargs.py
        base = expr.base
        if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
            allow_unpack = base.node.tvar_tuple_index is not None
            alias = base.node
            if any(isinstance(t, ParamSpecType) for t in alias.alias_tvars):
                has_param_spec = True
                num_args = len(alias.alias_tvars)
            else:
                has_param_spec = False
                num_args = -1
        elif isinstance(base, RefExpr) and isinstance(base.node, TypeInfo):
            allow_unpack = base.node.has_type_var_tuple_type
            has_param_spec = base.node.has_param_spec_type
            num_args = len(base.node.type_vars)
        else:
            allow_unpack = False
            has_param_spec = False
            num_args = -1

        for item in items:
            try:
                typearg = self.expr_to_unanalyzed_type(item)
            except TypeTranslationError:
                self.fail("Type expected within [...]", expr)
                return None
            analyzed = self.anal_type(
                typearg,
                # The type application may appear in base class expression,
                # where type variables are not bound yet. Or when accepting
                # r.h.s. of type alias before we figured out it is a type alias.
                allow_unbound_tvars=self.allow_unbound_tvars,
                allow_placeholder=True,
                allow_param_spec_literals=has_param_spec,
                allow_unpack=allow_unpack,
            )
            if analyzed is None:
                return None
            types.append(analyzed)

        if has_param_spec and num_args == 1 and types:
            first_arg = get_proper_type(types[0])
            if not (
                len(types) == 1 and isinstance(first_arg, (Parameters, ParamSpecType, AnyType))
            ):
                types = [Parameters(types, [ARG_POS] * len(types), [None] * len(types))]

        return types

</t>
<t tx="ekr.20230831011820.1287">    def visit_slice_expr(self, expr: SliceExpr) -&gt; None:
        if expr.begin_index:
            expr.begin_index.accept(self)
        if expr.end_index:
            expr.end_index.accept(self)
        if expr.stride:
            expr.stride.accept(self)

</t>
<t tx="ekr.20230831011820.1288">    def visit_cast_expr(self, expr: CastExpr) -&gt; None:
        expr.expr.accept(self)
        analyzed = self.anal_type(expr.type)
        if analyzed is not None:
            expr.type = analyzed

</t>
<t tx="ekr.20230831011820.1289">    def visit_assert_type_expr(self, expr: AssertTypeExpr) -&gt; None:
        expr.expr.accept(self)
        analyzed = self.anal_type(expr.type)
        if analyzed is not None:
            expr.type = analyzed

</t>
<t tx="ekr.20230831011820.129">    def requires_int_or_char(self, context: Context, format_call: bool = False) -&gt; None:
        self.fail(
            '"{}c" requires int or char'.format(":" if format_call else "%"),
            context,
            code=codes.STRING_FORMATTING,
        )

</t>
<t tx="ekr.20230831011820.1290">    def visit_reveal_expr(self, expr: RevealExpr) -&gt; None:
        if expr.kind == REVEAL_TYPE:
            if expr.expr is not None:
                expr.expr.accept(self)
        else:
            # Reveal locals doesn't have an inner expression, there's no
            # need to traverse inside it
            pass

</t>
<t tx="ekr.20230831011820.1291">    def visit_type_application(self, expr: TypeApplication) -&gt; None:
        expr.expr.accept(self)
        for i in range(len(expr.types)):
            analyzed = self.anal_type(expr.types[i])
            if analyzed is not None:
                expr.types[i] = analyzed

</t>
<t tx="ekr.20230831011820.1292">    def visit_list_comprehension(self, expr: ListComprehension) -&gt; None:
        if any(expr.generator.is_async):
            if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
                self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

        expr.generator.accept(self)

</t>
<t tx="ekr.20230831011820.1293">    def visit_set_comprehension(self, expr: SetComprehension) -&gt; None:
        if any(expr.generator.is_async):
            if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
                self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

        expr.generator.accept(self)

</t>
<t tx="ekr.20230831011820.1294">    def visit_dictionary_comprehension(self, expr: DictionaryComprehension) -&gt; None:
        if any(expr.is_async):
            if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
                self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

        with self.enter(expr):
            self.analyze_comp_for(expr)
            expr.key.accept(self)
            expr.value.accept(self)
        self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20230831011820.1295">    def visit_generator_expr(self, expr: GeneratorExpr) -&gt; None:
        with self.enter(expr):
            self.analyze_comp_for(expr)
            expr.left_expr.accept(self)
        self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20230831011820.1296">    def analyze_comp_for(self, expr: GeneratorExpr | DictionaryComprehension) -&gt; None:
        """Analyses the 'comp_for' part of comprehensions (part 1).

        That is the part after 'for' in (x for x in l if p). This analyzes
        variables and conditions which are analyzed in a local scope.
        """
        for i, (index, sequence, conditions) in enumerate(
            zip(expr.indices, expr.sequences, expr.condlists)
        ):
            if i &gt; 0:
                sequence.accept(self)
            # Bind index variables.
            self.analyze_lvalue(index)
            for cond in conditions:
                cond.accept(self)

</t>
<t tx="ekr.20230831011820.1297">    def analyze_comp_for_2(self, expr: GeneratorExpr | DictionaryComprehension) -&gt; None:
        """Analyses the 'comp_for' part of comprehensions (part 2).

        That is the part after 'for' in (x for x in l if p). This analyzes
        the 'l' part which is analyzed in the surrounding scope.
        """
        expr.sequences[0].accept(self)

</t>
<t tx="ekr.20230831011820.1298">    def visit_lambda_expr(self, expr: LambdaExpr) -&gt; None:
        self.analyze_arg_initializers(expr)
        self.analyze_function_body(expr)

</t>
<t tx="ekr.20230831011820.1299">    def visit_conditional_expr(self, expr: ConditionalExpr) -&gt; None:
        expr.if_expr.accept(self)
        expr.cond.accept(self)
        expr.else_expr.accept(self)

</t>
<t tx="ekr.20230831011820.13">@path mypy
&lt;&lt; meet.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.130">    def key_not_in_mapping(self, key: str, context: Context) -&gt; None:
        self.fail(f'Key "{key}" not found in mapping', context, code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20230831011820.1300">    def visit__promote_expr(self, expr: PromoteExpr) -&gt; None:
        analyzed = self.anal_type(expr.type)
        if analyzed is not None:
            assert isinstance(analyzed, ProperType), "Cannot use type aliases for promotions"
            expr.type = analyzed

</t>
<t tx="ekr.20230831011820.1301">    def visit_yield_expr(self, e: YieldExpr) -&gt; None:
        if not self.is_func_scope():
            self.fail('"yield" outside function', e, serious=True, blocker=True)
        elif self.is_comprehension_stack[-1]:
            self.fail(
                '"yield" inside comprehension or generator expression',
                e,
                serious=True,
                blocker=True,
            )
        elif self.function_stack[-1].is_coroutine:
            self.function_stack[-1].is_generator = True
            self.function_stack[-1].is_async_generator = True
        else:
            self.function_stack[-1].is_generator = True
        if e.expr:
            e.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1302">    def visit_await_expr(self, expr: AwaitExpr) -&gt; None:
        if not self.is_func_scope() or not self.function_stack:
            # We check both because is_function_scope() returns True inside comprehensions.
            # This is not a blocker, because some enviroments (like ipython)
            # support top level awaits.
            self.fail('"await" outside function', expr, serious=True, code=codes.TOP_LEVEL_AWAIT)
        elif not self.function_stack[-1].is_coroutine:
            self.fail(
                '"await" outside coroutine ("async def")',
                expr,
                serious=True,
                code=codes.AWAIT_NOT_ASYNC,
            )
        expr.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1303">    #
    # Patterns
    #

    def visit_as_pattern(self, p: AsPattern) -&gt; None:
        if p.pattern is not None:
            p.pattern.accept(self)
        if p.name is not None:
            self.analyze_lvalue(p.name)

</t>
<t tx="ekr.20230831011820.1304">    def visit_or_pattern(self, p: OrPattern) -&gt; None:
        for pattern in p.patterns:
            pattern.accept(self)

</t>
<t tx="ekr.20230831011820.1305">    def visit_value_pattern(self, p: ValuePattern) -&gt; None:
        p.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1306">    def visit_sequence_pattern(self, p: SequencePattern) -&gt; None:
        for pattern in p.patterns:
            pattern.accept(self)

</t>
<t tx="ekr.20230831011820.1307">    def visit_starred_pattern(self, p: StarredPattern) -&gt; None:
        if p.capture is not None:
            self.analyze_lvalue(p.capture)

</t>
<t tx="ekr.20230831011820.1308">    def visit_mapping_pattern(self, p: MappingPattern) -&gt; None:
        for key in p.keys:
            key.accept(self)
        for value in p.values:
            value.accept(self)
        if p.rest is not None:
            self.analyze_lvalue(p.rest)

</t>
<t tx="ekr.20230831011820.1309">    def visit_class_pattern(self, p: ClassPattern) -&gt; None:
        p.class_ref.accept(self)
        for pos in p.positionals:
            pos.accept(self)
        for v in p.keyword_values:
            v.accept(self)

</t>
<t tx="ekr.20230831011820.131">    def string_interpolation_mixing_key_and_non_keys(self, context: Context) -&gt; None:
        self.fail(
            "String interpolation mixes specifier with and without mapping keys",
            context,
            code=codes.STRING_FORMATTING,
        )

</t>
<t tx="ekr.20230831011820.1310">    #
    # Lookup functions
    #

    def lookup(
        self, name: str, ctx: Context, suppress_errors: bool = False
    ) -&gt; SymbolTableNode | None:
        """Look up an unqualified (no dots) name in all active namespaces.

        Note that the result may contain a PlaceholderNode. The caller may
        want to defer in that case.

        Generate an error if the name is not defined unless suppress_errors
        is true or the current namespace is incomplete. In the latter case
        defer.
        """
        implicit_name = False
        # 1a. Name declared using 'global x' takes precedence
        if name in self.global_decls[-1]:
            if name in self.globals:
                return self.globals[name]
            if not suppress_errors:
                self.name_not_defined(name, ctx)
            return None
        # 1b. Name declared using 'nonlocal x' takes precedence
        if name in self.nonlocal_decls[-1]:
            for table in reversed(self.locals[:-1]):
                if table is not None and name in table:
                    return table[name]
            if not suppress_errors:
                self.name_not_defined(name, ctx)
            return None
        # 2. Class attributes (if within class definition)
        if self.type and not self.is_func_scope() and name in self.type.names:
            node = self.type.names[name]
            if not node.implicit:
                if self.is_active_symbol_in_class_body(node.node):
                    return node
            else:
                # Defined through self.x assignment
                implicit_name = True
                implicit_node = node
        # 3. Local (function) scopes
        for table in reversed(self.locals):
            if table is not None and name in table:
                return table[name]
        # 4. Current file global scope
        if name in self.globals:
            return self.globals[name]
        # 5. Builtins
        b = self.globals.get("__builtins__", None)
        if b:
            assert isinstance(b.node, MypyFile)
            table = b.node.names
            if name in table:
                if len(name) &gt; 1 and name[0] == "_" and name[1] != "_":
                    if not suppress_errors:
                        self.name_not_defined(name, ctx)
                    return None
                node = table[name]
                return node
        # Give up.
        if not implicit_name and not suppress_errors:
            self.name_not_defined(name, ctx)
        else:
            if implicit_name:
                return implicit_node
        return None

</t>
<t tx="ekr.20230831011820.1311">    def is_active_symbol_in_class_body(self, node: SymbolNode | None) -&gt; bool:
        """Can a symbol defined in class body accessed at current statement?

        Only allow access to class attributes textually after
        the definition, so that it's possible to fall back to the
        outer scope. Example:

            class X: ...

            class C:
                X = X  # Initializer refers to outer scope

        Nested classes are an exception, since we want to support
        arbitrary forward references in type annotations. Also, we
        allow forward references to type aliases to support recursive
        types.
        """
        # TODO: Forward reference to name imported in class body is not
        #       caught.
        if self.statement is None:
            # Assume it's fine -- don't have enough context to check
            return True
        return (
            node is None
            or self.is_textually_before_statement(node)
            or not self.is_defined_in_current_module(node.fullname)
            or isinstance(node, (TypeInfo, TypeAlias))
            or (isinstance(node, PlaceholderNode) and node.becomes_typeinfo)
        )

</t>
<t tx="ekr.20230831011820.1312">    def is_textually_before_statement(self, node: SymbolNode) -&gt; bool:
        """Check if a node is defined textually before the current statement

        Note that decorated functions' line number are the same as
        the top decorator.
        """
        assert self.statement
        line_diff = self.statement.line - node.line

        # The first branch handles reference an overloaded function variant inside itself,
        # this is a corner case where mypy technically deviates from runtime name resolution,
        # but it is fine because we want an overloaded function to be treated as a single unit.
        if self.is_overloaded_item(node, self.statement):
            return False
        elif isinstance(node, Decorator) and not node.is_overload:
            return line_diff &gt; len(node.original_decorators)
        else:
            return line_diff &gt; 0

</t>
<t tx="ekr.20230831011820.1313">    def is_overloaded_item(self, node: SymbolNode, statement: Statement) -&gt; bool:
        """Check whether the function belongs to the overloaded variants"""
        if isinstance(node, OverloadedFuncDef) and isinstance(statement, FuncDef):
            in_items = statement in {
                item.func if isinstance(item, Decorator) else item for item in node.items
            }
            in_impl = node.impl is not None and (
                (isinstance(node.impl, Decorator) and statement is node.impl.func)
                or statement is node.impl
            )
            return in_items or in_impl
        return False

</t>
<t tx="ekr.20230831011820.1314">    def is_defined_in_current_module(self, fullname: str | None) -&gt; bool:
        if not fullname:
            return False
        return module_prefix(self.modules, fullname) == self.cur_mod_id

</t>
<t tx="ekr.20230831011820.1315">    def lookup_qualified(
        self, name: str, ctx: Context, suppress_errors: bool = False
    ) -&gt; SymbolTableNode | None:
        """Lookup a qualified name in all activate namespaces.

        Note that the result may contain a PlaceholderNode. The caller may
        want to defer in that case.

        Generate an error if the name is not defined unless suppress_errors
        is true or the current namespace is incomplete. In the latter case
        defer.
        """
        if "." not in name:
            # Simple case: look up a short name.
            return self.lookup(name, ctx, suppress_errors=suppress_errors)
        parts = name.split(".")
        namespace = self.cur_mod_id
        sym = self.lookup(parts[0], ctx, suppress_errors=suppress_errors)
        if sym:
            for i in range(1, len(parts)):
                node = sym.node
                part = parts[i]
                if isinstance(node, TypeInfo):
                    nextsym = node.get(part)
                elif isinstance(node, MypyFile):
                    nextsym = self.get_module_symbol(node, part)
                    namespace = node.fullname
                elif isinstance(node, PlaceholderNode):
                    return sym
                elif isinstance(node, TypeAlias) and node.no_args:
                    assert isinstance(node.target, ProperType)
                    if isinstance(node.target, Instance):
                        nextsym = node.target.type.get(part)
                    else:
                        nextsym = None
                else:
                    if isinstance(node, Var):
                        typ = get_proper_type(node.type)
                        if isinstance(typ, AnyType):
                            # Allow access through Var with Any type without error.
                            return self.implicit_symbol(sym, name, parts[i:], typ)
                    # This might be something like valid `P.args` or invalid `P.__bound__` access.
                    # Important note that `ParamSpecExpr` is also ignored in other places.
                    # See https://github.com/python/mypy/pull/13468
                    if isinstance(node, ParamSpecExpr) and part in ("args", "kwargs"):
                        return None
                    # Lookup through invalid node, such as variable or function
                    nextsym = None
                if not nextsym or nextsym.module_hidden:
                    if not suppress_errors:
                        self.name_not_defined(name, ctx, namespace=namespace)
                    return None
                sym = nextsym
        return sym

</t>
<t tx="ekr.20230831011820.1316">    def lookup_type_node(self, expr: Expression) -&gt; SymbolTableNode | None:
        try:
            t = self.expr_to_unanalyzed_type(expr)
        except TypeTranslationError:
            return None
        if isinstance(t, UnboundType):
            n = self.lookup_qualified(t.name, expr, suppress_errors=True)
            return n
        return None

</t>
<t tx="ekr.20230831011820.1317">    def get_module_symbol(self, node: MypyFile, name: str) -&gt; SymbolTableNode | None:
        """Look up a symbol from a module.

        Return None if no matching symbol could be bound.
        """
        module = node.fullname
        names = node.names
        sym = names.get(name)
        if not sym:
            fullname = module + "." + name
            if fullname in self.modules:
                sym = SymbolTableNode(GDEF, self.modules[fullname])
            elif self.is_incomplete_namespace(module):
                self.record_incomplete_ref()
            elif "__getattr__" in names:
                gvar = self.create_getattr_var(names["__getattr__"], name, fullname)
                if gvar:
                    sym = SymbolTableNode(GDEF, gvar)
            elif self.is_missing_module(fullname):
                # We use the fullname of the original definition so that we can
                # detect whether two names refer to the same thing.
                var_type = AnyType(TypeOfAny.from_unimported_type)
                v = Var(name, type=var_type)
                v._fullname = fullname
                sym = SymbolTableNode(GDEF, v)
        elif sym.module_hidden:
            sym = None
        return sym

</t>
<t tx="ekr.20230831011820.1318">    def is_missing_module(self, module: str) -&gt; bool:
        return module in self.missing_modules

</t>
<t tx="ekr.20230831011820.1319">    def implicit_symbol(
        self, sym: SymbolTableNode, name: str, parts: list[str], source_type: AnyType
    ) -&gt; SymbolTableNode:
        """Create symbol for a qualified name reference through Any type."""
        if sym.node is None:
            basename = None
        else:
            basename = sym.node.fullname
        if basename is None:
            fullname = name
        else:
            fullname = basename + "." + ".".join(parts)
        var_type = AnyType(TypeOfAny.from_another_any, source_type)
        var = Var(parts[-1], var_type)
        var._fullname = fullname
        return SymbolTableNode(GDEF, var)

</t>
<t tx="ekr.20230831011820.132">    def cannot_determine_type(self, name: str, context: Context) -&gt; None:
        self.fail(f'Cannot determine type of "{name}"', context, code=codes.HAS_TYPE)

</t>
<t tx="ekr.20230831011820.1320">    def create_getattr_var(
        self, getattr_defn: SymbolTableNode, name: str, fullname: str
    ) -&gt; Var | None:
        """Create a dummy variable using module-level __getattr__ return type.

        If not possible, return None.

        Note that multiple Var nodes can be created for a single name. We
        can use the from_module_getattr and the fullname attributes to
        check if two dummy Var nodes refer to the same thing. Reusing Var
        nodes would require non-local mutable state, which we prefer to
        avoid.
        """
        if isinstance(getattr_defn.node, (FuncDef, Var)):
            node_type = get_proper_type(getattr_defn.node.type)
            if isinstance(node_type, CallableType):
                typ = node_type.ret_type
            else:
                typ = AnyType(TypeOfAny.from_error)
            v = Var(name, type=typ)
            v._fullname = fullname
            v.from_module_getattr = True
            return v
        return None

</t>
<t tx="ekr.20230831011820.1321">    def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode:
        ret = self.lookup_fully_qualified_or_none(fullname)
        assert ret is not None, fullname
        return ret

</t>
<t tx="ekr.20230831011820.1322">    def lookup_fully_qualified_or_none(self, fullname: str) -&gt; SymbolTableNode | None:
        """Lookup a fully qualified name that refers to a module-level definition.

        Don't assume that the name is defined. This happens in the global namespace --
        the local module namespace is ignored. This does not dereference indirect
        refs.

        Note that this can't be used for names nested in class namespaces.
        """
        # TODO: unify/clean-up/simplify lookup methods, see #4157.
        # TODO: support nested classes (but consider performance impact,
        #       we might keep the module level only lookup for thing like 'builtins.int').
        assert "." in fullname
        module, name = fullname.rsplit(".", maxsplit=1)
        if module not in self.modules:
            return None
        filenode = self.modules[module]
        result = filenode.names.get(name)
        if result is None and self.is_incomplete_namespace(module):
            # TODO: More explicit handling of incomplete refs?
            self.record_incomplete_ref()
        return result

</t>
<t tx="ekr.20230831011820.1323">    def object_type(self) -&gt; Instance:
        return self.named_type("builtins.object")

</t>
<t tx="ekr.20230831011820.1324">    def str_type(self) -&gt; Instance:
        return self.named_type("builtins.str")

</t>
<t tx="ekr.20230831011820.1325">    def named_type(self, fullname: str, args: list[Type] | None = None) -&gt; Instance:
        sym = self.lookup_fully_qualified(fullname)
        assert sym, "Internal error: attempted to construct unknown type"
        node = sym.node
        assert isinstance(node, TypeInfo)
        if args:
            # TODO: assert len(args) == len(node.defn.type_vars)
            return Instance(node, args)
        return Instance(node, [AnyType(TypeOfAny.special_form)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20230831011820.1326">    def named_type_or_none(self, fullname: str, args: list[Type] | None = None) -&gt; Instance | None:
        sym = self.lookup_fully_qualified_or_none(fullname)
        if not sym or isinstance(sym.node, PlaceholderNode):
            return None
        node = sym.node
        if isinstance(node, TypeAlias):
            assert isinstance(node.target, Instance)  # type: ignore[misc]
            node = node.target.type
        assert isinstance(node, TypeInfo), node
        if args is not None:
            # TODO: assert len(args) == len(node.defn.type_vars)
            return Instance(node, args)
        return Instance(node, [AnyType(TypeOfAny.unannotated)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20230831011820.1327">    def builtin_type(self, fully_qualified_name: str) -&gt; Instance:
        """Legacy function -- use named_type() instead."""
        return self.named_type(fully_qualified_name)

</t>
<t tx="ekr.20230831011820.1328">    def lookup_current_scope(self, name: str) -&gt; SymbolTableNode | None:
        if self.locals[-1] is not None:
            return self.locals[-1].get(name)
        elif self.type is not None:
            return self.type.names.get(name)
        else:
            return self.globals.get(name)

</t>
<t tx="ekr.20230831011820.1329">    #
    # Adding symbols
    #

    def add_symbol(
        self,
        name: str,
        node: SymbolNode,
        context: Context,
        module_public: bool = True,
        module_hidden: bool = False,
        can_defer: bool = True,
        escape_comprehensions: bool = False,
    ) -&gt; bool:
        """Add symbol to the currently active symbol table.

        Generally additions to symbol table should go through this method or
        one of the methods below so that kinds, redefinitions, conditional
        definitions, and skipped names are handled consistently.

        Return True if we actually added the symbol, or False if we refused to do so
        (because something is not ready).

        If can_defer is True, defer current target if adding a placeholder.
        """
        if self.is_func_scope():
            kind = LDEF
        elif self.type is not None:
            kind = MDEF
        else:
            kind = GDEF
        symbol = SymbolTableNode(
            kind, node, module_public=module_public, module_hidden=module_hidden
        )
        return self.add_symbol_table_node(name, symbol, context, can_defer, escape_comprehensions)

</t>
<t tx="ekr.20230831011820.133">    def cannot_determine_type_in_base(self, name: str, base: str, context: Context) -&gt; None:
        self.fail(f'Cannot determine type of "{name}" in base class "{base}"', context)

</t>
<t tx="ekr.20230831011820.1330">    def add_symbol_skip_local(self, name: str, node: SymbolNode) -&gt; None:
        """Same as above, but skipping the local namespace.

        This doesn't check for previous definition and is only used
        for serialization of method-level classes.

        Classes defined within methods can be exposed through an
        attribute type, but method-level symbol tables aren't serialized.
        This method can be used to add such classes to an enclosing,
        serialized symbol table.
        """
        # TODO: currently this is only used by named tuples and typed dicts.
        # Use this method also by normal classes, see issue #6422.
        if self.type is not None:
            names = self.type.names
            kind = MDEF
        else:
            names = self.globals
            kind = GDEF
        symbol = SymbolTableNode(kind, node)
        names[name] = symbol

</t>
<t tx="ekr.20230831011820.1331">    def add_symbol_table_node(
        self,
        name: str,
        symbol: SymbolTableNode,
        context: Context | None = None,
        can_defer: bool = True,
        escape_comprehensions: bool = False,
    ) -&gt; bool:
        """Add symbol table node to the currently active symbol table.

        Return True if we actually added the symbol, or False if we refused
        to do so (because something is not ready or it was a no-op).

        Generate an error if there is an invalid redefinition.

        If context is None, unconditionally add node, since we can't report
        an error. Note that this is used by plugins to forcibly replace nodes!

        TODO: Prevent plugins from replacing nodes, as it could cause problems?

        Args:
            name: short name of symbol
            symbol: Node to add
            can_defer: if True, defer current target if adding a placeholder
            context: error context (see above about None value)
        """
        names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
        existing = names.get(name)
        if isinstance(symbol.node, PlaceholderNode) and can_defer:
            if context is not None:
                self.process_placeholder(name, "name", context)
            else:
                # see note in docstring describing None contexts
                self.defer()
        if (
            existing is not None
            and context is not None
            and not is_valid_replacement(existing, symbol)
        ):
            # There is an existing node, so this may be a redefinition.
            # If the new node points to the same node as the old one,
            # or if both old and new nodes are placeholders, we don't
            # need to do anything.
            old = existing.node
            new = symbol.node
            if isinstance(new, PlaceholderNode):
                # We don't know whether this is okay. Let's wait until the next iteration.
                return False
            if not is_same_symbol(old, new):
                if isinstance(new, (FuncDef, Decorator, OverloadedFuncDef, TypeInfo)):
                    self.add_redefinition(names, name, symbol)
                if not (isinstance(new, (FuncDef, Decorator)) and self.set_original_def(old, new)):
                    self.name_already_defined(name, context, existing)
        elif name not in self.missing_names[-1] and "*" not in self.missing_names[-1]:
            names[name] = symbol
            self.progress = True
            return True
        return False

</t>
<t tx="ekr.20230831011820.1332">    def add_redefinition(self, names: SymbolTable, name: str, symbol: SymbolTableNode) -&gt; None:
        """Add a symbol table node that reflects a redefinition as a function or a class.

        Redefinitions need to be added to the symbol table so that they can be found
        through AST traversal, but they have dummy names of form 'name-redefinition[N]',
        where N ranges over 2, 3, ... (omitted for the first redefinition).

        Note: we always store redefinitions independently of whether they are valid or not
        (so they will be semantically analyzed), the caller should give an error for invalid
        redefinitions (such as e.g. variable redefined as a class).
        """
        i = 1
        # Don't serialize redefined nodes. They are likely to have
        # busted internal references which can cause problems with
        # serialization and they can't have any external references to
        # them.
        symbol.no_serialize = True
        while True:
            if i == 1:
                new_name = f"{name}-redefinition"
            else:
                new_name = f"{name}-redefinition{i}"
            existing = names.get(new_name)
            if existing is None:
                names[new_name] = symbol
                return
            elif existing.node is symbol.node:
                # Already there
                return
            i += 1

</t>
<t tx="ekr.20230831011820.1333">    def add_local(self, node: Var | FuncDef | OverloadedFuncDef, context: Context) -&gt; None:
        """Add local variable or function."""
        assert self.is_func_scope()
        name = node.name
        node._fullname = name
        self.add_symbol(name, node, context)

</t>
<t tx="ekr.20230831011820.1334">    def _get_node_for_class_scoped_import(
        self, name: str, symbol_node: SymbolNode | None, context: Context
    ) -&gt; SymbolNode | None:
        if symbol_node is None:
            return None
        # I promise this type checks; I'm just making mypyc issues go away.
        # mypyc is absolutely convinced that `symbol_node` narrows to a Var in the following,
        # when it can also be a FuncBase. Once fixed, `f` in the following can be removed.
        # See also https://github.com/mypyc/mypyc/issues/892
        f: Callable[[object], Any] = lambda x: x
        if isinstance(f(symbol_node), (Decorator, FuncBase, Var)):
            # For imports in class scope, we construct a new node to represent the symbol and
            # set its `info` attribute to `self.type`.
            existing = self.current_symbol_table().get(name)
            if (
                # The redefinition checks in `add_symbol_table_node` don't work for our
                # constructed Var / FuncBase, so check for possible redefinitions here.
                existing is not None
                and isinstance(f(existing.node), (Decorator, FuncBase, Var))
                and (
                    isinstance(f(existing.type), f(AnyType))
                    or f(existing.type) == f(symbol_node).type
                )
            ):
                return existing.node

            # Construct the new node
            if isinstance(f(symbol_node), (FuncBase, Decorator)):
                # In theory we could construct a new node here as well, but in practice
                # it doesn't work well, see #12197
                typ: Type | None = AnyType(TypeOfAny.from_error)
                self.fail("Unsupported class scoped import", context)
            else:
                typ = f(symbol_node).type
            symbol_node = Var(name, typ)
            symbol_node._fullname = self.qualified_name(name)
            assert self.type is not None  # guaranteed by is_class_scope
            symbol_node.info = self.type
            symbol_node.line = context.line
            symbol_node.column = context.column
        return symbol_node

</t>
<t tx="ekr.20230831011820.1335">    def add_imported_symbol(
        self,
        name: str,
        node: SymbolTableNode,
        context: ImportBase,
        module_public: bool,
        module_hidden: bool,
    ) -&gt; None:
        """Add an alias to an existing symbol through import."""
        assert not module_hidden or not module_public

        existing_symbol = self.lookup_current_scope(name)
        if (
            existing_symbol
            and not isinstance(existing_symbol.node, PlaceholderNode)
            and not isinstance(node.node, PlaceholderNode)
        ):
            # Import can redefine a variable. They get special treatment.
            if self.process_import_over_existing_name(name, existing_symbol, node, context):
                return

        symbol_node: SymbolNode | None = node.node

        if self.is_class_scope():
            symbol_node = self._get_node_for_class_scoped_import(name, symbol_node, context)

        symbol = SymbolTableNode(
            node.kind, symbol_node, module_public=module_public, module_hidden=module_hidden
        )
        self.add_symbol_table_node(name, symbol, context)

</t>
<t tx="ekr.20230831011820.1336">    def add_unknown_imported_symbol(
        self,
        name: str,
        context: Context,
        target_name: str | None,
        module_public: bool,
        module_hidden: bool,
    ) -&gt; None:
        """Add symbol that we don't know what it points to because resolving an import failed.

        This can happen if a module is missing, or it is present, but doesn't have
        the imported attribute. The `target_name` is the name of symbol in the namespace
        it is imported from. For example, for 'from mod import x as y' the target_name is
        'mod.x'. This is currently used only to track logical dependencies.
        """
        existing = self.current_symbol_table().get(name)
        if existing and isinstance(existing.node, Var) and existing.node.is_suppressed_import:
            # This missing import was already added -- nothing to do here.
            return
        var = Var(name)
        if self.options.logical_deps and target_name is not None:
            # This makes it possible to add logical fine-grained dependencies
            # from a missing module. We can't use this by default, since in a
            # few places we assume that the full name points to a real
            # definition, but this name may point to nothing.
            var._fullname = target_name
        elif self.type:
            var._fullname = self.type.fullname + "." + name
            var.info = self.type
        else:
            var._fullname = self.qualified_name(name)
        var.is_ready = True
        any_type = AnyType(TypeOfAny.from_unimported_type, missing_import_name=var._fullname)
        var.type = any_type
        var.is_suppressed_import = True
        self.add_symbol(
            name, var, context, module_public=module_public, module_hidden=module_hidden
        )

</t>
<t tx="ekr.20230831011820.1337">    #
    # Other helpers
    #

    @contextmanager
    def tvar_scope_frame(self, frame: TypeVarLikeScope) -&gt; Iterator[None]:
        old_scope = self.tvar_scope
        self.tvar_scope = frame
        yield
        self.tvar_scope = old_scope

</t>
<t tx="ekr.20230831011820.1338">    def defer(self, debug_context: Context | None = None, force_progress: bool = False) -&gt; None:
        """Defer current analysis target to be analyzed again.

        This must be called if something in the current target is
        incomplete or has a placeholder node. However, this must *not*
        be called during the final analysis iteration! Instead, an error
        should be generated. Often 'process_placeholder' is a good
        way to either defer or generate an error.

        NOTE: Some methods, such as 'anal_type', 'mark_incomplete' and
              'record_incomplete_ref', call this implicitly, or when needed.
              They are usually preferable to a direct defer() call.
        """
        assert not self.final_iteration, "Must not defer during final iteration"
        if force_progress:
            # Usually, we report progress if we have replaced a placeholder node
            # with an actual valid node. However, sometimes we need to update an
            # existing node *in-place*. For example, this is used by type aliases
            # in context of forward references and/or recursive aliases, and in
            # similar situations (recursive named tuples etc).
            self.progress = True
        self.deferred = True
        # Store debug info for this deferral.
        line = (
            debug_context.line if debug_context else self.statement.line if self.statement else -1
        )
        self.deferral_debug_context.append((self.cur_mod_id, line))

</t>
<t tx="ekr.20230831011820.1339">    def track_incomplete_refs(self) -&gt; Tag:
        """Return tag that can be used for tracking references to incomplete names."""
        return self.num_incomplete_refs

</t>
<t tx="ekr.20230831011820.134">    def no_formal_self(self, name: str, item: CallableType, context: Context) -&gt; None:
        self.fail(
            'Attribute function "%s" with type %s does not accept self argument'
            % (name, format_type(item, self.options)),
            context,
        )

</t>
<t tx="ekr.20230831011820.1340">    def found_incomplete_ref(self, tag: Tag) -&gt; bool:
        """Have we encountered an incomplete reference since starting tracking?"""
        return self.num_incomplete_refs != tag

</t>
<t tx="ekr.20230831011820.1341">    def record_incomplete_ref(self) -&gt; None:
        """Record the encounter of an incomplete reference and defer current analysis target."""
        self.defer()
        self.num_incomplete_refs += 1

</t>
<t tx="ekr.20230831011820.1342">    def mark_incomplete(
        self,
        name: str,
        node: Node,
        becomes_typeinfo: bool = False,
        module_public: bool = True,
        module_hidden: bool = False,
    ) -&gt; None:
        """Mark a definition as incomplete (and defer current analysis target).

        Also potentially mark the current namespace as incomplete.

        Args:
            name: The name that we weren't able to define (or '*' if the name is unknown)
            node: The node that refers to the name (definition or lvalue)
            becomes_typeinfo: Pass this to PlaceholderNode (used by special forms like
                named tuples that will create TypeInfos).
        """
        self.defer(node)
        if name == "*":
            self.incomplete = True
        elif not self.is_global_or_nonlocal(name):
            fullname = self.qualified_name(name)
            assert self.statement
            placeholder = PlaceholderNode(
                fullname, node, self.statement.line, becomes_typeinfo=becomes_typeinfo
            )
            self.add_symbol(
                name,
                placeholder,
                module_public=module_public,
                module_hidden=module_hidden,
                context=dummy_context(),
            )
        self.missing_names[-1].add(name)

</t>
<t tx="ekr.20230831011820.1343">    def is_incomplete_namespace(self, fullname: str) -&gt; bool:
        """Is a module or class namespace potentially missing some definitions?

        If a name is missing from an incomplete namespace, we'll need to defer the
        current analysis target.
        """
        return fullname in self.incomplete_namespaces

</t>
<t tx="ekr.20230831011820.1344">    def process_placeholder(
        self, name: str | None, kind: str, ctx: Context, force_progress: bool = False
    ) -&gt; None:
        """Process a reference targeting placeholder node.

        If this is not a final iteration, defer current node,
        otherwise report an error.

        The 'kind' argument indicates if this a name or attribute expression
        (used for better error message).
        """
        if self.final_iteration:
            self.cannot_resolve_name(name, kind, ctx)
        else:
            self.defer(ctx, force_progress=force_progress)

</t>
<t tx="ekr.20230831011820.1345">    def cannot_resolve_name(self, name: str | None, kind: str, ctx: Context) -&gt; None:
        name_format = f' "{name}"' if name else ""
        self.fail(f"Cannot resolve {kind}{name_format} (possible cyclic definition)", ctx)
        if not self.options.disable_recursive_aliases and self.is_func_scope():
            self.note("Recursive types are not allowed at function scope", ctx)

</t>
<t tx="ekr.20230831011820.1346">    def qualified_name(self, name: str) -&gt; str:
        if self.type is not None:
            return self.type._fullname + "." + name
        elif self.is_func_scope():
            return name
        else:
            return self.cur_mod_id + "." + name

</t>
<t tx="ekr.20230831011820.1347">    @contextmanager
    def enter(
        self, function: FuncItem | GeneratorExpr | DictionaryComprehension
    ) -&gt; Iterator[None]:
        """Enter a function, generator or comprehension scope."""
        names = self.saved_locals.setdefault(function, SymbolTable())
        self.locals.append(names)
        is_comprehension = isinstance(function, (GeneratorExpr, DictionaryComprehension))
        self.is_comprehension_stack.append(is_comprehension)
        self.global_decls.append(set())
        self.nonlocal_decls.append(set())
        # -1 since entering block will increment this to 0.
        self.block_depth.append(-1)
        self.loop_depth.append(0)
        self.missing_names.append(set())
        try:
            yield
        finally:
            self.locals.pop()
            self.is_comprehension_stack.pop()
            self.global_decls.pop()
            self.nonlocal_decls.pop()
            self.block_depth.pop()
            self.loop_depth.pop()
            self.missing_names.pop()

</t>
<t tx="ekr.20230831011820.1348">    def is_func_scope(self) -&gt; bool:
        return self.locals[-1] is not None

</t>
<t tx="ekr.20230831011820.1349">    def is_nested_within_func_scope(self) -&gt; bool:
        """Are we underneath a function scope, even if we are in a nested class also?"""
        return any(l is not None for l in self.locals)

</t>
<t tx="ekr.20230831011820.135">    def incompatible_self_argument(
        self, name: str, arg: Type, sig: CallableType, is_classmethod: bool, context: Context
    ) -&gt; None:
        kind = "class attribute function" if is_classmethod else "attribute function"
        self.fail(
            'Invalid self argument %s to %s "%s" with type %s'
            % (format_type(arg, self.options), kind, name, format_type(sig, self.options)),
            context,
        )

</t>
<t tx="ekr.20230831011820.1350">    def is_class_scope(self) -&gt; bool:
        return self.type is not None and not self.is_func_scope()

</t>
<t tx="ekr.20230831011820.1351">    def is_module_scope(self) -&gt; bool:
        return not (self.is_class_scope() or self.is_func_scope())

</t>
<t tx="ekr.20230831011820.1352">    def current_symbol_kind(self) -&gt; int:
        if self.is_class_scope():
            kind = MDEF
        elif self.is_func_scope():
            kind = LDEF
        else:
            kind = GDEF
        return kind

</t>
<t tx="ekr.20230831011820.1353">    def current_symbol_table(self, escape_comprehensions: bool = False) -&gt; SymbolTable:
        if self.is_func_scope():
            assert self.locals[-1] is not None
            if escape_comprehensions:
                assert len(self.locals) == len(self.is_comprehension_stack)
                # Retrieve the symbol table from the enclosing non-comprehension scope.
                for i, is_comprehension in enumerate(reversed(self.is_comprehension_stack)):
                    if not is_comprehension:
                        if i == len(self.locals) - 1:  # The last iteration.
                            # The caller of the comprehension is in the global space.
                            names = self.globals
                        else:
                            names_candidate = self.locals[-1 - i]
                            assert (
                                names_candidate is not None
                            ), "Escaping comprehension from invalid scope"
                            names = names_candidate
                        break
                else:
                    assert False, "Should have at least one non-comprehension scope"
            else:
                names = self.locals[-1]
            assert names is not None
        elif self.type is not None:
            names = self.type.names
        else:
            names = self.globals
        return names

</t>
<t tx="ekr.20230831011820.1354">    def is_global_or_nonlocal(self, name: str) -&gt; bool:
        return self.is_func_scope() and (
            name in self.global_decls[-1] or name in self.nonlocal_decls[-1]
        )

</t>
<t tx="ekr.20230831011820.1355">    def add_exports(self, exp_or_exps: Iterable[Expression] | Expression) -&gt; None:
        exps = [exp_or_exps] if isinstance(exp_or_exps, Expression) else exp_or_exps
        for exp in exps:
            if isinstance(exp, StrExpr):
                self.all_exports.append(exp.value)

</t>
<t tx="ekr.20230831011820.1356">    def name_not_defined(self, name: str, ctx: Context, namespace: str | None = None) -&gt; None:
        incomplete = self.is_incomplete_namespace(namespace or self.cur_mod_id)
        if (
            namespace is None
            and self.type
            and not self.is_func_scope()
            and self.incomplete_type_stack[-1]
            and not self.final_iteration
        ):
            # We are processing a class body for the first time, so it is incomplete.
            incomplete = True
        if incomplete:
            # Target namespace is incomplete, so it's possible that the name will be defined
            # later on. Defer current target.
            self.record_incomplete_ref()
            return
        message = f'Name "{name}" is not defined'
        self.fail(message, ctx, code=codes.NAME_DEFINED)

        if f"builtins.{name}" in SUGGESTED_TEST_FIXTURES:
            # The user probably has a missing definition in a test fixture. Let's verify.
            fullname = f"builtins.{name}"
            if self.lookup_fully_qualified_or_none(fullname) is None:
                # Yes. Generate a helpful note.
                self.msg.add_fixture_note(fullname, ctx)

        modules_with_unimported_hints = {
            name.split(".", 1)[0] for name in TYPES_FOR_UNIMPORTED_HINTS
        }
        lowercased = {name.lower(): name for name in TYPES_FOR_UNIMPORTED_HINTS}
        for module in modules_with_unimported_hints:
            fullname = f"{module}.{name}".lower()
            if fullname not in lowercased:
                continue
            # User probably forgot to import these types.
            hint = (
                'Did you forget to import it from "{module}"?'
                ' (Suggestion: "from {module} import {name}")'
            ).format(module=module, name=lowercased[fullname].rsplit(".", 1)[-1])
            self.note(hint, ctx, code=codes.NAME_DEFINED)

</t>
<t tx="ekr.20230831011820.1357">    def already_defined(
        self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None, noun: str
    ) -&gt; None:
        if isinstance(original_ctx, SymbolTableNode):
            node: SymbolNode | None = original_ctx.node
        elif isinstance(original_ctx, SymbolNode):
            node = original_ctx
        else:
            node = None

        if isinstance(original_ctx, SymbolTableNode) and isinstance(original_ctx.node, MypyFile):
            # Since this is an import, original_ctx.node points to the module definition.
            # Therefore its line number is always 1, which is not useful for this
            # error message.
            extra_msg = " (by an import)"
        elif node and node.line != -1 and self.is_local_name(node.fullname):
            # TODO: Using previous symbol node may give wrong line. We should use
            #       the line number where the binding was established instead.
            extra_msg = f" on line {node.line}"
        else:
            extra_msg = " (possibly by an import)"
        self.fail(
            f'{noun} "{unmangle(name)}" already defined{extra_msg}', ctx, code=codes.NO_REDEF
        )

</t>
<t tx="ekr.20230831011820.1358">    def name_already_defined(
        self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None = None
    ) -&gt; None:
        self.already_defined(name, ctx, original_ctx, noun="Name")

</t>
<t tx="ekr.20230831011820.1359">    def attribute_already_defined(
        self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None = None
    ) -&gt; None:
        self.already_defined(name, ctx, original_ctx, noun="Attribute")

</t>
<t tx="ekr.20230831011820.136">    def incompatible_conditional_function_def(
        self, defn: FuncDef, old_type: FunctionLike, new_type: FunctionLike
    ) -&gt; None:
        self.fail("All conditional function variants must have identical signatures", defn)
        if isinstance(old_type, (CallableType, Overloaded)) and isinstance(
            new_type, (CallableType, Overloaded)
        ):
            self.note("Original:", defn)
            self.pretty_callable_or_overload(old_type, defn, offset=4)
            self.note("Redefinition:", defn)
            self.pretty_callable_or_overload(new_type, defn, offset=4)

</t>
<t tx="ekr.20230831011820.1360">    def is_local_name(self, name: str) -&gt; bool:
        """Does name look like reference to a definition in the current module?"""
        return self.is_defined_in_current_module(name) or "." not in name

</t>
<t tx="ekr.20230831011820.1361">    def in_checked_function(self) -&gt; bool:
        """Should we type-check the current function?

        - Yes if --check-untyped-defs is set.
        - Yes outside functions.
        - Yes in annotated functions.
        - No otherwise.
        """
        if self.options.check_untyped_defs or not self.function_stack:
            return True

        current_index = len(self.function_stack) - 1
        while current_index &gt;= 0:
            current_func = self.function_stack[current_index]
            if not isinstance(current_func, LambdaExpr):
                return not current_func.is_dynamic()

            # Special case, `lambda` inherits the "checked" state from its parent.
            # Because `lambda` itself cannot be annotated.
            # `lambdas` can be deeply nested, so we try to find at least one other parent.
            current_index -= 1

        # This means that we only have a stack of `lambda` functions,
        # no regular functions.
        return True

</t>
<t tx="ekr.20230831011820.1362">    def fail(
        self,
        msg: str,
        ctx: Context,
        serious: bool = False,
        *,
        code: ErrorCode | None = None,
        blocker: bool = False,
    ) -&gt; None:
        if not serious and not self.in_checked_function():
            return
        # In case it's a bug and we don't really have context
        assert ctx is not None, msg
        self.errors.report(ctx.line, ctx.column, msg, blocker=blocker, code=code)

</t>
<t tx="ekr.20230831011820.1363">    def note(self, msg: str, ctx: Context, code: ErrorCode | None = None) -&gt; None:
        if not self.in_checked_function():
            return
        self.errors.report(ctx.line, ctx.column, msg, severity="note", code=code)

</t>
<t tx="ekr.20230831011820.1364">    def incomplete_feature_enabled(self, feature: str, ctx: Context) -&gt; bool:
        if feature not in self.options.enable_incomplete_feature:
            self.fail(
                f'"{feature}" support is experimental,'
                f" use --enable-incomplete-feature={feature} to enable",
                ctx,
            )
            return False
        return True

</t>
<t tx="ekr.20230831011820.1365">    def accept(self, node: Node) -&gt; None:
        try:
            node.accept(self)
        except Exception as err:
            report_internal_error(err, self.errors.file, node.line, self.errors, self.options)

</t>
<t tx="ekr.20230831011820.1366">    def expr_to_analyzed_type(
        self,
        expr: Expression,
        report_invalid_types: bool = True,
        allow_placeholder: bool = False,
        allow_type_any: bool = False,
        allow_unbound_tvars: bool = False,
        allow_param_spec_literals: bool = False,
        allow_unpack: bool = False,
    ) -&gt; Type | None:
        if isinstance(expr, CallExpr):
            # This is a legacy syntax intended mostly for Python 2, we keep it for
            # backwards compatibility, but new features like generic named tuples
            # and recursive named tuples will be not supported.
            expr.accept(self)
            internal_name, info, tvar_defs = self.named_tuple_analyzer.check_namedtuple(
                expr, None, self.is_func_scope()
            )
            if tvar_defs:
                self.fail("Generic named tuples are not supported for legacy class syntax", expr)
                self.note("Use either Python 3 class syntax, or the assignment syntax", expr)
            if internal_name is None:
                # Some form of namedtuple is the only valid type that looks like a call
                # expression. This isn't a valid type.
                raise TypeTranslationError()
            elif not info:
                self.defer(expr)
                return None
            assert info.tuple_type, "NamedTuple without tuple type"
            fallback = Instance(info, [])
            return TupleType(info.tuple_type.items, fallback=fallback)
        typ = self.expr_to_unanalyzed_type(expr)
        return self.anal_type(
            typ,
            report_invalid_types=report_invalid_types,
            allow_placeholder=allow_placeholder,
            allow_type_any=allow_type_any,
            allow_unbound_tvars=allow_unbound_tvars,
            allow_param_spec_literals=allow_param_spec_literals,
            allow_unpack=allow_unpack,
        )

</t>
<t tx="ekr.20230831011820.1367">    def analyze_type_expr(self, expr: Expression) -&gt; None:
        # There are certain expressions that mypy does not need to semantically analyze,
        # since they analyzed solely as type. (For example, indexes in type alias definitions
        # and base classes in class defs). External consumers of the mypy AST may need
        # them semantically analyzed, however, if they need to treat it as an expression
        # and not a type. (Which is to say, mypyc needs to do this.) Do the analysis
        # in a fresh tvar scope in order to suppress any errors about using type variables.
        with self.tvar_scope_frame(TypeVarLikeScope()), self.allow_unbound_tvars_set():
            expr.accept(self)

</t>
<t tx="ekr.20230831011820.1368">    def type_analyzer(
        self,
        *,
        tvar_scope: TypeVarLikeScope | None = None,
        allow_tuple_literal: bool = False,
        allow_unbound_tvars: bool = False,
        allow_placeholder: bool = False,
        allow_required: bool = False,
        allow_param_spec_literals: bool = False,
        allow_unpack: bool = False,
        report_invalid_types: bool = True,
        prohibit_self_type: str | None = None,
        allow_type_any: bool = False,
    ) -&gt; TypeAnalyser:
        if tvar_scope is None:
            tvar_scope = self.tvar_scope
        tpan = TypeAnalyser(
            self,
            tvar_scope,
            self.plugin,
            self.options,
            self.is_typeshed_stub_file,
            allow_unbound_tvars=allow_unbound_tvars,
            allow_tuple_literal=allow_tuple_literal,
            report_invalid_types=report_invalid_types,
            allow_placeholder=allow_placeholder,
            allow_required=allow_required,
            allow_param_spec_literals=allow_param_spec_literals,
            allow_unpack=allow_unpack,
            prohibit_self_type=prohibit_self_type,
            allow_type_any=allow_type_any,
        )
        tpan.in_dynamic_func = bool(self.function_stack and self.function_stack[-1].is_dynamic())
        tpan.global_scope = not self.type and not self.function_stack
        return tpan

</t>
<t tx="ekr.20230831011820.1369">    def expr_to_unanalyzed_type(self, node: Expression) -&gt; ProperType:
        return expr_to_unanalyzed_type(node, self.options, self.is_stub_file)

</t>
<t tx="ekr.20230831011820.137">    def cannot_instantiate_abstract_class(
        self, class_name: str, abstract_attributes: dict[str, bool], context: Context
    ) -&gt; None:
        attrs = format_string_list([f'"{a}"' for a in abstract_attributes])
        self.fail(
            'Cannot instantiate abstract class "%s" with abstract '
            "attribute%s %s" % (class_name, plural_s(abstract_attributes), attrs),
            context,
            code=codes.ABSTRACT,
        )
        attrs_with_none = [
            f'"{a}"'
            for a, implicit_and_can_return_none in abstract_attributes.items()
            if implicit_and_can_return_none
        ]
        if not attrs_with_none:
            return
        if len(attrs_with_none) == 1:
            note = (
                f"{attrs_with_none[0]} is implicitly abstract because it has an empty function "
                "body. If it is not meant to be abstract, explicitly `return` or `return None`."
            )
        else:
            note = (
                "The following methods were marked implicitly abstract because they have empty "
                f"function bodies: {format_string_list(attrs_with_none)}. "
                "If they are not meant to be abstract, explicitly `return` or `return None`."
            )
        self.note(note, context, code=codes.ABSTRACT)

</t>
<t tx="ekr.20230831011820.1370">    def anal_type(
        self,
        typ: Type,
        *,
        tvar_scope: TypeVarLikeScope | None = None,
        allow_tuple_literal: bool = False,
        allow_unbound_tvars: bool = False,
        allow_placeholder: bool = False,
        allow_required: bool = False,
        allow_param_spec_literals: bool = False,
        allow_unpack: bool = False,
        report_invalid_types: bool = True,
        prohibit_self_type: str | None = None,
        allow_type_any: bool = False,
        third_pass: bool = False,
    ) -&gt; Type | None:
        """Semantically analyze a type.

        Args:
            typ: Type to analyze (if already analyzed, this is a no-op)
            allow_placeholder: If True, may return PlaceholderType if
                encountering an incomplete definition
            third_pass: Unused; only for compatibility with old semantic
                analyzer

        Return None only if some part of the type couldn't be bound *and* it
        referred to an incomplete namespace or definition. In this case also
        defer as needed. During a final iteration this won't return None;
        instead report an error if the type can't be analyzed and return
        AnyType.

        In case of other errors, report an error message and return AnyType.

        NOTE: The caller shouldn't defer even if this returns None or a
              placeholder type.
        """
        has_self_type = find_self_type(
            typ, lambda name: self.lookup_qualified(name, typ, suppress_errors=True)
        )
        if has_self_type and self.type and prohibit_self_type is None:
            self.setup_self_type()
        a = self.type_analyzer(
            tvar_scope=tvar_scope,
            allow_unbound_tvars=allow_unbound_tvars,
            allow_tuple_literal=allow_tuple_literal,
            allow_placeholder=allow_placeholder,
            allow_required=allow_required,
            allow_param_spec_literals=allow_param_spec_literals,
            allow_unpack=allow_unpack,
            report_invalid_types=report_invalid_types,
            prohibit_self_type=prohibit_self_type,
            allow_type_any=allow_type_any,
        )
        tag = self.track_incomplete_refs()
        typ = typ.accept(a)
        if self.found_incomplete_ref(tag):
            # Something could not be bound yet.
            return None
        self.add_type_alias_deps(a.aliases_used)
        return typ

</t>
<t tx="ekr.20230831011820.1371">    def class_type(self, self_type: Type) -&gt; Type:
        return TypeType.make_normalized(self_type)

</t>
<t tx="ekr.20230831011820.1372">    def schedule_patch(self, priority: int, patch: Callable[[], None]) -&gt; None:
        self.patches.append((priority, patch))

</t>
<t tx="ekr.20230831011820.1373">    def report_hang(self) -&gt; None:
        print("Deferral trace:")
        for mod, line in self.deferral_debug_context:
            print(f"    {mod}:{line}")
        self.errors.report(
            -1,
            -1,
            "INTERNAL ERROR: maximum semantic analysis iteration count reached",
            blocker=True,
        )

</t>
<t tx="ekr.20230831011820.1374">    def add_plugin_dependency(self, trigger: str, target: str | None = None) -&gt; None:
        """Add dependency from trigger to a target.

        If the target is not given explicitly, use the current target.
        """
        if target is None:
            target = self.scope.current_target()
        self.cur_mod_node.plugin_deps.setdefault(trigger, set()).add(target)

</t>
<t tx="ekr.20230831011820.1375">    def add_type_alias_deps(
        self, aliases_used: Collection[str], target: str | None = None
    ) -&gt; None:
        """Add full names of type aliases on which the current node depends.

        This is used by fine-grained incremental mode to re-check the corresponding nodes.
        If `target` is None, then the target node used will be the current scope.
        """
        if not aliases_used:
            # A basic optimization to avoid adding targets with no dependencies to
            # the `alias_deps` dict.
            return
        if target is None:
            target = self.scope.current_target()
        self.cur_mod_node.alias_deps[target].update(aliases_used)

</t>
<t tx="ekr.20230831011820.1376">    def is_mangled_global(self, name: str) -&gt; bool:
        # A global is mangled if there exists at least one renamed variant.
        return unmangle(name) + "'" in self.globals

</t>
<t tx="ekr.20230831011820.1377">    def is_initial_mangled_global(self, name: str) -&gt; bool:
        # If there are renamed definitions for a global, the first one has exactly one prime.
        return name == unmangle(name) + "'"

</t>
<t tx="ekr.20230831011820.1378">    def parse_bool(self, expr: Expression) -&gt; bool | None:
        # This wrapper is preserved for plugins.
        return parse_bool(expr)

</t>
<t tx="ekr.20230831011820.1379">    def parse_str_literal(self, expr: Expression) -&gt; str | None:
        """Attempt to find the string literal value of the given expression. Returns `None` if no
        literal value can be found."""
        if isinstance(expr, StrExpr):
            return expr.value
        if isinstance(expr, RefExpr) and isinstance(expr.node, Var) and expr.node.type is not None:
            values = try_getting_str_literals_from_type(expr.node.type)
            if values is not None and len(values) == 1:
                return values[0]
        return None

</t>
<t tx="ekr.20230831011820.138">    def base_class_definitions_incompatible(
        self, name: str, base1: TypeInfo, base2: TypeInfo, context: Context
    ) -&gt; None:
        self.fail(
            'Definition of "{}" in base class "{}" is incompatible '
            'with definition in base class "{}"'.format(name, base1.name, base2.name),
            context,
        )

</t>
<t tx="ekr.20230831011820.1380">    def set_future_import_flags(self, module_name: str) -&gt; None:
        if module_name in FUTURE_IMPORTS:
            self.modules[self.cur_mod_id].future_import_flags.add(FUTURE_IMPORTS[module_name])

</t>
<t tx="ekr.20230831011820.1381">    def is_future_flag_set(self, flag: str) -&gt; bool:
        return self.modules[self.cur_mod_id].is_future_flag_set(flag)

</t>
<t tx="ekr.20230831011820.1382">    def parse_dataclass_transform_spec(self, call: CallExpr) -&gt; DataclassTransformSpec:
        """Build a DataclassTransformSpec from the arguments passed to the given call to
        typing.dataclass_transform."""
        parameters = DataclassTransformSpec()
        for name, value in zip(call.arg_names, call.args):
            # Skip any positional args. Note that any such args are invalid, but we can rely on
            # typeshed to enforce this and don't need an additional error here.
            if name is None:
                continue

            # field_specifiers is currently the only non-boolean argument; check for it first so
            # so the rest of the block can fail through to handling booleans
            if name == "field_specifiers":
                parameters.field_specifiers = self.parse_dataclass_transform_field_specifiers(
                    value
                )
                continue

            boolean = require_bool_literal_argument(self, value, name)
            if boolean is None:
                continue

            if name == "eq_default":
                parameters.eq_default = boolean
            elif name == "order_default":
                parameters.order_default = boolean
            elif name == "kw_only_default":
                parameters.kw_only_default = boolean
            elif name == "frozen_default":
                parameters.frozen_default = boolean
            else:
                self.fail(f'Unrecognized dataclass_transform parameter "{name}"', call)

        return parameters

</t>
<t tx="ekr.20230831011820.1383">    def parse_dataclass_transform_field_specifiers(self, arg: Expression) -&gt; tuple[str, ...]:
        if not isinstance(arg, TupleExpr):
            self.fail('"field_specifiers" argument must be a tuple literal', arg)
            return tuple()

        names = []
        for specifier in arg.items:
            if not isinstance(specifier, RefExpr):
                self.fail('"field_specifiers" must only contain identifiers', specifier)
                return tuple()
            names.append(specifier.fullname)
        return tuple(names)


</t>
<t tx="ekr.20230831011820.1384">def replace_implicit_first_type(sig: FunctionLike, new: Type) -&gt; FunctionLike:
    if isinstance(sig, CallableType):
        if len(sig.arg_types) == 0:
            return sig
        return sig.copy_modified(arg_types=[new] + sig.arg_types[1:])
    elif isinstance(sig, Overloaded):
        return Overloaded(
            [cast(CallableType, replace_implicit_first_type(i, new)) for i in sig.items]
        )
    else:
        assert False


</t>
<t tx="ekr.20230831011820.1385">def refers_to_fullname(node: Expression, fullnames: str | tuple[str, ...]) -&gt; bool:
    """Is node a name or member expression with the given full name?"""
    if not isinstance(fullnames, tuple):
        fullnames = (fullnames,)

    if not isinstance(node, RefExpr):
        return False
    if node.fullname in fullnames:
        return True
    if isinstance(node.node, TypeAlias):
        return is_named_instance(node.node.target, fullnames)
    return False


</t>
<t tx="ekr.20230831011820.1386">def refers_to_class_or_function(node: Expression) -&gt; bool:
    """Does semantically analyzed node refer to a class?"""
    return isinstance(node, RefExpr) and isinstance(
        node.node, (TypeInfo, FuncDef, OverloadedFuncDef)
    )


</t>
<t tx="ekr.20230831011820.1387">def find_duplicate(list: list[T]) -&gt; T | None:
    """If the list has duplicates, return one of the duplicates.

    Otherwise, return None.
    """
    for i in range(1, len(list)):
        if list[i] in list[:i]:
            return list[i]
    return None


</t>
<t tx="ekr.20230831011820.1388">def remove_imported_names_from_symtable(names: SymbolTable, module: str) -&gt; None:
    """Remove all imported names from the symbol table of a module."""
    removed: list[str] = []
    for name, node in names.items():
        if node.node is None:
            continue
        fullname = node.node.fullname
        prefix = fullname[: fullname.rfind(".")]
        if prefix != module:
            removed.append(name)
    for name in removed:
        del names[name]


</t>
<t tx="ekr.20230831011820.1389">def make_any_non_explicit(t: Type) -&gt; Type:
    """Replace all Any types within in with Any that has attribute 'explicit' set to False"""
    return t.accept(MakeAnyNonExplicit())


</t>
<t tx="ekr.20230831011820.139">    def cant_assign_to_method(self, context: Context) -&gt; None:
        self.fail(message_registry.CANNOT_ASSIGN_TO_METHOD, context, code=codes.METHOD_ASSIGN)

</t>
<t tx="ekr.20230831011820.1390">class MakeAnyNonExplicit(TrivialSyntheticTypeTranslator):
    @others
</t>
<t tx="ekr.20230831011820.1391">def visit_any(self, t: AnyType) -&gt; Type:
    if t.type_of_any == TypeOfAny.explicit:
        return t.copy_modified(TypeOfAny.special_form)
    return t

</t>
<t tx="ekr.20230831011820.1392">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20230831011820.1393">def apply_semantic_analyzer_patches(patches: list[tuple[int, Callable[[], None]]]) -&gt; None:
    """Call patch callbacks in the right order.

    This should happen after semantic analyzer pass 3.
    """
    patches_by_priority = sorted(patches, key=lambda x: x[0])
    for priority, patch_func in patches_by_priority:
        patch_func()


</t>
<t tx="ekr.20230831011820.1394">def names_modified_by_assignment(s: AssignmentStmt) -&gt; list[NameExpr]:
    """Return all unqualified (short) names assigned to in an assignment statement."""
    result: list[NameExpr] = []
    for lvalue in s.lvalues:
        result += names_modified_in_lvalue(lvalue)
    return result


</t>
<t tx="ekr.20230831011820.1395">def names_modified_in_lvalue(lvalue: Lvalue) -&gt; list[NameExpr]:
    """Return all NameExpr assignment targets in an Lvalue."""
    if isinstance(lvalue, NameExpr):
        return [lvalue]
    elif isinstance(lvalue, StarExpr):
        return names_modified_in_lvalue(lvalue.expr)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        result: list[NameExpr] = []
        for item in lvalue.items:
            result += names_modified_in_lvalue(item)
        return result
    return []


</t>
<t tx="ekr.20230831011820.1396">def is_same_var_from_getattr(n1: SymbolNode | None, n2: SymbolNode | None) -&gt; bool:
    """Do n1 and n2 refer to the same Var derived from module-level __getattr__?"""
    return (
        isinstance(n1, Var)
        and n1.from_module_getattr
        and isinstance(n2, Var)
        and n2.from_module_getattr
        and n1.fullname == n2.fullname
    )


</t>
<t tx="ekr.20230831011820.1397">def dummy_context() -&gt; Context:
    return TempNode(AnyType(TypeOfAny.special_form))


</t>
<t tx="ekr.20230831011820.1398">def is_valid_replacement(old: SymbolTableNode, new: SymbolTableNode) -&gt; bool:
    """Can symbol table node replace an existing one?

    These are the only valid cases:

    1. Placeholder gets replaced with a non-placeholder
    2. Placeholder that isn't known to become type replaced with a
       placeholder that can become a type
    """
    if isinstance(old.node, PlaceholderNode):
        if isinstance(new.node, PlaceholderNode):
            return not old.node.becomes_typeinfo and new.node.becomes_typeinfo
        else:
            return True
    return False


</t>
<t tx="ekr.20230831011820.1399">def is_same_symbol(a: SymbolNode | None, b: SymbolNode | None) -&gt; bool:
    return (
        a == b
        or (isinstance(a, PlaceholderNode) and isinstance(b, PlaceholderNode))
        or is_same_var_from_getattr(a, b)
    )


</t>
<t tx="ekr.20230831011820.14">from __future__ import annotations

from typing import Callable

from mypy import join
from mypy.erasetype import erase_type
from mypy.maptype import map_instance_to_supertype
from mypy.state import state
from mypy.subtypes import (
    is_callable_compatible,
    is_equivalent,
    is_proper_subtype,
    is_same_type,
    is_subtype,
)
from mypy.typeops import is_recursive_pair, make_simplified_union, tuple_fallback
from mypy.types import (
    MYPYC_NATIVE_INT_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeGuardedType,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)

# TODO Describe this module.


</t>
<t tx="ekr.20230831011820.140">    def cant_assign_to_classvar(self, name: str, context: Context) -&gt; None:
        self.fail(f'Cannot assign to class variable "{name}" via instance', context)

</t>
<t tx="ekr.20230831011820.1400">def is_trivial_body(block: Block) -&gt; bool:
    """Returns 'true' if the given body is "trivial" -- if it contains just a "pass",
    "..." (ellipsis), or "raise NotImplementedError()". A trivial body may also
    start with a statement containing just a string (e.g. a docstring).

    Note: Functions that raise other kinds of exceptions do not count as
    "trivial". We use this function to help us determine when it's ok to
    relax certain checks on body, but functions that raise arbitrary exceptions
    are more likely to do non-trivial work. For example:

       def halt(self, reason: str = ...) -&gt; NoReturn:
           raise MyCustomError("Fatal error: " + reason, self.line, self.context)

    A function that raises just NotImplementedError is much less likely to be
    this complex.

    Note: If you update this, you may also need to update
    mypy.fastparse.is_possible_trivial_body!
    """
    body = block.body
    if not body:
        # Functions have empty bodies only if the body is stripped or the function is
        # generated or deserialized. In these cases the body is unknown.
        return False

    # Skip a docstring
    if isinstance(body[0], ExpressionStmt) and isinstance(body[0].expr, StrExpr):
        body = block.body[1:]

    if len(body) == 0:
        # There's only a docstring (or no body at all).
        return True
    elif len(body) &gt; 1:
        return False

    stmt = body[0]

    if isinstance(stmt, RaiseStmt):
        expr = stmt.expr
        if expr is None:
            return False
        if isinstance(expr, CallExpr):
            expr = expr.callee

        return isinstance(expr, NameExpr) and expr.fullname == "builtins.NotImplementedError"

    return isinstance(stmt, PassStmt) or (
        isinstance(stmt, ExpressionStmt) and isinstance(stmt.expr, EllipsisExpr)
    )
</t>
<t tx="ekr.20230831011820.1401">@path mypy
"""Calculate some properties of classes.

These happen after semantic analysis and before type checking.
"""

&lt;&lt; semanal_classprop.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1403">from __future__ import annotations

from typing import Final

from mypy.errors import Errors
from mypy.nodes import (
    IMPLICITLY_ABSTRACT,
    IS_ABSTRACT,
    CallExpr,
    Decorator,
    FuncDef,
    Node,
    OverloadedFuncDef,
    PromoteExpr,
    SymbolTable,
    TypeInfo,
    Var,
)
from mypy.options import Options
from mypy.types import MYPYC_NATIVE_INT_NAMES, Instance, ProperType

# Hard coded type promotions (shared between all Python versions).
# These add extra ad-hoc edges to the subtyping relation. For example,
# int is considered a subtype of float, even though there is no
# subclass relationship.
# Note that the bytearray -&gt; bytes promotion is a little unsafe
# as some functions only accept bytes objects. Here convenience
# trumps safety.
TYPE_PROMOTIONS: Final = {
    "builtins.int": "float",
    "builtins.float": "complex",
    "builtins.bytearray": "bytes",
    "builtins.memoryview": "bytes",
}


</t>
<t tx="ekr.20230831011820.1404">def calculate_class_abstract_status(typ: TypeInfo, is_stub_file: bool, errors: Errors) -&gt; None:
    """Calculate abstract status of a class.

    Set is_abstract of the type to True if the type has an unimplemented
    abstract attribute.  Also compute a list of abstract attributes.
    Report error is required ABCMeta metaclass is missing.
    """
    if typ.typeddict_type:
        return  # TypedDict can't be abstract
    concrete: set[str] = set()
    # List of abstract attributes together with their abstract status
    abstract: list[tuple[str, int]] = []
    abstract_in_this_class: list[str] = []
    if typ.is_newtype:
        # Special case: NewTypes are considered as always non-abstract, so they can be used as:
        #     Config = NewType('Config', Mapping[str, str])
        #     default = Config({'cannot': 'modify'})  # OK
        typ.abstract_attributes = []
        return
    for base in typ.mro:
        for name, symnode in base.names.items():
            node = symnode.node
            if isinstance(node, OverloadedFuncDef):
                # Unwrap an overloaded function definition. We can just
                # check arbitrarily the first overload item. If the
                # different items have a different abstract status, there
                # should be an error reported elsewhere.
                if node.items:  # can be empty for invalid overloads
                    func: Node | None = node.items[0]
                else:
                    func = None
            else:
                func = node
            if isinstance(func, Decorator):
                func = func.func
            if isinstance(func, FuncDef):
                if (
                    func.abstract_status in (IS_ABSTRACT, IMPLICITLY_ABSTRACT)
                    and name not in concrete
                ):
                    typ.is_abstract = True
                    abstract.append((name, func.abstract_status))
                    if base is typ:
                        abstract_in_this_class.append(name)
            elif isinstance(node, Var):
                if node.is_abstract_var and name not in concrete:
                    typ.is_abstract = True
                    abstract.append((name, IS_ABSTRACT))
                    if base is typ:
                        abstract_in_this_class.append(name)
            concrete.add(name)
    # In stubs, abstract classes need to be explicitly marked because it is too
    # easy to accidentally leave a concrete class abstract by forgetting to
    # implement some methods.
    typ.abstract_attributes = sorted(abstract)
    if is_stub_file:
        if typ.declared_metaclass and typ.declared_metaclass.type.has_base("abc.ABCMeta"):
            return
        if typ.is_protocol:
            return
        if abstract and not abstract_in_this_class:

            def report(message: str, severity: str) -&gt; None:
                errors.report(typ.line, typ.column, message, severity=severity)

            attrs = ", ".join(f'"{attr}"' for attr, _ in sorted(abstract))
            report(f"Class {typ.fullname} has abstract attributes {attrs}", "error")
            report(
                "If it is meant to be abstract, add 'abc.ABCMeta' as an explicit metaclass", "note"
            )
    if typ.is_final and abstract:
        attrs = ", ".join(f'"{attr}"' for attr, _ in sorted(abstract))
        errors.report(
            typ.line, typ.column, f"Final class {typ.fullname} has abstract attributes {attrs}"
        )


</t>
<t tx="ekr.20230831011820.1405">def check_protocol_status(info: TypeInfo, errors: Errors) -&gt; None:
    """Check that all classes in MRO of a protocol are protocols"""
    if info.is_protocol:
        for type in info.bases:
            if not type.type.is_protocol and type.type.fullname != "builtins.object":

                def report(message: str, severity: str) -&gt; None:
                    errors.report(info.line, info.column, message, severity=severity)

                report("All bases of a protocol must be protocols", "error")


</t>
<t tx="ekr.20230831011820.1406">def calculate_class_vars(info: TypeInfo) -&gt; None:
    """Try to infer additional class variables.

    Subclass attribute assignments with no type annotation are assumed
    to be classvar if overriding a declared classvar from the base
    class.

    This must happen after the main semantic analysis pass, since
    this depends on base class bodies having been fully analyzed.
    """
    for name, sym in info.names.items():
        node = sym.node
        if isinstance(node, Var) and node.info and node.is_inferred and not node.is_classvar:
            for base in info.mro[1:]:
                member = base.names.get(name)
                if member is not None and isinstance(member.node, Var) and member.node.is_classvar:
                    node.is_classvar = True


</t>
<t tx="ekr.20230831011820.1407">def add_type_promotion(
    info: TypeInfo, module_names: SymbolTable, options: Options, builtin_names: SymbolTable
) -&gt; None:
    """Setup extra, ad-hoc subtyping relationships between classes (promotion).

    This includes things like 'int' being compatible with 'float'.
    """
    defn = info.defn
    promote_targets: list[ProperType] = []
    for decorator in defn.decorators:
        if isinstance(decorator, CallExpr):
            analyzed = decorator.analyzed
            if isinstance(analyzed, PromoteExpr):
                # _promote class decorator (undocumented feature).
                promote_targets.append(analyzed.type)
    if not promote_targets:
        if defn.fullname in TYPE_PROMOTIONS:
            target_sym = module_names.get(TYPE_PROMOTIONS[defn.fullname])
            if defn.fullname == "builtins.bytearray" and options.disable_bytearray_promotion:
                target_sym = None
            elif defn.fullname == "builtins.memoryview" and options.disable_memoryview_promotion:
                target_sym = None
            # With test stubs, the target may not exist.
            if target_sym:
                target_info = target_sym.node
                assert isinstance(target_info, TypeInfo)
                promote_targets.append(Instance(target_info, []))
    # Special case the promotions between 'int' and native integer types.
    # These have promotions going both ways, such as from 'int' to 'i64'
    # and 'i64' to 'int', for convenience.
    if defn.fullname in MYPYC_NATIVE_INT_NAMES:
        int_sym = builtin_names["int"]
        assert isinstance(int_sym.node, TypeInfo)
        int_sym.node._promote.append(Instance(defn.info, []))
        defn.info.alt_promote = Instance(int_sym.node, [])
    if promote_targets:
        defn.info._promote.extend(promote_targets)
</t>
<t tx="ekr.20230831011820.1408">@path mypy
"""Semantic analysis of call-based Enum definitions.

This is conceptually part of mypy.semanal (semantic analyzer pass 2).
"""

&lt;&lt; semanal_enum.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.141">    def no_overridable_method(self, name: str, context: Context) -&gt; None:
        self.fail(
            f'Method "{name}" is marked as an override, '
            "but no base method was found with this name",
            context,
        )

</t>
<t tx="ekr.20230831011820.1410">from __future__ import annotations

from typing import Final, cast

from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    MDEF,
    AssignmentStmt,
    CallExpr,
    Context,
    DictExpr,
    EnumCallExpr,
    Expression,
    ListExpr,
    MemberExpr,
    NameExpr,
    RefExpr,
    StrExpr,
    SymbolTableNode,
    TupleExpr,
    TypeInfo,
    Var,
    is_StrExpr_list,
)
from mypy.options import Options
from mypy.semanal_shared import SemanticAnalyzerInterface
from mypy.types import ENUM_REMOVED_PROPS, LiteralType, get_proper_type

# Note: 'enum.EnumMeta' is deliberately excluded from this list. Classes that directly use
# enum.EnumMeta do not necessarily automatically have the 'name' and 'value' attributes.
ENUM_BASES: Final = frozenset(
    ("enum.Enum", "enum.IntEnum", "enum.Flag", "enum.IntFlag", "enum.StrEnum")
)
ENUM_SPECIAL_PROPS: Final = frozenset(
    (
        "name",
        "value",
        "_name_",
        "_value_",
        *ENUM_REMOVED_PROPS,
        # Also attributes from `object`:
        "__module__",
        "__annotations__",
        "__doc__",
        "__slots__",
        "__dict__",
    )
)


</t>
<t tx="ekr.20230831011820.1411">class EnumCallAnalyzer:
    @others
</t>
<t tx="ekr.20230831011820.1412">def __init__(self, options: Options, api: SemanticAnalyzerInterface) -&gt; None:
    self.options = options
    self.api = api

</t>
<t tx="ekr.20230831011820.1413">def process_enum_call(self, s: AssignmentStmt, is_func_scope: bool) -&gt; bool:
    """Check if s defines an Enum; if yes, store the definition in symbol table.

    Return True if this looks like an Enum definition (but maybe with errors),
    otherwise return False.
    """
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    enum_call = self.check_enum_call(s.rvalue, name, is_func_scope)
    if enum_call is None:
        return False
    if isinstance(lvalue, MemberExpr):
        self.fail("Enum type as attribute is not supported", lvalue)
        return False
    # Yes, it's a valid Enum definition. Add it to the symbol table.
    self.api.add_symbol(name, enum_call, s)
    return True

</t>
<t tx="ekr.20230831011820.1414">def check_enum_call(
    self, node: Expression, var_name: str, is_func_scope: bool
) -&gt; TypeInfo | None:
    """Check if a call defines an Enum.

    Example:

      A = enum.Enum('A', 'foo bar')

    is equivalent to:

      class A(enum.Enum):
          foo = 1
          bar = 2
    """
    if not isinstance(node, CallExpr):
        return None
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None
    fullname = callee.fullname
    if fullname not in ENUM_BASES:
        return None
    items, values, ok = self.parse_enum_call_args(call, fullname.split(".")[-1])
    if not ok:
        # Error. Construct dummy return value.
        info = self.build_enum_call_typeinfo(var_name, [], fullname, node.line)
    else:
        name = cast(StrExpr, call.args[0]).value
        if name != var_name or is_func_scope:
            # Give it a unique name derived from the line number.
            name += "@" + str(call.line)
        info = self.build_enum_call_typeinfo(name, items, fullname, call.line)
        # Store generated TypeInfo under both names, see semanal_namedtuple for more details.
        if name != var_name or is_func_scope:
            self.api.add_symbol_skip_local(name, info)
    call.analyzed = EnumCallExpr(info, items, values)
    call.analyzed.set_line(call)
    info.line = node.line
    return info

</t>
<t tx="ekr.20230831011820.1415">def build_enum_call_typeinfo(
    self, name: str, items: list[str], fullname: str, line: int
) -&gt; TypeInfo:
    base = self.api.named_type_or_none(fullname)
    assert base is not None
    info = self.api.basic_new_typeinfo(name, base, line)
    info.metaclass_type = info.calculate_metaclass_type()
    info.is_enum = True
    for item in items:
        var = Var(item)
        var.info = info
        var.is_property = True
        var._fullname = f"{info.fullname}.{item}"
        info.names[item] = SymbolTableNode(MDEF, var)
    return info

</t>
<t tx="ekr.20230831011820.1416">def parse_enum_call_args(
    self, call: CallExpr, class_name: str
) -&gt; tuple[list[str], list[Expression | None], bool]:
    """Parse arguments of an Enum call.

    Return a tuple of fields, values, was there an error.
    """
    args = call.args
    if not all([arg_kind in [ARG_POS, ARG_NAMED] for arg_kind in call.arg_kinds]):
        return self.fail_enum_call_arg(f"Unexpected arguments to {class_name}()", call)
    if len(args) &lt; 2:
        return self.fail_enum_call_arg(f"Too few arguments for {class_name}()", call)
    if len(args) &gt; 6:
        return self.fail_enum_call_arg(f"Too many arguments for {class_name}()", call)
    valid_name = [None, "value", "names", "module", "qualname", "type", "start"]
    for arg_name in call.arg_names:
        if arg_name not in valid_name:
            self.fail_enum_call_arg(f'Unexpected keyword argument "{arg_name}"', call)
    value, names = None, None
    for arg_name, arg in zip(call.arg_names, args):
        if arg_name == "value":
            value = arg
        if arg_name == "names":
            names = arg
    if value is None:
        value = args[0]
    if names is None:
        names = args[1]
    if not isinstance(value, StrExpr):
        return self.fail_enum_call_arg(
            f"{class_name}() expects a string literal as the first argument", call
        )
    items = []
    values: list[Expression | None] = []
    if isinstance(names, StrExpr):
        fields = names.value
        for field in fields.replace(",", " ").split():
            items.append(field)
    elif isinstance(names, (TupleExpr, ListExpr)):
        seq_items = names.items
        if is_StrExpr_list(seq_items):
            items = [seq_item.value for seq_item in seq_items]
        elif all(
            isinstance(seq_item, (TupleExpr, ListExpr))
            and len(seq_item.items) == 2
            and isinstance(seq_item.items[0], StrExpr)
            for seq_item in seq_items
        ):
            for seq_item in seq_items:
                assert isinstance(seq_item, (TupleExpr, ListExpr))
                name, value = seq_item.items
                assert isinstance(name, StrExpr)
                items.append(name.value)
                values.append(value)
        else:
            return self.fail_enum_call_arg(
                "%s() with tuple or list expects strings or (name, value) pairs" % class_name,
                call,
            )
    elif isinstance(names, DictExpr):
        for key, value in names.items:
            if not isinstance(key, StrExpr):
                return self.fail_enum_call_arg(
                    f"{class_name}() with dict literal requires string literals", call
                )
            items.append(key.value)
            values.append(value)
    elif isinstance(args[1], RefExpr) and isinstance(args[1].node, Var):
        proper_type = get_proper_type(args[1].node.type)
        if (
            proper_type is not None
            and isinstance(proper_type, LiteralType)
            and isinstance(proper_type.value, str)
        ):
            fields = proper_type.value
            for field in fields.replace(",", " ").split():
                items.append(field)
        elif args[1].node.is_final and isinstance(args[1].node.final_value, str):
            fields = args[1].node.final_value
            for field in fields.replace(",", " ").split():
                items.append(field)
        else:
            return self.fail_enum_call_arg(
                "Second argument of %s() must be string, tuple, list or dict literal for mypy to determine Enum members"
                % class_name,
                call,
            )
    else:
        # TODO: Allow dict(x=1, y=2) as a substitute for {'x': 1, 'y': 2}?
        return self.fail_enum_call_arg(
            "Second argument of %s() must be string, tuple, list or dict literal for mypy to determine Enum members"
            % class_name,
            call,
        )
    if not items:
        return self.fail_enum_call_arg(f"{class_name}() needs at least one item", call)
    if not values:
        values = [None] * len(items)
    assert len(items) == len(values)
    return items, values, True

</t>
<t tx="ekr.20230831011820.1417">def fail_enum_call_arg(
    self, message: str, context: Context
) -&gt; tuple[list[str], list[Expression | None], bool]:
    self.fail(message, context)
    return [], [], False

</t>
<t tx="ekr.20230831011820.1418"># Helpers

def fail(self, msg: str, ctx: Context) -&gt; None:
    self.api.fail(msg, ctx)
</t>
<t tx="ekr.20230831011820.1419">@path mypy
"""Simple type inference for decorated functions during semantic analysis."""
&lt;&lt; semanal_infer.py: imports &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.142">    def explicit_override_decorator_missing(
        self, name: str, base_name: str, context: Context
    ) -&gt; None:
        self.fail(
            f'Method "{name}" is not using @override '
            f'but is overriding a method in class "{base_name}"',
            context,
            code=codes.EXPLICIT_OVERRIDE_REQUIRED,
        )

</t>
<t tx="ekr.20230831011820.1420">
from __future__ import annotations

from mypy.nodes import ARG_POS, CallExpr, Decorator, Expression, FuncDef, RefExpr, Var
from mypy.semanal_shared import SemanticAnalyzerInterface
from mypy.typeops import function_type
from mypy.types import (
    AnyType,
    CallableType,
    ProperType,
    Type,
    TypeOfAny,
    TypeVarType,
    get_proper_type,
)
from mypy.typevars import has_no_typevars


</t>
<t tx="ekr.20230831011820.1421">def infer_decorator_signature_if_simple(
    dec: Decorator, analyzer: SemanticAnalyzerInterface
) -&gt; None:
    """Try to infer the type of the decorated function.

    This lets us resolve additional references to decorated functions
    during type checking. Otherwise the type might not be available
    when we need it, since module top levels can't be deferred.

    This basically uses a simple special-purpose type inference
    engine just for decorators.
    """
    if dec.var.is_property:
        # Decorators are expected to have a callable type (it's a little odd).
        if dec.func.type is None:
            dec.var.type = CallableType(
                [AnyType(TypeOfAny.special_form)],
                [ARG_POS],
                [None],
                AnyType(TypeOfAny.special_form),
                analyzer.named_type("builtins.function"),
                name=dec.var.name,
            )
        elif isinstance(dec.func.type, CallableType):
            dec.var.type = dec.func.type
        return
    decorator_preserves_type = True
    for expr in dec.decorators:
        preserve_type = False
        if isinstance(expr, RefExpr) and isinstance(expr.node, FuncDef):
            if expr.node.type and is_identity_signature(expr.node.type):
                preserve_type = True
        if not preserve_type:
            decorator_preserves_type = False
            break
    if decorator_preserves_type:
        # No non-identity decorators left. We can trivially infer the type
        # of the function here.
        dec.var.type = function_type(dec.func, analyzer.named_type("builtins.function"))
    if dec.decorators:
        return_type = calculate_return_type(dec.decorators[0])
        if return_type and isinstance(return_type, AnyType):
            # The outermost decorator will return Any so we know the type of the
            # decorated function.
            dec.var.type = AnyType(TypeOfAny.from_another_any, source_any=return_type)
        sig = find_fixed_callable_return(dec.decorators[0])
        if sig:
            # The outermost decorator always returns the same kind of function,
            # so we know that this is the type of the decorated function.
            orig_sig = function_type(dec.func, analyzer.named_type("builtins.function"))
            sig.name = orig_sig.items[0].name
            dec.var.type = sig


</t>
<t tx="ekr.20230831011820.1422">def is_identity_signature(sig: Type) -&gt; bool:
    """Is type a callable of form T -&gt; T (where T is a type variable)?"""
    sig = get_proper_type(sig)
    if isinstance(sig, CallableType) and sig.arg_kinds == [ARG_POS]:
        if isinstance(sig.arg_types[0], TypeVarType) and isinstance(sig.ret_type, TypeVarType):
            return sig.arg_types[0].id == sig.ret_type.id
    return False


</t>
<t tx="ekr.20230831011820.1423">def calculate_return_type(expr: Expression) -&gt; ProperType | None:
    """Return the return type if we can calculate it.

    This only uses information available during semantic analysis so this
    will sometimes return None because of insufficient information (as
    type inference hasn't run yet).
    """
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, FuncDef):
            typ = expr.node.type
            if typ is None:
                # No signature -&gt; default to Any.
                return AnyType(TypeOfAny.unannotated)
            # Explicit Any return?
            if isinstance(typ, CallableType):
                return get_proper_type(typ.ret_type)
            return None
        elif isinstance(expr.node, Var):
            return get_proper_type(expr.node.type)
    elif isinstance(expr, CallExpr):
        return calculate_return_type(expr.callee)
    return None


</t>
<t tx="ekr.20230831011820.1424">def find_fixed_callable_return(expr: Expression) -&gt; CallableType | None:
    """Return the return type, if expression refers to a callable that returns a callable.

    But only do this if the return type has no type variables. Return None otherwise.
    This approximates things a lot as this is supposed to be called before type checking
    when full type information is not available yet.
    """
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, FuncDef):
            typ = expr.node.type
            if typ:
                if isinstance(typ, CallableType) and has_no_typevars(typ.ret_type):
                    ret_type = get_proper_type(typ.ret_type)
                    if isinstance(ret_type, CallableType):
                        return ret_type
    elif isinstance(expr, CallExpr):
        t = find_fixed_callable_return(expr.callee)
        if t:
            ret_type = get_proper_type(t.ret_type)
            if isinstance(ret_type, CallableType):
                return ret_type
    return None
</t>
<t tx="ekr.20230831011820.1425">@path mypy
&lt;&lt; semanal_main.py: docstring &gt;&gt;
&lt;&lt; semanal_main.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1427">from __future__ import annotations

from contextlib import nullcontext
from typing import TYPE_CHECKING, Callable, Final, List, Optional, Tuple, Union
from typing_extensions import TypeAlias as _TypeAlias

import mypy.build
import mypy.state
from mypy.checker import FineGrainedDeferredNode
from mypy.errors import Errors
from mypy.nodes import Decorator, FuncDef, MypyFile, OverloadedFuncDef, TypeInfo, Var
from mypy.options import Options
from mypy.plugin import ClassDefContext
from mypy.plugins import dataclasses as dataclasses_plugin
from mypy.semanal import (
    SemanticAnalyzer,
    apply_semantic_analyzer_patches,
    remove_imported_names_from_symtable,
)
from mypy.semanal_classprop import (
    add_type_promotion,
    calculate_class_abstract_status,
    calculate_class_vars,
    check_protocol_status,
)
from mypy.semanal_infer import infer_decorator_signature_if_simple
from mypy.semanal_shared import find_dataclass_transform_spec
from mypy.semanal_typeargs import TypeArgumentAnalyzer
from mypy.server.aststrip import SavedAttributes
from mypy.util import is_typeshed_file

if TYPE_CHECKING:
    from mypy.build import Graph, State


Patches: _TypeAlias = List[Tuple[int, Callable[[], None]]]


# If we perform this many iterations, raise an exception since we are likely stuck.
MAX_ITERATIONS: Final = 20


# Number of passes over core modules before going on to the rest of the builtin SCC.
CORE_WARMUP: Final = 2
core_modules: Final = [
    "typing",
    "_collections_abc",
    "builtins",
    "abc",
    "collections",
    "collections.abc",
]


</t>
<t tx="ekr.20230831011820.1428">def semantic_analysis_for_scc(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    """Perform semantic analysis for all modules in a SCC (import cycle).

    Assume that reachability analysis has already been performed.

    The scc will be processed roughly in the order the modules are included
    in the list.
    """
    patches: Patches = []
    # Note that functions can't define new module-level attributes
    # using 'global x', since module top levels are fully processed
    # before functions. This limitation is unlikely to go away soon.
    process_top_levels(graph, scc, patches)
    process_functions(graph, scc, patches)
    # We use patch callbacks to fix up things when we expect relatively few
    # callbacks to be required.
    apply_semantic_analyzer_patches(patches)
    # Run class decorator hooks (they requite complete MROs and no placeholders).
    apply_class_plugin_hooks(graph, scc, errors)
    # This pass might need fallbacks calculated above and the results of hooks.
    check_type_arguments(graph, scc, errors)
    calculate_class_properties(graph, scc, errors)
    check_blockers(graph, scc)
    # Clean-up builtins, so that TypeVar etc. are not accessible without importing.
    if "builtins" in scc:
        cleanup_builtin_scc(graph["builtins"])


</t>
<t tx="ekr.20230831011820.1429">def cleanup_builtin_scc(state: State) -&gt; None:
    """Remove imported names from builtins namespace.

    This way names imported from typing in builtins.pyi aren't available
    by default (without importing them). We can only do this after processing
    the whole SCC is finished, when the imported names aren't needed for
    processing builtins.pyi itself.
    """
    assert state.tree is not None
    remove_imported_names_from_symtable(state.tree.names, "builtins")


</t>
<t tx="ekr.20230831011820.143">    def final_cant_override_writable(self, name: str, ctx: Context) -&gt; None:
        self.fail(f'Cannot override writable attribute "{name}" with a final one', ctx)

</t>
<t tx="ekr.20230831011820.1430">def semantic_analysis_for_targets(
    state: State, nodes: list[FineGrainedDeferredNode], graph: Graph, saved_attrs: SavedAttributes
) -&gt; None:
    """Semantically analyze only selected nodes in a given module.

    This essentially mirrors the logic of semantic_analysis_for_scc()
    except that we process only some targets. This is used in fine grained
    incremental mode, when propagating an update.

    The saved_attrs are implicitly declared instance attributes (attributes
    defined on self) removed by AST stripper that may need to be reintroduced
    here.  They must be added before any methods are analyzed.
    """
    patches: Patches = []
    if any(isinstance(n.node, MypyFile) for n in nodes):
        # Process module top level first (if needed).
        process_top_levels(graph, [state.id], patches)
    restore_saved_attrs(saved_attrs)
    analyzer = state.manager.semantic_analyzer
    for n in nodes:
        if isinstance(n.node, MypyFile):
            # Already done above.
            continue
        process_top_level_function(
            analyzer, state, state.id, n.node.fullname, n.node, n.active_typeinfo, patches
        )
    apply_semantic_analyzer_patches(patches)
    apply_class_plugin_hooks(graph, [state.id], state.manager.errors)
    check_type_arguments_in_targets(nodes, state, state.manager.errors)
    calculate_class_properties(graph, [state.id], state.manager.errors)


</t>
<t tx="ekr.20230831011820.1431">def restore_saved_attrs(saved_attrs: SavedAttributes) -&gt; None:
    """Restore instance variables removed during AST strip that haven't been added yet."""
    for (cdef, name), sym in saved_attrs.items():
        info = cdef.info
        existing = info.get(name)
        defined_in_this_class = name in info.names
        assert isinstance(sym.node, Var)
        # This needs to mimic the logic in SemanticAnalyzer.analyze_member_lvalue()
        # regarding the existing variable in class body or in a superclass:
        # If the attribute of self is not defined in superclasses, create a new Var.
        if (
            existing is None
            or
            # (An abstract Var is considered as not defined.)
            (isinstance(existing.node, Var) and existing.node.is_abstract_var)
            or
            # Also an explicit declaration on self creates a new Var unless
            # there is already one defined in the class body.
            sym.node.explicit_self_type
            and not defined_in_this_class
        ):
            info.names[name] = sym


</t>
<t tx="ekr.20230831011820.1432">def process_top_levels(graph: Graph, scc: list[str], patches: Patches) -&gt; None:
    # Process top levels until everything has been bound.

    # Reverse order of the scc so the first modules in the original list will be
    # be processed first. This helps with performance.
    scc = list(reversed(scc))

    # Initialize ASTs and symbol tables.
    for id in scc:
        state = graph[id]
        assert state.tree is not None
        state.manager.semantic_analyzer.prepare_file(state.tree)

    # Initially all namespaces in the SCC are incomplete (well they are empty).
    state.manager.incomplete_namespaces.update(scc)

    worklist = scc.copy()
    # HACK: process core stuff first. This is mostly needed to support defining
    # named tuples in builtin SCC.
    if all(m in worklist for m in core_modules):
        worklist += list(reversed(core_modules)) * CORE_WARMUP
    final_iteration = False
    iteration = 0
    analyzer = state.manager.semantic_analyzer
    analyzer.deferral_debug_context.clear()

    while worklist:
        iteration += 1
        if iteration &gt; MAX_ITERATIONS:
            # Just pick some module inside the current SCC for error context.
            assert state.tree is not None
            with analyzer.file_context(state.tree, state.options):
                analyzer.report_hang()
            break
        if final_iteration:
            # Give up. It's impossible to bind all names.
            state.manager.incomplete_namespaces.clear()
        all_deferred: list[str] = []
        any_progress = False
        while worklist:
            next_id = worklist.pop()
            state = graph[next_id]
            assert state.tree is not None
            deferred, incomplete, progress = semantic_analyze_target(
                next_id, next_id, state, state.tree, None, final_iteration, patches
            )
            all_deferred += deferred
            any_progress = any_progress or progress
            if not incomplete:
                state.manager.incomplete_namespaces.discard(next_id)
        if final_iteration:
            assert not all_deferred, "Must not defer during final iteration"
        # Reverse to process the targets in the same order on every iteration. This avoids
        # processing the same target twice in a row, which is inefficient.
        worklist = list(reversed(all_deferred))
        final_iteration = not any_progress


</t>
<t tx="ekr.20230831011820.1433">def process_functions(graph: Graph, scc: list[str], patches: Patches) -&gt; None:
    # Process functions.
    for module in scc:
        tree = graph[module].tree
        assert tree is not None
        analyzer = graph[module].manager.semantic_analyzer
        # In principle, functions can be processed in arbitrary order,
        # but _methods_ must be processed in the order they are defined,
        # because some features (most notably partial types) depend on
        # order of definitions on self.
        #
        # There can be multiple generated methods per line. Use target
        # name as the second sort key to get a repeatable sort order on
        # Python 3.5, which doesn't preserve dictionary order.
        targets = sorted(get_all_leaf_targets(tree), key=lambda x: (x[1].line, x[0]))
        for target, node, active_type in targets:
            assert isinstance(node, (FuncDef, OverloadedFuncDef, Decorator))
            process_top_level_function(
                analyzer, graph[module], module, target, node, active_type, patches
            )


</t>
<t tx="ekr.20230831011820.1434">def process_top_level_function(
    analyzer: SemanticAnalyzer,
    state: State,
    module: str,
    target: str,
    node: FuncDef | OverloadedFuncDef | Decorator,
    active_type: TypeInfo | None,
    patches: Patches,
) -&gt; None:
    """Analyze single top-level function or method.

    Process the body of the function (including nested functions) again and again,
    until all names have been resolved (or iteration limit reached).
    """
    # We need one more iteration after incomplete is False (e.g. to report errors, if any).
    final_iteration = False
    incomplete = True
    # Start in the incomplete state (no missing names will be reported on first pass).
    # Note that we use module name, since functions don't create qualified names.
    deferred = [module]
    analyzer.deferral_debug_context.clear()
    analyzer.incomplete_namespaces.add(module)
    iteration = 0
    while deferred:
        iteration += 1
        if iteration == MAX_ITERATIONS:
            # Just pick some module inside the current SCC for error context.
            assert state.tree is not None
            with analyzer.file_context(state.tree, state.options):
                analyzer.report_hang()
            break
        if not (deferred or incomplete) or final_iteration:
            # OK, this is one last pass, now missing names will be reported.
            analyzer.incomplete_namespaces.discard(module)
        deferred, incomplete, progress = semantic_analyze_target(
            target, module, state, node, active_type, final_iteration, patches
        )
        if final_iteration:
            assert not deferred, "Must not defer during final iteration"
        if not progress:
            final_iteration = True

    analyzer.incomplete_namespaces.discard(module)
    # After semantic analysis is done, discard local namespaces
    # to avoid memory hoarding.
    analyzer.saved_locals.clear()


</t>
<t tx="ekr.20230831011820.1435">TargetInfo: _TypeAlias = Tuple[
    str, Union[MypyFile, FuncDef, OverloadedFuncDef, Decorator], Optional[TypeInfo]
]


def get_all_leaf_targets(file: MypyFile) -&gt; list[TargetInfo]:
    """Return all leaf targets in a symbol table (module-level and methods)."""
    result: list[TargetInfo] = []
    for fullname, node, active_type in file.local_definitions():
        if isinstance(node.node, (FuncDef, OverloadedFuncDef, Decorator)):
            result.append((fullname, node.node, active_type))
    return result


</t>
<t tx="ekr.20230831011820.1436">def semantic_analyze_target(
    target: str,
    module: str,
    state: State,
    node: MypyFile | FuncDef | OverloadedFuncDef | Decorator,
    active_type: TypeInfo | None,
    final_iteration: bool,
    patches: Patches,
) -&gt; tuple[list[str], bool, bool]:
    """Semantically analyze a single target.

    Return tuple with these items:
    - list of deferred targets
    - was some definition incomplete (need to run another pass)
    - were any new names defined (or placeholders replaced)
    """
    state.manager.processed_targets.append((module, target))
    tree = state.tree
    assert tree is not None
    analyzer = state.manager.semantic_analyzer
    # TODO: Move initialization to somewhere else
    analyzer.global_decls = [set()]
    analyzer.nonlocal_decls = [set()]
    analyzer.globals = tree.names
    analyzer.progress = False
    with state.wrap_context(check_blockers=False):
        refresh_node = node
        if isinstance(refresh_node, Decorator):
            # Decorator expressions will be processed as part of the module top level.
            refresh_node = refresh_node.func
        analyzer.refresh_partial(
            refresh_node,
            patches,
            final_iteration,
            file_node=tree,
            options=state.options,
            active_type=active_type,
        )
        if isinstance(node, Decorator):
            infer_decorator_signature_if_simple(node, analyzer)
    for dep in analyzer.imports:
        state.add_dependency(dep)
        priority = mypy.build.PRI_LOW
        if priority &lt;= state.priorities.get(dep, priority):
            state.priorities[dep] = priority

    # Clear out some stale data to avoid memory leaks and astmerge
    # validity check confusion
    analyzer.statement = None
    del analyzer.cur_mod_node

    if analyzer.deferred:
        return [target], analyzer.incomplete, analyzer.progress
    else:
        return [], analyzer.incomplete, analyzer.progress


</t>
<t tx="ekr.20230831011820.1437">def check_type_arguments(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    for module in scc:
        state = graph[module]
        assert state.tree
        analyzer = TypeArgumentAnalyzer(
            errors,
            state.options,
            is_typeshed_file(state.options.abs_custom_typeshed_dir, state.path or ""),
        )
        with state.wrap_context():
            with mypy.state.state.strict_optional_set(state.options.strict_optional):
                state.tree.accept(analyzer)


</t>
<t tx="ekr.20230831011820.1438">def check_type_arguments_in_targets(
    targets: list[FineGrainedDeferredNode], state: State, errors: Errors
) -&gt; None:
    """Check type arguments against type variable bounds and restrictions.

    This mirrors the logic in check_type_arguments() except that we process only
    some targets. This is used in fine grained incremental mode.
    """
    analyzer = TypeArgumentAnalyzer(
        errors,
        state.options,
        is_typeshed_file(state.options.abs_custom_typeshed_dir, state.path or ""),
    )
    with state.wrap_context():
        with mypy.state.state.strict_optional_set(state.options.strict_optional):
            for target in targets:
                func: FuncDef | OverloadedFuncDef | None = None
                if isinstance(target.node, (FuncDef, OverloadedFuncDef)):
                    func = target.node
                saved = (state.id, target.active_typeinfo, func)  # module, class, function
                with errors.scope.saved_scope(saved) if errors.scope else nullcontext():
                    analyzer.recurse_into_functions = func is not None
                    target.node.accept(analyzer)


</t>
<t tx="ekr.20230831011820.1439">def apply_class_plugin_hooks(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    """Apply class plugin hooks within a SCC.

    We run these after to the main semantic analysis so that the hooks
    don't need to deal with incomplete definitions such as placeholder
    types.

    Note that some hooks incorrectly run during the main semantic
    analysis pass, for historical reasons.
    """
    num_passes = 0
    incomplete = True
    # If we encounter a base class that has not been processed, we'll run another
    # pass. This should eventually reach a fixed point.
    while incomplete:
        assert num_passes &lt; 10, "Internal error: too many class plugin hook passes"
        num_passes += 1
        incomplete = False
        for module in scc:
            state = graph[module]
            tree = state.tree
            assert tree
            for _, node, _ in tree.local_definitions():
                if isinstance(node.node, TypeInfo):
                    if not apply_hooks_to_class(
                        state.manager.semantic_analyzer,
                        module,
                        node.node,
                        state.options,
                        tree,
                        errors,
                    ):
                        incomplete = True


</t>
<t tx="ekr.20230831011820.144">    def cant_override_final(self, name: str, base_name: str, ctx: Context) -&gt; None:
        self.fail(
            'Cannot override final attribute "{}"'
            ' (previously declared in base class "{}")'.format(name, base_name),
            ctx,
        )

</t>
<t tx="ekr.20230831011820.1440">def apply_hooks_to_class(
    self: SemanticAnalyzer,
    module: str,
    info: TypeInfo,
    options: Options,
    file_node: MypyFile,
    errors: Errors,
) -&gt; bool:
    # TODO: Move more class-related hooks here?
    defn = info.defn
    ok = True
    for decorator in defn.decorators:
        with self.file_context(file_node, options, info):
            hook = None

            decorator_name = self.get_fullname_for_hook(decorator)
            if decorator_name:
                hook = self.plugin.get_class_decorator_hook_2(decorator_name)
            # Special case: if the decorator is itself decorated with
            # typing.dataclass_transform, apply the hook for the dataclasses plugin
            # TODO: remove special casing here
            if hook is None and find_dataclass_transform_spec(decorator):
                hook = dataclasses_plugin.dataclass_class_maker_callback

            if hook:
                ok = ok and hook(ClassDefContext(defn, decorator, self))

    # Check if the class definition itself triggers a dataclass transform (via a parent class/
    # metaclass)
    spec = find_dataclass_transform_spec(info)
    if spec is not None:
        with self.file_context(file_node, options, info):
            # We can't use the normal hook because reason = defn, and ClassDefContext only accepts
            # an Expression for reason
            ok = ok and dataclasses_plugin.DataclassTransformer(defn, defn, spec, self).transform()

    return ok


</t>
<t tx="ekr.20230831011820.1441">def calculate_class_properties(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    builtins = graph["builtins"].tree
    assert builtins
    for module in scc:
        state = graph[module]
        tree = state.tree
        assert tree
        for _, node, _ in tree.local_definitions():
            if isinstance(node.node, TypeInfo):
                with state.manager.semantic_analyzer.file_context(tree, state.options, node.node):
                    calculate_class_abstract_status(node.node, tree.is_stub, errors)
                    check_protocol_status(node.node, errors)
                    calculate_class_vars(node.node)
                    add_type_promotion(
                        node.node, tree.names, graph[module].options, builtins.names
                    )


</t>
<t tx="ekr.20230831011820.1442">def check_blockers(graph: Graph, scc: list[str]) -&gt; None:
    for module in scc:
        graph[module].check_blockers()
</t>
<t tx="ekr.20230831011820.1443">@path mypy
"""Semantic analysis of named tuple definitions.

This is conceptually part of mypy.semanal.
"""

&lt;&lt; semanal_namedtuple.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1445">from __future__ import annotations

from contextlib import contextmanager
from typing import Final, Iterator, List, Mapping, cast

from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.nodes import (
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    MDEF,
    Argument,
    AssignmentStmt,
    Block,
    CallExpr,
    ClassDef,
    Context,
    Decorator,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    FuncBase,
    FuncDef,
    ListExpr,
    NamedTupleExpr,
    NameExpr,
    PassStmt,
    RefExpr,
    Statement,
    StrExpr,
    SymbolTable,
    SymbolTableNode,
    TempNode,
    TupleExpr,
    TypeInfo,
    TypeVarExpr,
    Var,
    is_StrExpr_list,
)
from mypy.options import Options
from mypy.semanal_shared import (
    PRIORITY_FALLBACKS,
    SemanticAnalyzerInterface,
    calculate_tuple_fallback,
    has_placeholder,
    set_callable_name,
)
from mypy.types import (
    TYPED_NAMEDTUPLE_NAMES,
    AnyType,
    CallableType,
    LiteralType,
    TupleType,
    Type,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarType,
    UnboundType,
    has_type_vars,
)
from mypy.util import get_unique_redefinition_name

# Matches "_prohibited" in typing.py, but adds __annotations__, which works at runtime but can't
# easily be supported in a static checker.
NAMEDTUPLE_PROHIBITED_NAMES: Final = (
    "__new__",
    "__init__",
    "__slots__",
    "__getnewargs__",
    "_fields",
    "_field_defaults",
    "_field_types",
    "_make",
    "_replace",
    "_asdict",
    "_source",
    "__annotations__",
)

NAMEDTUP_CLASS_ERROR: Final = (
    "Invalid statement in NamedTuple definition; " 'expected "field_name: field_type [= default]"'
)

SELF_TVAR_NAME: Final = "_NT"


</t>
<t tx="ekr.20230831011820.1446">class NamedTupleAnalyzer:
    @others
</t>
<t tx="ekr.20230831011820.1447">def __init__(self, options: Options, api: SemanticAnalyzerInterface) -&gt; None:
    self.options = options
    self.api = api

</t>
<t tx="ekr.20230831011820.1448">def analyze_namedtuple_classdef(
    self, defn: ClassDef, is_stub_file: bool, is_func_scope: bool
) -&gt; tuple[bool, TypeInfo | None]:
    """Analyze if given class definition can be a named tuple definition.

    Return a tuple where first item indicates whether this can possibly be a named tuple,
    and the second item is the corresponding TypeInfo (may be None if not ready and should be
    deferred).
    """
    for base_expr in defn.base_type_exprs:
        if isinstance(base_expr, RefExpr):
            self.api.accept(base_expr)
            if base_expr.fullname in TYPED_NAMEDTUPLE_NAMES:
                result = self.check_namedtuple_classdef(defn, is_stub_file)
                if result is None:
                    # This is a valid named tuple, but some types are incomplete.
                    return True, None
                items, types, default_items, statements = result
                if is_func_scope and "@" not in defn.name:
                    defn.name += "@" + str(defn.line)
                existing_info = None
                if isinstance(defn.analyzed, NamedTupleExpr):
                    existing_info = defn.analyzed.info
                info = self.build_namedtuple_typeinfo(
                    defn.name, items, types, default_items, defn.line, existing_info
                )
                defn.analyzed = NamedTupleExpr(info, is_typed=True)
                defn.analyzed.line = defn.line
                defn.analyzed.column = defn.column
                defn.defs.body = statements
                # All done: this is a valid named tuple with all types known.
                return True, info
    # This can't be a valid named tuple.
    return False, None

</t>
<t tx="ekr.20230831011820.1449">def check_namedtuple_classdef(
    self, defn: ClassDef, is_stub_file: bool
) -&gt; tuple[list[str], list[Type], dict[str, Expression], list[Statement]] | None:
    """Parse and validate fields in named tuple class definition.

    Return a four tuple:
      * field names
      * field types
      * field default values
      * valid statements
    or None, if any of the types are not ready.
    """
    if len(defn.base_type_exprs) &gt; 1:
        self.fail("NamedTuple should be a single base", defn)
    items: list[str] = []
    types: list[Type] = []
    default_items: dict[str, Expression] = {}
    statements: list[Statement] = []
    for stmt in defn.defs.body:
        statements.append(stmt)
        if not isinstance(stmt, AssignmentStmt):
            # Still allow pass or ... (for empty namedtuples).
            if isinstance(stmt, PassStmt) or (
                isinstance(stmt, ExpressionStmt) and isinstance(stmt.expr, EllipsisExpr)
            ):
                continue
            # Also allow methods, including decorated ones.
            if isinstance(stmt, (Decorator, FuncBase)):
                continue
            # And docstrings.
            if isinstance(stmt, ExpressionStmt) and isinstance(stmt.expr, StrExpr):
                continue
            statements.pop()
            defn.removed_statements.append(stmt)
            self.fail(NAMEDTUP_CLASS_ERROR, stmt)
        elif len(stmt.lvalues) &gt; 1 or not isinstance(stmt.lvalues[0], NameExpr):
            # An assignment, but an invalid one.
            statements.pop()
            defn.removed_statements.append(stmt)
            self.fail(NAMEDTUP_CLASS_ERROR, stmt)
        else:
            # Append name and type in this case...
            name = stmt.lvalues[0].name
            items.append(name)
            if stmt.type is None:
                types.append(AnyType(TypeOfAny.unannotated))
            else:
                # We never allow recursive types at function scope. Although it is
                # possible to support this for named tuples, it is still tricky, and
                # it would be inconsistent with type aliases.
                analyzed = self.api.anal_type(
                    stmt.type,
                    allow_placeholder=not self.options.disable_recursive_aliases
                    and not self.api.is_func_scope(),
                    prohibit_self_type="NamedTuple item type",
                )
                if analyzed is None:
                    # Something is incomplete. We need to defer this named tuple.
                    return None
                types.append(analyzed)
            # ...despite possible minor failures that allow further analyzis.
            if name.startswith("_"):
                self.fail(
                    f"NamedTuple field name cannot start with an underscore: {name}", stmt
                )
            if stmt.type is None or hasattr(stmt, "new_syntax") and not stmt.new_syntax:
                self.fail(NAMEDTUP_CLASS_ERROR, stmt)
            elif isinstance(stmt.rvalue, TempNode):
                # x: int assigns rvalue to TempNode(AnyType())
                if default_items:
                    self.fail(
                        "Non-default NamedTuple fields cannot follow default fields", stmt
                    )
            else:
                default_items[name] = stmt.rvalue
    return items, types, default_items, statements

</t>
<t tx="ekr.20230831011820.145">    def cant_assign_to_final(self, name: str, attr_assign: bool, ctx: Context) -&gt; None:
        """Warn about a prohibited assignment to a final attribute.

        Pass `attr_assign=True` if the assignment assigns to an attribute.
        """
        kind = "attribute" if attr_assign else "name"
        self.fail(f'Cannot assign to final {kind} "{unmangle(name)}"', ctx)

</t>
<t tx="ekr.20230831011820.1450">def check_namedtuple(
    self, node: Expression, var_name: str | None, is_func_scope: bool
) -&gt; tuple[str | None, TypeInfo | None, list[TypeVarLikeType]]:
    """Check if a call defines a namedtuple.

    The optional var_name argument is the name of the variable to
    which this is assigned, if any.

    Return a tuple of two items:
      * Internal name of the named tuple (e.g. the name passed as an argument to namedtuple)
        or None if it is not a valid named tuple
      * Corresponding TypeInfo, or None if not ready.

    If the definition is invalid but looks like a namedtuple,
    report errors but return (some) TypeInfo.
    """
    if not isinstance(node, CallExpr):
        return None, None, []
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None, None, []
    fullname = callee.fullname
    if fullname == "collections.namedtuple":
        is_typed = False
    elif fullname in TYPED_NAMEDTUPLE_NAMES:
        is_typed = True
    else:
        return None, None, []
    result = self.parse_namedtuple_args(call, fullname)
    if result:
        items, types, defaults, typename, tvar_defs, ok = result
    else:
        # Error. Construct dummy return value.
        if var_name:
            name = var_name
            if is_func_scope:
                name += "@" + str(call.line)
        else:
            name = var_name = "namedtuple@" + str(call.line)
        info = self.build_namedtuple_typeinfo(name, [], [], {}, node.line, None)
        self.store_namedtuple_info(info, var_name, call, is_typed)
        if name != var_name or is_func_scope:
            # NOTE: we skip local namespaces since they are not serialized.
            self.api.add_symbol_skip_local(name, info)
        return var_name, info, []
    if not ok:
        # This is a valid named tuple but some types are not ready.
        return typename, None, []

    # We use the variable name as the class name if it exists. If
    # it doesn't, we use the name passed as an argument. We prefer
    # the variable name because it should be unique inside a
    # module, and so we don't need to disambiguate it with a line
    # number.
    if var_name:
        name = var_name
    else:
        name = typename

    if var_name is None or is_func_scope:
        # There are two special cases where need to give it a unique name derived
        # from the line number:
        #   * This is a base class expression, since it often matches the class name:
        #         class NT(NamedTuple('NT', [...])):
        #             ...
        #   * This is a local (function or method level) named tuple, since
        #     two methods of a class can define a named tuple with the same name,
        #     and they will be stored in the same namespace (see below).
        name += "@" + str(call.line)
    if defaults:
        default_items = {
            arg_name: default for arg_name, default in zip(items[-len(defaults) :], defaults)
        }
    else:
        default_items = {}

    existing_info = None
    if isinstance(node.analyzed, NamedTupleExpr):
        existing_info = node.analyzed.info
    info = self.build_namedtuple_typeinfo(
        name, items, types, default_items, node.line, existing_info
    )

    # If var_name is not None (i.e. this is not a base class expression), we always
    # store the generated TypeInfo under var_name in the current scope, so that
    # other definitions can use it.
    if var_name:
        self.store_namedtuple_info(info, var_name, call, is_typed)
    else:
        call.analyzed = NamedTupleExpr(info, is_typed=is_typed)
        call.analyzed.set_line(call)
    # There are three cases where we need to store the generated TypeInfo
    # second time (for the purpose of serialization):
    #   * If there is a name mismatch like One = NamedTuple('Other', [...])
    #     we also store the info under name 'Other@lineno', this is needed
    #     because classes are (de)serialized using their actual fullname, not
    #     the name of l.h.s.
    #   * If this is a method level named tuple. It can leak from the method
    #     via assignment to self attribute and therefore needs to be serialized
    #     (local namespaces are not serialized).
    #   * If it is a base class expression. It was not stored above, since
    #     there is no var_name (but it still needs to be serialized
    #     since it is in MRO of some class).
    if name != var_name or is_func_scope:
        # NOTE: we skip local namespaces since they are not serialized.
        self.api.add_symbol_skip_local(name, info)
    return typename, info, tvar_defs

</t>
<t tx="ekr.20230831011820.1451">def store_namedtuple_info(
    self, info: TypeInfo, name: str, call: CallExpr, is_typed: bool
) -&gt; None:
    self.api.add_symbol(name, info, call)
    call.analyzed = NamedTupleExpr(info, is_typed=is_typed)
    call.analyzed.set_line(call)

</t>
<t tx="ekr.20230831011820.1452">def parse_namedtuple_args(
    self, call: CallExpr, fullname: str
) -&gt; None | (tuple[list[str], list[Type], list[Expression], str, list[TypeVarLikeType], bool]):
    """Parse a namedtuple() call into data needed to construct a type.

    Returns a 6-tuple:
    - List of argument names
    - List of argument types
    - List of default values
    - First argument of namedtuple
    - All typevars found in the field definition
    - Whether all types are ready.

    Return None if the definition didn't typecheck.
    """
    type_name = "NamedTuple" if fullname in TYPED_NAMEDTUPLE_NAMES else "namedtuple"
    # TODO: Share code with check_argument_count in checkexpr.py?
    args = call.args
    if len(args) &lt; 2:
        self.fail(f'Too few arguments for "{type_name}()"', call)
        return None
    defaults: list[Expression] = []
    if len(args) &gt; 2:
        # Typed namedtuple doesn't support additional arguments.
        if fullname in TYPED_NAMEDTUPLE_NAMES:
            self.fail('Too many arguments for "NamedTuple()"', call)
            return None
        for i, arg_name in enumerate(call.arg_names[2:], 2):
            if arg_name == "defaults":
                arg = args[i]
                # We don't care what the values are, as long as the argument is an iterable
                # and we can count how many defaults there are.
                if isinstance(arg, (ListExpr, TupleExpr)):
                    defaults = list(arg.items)
                else:
                    self.fail(
                        "List or tuple literal expected as the defaults argument to "
                        "{}()".format(type_name),
                        arg,
                    )
                break
    if call.arg_kinds[:2] != [ARG_POS, ARG_POS]:
        self.fail(f'Unexpected arguments to "{type_name}()"', call)
        return None
    if not isinstance(args[0], StrExpr):
        self.fail(f'"{type_name}()" expects a string literal as the first argument', call)
        return None
    typename = args[0].value
    types: list[Type] = []
    tvar_defs = []
    if not isinstance(args[1], (ListExpr, TupleExpr)):
        if fullname == "collections.namedtuple" and isinstance(args[1], StrExpr):
            str_expr = args[1]
            items = str_expr.value.replace(",", " ").split()
        else:
            self.fail(
                'List or tuple literal expected as the second argument to "{}()"'.format(
                    type_name
                ),
                call,
            )
            return None
    else:
        listexpr = args[1]
        if fullname == "collections.namedtuple":
            # The fields argument contains just names, with implicit Any types.
            if not is_StrExpr_list(listexpr.items):
                self.fail('String literal expected as "namedtuple()" item', call)
                return None
            items = [item.value for item in listexpr.items]
        else:
            type_exprs = [
                t.items[1]
                for t in listexpr.items
                if isinstance(t, TupleExpr) and len(t.items) == 2
            ]
            tvar_defs = self.api.get_and_bind_all_tvars(type_exprs)
            # The fields argument contains (name, type) tuples.
            result = self.parse_namedtuple_fields_with_types(listexpr.items, call)
            if result is None:
                # One of the types is not ready, defer.
                return None
            items, types, _, ok = result
            if not ok:
                return [], [], [], typename, [], False
    if not types:
        types = [AnyType(TypeOfAny.unannotated) for _ in items]
    underscore = [item for item in items if item.startswith("_")]
    if underscore:
        self.fail(
            f'"{type_name}()" field names cannot start with an underscore: '
            + ", ".join(underscore),
            call,
        )
    if len(defaults) &gt; len(items):
        self.fail(f'Too many defaults given in call to "{type_name}()"', call)
        defaults = defaults[: len(items)]
    return items, types, defaults, typename, tvar_defs, True

</t>
<t tx="ekr.20230831011820.1453">def parse_namedtuple_fields_with_types(
    self, nodes: list[Expression], context: Context
) -&gt; tuple[list[str], list[Type], list[Expression], bool] | None:
    """Parse typed named tuple fields.

    Return (names, types, defaults, whether types are all ready), or None if error occurred.
    """
    items: list[str] = []
    types: list[Type] = []
    for item in nodes:
        if isinstance(item, TupleExpr):
            if len(item.items) != 2:
                self.fail('Invalid "NamedTuple()" field definition', item)
                return None
            name, type_node = item.items
            if isinstance(name, StrExpr):
                items.append(name.value)
            else:
                self.fail('Invalid "NamedTuple()" field name', item)
                return None
            try:
                type = expr_to_unanalyzed_type(type_node, self.options, self.api.is_stub_file)
            except TypeTranslationError:
                self.fail("Invalid field type", type_node)
                return None
            # We never allow recursive types at function scope.
            analyzed = self.api.anal_type(
                type,
                allow_placeholder=not self.options.disable_recursive_aliases
                and not self.api.is_func_scope(),
                prohibit_self_type="NamedTuple item type",
            )
            # Workaround #4987 and avoid introducing a bogus UnboundType
            if isinstance(analyzed, UnboundType):
                analyzed = AnyType(TypeOfAny.from_error)
            # These should be all known, otherwise we would defer in visit_assignment_stmt().
            if analyzed is None:
                return [], [], [], False
            types.append(analyzed)
        else:
            self.fail('Tuple expected as "NamedTuple()" field', item)
            return None
    return items, types, [], True

</t>
<t tx="ekr.20230831011820.1454">def build_namedtuple_typeinfo(
    self,
    name: str,
    items: list[str],
    types: list[Type],
    default_items: Mapping[str, Expression],
    line: int,
    existing_info: TypeInfo | None,
) -&gt; TypeInfo:
    strtype = self.api.named_type("builtins.str")
    implicit_any = AnyType(TypeOfAny.special_form)
    basetuple_type = self.api.named_type("builtins.tuple", [implicit_any])
    dictype = self.api.named_type("builtins.dict", [strtype, implicit_any])
    # Actual signature should return OrderedDict[str, Union[types]]
    ordereddictype = self.api.named_type("builtins.dict", [strtype, implicit_any])
    fallback = self.api.named_type("builtins.tuple", [implicit_any])
    # Note: actual signature should accept an invariant version of Iterable[UnionType[types]].
    # but it can't be expressed. 'new' and 'len' should be callable types.
    iterable_type = self.api.named_type_or_none("typing.Iterable", [implicit_any])
    function_type = self.api.named_type("builtins.function")

    literals: list[Type] = [LiteralType(item, strtype) for item in items]
    match_args_type = TupleType(literals, basetuple_type)

    info = existing_info or self.api.basic_new_typeinfo(name, fallback, line)
    info.is_named_tuple = True
    tuple_base = TupleType(types, fallback)
    if info.special_alias and has_placeholder(info.special_alias.target):
        self.api.process_placeholder(
            None, "NamedTuple item", info, force_progress=tuple_base != info.tuple_type
        )
    info.update_tuple_type(tuple_base)
    info.line = line
    # For use by mypyc.
    info.metadata["namedtuple"] = {"fields": items.copy()}

    # We can't calculate the complete fallback type until after semantic
    # analysis, since otherwise base classes might be incomplete. Postpone a
    # callback function that patches the fallback.
    if not has_placeholder(tuple_base) and not has_type_vars(tuple_base):
        self.api.schedule_patch(
            PRIORITY_FALLBACKS, lambda: calculate_tuple_fallback(tuple_base)
        )

    def add_field(
        var: Var, is_initialized_in_class: bool = False, is_property: bool = False
    ) -&gt; None:
        var.info = info
        var.is_initialized_in_class = is_initialized_in_class
        var.is_property = is_property
        var._fullname = f"{info.fullname}.{var.name}"
        info.names[var.name] = SymbolTableNode(MDEF, var)

    fields = [Var(item, typ) for item, typ in zip(items, types)]
    for var in fields:
        add_field(var, is_property=True)
    # We can't share Vars between fields and method arguments, since they
    # have different full names (the latter are normally used as local variables
    # in functions, so their full names are set to short names when generated methods
    # are analyzed).
    vars = [Var(item, typ) for item, typ in zip(items, types)]

    tuple_of_strings = TupleType([strtype for _ in items], basetuple_type)
    add_field(Var("_fields", tuple_of_strings), is_initialized_in_class=True)
    add_field(Var("_field_types", dictype), is_initialized_in_class=True)
    add_field(Var("_field_defaults", dictype), is_initialized_in_class=True)
    add_field(Var("_source", strtype), is_initialized_in_class=True)
    add_field(Var("__annotations__", ordereddictype), is_initialized_in_class=True)
    add_field(Var("__doc__", strtype), is_initialized_in_class=True)
    if self.options.python_version &gt;= (3, 10):
        add_field(Var("__match_args__", match_args_type), is_initialized_in_class=True)

    assert info.tuple_type is not None  # Set by update_tuple_type() above.
    tvd = TypeVarType(
        name=SELF_TVAR_NAME,
        fullname=info.fullname + "." + SELF_TVAR_NAME,
        id=self.api.tvar_scope.new_unique_func_id(),
        values=[],
        upper_bound=info.tuple_type,
        default=AnyType(TypeOfAny.from_omitted_generics),
    )
    selftype = tvd

    def add_method(
        funcname: str,
        ret: Type,
        args: list[Argument],
        is_classmethod: bool = False,
        is_new: bool = False,
    ) -&gt; None:
        if is_classmethod or is_new:
            first = [Argument(Var("_cls"), TypeType.make_normalized(selftype), None, ARG_POS)]
        else:
            first = [Argument(Var("_self"), selftype, None, ARG_POS)]
        args = first + args

        types = [arg.type_annotation for arg in args]
        items = [arg.variable.name for arg in args]
        arg_kinds = [arg.kind for arg in args]
        assert None not in types
        signature = CallableType(cast(List[Type], types), arg_kinds, items, ret, function_type)
        signature.variables = [tvd]
        func = FuncDef(funcname, args, Block([]))
        func.info = info
        func.is_class = is_classmethod
        func.type = set_callable_name(signature, func)
        func._fullname = info.fullname + "." + funcname
        func.line = line
        if is_classmethod:
            v = Var(funcname, func.type)
            v.is_classmethod = True
            v.info = info
            v._fullname = func._fullname
            func.is_decorated = True
            dec = Decorator(func, [NameExpr("classmethod")], v)
            dec.line = line
            sym = SymbolTableNode(MDEF, dec)
        else:
            sym = SymbolTableNode(MDEF, func)
        sym.plugin_generated = True
        info.names[funcname] = sym

    add_method(
        "_replace",
        ret=selftype,
        args=[Argument(var, var.type, EllipsisExpr(), ARG_NAMED_OPT) for var in vars],
    )

    def make_init_arg(var: Var) -&gt; Argument:
        default = default_items.get(var.name, None)
        kind = ARG_POS if default is None else ARG_OPT
        return Argument(var, var.type, default, kind)

    add_method("__new__", ret=selftype, args=[make_init_arg(var) for var in vars], is_new=True)
    add_method("_asdict", args=[], ret=ordereddictype)
    add_method(
        "_make",
        ret=selftype,
        is_classmethod=True,
        args=[Argument(Var("iterable", iterable_type), iterable_type, None, ARG_POS)],
    )

    self_tvar_expr = TypeVarExpr(
        SELF_TVAR_NAME,
        info.fullname + "." + SELF_TVAR_NAME,
        [],
        info.tuple_type,
        AnyType(TypeOfAny.from_omitted_generics),
    )
    info.names[SELF_TVAR_NAME] = SymbolTableNode(MDEF, self_tvar_expr)
    return info

</t>
<t tx="ekr.20230831011820.1455">@contextmanager
def save_namedtuple_body(self, named_tuple_info: TypeInfo) -&gt; Iterator[None]:
    """Preserve the generated body of class-based named tuple and then restore it.

    Temporarily clear the names dict so we don't get errors about duplicate names
    that were already set in build_namedtuple_typeinfo (we already added the tuple
    field names while generating the TypeInfo, and actual duplicates are
    already reported).
    """
    nt_names = named_tuple_info.names
    named_tuple_info.names = SymbolTable()

    yield

    # Make sure we didn't use illegal names, then reset the names in the typeinfo.
    for prohibited in NAMEDTUPLE_PROHIBITED_NAMES:
        if prohibited in named_tuple_info.names:
            if nt_names.get(prohibited) is named_tuple_info.names[prohibited]:
                continue
            ctx = named_tuple_info.names[prohibited].node
            assert ctx is not None
            self.fail(f'Cannot overwrite NamedTuple attribute "{prohibited}"', ctx)

    # Restore the names in the original symbol table. This ensures that the symbol
    # table contains the field objects created by build_namedtuple_typeinfo. Exclude
    # __doc__, which can legally be overwritten by the class.
    for key, value in nt_names.items():
        if key in named_tuple_info.names:
            if key == "__doc__":
                continue
            sym = named_tuple_info.names[key]
            if isinstance(sym.node, (FuncBase, Decorator)) and not sym.plugin_generated:
                # Keep user-defined methods as is.
                continue
            # Keep existing (user-provided) definitions under mangled names, so they
            # get semantically analyzed.
            r_key = get_unique_redefinition_name(key, named_tuple_info.names)
            named_tuple_info.names[r_key] = sym
        named_tuple_info.names[key] = value

</t>
<t tx="ekr.20230831011820.1456"># Helpers

def fail(self, msg: str, ctx: Context) -&gt; None:
    self.api.fail(msg, ctx)
</t>
<t tx="ekr.20230831011820.1457">@path mypy
"""Semantic analysis of NewType definitions.

This is conceptually part of mypy.semanal (semantic analyzer pass 2).
"""

&lt;&lt; semanal_newtype.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1459">from __future__ import annotations

from mypy import errorcodes as codes
from mypy.errorcodes import ErrorCode
from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.messages import MessageBuilder, format_type
from mypy.nodes import (
    ARG_POS,
    MDEF,
    Argument,
    AssignmentStmt,
    Block,
    CallExpr,
    Context,
    FuncDef,
    NameExpr,
    NewTypeExpr,
    PlaceholderNode,
    RefExpr,
    StrExpr,
    SymbolTableNode,
    TypeInfo,
    Var,
)
from mypy.options import Options
from mypy.semanal_shared import SemanticAnalyzerInterface, has_placeholder
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type
from mypy.types import (
    AnyType,
    CallableType,
    Instance,
    NoneType,
    PlaceholderType,
    TupleType,
    Type,
    TypeOfAny,
    get_proper_type,
)


</t>
<t tx="ekr.20230831011820.146">    def protocol_members_cant_be_final(self, ctx: Context) -&gt; None:
        self.fail("Protocol member cannot be final", ctx)

</t>
<t tx="ekr.20230831011820.1460">class NewTypeAnalyzer:
    @others
</t>
<t tx="ekr.20230831011820.1461">def __init__(
    self, options: Options, api: SemanticAnalyzerInterface, msg: MessageBuilder
) -&gt; None:
    self.options = options
    self.api = api
    self.msg = msg

</t>
<t tx="ekr.20230831011820.1462">def process_newtype_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Check if s declares a NewType; if yes, store it in symbol table.

    Return True if it's a NewType declaration. The current target may be
    deferred as a side effect if the base type is not ready, even if
    the return value is True.

    The logic in this function mostly copies the logic for visit_class_def()
    with a single (non-Generic) base.
    """
    var_name, call = self.analyze_newtype_declaration(s)
    if var_name is None or call is None:
        return False
    name = var_name
    # OK, now we know this is a NewType. But the base type may be not ready yet,
    # add placeholder as we do for ClassDef.

    if self.api.is_func_scope():
        name += "@" + str(s.line)
    fullname = self.api.qualified_name(name)

    if not call.analyzed or isinstance(call.analyzed, NewTypeExpr) and not call.analyzed.info:
        # Start from labeling this as a future class, as we do for normal ClassDefs.
        placeholder = PlaceholderNode(fullname, s, s.line, becomes_typeinfo=True)
        self.api.add_symbol(var_name, placeholder, s, can_defer=False)

    old_type, should_defer = self.check_newtype_args(var_name, call, s)
    old_type = get_proper_type(old_type)
    if not isinstance(call.analyzed, NewTypeExpr):
        call.analyzed = NewTypeExpr(var_name, old_type, line=call.line, column=call.column)
    else:
        call.analyzed.old_type = old_type
    if old_type is None:
        if should_defer:
            # Base type is not ready.
            self.api.defer()
            return True

    # Create the corresponding class definition if the aliased type is subtypeable
    assert isinstance(call.analyzed, NewTypeExpr)
    if isinstance(old_type, TupleType):
        newtype_class_info = self.build_newtype_typeinfo(
            name, old_type, old_type.partial_fallback, s.line, call.analyzed.info
        )
        newtype_class_info.update_tuple_type(old_type)
    elif isinstance(old_type, Instance):
        if old_type.type.is_protocol:
            self.fail("NewType cannot be used with protocol classes", s)
        newtype_class_info = self.build_newtype_typeinfo(
            name, old_type, old_type, s.line, call.analyzed.info
        )
    else:
        if old_type is not None:
            message = "Argument 2 to NewType(...) must be subclassable (got {})"
            self.fail(
                message.format(format_type(old_type, self.options)),
                s,
                code=codes.VALID_NEWTYPE,
            )
        # Otherwise the error was already reported.
        old_type = AnyType(TypeOfAny.from_error)
        object_type = self.api.named_type("builtins.object")
        newtype_class_info = self.build_newtype_typeinfo(
            name, old_type, object_type, s.line, call.analyzed.info
        )
        newtype_class_info.fallback_to_any = True

    check_for_explicit_any(
        old_type, self.options, self.api.is_typeshed_stub_file, self.msg, context=s
    )

    if self.options.disallow_any_unimported and has_any_from_unimported_type(old_type):
        self.msg.unimported_type_becomes_any("Argument 2 to NewType(...)", old_type, s)

    # If so, add it to the symbol table.
    assert isinstance(call.analyzed, NewTypeExpr)
    # As we do for normal classes, create the TypeInfo only once, then just
    # update base classes on next iterations (to get rid of placeholders there).
    if not call.analyzed.info:
        call.analyzed.info = newtype_class_info
    else:
        call.analyzed.info.bases = newtype_class_info.bases
    self.api.add_symbol(var_name, call.analyzed.info, s)
    if self.api.is_func_scope():
        self.api.add_symbol_skip_local(name, call.analyzed.info)
    newtype_class_info.line = s.line
    return True

</t>
<t tx="ekr.20230831011820.1463">def analyze_newtype_declaration(self, s: AssignmentStmt) -&gt; tuple[str | None, CallExpr | None]:
    """Return the NewType call expression if `s` is a newtype declaration or None otherwise."""
    name, call = None, None
    if (
        len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and isinstance(s.rvalue, CallExpr)
        and isinstance(s.rvalue.callee, RefExpr)
        and s.rvalue.callee.fullname == "typing.NewType"
    ):
        name = s.lvalues[0].name

        if s.type:
            self.fail("Cannot declare the type of a NewType declaration", s)

        names = self.api.current_symbol_table()
        existing = names.get(name)
        # Give a better error message than generic "Name already defined".
        if (
            existing
            and not isinstance(existing.node, PlaceholderNode)
            and not s.rvalue.analyzed
        ):
            self.fail(f'Cannot redefine "{name}" as a NewType', s)

        # This dummy NewTypeExpr marks the call as sufficiently analyzed; it will be
        # overwritten later with a fully complete NewTypeExpr if there are no other
        # errors with the NewType() call.
        call = s.rvalue

    return name, call

</t>
<t tx="ekr.20230831011820.1464">def check_newtype_args(
    self, name: str, call: CallExpr, context: Context
) -&gt; tuple[Type | None, bool]:
    """Ananlyze base type in NewType call.

    Return a tuple (type, should defer).
    """
    has_failed = False
    args, arg_kinds = call.args, call.arg_kinds
    if len(args) != 2 or arg_kinds[0] != ARG_POS or arg_kinds[1] != ARG_POS:
        self.fail("NewType(...) expects exactly two positional arguments", context)
        return None, False

    # Check first argument
    if not isinstance(args[0], StrExpr):
        self.fail("Argument 1 to NewType(...) must be a string literal", context)
        has_failed = True
    elif args[0].value != name:
        msg = 'String argument 1 "{}" to NewType(...) does not match variable name "{}"'
        self.fail(msg.format(args[0].value, name), context)
        has_failed = True

    # Check second argument
    msg = "Argument 2 to NewType(...) must be a valid type"
    try:
        unanalyzed_type = expr_to_unanalyzed_type(args[1], self.options, self.api.is_stub_file)
    except TypeTranslationError:
        self.fail(msg, context)
        return None, False

    # We want to use our custom error message (see above), so we suppress
    # the default error message for invalid types here.
    old_type = get_proper_type(
        self.api.anal_type(
            unanalyzed_type,
            report_invalid_types=False,
            allow_placeholder=not self.options.disable_recursive_aliases
            and not self.api.is_func_scope(),
        )
    )
    should_defer = False
    if isinstance(old_type, PlaceholderType):
        old_type = None
    if old_type is None:
        should_defer = True

    # The caller of this function assumes that if we return a Type, it's always
    # a valid one. So, we translate AnyTypes created from errors into None.
    if isinstance(old_type, AnyType) and old_type.is_from_error:
        self.fail(msg, context)
        return None, False

    return None if has_failed else old_type, should_defer

</t>
<t tx="ekr.20230831011820.1465">def build_newtype_typeinfo(
    self,
    name: str,
    old_type: Type,
    base_type: Instance,
    line: int,
    existing_info: TypeInfo | None,
) -&gt; TypeInfo:
    info = existing_info or self.api.basic_new_typeinfo(name, base_type, line)
    info.bases = [base_type]  # Update in case there were nested placeholders.
    info.is_newtype = True

    # Add __init__ method
    args = [
        Argument(Var("self"), NoneType(), None, ARG_POS),
        self.make_argument("item", old_type),
    ]
    signature = CallableType(
        arg_types=[Instance(info, []), old_type],
        arg_kinds=[arg.kind for arg in args],
        arg_names=["self", "item"],
        ret_type=NoneType(),
        fallback=self.api.named_type("builtins.function"),
        name=name,
    )
    init_func = FuncDef("__init__", args, Block([]), typ=signature)
    init_func.info = info
    init_func._fullname = info.fullname + ".__init__"
    if not existing_info:
        updated = True
    else:
        previous_sym = info.names["__init__"].node
        assert isinstance(previous_sym, FuncDef)
        updated = old_type != previous_sym.arguments[1].variable.type
    info.names["__init__"] = SymbolTableNode(MDEF, init_func)

    if has_placeholder(old_type):
        self.api.process_placeholder(None, "NewType base", info, force_progress=updated)
    return info

</t>
<t tx="ekr.20230831011820.1466"># Helpers

def make_argument(self, name: str, type: Type) -&gt; Argument:
    return Argument(Var(name), type, None, ARG_POS)

</t>
<t tx="ekr.20230831011820.1467">def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.api.fail(msg, ctx, code=code)
</t>
<t tx="ekr.20230831011820.1468">@path mypy
"""Block/import reachability analysis."""
&lt;&lt; semanal_pass1.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1469">
from __future__ import annotations

from mypy.nodes import (
    AssertStmt,
    AssignmentStmt,
    Block,
    ClassDef,
    ExpressionStmt,
    ForStmt,
    FuncDef,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    MatchStmt,
    MypyFile,
    ReturnStmt,
)
from mypy.options import Options
from mypy.reachability import (
    assert_will_always_fail,
    infer_reachability_of_if_statement,
    infer_reachability_of_match_statement,
)
from mypy.traverser import TraverserVisitor


</t>
<t tx="ekr.20230831011820.147">    def final_without_value(self, ctx: Context) -&gt; None:
        self.fail("Final name must be initialized with a value", ctx)

</t>
<t tx="ekr.20230831011820.1470">class SemanticAnalyzerPreAnalysis(TraverserVisitor):
    """Analyze reachability of blocks and imports and other local things.

    This runs before semantic analysis, so names have not been bound. Imports are
    also not resolved yet, so we can only access the current module.

    This determines static reachability of blocks and imports due to version and
    platform checks, among others.

    The main entry point is 'visit_file'.

    Reachability of imports needs to be determined very early in the build since
    this affects which modules will ultimately be processed.

    Consider this example:

      import sys

      def do_stuff() -&gt; None:
          if sys.version_info &gt;= (3, 10):
              import xyz  # Only available in Python 3.10+
              xyz.whatever()
          ...

    The block containing 'import xyz' is unreachable in Python 3 mode. The import
    shouldn't be processed in Python 3 mode, even if the module happens to exist.
    """

    @others
</t>
<t tx="ekr.20230831011820.1471">def visit_file(self, file: MypyFile, fnam: str, mod_id: str, options: Options) -&gt; None:
    self.platform = options.platform
    self.cur_mod_id = mod_id
    self.cur_mod_node = file
    self.options = options
    self.is_global_scope = True
    self.skipped_lines: set[int] = set()

    for i, defn in enumerate(file.defs):
        defn.accept(self)
        if isinstance(defn, AssertStmt) and assert_will_always_fail(defn, options):
            # We've encountered an assert that's always false,
            # e.g. assert sys.platform == 'lol'.  Truncate the
            # list of statements.  This mutates file.defs too.
            if i &lt; len(file.defs) - 1:
                next_def, last = file.defs[i + 1], file.defs[-1]
                if last.end_line is not None:
                    # We are on a Python version recent enough to support end lines.
                    self.skipped_lines |= set(range(next_def.line, last.end_line + 1))
            del file.defs[i + 1 :]
            break
    file.skipped_lines = self.skipped_lines

</t>
<t tx="ekr.20230831011820.1472">def visit_func_def(self, node: FuncDef) -&gt; None:
    old_global_scope = self.is_global_scope
    self.is_global_scope = False
    super().visit_func_def(node)
    self.is_global_scope = old_global_scope
    file_node = self.cur_mod_node
    if (
        self.is_global_scope
        and file_node.is_stub
        and node.name == "__getattr__"
        and file_node.is_package_init_file()
    ):
        # __init__.pyi with __getattr__ means that any submodules are assumed
        # to exist, even if there is no stub. Note that we can't verify that the
        # return type is compatible, since we haven't bound types yet.
        file_node.is_partial_stub_package = True

</t>
<t tx="ekr.20230831011820.1473">def visit_class_def(self, node: ClassDef) -&gt; None:
    old_global_scope = self.is_global_scope
    self.is_global_scope = False
    super().visit_class_def(node)
    self.is_global_scope = old_global_scope

</t>
<t tx="ekr.20230831011820.1474">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import_from(node)

</t>
<t tx="ekr.20230831011820.1475">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import_all(node)

</t>
<t tx="ekr.20230831011820.1476">def visit_import(self, node: Import) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import(node)

</t>
<t tx="ekr.20230831011820.1477">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    infer_reachability_of_if_statement(s, self.options)
    for expr in s.expr:
        expr.accept(self)
    for node in s.body:
        node.accept(self)
    if s.else_body:
        s.else_body.accept(self)

</t>
<t tx="ekr.20230831011820.1478">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        if b.end_line is not None:
            # We are on a Python version recent enough to support end lines.
            self.skipped_lines |= set(range(b.line, b.end_line + 1))
        return
    super().visit_block(b)

</t>
<t tx="ekr.20230831011820.1479">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    infer_reachability_of_match_statement(s, self.options)
    for guard in s.guards:
        if guard is not None:
            guard.accept(self)
    for body in s.bodies:
        body.accept(self)

</t>
<t tx="ekr.20230831011820.148">    def read_only_property(self, name: str, type: TypeInfo, context: Context) -&gt; None:
        self.fail(f'Property "{name}" defined in "{type.name}" is read-only', context)

</t>
<t tx="ekr.20230831011820.1480"># The remaining methods are an optimization: don't visit nested expressions
# of common statements, since they can have no effect.

def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011820.1481">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011820.1482">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011820.1483">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    s.body.accept(self)
    if s.else_body is not None:
        s.else_body.accept(self)
</t>
<t tx="ekr.20230831011820.1484">@path mypy
"""Shared definitions used by different parts of semantic analysis."""
&lt;&lt; semanal_shared.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1485">
from __future__ import annotations

from abc import abstractmethod
from typing import Callable, Final, overload
from typing_extensions import Literal, Protocol

from mypy_extensions import trait

from mypy import join
from mypy.errorcodes import LITERAL_REQ, ErrorCode
from mypy.nodes import (
    CallExpr,
    ClassDef,
    Context,
    DataclassTransformSpec,
    Decorator,
    Expression,
    FuncDef,
    NameExpr,
    Node,
    OverloadedFuncDef,
    RefExpr,
    SymbolNode,
    SymbolTable,
    SymbolTableNode,
    TypeInfo,
)
from mypy.plugin import SemanticAnalyzerPluginInterface
from mypy.tvar_scope import TypeVarLikeScope
from mypy.type_visitor import ANY_STRATEGY, BoolTypeQuery
from mypy.types import (
    TPDICT_FB_NAMES,
    AnyType,
    FunctionLike,
    Instance,
    Parameters,
    ParamSpecFlavor,
    ParamSpecType,
    PlaceholderType,
    ProperType,
    TupleType,
    Type,
    TypeOfAny,
    TypeVarId,
    TypeVarLikeType,
    get_proper_type,
)

# Subclasses can override these Var attributes with incompatible types. This can also be
# set for individual attributes using 'allow_incompatible_override' of Var.
ALLOW_INCOMPATIBLE_OVERRIDE: Final = ("__slots__", "__deletable__", "__match_args__")


# Priorities for ordering of patches within the "patch" phase of semantic analysis
# (after the main pass):

# Fix fallbacks (does joins)
PRIORITY_FALLBACKS: Final = 1


</t>
<t tx="ekr.20230831011820.1486">@trait
class SemanticAnalyzerCoreInterface:
    """A core abstract interface to generic semantic analyzer functionality.

    This is implemented by both semantic analyzer passes 2 and 3.
    """

    @others
</t>
<t tx="ekr.20230831011820.1487">@abstractmethod
def lookup_qualified(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1488">@abstractmethod
def lookup_fully_qualified(self, name: str) -&gt; SymbolTableNode:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1489">@abstractmethod
def lookup_fully_qualified_or_none(self, name: str) -&gt; SymbolTableNode | None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.149">    def incompatible_typevar_value(
        self, callee: CallableType, typ: Type, typevar_name: str, context: Context
    ) -&gt; None:
        self.fail(
            message_registry.INCOMPATIBLE_TYPEVAR_VALUE.format(
                typevar_name, callable_name(callee) or "function", format_type(typ, self.options)
            ),
            context,
            code=codes.TYPE_VAR,
        )

</t>
<t tx="ekr.20230831011820.1490">@abstractmethod
def fail(
    self,
    msg: str,
    ctx: Context,
    serious: bool = False,
    *,
    blocker: bool = False,
    code: ErrorCode | None = None,
) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1491">@abstractmethod
def note(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1492">@abstractmethod
def incomplete_feature_enabled(self, feature: str, ctx: Context) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1493">@abstractmethod
def record_incomplete_ref(self) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1494">@abstractmethod
def defer(self, debug_context: Context | None = None, force_progress: bool = False) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1495">@abstractmethod
def is_incomplete_namespace(self, fullname: str) -&gt; bool:
    """Is a module or class namespace potentially missing some definitions?"""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1496">@property
@abstractmethod
def final_iteration(self) -&gt; bool:
    """Is this the final iteration of semantic analysis?"""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1497">@abstractmethod
def is_future_flag_set(self, flag: str) -&gt; bool:
    """Is the specific __future__ feature imported"""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1498">@property
@abstractmethod
def is_stub_file(self) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1499">@abstractmethod
def is_func_scope(self) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.15">def trivial_meet(s: Type, t: Type) -&gt; ProperType:
    """Return one of types (expanded) if it is a subtype of other, otherwise bottom type."""
    if is_subtype(s, t):
        return get_proper_type(s)
    elif is_subtype(t, s):
        return get_proper_type(t)
    else:
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()


</t>
<t tx="ekr.20230831011820.150">    def dangerous_comparison(self, left: Type, right: Type, kind: str, ctx: Context) -&gt; None:
        left_str = "element" if kind == "container" else "left operand"
        right_str = "container item" if kind == "container" else "right operand"
        message = "Non-overlapping {} check ({} type: {}, {} type: {})"
        left_typ, right_typ = format_type_distinctly(left, right, options=self.options)
        self.fail(
            message.format(kind, left_str, left_typ, right_str, right_typ),
            ctx,
            code=codes.COMPARISON_OVERLAP,
        )

</t>
<t tx="ekr.20230831011820.1500">@property
@abstractmethod
def type(self) -&gt; TypeInfo | None:
    raise NotImplementedError


</t>
<t tx="ekr.20230831011820.1501">@trait
class SemanticAnalyzerInterface(SemanticAnalyzerCoreInterface):
    """A limited abstract interface to some generic semantic analyzer pass 2 functionality.

    We use this interface for various reasons:

    * Looser coupling
    * Cleaner import graph
    * Less need to pass around callback functions
    """

    @others
</t>
<t tx="ekr.20230831011820.1502">tvar_scope: TypeVarLikeScope

@abstractmethod
def lookup(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1503">@abstractmethod
def named_type(self, fullname: str, args: list[Type] | None = None) -&gt; Instance:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1504">@abstractmethod
def named_type_or_none(self, fullname: str, args: list[Type] | None = None) -&gt; Instance | None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1505">@abstractmethod
def accept(self, node: Node) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1506">@abstractmethod
def anal_type(
    self,
    t: Type,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_required: bool = False,
    allow_placeholder: bool = False,
    report_invalid_types: bool = True,
    prohibit_self_type: str | None = None,
) -&gt; Type | None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1507">@abstractmethod
def get_and_bind_all_tvars(self, type_exprs: list[Expression]) -&gt; list[TypeVarLikeType]:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1508">@abstractmethod
def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1509">@abstractmethod
def schedule_patch(self, priority: int, fn: Callable[[], None]) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.151">    def overload_inconsistently_applies_decorator(self, decorator: str, context: Context) -&gt; None:
        self.fail(
            f'Overload does not consistently use the "@{decorator}" '
            + "decorator on all function signatures.",
            context,
        )

</t>
<t tx="ekr.20230831011820.1510">@abstractmethod
def add_symbol_table_node(self, name: str, stnode: SymbolTableNode) -&gt; bool:
    """Add node to the current symbol table."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1511">@abstractmethod
def current_symbol_table(self) -&gt; SymbolTable:
    """Get currently active symbol table.

    May be module, class, or local namespace.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1512">@abstractmethod
def add_symbol(
    self,
    name: str,
    node: SymbolNode,
    context: Context,
    module_public: bool = True,
    module_hidden: bool = False,
    can_defer: bool = True,
) -&gt; bool:
    """Add symbol to the current symbol table."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1513">@abstractmethod
def add_symbol_skip_local(self, name: str, node: SymbolNode) -&gt; None:
    """Add symbol to the current symbol table, skipping locals.

    This is used to store symbol nodes in a symbol table that
    is going to be serialized (local namespaces are not serialized).
    See implementation docstring for more details.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1514">@abstractmethod
def parse_bool(self, expr: Expression) -&gt; bool | None:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1515">@abstractmethod
def qualified_name(self, n: str) -&gt; str:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1516">@property
@abstractmethod
def is_typeshed_stub_file(self) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.1517">@abstractmethod
def process_placeholder(
    self, name: str | None, kind: str, ctx: Context, force_progress: bool = False
) -&gt; None:
    raise NotImplementedError


</t>
<t tx="ekr.20230831011820.1518">def set_callable_name(sig: Type, fdef: FuncDef) -&gt; ProperType:
    sig = get_proper_type(sig)
    if isinstance(sig, FunctionLike):
        if fdef.info:
            if fdef.info.fullname in TPDICT_FB_NAMES:
                # Avoid exposing the internal _TypedDict name.
                class_name = "TypedDict"
            else:
                class_name = fdef.info.name
            return sig.with_name(f"{fdef.name} of {class_name}")
        else:
            return sig.with_name(fdef.name)
    else:
        return sig


</t>
<t tx="ekr.20230831011820.1519">def calculate_tuple_fallback(typ: TupleType) -&gt; None:
    """Calculate a precise item type for the fallback of a tuple type.

    This must be called only after the main semantic analysis pass, since joins
    aren't available before that.

    Note that there is an apparent chicken and egg problem with respect
    to verifying type arguments against bounds. Verifying bounds might
    require fallbacks, but we might use the bounds to calculate the
    fallbacks. In practice this is not a problem, since the worst that
    can happen is that we have invalid type argument values, and these
    can happen in later stages as well (they will generate errors, but
    we don't prevent their existence).
    """
    fallback = typ.partial_fallback
    assert fallback.type.fullname == "builtins.tuple"
    fallback.args = (join.join_type_list(list(typ.items)),) + fallback.args[1:]


</t>
<t tx="ekr.20230831011820.152">    def overloaded_signatures_overlap(self, index1: int, index2: int, context: Context) -&gt; None:
        self.fail(
            "Overloaded function signatures {} and {} overlap with "
            "incompatible return types".format(index1, index2),
            context,
        )

</t>
<t tx="ekr.20230831011820.1520">class _NamedTypeCallback(Protocol):
    @others
</t>
<t tx="ekr.20230831011820.1521">def __call__(self, fully_qualified_name: str, args: list[Type] | None = None) -&gt; Instance:
    ...


</t>
<t tx="ekr.20230831011820.1522">def paramspec_args(
    name: str,
    fullname: str,
    id: TypeVarId | int,
    *,
    named_type_func: _NamedTypeCallback,
    line: int = -1,
    column: int = -1,
    prefix: Parameters | None = None,
) -&gt; ParamSpecType:
    return ParamSpecType(
        name,
        fullname,
        id,
        flavor=ParamSpecFlavor.ARGS,
        upper_bound=named_type_func("builtins.tuple", [named_type_func("builtins.object")]),
        default=AnyType(TypeOfAny.from_omitted_generics),
        line=line,
        column=column,
        prefix=prefix,
    )


</t>
<t tx="ekr.20230831011820.1523">def paramspec_kwargs(
    name: str,
    fullname: str,
    id: TypeVarId | int,
    *,
    named_type_func: _NamedTypeCallback,
    line: int = -1,
    column: int = -1,
    prefix: Parameters | None = None,
) -&gt; ParamSpecType:
    return ParamSpecType(
        name,
        fullname,
        id,
        flavor=ParamSpecFlavor.KWARGS,
        upper_bound=named_type_func(
            "builtins.dict", [named_type_func("builtins.str"), named_type_func("builtins.object")]
        ),
        default=AnyType(TypeOfAny.from_omitted_generics),
        line=line,
        column=column,
        prefix=prefix,
    )


</t>
<t tx="ekr.20230831011820.1524">class HasPlaceholders(BoolTypeQuery):
    @others
</t>
<t tx="ekr.20230831011820.1525">def __init__(self) -&gt; None:
    super().__init__(ANY_STRATEGY)

</t>
<t tx="ekr.20230831011820.1526">def visit_placeholder_type(self, t: PlaceholderType) -&gt; bool:
    return True


</t>
<t tx="ekr.20230831011820.1527">def has_placeholder(typ: Type) -&gt; bool:
    """Check if a type contains any placeholder types (recursively)."""
    return typ.accept(HasPlaceholders())


</t>
<t tx="ekr.20230831011820.1528">def find_dataclass_transform_spec(node: Node | None) -&gt; DataclassTransformSpec | None:
    """
    Find the dataclass transform spec for the given node, if any exists.

    Per PEP 681 (https://peps.python.org/pep-0681/#the-dataclass-transform-decorator), dataclass
    transforms can be specified in multiple ways, including decorator functions and
    metaclasses/base classes. This function resolves the spec from any of these variants.
    """

    # The spec only lives on the function/class definition itself, so we need to unwrap down to that
    # point
    if isinstance(node, CallExpr):
        # Like dataclasses.dataclass, transform-based decorators can be applied either with or
        # without parameters; ie, both of these forms are accepted:
        #
        # @typing.dataclass_transform
        # class Foo: ...
        # @typing.dataclass_transform(eq=True, order=True, ...)
        # class Bar: ...
        #
        # We need to unwrap the call for the second variant.
        node = node.callee

    if isinstance(node, RefExpr):
        node = node.node

    if isinstance(node, Decorator):
        # typing.dataclass_transform usage must always result in a Decorator; it always uses the
        # `@dataclass_transform(...)` syntax and never `@dataclass_transform`
        node = node.func

    if isinstance(node, OverloadedFuncDef):
        # The dataclass_transform decorator may be attached to any single overload, so we must
        # search them all.
        # Note that using more than one decorator is undefined behavior, so we can just take the
        # first that we find.
        for candidate in node.items:
            spec = find_dataclass_transform_spec(candidate)
            if spec is not None:
                return spec
        return find_dataclass_transform_spec(node.impl)

    # For functions, we can directly consult the AST field for the spec
    if isinstance(node, FuncDef):
        return node.dataclass_transform_spec

    if isinstance(node, ClassDef):
        node = node.info
    if isinstance(node, TypeInfo):
        # Search all parent classes to see if any are decorated with `typing.dataclass_transform`
        for base in node.mro[1:]:
            if base.dataclass_transform_spec is not None:
                return base.dataclass_transform_spec

        # Check if there is a metaclass that is decorated with `typing.dataclass_transform`
        #
        # Note that PEP 681 only discusses using a metaclass that is directly decorated with
        # `typing.dataclass_transform`; subclasses thereof should be treated with dataclass
        # semantics rather than as transforms:
        #
        # &gt; If dataclass_transform is applied to a class, dataclass-like semantics will be assumed
        # &gt; for any class that directly or indirectly derives from the decorated class or uses the
        # &gt; decorated class as a metaclass.
        #
        # The wording doesn't make this entirely explicit, but Pyright (the reference
        # implementation for this PEP) only handles directly-decorated metaclasses.
        metaclass_type = node.metaclass_type
        if metaclass_type is not None and metaclass_type.type.dataclass_transform_spec is not None:
            return metaclass_type.type.dataclass_transform_spec

    return None


</t>
<t tx="ekr.20230831011820.1529"># Never returns `None` if a default is given
@overload
def require_bool_literal_argument(
    api: SemanticAnalyzerInterface | SemanticAnalyzerPluginInterface,
    expression: Expression,
    name: str,
    default: Literal[True] | Literal[False],
) -&gt; bool:
    ...


</t>
<t tx="ekr.20230831011820.153">    def overloaded_signature_will_never_match(
        self, index1: int, index2: int, context: Context
    ) -&gt; None:
        self.fail(
            "Overloaded function signature {index2} will never be matched: "
            "signature {index1}'s parameter type(s) are the same or broader".format(
                index1=index1, index2=index2
            ),
            context,
        )

</t>
<t tx="ekr.20230831011820.1530">@overload
def require_bool_literal_argument(
    api: SemanticAnalyzerInterface | SemanticAnalyzerPluginInterface,
    expression: Expression,
    name: str,
    default: None = None,
) -&gt; bool | None:
    ...


</t>
<t tx="ekr.20230831011820.1531">def require_bool_literal_argument(
    api: SemanticAnalyzerInterface | SemanticAnalyzerPluginInterface,
    expression: Expression,
    name: str,
    default: bool | None = None,
) -&gt; bool | None:
    """Attempt to interpret an expression as a boolean literal, and fail analysis if we can't."""
    value = parse_bool(expression)
    if value is None:
        api.fail(
            f'"{name}" argument must be a True or False literal', expression, code=LITERAL_REQ
        )
        return default

    return value


</t>
<t tx="ekr.20230831011820.1532">def parse_bool(expr: Expression) -&gt; bool | None:
    if isinstance(expr, NameExpr):
        if expr.fullname == "builtins.True":
            return True
        if expr.fullname == "builtins.False":
            return False
    return None
</t>
<t tx="ekr.20230831011820.1533">@path mypy
"""Verify properties of type arguments, like 'int' in C[int] being valid.

This must happen after semantic analysis since there can be placeholder
types until the end of semantic analysis, and these break various type
operations, including subtype checks.
"""

&lt;&lt; semanal_typeargs.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1535">from __future__ import annotations

from typing import Sequence

from mypy import errorcodes as codes, message_registry
from mypy.errorcodes import ErrorCode
from mypy.errors import Errors
from mypy.messages import format_type
from mypy.mixedtraverser import MixedTraverserVisitor
from mypy.nodes import ARG_STAR, Block, ClassDef, Context, FakeInfo, FuncItem, MypyFile
from mypy.options import Options
from mypy.scope import Scope
from mypy.subtypes import is_same_type, is_subtype
from mypy.typeanal import fix_type_var_tuple_argument, set_any_tvars
from mypy.types import (
    AnyType,
    CallableType,
    Instance,
    Parameters,
    ParamSpecType,
    TupleType,
    Type,
    TypeAliasType,
    TypeOfAny,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UnpackType,
    flatten_nested_tuples,
    get_proper_type,
    get_proper_types,
    split_with_prefix_and_suffix,
)


</t>
<t tx="ekr.20230831011820.1536">class TypeArgumentAnalyzer(MixedTraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011820.1537">def __init__(self, errors: Errors, options: Options, is_typeshed_file: bool) -&gt; None:
    super().__init__()
    self.errors = errors
    self.options = options
    self.is_typeshed_file = is_typeshed_file
    self.scope = Scope()
    # Should we also analyze function definitions, or only module top-levels?
    self.recurse_into_functions = True
    # Keep track of the type aliases already visited. This is needed to avoid
    # infinite recursion on types like A = Union[int, List[A]].
    self.seen_aliases: set[TypeAliasType] = set()

</t>
<t tx="ekr.20230831011820.1538">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.errors.set_file(o.path, o.fullname, scope=self.scope, options=self.options)
    with self.scope.module_scope(o.fullname):
        super().visit_mypy_file(o)

</t>
<t tx="ekr.20230831011820.1539">def visit_func(self, defn: FuncItem) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.scope.function_scope(defn):
        super().visit_func(defn)

</t>
<t tx="ekr.20230831011820.154">    def overloaded_signatures_typevar_specific(self, index: int, context: Context) -&gt; None:
        self.fail(
            f"Overloaded function implementation cannot satisfy signature {index} "
            + "due to inconsistencies in how they use type variables",
            context,
        )

</t>
<t tx="ekr.20230831011820.1540">def visit_class_def(self, defn: ClassDef) -&gt; None:
    with self.scope.class_scope(defn.info):
        super().visit_class_def(defn)

</t>
<t tx="ekr.20230831011820.1541">def visit_block(self, o: Block) -&gt; None:
    if not o.is_unreachable:
        super().visit_block(o)

</t>
<t tx="ekr.20230831011820.1542">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    super().visit_type_alias_type(t)
    if t in self.seen_aliases:
        # Avoid infinite recursion on recursive type aliases.
        # Note: it is fine to skip the aliases we have already seen in non-recursive
        # types, since errors there have already been reported.
        return
    self.seen_aliases.add(t)
    # Some recursive aliases may produce spurious args. In principle this is not very
    # important, as we would simply ignore them when expanding, but it is better to keep
    # correct aliases. Also, variadic aliases are better to check when fully analyzed,
    # so we do this here.
    assert t.alias is not None, f"Unfixed type alias {t.type_ref}"
    args = flatten_nested_tuples(t.args)
    if t.alias.tvar_tuple_index is not None:
        correct = len(args) &gt;= len(t.alias.alias_tvars) - 1
        if any(
            isinstance(a, UnpackType) and isinstance(get_proper_type(a.type), Instance)
            for a in args
        ):
            correct = True
    else:
        correct = len(args) == len(t.alias.alias_tvars)
    if not correct:
        if t.alias.tvar_tuple_index is not None:
            exp_len = f"at least {len(t.alias.alias_tvars) - 1}"
        else:
            exp_len = f"{len(t.alias.alias_tvars)}"
        self.fail(
            f"Bad number of arguments for type alias, expected: {exp_len}, given: {len(args)}",
            t,
            code=codes.TYPE_ARG,
        )
        t.args = set_any_tvars(
            t.alias, t.line, t.column, self.options, from_error=True, fail=self.fail
        ).args
    else:
        t.args = args
    is_error = self.validate_args(t.alias.name, t.args, t.alias.alias_tvars, t)
    if not is_error:
        # If there was already an error for the alias itself, there is no point in checking
        # the expansion, most likely it will result in the same kind of error.
        get_proper_type(t).accept(self)

</t>
<t tx="ekr.20230831011820.1543">def visit_tuple_type(self, t: TupleType) -&gt; None:
    t.items = flatten_nested_tuples(t.items)
    # We could also normalize Tuple[*tuple[X, ...]] -&gt; tuple[X, ...] like in
    # expand_type() but we can't do this here since it is not a translator visitor,
    # and we need to return an Instance instead of TupleType.
    super().visit_tuple_type(t)

</t>
<t tx="ekr.20230831011820.1544">def visit_callable_type(self, t: CallableType) -&gt; None:
    super().visit_callable_type(t)
    # Normalize trivial unpack in var args as *args: *tuple[X, ...] -&gt; *args: X
    if t.is_var_arg:
        star_index = t.arg_kinds.index(ARG_STAR)
        star_type = t.arg_types[star_index]
        if isinstance(star_type, UnpackType):
            p_type = get_proper_type(star_type.type)
            if isinstance(p_type, Instance):
                assert p_type.type.fullname == "builtins.tuple"
                t.arg_types[star_index] = p_type.args[0]

</t>
<t tx="ekr.20230831011820.1545">def visit_instance(self, t: Instance) -&gt; None:
    # Type argument counts were checked in the main semantic analyzer pass. We assume
    # that the counts are correct here.
    info = t.type
    if isinstance(info, FakeInfo):
        return  # https://github.com/python/mypy/issues/11079
    t.args = tuple(flatten_nested_tuples(t.args))
    if t.type.has_type_var_tuple_type:
        # Regular Instances are already validated in typeanal.py.
        # TODO: do something with partial overlap (probably just reject).
        # also in other places where split_with_prefix_and_suffix() is used.
        correct = len(t.args) &gt;= len(t.type.type_vars) - 1
        if any(
            isinstance(a, UnpackType) and isinstance(get_proper_type(a.type), Instance)
            for a in t.args
        ):
            correct = True
        if not correct:
            exp_len = f"at least {len(t.type.type_vars) - 1}"
            self.fail(
                f"Bad number of arguments, expected: {exp_len}, given: {len(t.args)}",
                t,
                code=codes.TYPE_ARG,
            )
            any_type = AnyType(TypeOfAny.from_error)
            t.args = (any_type,) * len(t.type.type_vars)
            fix_type_var_tuple_argument(any_type, t)
    self.validate_args(info.name, t.args, info.defn.type_vars, t)
    super().visit_instance(t)

</t>
<t tx="ekr.20230831011820.1546">def validate_args(
    self, name: str, args: Sequence[Type], type_vars: list[TypeVarLikeType], ctx: Context
) -&gt; bool:
    if any(isinstance(v, TypeVarTupleType) for v in type_vars):
        prefix = next(i for (i, v) in enumerate(type_vars) if isinstance(v, TypeVarTupleType))
        tvt = type_vars[prefix]
        assert isinstance(tvt, TypeVarTupleType)
        start, middle, end = split_with_prefix_and_suffix(
            tuple(args), prefix, len(type_vars) - prefix - 1
        )
        args = list(start) + [TupleType(list(middle), tvt.tuple_fallback)] + list(end)

    is_error = False
    for (i, arg), tvar in zip(enumerate(args), type_vars):
        if isinstance(tvar, TypeVarType):
            if isinstance(arg, ParamSpecType):
                # TODO: Better message
                is_error = True
                self.fail(f'Invalid location for ParamSpec "{arg.name}"', ctx)
                self.note(
                    "You can use ParamSpec as the first argument to Callable, e.g., "
                    "'Callable[{}, int]'".format(arg.name),
                    ctx,
                )
                continue
            if tvar.values:
                if isinstance(arg, TypeVarType):
                    if self.in_type_alias_expr:
                        # Type aliases are allowed to use unconstrained type variables
                        # error will be checked at substitution point.
                        continue
                    arg_values = arg.values
                    if not arg_values:
                        is_error = True
                        self.fail(
                            message_registry.INVALID_TYPEVAR_AS_TYPEARG.format(arg.name, name),
                            ctx,
                            code=codes.TYPE_VAR,
                        )
                        continue
                else:
                    arg_values = [arg]
                if self.check_type_var_values(name, arg_values, tvar.name, tvar.values, ctx):
                    is_error = True
            if not is_subtype(arg, tvar.upper_bound):
                if self.in_type_alias_expr and isinstance(arg, TypeVarType):
                    # Type aliases are allowed to use unconstrained type variables
                    # error will be checked at substitution point.
                    continue
                is_error = True
                self.fail(
                    message_registry.INVALID_TYPEVAR_ARG_BOUND.format(
                        format_type(arg, self.options),
                        name,
                        format_type(tvar.upper_bound, self.options),
                    ),
                    ctx,
                    code=codes.TYPE_VAR,
                )
        elif isinstance(tvar, ParamSpecType):
            if not isinstance(
                get_proper_type(arg), (ParamSpecType, Parameters, AnyType, UnboundType)
            ):
                self.fail(
                    "Can only replace ParamSpec with a parameter types list or"
                    f" another ParamSpec, got {format_type(arg, self.options)}",
                    ctx,
                )
    return is_error

</t>
<t tx="ekr.20230831011820.1547">def visit_unpack_type(self, typ: UnpackType) -&gt; None:
    super().visit_unpack_type(typ)
    proper_type = get_proper_type(typ.type)
    if isinstance(proper_type, TupleType):
        return
    if isinstance(proper_type, TypeVarTupleType):
        return
    if isinstance(proper_type, Instance) and proper_type.type.fullname == "builtins.tuple":
        return
    if isinstance(proper_type, AnyType) and proper_type.type_of_any == TypeOfAny.from_error:
        return
    if not isinstance(proper_type, UnboundType):
        # Avoid extra errors if there were some errors already.
        self.fail(
            message_registry.INVALID_UNPACK.format(format_type(proper_type, self.options)), typ
        )
    typ.type = AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011820.1548">def check_type_var_values(
    self, name: str, actuals: list[Type], arg_name: str, valids: list[Type], context: Context
) -&gt; bool:
    is_error = False
    for actual in get_proper_types(actuals):
        # We skip UnboundType here, since they may appear in defn.bases,
        # the error will be caught when visiting info.bases, that have bound type
        # variables.
        if not isinstance(actual, (AnyType, UnboundType)) and not any(
            is_same_type(actual, value) for value in valids
        ):
            is_error = True
            if len(actuals) &gt; 1 or not isinstance(actual, Instance):
                self.fail(
                    message_registry.INVALID_TYPEVAR_ARG_VALUE.format(name),
                    context,
                    code=codes.TYPE_VAR,
                )
            else:
                class_name = f'"{name}"'
                actual_type_name = f'"{actual.type.name}"'
                self.fail(
                    message_registry.INCOMPATIBLE_TYPEVAR_VALUE.format(
                        arg_name, class_name, actual_type_name
                    ),
                    context,
                    code=codes.TYPE_VAR,
                )
    return is_error

</t>
<t tx="ekr.20230831011820.1549">def fail(self, msg: str, context: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.errors.report(context.line, context.column, msg, code=code)

</t>
<t tx="ekr.20230831011820.155">    def overloaded_signatures_arg_specific(self, index: int, context: Context) -&gt; None:
        self.fail(
            "Overloaded function implementation does not accept all possible arguments "
            "of signature {}".format(index),
            context,
        )

</t>
<t tx="ekr.20230831011820.1550">def note(self, msg: str, context: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.errors.report(context.line, context.column, msg, severity="note", code=code)
</t>
<t tx="ekr.20230831011820.1551">@path mypy
"""Semantic analysis of TypedDict definitions."""
&lt;&lt; semanal_typeddict.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1552">
from __future__ import annotations

from typing import Final

from mypy import errorcodes as codes, message_registry
from mypy.errorcodes import ErrorCode
from mypy.expandtype import expand_type
from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    AssignmentStmt,
    CallExpr,
    ClassDef,
    Context,
    DictExpr,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    IndexExpr,
    NameExpr,
    PassStmt,
    RefExpr,
    Statement,
    StrExpr,
    TempNode,
    TupleExpr,
    TypedDictExpr,
    TypeInfo,
)
from mypy.options import Options
from mypy.semanal_shared import (
    SemanticAnalyzerInterface,
    has_placeholder,
    require_bool_literal_argument,
)
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type
from mypy.types import (
    TPDICT_NAMES,
    AnyType,
    RequiredType,
    Type,
    TypedDictType,
    TypeOfAny,
    TypeVarLikeType,
)

TPDICT_CLASS_ERROR: Final = (
    "Invalid statement in TypedDict definition; " 'expected "field_name: field_type"'
)


</t>
<t tx="ekr.20230831011820.1553">class TypedDictAnalyzer:
    @others
</t>
<t tx="ekr.20230831011820.1554">def __init__(
    self, options: Options, api: SemanticAnalyzerInterface, msg: MessageBuilder
) -&gt; None:
    self.options = options
    self.api = api
    self.msg = msg

</t>
<t tx="ekr.20230831011820.1555">def analyze_typeddict_classdef(self, defn: ClassDef) -&gt; tuple[bool, TypeInfo | None]:
    """Analyze a class that may define a TypedDict.

    Assume that base classes have been analyzed already.

    Note: Unlike normal classes, we won't create a TypeInfo until
    the whole definition of the TypeDict (including the body and all
    key names and types) is complete.  This is mostly because we
    store the corresponding TypedDictType in the TypeInfo.

    Return (is this a TypedDict, new TypeInfo). Specifics:
     * If we couldn't finish due to incomplete reference anywhere in
       the definition, return (True, None).
     * If this is not a TypedDict, return (False, None).
    """
    possible = False
    for base_expr in defn.base_type_exprs:
        if isinstance(base_expr, IndexExpr):
            base_expr = base_expr.base
        if isinstance(base_expr, RefExpr):
            self.api.accept(base_expr)
            if base_expr.fullname in TPDICT_NAMES or self.is_typeddict(base_expr):
                possible = True
                if isinstance(base_expr.node, TypeInfo) and base_expr.node.is_final:
                    err = message_registry.CANNOT_INHERIT_FROM_FINAL
                    self.fail(err.format(base_expr.node.name).value, defn, code=err.code)
    if not possible:
        return False, None
    existing_info = None
    if isinstance(defn.analyzed, TypedDictExpr):
        existing_info = defn.analyzed.info
    if (
        len(defn.base_type_exprs) == 1
        and isinstance(defn.base_type_exprs[0], RefExpr)
        and defn.base_type_exprs[0].fullname in TPDICT_NAMES
    ):
        # Building a new TypedDict
        fields, types, statements, required_keys = self.analyze_typeddict_classdef_fields(defn)
        if fields is None:
            return True, None  # Defer
        info = self.build_typeddict_typeinfo(
            defn.name, fields, types, required_keys, defn.line, existing_info
        )
        defn.analyzed = TypedDictExpr(info)
        defn.analyzed.line = defn.line
        defn.analyzed.column = defn.column
        defn.defs.body = statements
        return True, info

    # Extending/merging existing TypedDicts
    typeddict_bases: list[Expression] = []
    typeddict_bases_set = set()
    for expr in defn.base_type_exprs:
        if isinstance(expr, RefExpr) and expr.fullname in TPDICT_NAMES:
            if "TypedDict" not in typeddict_bases_set:
                typeddict_bases_set.add("TypedDict")
            else:
                self.fail('Duplicate base class "TypedDict"', defn)
        elif isinstance(expr, RefExpr) and self.is_typeddict(expr):
            assert expr.fullname
            if expr.fullname not in typeddict_bases_set:
                typeddict_bases_set.add(expr.fullname)
                typeddict_bases.append(expr)
            else:
                assert isinstance(expr.node, TypeInfo)
                self.fail(f'Duplicate base class "{expr.node.name}"', defn)
        elif isinstance(expr, IndexExpr) and self.is_typeddict(expr.base):
            assert isinstance(expr.base, RefExpr)
            assert expr.base.fullname
            if expr.base.fullname not in typeddict_bases_set:
                typeddict_bases_set.add(expr.base.fullname)
                typeddict_bases.append(expr)
            else:
                assert isinstance(expr.base.node, TypeInfo)
                self.fail(f'Duplicate base class "{expr.base.node.name}"', defn)
        else:
            self.fail("All bases of a new TypedDict must be TypedDict types", defn)

    keys: list[str] = []
    types = []
    required_keys = set()
    # Iterate over bases in reverse order so that leftmost base class' keys take precedence
    for base in reversed(typeddict_bases):
        self.add_keys_and_types_from_base(base, keys, types, required_keys, defn)
    (
        new_keys,
        new_types,
        new_statements,
        new_required_keys,
    ) = self.analyze_typeddict_classdef_fields(defn, keys)
    if new_keys is None:
        return True, None  # Defer
    keys.extend(new_keys)
    types.extend(new_types)
    required_keys.update(new_required_keys)
    info = self.build_typeddict_typeinfo(
        defn.name, keys, types, required_keys, defn.line, existing_info
    )
    defn.analyzed = TypedDictExpr(info)
    defn.analyzed.line = defn.line
    defn.analyzed.column = defn.column
    defn.defs.body = new_statements
    return True, info

</t>
<t tx="ekr.20230831011820.1556">def add_keys_and_types_from_base(
    self,
    base: Expression,
    keys: list[str],
    types: list[Type],
    required_keys: set[str],
    ctx: Context,
) -&gt; None:
    if isinstance(base, RefExpr):
        assert isinstance(base.node, TypeInfo)
        info = base.node
        base_args: list[Type] = []
    else:
        assert isinstance(base, IndexExpr)
        assert isinstance(base.base, RefExpr)
        assert isinstance(base.base.node, TypeInfo)
        info = base.base.node
        args = self.analyze_base_args(base, ctx)
        if args is None:
            return
        base_args = args

    assert info.typeddict_type is not None
    base_typed_dict = info.typeddict_type
    base_items = base_typed_dict.items
    valid_items = base_items.copy()

    # Always fix invalid bases to avoid crashes.
    tvars = info.defn.type_vars
    if len(base_args) != len(tvars):
        any_kind = TypeOfAny.from_omitted_generics
        if base_args:
            self.fail(f'Invalid number of type arguments for "{info.name}"', ctx)
            any_kind = TypeOfAny.from_error
        base_args = [AnyType(any_kind) for _ in tvars]

    valid_items = self.map_items_to_base(valid_items, tvars, base_args)
    for key in base_items:
        if key in keys:
            self.fail(f'Overwriting TypedDict field "{key}" while merging', ctx)
    keys.extend(valid_items.keys())
    types.extend(valid_items.values())
    required_keys.update(base_typed_dict.required_keys)

</t>
<t tx="ekr.20230831011820.1557">def analyze_base_args(self, base: IndexExpr, ctx: Context) -&gt; list[Type] | None:
    """Analyze arguments of base type expressions as types.

    We need to do this, because normal base class processing happens after
    the TypedDict special-casing (plus we get a custom error message).
    """
    base_args = []
    if isinstance(base.index, TupleExpr):
        args = base.index.items
    else:
        args = [base.index]

    for arg_expr in args:
        try:
            type = expr_to_unanalyzed_type(arg_expr, self.options, self.api.is_stub_file)
        except TypeTranslationError:
            self.fail("Invalid TypedDict type argument", ctx)
            return None
        analyzed = self.api.anal_type(
            type,
            allow_required=True,
            allow_placeholder=not self.options.disable_recursive_aliases
            and not self.api.is_func_scope(),
        )
        if analyzed is None:
            return None
        base_args.append(analyzed)
    return base_args

</t>
<t tx="ekr.20230831011820.1558">def map_items_to_base(
    self, valid_items: dict[str, Type], tvars: list[TypeVarLikeType], base_args: list[Type]
) -&gt; dict[str, Type]:
    """Map item types to how they would look in their base with type arguments applied.

    Note it is safe to use expand_type() during semantic analysis, because it should never
    (indirectly) call is_subtype().
    """
    mapped_items = {}
    for key in valid_items:
        type_in_base = valid_items[key]
        if not tvars:
            mapped_items[key] = type_in_base
            continue
        mapped_items[key] = expand_type(
            type_in_base, {t.id: a for (t, a) in zip(tvars, base_args)}
        )
    return mapped_items

</t>
<t tx="ekr.20230831011820.1559">def analyze_typeddict_classdef_fields(
    self, defn: ClassDef, oldfields: list[str] | None = None
) -&gt; tuple[list[str] | None, list[Type], list[Statement], set[str]]:
    """Analyze fields defined in a TypedDict class definition.

    This doesn't consider inherited fields (if any). Also consider totality,
    if given.

    Return tuple with these items:
     * List of keys (or None if found an incomplete reference --&gt; deferral)
     * List of types for each key
     * List of statements from defn.defs.body that are legally allowed to be a
       part of a TypedDict definition
     * Set of required keys
    """
    fields: list[str] = []
    types: list[Type] = []
    statements: list[Statement] = []
    for stmt in defn.defs.body:
        if not isinstance(stmt, AssignmentStmt):
            # Still allow pass or ... (for empty TypedDict's) and docstrings
            if isinstance(stmt, PassStmt) or (
                isinstance(stmt, ExpressionStmt)
                and isinstance(stmt.expr, (EllipsisExpr, StrExpr))
            ):
                statements.append(stmt)
            else:
                defn.removed_statements.append(stmt)
                self.fail(TPDICT_CLASS_ERROR, stmt)
        elif len(stmt.lvalues) &gt; 1 or not isinstance(stmt.lvalues[0], NameExpr):
            # An assignment, but an invalid one.
            defn.removed_statements.append(stmt)
            self.fail(TPDICT_CLASS_ERROR, stmt)
        else:
            name = stmt.lvalues[0].name
            if name in (oldfields or []):
                self.fail(f'Overwriting TypedDict field "{name}" while extending', stmt)
            if name in fields:
                self.fail(f'Duplicate TypedDict key "{name}"', stmt)
                continue
            # Append stmt, name, and type in this case...
            fields.append(name)
            statements.append(stmt)
            if stmt.type is None:
                types.append(AnyType(TypeOfAny.unannotated))
            else:
                analyzed = self.api.anal_type(
                    stmt.type,
                    allow_required=True,
                    allow_placeholder=not self.options.disable_recursive_aliases
                    and not self.api.is_func_scope(),
                    prohibit_self_type="TypedDict item type",
                )
                if analyzed is None:
                    return None, [], [], set()  # Need to defer
                types.append(analyzed)
            # ...despite possible minor failures that allow further analysis.
            if stmt.type is None or hasattr(stmt, "new_syntax") and not stmt.new_syntax:
                self.fail(TPDICT_CLASS_ERROR, stmt)
            elif not isinstance(stmt.rvalue, TempNode):
                # x: int assigns rvalue to TempNode(AnyType())
                self.fail("Right hand side values are not supported in TypedDict", stmt)
    total: bool | None = True
    if "total" in defn.keywords:
        total = require_bool_literal_argument(self.api, defn.keywords["total"], "total", True)
    required_keys = {
        field
        for (field, t) in zip(fields, types)
        if (total or (isinstance(t, RequiredType) and t.required))
        and not (isinstance(t, RequiredType) and not t.required)
    }
    types = [  # unwrap Required[T] to just T
        t.item if isinstance(t, RequiredType) else t for t in types
    ]

    return fields, types, statements, required_keys

</t>
<t tx="ekr.20230831011820.156">    def overloaded_signatures_ret_specific(self, index: int, context: Context) -&gt; None:
        self.fail(
            "Overloaded function implementation cannot produce return type "
            "of signature {}".format(index),
            context,
        )

</t>
<t tx="ekr.20230831011820.1560">def check_typeddict(
    self, node: Expression, var_name: str | None, is_func_scope: bool
) -&gt; tuple[bool, TypeInfo | None, list[TypeVarLikeType]]:
    """Check if a call defines a TypedDict.

    The optional var_name argument is the name of the variable to
    which this is assigned, if any.

    Return a pair (is it a typed dict, corresponding TypeInfo).

    If the definition is invalid but looks like a TypedDict,
    report errors but return (some) TypeInfo. If some type is not ready,
    return (True, None).
    """
    if not isinstance(node, CallExpr):
        return False, None, []
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return False, None, []
    fullname = callee.fullname
    if fullname not in TPDICT_NAMES:
        return False, None, []
    res = self.parse_typeddict_args(call)
    if res is None:
        # This is a valid typed dict, but some type is not ready.
        # The caller should defer this until next iteration.
        return True, None, []
    name, items, types, total, tvar_defs, ok = res
    if not ok:
        # Error. Construct dummy return value.
        info = self.build_typeddict_typeinfo("TypedDict", [], [], set(), call.line, None)
    else:
        if var_name is not None and name != var_name:
            self.fail(
                'First argument "{}" to TypedDict() does not match variable name "{}"'.format(
                    name, var_name
                ),
                node,
                code=codes.NAME_MATCH,
            )
        if name != var_name or is_func_scope:
            # Give it a unique name derived from the line number.
            name += "@" + str(call.line)
        required_keys = {
            field
            for (field, t) in zip(items, types)
            if (total or (isinstance(t, RequiredType) and t.required))
            and not (isinstance(t, RequiredType) and not t.required)
        }
        types = [  # unwrap Required[T] to just T
            t.item if isinstance(t, RequiredType) else t for t in types
        ]
        existing_info = None
        if isinstance(node.analyzed, TypedDictExpr):
            existing_info = node.analyzed.info
        info = self.build_typeddict_typeinfo(
            name, items, types, required_keys, call.line, existing_info
        )
        info.line = node.line
        # Store generated TypeInfo under both names, see semanal_namedtuple for more details.
        if name != var_name or is_func_scope:
            self.api.add_symbol_skip_local(name, info)
    if var_name:
        self.api.add_symbol(var_name, info, node)
    call.analyzed = TypedDictExpr(info)
    call.analyzed.set_line(call)
    return True, info, tvar_defs

</t>
<t tx="ekr.20230831011820.1561">def parse_typeddict_args(
    self, call: CallExpr
) -&gt; tuple[str, list[str], list[Type], bool, list[TypeVarLikeType], bool] | None:
    """Parse typed dict call expression.

    Return names, types, totality, was there an error during parsing.
    If some type is not ready, return None.
    """
    # TODO: Share code with check_argument_count in checkexpr.py?
    args = call.args
    if len(args) &lt; 2:
        return self.fail_typeddict_arg("Too few arguments for TypedDict()", call)
    if len(args) &gt; 3:
        return self.fail_typeddict_arg("Too many arguments for TypedDict()", call)
    # TODO: Support keyword arguments
    if call.arg_kinds not in ([ARG_POS, ARG_POS], [ARG_POS, ARG_POS, ARG_NAMED]):
        return self.fail_typeddict_arg("Unexpected arguments to TypedDict()", call)
    if len(args) == 3 and call.arg_names[2] != "total":
        return self.fail_typeddict_arg(
            f'Unexpected keyword argument "{call.arg_names[2]}" for "TypedDict"', call
        )
    if not isinstance(args[0], StrExpr):
        return self.fail_typeddict_arg(
            "TypedDict() expects a string literal as the first argument", call
        )
    if not isinstance(args[1], DictExpr):
        return self.fail_typeddict_arg(
            "TypedDict() expects a dictionary literal as the second argument", call
        )
    total: bool | None = True
    if len(args) == 3:
        total = require_bool_literal_argument(self.api, call.args[2], "total")
        if total is None:
            return "", [], [], True, [], False
    dictexpr = args[1]
    tvar_defs = self.api.get_and_bind_all_tvars([t for k, t in dictexpr.items])
    res = self.parse_typeddict_fields_with_types(dictexpr.items, call)
    if res is None:
        # One of the types is not ready, defer.
        return None
    items, types, ok = res
    for t in types:
        check_for_explicit_any(
            t, self.options, self.api.is_typeshed_stub_file, self.msg, context=call
        )

    if self.options.disallow_any_unimported:
        for t in types:
            if has_any_from_unimported_type(t):
                self.msg.unimported_type_becomes_any("Type of a TypedDict key", t, dictexpr)
    assert total is not None
    return args[0].value, items, types, total, tvar_defs, ok

</t>
<t tx="ekr.20230831011820.1562">def parse_typeddict_fields_with_types(
    self, dict_items: list[tuple[Expression | None, Expression]], context: Context
) -&gt; tuple[list[str], list[Type], bool] | None:
    """Parse typed dict items passed as pairs (name expression, type expression).

    Return names, types, was there an error. If some type is not ready, return None.
    """
    seen_keys = set()
    items: list[str] = []
    types: list[Type] = []
    for field_name_expr, field_type_expr in dict_items:
        if isinstance(field_name_expr, StrExpr):
            key = field_name_expr.value
            items.append(key)
            if key in seen_keys:
                self.fail(f'Duplicate TypedDict key "{key}"', field_name_expr)
            seen_keys.add(key)
        else:
            name_context = field_name_expr or field_type_expr
            self.fail_typeddict_arg("Invalid TypedDict() field name", name_context)
            return [], [], False
        try:
            type = expr_to_unanalyzed_type(
                field_type_expr, self.options, self.api.is_stub_file
            )
        except TypeTranslationError:
            if (
                isinstance(field_type_expr, CallExpr)
                and isinstance(field_type_expr.callee, RefExpr)
                and field_type_expr.callee.fullname in TPDICT_NAMES
            ):
                self.fail_typeddict_arg(
                    "Inline TypedDict types not supported; use assignment to define TypedDict",
                    field_type_expr,
                )
            else:
                self.fail_typeddict_arg("Invalid field type", field_type_expr)
            return [], [], False
        analyzed = self.api.anal_type(
            type,
            allow_required=True,
            allow_placeholder=not self.options.disable_recursive_aliases
            and not self.api.is_func_scope(),
            prohibit_self_type="TypedDict item type",
        )
        if analyzed is None:
            return None
        types.append(analyzed)
    return items, types, True

</t>
<t tx="ekr.20230831011820.1563">def fail_typeddict_arg(
    self, message: str, context: Context
) -&gt; tuple[str, list[str], list[Type], bool, list[TypeVarLikeType], bool]:
    self.fail(message, context)
    return "", [], [], True, [], False

</t>
<t tx="ekr.20230831011820.1564">def build_typeddict_typeinfo(
    self,
    name: str,
    items: list[str],
    types: list[Type],
    required_keys: set[str],
    line: int,
    existing_info: TypeInfo | None,
) -&gt; TypeInfo:
    # Prefer typing then typing_extensions if available.
    fallback = (
        self.api.named_type_or_none("typing._TypedDict", [])
        or self.api.named_type_or_none("typing_extensions._TypedDict", [])
        or self.api.named_type_or_none("mypy_extensions._TypedDict", [])
    )
    assert fallback is not None
    info = existing_info or self.api.basic_new_typeinfo(name, fallback, line)
    typeddict_type = TypedDictType(dict(zip(items, types)), required_keys, fallback)
    if info.special_alias and has_placeholder(info.special_alias.target):
        self.api.process_placeholder(
            None, "TypedDict item", info, force_progress=typeddict_type != info.typeddict_type
        )
    info.update_typeddict_type(typeddict_type)
    return info

</t>
<t tx="ekr.20230831011820.1565"># Helpers

def is_typeddict(self, expr: Expression) -&gt; bool:
    return (
        isinstance(expr, RefExpr)
        and isinstance(expr.node, TypeInfo)
        and expr.node.typeddict_type is not None
    )

</t>
<t tx="ekr.20230831011820.1566">def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.api.fail(msg, ctx, code=code)

</t>
<t tx="ekr.20230831011820.1567">def note(self, msg: str, ctx: Context) -&gt; None:
    self.api.note(msg, ctx)
</t>
<t tx="ekr.20230831011820.1568">@path mypy
&lt;&lt; sharedparse.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1569">from __future__ import annotations

from typing import Final

"""Shared logic between our three mypy parser files."""


_NON_BINARY_MAGIC_METHODS: Final = {
    "__abs__",
    "__call__",
    "__complex__",
    "__contains__",
    "__del__",
    "__delattr__",
    "__delitem__",
    "__enter__",
    "__exit__",
    "__float__",
    "__getattr__",
    "__getattribute__",
    "__getitem__",
    "__hex__",
    "__init__",
    "__init_subclass__",
    "__int__",
    "__invert__",
    "__iter__",
    "__len__",
    "__long__",
    "__neg__",
    "__new__",
    "__oct__",
    "__pos__",
    "__repr__",
    "__reversed__",
    "__setattr__",
    "__setitem__",
    "__str__",
}

MAGIC_METHODS_ALLOWING_KWARGS: Final = {
    "__init__",
    "__init_subclass__",
    "__new__",
    "__call__",
    "__setattr__",
}

BINARY_MAGIC_METHODS: Final = {
    "__add__",
    "__and__",
    "__divmod__",
    "__eq__",
    "__floordiv__",
    "__ge__",
    "__gt__",
    "__iadd__",
    "__iand__",
    "__idiv__",
    "__ifloordiv__",
    "__ilshift__",
    "__imatmul__",
    "__imod__",
    "__imul__",
    "__ior__",
    "__ipow__",
    "__irshift__",
    "__isub__",
    "__itruediv__",
    "__ixor__",
    "__le__",
    "__lshift__",
    "__lt__",
    "__matmul__",
    "__mod__",
    "__mul__",
    "__ne__",
    "__or__",
    "__pow__",
    "__radd__",
    "__rand__",
    "__rdiv__",
    "__rfloordiv__",
    "__rlshift__",
    "__rmatmul__",
    "__rmod__",
    "__rmul__",
    "__ror__",
    "__rpow__",
    "__rrshift__",
    "__rshift__",
    "__rsub__",
    "__rtruediv__",
    "__rxor__",
    "__sub__",
    "__truediv__",
    "__xor__",
}

assert not (_NON_BINARY_MAGIC_METHODS &amp; BINARY_MAGIC_METHODS)

MAGIC_METHODS: Final = _NON_BINARY_MAGIC_METHODS | BINARY_MAGIC_METHODS

MAGIC_METHODS_POS_ARGS_ONLY: Final = MAGIC_METHODS - MAGIC_METHODS_ALLOWING_KWARGS


</t>
<t tx="ekr.20230831011820.157">    def warn_both_operands_are_from_unions(self, context: Context) -&gt; None:
        self.note("Both left and right operands are unions", context, code=codes.OPERATOR)

</t>
<t tx="ekr.20230831011820.1570">def special_function_elide_names(name: str) -&gt; bool:
    return name in MAGIC_METHODS_POS_ARGS_ONLY


</t>
<t tx="ekr.20230831011820.1571">def argument_elide_name(name: str | None) -&gt; bool:
    return name is not None and name.startswith("__") and not name.endswith("__")
</t>
<t tx="ekr.20230831011820.1572">@path mypy
"""Type inference constraint solving"""
&lt;&lt; solve.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1573">
from __future__ import annotations

from collections import defaultdict
from typing import Iterable, Sequence, Tuple
from typing_extensions import TypeAlias as _TypeAlias

from mypy.constraints import SUBTYPE_OF, SUPERTYPE_OF, Constraint, infer_constraints
from mypy.expandtype import expand_type
from mypy.graph_utils import prepare_sccs, strongly_connected_components, topsort
from mypy.join import join_types
from mypy.meet import meet_type_list, meet_types
from mypy.subtypes import is_subtype
from mypy.typeops import get_all_type_vars
from mypy.types import (
    AnyType,
    Instance,
    NoneType,
    ParamSpecType,
    ProperType,
    TupleType,
    Type,
    TypeOfAny,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
)
from mypy.typestate import type_state

Bounds: _TypeAlias = "dict[TypeVarId, set[Type]]"
Graph: _TypeAlias = "set[tuple[TypeVarId, TypeVarId]]"
Solutions: _TypeAlias = "dict[TypeVarId, Type | None]"


</t>
<t tx="ekr.20230831011820.1574">def solve_constraints(
    original_vars: Sequence[TypeVarLikeType],
    constraints: list[Constraint],
    strict: bool = True,
    allow_polymorphic: bool = False,
) -&gt; tuple[list[Type | None], list[TypeVarLikeType]]:
    """Solve type constraints.

    Return the best type(s) for type variables; each type can be None if the value of
    the variable could not be solved.

    If a variable has no constraints, if strict=True then arbitrarily
    pick UninhabitedType as the value of the type variable. If strict=False, pick AnyType.
    If allow_polymorphic=True, then use the full algorithm that can potentially return
    free type variables in solutions (these require special care when applying). Otherwise,
    use a simplified algorithm that just solves each type variable individually if possible.
    """
    vars = [tv.id for tv in original_vars]
    if not vars:
        return [], []

    originals = {tv.id: tv for tv in original_vars}
    extra_vars: list[TypeVarId] = []
    # Get additional type variables from generic actuals.
    for c in constraints:
        extra_vars.extend([v.id for v in c.extra_tvars if v.id not in vars + extra_vars])
        originals.update({v.id: v for v in c.extra_tvars if v.id not in originals})

    # Collect a list of constraints for each type variable.
    cmap: dict[TypeVarId, list[Constraint]] = {tv: [] for tv in vars + extra_vars}
    for con in constraints:
        if con.type_var in vars + extra_vars:
            cmap[con.type_var].append(con)

    if allow_polymorphic:
        if constraints:
            solutions, free_vars = solve_with_dependent(
                vars + extra_vars, constraints, vars, originals
            )
        else:
            solutions = {}
            free_vars = []
    else:
        solutions = {}
        free_vars = []
        for tv, cs in cmap.items():
            if not cs:
                continue
            lowers = [c.target for c in cs if c.op == SUPERTYPE_OF]
            uppers = [c.target for c in cs if c.op == SUBTYPE_OF]
            solution = solve_one(lowers, uppers)

            # Do not leak type variables in non-polymorphic solutions.
            if solution is None or not get_vars(
                solution, [tv for tv in extra_vars if tv not in vars]
            ):
                solutions[tv] = solution

    res: list[Type | None] = []
    for v in vars:
        if v in solutions:
            res.append(solutions[v])
        else:
            # No constraints for type variable -- 'UninhabitedType' is the most specific type.
            candidate: Type
            if strict:
                candidate = UninhabitedType()
                candidate.ambiguous = True
            else:
                candidate = AnyType(TypeOfAny.special_form)
            res.append(candidate)
    return res, free_vars


</t>
<t tx="ekr.20230831011820.1575">def solve_with_dependent(
    vars: list[TypeVarId],
    constraints: list[Constraint],
    original_vars: list[TypeVarId],
    originals: dict[TypeVarId, TypeVarLikeType],
) -&gt; tuple[Solutions, list[TypeVarLikeType]]:
    """Solve set of constraints that may depend on each other, like T &lt;: List[S].

    The whole algorithm consists of five steps:
      * Propagate via linear constraints and use secondary constraints to get transitive closure
      * Find dependencies between type variables, group them in SCCs, and sort topologically
      * Check that all SCC are intrinsically linear, we can't solve (express) T &lt;: List[T]
      * Variables in leaf SCCs that don't have constant bounds are free (choose one per SCC)
      * Solve constraints iteratively starting from leafs, updating bounds after each step.
    """
    graph, lowers, uppers = transitive_closure(vars, constraints)

    dmap = compute_dependencies(vars, graph, lowers, uppers)
    sccs = list(strongly_connected_components(set(vars), dmap))
    if not all(check_linear(scc, lowers, uppers) for scc in sccs):
        return {}, []
    raw_batches = list(topsort(prepare_sccs(sccs, dmap)))

    free_vars = []
    free_solutions = {}
    for scc in raw_batches[0]:
        # If there are no bounds on this SCC, then the only meaningful solution we can
        # express, is that each variable is equal to a new free variable. For example,
        # if we have T &lt;: S, S &lt;: U, we deduce: T = S = U = &lt;free&gt;.
        if all(not lowers[tv] and not uppers[tv] for tv in scc):
            best_free = choose_free([originals[tv] for tv in scc], original_vars)
            if best_free:
                free_vars.append(best_free.id)
                free_solutions[best_free.id] = best_free

    # Update lowers/uppers with free vars, so these can now be used
    # as valid solutions.
    for l, u in graph:
        if l in free_vars:
            lowers[u].add(free_solutions[l])
        if u in free_vars:
            uppers[l].add(free_solutions[u])

    # Flatten the SCCs that are independent, we can solve them together,
    # since we don't need to update any targets in between.
    batches = []
    for batch in raw_batches:
        next_bc = []
        for scc in batch:
            next_bc.extend(list(scc))
        batches.append(next_bc)

    solutions: dict[TypeVarId, Type | None] = {}
    for flat_batch in batches:
        res = solve_iteratively(flat_batch, graph, lowers, uppers)
        solutions.update(res)
    return solutions, [free_solutions[tv] for tv in free_vars]


</t>
<t tx="ekr.20230831011820.1576">def solve_iteratively(
    batch: list[TypeVarId], graph: Graph, lowers: Bounds, uppers: Bounds
) -&gt; Solutions:
    """Solve transitive closure sequentially, updating upper/lower bounds after each step.

    Transitive closure is represented as a linear graph plus lower/upper bounds for each
    type variable, see transitive_closure() docstring for details.

    We solve for type variables that appear in `batch`. If a bound is not constant (i.e. it
    looks like T :&gt; F[S, ...]), we substitute solutions found so far in the target F[S, ...]
    after solving the batch.

    Importantly, after solving each variable in a batch, we move it from linear graph to
    upper/lower bounds, this way we can guarantee consistency of solutions (see comment below
    for an example when this is important).
    """
    solutions = {}
    s_batch = set(batch)
    while s_batch:
        for tv in sorted(s_batch, key=lambda x: x.raw_id):
            if lowers[tv] or uppers[tv]:
                solvable_tv = tv
                break
        else:
            break
        # Solve each solvable type variable separately.
        s_batch.remove(solvable_tv)
        result = solve_one(lowers[solvable_tv], uppers[solvable_tv])
        solutions[solvable_tv] = result
        if result is None:
            # TODO: support backtracking lower/upper bound choices and order within SCCs.
            # (will require switching this function from iterative to recursive).
            continue

        # Update the (transitive) bounds from graph if there is a solution.
        # This is needed to guarantee solutions will never contradict the initial
        # constraints. For example, consider {T &lt;: S, T &lt;: A, S :&gt; B} with A :&gt; B.
        # If we would not update the uppers/lowers from graph, we would infer T = A, S = B
        # which is not correct.
        for l, u in graph.copy():
            if l == u:
                continue
            if l == solvable_tv:
                lowers[u].add(result)
                graph.remove((l, u))
            if u == solvable_tv:
                uppers[l].add(result)
                graph.remove((l, u))

    # We can update uppers/lowers only once after solving the whole SCC,
    # since uppers/lowers can't depend on type variables in the SCC
    # (and we would reject such SCC as non-linear and therefore not solvable).
    subs = {tv: s for (tv, s) in solutions.items() if s is not None}
    for tv in lowers:
        lowers[tv] = {expand_type(lt, subs) for lt in lowers[tv]}
    for tv in uppers:
        uppers[tv] = {expand_type(ut, subs) for ut in uppers[tv]}
    return solutions


</t>
<t tx="ekr.20230831011820.1577">def solve_one(lowers: Iterable[Type], uppers: Iterable[Type]) -&gt; Type | None:
    """Solve constraints by finding by using meets of upper bounds, and joins of lower bounds."""
    bottom: Type | None = None
    top: Type | None = None
    candidate: Type | None = None

    # Process each bound separately, and calculate the lower and upper
    # bounds based on constraints. Note that we assume that the constraint
    # targets do not have constraint references.
    for target in lowers:
        if bottom is None:
            bottom = target
        else:
            if type_state.infer_unions:
                # This deviates from the general mypy semantics because
                # recursive types are union-heavy in 95% of cases.
                bottom = UnionType.make_union([bottom, target])
            else:
                bottom = join_types(bottom, target)

    for target in uppers:
        if top is None:
            top = target
        else:
            top = meet_types(top, target)

    p_top = get_proper_type(top)
    p_bottom = get_proper_type(bottom)
    if isinstance(p_top, AnyType) or isinstance(p_bottom, AnyType):
        source_any = top if isinstance(p_top, AnyType) else bottom
        assert isinstance(source_any, ProperType) and isinstance(source_any, AnyType)
        return AnyType(TypeOfAny.from_another_any, source_any=source_any)
    elif bottom is None:
        if top:
            candidate = top
        else:
            # No constraints for type variable
            return None
    elif top is None:
        candidate = bottom
    elif is_subtype(bottom, top):
        candidate = bottom
    else:
        candidate = None
    return candidate


</t>
<t tx="ekr.20230831011820.1578">def choose_free(
    scc: list[TypeVarLikeType], original_vars: list[TypeVarId]
) -&gt; TypeVarLikeType | None:
    """Choose the best solution for an SCC containing only type variables.

    This is needed to preserve e.g. the upper bound in a situation like this:
        def dec(f: Callable[[T], S]) -&gt; Callable[[T], S]: ...

        @dec
        def test(x: U) -&gt; U: ...

    where U &lt;: A.
    """

    if len(scc) == 1:
        # Fast path, choice is trivial.
        return scc[0]

    common_upper_bound = meet_type_list([t.upper_bound for t in scc])
    common_upper_bound_p = get_proper_type(common_upper_bound)
    # We include None for when strict-optional is disabled.
    if isinstance(common_upper_bound_p, (UninhabitedType, NoneType)):
        # This will cause to infer &lt;nothing&gt;, which is better than a free TypeVar
        # that has an upper bound &lt;nothing&gt;.
        return None

    values: list[Type] = []
    for tv in scc:
        if isinstance(tv, TypeVarType) and tv.values:
            if values:
                # It is too tricky to support multiple TypeVars with values
                # within the same SCC.
                return None
            values = tv.values.copy()

    if values and not is_trivial_bound(common_upper_bound_p):
        # If there are both values and upper bound present, we give up,
        # since type variables having both are not supported.
        return None

    # For convenience with current type application machinery, we use a stable
    # choice that prefers the original type variables (not polymorphic ones) in SCC.
    best = sorted(scc, key=lambda x: (x.id not in original_vars, x.id.raw_id))[0]
    if isinstance(best, TypeVarType):
        return best.copy_modified(values=values, upper_bound=common_upper_bound)
    if is_trivial_bound(common_upper_bound_p):
        # TODO: support more cases for ParamSpecs/TypeVarTuples
        return best
    return None


</t>
<t tx="ekr.20230831011820.1579">def is_trivial_bound(tp: ProperType) -&gt; bool:
    return isinstance(tp, Instance) and tp.type.fullname == "builtins.object"


</t>
<t tx="ekr.20230831011820.158">    def warn_operand_was_from_union(self, side: str, original: Type, context: Context) -&gt; None:
        self.note(
            f"{side} operand is of type {format_type(original, self.options)}",
            context,
            code=codes.OPERATOR,
        )

</t>
<t tx="ekr.20230831011820.1580">def find_linear(c: Constraint) -&gt; Tuple[bool, TypeVarId | None]:
    """Find out if this constraint represent a linear relationship, return target id if yes."""
    if isinstance(c.origin_type_var, TypeVarType):
        if isinstance(c.target, TypeVarType):
            return True, c.target.id
    if isinstance(c.origin_type_var, ParamSpecType):
        if isinstance(c.target, ParamSpecType) and not c.target.prefix.arg_types:
            return True, c.target.id
    if isinstance(c.origin_type_var, TypeVarTupleType):
        target = get_proper_type(c.target)
        if isinstance(target, TupleType) and len(target.items) == 1:
            item = target.items[0]
            if isinstance(item, UnpackType) and isinstance(item.type, TypeVarTupleType):
                return True, item.type.id
    return False, None


</t>
<t tx="ekr.20230831011820.1581">def transitive_closure(
    tvars: list[TypeVarId], constraints: list[Constraint]
) -&gt; tuple[Graph, Bounds, Bounds]:
    """Find transitive closure for given constraints on type variables.

    Transitive closure gives maximal set of lower/upper bounds for each type variable,
    such that we cannot deduce any further bounds by chaining other existing bounds.

    The transitive closure is represented by:
      * A set of lower and upper bounds for each type variable, where only constant and
        non-linear terms are included in the bounds.
      * A graph of linear constraints between type variables (represented as a set of pairs)
    Such separation simplifies reasoning, and allows an efficient and simple incremental
    transitive closure algorithm that we use here.

    For example if we have initial constraints [T &lt;: S, S &lt;: U, U &lt;: int], the transitive
    closure is given by:
      * {} &lt;: T &lt;: {int}
      * {} &lt;: S &lt;: {int}
      * {} &lt;: U &lt;: {int}
      * {T &lt;: S, S &lt;: U, T &lt;: U}
    """
    uppers: Bounds = defaultdict(set)
    lowers: Bounds = defaultdict(set)
    graph: Graph = {(tv, tv) for tv in tvars}

    remaining = set(constraints)
    while remaining:
        c = remaining.pop()
        # Note that ParamSpec constraint P &lt;: Q may be considered linear only if Q has no prefix,
        # for cases like P &lt;: Concatenate[T, Q] we should consider this non-linear and put {P} and
        # {T, Q} into separate SCCs. Similarly, Ts &lt;: Tuple[*Us] considered linear, while
        # Ts &lt;: Tuple[*Us, U] is non-linear.
        is_linear, target_id = find_linear(c)
        if is_linear and target_id in tvars:
            assert target_id is not None
            if c.op == SUBTYPE_OF:
                lower, upper = c.type_var, target_id
            else:
                lower, upper = target_id, c.type_var
            if (lower, upper) in graph:
                continue
            graph |= {
                (l, u) for l in tvars for u in tvars if (l, lower) in graph and (upper, u) in graph
            }
            for u in tvars:
                if (upper, u) in graph:
                    lowers[u] |= lowers[lower]
            for l in tvars:
                if (l, lower) in graph:
                    uppers[l] |= uppers[upper]
            for lt in lowers[lower]:
                for ut in uppers[upper]:
                    # TODO: what if secondary constraints result in inference
                    # against polymorphic actual (also in below branches)?
                    remaining |= set(infer_constraints(lt, ut, SUBTYPE_OF))
                    remaining |= set(infer_constraints(ut, lt, SUPERTYPE_OF))
        elif c.op == SUBTYPE_OF:
            if c.target in uppers[c.type_var]:
                continue
            for l in tvars:
                if (l, c.type_var) in graph:
                    uppers[l].add(c.target)
            for lt in lowers[c.type_var]:
                remaining |= set(infer_constraints(lt, c.target, SUBTYPE_OF))
                remaining |= set(infer_constraints(c.target, lt, SUPERTYPE_OF))
        else:
            assert c.op == SUPERTYPE_OF
            if c.target in lowers[c.type_var]:
                continue
            for u in tvars:
                if (c.type_var, u) in graph:
                    lowers[u].add(c.target)
            for ut in uppers[c.type_var]:
                remaining |= set(infer_constraints(ut, c.target, SUPERTYPE_OF))
                remaining |= set(infer_constraints(c.target, ut, SUBTYPE_OF))
    return graph, lowers, uppers


</t>
<t tx="ekr.20230831011820.1582">def compute_dependencies(
    tvars: list[TypeVarId], graph: Graph, lowers: Bounds, uppers: Bounds
) -&gt; dict[TypeVarId, list[TypeVarId]]:
    """Compute dependencies between type variables induced by constraints.

    If we have a constraint like T &lt;: List[S], we say that T depends on S, since
    we will need to solve for S first before we can solve for T.
    """
    res = {}
    for tv in tvars:
        deps = set()
        for lt in lowers[tv]:
            deps |= get_vars(lt, tvars)
        for ut in uppers[tv]:
            deps |= get_vars(ut, tvars)
        for other in tvars:
            if other == tv:
                continue
            if (tv, other) in graph or (other, tv) in graph:
                deps.add(other)
        res[tv] = list(deps)
    return res


</t>
<t tx="ekr.20230831011820.1583">def check_linear(scc: set[TypeVarId], lowers: Bounds, uppers: Bounds) -&gt; bool:
    """Check there are only linear constraints between type variables in SCC.

    Linear are constraints like T &lt;: S (while T &lt;: F[S] are non-linear).
    """
    for tv in scc:
        if any(get_vars(lt, list(scc)) for lt in lowers[tv]):
            return False
        if any(get_vars(ut, list(scc)) for ut in uppers[tv]):
            return False
    return True


</t>
<t tx="ekr.20230831011820.1584">def get_vars(target: Type, vars: list[TypeVarId]) -&gt; set[TypeVarId]:
    """Find type variables for which we are solving in a target type."""
    return {tv.id for tv in get_all_type_vars(target)} &amp; set(vars)
</t>
<t tx="ekr.20230831011820.1585">@path mypy
"""Split namespace for argparse to allow separating options by prefix.

We use this to direct some options to an Options object and some to a
regular namespace.
"""

&lt;&lt; split_namespace.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1587"># In its own file largely because mypyc doesn't support its use of
# __getattr__/__setattr__ and has some issues with __dict__

from __future__ import annotations

import argparse
from typing import Any


</t>
<t tx="ekr.20230831011820.1588">class SplitNamespace(argparse.Namespace):
    @others
</t>
<t tx="ekr.20230831011820.1589">def __init__(self, standard_namespace: object, alt_namespace: object, alt_prefix: str) -&gt; None:
    self.__dict__["_standard_namespace"] = standard_namespace
    self.__dict__["_alt_namespace"] = alt_namespace
    self.__dict__["_alt_prefix"] = alt_prefix

</t>
<t tx="ekr.20230831011820.159">    def operator_method_signatures_overlap(
        self,
        reverse_class: TypeInfo,
        reverse_method: str,
        forward_class: Type,
        forward_method: str,
        context: Context,
    ) -&gt; None:
        self.fail(
            'Signatures of "{}" of "{}" and "{}" of {} '
            "are unsafely overlapping".format(
                reverse_method,
                reverse_class.name,
                forward_method,
                format_type(forward_class, self.options),
            ),
            context,
        )

</t>
<t tx="ekr.20230831011820.1590">def _get(self) -&gt; tuple[Any, Any]:
    return (self._standard_namespace, self._alt_namespace)

</t>
<t tx="ekr.20230831011820.1591">def __setattr__(self, name: str, value: Any) -&gt; None:
    if name.startswith(self._alt_prefix):
        setattr(self._alt_namespace, name[len(self._alt_prefix) :], value)
    else:
        setattr(self._standard_namespace, name, value)

</t>
<t tx="ekr.20230831011820.1592">def __getattr__(self, name: str) -&gt; Any:
    if name.startswith(self._alt_prefix):
        return getattr(self._alt_namespace, name[len(self._alt_prefix) :])
    else:
        return getattr(self._standard_namespace, name)
</t>
<t tx="ekr.20230831011820.1593">@path mypy
&lt;&lt; state.py: preamble &gt;&gt;
@others


state: Final = StrictOptionalState(strict_optional=False)
find_occurrences: tuple[str, str] | None = None
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1594">from __future__ import annotations

from contextlib import contextmanager
from typing import Final, Iterator

# These are global mutable state. Don't add anything here unless there's a very
# good reason.


</t>
<t tx="ekr.20230831011820.1595">class StrictOptionalState:
    @others
</t>
<t tx="ekr.20230831011820.1596"># Wrap this in a class since it's faster that using a module-level attribute.

def __init__(self, strict_optional: bool) -&gt; None:
    # Value varies by file being processed
    self.strict_optional = strict_optional

</t>
<t tx="ekr.20230831011820.1597">@contextmanager
def strict_optional_set(self, value: bool) -&gt; Iterator[None]:
    saved = self.strict_optional
    self.strict_optional = value
    try:
        yield
    finally:
        self.strict_optional = saved
</t>
<t tx="ekr.20230831011820.1598">@path mypy
"""Utilities for calculating and reporting statistics about types."""
&lt;&lt; stats.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1599">
from __future__ import annotations

import os
from collections import Counter
from contextlib import contextmanager
from typing import Final, Iterator

from mypy import nodes
from mypy.argmap import map_formals_to_actuals
from mypy.nodes import (
    AssignmentExpr,
    AssignmentStmt,
    BreakStmt,
    BytesExpr,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ContinueStmt,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    FloatExpr,
    FuncDef,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    OpExpr,
    PassStmt,
    RefExpr,
    StrExpr,
    TypeApplication,
    UnaryExpr,
    YieldFromExpr,
)
from mypy.traverser import TraverserVisitor
from mypy.typeanal import collect_all_inner_types
from mypy.types import (
    AnyType,
    CallableType,
    FunctionLike,
    Instance,
    TupleType,
    Type,
    TypeOfAny,
    TypeQuery,
    TypeVarType,
    get_proper_type,
    get_proper_types,
)
from mypy.util import correct_relative_import

TYPE_EMPTY: Final = 0
TYPE_UNANALYZED: Final = 1  # type of non-typechecked code
TYPE_PRECISE: Final = 2
TYPE_IMPRECISE: Final = 3
TYPE_ANY: Final = 4

precision_names: Final = ["empty", "unanalyzed", "precise", "imprecise", "any"]


</t>
<t tx="ekr.20230831011820.16">def meet_types(s: Type, t: Type) -&gt; ProperType:
    """Return the greatest lower bound of two types."""
    if is_recursive_pair(s, t):
        # This case can trigger an infinite recursion, general support for this will be
        # tricky, so we use a trivial meet (like for protocols).
        return trivial_meet(s, t)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if isinstance(s, Instance) and isinstance(t, Instance) and s.type == t.type:
        # Code in checker.py should merge any extra_items where possible, so we
        # should have only compatible extra_items here. We check this before
        # the below subtype check, so that extra_attrs will not get erased.
        if (s.extra_attrs or t.extra_attrs) and is_same_type(s, t):
            if s.extra_attrs and t.extra_attrs:
                if len(s.extra_attrs.attrs) &gt; len(t.extra_attrs.attrs):
                    # Return the one that has more precise information.
                    return s
                return t
            if s.extra_attrs:
                return s
            return t

    if not isinstance(s, UnboundType) and not isinstance(t, UnboundType):
        if is_proper_subtype(s, t, ignore_promotions=True):
            return s
        if is_proper_subtype(t, s, ignore_promotions=True):
            return t

    if isinstance(s, ErasedType):
        return s
    if isinstance(s, AnyType):
        return t
    if isinstance(s, UnionType) and not isinstance(t, UnionType):
        s, t = t, s

    # Meets/joins require callable type normalization.
    s, t = join.normalize_callables(s, t)

    return t.accept(TypeMeetVisitor(s))


</t>
<t tx="ekr.20230831011820.160">    def forward_operator_not_callable(self, forward_method: str, context: Context) -&gt; None:
        self.fail(f'Forward operator "{forward_method}" is not callable', context)

</t>
<t tx="ekr.20230831011820.1600">class StatisticsVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011820.1601">def __init__(
    self,
    inferred: bool,
    filename: str,
    modules: dict[str, MypyFile],
    typemap: dict[Expression, Type] | None = None,
    all_nodes: bool = False,
    visit_untyped_defs: bool = True,
) -&gt; None:
    self.inferred = inferred
    self.filename = filename
    self.modules = modules
    self.typemap = typemap
    self.all_nodes = all_nodes
    self.visit_untyped_defs = visit_untyped_defs

    self.num_precise_exprs = 0
    self.num_imprecise_exprs = 0
    self.num_any_exprs = 0

    self.num_simple_types = 0
    self.num_generic_types = 0
    self.num_tuple_types = 0
    self.num_function_types = 0
    self.num_typevar_types = 0
    self.num_complex_types = 0
    self.num_any_types = 0

    self.line = -1

    self.line_map: dict[int, int] = {}

    self.type_of_any_counter: Counter[int] = Counter()
    self.any_line_map: dict[int, list[AnyType]] = {}

    # For each scope (top level/function), whether the scope was type checked
    # (annotated function).
    #
    # TODO: Handle --check-untyped-defs
    self.checked_scopes = [True]

    self.output: list[str] = []

    TraverserVisitor.__init__(self)

</t>
<t tx="ekr.20230831011820.1602">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.cur_mod_node = o
    self.cur_mod_id = o.fullname
    super().visit_mypy_file(o)

</t>
<t tx="ekr.20230831011820.1603">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    self.process_import(imp)

</t>
<t tx="ekr.20230831011820.1604">def visit_import_all(self, imp: ImportAll) -&gt; None:
    self.process_import(imp)

</t>
<t tx="ekr.20230831011820.1605">def process_import(self, imp: ImportFrom | ImportAll) -&gt; None:
    import_id, ok = correct_relative_import(
        self.cur_mod_id, imp.relative, imp.id, self.cur_mod_node.is_package_init_file()
    )
    if ok and import_id in self.modules:
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(imp.line, kind)

</t>
<t tx="ekr.20230831011820.1606">def visit_import(self, imp: Import) -&gt; None:
    if all(id in self.modules for id, _ in imp.ids):
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(imp.line, kind)

</t>
<t tx="ekr.20230831011820.1607">def visit_func_def(self, o: FuncDef) -&gt; None:
    with self.enter_scope(o):
        self.line = o.line
        if len(o.expanded) &gt; 1 and o.expanded != [o] * len(o.expanded):
            if o in o.expanded:
                print(
                    "{}:{}: ERROR: cycle in function expansion; skipping".format(
                        self.filename, o.line
                    )
                )
                return
            for defn in o.expanded:
                assert isinstance(defn, FuncDef)
                self.visit_func_def(defn)
        else:
            if o.type:
                assert isinstance(o.type, CallableType)
                sig = o.type
                arg_types = sig.arg_types
                if sig.arg_names and sig.arg_names[0] == "self" and not self.inferred:
                    arg_types = arg_types[1:]
                for arg in arg_types:
                    self.type(arg)
                self.type(sig.ret_type)
            elif self.all_nodes:
                self.record_line(self.line, TYPE_ANY)
            if not o.is_dynamic() or self.visit_untyped_defs:
                super().visit_func_def(o)

</t>
<t tx="ekr.20230831011820.1608">@contextmanager
def enter_scope(self, o: FuncDef) -&gt; Iterator[None]:
    self.checked_scopes.append(o.type is not None and self.checked_scopes[-1])
    yield None
    self.checked_scopes.pop()

</t>
<t tx="ekr.20230831011820.1609">def is_checked_scope(self) -&gt; bool:
    return self.checked_scopes[-1]

</t>
<t tx="ekr.20230831011820.161">    def signatures_incompatible(self, method: str, other_method: str, context: Context) -&gt; None:
        self.fail(f'Signatures of "{method}" and "{other_method}" are incompatible', context)

</t>
<t tx="ekr.20230831011820.1610">def visit_class_def(self, o: ClassDef) -&gt; None:
    self.record_line(o.line, TYPE_PRECISE)  # TODO: Look at base classes
    # Override this method because we don't want to analyze base_type_exprs (base_type_exprs
    # are base classes in a class declaration).
    # While base_type_exprs are technically expressions, type analyzer does not visit them and
    # they are not in the typemap.
    for d in o.decorators:
        d.accept(self)
    o.defs.accept(self)

</t>
<t tx="ekr.20230831011820.1611">def visit_type_application(self, o: TypeApplication) -&gt; None:
    self.line = o.line
    for t in o.types:
        self.type(t)
    super().visit_type_application(o)

</t>
<t tx="ekr.20230831011820.1612">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    self.line = o.line
    if isinstance(o.rvalue, nodes.CallExpr) and isinstance(
        o.rvalue.analyzed, nodes.TypeVarExpr
    ):
        # Type variable definition -- not a real assignment.
        return
    if o.type:
        self.type(o.type)
    elif self.inferred and not self.all_nodes:
        # if self.all_nodes is set, lvalues will be visited later
        for lvalue in o.lvalues:
            if isinstance(lvalue, nodes.TupleExpr):
                items = lvalue.items
            else:
                items = [lvalue]
            for item in items:
                if isinstance(item, RefExpr) and item.is_inferred_def:
                    if self.typemap is not None:
                        self.type(self.typemap.get(item))
    super().visit_assignment_stmt(o)

</t>
<t tx="ekr.20230831011820.1613">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    if isinstance(o.expr, (StrExpr, BytesExpr)):
        # Docstring
        self.record_line(o.line, TYPE_EMPTY)
    else:
        super().visit_expression_stmt(o)

</t>
<t tx="ekr.20230831011820.1614">def visit_pass_stmt(self, o: PassStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1615">def visit_break_stmt(self, o: BreakStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1616">def visit_continue_stmt(self, o: ContinueStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1617">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if o.fullname in ("builtins.None", "builtins.True", "builtins.False", "builtins.Ellipsis"):
        self.record_precise_if_checked_scope(o)
    else:
        self.process_node(o)
        super().visit_name_expr(o)

</t>
<t tx="ekr.20230831011820.1618">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    if o.expr:
        o.expr.accept(self)

</t>
<t tx="ekr.20230831011820.1619">def visit_call_expr(self, o: CallExpr) -&gt; None:
    self.process_node(o)
    if o.analyzed:
        o.analyzed.accept(self)
    else:
        o.callee.accept(self)
        for a in o.args:
            a.accept(self)
        self.record_call_target_precision(o)

</t>
<t tx="ekr.20230831011820.162">    def yield_from_invalid_operand_type(self, expr: Type, context: Context) -&gt; Type:
        text = (
            format_type(expr, self.options)
            if format_type(expr, self.options) != "object"
            else expr
        )
        self.fail(f'"yield from" can\'t be applied to {text}', context)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011820.1620">def record_call_target_precision(self, o: CallExpr) -&gt; None:
    """Record precision of formal argument types used in a call."""
    if not self.typemap or o.callee not in self.typemap:
        # Type not available.
        return
    callee_type = get_proper_type(self.typemap[o.callee])
    if isinstance(callee_type, CallableType):
        self.record_callable_target_precision(o, callee_type)
    else:
        pass  # TODO: Handle overloaded functions, etc.

</t>
<t tx="ekr.20230831011820.1621">def record_callable_target_precision(self, o: CallExpr, callee: CallableType) -&gt; None:
    """Record imprecision caused by callee argument types.

    This only considers arguments passed in a call expression. Arguments
    with default values that aren't provided in a call arguably don't
    contribute to typing imprecision at the *call site* (but they
    contribute at the function definition).
    """
    assert self.typemap
    typemap = self.typemap
    actual_to_formal = map_formals_to_actuals(
        o.arg_kinds,
        o.arg_names,
        callee.arg_kinds,
        callee.arg_names,
        lambda n: typemap[o.args[n]],
    )
    for formals in actual_to_formal:
        for n in formals:
            formal = get_proper_type(callee.arg_types[n])
            if isinstance(formal, AnyType):
                self.record_line(o.line, TYPE_ANY)
            elif is_imprecise(formal):
                self.record_line(o.line, TYPE_IMPRECISE)

</t>
<t tx="ekr.20230831011820.1622">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    self.process_node(o)
    super().visit_member_expr(o)

</t>
<t tx="ekr.20230831011820.1623">def visit_op_expr(self, o: OpExpr) -&gt; None:
    self.process_node(o)
    super().visit_op_expr(o)

</t>
<t tx="ekr.20230831011820.1624">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    self.process_node(o)
    super().visit_comparison_expr(o)

</t>
<t tx="ekr.20230831011820.1625">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    self.process_node(o)
    super().visit_index_expr(o)

</t>
<t tx="ekr.20230831011820.1626">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    self.process_node(o)
    super().visit_assignment_expr(o)

</t>
<t tx="ekr.20230831011820.1627">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    self.process_node(o)
    super().visit_unary_expr(o)

</t>
<t tx="ekr.20230831011820.1628">def visit_str_expr(self, o: StrExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1629">def visit_bytes_expr(self, o: BytesExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.163">    def invalid_signature(self, func_type: Type, context: Context) -&gt; None:
        self.fail(f"Invalid signature {format_type(func_type, self.options)}", context)

</t>
<t tx="ekr.20230831011820.1630">def visit_int_expr(self, o: IntExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1631">def visit_float_expr(self, o: FloatExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1632">def visit_complex_expr(self, o: ComplexExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1633">def visit_ellipsis(self, o: EllipsisExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20230831011820.1634"># Helpers

def process_node(self, node: Expression) -&gt; None:
    if self.all_nodes:
        if self.typemap is not None:
            self.line = node.line
            self.type(self.typemap.get(node))

</t>
<t tx="ekr.20230831011820.1635">def record_precise_if_checked_scope(self, node: Node) -&gt; None:
    if isinstance(node, Expression) and self.typemap and node not in self.typemap:
        kind = TYPE_UNANALYZED
    elif self.is_checked_scope():
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(node.line, kind)

</t>
<t tx="ekr.20230831011820.1636">def type(self, t: Type | None) -&gt; None:
    t = get_proper_type(t)

    if not t:
        # If an expression does not have a type, it is often due to dead code.
        # Don't count these because there can be an unanalyzed value on a line with other
        # analyzed expressions, which overwrite the TYPE_UNANALYZED.
        self.record_line(self.line, TYPE_UNANALYZED)
        return

    if isinstance(t, AnyType) and is_special_form_any(t):
        # TODO: What if there is an error in special form definition?
        self.record_line(self.line, TYPE_PRECISE)
        return

    if isinstance(t, AnyType):
        self.log("  !! Any type around line %d" % self.line)
        self.num_any_exprs += 1
        self.record_line(self.line, TYPE_ANY)
    elif (not self.all_nodes and is_imprecise(t)) or (self.all_nodes and is_imprecise2(t)):
        self.log("  !! Imprecise type around line %d" % self.line)
        self.num_imprecise_exprs += 1
        self.record_line(self.line, TYPE_IMPRECISE)
    else:
        self.num_precise_exprs += 1
        self.record_line(self.line, TYPE_PRECISE)

    for typ in get_proper_types(collect_all_inner_types(t)) + [t]:
        if isinstance(typ, AnyType):
            typ = get_original_any(typ)
            if is_special_form_any(typ):
                continue
            self.type_of_any_counter[typ.type_of_any] += 1
            self.num_any_types += 1
            if self.line in self.any_line_map:
                self.any_line_map[self.line].append(typ)
            else:
                self.any_line_map[self.line] = [typ]
        elif isinstance(typ, Instance):
            if typ.args:
                if any(is_complex(arg) for arg in typ.args):
                    self.num_complex_types += 1
                else:
                    self.num_generic_types += 1
            else:
                self.num_simple_types += 1
        elif isinstance(typ, FunctionLike):
            self.num_function_types += 1
        elif isinstance(typ, TupleType):
            if any(is_complex(item) for item in typ.items):
                self.num_complex_types += 1
            else:
                self.num_tuple_types += 1
        elif isinstance(typ, TypeVarType):
            self.num_typevar_types += 1

</t>
<t tx="ekr.20230831011820.1637">def log(self, string: str) -&gt; None:
    self.output.append(string)

</t>
<t tx="ekr.20230831011820.1638">def record_line(self, line: int, precision: int) -&gt; None:
    self.line_map[line] = max(precision, self.line_map.get(line, TYPE_EMPTY))


</t>
<t tx="ekr.20230831011820.1639">def dump_type_stats(
    tree: MypyFile,
    path: str,
    modules: dict[str, MypyFile],
    inferred: bool = False,
    typemap: dict[Expression, Type] | None = None,
) -&gt; None:
    if is_special_module(path):
        return
    print(path)
    visitor = StatisticsVisitor(inferred, filename=tree.fullname, modules=modules, typemap=typemap)
    tree.accept(visitor)
    for line in visitor.output:
        print(line)
    print("  ** precision **")
    print("  precise  ", visitor.num_precise_exprs)
    print("  imprecise", visitor.num_imprecise_exprs)
    print("  any      ", visitor.num_any_exprs)
    print("  ** kinds **")
    print("  simple   ", visitor.num_simple_types)
    print("  generic  ", visitor.num_generic_types)
    print("  function ", visitor.num_function_types)
    print("  tuple    ", visitor.num_tuple_types)
    print("  TypeVar  ", visitor.num_typevar_types)
    print("  complex  ", visitor.num_complex_types)
    print("  any      ", visitor.num_any_types)


</t>
<t tx="ekr.20230831011820.164">    def invalid_signature_for_special_method(
        self, func_type: Type, context: Context, method_name: str
    ) -&gt; None:
        self.fail(
            f'Invalid signature {format_type(func_type, self.options)} for "{method_name}"',
            context,
        )

</t>
<t tx="ekr.20230831011820.1640">def is_special_module(path: str) -&gt; bool:
    return os.path.basename(path) in ("abc.pyi", "typing.pyi", "builtins.pyi")


</t>
<t tx="ekr.20230831011820.1641">def is_imprecise(t: Type) -&gt; bool:
    return t.accept(HasAnyQuery())


</t>
<t tx="ekr.20230831011820.1642">class HasAnyQuery(TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20230831011820.1643">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20230831011820.1644">def visit_any(self, t: AnyType) -&gt; bool:
    return not is_special_form_any(t)


</t>
<t tx="ekr.20230831011820.1645">def is_imprecise2(t: Type) -&gt; bool:
    return t.accept(HasAnyQuery2())


</t>
<t tx="ekr.20230831011820.1646">class HasAnyQuery2(HasAnyQuery):
    @others
</t>
<t tx="ekr.20230831011820.1647">def visit_callable_type(self, t: CallableType) -&gt; bool:
    # We don't want to flag references to functions with some Any
    # argument types (etc.) since they generally don't mean trouble.
    return False


</t>
<t tx="ekr.20230831011820.1648">def is_generic(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, Instance) and bool(t.args)


</t>
<t tx="ekr.20230831011820.1649">def is_complex(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return is_generic(t) or isinstance(t, (FunctionLike, TupleType, TypeVarType))


</t>
<t tx="ekr.20230831011820.165">    def reveal_type(self, typ: Type, context: Context) -&gt; None:
        visitor = TypeStrVisitor(options=self.options)
        self.note(f'Revealed type is "{typ.accept(visitor)}"', context)

</t>
<t tx="ekr.20230831011820.1650">def ensure_dir_exists(dir: str) -&gt; None:
    if not os.path.exists(dir):
        os.makedirs(dir)


</t>
<t tx="ekr.20230831011820.1651">def is_special_form_any(t: AnyType) -&gt; bool:
    return get_original_any(t).type_of_any == TypeOfAny.special_form


</t>
<t tx="ekr.20230831011820.1652">def get_original_any(t: AnyType) -&gt; AnyType:
    if t.type_of_any == TypeOfAny.from_another_any:
        assert t.source_any
        assert t.source_any.type_of_any != TypeOfAny.from_another_any
        t = t.source_any
    return t
</t>
<t tx="ekr.20230831011820.1653">@path mypy
"""Conversion of parse tree nodes to strings."""
&lt;&lt; strconv.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1654">
from __future__ import annotations

import os
import re
from typing import TYPE_CHECKING, Any, Sequence

import mypy.nodes
from mypy.options import Options
from mypy.util import IdMapper, short_type
from mypy.visitor import NodeVisitor

if TYPE_CHECKING:
    import mypy.patterns
    import mypy.types


</t>
<t tx="ekr.20230831011820.1655">class StrConv(NodeVisitor[str]):
    """Visitor for converting a node to a human-readable string.

    For example, an MypyFile node from program '1' is converted into
    something like this:

      MypyFile:1(
        fnam
        ExpressionStmt:1(
          IntExpr(1)))
    """

    @others
</t>
<t tx="ekr.20230831011820.1656">__slots__ = ["options", "show_ids", "id_mapper"]

def __init__(self, *, show_ids: bool = False, options: Options) -&gt; None:
    self.options = options
    self.show_ids = show_ids
    self.id_mapper: IdMapper | None = None
    if show_ids:
        self.id_mapper = IdMapper()

</t>
<t tx="ekr.20230831011820.1657">def stringify_type(self, t: mypy.types.Type) -&gt; str:
    import mypy.types

    return t.accept(mypy.types.TypeStrVisitor(id_mapper=self.id_mapper, options=self.options))

</t>
<t tx="ekr.20230831011820.1658">def get_id(self, o: object) -&gt; int | None:
    if self.id_mapper:
        return self.id_mapper.id(o)
    return None

</t>
<t tx="ekr.20230831011820.1659">def format_id(self, o: object) -&gt; str:
    if self.id_mapper:
        return f"&lt;{self.get_id(o)}&gt;"
    else:
        return ""

</t>
<t tx="ekr.20230831011820.166">    def reveal_locals(self, type_map: dict[str, Type | None], context: Context) -&gt; None:
        # To ensure that the output is predictable on Python &lt; 3.6,
        # use an ordered dictionary sorted by variable name
        sorted_locals = dict(sorted(type_map.items(), key=lambda t: t[0]))
        if sorted_locals:
            self.note("Revealed local types are:", context)
            for k, v in sorted_locals.items():
                visitor = TypeStrVisitor(options=self.options)
                self.note(f"    {k}: {v.accept(visitor) if v is not None else None}", context)
        else:
            self.note("There are no locals to reveal", context)

</t>
<t tx="ekr.20230831011820.1660">def dump(self, nodes: Sequence[object], obj: mypy.nodes.Context) -&gt; str:
    """Convert a list of items to a multiline pretty-printed string.

    The tag is produced from the type name of obj and its line
    number. See mypy.util.dump_tagged for a description of the nodes
    argument.
    """
    tag = short_type(obj) + ":" + str(obj.line)
    if self.show_ids:
        assert self.id_mapper is not None
        tag += f"&lt;{self.get_id(obj)}&gt;"
    return dump_tagged(nodes, tag, self)

</t>
<t tx="ekr.20230831011820.1661">def func_helper(self, o: mypy.nodes.FuncItem) -&gt; list[object]:
    """Return a list in a format suitable for dump() that represents the
    arguments and the body of a function. The caller can then decorate the
    array with information specific to methods, global functions or
    anonymous functions.
    """
    args: list[mypy.nodes.Var | tuple[str, list[mypy.nodes.Node]]] = []
    extra: list[tuple[str, list[mypy.nodes.Var]]] = []
    for arg in o.arguments:
        kind: mypy.nodes.ArgKind = arg.kind
        if kind.is_required():
            args.append(arg.variable)
        elif kind.is_optional():
            assert arg.initializer is not None
            args.append(("default", [arg.variable, arg.initializer]))
        elif kind == mypy.nodes.ARG_STAR:
            extra.append(("VarArg", [arg.variable]))
        elif kind == mypy.nodes.ARG_STAR2:
            extra.append(("DictVarArg", [arg.variable]))
    a: list[Any] = []
    if args:
        a.append(("Args", args))
    if o.type:
        a.append(o.type)
    if o.is_generator:
        a.append("Generator")
    a.extend(extra)
    a.append(o.body)
    return a

</t>
<t tx="ekr.20230831011820.1662"># Top-level structures

def visit_mypy_file(self, o: mypy.nodes.MypyFile) -&gt; str:
    # Skip implicit definitions.
    a: list[Any] = [o.defs]
    if o.is_bom:
        a.insert(0, "BOM")
    # Omit path to special file with name "main". This is used to simplify
    # test case descriptions; the file "main" is used by default in many
    # test cases.
    if o.path != "main":
        # Insert path. Normalize directory separators to / to unify test
        # case# output in all platforms.
        a.insert(0, o.path.replace(os.sep, "/"))
    if o.ignored_lines:
        a.append("IgnoredLines(%s)" % ", ".join(str(line) for line in sorted(o.ignored_lines)))
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1663">def visit_import(self, o: mypy.nodes.Import) -&gt; str:
    a = []
    for id, as_id in o.ids:
        if as_id is not None:
            a.append(f"{id} : {as_id}")
        else:
            a.append(id)
    return f"Import:{o.line}({', '.join(a)})"

</t>
<t tx="ekr.20230831011820.1664">def visit_import_from(self, o: mypy.nodes.ImportFrom) -&gt; str:
    a = []
    for name, as_name in o.names:
        if as_name is not None:
            a.append(f"{name} : {as_name}")
        else:
            a.append(name)
    return f"ImportFrom:{o.line}({'.' * o.relative + o.id}, [{', '.join(a)}])"

</t>
<t tx="ekr.20230831011820.1665">def visit_import_all(self, o: mypy.nodes.ImportAll) -&gt; str:
    return f"ImportAll:{o.line}({'.' * o.relative + o.id})"

</t>
<t tx="ekr.20230831011820.1666"># Definitions

def visit_func_def(self, o: mypy.nodes.FuncDef) -&gt; str:
    a = self.func_helper(o)
    a.insert(0, o.name)
    arg_kinds = {arg.kind for arg in o.arguments}
    if len(arg_kinds &amp; {mypy.nodes.ARG_NAMED, mypy.nodes.ARG_NAMED_OPT}) &gt; 0:
        a.insert(1, f"MaxPos({o.max_pos})")
    if o.abstract_status in (mypy.nodes.IS_ABSTRACT, mypy.nodes.IMPLICITLY_ABSTRACT):
        a.insert(-1, "Abstract")
    if o.is_static:
        a.insert(-1, "Static")
    if o.is_class:
        a.insert(-1, "Class")
    if o.is_property:
        a.insert(-1, "Property")
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1667">def visit_overloaded_func_def(self, o: mypy.nodes.OverloadedFuncDef) -&gt; str:
    a: Any = o.items.copy()
    if o.type:
        a.insert(0, o.type)
    if o.impl:
        a.insert(0, o.impl)
    if o.is_static:
        a.insert(-1, "Static")
    if o.is_class:
        a.insert(-1, "Class")
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1668">def visit_class_def(self, o: mypy.nodes.ClassDef) -&gt; str:
    a = [o.name, o.defs.body]
    # Display base types unless they are implicitly just builtins.object
    # (in this case base_type_exprs is empty).
    if o.base_type_exprs:
        if o.info and o.info.bases:
            if len(o.info.bases) != 1 or o.info.bases[0].type.fullname != "builtins.object":
                a.insert(1, ("BaseType", o.info.bases))
        else:
            a.insert(1, ("BaseTypeExpr", o.base_type_exprs))
    if o.type_vars:
        a.insert(1, ("TypeVars", o.type_vars))
    if o.metaclass:
        a.insert(1, f"Metaclass({o.metaclass.accept(self)})")
    if o.decorators:
        a.insert(1, ("Decorators", o.decorators))
    if o.info and o.info._promote:
        a.insert(1, f"Promote([{','.join(self.stringify_type(p) for p in o.info._promote)}])")
    if o.info and o.info.tuple_type:
        a.insert(1, ("TupleType", [o.info.tuple_type]))
    if o.info and o.info.fallback_to_any:
        a.insert(1, "FallbackToAny")
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1669">def visit_var(self, o: mypy.nodes.Var) -&gt; str:
    lst = ""
    # Add :nil line number tag if no line number is specified to remain
    # compatible with old test case descriptions that assume this.
    if o.line &lt; 0:
        lst = ":nil"
    return "Var" + lst + "(" + o.name + ")"

</t>
<t tx="ekr.20230831011820.167">    def unsupported_type_type(self, item: Type, context: Context) -&gt; None:
        self.fail(
            f'Cannot instantiate type "Type[{format_type_bare(item, self.options)}]"', context
        )

</t>
<t tx="ekr.20230831011820.1670">def visit_global_decl(self, o: mypy.nodes.GlobalDecl) -&gt; str:
    return self.dump([o.names], o)

</t>
<t tx="ekr.20230831011820.1671">def visit_nonlocal_decl(self, o: mypy.nodes.NonlocalDecl) -&gt; str:
    return self.dump([o.names], o)

</t>
<t tx="ekr.20230831011820.1672">def visit_decorator(self, o: mypy.nodes.Decorator) -&gt; str:
    return self.dump([o.var, o.decorators, o.func], o)

</t>
<t tx="ekr.20230831011820.1673"># Statements

def visit_block(self, o: mypy.nodes.Block) -&gt; str:
    return self.dump(o.body, o)

</t>
<t tx="ekr.20230831011820.1674">def visit_expression_stmt(self, o: mypy.nodes.ExpressionStmt) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.1675">def visit_assignment_stmt(self, o: mypy.nodes.AssignmentStmt) -&gt; str:
    a: list[Any] = []
    if len(o.lvalues) &gt; 1:
        a = [("Lvalues", o.lvalues)]
    else:
        a = [o.lvalues[0]]
    a.append(o.rvalue)
    if o.type:
        a.append(o.type)
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1676">def visit_operator_assignment_stmt(self, o: mypy.nodes.OperatorAssignmentStmt) -&gt; str:
    return self.dump([o.op, o.lvalue, o.rvalue], o)

</t>
<t tx="ekr.20230831011820.1677">def visit_while_stmt(self, o: mypy.nodes.WhileStmt) -&gt; str:
    a: list[Any] = [o.expr, o.body]
    if o.else_body:
        a.append(("Else", o.else_body.body))
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1678">def visit_for_stmt(self, o: mypy.nodes.ForStmt) -&gt; str:
    a: list[Any] = []
    if o.is_async:
        a.append(("Async", ""))
    a.append(o.index)
    if o.index_type:
        a.append(o.index_type)
    a.extend([o.expr, o.body])
    if o.else_body:
        a.append(("Else", o.else_body.body))
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1679">def visit_return_stmt(self, o: mypy.nodes.ReturnStmt) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.168">    def redundant_cast(self, typ: Type, context: Context) -&gt; None:
        self.fail(
            f"Redundant cast to {format_type(typ, self.options)}",
            context,
            code=codes.REDUNDANT_CAST,
        )

</t>
<t tx="ekr.20230831011820.1680">def visit_if_stmt(self, o: mypy.nodes.IfStmt) -&gt; str:
    a: list[Any] = []
    for i in range(len(o.expr)):
        a.append(("If", [o.expr[i]]))
        a.append(("Then", o.body[i].body))

    if not o.else_body:
        return self.dump(a, o)
    else:
        return self.dump([a, ("Else", o.else_body.body)], o)

</t>
<t tx="ekr.20230831011820.1681">def visit_break_stmt(self, o: mypy.nodes.BreakStmt) -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20230831011820.1682">def visit_continue_stmt(self, o: mypy.nodes.ContinueStmt) -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20230831011820.1683">def visit_pass_stmt(self, o: mypy.nodes.PassStmt) -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20230831011820.1684">def visit_raise_stmt(self, o: mypy.nodes.RaiseStmt) -&gt; str:
    return self.dump([o.expr, o.from_expr], o)

</t>
<t tx="ekr.20230831011820.1685">def visit_assert_stmt(self, o: mypy.nodes.AssertStmt) -&gt; str:
    if o.msg is not None:
        return self.dump([o.expr, o.msg], o)
    else:
        return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.1686">def visit_await_expr(self, o: mypy.nodes.AwaitExpr) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.1687">def visit_del_stmt(self, o: mypy.nodes.DelStmt) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.1688">def visit_try_stmt(self, o: mypy.nodes.TryStmt) -&gt; str:
    a: list[Any] = [o.body]
    if o.is_star:
        a.append("*")

    for i in range(len(o.vars)):
        a.append(o.types[i])
        if o.vars[i]:
            a.append(o.vars[i])
        a.append(o.handlers[i])

    if o.else_body:
        a.append(("Else", o.else_body.body))
    if o.finally_body:
        a.append(("Finally", o.finally_body.body))

    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1689">def visit_with_stmt(self, o: mypy.nodes.WithStmt) -&gt; str:
    a: list[Any] = []
    if o.is_async:
        a.append(("Async", ""))
    for i in range(len(o.expr)):
        a.append(("Expr", [o.expr[i]]))
        if o.target[i]:
            a.append(("Target", [o.target[i]]))
    if o.unanalyzed_type:
        a.append(o.unanalyzed_type)
    return self.dump(a + [o.body], o)

</t>
<t tx="ekr.20230831011820.169">    def assert_type_fail(self, source_type: Type, target_type: Type, context: Context) -&gt; None:
        (source, target) = format_type_distinctly(source_type, target_type, options=self.options)
        self.fail(f"Expression is of type {source}, not {target}", context, code=codes.ASSERT_TYPE)

</t>
<t tx="ekr.20230831011820.1690">def visit_match_stmt(self, o: mypy.nodes.MatchStmt) -&gt; str:
    a: list[Any] = [o.subject]
    for i in range(len(o.patterns)):
        a.append(("Pattern", [o.patterns[i]]))
        if o.guards[i] is not None:
            a.append(("Guard", [o.guards[i]]))
        a.append(("Body", o.bodies[i].body))
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1691"># Expressions

# Simple expressions

def visit_int_expr(self, o: mypy.nodes.IntExpr) -&gt; str:
    return f"IntExpr({o.value})"

</t>
<t tx="ekr.20230831011820.1692">def visit_str_expr(self, o: mypy.nodes.StrExpr) -&gt; str:
    return f"StrExpr({self.str_repr(o.value)})"

</t>
<t tx="ekr.20230831011820.1693">def visit_bytes_expr(self, o: mypy.nodes.BytesExpr) -&gt; str:
    return f"BytesExpr({self.str_repr(o.value)})"

</t>
<t tx="ekr.20230831011820.1694">def str_repr(self, s: str) -&gt; str:
    s = re.sub(r"\\u[0-9a-fA-F]{4}", lambda m: "\\" + m.group(0), s)
    return re.sub("[^\\x20-\\x7e]", lambda m: r"\u%.4x" % ord(m.group(0)), s)

</t>
<t tx="ekr.20230831011820.1695">def visit_float_expr(self, o: mypy.nodes.FloatExpr) -&gt; str:
    return f"FloatExpr({o.value})"

</t>
<t tx="ekr.20230831011820.1696">def visit_complex_expr(self, o: mypy.nodes.ComplexExpr) -&gt; str:
    return f"ComplexExpr({o.value})"

</t>
<t tx="ekr.20230831011820.1697">def visit_ellipsis(self, o: mypy.nodes.EllipsisExpr) -&gt; str:
    return "Ellipsis"

</t>
<t tx="ekr.20230831011820.1698">def visit_star_expr(self, o: mypy.nodes.StarExpr) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.1699">def visit_name_expr(self, o: mypy.nodes.NameExpr) -&gt; str:
    pretty = self.pretty_name(
        o.name, o.kind, o.fullname, o.is_inferred_def or o.is_special_form, o.node
    )
    if isinstance(o.node, mypy.nodes.Var) and o.node.is_final:
        pretty += f" = {o.node.final_value}"
    return short_type(o) + "(" + pretty + ")"

</t>
<t tx="ekr.20230831011820.17">def narrow_declared_type(declared: Type, narrowed: Type) -&gt; Type:
    """Return the declared type narrowed down to another type."""
    # TODO: check infinite recursion for aliases here.
    if isinstance(narrowed, TypeGuardedType):  # type: ignore[misc]
        # A type guard forces the new type even if it doesn't overlap the old.
        return narrowed.type_guard

    original_declared = declared
    original_narrowed = narrowed
    declared = get_proper_type(declared)
    narrowed = get_proper_type(narrowed)

    if declared == narrowed:
        return original_declared
    if isinstance(declared, UnionType):
        return make_simplified_union(
            [
                narrow_declared_type(x, narrowed)
                for x in declared.relevant_items()
                # This (ugly) special-casing is needed to support checking
                # branches like this:
                # x: Union[float, complex]
                # if isinstance(x, int):
                #     ...
                if (
                    is_overlapping_types(x, narrowed, ignore_promotions=True)
                    or is_subtype(narrowed, x, ignore_promotions=False)
                )
            ]
        )
    if is_enum_overlapping_union(declared, narrowed):
        return original_narrowed
    elif not is_overlapping_types(declared, narrowed, prohibit_none_typevar_overlap=True):
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()
    elif isinstance(narrowed, UnionType):
        return make_simplified_union(
            [narrow_declared_type(declared, x) for x in narrowed.relevant_items()]
        )
    elif isinstance(narrowed, AnyType):
        return original_narrowed
    elif isinstance(narrowed, TypeVarType) and is_subtype(narrowed.upper_bound, declared):
        return narrowed
    elif isinstance(declared, TypeType) and isinstance(narrowed, TypeType):
        return TypeType.make_normalized(narrow_declared_type(declared.item, narrowed.item))
    elif (
        isinstance(declared, TypeType)
        and isinstance(narrowed, Instance)
        and narrowed.type.is_metaclass()
    ):
        # We'd need intersection types, so give up.
        return original_declared
    elif isinstance(declared, Instance):
        if declared.type.alt_promote:
            # Special case: low-level integer type can't be narrowed
            return original_declared
        if (
            isinstance(narrowed, Instance)
            and narrowed.type.alt_promote
            and narrowed.type.alt_promote.type is declared.type
        ):
            # Special case: 'int' can't be narrowed down to a native int type such as
            # i64, since they have different runtime representations.
            return original_declared
        return meet_types(original_declared, original_narrowed)
    elif isinstance(declared, (TupleType, TypeType, LiteralType)):
        return meet_types(original_declared, original_narrowed)
    elif isinstance(declared, TypedDictType) and isinstance(narrowed, Instance):
        # Special case useful for selecting TypedDicts from unions using isinstance(x, dict).
        if narrowed.type.fullname == "builtins.dict" and all(
            isinstance(t, AnyType) for t in get_proper_types(narrowed.args)
        ):
            return original_declared
        return meet_types(original_declared, original_narrowed)
    return original_narrowed


</t>
<t tx="ekr.20230831011820.170">    def unimported_type_becomes_any(self, prefix: str, typ: Type, ctx: Context) -&gt; None:
        self.fail(
            f"{prefix} becomes {format_type(typ, self.options)} due to an unfollowed import",
            ctx,
            code=codes.NO_ANY_UNIMPORTED,
        )

</t>
<t tx="ekr.20230831011820.1700">def pretty_name(
    self,
    name: str,
    kind: int | None,
    fullname: str | None,
    is_inferred_def: bool,
    target_node: mypy.nodes.Node | None = None,
) -&gt; str:
    n = name
    if is_inferred_def:
        n += "*"
    if target_node:
        id = self.format_id(target_node)
    else:
        id = ""
    if isinstance(target_node, mypy.nodes.MypyFile) and name == fullname:
        n += id
    elif kind == mypy.nodes.GDEF or (fullname != name and fullname):
        # Append fully qualified name for global references.
        n += f" [{fullname}{id}]"
    elif kind == mypy.nodes.LDEF:
        # Add tag to signify a local reference.
        n += f" [l{id}]"
    elif kind == mypy.nodes.MDEF:
        # Add tag to signify a member reference.
        n += f" [m{id}]"
    else:
        n += id
    return n

</t>
<t tx="ekr.20230831011820.1701">def visit_member_expr(self, o: mypy.nodes.MemberExpr) -&gt; str:
    pretty = self.pretty_name(o.name, o.kind, o.fullname, o.is_inferred_def, o.node)
    return self.dump([o.expr, pretty], o)

</t>
<t tx="ekr.20230831011820.1702">def visit_yield_expr(self, o: mypy.nodes.YieldExpr) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.1703">def visit_yield_from_expr(self, o: mypy.nodes.YieldFromExpr) -&gt; str:
    if o.expr:
        return self.dump([o.expr.accept(self)], o)
    else:
        return self.dump([], o)

</t>
<t tx="ekr.20230831011820.1704">def visit_call_expr(self, o: mypy.nodes.CallExpr) -&gt; str:
    if o.analyzed:
        return o.analyzed.accept(self)
    args: list[mypy.nodes.Expression] = []
    extra: list[str | tuple[str, list[Any]]] = []
    for i, kind in enumerate(o.arg_kinds):
        if kind in [mypy.nodes.ARG_POS, mypy.nodes.ARG_STAR]:
            args.append(o.args[i])
            if kind == mypy.nodes.ARG_STAR:
                extra.append("VarArg")
        elif kind == mypy.nodes.ARG_NAMED:
            extra.append(("KwArgs", [o.arg_names[i], o.args[i]]))
        elif kind == mypy.nodes.ARG_STAR2:
            extra.append(("DictVarArg", [o.args[i]]))
        else:
            raise RuntimeError(f"unknown kind {kind}")
    a: list[Any] = [o.callee, ("Args", args)]
    return self.dump(a + extra, o)

</t>
<t tx="ekr.20230831011820.1705">def visit_op_expr(self, o: mypy.nodes.OpExpr) -&gt; str:
    if o.analyzed:
        return o.analyzed.accept(self)
    return self.dump([o.op, o.left, o.right], o)

</t>
<t tx="ekr.20230831011820.1706">def visit_comparison_expr(self, o: mypy.nodes.ComparisonExpr) -&gt; str:
    return self.dump([o.operators, o.operands], o)

</t>
<t tx="ekr.20230831011820.1707">def visit_cast_expr(self, o: mypy.nodes.CastExpr) -&gt; str:
    return self.dump([o.expr, o.type], o)

</t>
<t tx="ekr.20230831011820.1708">def visit_assert_type_expr(self, o: mypy.nodes.AssertTypeExpr) -&gt; str:
    return self.dump([o.expr, o.type], o)

</t>
<t tx="ekr.20230831011820.1709">def visit_reveal_expr(self, o: mypy.nodes.RevealExpr) -&gt; str:
    if o.kind == mypy.nodes.REVEAL_TYPE:
        return self.dump([o.expr], o)
    else:
        # REVEAL_LOCALS
        return self.dump([o.local_nodes], o)

</t>
<t tx="ekr.20230831011820.171">    def need_annotation_for_var(
        self, node: SymbolNode, context: Context, python_version: tuple[int, int] | None = None
    ) -&gt; None:
        hint = ""
        pep604_supported = not python_version or python_version &gt;= (3, 10)
        # type to recommend the user adds
        recommended_type = None
        # Only gives hint if it's a variable declaration and the partial type is a builtin type
        if python_version and isinstance(node, Var) and isinstance(node.type, PartialType):
            type_dec = "&lt;type&gt;"
            if not node.type.type:
                # partial None
                if pep604_supported:
                    recommended_type = f"{type_dec} | None"
                else:
                    recommended_type = f"Optional[{type_dec}]"
            elif node.type.type.fullname in reverse_builtin_aliases:
                # partial types other than partial None
                alias = reverse_builtin_aliases[node.type.type.fullname]
                alias = alias.split(".")[-1]
                if alias == "Dict":
                    type_dec = f"{type_dec}, {type_dec}"
                recommended_type = f"{alias}[{type_dec}]"
        if recommended_type is not None:
            hint = f' (hint: "{node.name}: {recommended_type} = ...")'

        self.fail(
            f'Need type annotation for "{unmangle(node.name)}"{hint}',
            context,
            code=codes.VAR_ANNOTATED,
        )

</t>
<t tx="ekr.20230831011820.1710">def visit_assignment_expr(self, o: mypy.nodes.AssignmentExpr) -&gt; str:
    return self.dump([o.target, o.value], o)

</t>
<t tx="ekr.20230831011820.1711">def visit_unary_expr(self, o: mypy.nodes.UnaryExpr) -&gt; str:
    return self.dump([o.op, o.expr], o)

</t>
<t tx="ekr.20230831011820.1712">def visit_list_expr(self, o: mypy.nodes.ListExpr) -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20230831011820.1713">def visit_dict_expr(self, o: mypy.nodes.DictExpr) -&gt; str:
    return self.dump([[k, v] for k, v in o.items], o)

</t>
<t tx="ekr.20230831011820.1714">def visit_set_expr(self, o: mypy.nodes.SetExpr) -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20230831011820.1715">def visit_tuple_expr(self, o: mypy.nodes.TupleExpr) -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20230831011820.1716">def visit_index_expr(self, o: mypy.nodes.IndexExpr) -&gt; str:
    if o.analyzed:
        return o.analyzed.accept(self)
    return self.dump([o.base, o.index], o)

</t>
<t tx="ekr.20230831011820.1717">def visit_super_expr(self, o: mypy.nodes.SuperExpr) -&gt; str:
    return self.dump([o.name, o.call], o)

</t>
<t tx="ekr.20230831011820.1718">def visit_type_application(self, o: mypy.nodes.TypeApplication) -&gt; str:
    return self.dump([o.expr, ("Types", o.types)], o)

</t>
<t tx="ekr.20230831011820.1719">def visit_type_var_expr(self, o: mypy.nodes.TypeVarExpr) -&gt; str:
    import mypy.types

    a: list[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ["Variance(COVARIANT)"]
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ["Variance(CONTRAVARIANT)"]
    if o.values:
        a += [("Values", o.values)]
    if not mypy.types.is_named_instance(o.upper_bound, "builtins.object"):
        a += [f"UpperBound({self.stringify_type(o.upper_bound)})"]
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.172">    def explicit_any(self, ctx: Context) -&gt; None:
        self.fail('Explicit "Any" is not allowed', ctx)

</t>
<t tx="ekr.20230831011820.1720">def visit_paramspec_expr(self, o: mypy.nodes.ParamSpecExpr) -&gt; str:
    import mypy.types

    a: list[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ["Variance(COVARIANT)"]
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ["Variance(CONTRAVARIANT)"]
    if not mypy.types.is_named_instance(o.upper_bound, "builtins.object"):
        a += [f"UpperBound({self.stringify_type(o.upper_bound)})"]
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1721">def visit_type_var_tuple_expr(self, o: mypy.nodes.TypeVarTupleExpr) -&gt; str:
    import mypy.types

    a: list[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ["Variance(COVARIANT)"]
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ["Variance(CONTRAVARIANT)"]
    if not mypy.types.is_named_instance(o.upper_bound, "builtins.object"):
        a += [f"UpperBound({self.stringify_type(o.upper_bound)})"]
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1722">def visit_type_alias_expr(self, o: mypy.nodes.TypeAliasExpr) -&gt; str:
    return f"TypeAliasExpr({self.stringify_type(o.node.target)})"

</t>
<t tx="ekr.20230831011820.1723">def visit_namedtuple_expr(self, o: mypy.nodes.NamedTupleExpr) -&gt; str:
    return f"NamedTupleExpr:{o.line}({o.info.name}, {self.stringify_type(o.info.tuple_type) if o.info.tuple_type is not None else None})"

</t>
<t tx="ekr.20230831011820.1724">def visit_enum_call_expr(self, o: mypy.nodes.EnumCallExpr) -&gt; str:
    return f"EnumCallExpr:{o.line}({o.info.name}, {o.items})"

</t>
<t tx="ekr.20230831011820.1725">def visit_typeddict_expr(self, o: mypy.nodes.TypedDictExpr) -&gt; str:
    return f"TypedDictExpr:{o.line}({o.info.name})"

</t>
<t tx="ekr.20230831011820.1726">def visit__promote_expr(self, o: mypy.nodes.PromoteExpr) -&gt; str:
    return f"PromoteExpr:{o.line}({self.stringify_type(o.type)})"

</t>
<t tx="ekr.20230831011820.1727">def visit_newtype_expr(self, o: mypy.nodes.NewTypeExpr) -&gt; str:
    return f"NewTypeExpr:{o.line}({o.name}, {self.dump([o.old_type], o)})"

</t>
<t tx="ekr.20230831011820.1728">def visit_lambda_expr(self, o: mypy.nodes.LambdaExpr) -&gt; str:
    a = self.func_helper(o)
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1729">def visit_generator_expr(self, o: mypy.nodes.GeneratorExpr) -&gt; str:
    condlists = o.condlists if any(o.condlists) else None
    return self.dump([o.left_expr, o.indices, o.sequences, condlists], o)

</t>
<t tx="ekr.20230831011820.173">    def unsupported_target_for_star_typeddict(self, typ: Type, ctx: Context) -&gt; None:
        self.fail(
            "Unsupported type {} for ** expansion in TypedDict".format(
                format_type(typ, self.options)
            ),
            ctx,
            code=codes.TYPEDDICT_ITEM,
        )

</t>
<t tx="ekr.20230831011820.1730">def visit_list_comprehension(self, o: mypy.nodes.ListComprehension) -&gt; str:
    return self.dump([o.generator], o)

</t>
<t tx="ekr.20230831011820.1731">def visit_set_comprehension(self, o: mypy.nodes.SetComprehension) -&gt; str:
    return self.dump([o.generator], o)

</t>
<t tx="ekr.20230831011820.1732">def visit_dictionary_comprehension(self, o: mypy.nodes.DictionaryComprehension) -&gt; str:
    condlists = o.condlists if any(o.condlists) else None
    return self.dump([o.key, o.value, o.indices, o.sequences, condlists], o)

</t>
<t tx="ekr.20230831011820.1733">def visit_conditional_expr(self, o: mypy.nodes.ConditionalExpr) -&gt; str:
    return self.dump([("Condition", [o.cond]), o.if_expr, o.else_expr], o)

</t>
<t tx="ekr.20230831011820.1734">def visit_slice_expr(self, o: mypy.nodes.SliceExpr) -&gt; str:
    a: list[Any] = [o.begin_index, o.end_index, o.stride]
    if not a[0]:
        a[0] = "&lt;empty&gt;"
    if not a[1]:
        a[1] = "&lt;empty&gt;"
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1735">def visit_temp_node(self, o: mypy.nodes.TempNode) -&gt; str:
    return self.dump([o.type], o)

</t>
<t tx="ekr.20230831011820.1736">def visit_as_pattern(self, o: mypy.patterns.AsPattern) -&gt; str:
    return self.dump([o.pattern, o.name], o)

</t>
<t tx="ekr.20230831011820.1737">def visit_or_pattern(self, o: mypy.patterns.OrPattern) -&gt; str:
    return self.dump(o.patterns, o)

</t>
<t tx="ekr.20230831011820.1738">def visit_value_pattern(self, o: mypy.patterns.ValuePattern) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20230831011820.1739">def visit_singleton_pattern(self, o: mypy.patterns.SingletonPattern) -&gt; str:
    return self.dump([o.value], o)

</t>
<t tx="ekr.20230831011820.174">    def non_required_keys_absent_with_star(self, keys: list[str], ctx: Context) -&gt; None:
        self.fail(
            "Non-required {} not explicitly found in any ** item".format(
                format_key_list(keys, short=True)
            ),
            ctx,
            code=codes.TYPEDDICT_ITEM,
        )

</t>
<t tx="ekr.20230831011820.1740">def visit_sequence_pattern(self, o: mypy.patterns.SequencePattern) -&gt; str:
    return self.dump(o.patterns, o)

</t>
<t tx="ekr.20230831011820.1741">def visit_starred_pattern(self, o: mypy.patterns.StarredPattern) -&gt; str:
    return self.dump([o.capture], o)

</t>
<t tx="ekr.20230831011820.1742">def visit_mapping_pattern(self, o: mypy.patterns.MappingPattern) -&gt; str:
    a: list[Any] = []
    for i in range(len(o.keys)):
        a.append(("Key", [o.keys[i]]))
        a.append(("Value", [o.values[i]]))
    if o.rest is not None:
        a.append(("Rest", [o.rest]))
    return self.dump(a, o)

</t>
<t tx="ekr.20230831011820.1743">def visit_class_pattern(self, o: mypy.patterns.ClassPattern) -&gt; str:
    a: list[Any] = [o.class_ref]
    if len(o.positionals) &gt; 0:
        a.append(("Positionals", o.positionals))
    for i in range(len(o.keyword_keys)):
        a.append(("Keyword", [o.keyword_keys[i], o.keyword_values[i]]))

    return self.dump(a, o)


</t>
<t tx="ekr.20230831011820.1744">def dump_tagged(nodes: Sequence[object], tag: str | None, str_conv: StrConv) -&gt; str:
    """Convert an array into a pretty-printed multiline string representation.

    The format is
      tag(
        item1..
        itemN)
    Individual items are formatted like this:
     - arrays are flattened
     - pairs (str, array) are converted recursively, so that str is the tag
     - other items are converted to strings and indented
    """
    from mypy.types import Type, TypeStrVisitor

    a: list[str] = []
    if tag:
        a.append(tag + "(")
    for n in nodes:
        if isinstance(n, list):
            if n:
                a.append(dump_tagged(n, None, str_conv))
        elif isinstance(n, tuple):
            s = dump_tagged(n[1], n[0], str_conv)
            a.append(indent(s, 2))
        elif isinstance(n, mypy.nodes.Node):
            a.append(indent(n.accept(str_conv), 2))
        elif isinstance(n, Type):
            a.append(
                indent(n.accept(TypeStrVisitor(str_conv.id_mapper, options=str_conv.options)), 2)
            )
        elif n is not None:
            a.append(indent(str(n), 2))
    if tag:
        a[-1] += ")"
    return "\n".join(a)


</t>
<t tx="ekr.20230831011820.1745">def indent(s: str, n: int) -&gt; str:
    """Indent all the lines in s (separated by newlines) by n spaces."""
    s = " " * n + s
    s = s.replace("\n", "\n" + " " * n)
    return s
</t>
<t tx="ekr.20230831011820.1746">@path mypy
"""Parsing/inferring signatures from documentation.

This module provides several functions to generate better stubs using
docstrings and Sphinx docs (.rst files).
"""

&lt;&lt; stubdoc.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1748">from __future__ import annotations

import contextlib
import io
import re
import tokenize
from typing import Any, Final, MutableMapping, MutableSequence, NamedTuple, Sequence, Tuple
from typing_extensions import TypeAlias as _TypeAlias

# Type alias for signatures strings in format ('func_name', '(arg, opt_arg=False)').
Sig: _TypeAlias = Tuple[str, str]


_TYPE_RE: Final = re.compile(r"^[a-zA-Z_][\w\[\], ]*(\.[a-zA-Z_][\w\[\], ]*)*$")
_ARG_NAME_RE: Final = re.compile(r"\**[A-Za-z_][A-Za-z0-9_]*$")


</t>
<t tx="ekr.20230831011820.1749">def is_valid_type(s: str) -&gt; bool:
    """Try to determine whether a string might be a valid type annotation."""
    if s in ("True", "False", "retval"):
        return False
    if "," in s and "[" not in s:
        return False
    return _TYPE_RE.match(s) is not None


</t>
<t tx="ekr.20230831011820.175">    def unexpected_typeddict_keys(
        self,
        typ: TypedDictType,
        expected_keys: list[str],
        actual_keys: list[str],
        context: Context,
    ) -&gt; None:
        actual_set = set(actual_keys)
        expected_set = set(expected_keys)
        if not typ.is_anonymous():
            # Generate simpler messages for some common special cases.
            # Use list comprehension instead of set operations to preserve order.
            missing = [key for key in expected_keys if key not in actual_set]
            if missing:
                self.fail(
                    "Missing {} for TypedDict {}".format(
                        format_key_list(missing, short=True), format_type(typ, self.options)
                    ),
                    context,
                    code=codes.TYPEDDICT_ITEM,
                )
            extra = [key for key in actual_keys if key not in expected_set]
            if extra:
                self.fail(
                    "Extra {} for TypedDict {}".format(
                        format_key_list(extra, short=True), format_type(typ, self.options)
                    ),
                    context,
                    code=codes.TYPEDDICT_UNKNOWN_KEY,
                )
            if missing or extra:
                # No need to check for further errors
                return
        found = format_key_list(actual_keys, short=True)
        if not expected_keys:
            self.fail(f"Unexpected TypedDict {found}", context)
            return
        expected = format_key_list(expected_keys)
        if actual_keys and actual_set &lt; expected_set:
            found = f"only {found}"
        self.fail(f"Expected {expected} but found {found}", context, code=codes.TYPEDDICT_ITEM)

</t>
<t tx="ekr.20230831011820.1750">class ArgSig:
    """Signature info for a single argument."""

    @others
</t>
<t tx="ekr.20230831011820.1751">def __init__(self, name: str, type: str | None = None, default: bool = False):
    self.name = name
    if type and not is_valid_type(type):
        raise ValueError("Invalid type: " + type)
    self.type = type
    # Does this argument have a default value?
    self.default = default

</t>
<t tx="ekr.20230831011820.1752">def __repr__(self) -&gt; str:
    return "ArgSig(name={}, type={}, default={})".format(
        repr(self.name), repr(self.type), repr(self.default)
    )

</t>
<t tx="ekr.20230831011820.1753">def __eq__(self, other: Any) -&gt; bool:
    if isinstance(other, ArgSig):
        return (
            self.name == other.name
            and self.type == other.type
            and self.default == other.default
        )
    return False


</t>
<t tx="ekr.20230831011820.1754">class FunctionSig(NamedTuple):
    name: str
    args: list[ArgSig]
    ret_type: str


</t>
<t tx="ekr.20230831011820.1755"># States of the docstring parser.
STATE_INIT: Final = 1
STATE_FUNCTION_NAME: Final = 2
STATE_ARGUMENT_LIST: Final = 3
STATE_ARGUMENT_TYPE: Final = 4
STATE_ARGUMENT_DEFAULT: Final = 5
STATE_RETURN_VALUE: Final = 6
STATE_OPEN_BRACKET: Final = 7  # For generic types.


class DocStringParser:
    """Parse function signatures in documentation."""

    @others
</t>
<t tx="ekr.20230831011820.1756">def __init__(self, function_name: str) -&gt; None:
    # Only search for signatures of function with this name.
    self.function_name = function_name
    self.state = [STATE_INIT]
    self.accumulator = ""
    self.arg_type: str | None = None
    self.arg_name = ""
    self.arg_default: str | None = None
    self.ret_type = "Any"
    self.found = False
    self.args: list[ArgSig] = []
    # Valid signatures found so far.
    self.signatures: list[FunctionSig] = []

</t>
<t tx="ekr.20230831011820.1757">def add_token(self, token: tokenize.TokenInfo) -&gt; None:
    """Process next token from the token stream."""
    if (
        token.type == tokenize.NAME
        and token.string == self.function_name
        and self.state[-1] == STATE_INIT
    ):
        self.state.append(STATE_FUNCTION_NAME)

    elif (
        token.type == tokenize.OP
        and token.string == "("
        and self.state[-1] == STATE_FUNCTION_NAME
    ):
        self.state.pop()
        self.accumulator = ""
        self.found = True
        self.state.append(STATE_ARGUMENT_LIST)

    elif self.state[-1] == STATE_FUNCTION_NAME:
        # Reset state, function name not followed by '('.
        self.state.pop()

    elif (
        token.type == tokenize.OP
        and token.string in ("[", "(", "{")
        and self.state[-1] != STATE_INIT
    ):
        self.accumulator += token.string
        self.state.append(STATE_OPEN_BRACKET)

    elif (
        token.type == tokenize.OP
        and token.string in ("]", ")", "}")
        and self.state[-1] == STATE_OPEN_BRACKET
    ):
        self.accumulator += token.string
        self.state.pop()

    elif (
        token.type == tokenize.OP
        and token.string == ":"
        and self.state[-1] == STATE_ARGUMENT_LIST
    ):
        self.arg_name = self.accumulator
        self.accumulator = ""
        self.state.append(STATE_ARGUMENT_TYPE)

    elif (
        token.type == tokenize.OP
        and token.string == "="
        and self.state[-1] in (STATE_ARGUMENT_LIST, STATE_ARGUMENT_TYPE)
    ):
        if self.state[-1] == STATE_ARGUMENT_TYPE:
            self.arg_type = self.accumulator
            self.state.pop()
        else:
            self.arg_name = self.accumulator
        self.accumulator = ""
        self.state.append(STATE_ARGUMENT_DEFAULT)

    elif (
        token.type == tokenize.OP
        and token.string in (",", ")")
        and self.state[-1]
        in (STATE_ARGUMENT_LIST, STATE_ARGUMENT_DEFAULT, STATE_ARGUMENT_TYPE)
    ):
        if self.state[-1] == STATE_ARGUMENT_DEFAULT:
            self.arg_default = self.accumulator
            self.state.pop()
        elif self.state[-1] == STATE_ARGUMENT_TYPE:
            self.arg_type = self.accumulator
            self.state.pop()
        elif self.state[-1] == STATE_ARGUMENT_LIST:
            self.arg_name = self.accumulator
            if not (
                token.string == ")" and self.accumulator.strip() == ""
            ) and not _ARG_NAME_RE.match(self.arg_name):
                # Invalid argument name.
                self.reset()
                return

        if token.string == ")":
            self.state.pop()

        # arg_name is empty when there are no args. e.g. func()
        if self.arg_name:
            try:
                self.args.append(
                    ArgSig(
                        name=self.arg_name, type=self.arg_type, default=bool(self.arg_default)
                    )
                )
            except ValueError:
                # wrong type, use Any
                self.args.append(
                    ArgSig(name=self.arg_name, type=None, default=bool(self.arg_default))
                )
        self.arg_name = ""
        self.arg_type = None
        self.arg_default = None
        self.accumulator = ""

    elif token.type == tokenize.OP and token.string == "-&gt;" and self.state[-1] == STATE_INIT:
        self.accumulator = ""
        self.state.append(STATE_RETURN_VALUE)

    # ENDMAKER is necessary for python 3.4 and 3.5.
    elif token.type in (tokenize.NEWLINE, tokenize.ENDMARKER) and self.state[-1] in (
        STATE_INIT,
        STATE_RETURN_VALUE,
    ):
        if self.state[-1] == STATE_RETURN_VALUE:
            if not is_valid_type(self.accumulator):
                self.reset()
                return
            self.ret_type = self.accumulator
            self.accumulator = ""
            self.state.pop()

        if self.found:
            self.signatures.append(
                FunctionSig(name=self.function_name, args=self.args, ret_type=self.ret_type)
            )
            self.found = False
        self.args = []
        self.ret_type = "Any"
        # Leave state as INIT.
    else:
        self.accumulator += token.string

</t>
<t tx="ekr.20230831011820.1758">def reset(self) -&gt; None:
    self.state = [STATE_INIT]
    self.args = []
    self.found = False
    self.accumulator = ""

</t>
<t tx="ekr.20230831011820.1759">def get_signatures(self) -&gt; list[FunctionSig]:
    """Return sorted copy of the list of signatures found so far."""

    def has_arg(name: str, signature: FunctionSig) -&gt; bool:
        return any(x.name == name for x in signature.args)

    def args_kwargs(signature: FunctionSig) -&gt; bool:
        return has_arg("*args", signature) and has_arg("**kwargs", signature)

    # Move functions with (*args, **kwargs) in their signature to last place.
    return list(sorted(self.signatures, key=lambda x: 1 if args_kwargs(x) else 0))


</t>
<t tx="ekr.20230831011820.176">    def typeddict_key_must_be_string_literal(self, typ: TypedDictType, context: Context) -&gt; None:
        self.fail(
            "TypedDict key must be a string literal; expected one of {}".format(
                format_item_name_list(typ.items.keys())
            ),
            context,
            code=codes.LITERAL_REQ,
        )

</t>
<t tx="ekr.20230831011820.1760">def infer_sig_from_docstring(docstr: str | None, name: str) -&gt; list[FunctionSig] | None:
    """Convert function signature to list of TypedFunctionSig

    Look for function signatures of function in docstring. Signature is a string of
    the format &lt;function_name&gt;(&lt;signature&gt;) -&gt; &lt;return type&gt; or perhaps without
    the return type.

    Returns empty list, when no signature is found, one signature in typical case,
    multiple signatures, if docstring specifies multiple signatures for overload functions.
    Return None if the docstring is empty.

    Arguments:
        * docstr: docstring
        * name: name of function for which signatures are to be found
    """
    if not (isinstance(docstr, str) and docstr):
        return None

    state = DocStringParser(name)
    # Return all found signatures, even if there is a parse error after some are found.
    with contextlib.suppress(tokenize.TokenError):
        try:
            tokens = tokenize.tokenize(io.BytesIO(docstr.encode("utf-8")).readline)
            for token in tokens:
                state.add_token(token)
        except IndentationError:
            return None
    sigs = state.get_signatures()

    def is_unique_args(sig: FunctionSig) -&gt; bool:
        """return true if function argument names are unique"""
        return len(sig.args) == len({arg.name for arg in sig.args})

    # Return only signatures that have unique argument names. Mypy fails on non-unique arg names.
    return [sig for sig in sigs if is_unique_args(sig)]


</t>
<t tx="ekr.20230831011820.1761">def infer_arg_sig_from_anon_docstring(docstr: str) -&gt; list[ArgSig]:
    """Convert signature in form of "(self: TestClass, arg0: str='ada')" to List[TypedArgList]."""
    ret = infer_sig_from_docstring("stub" + docstr, "stub")
    if ret:
        return ret[0].args
    return []


</t>
<t tx="ekr.20230831011820.1762">def infer_ret_type_sig_from_docstring(docstr: str, name: str) -&gt; str | None:
    """Convert signature in form of "func(self: TestClass, arg0) -&gt; int" to their return type."""
    ret = infer_sig_from_docstring(docstr, name)
    if ret:
        return ret[0].ret_type
    return None


</t>
<t tx="ekr.20230831011820.1763">def infer_ret_type_sig_from_anon_docstring(docstr: str) -&gt; str | None:
    """Convert signature in form of "(self: TestClass, arg0) -&gt; int" to their return type."""
    return infer_ret_type_sig_from_docstring("stub" + docstr.strip(), "stub")


</t>
<t tx="ekr.20230831011820.1764">def parse_signature(sig: str) -&gt; tuple[str, list[str], list[str]] | None:
    """Split function signature into its name, positional an optional arguments.

    The expected format is "func_name(arg, opt_arg=False)". Return the name of function
    and lists of positional and optional argument names.
    """
    m = re.match(r"([.a-zA-Z0-9_]+)\(([^)]*)\)", sig)
    if not m:
        return None
    name = m.group(1)
    name = name.split(".")[-1]
    arg_string = m.group(2)
    if not arg_string.strip():
        # Simple case -- no arguments.
        return name, [], []

    args = [arg.strip() for arg in arg_string.split(",")]
    positional = []
    optional = []
    i = 0
    while i &lt; len(args):
        # Accept optional arguments as in both formats: x=None and [x].
        if args[i].startswith("[") or "=" in args[i]:
            break
        positional.append(args[i].rstrip("["))
        i += 1
        if args[i - 1].endswith("["):
            break
    while i &lt; len(args):
        arg = args[i]
        arg = arg.strip("[]")
        arg = arg.split("=")[0]
        optional.append(arg)
        i += 1
    return name, positional, optional


</t>
<t tx="ekr.20230831011820.1765">def build_signature(positional: Sequence[str], optional: Sequence[str]) -&gt; str:
    """Build function signature from lists of positional and optional argument names."""
    args: MutableSequence[str] = []
    args.extend(positional)
    for arg in optional:
        if arg.startswith("*"):
            args.append(arg)
        else:
            args.append(f"{arg}=...")
    sig = f"({', '.join(args)})"
    # Ad-hoc fixes.
    sig = sig.replace("(self)", "")
    return sig


</t>
<t tx="ekr.20230831011820.1766">def parse_all_signatures(lines: Sequence[str]) -&gt; tuple[list[Sig], list[Sig]]:
    """Parse all signatures in a given reST document.

    Return lists of found signatures for functions and classes.
    """
    sigs = []
    class_sigs = []
    for line in lines:
        line = line.strip()
        m = re.match(r"\.\. *(function|method|class) *:: *[a-zA-Z_]", line)
        if m:
            sig = line.split("::")[1].strip()
            parsed = parse_signature(sig)
            if parsed:
                name, fixed, optional = parsed
                if m.group(1) != "class":
                    sigs.append((name, build_signature(fixed, optional)))
                else:
                    class_sigs.append((name, build_signature(fixed, optional)))

    return sorted(sigs), sorted(class_sigs)


</t>
<t tx="ekr.20230831011820.1767">def find_unique_signatures(sigs: Sequence[Sig]) -&gt; list[Sig]:
    """Remove names with duplicate found signatures."""
    sig_map: MutableMapping[str, list[str]] = {}
    for name, sig in sigs:
        sig_map.setdefault(name, []).append(sig)

    result = []
    for name, name_sigs in sig_map.items():
        if len(set(name_sigs)) == 1:
            result.append((name, name_sigs[0]))
    return sorted(result)


</t>
<t tx="ekr.20230831011820.1768">def infer_prop_type_from_docstring(docstr: str | None) -&gt; str | None:
    """Check for Google/Numpy style docstring type annotation for a property.

    The docstring has the format "&lt;type&gt;: &lt;descriptions&gt;".
    In the type string, we allow the following characters:
    * dot: because sometimes classes are annotated using full path
    * brackets: to allow type hints like List[int]
    * comma/space: things like Tuple[int, int]
    """
    if not docstr:
        return None
    test_str = r"^([a-zA-Z0-9_, \.\[\]]*): "
    m = re.match(test_str, docstr)
    return m.group(1) if m else None
</t>
<t tx="ekr.20230831011820.1769">@path mypy
&lt;&lt; stubgen.py: docstring &gt;&gt;
&lt;&lt; stubgen.py: declarations &gt;&gt;
@others


if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.177">    def typeddict_key_not_found(
        self, typ: TypedDictType, item_name: str, context: Context, setitem: bool = False
    ) -&gt; None:
        """Handle error messages for TypedDicts that have unknown keys.

        Note, that we differentiate in between reading a value and setting a
        value.
        Setting a value on a TypedDict is an 'unknown-key' error, whereas
        reading it is the more serious/general 'item' error.
        """
        if typ.is_anonymous():
            self.fail(
                '"{}" is not a valid TypedDict key; expected one of {}'.format(
                    item_name, format_item_name_list(typ.items.keys())
                ),
                context,
            )
        else:
            err_code = codes.TYPEDDICT_UNKNOWN_KEY if setitem else codes.TYPEDDICT_ITEM
            self.fail(
                f'TypedDict {format_type(typ, self.options)} has no key "{item_name}"',
                context,
                code=err_code,
            )
            matches = best_matches(item_name, typ.items.keys(), n=3)
            if matches:
                self.note(
                    "Did you mean {}?".format(pretty_seq(matches, "or")), context, code=err_code
                )

</t>
<t tx="ekr.20230831011820.1770">#!/usr/bin/env python3
"""Generator of dynamically typed draft stubs for arbitrary modules.

The logic of this script can be split in three steps:
* parsing options and finding sources:
  - use runtime imports be default (to find also C modules)
  - or use mypy's mechanisms, if importing is prohibited
* (optionally) semantically analysing the sources using mypy (as a single set)
* emitting the stubs text:
  - for Python modules: from ASTs using StubGenerator
  - for C modules using runtime introspection and (optionally) Sphinx docs

During first and third steps some problematic files can be skipped, but any
blocking error during second step will cause the whole program to stop.

Basic usage:

  $ stubgen foo.py bar.py some_directory
  =&gt; Generate out/foo.pyi, out/bar.pyi, and stubs for some_directory (recursively).

  $ stubgen -m urllib.parse
  =&gt; Generate out/urllib/parse.pyi.

  $ stubgen -p urllib
  =&gt; Generate stubs for whole urlib package (recursively).

For C modules, you can get more precise function signatures by parsing .rst (Sphinx)
documentation for extra information. For this, use the --doc-dir option:

  $ stubgen --doc-dir &lt;DIR&gt;/Python-3.4.2/Doc/library -m curses

Note: The generated stubs should be verified manually.

TODO:
 - maybe use .rst docs also for Python modules
 - maybe export more imported names if there is no __all__ (this affects ssl.SSLError, for example)
   - a quick and dirty heuristic would be to turn this on if a module has something like
     'from x import y as _y'
 - we don't seem to always detect properties ('closed' in 'io', for example)
"""

</t>
<t tx="ekr.20230831011820.1771">from __future__ import annotations

import argparse
import glob
import keyword
import os
import os.path
import sys
import traceback
from collections import defaultdict
from typing import Final, Iterable, Mapping

import mypy.build
import mypy.mixedtraverser
import mypy.parse
import mypy.traverser
import mypy.util
from mypy.build import build
from mypy.errors import CompileError, Errors
from mypy.find_sources import InvalidSourceList, create_source_list
from mypy.modulefinder import (
    BuildSource,
    FindModuleCache,
    ModuleNotFoundReason,
    SearchPaths,
    default_lib_path,
)
from mypy.moduleinspect import ModuleInspect
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    IS_ABSTRACT,
    NOT_ABSTRACT,
    AssignmentStmt,
    Block,
    BytesExpr,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    Decorator,
    DictExpr,
    EllipsisExpr,
    Expression,
    FloatExpr,
    FuncBase,
    FuncDef,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    ListExpr,
    MemberExpr,
    MypyFile,
    NameExpr,
    OpExpr,
    OverloadedFuncDef,
    Statement,
    StrExpr,
    TempNode,
    TupleExpr,
    TypeInfo,
    UnaryExpr,
    Var,
)
from mypy.options import Options as MypyOptions
from mypy.stubdoc import Sig, find_unique_signatures, parse_all_signatures
from mypy.stubgenc import (
    DocstringSignatureGenerator,
    ExternalSignatureGenerator,
    FallbackSignatureGenerator,
    SignatureGenerator,
    generate_stub_for_c_module,
)
from mypy.stubutil import (
    CantImport,
    common_dir_prefix,
    fail_missing,
    find_module_path_and_all_py3,
    generate_guarded,
    remove_misplaced_type_comments,
    report_missing,
    walk_packages,
)
from mypy.traverser import (
    all_yield_expressions,
    has_return_statement,
    has_yield_expression,
    has_yield_from_expression,
)
from mypy.types import (
    OVERLOAD_NAMES,
    TPDICT_NAMES,
    TYPED_NAMEDTUPLE_NAMES,
    AnyType,
    CallableType,
    Instance,
    NoneType,
    TupleType,
    Type,
    TypeList,
    TypeStrVisitor,
    UnboundType,
    UnionType,
    get_proper_type,
)
from mypy.visitor import NodeVisitor

TYPING_MODULE_NAMES: Final = ("typing", "typing_extensions")

# Common ways of naming package containing vendored modules.
VENDOR_PACKAGES: Final = ["packages", "vendor", "vendored", "_vendor", "_vendored_packages"]

# Avoid some file names that are unnecessary or likely to cause trouble (\n for end of path).
BLACKLIST: Final = [
    "/six.py\n",  # Likely vendored six; too dynamic for us to handle
    "/vendored/",  # Vendored packages
    "/vendor/",  # Vendored packages
    "/_vendor/",
    "/_vendored_packages/",
]

# Special-cased names that are implicitly exported from the stub (from m import y as y).
EXTRA_EXPORTED: Final = {
    "pyasn1_modules.rfc2437.univ",
    "pyasn1_modules.rfc2459.char",
    "pyasn1_modules.rfc2459.univ",
}

# These names should be omitted from generated stubs.
IGNORED_DUNDERS: Final = {
    "__all__",
    "__author__",
    "__version__",
    "__about__",
    "__copyright__",
    "__email__",
    "__license__",
    "__summary__",
    "__title__",
    "__uri__",
    "__str__",
    "__repr__",
    "__getstate__",
    "__setstate__",
    "__slots__",
}

# These methods are expected to always return a non-trivial value.
METHODS_WITH_RETURN_VALUE: Final = {
    "__ne__",
    "__eq__",
    "__lt__",
    "__le__",
    "__gt__",
    "__ge__",
    "__hash__",
    "__iter__",
}

# These magic methods always return the same type.
KNOWN_MAGIC_METHODS_RETURN_TYPES: Final = {
    "__len__": "int",
    "__length_hint__": "int",
    "__init__": "None",
    "__del__": "None",
    "__bool__": "bool",
    "__bytes__": "bytes",
    "__format__": "str",
    "__contains__": "bool",
    "__complex__": "complex",
    "__int__": "int",
    "__float__": "float",
    "__index__": "int",
}


</t>
<t tx="ekr.20230831011820.1772">class Options:
    """Represents stubgen options.

    This class is mutable to simplify testing.
    """

    @others
</t>
<t tx="ekr.20230831011820.1773">def __init__(
    self,
    pyversion: tuple[int, int],
    no_import: bool,
    doc_dir: str,
    search_path: list[str],
    interpreter: str,
    parse_only: bool,
    ignore_errors: bool,
    include_private: bool,
    output_dir: str,
    modules: list[str],
    packages: list[str],
    files: list[str],
    verbose: bool,
    quiet: bool,
    export_less: bool,
    include_docstrings: bool,
) -&gt; None:
    # See parse_options for descriptions of the flags.
    self.pyversion = pyversion
    self.no_import = no_import
    self.doc_dir = doc_dir
    self.search_path = search_path
    self.interpreter = interpreter
    self.decointerpreter = interpreter
    self.parse_only = parse_only
    self.ignore_errors = ignore_errors
    self.include_private = include_private
    self.output_dir = output_dir
    self.modules = modules
    self.packages = packages
    self.files = files
    self.verbose = verbose
    self.quiet = quiet
    self.export_less = export_less
    self.include_docstrings = include_docstrings


</t>
<t tx="ekr.20230831011820.1774">class StubSource:
    """A single source for stub: can be a Python or C module.

    A simple extension of BuildSource that also carries the AST and
    the value of __all__ detected at runtime.
    """

    @others
</t>
<t tx="ekr.20230831011820.1775">def __init__(
    self, module: str, path: str | None = None, runtime_all: list[str] | None = None
) -&gt; None:
    self.source = BuildSource(path, module, None)
    self.runtime_all = runtime_all
    self.ast: MypyFile | None = None

</t>
<t tx="ekr.20230831011820.1776">@property
def module(self) -&gt; str:
    return self.source.module

</t>
<t tx="ekr.20230831011820.1777">@property
def path(self) -&gt; str | None:
    return self.source.path


</t>
<t tx="ekr.20230831011820.1778"># What was generated previously in the stub file. We keep track of these to generate
# nicely formatted output (add empty line between non-empty classes, for example).
EMPTY: Final = "EMPTY"
FUNC: Final = "FUNC"
CLASS: Final = "CLASS"
EMPTY_CLASS: Final = "EMPTY_CLASS"
VAR: Final = "VAR"
NOT_IN_ALL: Final = "NOT_IN_ALL"

# Indicates that we failed to generate a reasonable output
# for a given node. These should be manually replaced by a user.

ERROR_MARKER: Final = "&lt;ERROR&gt;"


class AnnotationPrinter(TypeStrVisitor):
    """Visitor used to print existing annotations in a file.

    The main difference from TypeStrVisitor is a better treatment of
    unbound types.

    Notes:
    * This visitor doesn't add imports necessary for annotations, this is done separately
      by ImportTracker.
    * It can print all kinds of types, but the generated strings may not be valid (notably
      callable types) since it prints the same string that reveal_type() does.
    * For Instance types it prints the fully qualified names.
    """

    @others
</t>
<t tx="ekr.20230831011820.1779"># TODO: Generate valid string representation for callable types.
# TODO: Use short names for Instances.
def __init__(self, stubgen: StubGenerator) -&gt; None:
    super().__init__(options=mypy.options.Options())
    self.stubgen = stubgen

</t>
<t tx="ekr.20230831011820.178">    def typeddict_context_ambiguous(self, types: list[TypedDictType], context: Context) -&gt; None:
        formatted_types = ", ".join(list(format_type_distinctly(*types, options=self.options)))
        self.fail(
            f"Type of TypedDict is ambiguous, none of ({formatted_types}) matches cleanly", context
        )

</t>
<t tx="ekr.20230831011820.1780">def visit_any(self, t: AnyType) -&gt; str:
    s = super().visit_any(t)
    self.stubgen.import_tracker.require_name(s)
    return s

</t>
<t tx="ekr.20230831011820.1781">def visit_unbound_type(self, t: UnboundType) -&gt; str:
    s = t.name
    self.stubgen.import_tracker.require_name(s)
    if t.args:
        s += f"[{self.args_str(t.args)}]"
    return s

</t>
<t tx="ekr.20230831011820.1782">def visit_none_type(self, t: NoneType) -&gt; str:
    return "None"

</t>
<t tx="ekr.20230831011820.1783">def visit_type_list(self, t: TypeList) -&gt; str:
    return f"[{self.list_str(t.items)}]"

</t>
<t tx="ekr.20230831011820.1784">def visit_union_type(self, t: UnionType) -&gt; str:
    return " | ".join([item.accept(self) for item in t.items])

</t>
<t tx="ekr.20230831011820.1785">def args_str(self, args: Iterable[Type]) -&gt; str:
    """Convert an array of arguments to strings and join the results with commas.

    The main difference from list_str is the preservation of quotes for string
    arguments
    """
    types = ["builtins.bytes", "builtins.str"]
    res = []
    for arg in args:
        arg_str = arg.accept(self)
        if isinstance(arg, UnboundType) and arg.original_str_fallback in types:
            res.append(f"'{arg_str}'")
        else:
            res.append(arg_str)
    return ", ".join(res)


</t>
<t tx="ekr.20230831011820.1786">class AliasPrinter(NodeVisitor[str]):
    """Visitor used to collect type aliases _and_ type variable definitions.

    Visit r.h.s of the definition to get the string representation of type alias.
    """

    @others
</t>
<t tx="ekr.20230831011820.1787">def __init__(self, stubgen: StubGenerator) -&gt; None:
    self.stubgen = stubgen
    super().__init__()

</t>
<t tx="ekr.20230831011820.1788">def visit_call_expr(self, node: CallExpr) -&gt; str:
    # Call expressions are not usually types, but we also treat `X = TypeVar(...)` as a
    # type alias that has to be preserved (even if TypeVar is not the same as an alias)
    callee = node.callee.accept(self)
    args = []
    for name, arg, kind in zip(node.arg_names, node.args, node.arg_kinds):
        if kind == ARG_POS:
            args.append(arg.accept(self))
        elif kind == ARG_STAR:
            args.append("*" + arg.accept(self))
        elif kind == ARG_STAR2:
            args.append("**" + arg.accept(self))
        elif kind == ARG_NAMED:
            args.append(f"{name}={arg.accept(self)}")
        else:
            raise ValueError(f"Unknown argument kind {kind} in call")
    return f"{callee}({', '.join(args)})"

</t>
<t tx="ekr.20230831011820.1789">def visit_name_expr(self, node: NameExpr) -&gt; str:
    self.stubgen.import_tracker.require_name(node.name)
    return node.name

</t>
<t tx="ekr.20230831011820.179">    def typeddict_key_cannot_be_deleted(
        self, typ: TypedDictType, item_name: str, context: Context
    ) -&gt; None:
        if typ.is_anonymous():
            self.fail(f'TypedDict key "{item_name}" cannot be deleted', context)
        else:
            self.fail(
                f'Key "{item_name}" of TypedDict {format_type(typ, self.options)} cannot be deleted',
                context,
            )

</t>
<t tx="ekr.20230831011820.1790">def visit_member_expr(self, o: MemberExpr) -&gt; str:
    node: Expression = o
    trailer = ""
    while isinstance(node, MemberExpr):
        trailer = "." + node.name + trailer
        node = node.expr
    if not isinstance(node, NameExpr):
        return ERROR_MARKER
    self.stubgen.import_tracker.require_name(node.name)
    return node.name + trailer

</t>
<t tx="ekr.20230831011820.1791">def visit_str_expr(self, node: StrExpr) -&gt; str:
    return repr(node.value)

</t>
<t tx="ekr.20230831011820.1792">def visit_index_expr(self, node: IndexExpr) -&gt; str:
    base = node.base.accept(self)
    index = node.index.accept(self)
    if len(index) &gt; 2 and index.startswith("(") and index.endswith(")"):
        index = index[1:-1]
    return f"{base}[{index}]"

</t>
<t tx="ekr.20230831011820.1793">def visit_tuple_expr(self, node: TupleExpr) -&gt; str:
    return f"({', '.join(n.accept(self) for n in node.items)})"

</t>
<t tx="ekr.20230831011820.1794">def visit_list_expr(self, node: ListExpr) -&gt; str:
    return f"[{', '.join(n.accept(self) for n in node.items)}]"

</t>
<t tx="ekr.20230831011820.1795">def visit_dict_expr(self, o: DictExpr) -&gt; str:
    dict_items = []
    for key, value in o.items:
        # This is currently only used for TypedDict where all keys are strings.
        assert isinstance(key, StrExpr)
        dict_items.append(f"{key.accept(self)}: {value.accept(self)}")
    return f"{{{', '.join(dict_items)}}}"

</t>
<t tx="ekr.20230831011820.1796">def visit_ellipsis(self, node: EllipsisExpr) -&gt; str:
    return "..."

</t>
<t tx="ekr.20230831011820.1797">def visit_op_expr(self, o: OpExpr) -&gt; str:
    return f"{o.left.accept(self)} {o.op} {o.right.accept(self)}"


</t>
<t tx="ekr.20230831011820.1798">class ImportTracker:
    """Record necessary imports during stub generation."""

    @others
</t>
<t tx="ekr.20230831011820.1799">def __init__(self) -&gt; None:
    # module_for['foo'] has the module name where 'foo' was imported from, or None if
    # 'foo' is a module imported directly; examples
    #     'from pkg.m import f as foo' ==&gt; module_for['foo'] == 'pkg.m'
    #     'from m import f' ==&gt; module_for['f'] == 'm'
    #     'import m' ==&gt; module_for['m'] == None
    #     'import pkg.m' ==&gt; module_for['pkg.m'] == None
    #                    ==&gt; module_for['pkg'] == None
    self.module_for: dict[str, str | None] = {}

    # direct_imports['foo'] is the module path used when the name 'foo' was added to the
    # namespace.
    #   import foo.bar.baz  ==&gt; direct_imports['foo'] == 'foo.bar.baz'
    #                       ==&gt; direct_imports['foo.bar'] == 'foo.bar.baz'
    #                       ==&gt; direct_imports['foo.bar.baz'] == 'foo.bar.baz'
    self.direct_imports: dict[str, str] = {}

    # reverse_alias['foo'] is the name that 'foo' had originally when imported with an
    # alias; examples
    #     'import numpy as np' ==&gt; reverse_alias['np'] == 'numpy'
    #     'import foo.bar as bar' ==&gt; reverse_alias['bar'] == 'foo.bar'
    #     'from decimal import Decimal as D' ==&gt; reverse_alias['D'] == 'Decimal'
    self.reverse_alias: dict[str, str] = {}

    # required_names is the set of names that are actually used in a type annotation
    self.required_names: set[str] = set()

    # Names that should be reexported if they come from another module
    self.reexports: set[str] = set()

</t>
<t tx="ekr.20230831011820.18">def get_possible_variants(typ: Type) -&gt; list[Type]:
    """This function takes any "Union-like" type and returns a list of the available "options".

    Specifically, there are currently exactly three different types that can have
    "variants" or are "union-like":

    - Unions
    - TypeVars with value restrictions
    - Overloads

    This function will return a list of each "option" present in those types.

    If this function receives any other type, we return a list containing just that
    original type. (E.g. pretend the type was contained within a singleton union).

    The only current exceptions are regular TypeVars and ParamSpecs. For these "TypeVarLike"s,
    we return a list containing that TypeVarLike's upper bound.

    This function is useful primarily when checking to see if two types are overlapping:
    the algorithm to check if two unions are overlapping is fundamentally the same as
    the algorithm for checking if two overloads are overlapping.

    Normalizing both kinds of types in the same way lets us reuse the same algorithm
    for both.
    """
    typ = get_proper_type(typ)

    if isinstance(typ, TypeVarType):
        if len(typ.values) &gt; 0:
            return typ.values
        else:
            return [typ.upper_bound]
    elif isinstance(typ, ParamSpecType):
        return [typ.upper_bound]
    elif isinstance(typ, UnionType):
        return list(typ.items)
    elif isinstance(typ, Overloaded):
        # Note: doing 'return typ.items()' makes mypy
        # infer a too-specific return type of List[CallableType]
        return list(typ.items)
    else:
        return [typ]


</t>
<t tx="ekr.20230831011820.180">    def typeddict_setdefault_arguments_inconsistent(
        self, default: Type, expected: Type, context: Context
    ) -&gt; None:
        msg = 'Argument 2 to "setdefault" of "TypedDict" has incompatible type {}; expected {}'
        self.fail(
            msg.format(format_type(default, self.options), format_type(expected, self.options)),
            context,
            code=codes.TYPEDDICT_ITEM,
        )

</t>
<t tx="ekr.20230831011820.1800">def add_import_from(self, module: str, names: list[tuple[str, str | None]]) -&gt; None:
    for name, alias in names:
        if alias:
            # 'from {module} import {name} as {alias}'
            self.module_for[alias] = module
            self.reverse_alias[alias] = name
        else:
            # 'from {module} import {name}'
            self.module_for[name] = module
            self.reverse_alias.pop(name, None)
        self.direct_imports.pop(alias or name, None)

</t>
<t tx="ekr.20230831011820.1801">def add_import(self, module: str, alias: str | None = None) -&gt; None:
    if alias:
        # 'import {module} as {alias}'
        self.module_for[alias] = None
        self.reverse_alias[alias] = module
    else:
        # 'import {module}'
        name = module
        # add module and its parent packages
        while name:
            self.module_for[name] = None
            self.direct_imports[name] = module
            self.reverse_alias.pop(name, None)
            name = name.rpartition(".")[0]

</t>
<t tx="ekr.20230831011820.1802">def require_name(self, name: str) -&gt; None:
    self.required_names.add(name.split(".")[0])

</t>
<t tx="ekr.20230831011820.1803">def reexport(self, name: str) -&gt; None:
    """Mark a given non qualified name as needed in __all__.

    This means that in case it comes from a module, it should be
    imported with an alias even is the alias is the same as the name.
    """
    self.require_name(name)
    self.reexports.add(name)

</t>
<t tx="ekr.20230831011820.1804">def import_lines(self) -&gt; list[str]:
    """The list of required import lines (as strings with python code)."""
    result = []

    # To summarize multiple names imported from a same module, we collect those
    # in the `module_map` dictionary, mapping a module path to the list of names that should
    # be imported from it. the names can also be alias in the form 'original as alias'
    module_map: Mapping[str, list[str]] = defaultdict(list)

    for name in sorted(self.required_names):
        # If we haven't seen this name in an import statement, ignore it
        if name not in self.module_for:
            continue

        m = self.module_for[name]
        if m is not None:
            # This name was found in a from ... import ...
            # Collect the name in the module_map
            if name in self.reverse_alias:
                name = f"{self.reverse_alias[name]} as {name}"
            elif name in self.reexports:
                name = f"{name} as {name}"
            module_map[m].append(name)
        else:
            # This name was found in an import ...
            # We can already generate the import line
            if name in self.reverse_alias:
                source = self.reverse_alias[name]
                result.append(f"import {source} as {name}\n")
            elif name in self.reexports:
                assert "." not in name  # Because reexports only has nonqualified names
                result.append(f"import {name} as {name}\n")
            else:
                result.append(f"import {self.direct_imports[name]}\n")

    # Now generate all the from ... import ... lines collected in module_map
    for module, names in sorted(module_map.items()):
        result.append(f"from {module} import {', '.join(sorted(names))}\n")
    return result


</t>
<t tx="ekr.20230831011820.1805">def find_defined_names(file: MypyFile) -&gt; set[str]:
    finder = DefinitionFinder()
    file.accept(finder)
    return finder.names


</t>
<t tx="ekr.20230831011820.1806">class DefinitionFinder(mypy.traverser.TraverserVisitor):
    """Find names of things defined at the top level of a module."""

    @others
</t>
<t tx="ekr.20230831011820.1807"># TODO: Assignment statements etc.

def __init__(self) -&gt; None:
    # Short names of things defined at the top level.
    self.names: set[str] = set()

</t>
<t tx="ekr.20230831011820.1808">def visit_class_def(self, o: ClassDef) -&gt; None:
    # Don't recurse into classes, as we only keep track of top-level definitions.
    self.names.add(o.name)

</t>
<t tx="ekr.20230831011820.1809">def visit_func_def(self, o: FuncDef) -&gt; None:
    # Don't recurse, as we only keep track of top-level definitions.
    self.names.add(o.name)


</t>
<t tx="ekr.20230831011820.181">    def type_arguments_not_allowed(self, context: Context) -&gt; None:
        self.fail("Parameterized generics cannot be used with class or instance checks", context)

</t>
<t tx="ekr.20230831011820.1810">def find_referenced_names(file: MypyFile) -&gt; set[str]:
    finder = ReferenceFinder()
    file.accept(finder)
    return finder.refs


</t>
<t tx="ekr.20230831011820.1811">class ReferenceFinder(mypy.mixedtraverser.MixedTraverserVisitor):
    """Find all name references (both local and global)."""

    @others
</t>
<t tx="ekr.20230831011820.1812"># TODO: Filter out local variable and class attribute references

def __init__(self) -&gt; None:
    # Short names of things defined at the top level.
    self.refs: set[str] = set()

</t>
<t tx="ekr.20230831011820.1813">def visit_block(self, block: Block) -&gt; None:
    if not block.is_unreachable:
        super().visit_block(block)

</t>
<t tx="ekr.20230831011820.1814">def visit_name_expr(self, e: NameExpr) -&gt; None:
    self.refs.add(e.name)

</t>
<t tx="ekr.20230831011820.1815">def visit_instance(self, t: Instance) -&gt; None:
    self.add_ref(t.type.fullname)
    super().visit_instance(t)

</t>
<t tx="ekr.20230831011820.1816">def visit_unbound_type(self, t: UnboundType) -&gt; None:
    if t.name:
        self.add_ref(t.name)

</t>
<t tx="ekr.20230831011820.1817">def visit_tuple_type(self, t: TupleType) -&gt; None:
    # Ignore fallback
    for item in t.items:
        item.accept(self)

</t>
<t tx="ekr.20230831011820.1818">def visit_callable_type(self, t: CallableType) -&gt; None:
    # Ignore fallback
    for arg in t.arg_types:
        arg.accept(self)
    t.ret_type.accept(self)

</t>
<t tx="ekr.20230831011820.1819">def add_ref(self, fullname: str) -&gt; None:
    self.refs.add(fullname.split(".")[-1])


</t>
<t tx="ekr.20230831011820.182">    def disallowed_any_type(self, typ: Type, context: Context) -&gt; None:
        typ = get_proper_type(typ)
        if isinstance(typ, AnyType):
            message = 'Expression has type "Any"'
        else:
            message = f'Expression type contains "Any" (has type {format_type(typ, self.options)})'
        self.fail(message, context)

</t>
<t tx="ekr.20230831011820.1820">class StubGenerator(mypy.traverser.TraverserVisitor):
    """Generate stub text from a mypy AST."""

    @others
</t>
<t tx="ekr.20230831011820.1821">def __init__(
    self,
    _all_: list[str] | None,
    include_private: bool = False,
    analyzed: bool = False,
    export_less: bool = False,
    include_docstrings: bool = False,
) -&gt; None:
    # Best known value of __all__.
    self._all_ = _all_
    self._output: list[str] = []
    self._decorators: list[str] = []
    self._import_lines: list[str] = []
    # Current indent level (indent is hardcoded to 4 spaces).
    self._indent = ""
    # Stack of defined variables (per scope).
    self._vars: list[list[str]] = [[]]
    # What was generated previously in the stub file.
    self._state = EMPTY
    self._toplevel_names: list[str] = []
    self._include_private = include_private
    self._include_docstrings = include_docstrings
    self._current_class: ClassDef | None = None
    self.import_tracker = ImportTracker()
    # Was the tree semantically analysed before?
    self.analyzed = analyzed
    # Disable implicit exports of package-internal imports?
    self.export_less = export_less
    # Add imports that could be implicitly generated
    self.import_tracker.add_import_from("typing", [("NamedTuple", None)])
    # Names in __all__ are required
    for name in _all_ or ():
        if name not in IGNORED_DUNDERS:
            self.import_tracker.reexport(name)
    self.defined_names: set[str] = set()
    # Short names of methods defined in the body of the current class
    self.method_names: set[str] = set()

</t>
<t tx="ekr.20230831011820.1822">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.module = o.fullname  # Current module being processed
    self.path = o.path
    self.defined_names = find_defined_names(o)
    self.referenced_names = find_referenced_names(o)
    known_imports = {
        "_typeshed": ["Incomplete"],
        "typing": ["Any", "TypeVar", "NamedTuple"],
        "collections.abc": ["Generator"],
        "typing_extensions": ["TypedDict", "ParamSpec", "TypeVarTuple"],
    }
    for pkg, imports in known_imports.items():
        for t in imports:
            if t not in self.defined_names:
                alias = None
            else:
                alias = "_" + t
            self.import_tracker.add_import_from(pkg, [(t, alias)])
    super().visit_mypy_file(o)
    undefined_names = [name for name in self._all_ or [] if name not in self._toplevel_names]
    if undefined_names:
        if self._state != EMPTY:
            self.add("\n")
        self.add("# Names in __all__ with no definition:\n")
        for name in sorted(undefined_names):
            self.add(f"#   {name}\n")

</t>
<t tx="ekr.20230831011820.1823">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    """@property with setters and getters, @overload chain and some others."""
    overload_chain = False
    for item in o.items:
        if not isinstance(item, Decorator):
            continue
        if self.is_private_name(item.func.name, item.func.fullname):
            continue

        self.process_decorator(item)
        if not overload_chain:
            self.visit_func_def(item.func)
            if item.func.is_overload:
                overload_chain = True
        elif item.func.is_overload:
            self.visit_func_def(item.func)
        else:
            # skip the overload implementation and clear the decorator we just processed
            self.clear_decorators()

</t>
<t tx="ekr.20230831011820.1824">def visit_func_def(self, o: FuncDef) -&gt; None:
    if (
        self.is_private_name(o.name, o.fullname)
        or self.is_not_in_all(o.name)
        or (self.is_recorded_name(o.name) and not o.is_overload)
    ):
        self.clear_decorators()
        return
    if not self._indent and self._state not in (EMPTY, FUNC) and not o.is_awaitable_coroutine:
        self.add("\n")
    if not self.is_top_level():
        self_inits = find_self_initializers(o)
        for init, value in self_inits:
            if init in self.method_names:
                # Can't have both an attribute and a method/property with the same name.
                continue
            init_code = self.get_init(init, value)
            if init_code:
                self.add(init_code)
    # dump decorators, just before "def ..."
    for s in self._decorators:
        self.add(s)
    self.clear_decorators()
    self.add(f"{self._indent}{'async ' if o.is_coroutine else ''}def {o.name}(")
    self.record_name(o.name)
    args: list[str] = []
    for i, arg_ in enumerate(o.arguments):
        var = arg_.variable
        kind = arg_.kind
        name = var.name
        annotated_type = (
            o.unanalyzed_type.arg_types[i]
            if isinstance(o.unanalyzed_type, CallableType)
            else None
        )
        # I think the name check is incorrect: there are libraries which
        # name their 0th argument other than self/cls
        is_self_arg = i == 0 and name == "self"
        is_cls_arg = i == 0 and name == "cls"
        annotation = ""
        if annotated_type and not is_self_arg and not is_cls_arg:
            # Luckily, an argument explicitly annotated with "Any" has
            # type "UnboundType" and will not match.
            if not isinstance(get_proper_type(annotated_type), AnyType):
                annotation = f": {self.print_annotation(annotated_type)}"

        if kind.is_named() and not any(arg.startswith("*") for arg in args):
            args.append("*")

        if arg_.initializer:
            if not annotation:
                typename = self.get_str_type_of_node(arg_.initializer, True, False)
                if typename == "":
                    annotation = "=..."
                else:
                    annotation = f": {typename} = ..."
            else:
                annotation += " = ..."
            arg = name + annotation
        elif kind == ARG_STAR:
            arg = f"*{name}{annotation}"
        elif kind == ARG_STAR2:
            arg = f"**{name}{annotation}"
        else:
            arg = name + annotation
        args.append(arg)
    retname = None
    if o.name != "__init__" and isinstance(o.unanalyzed_type, CallableType):
        if isinstance(get_proper_type(o.unanalyzed_type.ret_type), AnyType):
            # Luckily, a return type explicitly annotated with "Any" has
            # type "UnboundType" and will enter the else branch.
            retname = None  # implicit Any
        else:
            retname = self.print_annotation(o.unanalyzed_type.ret_type)
    elif o.abstract_status == IS_ABSTRACT or o.name in METHODS_WITH_RETURN_VALUE:
        # Always assume abstract methods return Any unless explicitly annotated. Also
        # some dunder methods should not have a None return type.
        retname = None  # implicit Any
    elif o.name in KNOWN_MAGIC_METHODS_RETURN_TYPES:
        retname = KNOWN_MAGIC_METHODS_RETURN_TYPES[o.name]
    elif has_yield_expression(o) or has_yield_from_expression(o):
        generator_name = self.add_typing_import("Generator")
        yield_name = "None"
        send_name = "None"
        return_name = "None"
        if has_yield_from_expression(o):
            yield_name = send_name = self.add_typing_import("Incomplete")
        else:
            for expr, in_assignment in all_yield_expressions(o):
                if expr.expr is not None and not self.is_none_expr(expr.expr):
                    yield_name = self.add_typing_import("Incomplete")
                if in_assignment:
                    send_name = self.add_typing_import("Incomplete")
        if has_return_statement(o):
            return_name = self.add_typing_import("Incomplete")
        retname = f"{generator_name}[{yield_name}, {send_name}, {return_name}]"
    elif not has_return_statement(o) and o.abstract_status == NOT_ABSTRACT:
        retname = "None"
    retfield = ""
    if retname is not None:
        retfield = " -&gt; " + retname

    self.add(", ".join(args))
    self.add(f"){retfield}:")
    if self._include_docstrings and o.docstring:
        docstring = mypy.util.quote_docstring(o.docstring)
        self.add(f"\n{self._indent}    {docstring}\n")
    else:
        self.add(" ...\n")

    self._state = FUNC

</t>
<t tx="ekr.20230831011820.1825">def is_none_expr(self, expr: Expression) -&gt; bool:
    return isinstance(expr, NameExpr) and expr.name == "None"

</t>
<t tx="ekr.20230831011820.1826">def visit_decorator(self, o: Decorator) -&gt; None:
    if self.is_private_name(o.func.name, o.func.fullname):
        return
    self.process_decorator(o)
    self.visit_func_def(o.func)

</t>
<t tx="ekr.20230831011820.1827">def process_decorator(self, o: Decorator) -&gt; None:
    """Process a series of decorators.

    Only preserve certain special decorators such as @abstractmethod.
    """
    for decorator in o.original_decorators:
        if not isinstance(decorator, (NameExpr, MemberExpr)):
            continue
        qualname = get_qualified_name(decorator)
        fullname = self.get_fullname(decorator)
        if fullname in (
            "builtins.property",
            "builtins.staticmethod",
            "builtins.classmethod",
            "functools.cached_property",
        ):
            self.add_decorator(qualname, require_name=True)
        elif fullname in (
            "asyncio.coroutine",
            "asyncio.coroutines.coroutine",
            "types.coroutine",
        ):
            o.func.is_awaitable_coroutine = True
            self.add_decorator(qualname, require_name=True)
        elif fullname == "abc.abstractmethod":
            self.add_decorator(qualname, require_name=True)
            o.func.abstract_status = IS_ABSTRACT
        elif fullname in (
            "abc.abstractproperty",
            "abc.abstractstaticmethod",
            "abc.abstractclassmethod",
        ):
            abc_module = qualname.rpartition(".")[0]
            if not abc_module:
                self.import_tracker.add_import("abc")
            builtin_decorator_replacement = fullname[len("abc.abstract") :]
            self.add_decorator(builtin_decorator_replacement, require_name=False)
            self.add_decorator(f"{abc_module or 'abc'}.abstractmethod", require_name=True)
            o.func.abstract_status = IS_ABSTRACT
        elif fullname in OVERLOAD_NAMES:
            self.add_decorator(qualname, require_name=True)
            o.func.is_overload = True
        elif qualname.endswith(".setter"):
            self.add_decorator(qualname, require_name=False)

</t>
<t tx="ekr.20230831011820.1828">def get_fullname(self, expr: Expression) -&gt; str:
    """Return the full name resolving imports and import aliases."""
    if (
        self.analyzed
        and isinstance(expr, (NameExpr, MemberExpr))
        and expr.fullname
        and not (isinstance(expr.node, Var) and expr.node.is_suppressed_import)
    ):
        return expr.fullname
    name = get_qualified_name(expr)
    if "." not in name:
        real_module = self.import_tracker.module_for.get(name)
        real_short = self.import_tracker.reverse_alias.get(name, name)
        if real_module is None and real_short not in self.defined_names:
            real_module = "builtins"  # not imported and not defined, must be a builtin
    else:
        name_module, real_short = name.split(".", 1)
        real_module = self.import_tracker.reverse_alias.get(name_module, name_module)
    resolved_name = real_short if real_module is None else f"{real_module}.{real_short}"
    return resolved_name

</t>
<t tx="ekr.20230831011820.1829">def visit_class_def(self, o: ClassDef) -&gt; None:
    self._current_class = o
    self.method_names = find_method_names(o.defs.body)
    sep: int | None = None
    if not self._indent and self._state != EMPTY:
        sep = len(self._output)
        self.add("\n")
    self.add(f"{self._indent}class {o.name}")
    self.record_name(o.name)
    base_types = self.get_base_types(o)
    if base_types:
        for base in base_types:
            self.import_tracker.require_name(base)
    if isinstance(o.metaclass, (NameExpr, MemberExpr)):
        meta = o.metaclass.accept(AliasPrinter(self))
        base_types.append("metaclass=" + meta)
    elif self.analyzed and o.info.is_abstract and not o.info.is_protocol:
        base_types.append("metaclass=abc.ABCMeta")
        self.import_tracker.add_import("abc")
        self.import_tracker.require_name("abc")
    if base_types:
        self.add(f"({', '.join(base_types)})")
    self.add(":\n")
    self._indent += "    "
    if self._include_docstrings and o.docstring:
        docstring = mypy.util.quote_docstring(o.docstring)
        self.add(f"{self._indent}{docstring}\n")
    n = len(self._output)
    self._vars.append([])
    super().visit_class_def(o)
    self._indent = self._indent[:-4]
    self._vars.pop()
    self._vars[-1].append(o.name)
    if len(self._output) == n:
        if self._state == EMPTY_CLASS and sep is not None:
            self._output[sep] = ""
        if not (self._include_docstrings and o.docstring):
            self._output[-1] = self._output[-1][:-1] + " ...\n"
        self._state = EMPTY_CLASS
    else:
        self._state = CLASS
    self.method_names = set()
    self._current_class = None

</t>
<t tx="ekr.20230831011820.183">    def incorrectly_returning_any(self, typ: Type, context: Context) -&gt; None:
        message = (
            f"Returning Any from function declared to return {format_type(typ, self.options)}"
        )
        self.fail(message, context, code=codes.NO_ANY_RETURN)

</t>
<t tx="ekr.20230831011820.1830">def get_base_types(self, cdef: ClassDef) -&gt; list[str]:
    """Get list of base classes for a class."""
    base_types: list[str] = []
    p = AliasPrinter(self)
    for base in cdef.base_type_exprs + cdef.removed_base_type_exprs:
        if isinstance(base, (NameExpr, MemberExpr)):
            if self.get_fullname(base) != "builtins.object":
                base_types.append(get_qualified_name(base))
        elif isinstance(base, IndexExpr):
            base_types.append(base.accept(p))
        elif isinstance(base, CallExpr):
            # namedtuple(typename, fields), NamedTuple(typename, fields) calls can
            # be used as a base class. The first argument is a string literal that
            # is usually the same as the class name.
            #
            # Note:
            # A call-based named tuple as a base class cannot be safely converted to
            # a class-based NamedTuple definition because class attributes defined
            # in the body of the class inheriting from the named tuple call are not
            # namedtuple fields at runtime.
            if self.is_namedtuple(base):
                nt_fields = self._get_namedtuple_fields(base)
                assert isinstance(base.args[0], StrExpr)
                typename = base.args[0].value
                if nt_fields is None:
                    # Invalid namedtuple() call, cannot determine fields
                    base_types.append(self.add_typing_import("Incomplete"))
                    continue
                fields_str = ", ".join(f"({f!r}, {t})" for f, t in nt_fields)
                namedtuple_name = self.add_typing_import("NamedTuple")
                base_types.append(f"{namedtuple_name}({typename!r}, [{fields_str}])")
            elif self.is_typed_namedtuple(base):
                base_types.append(base.accept(p))
            else:
                # At this point, we don't know what the base class is, so we
                # just use Incomplete as the base class.
                base_types.append(self.add_typing_import("Incomplete"))
    for name, value in cdef.keywords.items():
        if name == "metaclass":
            continue  # handled separately
        base_types.append(f"{name}={value.accept(p)}")
    return base_types

</t>
<t tx="ekr.20230831011820.1831">def visit_block(self, o: Block) -&gt; None:
    # Unreachable statements may be partially uninitialized and that may
    # cause trouble.
    if not o.is_unreachable:
        super().visit_block(o)

</t>
<t tx="ekr.20230831011820.1832">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    foundl = []

    for lvalue in o.lvalues:
        if isinstance(lvalue, NameExpr) and isinstance(o.rvalue, CallExpr):
            if self.is_namedtuple(o.rvalue) or self.is_typed_namedtuple(o.rvalue):
                self.process_namedtuple(lvalue, o.rvalue)
                foundl.append(False)  # state is updated in process_namedtuple
                continue
            if self.is_typeddict(o.rvalue):
                self.process_typeddict(lvalue, o.rvalue)
                foundl.append(False)  # state is updated in process_typeddict
                continue
        if (
            isinstance(lvalue, NameExpr)
            and not self.is_private_name(lvalue.name)
            # it is never an alias with explicit annotation
            and not o.unanalyzed_type
            and self.is_alias_expression(o.rvalue)
        ):
            self.process_typealias(lvalue, o.rvalue)
            continue
        if isinstance(lvalue, (TupleExpr, ListExpr)):
            items = lvalue.items
            if isinstance(o.unanalyzed_type, TupleType):  # type: ignore[misc]
                annotations: Iterable[Type | None] = o.unanalyzed_type.items
            else:
                annotations = [None] * len(items)
        else:
            items = [lvalue]
            annotations = [o.unanalyzed_type]
        sep = False
        found = False
        for item, annotation in zip(items, annotations):
            if isinstance(item, NameExpr):
                init = self.get_init(item.name, o.rvalue, annotation)
                if init:
                    found = True
                    if not sep and not self._indent and self._state not in (EMPTY, VAR):
                        init = "\n" + init
                        sep = True
                    self.add(init)
                    self.record_name(item.name)
        foundl.append(found)

    if all(foundl):
        self._state = VAR

</t>
<t tx="ekr.20230831011820.1833">def is_namedtuple(self, expr: CallExpr) -&gt; bool:
    return self.get_fullname(expr.callee) == "collections.namedtuple"

</t>
<t tx="ekr.20230831011820.1834">def is_typed_namedtuple(self, expr: CallExpr) -&gt; bool:
    return self.get_fullname(expr.callee) in TYPED_NAMEDTUPLE_NAMES

</t>
<t tx="ekr.20230831011820.1835">def _get_namedtuple_fields(self, call: CallExpr) -&gt; list[tuple[str, str]] | None:
    if self.is_namedtuple(call):
        fields_arg = call.args[1]
        if isinstance(fields_arg, StrExpr):
            field_names = fields_arg.value.replace(",", " ").split()
        elif isinstance(fields_arg, (ListExpr, TupleExpr)):
            field_names = []
            for field in fields_arg.items:
                if not isinstance(field, StrExpr):
                    return None
                field_names.append(field.value)
        else:
            return None  # Invalid namedtuple fields type
        if not field_names:
            return []
        incomplete = self.add_typing_import("Incomplete")
        return [(field_name, incomplete) for field_name in field_names]
    elif self.is_typed_namedtuple(call):
        fields_arg = call.args[1]
        if not isinstance(fields_arg, (ListExpr, TupleExpr)):
            return None
        fields: list[tuple[str, str]] = []
        p = AliasPrinter(self)
        for field in fields_arg.items:
            if not (isinstance(field, TupleExpr) and len(field.items) == 2):
                return None
            field_name, field_type = field.items
            if not isinstance(field_name, StrExpr):
                return None
            fields.append((field_name.value, field_type.accept(p)))
        return fields
    else:
        return None  # Not a named tuple call

</t>
<t tx="ekr.20230831011820.1836">def process_namedtuple(self, lvalue: NameExpr, rvalue: CallExpr) -&gt; None:
    if self._state == CLASS:
        self.add("\n")

    if not isinstance(rvalue.args[0], StrExpr):
        self.annotate_as_incomplete(lvalue)
        return

    fields = self._get_namedtuple_fields(rvalue)
    if fields is None:
        self.annotate_as_incomplete(lvalue)
        return
    bases = self.add_typing_import("NamedTuple")
    # TODO: Add support for generic NamedTuples. Requires `Generic` as base class.
    class_def = f"{self._indent}class {lvalue.name}({bases}):"
    if len(fields) == 0:
        self.add(f"{class_def} ...\n")
        self._state = EMPTY_CLASS
    else:
        if self._state not in (EMPTY, CLASS):
            self.add("\n")
        self.add(f"{class_def}\n")
        for f_name, f_type in fields:
            self.add(f"{self._indent}    {f_name}: {f_type}\n")
        self._state = CLASS

</t>
<t tx="ekr.20230831011820.1837">def is_typeddict(self, expr: CallExpr) -&gt; bool:
    return self.get_fullname(expr.callee) in TPDICT_NAMES

</t>
<t tx="ekr.20230831011820.1838">def process_typeddict(self, lvalue: NameExpr, rvalue: CallExpr) -&gt; None:
    if self._state == CLASS:
        self.add("\n")

    if not isinstance(rvalue.args[0], StrExpr):
        self.annotate_as_incomplete(lvalue)
        return

    items: list[tuple[str, Expression]] = []
    total: Expression | None = None
    if len(rvalue.args) &gt; 1 and rvalue.arg_kinds[1] == ARG_POS:
        if not isinstance(rvalue.args[1], DictExpr):
            self.annotate_as_incomplete(lvalue)
            return
        for attr_name, attr_type in rvalue.args[1].items:
            if not isinstance(attr_name, StrExpr):
                self.annotate_as_incomplete(lvalue)
                return
            items.append((attr_name.value, attr_type))
        if len(rvalue.args) &gt; 2:
            if rvalue.arg_kinds[2] != ARG_NAMED or rvalue.arg_names[2] != "total":
                self.annotate_as_incomplete(lvalue)
                return
            total = rvalue.args[2]
    else:
        for arg_name, arg in zip(rvalue.arg_names[1:], rvalue.args[1:]):
            if not isinstance(arg_name, str):
                self.annotate_as_incomplete(lvalue)
                return
            if arg_name == "total":
                total = arg
            else:
                items.append((arg_name, arg))
    bases = self.add_typing_import("TypedDict")
    p = AliasPrinter(self)
    if any(not key.isidentifier() or keyword.iskeyword(key) for key, _ in items):
        # Keep the call syntax if there are non-identifier or reserved keyword keys.
        self.add(f"{self._indent}{lvalue.name} = {rvalue.accept(p)}\n")
        self._state = VAR
    else:
        # TODO: Add support for generic TypedDicts. Requires `Generic` as base class.
        if total is not None:
            bases += f", total={total.accept(p)}"
        class_def = f"{self._indent}class {lvalue.name}({bases}):"
        if len(items) == 0:
            self.add(f"{class_def} ...\n")
            self._state = EMPTY_CLASS
        else:
            if self._state not in (EMPTY, CLASS):
                self.add("\n")
            self.add(f"{class_def}\n")
            for key, key_type in items:
                self.add(f"{self._indent}    {key}: {key_type.accept(p)}\n")
            self._state = CLASS

</t>
<t tx="ekr.20230831011820.1839">def annotate_as_incomplete(self, lvalue: NameExpr) -&gt; None:
    self.add(f"{self._indent}{lvalue.name}: {self.add_typing_import('Incomplete')}\n")
    self._state = VAR

</t>
<t tx="ekr.20230831011820.184">    def incorrect__exit__return(self, context: Context) -&gt; None:
        self.fail(
            '"bool" is invalid as return type for "__exit__" that always returns False',
            context,
            code=codes.EXIT_RETURN,
        )
        self.note(
            'Use "typing_extensions.Literal[False]" as the return type or change it to "None"',
            context,
            code=codes.EXIT_RETURN,
        )
        self.note(
            'If return type of "__exit__" implies that it may return True, '
            "the context manager may swallow exceptions",
            context,
            code=codes.EXIT_RETURN,
        )

</t>
<t tx="ekr.20230831011820.1840">def is_alias_expression(self, expr: Expression, top_level: bool = True) -&gt; bool:
    """Return True for things that look like target for an alias.

    Used to know if assignments look like type aliases, function alias,
    or module alias.
    """
    # Assignment of TypeVar(...)  and other typevar-likes are passed through
    if isinstance(expr, CallExpr) and self.get_fullname(expr.callee) in (
        "typing.TypeVar",
        "typing_extensions.TypeVar",
        "typing.ParamSpec",
        "typing_extensions.ParamSpec",
        "typing.TypeVarTuple",
        "typing_extensions.TypeVarTuple",
    ):
        return True
    elif isinstance(expr, EllipsisExpr):
        return not top_level
    elif isinstance(expr, NameExpr):
        if expr.name in ("True", "False"):
            return False
        elif expr.name == "None":
            return not top_level
        else:
            return not self.is_private_name(expr.name)
    elif isinstance(expr, MemberExpr) and self.analyzed:
        # Also add function and module aliases.
        return (
            top_level
            and isinstance(expr.node, (FuncDef, Decorator, MypyFile))
            or isinstance(expr.node, TypeInfo)
        ) and not self.is_private_member(expr.node.fullname)
    elif (
        isinstance(expr, IndexExpr)
        and isinstance(expr.base, NameExpr)
        and not self.is_private_name(expr.base.name)
    ):
        if isinstance(expr.index, TupleExpr):
            indices = expr.index.items
        else:
            indices = [expr.index]
        if expr.base.name == "Callable" and len(indices) == 2:
            args, ret = indices
            if isinstance(args, EllipsisExpr):
                indices = [ret]
            elif isinstance(args, ListExpr):
                indices = args.items + [ret]
            else:
                return False
        return all(self.is_alias_expression(i, top_level=False) for i in indices)
    else:
        return False

</t>
<t tx="ekr.20230831011820.1841">def process_typealias(self, lvalue: NameExpr, rvalue: Expression) -&gt; None:
    p = AliasPrinter(self)
    self.add(f"{self._indent}{lvalue.name} = {rvalue.accept(p)}\n")
    self.record_name(lvalue.name)
    self._vars[-1].append(lvalue.name)

</t>
<t tx="ekr.20230831011820.1842">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    # Ignore if __name__ == '__main__'.
    expr = o.expr[0]
    if (
        isinstance(expr, ComparisonExpr)
        and isinstance(expr.operands[0], NameExpr)
        and isinstance(expr.operands[1], StrExpr)
        and expr.operands[0].name == "__name__"
        and "__main__" in expr.operands[1].value
    ):
        return
    super().visit_if_stmt(o)

</t>
<t tx="ekr.20230831011820.1843">def visit_import_all(self, o: ImportAll) -&gt; None:
    self.add_import_line(f"from {'.' * o.relative}{o.id} import *\n")

</t>
<t tx="ekr.20230831011820.1844">def visit_import_from(self, o: ImportFrom) -&gt; None:
    exported_names: set[str] = set()
    import_names = []
    module, relative = translate_module_name(o.id, o.relative)
    if self.module:
        full_module, ok = mypy.util.correct_relative_import(
            self.module, relative, module, self.path.endswith(".__init__.py")
        )
        if not ok:
            full_module = module
    else:
        full_module = module
    if module == "__future__":
        return  # Not preserved
    for name, as_name in o.names:
        if name == "six":
            # Vendored six -- translate into plain 'import six'.
            self.visit_import(Import([("six", None)]))
            continue
        exported = False
        if as_name is None and self.module and (self.module + "." + name) in EXTRA_EXPORTED:
            # Special case certain names that should be exported, against our general rules.
            exported = True
        is_private = self.is_private_name(name, full_module + "." + name)
        if (
            as_name is None
            and name not in self.referenced_names
            and (not self._all_ or name in IGNORED_DUNDERS)
            and not is_private
            and module not in ("abc", "asyncio") + TYPING_MODULE_NAMES
        ):
            # An imported name that is never referenced in the module is assumed to be
            # exported, unless there is an explicit __all__. Note that we need to special
            # case 'abc' since some references are deleted during semantic analysis.
            exported = True
        top_level = full_module.split(".")[0]
        if (
            as_name is None
            and not self.export_less
            and (not self._all_ or name in IGNORED_DUNDERS)
            and self.module
            and not is_private
            and top_level in (self.module.split(".")[0], "_" + self.module.split(".")[0])
        ):
            # Export imports from the same package, since we can't reliably tell whether they
            # are part of the public API.
            exported = True
        if exported:
            self.import_tracker.reexport(name)
            as_name = name
        import_names.append((name, as_name))
    self.import_tracker.add_import_from("." * relative + module, import_names)
    self._vars[-1].extend(alias or name for name, alias in import_names)
    for name, alias in import_names:
        self.record_name(alias or name)

    if self._all_:
        # Include "import from"s that import names defined in __all__.
        names = [
            name
            for name, alias in o.names
            if name in self._all_ and alias is None and name not in IGNORED_DUNDERS
        ]
        exported_names.update(names)

</t>
<t tx="ekr.20230831011820.1845">def visit_import(self, o: Import) -&gt; None:
    for id, as_id in o.ids:
        self.import_tracker.add_import(id, as_id)
        if as_id is None:
            target_name = id.split(".")[0]
        else:
            target_name = as_id
        self._vars[-1].append(target_name)
        self.record_name(target_name)

</t>
<t tx="ekr.20230831011820.1846">def get_init(
    self, lvalue: str, rvalue: Expression, annotation: Type | None = None
) -&gt; str | None:
    """Return initializer for a variable.

    Return None if we've generated one already or if the variable is internal.
    """
    if lvalue in self._vars[-1]:
        # We've generated an initializer already for this variable.
        return None
    # TODO: Only do this at module top level.
    if self.is_private_name(lvalue) or self.is_not_in_all(lvalue):
        return None
    self._vars[-1].append(lvalue)
    if annotation is not None:
        typename = self.print_annotation(annotation)
        if (
            isinstance(annotation, UnboundType)
            and not annotation.args
            and annotation.name == "Final"
            and self.import_tracker.module_for.get("Final") in TYPING_MODULE_NAMES
        ):
            # Final without type argument is invalid in stubs.
            final_arg = self.get_str_type_of_node(rvalue)
            typename += f"[{final_arg}]"
    else:
        typename = self.get_str_type_of_node(rvalue)
    initializer = self.get_assign_initializer(rvalue)
    return f"{self._indent}{lvalue}: {typename}{initializer}\n"

</t>
<t tx="ekr.20230831011820.1847">def get_assign_initializer(self, rvalue: Expression) -&gt; str:
    """Does this rvalue need some special initializer value?"""
    if self._current_class and self._current_class.info:
        # Current rules
        # 1. Return `...` if we are dealing with `NamedTuple` and it has an existing default value
        if self._current_class.info.is_named_tuple and not isinstance(rvalue, TempNode):
            return " = ..."
        # TODO: support other possible cases, where initializer is important

    # By default, no initializer is required:
    return ""

</t>
<t tx="ekr.20230831011820.1848">def add(self, string: str) -&gt; None:
    """Add text to generated stub."""
    self._output.append(string)

</t>
<t tx="ekr.20230831011820.1849">def add_decorator(self, name: str, require_name: bool = False) -&gt; None:
    if require_name:
        self.import_tracker.require_name(name)
    if not self._indent and self._state not in (EMPTY, FUNC):
        self._decorators.append("\n")
    self._decorators.append(f"{self._indent}@{name}\n")

</t>
<t tx="ekr.20230831011820.185">    def untyped_decorated_function(self, typ: Type, context: Context) -&gt; None:
        typ = get_proper_type(typ)
        if isinstance(typ, AnyType):
            self.fail("Function is untyped after decorator transformation", context)
        else:
            self.fail(
                f'Type of decorated function contains type "Any" ({format_type(typ, self.options)})',
                context,
            )

</t>
<t tx="ekr.20230831011820.1850">def clear_decorators(self) -&gt; None:
    self._decorators.clear()

</t>
<t tx="ekr.20230831011820.1851">def typing_name(self, name: str) -&gt; str:
    if name in self.defined_names:
        # Avoid name clash between name from typing and a name defined in stub.
        return "_" + name
    else:
        return name

</t>
<t tx="ekr.20230831011820.1852">def add_typing_import(self, name: str) -&gt; str:
    """Add a name to be imported for typing, unless it's imported already.

    The import will be internal to the stub.
    """
    name = self.typing_name(name)
    self.import_tracker.require_name(name)
    return name

</t>
<t tx="ekr.20230831011820.1853">def add_import_line(self, line: str) -&gt; None:
    """Add a line of text to the import section, unless it's already there."""
    if line not in self._import_lines:
        self._import_lines.append(line)

</t>
<t tx="ekr.20230831011820.1854">def output(self) -&gt; str:
    """Return the text for the stub."""
    imports = ""
    if self._import_lines:
        imports += "".join(self._import_lines)
    imports += "".join(self.import_tracker.import_lines())
    if imports and self._output:
        imports += "\n"
    return imports + "".join(self._output)

</t>
<t tx="ekr.20230831011820.1855">def is_not_in_all(self, name: str) -&gt; bool:
    if self.is_private_name(name):
        return False
    if self._all_:
        return self.is_top_level() and name not in self._all_
    return False

</t>
<t tx="ekr.20230831011820.1856">def is_private_name(self, name: str, fullname: str | None = None) -&gt; bool:
    if self._include_private:
        return False
    if fullname in EXTRA_EXPORTED:
        return False
    return name.startswith("_") and (not name.endswith("__") or name in IGNORED_DUNDERS)

</t>
<t tx="ekr.20230831011820.1857">def is_private_member(self, fullname: str) -&gt; bool:
    parts = fullname.split(".")
    return any(self.is_private_name(part) for part in parts)

</t>
<t tx="ekr.20230831011820.1858">def get_str_type_of_node(
    self, rvalue: Expression, can_infer_optional: bool = False, can_be_any: bool = True
) -&gt; str:
    rvalue = self.maybe_unwrap_unary_expr(rvalue)

    if isinstance(rvalue, IntExpr):
        return "int"
    if isinstance(rvalue, StrExpr):
        return "str"
    if isinstance(rvalue, BytesExpr):
        return "bytes"
    if isinstance(rvalue, FloatExpr):
        return "float"
    if isinstance(rvalue, ComplexExpr):  # 1j
        return "complex"
    if isinstance(rvalue, OpExpr) and rvalue.op in ("-", "+"):  # -1j + 1
        if isinstance(self.maybe_unwrap_unary_expr(rvalue.left), ComplexExpr) or isinstance(
            self.maybe_unwrap_unary_expr(rvalue.right), ComplexExpr
        ):
            return "complex"
    if isinstance(rvalue, NameExpr) and rvalue.name in ("True", "False"):
        return "bool"
    if can_infer_optional and isinstance(rvalue, NameExpr) and rvalue.name == "None":
        return f"{self.add_typing_import('Incomplete')} | None"
    if can_be_any:
        return self.add_typing_import("Incomplete")
    else:
        return ""

</t>
<t tx="ekr.20230831011820.1859">def maybe_unwrap_unary_expr(self, expr: Expression) -&gt; Expression:
    """Unwrap (possibly nested) unary expressions.

    But, some unary expressions can change the type of expression.
    While we want to preserve it. For example, `~True` is `int`.
    So, we only allow a subset of unary expressions to be unwrapped.
    """
    if not isinstance(expr, UnaryExpr):
        return expr

    # First, try to unwrap `[+-]+ (int|float|complex)` expr:
    math_ops = ("+", "-")
    if expr.op in math_ops:
        while isinstance(expr, UnaryExpr):
            if expr.op not in math_ops or not isinstance(
                expr.expr, (IntExpr, FloatExpr, ComplexExpr, UnaryExpr)
            ):
                break
            expr = expr.expr
        return expr

    # Next, try `not bool` expr:
    if expr.op == "not":
        while isinstance(expr, UnaryExpr):
            if expr.op != "not" or not isinstance(expr.expr, (NameExpr, UnaryExpr)):
                break
            if isinstance(expr.expr, NameExpr) and expr.expr.name not in ("True", "False"):
                break
            expr = expr.expr
        return expr

    # This is some other unary expr, we cannot do anything with it (yet?).
    return expr

</t>
<t tx="ekr.20230831011820.186">    def typed_function_untyped_decorator(self, func_name: str, context: Context) -&gt; None:
        self.fail(f'Untyped decorator makes function "{func_name}" untyped', context)

</t>
<t tx="ekr.20230831011820.1860">def print_annotation(self, t: Type) -&gt; str:
    printer = AnnotationPrinter(self)
    return t.accept(printer)

</t>
<t tx="ekr.20230831011820.1861">def is_top_level(self) -&gt; bool:
    """Are we processing the top level of a file?"""
    return self._indent == ""

</t>
<t tx="ekr.20230831011820.1862">def record_name(self, name: str) -&gt; None:
    """Mark a name as defined.

    This only does anything if at the top level of a module.
    """
    if self.is_top_level():
        self._toplevel_names.append(name)

</t>
<t tx="ekr.20230831011820.1863">def is_recorded_name(self, name: str) -&gt; bool:
    """Has this name been recorded previously?"""
    return self.is_top_level() and name in self._toplevel_names


</t>
<t tx="ekr.20230831011820.1864">def find_method_names(defs: list[Statement]) -&gt; set[str]:
    # TODO: Traverse into nested definitions
    result = set()
    for defn in defs:
        if isinstance(defn, FuncDef):
            result.add(defn.name)
        elif isinstance(defn, Decorator):
            result.add(defn.func.name)
        elif isinstance(defn, OverloadedFuncDef):
            for item in defn.items:
                result.update(find_method_names([item]))
    return result


</t>
<t tx="ekr.20230831011820.1865">class SelfTraverser(mypy.traverser.TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011820.1866">def __init__(self) -&gt; None:
    self.results: list[tuple[str, Expression]] = []

</t>
<t tx="ekr.20230831011820.1867">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    lvalue = o.lvalues[0]
    if (
        isinstance(lvalue, MemberExpr)
        and isinstance(lvalue.expr, NameExpr)
        and lvalue.expr.name == "self"
    ):
        self.results.append((lvalue.name, o.rvalue))


</t>
<t tx="ekr.20230831011820.1868">def find_self_initializers(fdef: FuncBase) -&gt; list[tuple[str, Expression]]:
    """Find attribute initializers in a method.

    Return a list of pairs (attribute name, r.h.s. expression).
    """
    traverser = SelfTraverser()
    fdef.accept(traverser)
    return traverser.results


</t>
<t tx="ekr.20230831011820.1869">def get_qualified_name(o: Expression) -&gt; str:
    if isinstance(o, NameExpr):
        return o.name
    elif isinstance(o, MemberExpr):
        return f"{get_qualified_name(o.expr)}.{o.name}"
    else:
        return ERROR_MARKER


</t>
<t tx="ekr.20230831011820.187">    def bad_proto_variance(
        self, actual: int, tvar_name: str, expected: int, context: Context
    ) -&gt; None:
        msg = capitalize(
            '{} type variable "{}" used in protocol where'
            " {} one is expected".format(
                variance_string(actual), tvar_name, variance_string(expected)
            )
        )
        self.fail(msg, context)

</t>
<t tx="ekr.20230831011820.1870">def remove_blacklisted_modules(modules: list[StubSource]) -&gt; list[StubSource]:
    return [
        module for module in modules if module.path is None or not is_blacklisted_path(module.path)
    ]


</t>
<t tx="ekr.20230831011820.1871">def is_blacklisted_path(path: str) -&gt; bool:
    return any(substr in (normalize_path_separators(path) + "\n") for substr in BLACKLIST)


</t>
<t tx="ekr.20230831011820.1872">def normalize_path_separators(path: str) -&gt; str:
    if sys.platform == "win32":
        return path.replace("\\", "/")
    return path


</t>
<t tx="ekr.20230831011820.1873">def collect_build_targets(
    options: Options, mypy_opts: MypyOptions
) -&gt; tuple[list[StubSource], list[StubSource]]:
    """Collect files for which we need to generate stubs.

    Return list of Python modules and C modules.
    """
    if options.packages or options.modules:
        if options.no_import:
            py_modules = find_module_paths_using_search(
                options.modules, options.packages, options.search_path, options.pyversion
            )
            c_modules: list[StubSource] = []
        else:
            # Using imports is the default, since we can also find C modules.
            py_modules, c_modules = find_module_paths_using_imports(
                options.modules, options.packages, options.verbose, options.quiet
            )
    else:
        # Use mypy native source collection for files and directories.
        try:
            source_list = create_source_list(options.files, mypy_opts)
        except InvalidSourceList as e:
            raise SystemExit(str(e)) from e
        py_modules = [StubSource(m.module, m.path) for m in source_list]
        c_modules = []

    py_modules = remove_blacklisted_modules(py_modules)

    return py_modules, c_modules


</t>
<t tx="ekr.20230831011820.1874">def find_module_paths_using_imports(
    modules: list[str], packages: list[str], verbose: bool, quiet: bool
) -&gt; tuple[list[StubSource], list[StubSource]]:
    """Find path and runtime value of __all__ (if possible) for modules and packages.

    This function uses runtime Python imports to get the information.
    """
    with ModuleInspect() as inspect:
        py_modules: list[StubSource] = []
        c_modules: list[StubSource] = []
        found = list(walk_packages(inspect, packages, verbose))
        modules = modules + found
        modules = [
            mod for mod in modules if not is_non_library_module(mod)
        ]  # We don't want to run any tests or scripts
        for mod in modules:
            try:
                result = find_module_path_and_all_py3(inspect, mod, verbose)
            except CantImport as e:
                tb = traceback.format_exc()
                if verbose:
                    sys.stdout.write(tb)
                if not quiet:
                    report_missing(mod, e.message, tb)
                continue
            if not result:
                c_modules.append(StubSource(mod))
            else:
                path, runtime_all = result
                py_modules.append(StubSource(mod, path, runtime_all))
        return py_modules, c_modules


</t>
<t tx="ekr.20230831011820.1875">def is_non_library_module(module: str) -&gt; bool:
    """Does module look like a test module or a script?"""
    if module.endswith(
        (
            ".tests",
            ".test",
            ".testing",
            "_tests",
            "_test_suite",
            "test_util",
            "test_utils",
            "test_base",
            ".__main__",
            ".conftest",  # Used by pytest
            ".setup",  # Typically an install script
        )
    ):
        return True
    if module.split(".")[-1].startswith("test_"):
        return True
    if (
        ".tests." in module
        or ".test." in module
        or ".testing." in module
        or ".SelfTest." in module
    ):
        return True
    return False


</t>
<t tx="ekr.20230831011820.1876">def translate_module_name(module: str, relative: int) -&gt; tuple[str, int]:
    for pkg in VENDOR_PACKAGES:
        for alt in "six.moves", "six":
            substr = f"{pkg}.{alt}"
            if module.endswith("." + substr) or (module == substr and relative):
                return alt, 0
            if "." + substr + "." in module:
                return alt + "." + module.partition("." + substr + ".")[2], 0
    return module, relative


</t>
<t tx="ekr.20230831011820.1877">def find_module_paths_using_search(
    modules: list[str], packages: list[str], search_path: list[str], pyversion: tuple[int, int]
) -&gt; list[StubSource]:
    """Find sources for modules and packages requested.

    This function just looks for source files at the file system level.
    This is used if user passes --no-import, and will not find C modules.
    Exit if some of the modules or packages can't be found.
    """
    result: list[StubSource] = []
    typeshed_path = default_lib_path(mypy.build.default_data_dir(), pyversion, None)
    search_paths = SearchPaths((".",) + tuple(search_path), (), (), tuple(typeshed_path))
    cache = FindModuleCache(search_paths, fscache=None, options=None)
    for module in modules:
        m_result = cache.find_module(module)
        if isinstance(m_result, ModuleNotFoundReason):
            fail_missing(module, m_result)
            module_path = None
        else:
            module_path = m_result
        result.append(StubSource(module, module_path))
    for package in packages:
        p_result = cache.find_modules_recursive(package)
        if p_result:
            fail_missing(package, ModuleNotFoundReason.NOT_FOUND)
        sources = [StubSource(m.module, m.path) for m in p_result]
        result.extend(sources)

    result = [m for m in result if not is_non_library_module(m.module)]

    return result


</t>
<t tx="ekr.20230831011820.1878">def mypy_options(stubgen_options: Options) -&gt; MypyOptions:
    """Generate mypy options using the flag passed by user."""
    options = MypyOptions()
    options.follow_imports = "skip"
    options.incremental = False
    options.ignore_errors = True
    options.semantic_analysis_only = True
    options.python_version = stubgen_options.pyversion
    options.show_traceback = True
    options.transform_source = remove_misplaced_type_comments
    options.preserve_asts = True
    options.include_docstrings = stubgen_options.include_docstrings

    # Override cache_dir if provided in the environment
    environ_cache_dir = os.getenv("MYPY_CACHE_DIR", "")
    if environ_cache_dir.strip():
        options.cache_dir = environ_cache_dir
    options.cache_dir = os.path.expanduser(options.cache_dir)

    return options


</t>
<t tx="ekr.20230831011820.1879">def parse_source_file(mod: StubSource, mypy_options: MypyOptions) -&gt; None:
    """Parse a source file.

    On success, store AST in the corresponding attribute of the stub source.
    If there are syntax errors, print them and exit.
    """
    assert mod.path is not None, "Not found module was not skipped"
    with open(mod.path, "rb") as f:
        data = f.read()
    source = mypy.util.decode_python_encoding(data)
    errors = Errors(mypy_options)
    mod.ast = mypy.parse.parse(
        source, fnam=mod.path, module=mod.module, errors=errors, options=mypy_options
    )
    mod.ast._fullname = mod.module
    if errors.is_blockers():
        # Syntax error!
        for m in errors.new_messages():
            sys.stderr.write(f"{m}\n")
        sys.exit(1)


</t>
<t tx="ekr.20230831011820.188">    def concrete_only_assign(self, typ: Type, context: Context) -&gt; None:
        self.fail(
            f"Can only assign concrete classes to a variable of type {format_type(typ, self.options)}",
            context,
            code=codes.TYPE_ABSTRACT,
        )

</t>
<t tx="ekr.20230831011820.1880">def generate_asts_for_modules(
    py_modules: list[StubSource], parse_only: bool, mypy_options: MypyOptions, verbose: bool
) -&gt; None:
    """Use mypy to parse (and optionally analyze) source files."""
    if not py_modules:
        return  # Nothing to do here, but there may be C modules
    if verbose:
        print(f"Processing {len(py_modules)} files...")
    if parse_only:
        for mod in py_modules:
            parse_source_file(mod, mypy_options)
        return
    # Perform full semantic analysis of the source set.
    try:
        res = build([module.source for module in py_modules], mypy_options)
    except CompileError as e:
        raise SystemExit(f"Critical error during semantic analysis: {e}") from e

    for mod in py_modules:
        mod.ast = res.graph[mod.module].tree
        # Use statically inferred __all__ if there is no runtime one.
        if mod.runtime_all is None:
            mod.runtime_all = res.manager.semantic_analyzer.export_map[mod.module]


</t>
<t tx="ekr.20230831011820.1881">def generate_stub_from_ast(
    mod: StubSource,
    target: str,
    parse_only: bool = False,
    include_private: bool = False,
    export_less: bool = False,
    include_docstrings: bool = False,
) -&gt; None:
    """Use analysed (or just parsed) AST to generate type stub for single file.

    If directory for target doesn't exist it will created. Existing stub
    will be overwritten.
    """
    gen = StubGenerator(
        mod.runtime_all,
        include_private=include_private,
        analyzed=not parse_only,
        export_less=export_less,
        include_docstrings=include_docstrings,
    )
    assert mod.ast is not None, "This function must be used only with analyzed modules"
    mod.ast.accept(gen)

    # Write output to file.
    subdir = os.path.dirname(target)
    if subdir and not os.path.isdir(subdir):
        os.makedirs(subdir)
    with open(target, "w") as file:
        file.write("".join(gen.output()))


</t>
<t tx="ekr.20230831011820.1882">def get_sig_generators(options: Options) -&gt; list[SignatureGenerator]:
    sig_generators: list[SignatureGenerator] = [
        DocstringSignatureGenerator(),
        FallbackSignatureGenerator(),
    ]
    if options.doc_dir:
        # Collect info from docs (if given). Always check these first.
        sigs, class_sigs = collect_docs_signatures(options.doc_dir)
        sig_generators.insert(0, ExternalSignatureGenerator(sigs, class_sigs))
    return sig_generators


</t>
<t tx="ekr.20230831011820.1883">def collect_docs_signatures(doc_dir: str) -&gt; tuple[dict[str, str], dict[str, str]]:
    """Gather all function and class signatures in the docs.

    Return a tuple (function signatures, class signatures).
    Currently only used for C modules.
    """
    all_sigs: list[Sig] = []
    all_class_sigs: list[Sig] = []
    for path in glob.glob(f"{doc_dir}/*.rst"):
        with open(path) as f:
            loc_sigs, loc_class_sigs = parse_all_signatures(f.readlines())
        all_sigs += loc_sigs
        all_class_sigs += loc_class_sigs
    sigs = dict(find_unique_signatures(all_sigs))
    class_sigs = dict(find_unique_signatures(all_class_sigs))
    return sigs, class_sigs


</t>
<t tx="ekr.20230831011820.1884">def generate_stubs(options: Options) -&gt; None:
    """Main entry point for the program."""
    mypy_opts = mypy_options(options)
    py_modules, c_modules = collect_build_targets(options, mypy_opts)
    sig_generators = get_sig_generators(options)
    # Use parsed sources to generate stubs for Python modules.
    generate_asts_for_modules(py_modules, options.parse_only, mypy_opts, options.verbose)
    files = []
    for mod in py_modules:
        assert mod.path is not None, "Not found module was not skipped"
        target = mod.module.replace(".", "/")
        if os.path.basename(mod.path) == "__init__.py":
            target += "/__init__.pyi"
        else:
            target += ".pyi"
        target = os.path.join(options.output_dir, target)
        files.append(target)
        with generate_guarded(mod.module, target, options.ignore_errors, options.verbose):
            generate_stub_from_ast(
                mod,
                target,
                options.parse_only,
                options.include_private,
                options.export_less,
                include_docstrings=options.include_docstrings,
            )

    # Separately analyse C modules using different logic.
    all_modules = sorted(m.module for m in (py_modules + c_modules))
    for mod in c_modules:
        if any(py_mod.module.startswith(mod.module + ".") for py_mod in py_modules + c_modules):
            target = mod.module.replace(".", "/") + "/__init__.pyi"
        else:
            target = mod.module.replace(".", "/") + ".pyi"
        target = os.path.join(options.output_dir, target)
        files.append(target)
        with generate_guarded(mod.module, target, options.ignore_errors, options.verbose):
            generate_stub_for_c_module(
                mod.module,
                target,
                known_modules=all_modules,
                sig_generators=sig_generators,
                include_docstrings=options.include_docstrings,
            )
    num_modules = len(py_modules) + len(c_modules)
    if not options.quiet and num_modules &gt; 0:
        print("Processed %d modules" % num_modules)
        if len(files) == 1:
            print(f"Generated {files[0]}")
        else:
            print(f"Generated files under {common_dir_prefix(files)}" + os.sep)


</t>
<t tx="ekr.20230831011820.1885">HEADER = """%(prog)s [-h] [more options, see -h]
                     [-m MODULE] [-p PACKAGE] [files ...]"""

DESCRIPTION = """
Generate draft stubs for modules.

Stubs are generated in directory ./out, to avoid overriding files with
manual changes.  This directory is assumed to exist.
"""


def parse_options(args: list[str]) -&gt; Options:
    parser = argparse.ArgumentParser(prog="stubgen", usage=HEADER, description=DESCRIPTION)

    parser.add_argument(
        "--ignore-errors",
        action="store_true",
        help="ignore errors when trying to generate stubs for modules",
    )
    parser.add_argument(
        "--no-import",
        action="store_true",
        help="don't import the modules, just parse and analyze them "
        "(doesn't work with C extension modules and might not "
        "respect __all__)",
    )
    parser.add_argument(
        "--parse-only",
        action="store_true",
        help="don't perform semantic analysis of sources, just parse them "
        "(only applies to Python modules, might affect quality of stubs)",
    )
    parser.add_argument(
        "--include-private",
        action="store_true",
        help="generate stubs for objects and members considered private "
        "(single leading underscore and no trailing underscores)",
    )
    parser.add_argument(
        "--export-less",
        action="store_true",
        help="don't implicitly export all names imported from other modules in the same package",
    )
    parser.add_argument(
        "--include-docstrings",
        action="store_true",
        help="include existing docstrings with the stubs",
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="show more verbose messages")
    parser.add_argument("-q", "--quiet", action="store_true", help="show fewer messages")
    parser.add_argument(
        "--doc-dir",
        metavar="PATH",
        default="",
        help="use .rst documentation in PATH (this may result in "
        "better stubs in some cases; consider setting this to "
        "DIR/Python-X.Y.Z/Doc/library)",
    )
    parser.add_argument(
        "--search-path",
        metavar="PATH",
        default="",
        help="specify module search directories, separated by ':' "
        "(currently only used if --no-import is given)",
    )
    parser.add_argument(
        "-o",
        "--output",
        metavar="PATH",
        dest="output_dir",
        default="out",
        help="change the output directory [default: %(default)s]",
    )
    parser.add_argument(
        "-m",
        "--module",
        action="append",
        metavar="MODULE",
        dest="modules",
        default=[],
        help="generate stub for module; can repeat for more modules",
    )
    parser.add_argument(
        "-p",
        "--package",
        action="append",
        metavar="PACKAGE",
        dest="packages",
        default=[],
        help="generate stubs for package recursively; can be repeated",
    )
    parser.add_argument(
        metavar="files",
        nargs="*",
        dest="files",
        help="generate stubs for given files or directories",
    )

    ns = parser.parse_args(args)

    pyversion = sys.version_info[:2]
    ns.interpreter = sys.executable

    if ns.modules + ns.packages and ns.files:
        parser.error("May only specify one of: modules/packages or files.")
    if ns.quiet and ns.verbose:
        parser.error("Cannot specify both quiet and verbose messages")

    # Create the output folder if it doesn't already exist.
    if not os.path.exists(ns.output_dir):
        os.makedirs(ns.output_dir)

    return Options(
        pyversion=pyversion,
        no_import=ns.no_import,
        doc_dir=ns.doc_dir,
        search_path=ns.search_path.split(":"),
        interpreter=ns.interpreter,
        ignore_errors=ns.ignore_errors,
        parse_only=ns.parse_only,
        include_private=ns.include_private,
        output_dir=ns.output_dir,
        modules=ns.modules,
        packages=ns.packages,
        files=ns.files,
        verbose=ns.verbose,
        quiet=ns.quiet,
        export_less=ns.export_less,
        include_docstrings=ns.include_docstrings,
    )


</t>
<t tx="ekr.20230831011820.1886">def main(args: list[str] | None = None) -&gt; None:
    mypy.util.check_python_version("stubgen")
    # Make sure that the current directory is in sys.path so that
    # stubgen can be run on packages in the current directory.
    if not ("" in sys.path or "." in sys.path):
        sys.path.insert(0, "")

    options = parse_options(sys.argv[1:] if args is None else args)
    generate_stubs(options)
</t>
<t tx="ekr.20230831011820.1887">@path mypy
&lt;&lt; stubgenc.py: docstring &gt;&gt;
&lt;&lt; stubgenc.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1888">#!/usr/bin/env python3
"""Stub generator for C modules.

The public interface is via the mypy.stubgen module.
"""

</t>
<t tx="ekr.20230831011820.1889">from __future__ import annotations

import importlib
import inspect
import os.path
import re
from abc import abstractmethod
from types import ModuleType
from typing import Any, Final, Iterable, Mapping

import mypy.util
from mypy.moduleinspect import is_c_module
from mypy.stubdoc import (
    ArgSig,
    FunctionSig,
    infer_arg_sig_from_anon_docstring,
    infer_prop_type_from_docstring,
    infer_ret_type_sig_from_anon_docstring,
    infer_ret_type_sig_from_docstring,
    infer_sig_from_docstring,
)

# Members of the typing module to consider for importing by default.
_DEFAULT_TYPING_IMPORTS: Final = (
    "Any",
    "Callable",
    "ClassVar",
    "Dict",
    "Iterable",
    "Iterator",
    "List",
    "Optional",
    "Tuple",
    "Union",
)


</t>
<t tx="ekr.20230831011820.189">    def concrete_only_call(self, typ: Type, context: Context) -&gt; None:
        self.fail(
            f"Only concrete class can be given where {format_type(typ, self.options)} is expected",
            context,
            code=codes.TYPE_ABSTRACT,
        )

</t>
<t tx="ekr.20230831011820.1890">class SignatureGenerator:
    """Abstract base class for extracting a list of FunctionSigs for each function."""

    @others
</t>
<t tx="ekr.20230831011820.1891">def remove_self_type(
    self, inferred: list[FunctionSig] | None, self_var: str
) -&gt; list[FunctionSig] | None:
    """Remove type annotation from self/cls argument"""
    if inferred:
        for signature in inferred:
            if signature.args:
                if signature.args[0].name == self_var:
                    signature.args[0].type = None
    return inferred

</t>
<t tx="ekr.20230831011820.1892">@abstractmethod
def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    pass

</t>
<t tx="ekr.20230831011820.1893">@abstractmethod
def get_method_sig(
    self, cls: type, func: object, module_name: str, class_name: str, name: str, self_var: str
) -&gt; list[FunctionSig] | None:
    pass


</t>
<t tx="ekr.20230831011820.1894">class ExternalSignatureGenerator(SignatureGenerator):
    @others
</t>
<t tx="ekr.20230831011820.1895">def __init__(
    self, func_sigs: dict[str, str] | None = None, class_sigs: dict[str, str] | None = None
):
    """
    Takes a mapping of function/method names to signatures and class name to
    class signatures (usually corresponds to __init__).
    """
    self.func_sigs = func_sigs or {}
    self.class_sigs = class_sigs or {}

</t>
<t tx="ekr.20230831011820.1896">def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    if name in self.func_sigs:
        return [
            FunctionSig(
                name=name,
                args=infer_arg_sig_from_anon_docstring(self.func_sigs[name]),
                ret_type="Any",
            )
        ]
    else:
        return None

</t>
<t tx="ekr.20230831011820.1897">def get_method_sig(
    self, cls: type, func: object, module_name: str, class_name: str, name: str, self_var: str
) -&gt; list[FunctionSig] | None:
    if (
        name in ("__new__", "__init__")
        and name not in self.func_sigs
        and class_name in self.class_sigs
    ):
        return [
            FunctionSig(
                name=name,
                args=infer_arg_sig_from_anon_docstring(self.class_sigs[class_name]),
                ret_type=infer_method_ret_type(name),
            )
        ]
    inferred = self.get_function_sig(func, module_name, name)
    return self.remove_self_type(inferred, self_var)


</t>
<t tx="ekr.20230831011820.1898">class DocstringSignatureGenerator(SignatureGenerator):
    @others
</t>
<t tx="ekr.20230831011820.1899">def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    docstr = getattr(func, "__doc__", None)
    inferred = infer_sig_from_docstring(docstr, name)
    if inferred:
        assert docstr is not None
        if is_pybind11_overloaded_function_docstring(docstr, name):
            # Remove pybind11 umbrella (*args, **kwargs) for overloaded functions
            del inferred[-1]
    return inferred

</t>
<t tx="ekr.20230831011820.19">def is_enum_overlapping_union(x: ProperType, y: ProperType) -&gt; bool:
    """Return True if x is an Enum, and y is an Union with at least one Literal from x"""
    return (
        isinstance(x, Instance)
        and x.type.is_enum
        and isinstance(y, UnionType)
        and any(
            isinstance(p, LiteralType) and x.type == p.fallback.type
            for p in (get_proper_type(z) for z in y.relevant_items())
        )
    )


</t>
<t tx="ekr.20230831011820.190">    def cannot_use_function_with_type(
        self, method_name: str, type_name: str, context: Context
    ) -&gt; None:
        self.fail(f"Cannot use {method_name}() with {type_name} type", context)

</t>
<t tx="ekr.20230831011820.1900">def get_method_sig(
    self,
    cls: type,
    func: object,
    module_name: str,
    class_name: str,
    func_name: str,
    self_var: str,
) -&gt; list[FunctionSig] | None:
    inferred = self.get_function_sig(func, module_name, func_name)
    if not inferred and func_name == "__init__":
        # look for class-level constructor signatures of the form &lt;class_name&gt;(&lt;signature&gt;)
        inferred = self.get_function_sig(cls, module_name, class_name)
    return self.remove_self_type(inferred, self_var)


</t>
<t tx="ekr.20230831011820.1901">class FallbackSignatureGenerator(SignatureGenerator):
    @others
</t>
<t tx="ekr.20230831011820.1902">def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    return [
        FunctionSig(
            name=name,
            args=infer_arg_sig_from_anon_docstring("(*args, **kwargs)"),
            ret_type="Any",
        )
    ]

</t>
<t tx="ekr.20230831011820.1903">def get_method_sig(
    self, cls: type, func: object, module_name: str, class_name: str, name: str, self_var: str
) -&gt; list[FunctionSig] | None:
    return [
        FunctionSig(
            name=name,
            args=infer_method_args(name, self_var),
            ret_type=infer_method_ret_type(name),
        )
    ]


</t>
<t tx="ekr.20230831011820.1904">def generate_stub_for_c_module(
    module_name: str,
    target: str,
    known_modules: list[str],
    sig_generators: Iterable[SignatureGenerator],
    include_docstrings: bool = False,
) -&gt; None:
    """Generate stub for C module.

    Signature generators are called in order until a list of signatures is returned.  The order
    is:
    - signatures inferred from .rst documentation (if given)
    - simple runtime introspection (looking for docstrings and attributes
      with simple builtin types)
    - fallback based special method names or "(*args, **kwargs)"

    If directory for target doesn't exist it will be created. Existing stub
    will be overwritten.
    """
    module = importlib.import_module(module_name)
    assert is_c_module(module), f"{module_name} is not a C module"
    subdir = os.path.dirname(target)
    if subdir and not os.path.isdir(subdir):
        os.makedirs(subdir)
    imports: list[str] = []
    functions: list[str] = []
    done = set()
    items = sorted(get_members(module), key=lambda x: x[0])
    for name, obj in items:
        if is_c_function(obj):
            generate_c_function_stub(
                module,
                name,
                obj,
                output=functions,
                known_modules=known_modules,
                imports=imports,
                sig_generators=sig_generators,
                include_docstrings=include_docstrings,
            )
            done.add(name)
    types: list[str] = []
    for name, obj in items:
        if name.startswith("__") and name.endswith("__"):
            continue
        if is_c_type(obj):
            generate_c_type_stub(
                module,
                name,
                obj,
                output=types,
                known_modules=known_modules,
                imports=imports,
                sig_generators=sig_generators,
                include_docstrings=include_docstrings,
            )
            done.add(name)
    variables = []
    for name, obj in items:
        if name.startswith("__") and name.endswith("__"):
            continue
        if name not in done and not inspect.ismodule(obj):
            type_str = strip_or_import(
                get_type_fullname(type(obj)), module, known_modules, imports
            )
            variables.append(f"{name}: {type_str}")
    output = sorted(set(imports))
    for line in variables:
        output.append(line)
    for line in types:
        if line.startswith("class") and output and output[-1]:
            output.append("")
        output.append(line)
    if output and functions:
        output.append("")
    for line in functions:
        output.append(line)
    output = add_typing_import(output)
    with open(target, "w") as file:
        for line in output:
            file.write(f"{line}\n")


</t>
<t tx="ekr.20230831011820.1905">def add_typing_import(output: list[str]) -&gt; list[str]:
    """Add typing imports for collections/types that occur in the generated stub."""
    names = []
    for name in _DEFAULT_TYPING_IMPORTS:
        if any(re.search(r"\b%s\b" % name, line) for line in output):
            names.append(name)
    if names:
        return [f"from typing import {', '.join(names)}", ""] + output
    else:
        return output.copy()


</t>
<t tx="ekr.20230831011820.1906">def get_members(obj: object) -&gt; list[tuple[str, Any]]:
    obj_dict: Mapping[str, Any] = getattr(obj, "__dict__")  # noqa: B009
    results = []
    for name in obj_dict:
        if is_skipped_attribute(name):
            continue
        # Try to get the value via getattr
        try:
            value = getattr(obj, name)
        except AttributeError:
            continue
        else:
            results.append((name, value))
    return results


</t>
<t tx="ekr.20230831011820.1907">def is_c_function(obj: object) -&gt; bool:
    return inspect.isbuiltin(obj) or type(obj) is type(ord)


</t>
<t tx="ekr.20230831011820.1908">def is_c_method(obj: object) -&gt; bool:
    return inspect.ismethoddescriptor(obj) or type(obj) in (
        type(str.index),
        type(str.__add__),
        type(str.__new__),
    )


</t>
<t tx="ekr.20230831011820.1909">def is_c_classmethod(obj: object) -&gt; bool:
    return inspect.isbuiltin(obj) or type(obj).__name__ in (
        "classmethod",
        "classmethod_descriptor",
    )


</t>
<t tx="ekr.20230831011820.191">    def report_non_method_protocol(
        self, tp: TypeInfo, members: list[str], context: Context
    ) -&gt; None:
        self.fail(
            "Only protocols that don't have non-method members can be used with issubclass()",
            context,
        )
        if len(members) &lt; 3:
            attrs = ", ".join(members)
            self.note(f'Protocol "{tp.name}" has non-method member(s): {attrs}', context)

</t>
<t tx="ekr.20230831011820.1910">def is_c_property(obj: object) -&gt; bool:
    return inspect.isdatadescriptor(obj) or hasattr(obj, "fget")


</t>
<t tx="ekr.20230831011820.1911">def is_c_property_readonly(prop: Any) -&gt; bool:
    return hasattr(prop, "fset") and prop.fset is None


</t>
<t tx="ekr.20230831011820.1912">def is_c_type(obj: object) -&gt; bool:
    return inspect.isclass(obj) or type(obj) is type(int)


</t>
<t tx="ekr.20230831011820.1913">def is_pybind11_overloaded_function_docstring(docstr: str, name: str) -&gt; bool:
    return docstr.startswith(f"{name}(*args, **kwargs)\n" + "Overloaded function.\n\n")


</t>
<t tx="ekr.20230831011820.1914">def generate_c_function_stub(
    module: ModuleType,
    name: str,
    obj: object,
    *,
    known_modules: list[str],
    sig_generators: Iterable[SignatureGenerator],
    output: list[str],
    imports: list[str],
    self_var: str | None = None,
    cls: type | None = None,
    class_name: str | None = None,
    include_docstrings: bool = False,
) -&gt; None:
    """Generate stub for a single function or method.

    The result will be appended to 'output'.
    If necessary, any required names will be added to 'imports'.
    The 'class_name' is used to find signature of __init__ or __new__ in
    'class_sigs'.
    """
    inferred: list[FunctionSig] | None = None
    docstr: str | None = None
    if class_name:
        # method:
        assert cls is not None, "cls should be provided for methods"
        assert self_var is not None, "self_var should be provided for methods"
        for sig_gen in sig_generators:
            inferred = sig_gen.get_method_sig(
                cls, obj, module.__name__, class_name, name, self_var
            )
            if inferred:
                # add self/cls var, if not present
                for sig in inferred:
                    if not sig.args or sig.args[0].name not in ("self", "cls"):
                        sig.args.insert(0, ArgSig(name=self_var))
                break
    else:
        # function:
        for sig_gen in sig_generators:
            inferred = sig_gen.get_function_sig(obj, module.__name__, name)
            if inferred:
                break

    if not inferred:
        raise ValueError(
            "No signature was found. This should never happen "
            "if FallbackSignatureGenerator is provided"
        )

    is_overloaded = len(inferred) &gt; 1 if inferred else False
    if is_overloaded:
        imports.append("from typing import overload")
    if inferred:
        for signature in inferred:
            args: list[str] = []
            for arg in signature.args:
                arg_def = arg.name
                if arg_def == "None":
                    arg_def = "_none"  # None is not a valid argument name

                if arg.type:
                    arg_def += ": " + strip_or_import(arg.type, module, known_modules, imports)

                if arg.default:
                    arg_def += " = ..."

                args.append(arg_def)

            if is_overloaded:
                output.append("@overload")
            # a sig generator indicates @classmethod by specifying the cls arg
            if class_name and signature.args and signature.args[0].name == "cls":
                output.append("@classmethod")
            output_signature = "def {function}({args}) -&gt; {ret}:".format(
                function=name,
                args=", ".join(args),
                ret=strip_or_import(signature.ret_type, module, known_modules, imports),
            )
            if include_docstrings and docstr:
                docstr_quoted = mypy.util.quote_docstring(docstr.strip())
                docstr_indented = "\n    ".join(docstr_quoted.split("\n"))
                output.append(output_signature)
                output.extend(f"    {docstr_indented}".split("\n"))
            else:
                output_signature += " ..."
                output.append(output_signature)


</t>
<t tx="ekr.20230831011820.1915">def strip_or_import(
    typ: str, module: ModuleType, known_modules: list[str], imports: list[str]
) -&gt; str:
    """Strips unnecessary module names from typ.

    If typ represents a type that is inside module or is a type coming from builtins, remove
    module declaration from it. Return stripped name of the type.

    Arguments:
        typ: name of the type
        module: in which this type is used
        known_modules: other modules being processed
        imports: list of import statements (may be modified during the call)
    """
    local_modules = ["builtins"]
    if module:
        local_modules.append(module.__name__)

    stripped_type = typ
    if any(c in typ for c in "[,"):
        for subtyp in re.split(r"[\[,\]]", typ):
            stripped_subtyp = strip_or_import(subtyp.strip(), module, known_modules, imports)
            if stripped_subtyp != subtyp:
                stripped_type = re.sub(
                    r"(^|[\[, ]+)" + re.escape(subtyp) + r"($|[\], ]+)",
                    r"\1" + stripped_subtyp + r"\2",
                    stripped_type,
                )
    elif "." in typ:
        for module_name in local_modules + list(reversed(known_modules)):
            if typ.startswith(module_name + "."):
                if module_name in local_modules:
                    stripped_type = typ[len(module_name) + 1 :]
                arg_module = module_name
                break
        else:
            arg_module = typ[: typ.rindex(".")]
        if arg_module not in local_modules:
            imports.append(f"import {arg_module}")
    if stripped_type == "NoneType":
        stripped_type = "None"
    return stripped_type


</t>
<t tx="ekr.20230831011820.1916">def is_static_property(obj: object) -&gt; bool:
    return type(obj).__name__ == "pybind11_static_property"


</t>
<t tx="ekr.20230831011820.1917">def generate_c_property_stub(
    name: str,
    obj: object,
    static_properties: list[str],
    rw_properties: list[str],
    ro_properties: list[str],
    readonly: bool,
    module: ModuleType | None = None,
    known_modules: list[str] | None = None,
    imports: list[str] | None = None,
) -&gt; None:
    """Generate property stub using introspection of 'obj'.

    Try to infer type from docstring, append resulting lines to 'output'.
    """

    def infer_prop_type(docstr: str | None) -&gt; str | None:
        """Infer property type from docstring or docstring signature."""
        if docstr is not None:
            inferred = infer_ret_type_sig_from_anon_docstring(docstr)
            if not inferred:
                inferred = infer_ret_type_sig_from_docstring(docstr, name)
            if not inferred:
                inferred = infer_prop_type_from_docstring(docstr)
            return inferred
        else:
            return None

    inferred = infer_prop_type(getattr(obj, "__doc__", None))
    if not inferred:
        fget = getattr(obj, "fget", None)
        inferred = infer_prop_type(getattr(fget, "__doc__", None))
    if not inferred:
        inferred = "Any"

    if module is not None and imports is not None and known_modules is not None:
        inferred = strip_or_import(inferred, module, known_modules, imports)

    if is_static_property(obj):
        trailing_comment = "  # read-only" if readonly else ""
        static_properties.append(f"{name}: ClassVar[{inferred}] = ...{trailing_comment}")
    else:  # regular property
        if readonly:
            ro_properties.append("@property")
            ro_properties.append(f"def {name}(self) -&gt; {inferred}: ...")
        else:
            rw_properties.append(f"{name}: {inferred}")


</t>
<t tx="ekr.20230831011820.1918">def generate_c_type_stub(
    module: ModuleType,
    class_name: str,
    obj: type,
    output: list[str],
    known_modules: list[str],
    imports: list[str],
    sig_generators: Iterable[SignatureGenerator],
    include_docstrings: bool = False,
) -&gt; None:
    """Generate stub for a single class using runtime introspection.

    The result lines will be appended to 'output'. If necessary, any
    required names will be added to 'imports'.
    """
    raw_lookup = getattr(obj, "__dict__")  # noqa: B009
    items = sorted(get_members(obj), key=lambda x: method_name_sort_key(x[0]))
    names = {x[0] for x in items}
    methods: list[str] = []
    types: list[str] = []
    static_properties: list[str] = []
    rw_properties: list[str] = []
    ro_properties: list[str] = []
    attrs: list[tuple[str, Any]] = []
    for attr, value in items:
        # use unevaluated descriptors when dealing with property inspection
        raw_value = raw_lookup.get(attr, value)
        if is_c_method(value) or is_c_classmethod(value):
            if attr == "__new__":
                # TODO: We should support __new__.
                if "__init__" in names:
                    # Avoid duplicate functions if both are present.
                    # But is there any case where .__new__() has a
                    # better signature than __init__() ?
                    continue
                attr = "__init__"
            if is_c_classmethod(value):
                self_var = "cls"
            else:
                self_var = "self"
            generate_c_function_stub(
                module,
                attr,
                value,
                output=methods,
                known_modules=known_modules,
                imports=imports,
                self_var=self_var,
                cls=obj,
                class_name=class_name,
                sig_generators=sig_generators,
                include_docstrings=include_docstrings,
            )
        elif is_c_property(raw_value):
            generate_c_property_stub(
                attr,
                raw_value,
                static_properties,
                rw_properties,
                ro_properties,
                is_c_property_readonly(raw_value),
                module=module,
                known_modules=known_modules,
                imports=imports,
            )
        elif is_c_type(value):
            generate_c_type_stub(
                module,
                attr,
                value,
                types,
                imports=imports,
                known_modules=known_modules,
                sig_generators=sig_generators,
                include_docstrings=include_docstrings,
            )
        else:
            attrs.append((attr, value))

    for attr, value in attrs:
        static_properties.append(
            "{}: ClassVar[{}] = ...".format(
                attr,
                strip_or_import(get_type_fullname(type(value)), module, known_modules, imports),
            )
        )
    all_bases = type.mro(obj)
    if all_bases[-1] is object:
        # TODO: Is this always object?
        del all_bases[-1]
    # remove pybind11_object. All classes generated by pybind11 have pybind11_object in their MRO,
    # which only overrides a few functions in object type
    if all_bases and all_bases[-1].__name__ == "pybind11_object":
        del all_bases[-1]
    # remove the class itself
    all_bases = all_bases[1:]
    # Remove base classes of other bases as redundant.
    bases: list[type] = []
    for base in all_bases:
        if not any(issubclass(b, base) for b in bases):
            bases.append(base)
    if bases:
        bases_str = "(%s)" % ", ".join(
            strip_or_import(get_type_fullname(base), module, known_modules, imports)
            for base in bases
        )
    else:
        bases_str = ""
    if types or static_properties or rw_properties or methods or ro_properties:
        output.append(f"class {class_name}{bases_str}:")
        for line in types:
            if (
                output
                and output[-1]
                and not output[-1].startswith("class")
                and line.startswith("class")
            ):
                output.append("")
            output.append("    " + line)
        for line in static_properties:
            output.append(f"    {line}")
        for line in rw_properties:
            output.append(f"    {line}")
        for line in methods:
            output.append(f"    {line}")
        for line in ro_properties:
            output.append(f"    {line}")
    else:
        output.append(f"class {class_name}{bases_str}: ...")


</t>
<t tx="ekr.20230831011820.1919">def get_type_fullname(typ: type) -&gt; str:
    return f"{typ.__module__}.{getattr(typ, '__qualname__', typ.__name__)}"


</t>
<t tx="ekr.20230831011820.192">    def note_call(
        self, subtype: Type, call: Type, context: Context, *, code: ErrorCode | None
    ) -&gt; None:
        self.note(
            '"{}.__call__" has type {}'.format(
                format_type_bare(subtype, self.options),
                format_type(call, self.options, verbosity=1),
            ),
            context,
            code=code,
        )

</t>
<t tx="ekr.20230831011820.1920">def method_name_sort_key(name: str) -&gt; tuple[int, str]:
    """Sort methods in classes in a typical order.

    I.e.: constructor, normal methods, special methods.
    """
    if name in ("__new__", "__init__"):
        return 0, name
    if name.startswith("__") and name.endswith("__"):
        return 2, name
    return 1, name


</t>
<t tx="ekr.20230831011820.1921">def is_pybind_skipped_attribute(attr: str) -&gt; bool:
    return attr.startswith("__pybind11_module_local_")


</t>
<t tx="ekr.20230831011820.1922">def is_skipped_attribute(attr: str) -&gt; bool:
    return attr in (
        "__class__",
        "__getattribute__",
        "__str__",
        "__repr__",
        "__doc__",
        "__dict__",
        "__module__",
        "__weakref__",
    ) or is_pybind_skipped_attribute(  # For pickling
        attr
    )


</t>
<t tx="ekr.20230831011820.1923">def infer_method_args(name: str, self_var: str | None = None) -&gt; list[ArgSig]:
    args: list[ArgSig] | None = None
    if name.startswith("__") and name.endswith("__"):
        name = name[2:-2]
        if name in (
            "hash",
            "iter",
            "next",
            "sizeof",
            "copy",
            "deepcopy",
            "reduce",
            "getinitargs",
            "int",
            "float",
            "trunc",
            "complex",
            "bool",
            "abs",
            "bytes",
            "dir",
            "len",
            "reversed",
            "round",
            "index",
            "enter",
        ):
            args = []
        elif name == "getitem":
            args = [ArgSig(name="index")]
        elif name == "setitem":
            args = [ArgSig(name="index"), ArgSig(name="object")]
        elif name in ("delattr", "getattr"):
            args = [ArgSig(name="name")]
        elif name == "setattr":
            args = [ArgSig(name="name"), ArgSig(name="value")]
        elif name == "getstate":
            args = []
        elif name == "setstate":
            args = [ArgSig(name="state")]
        elif name in (
            "eq",
            "ne",
            "lt",
            "le",
            "gt",
            "ge",
            "add",
            "radd",
            "sub",
            "rsub",
            "mul",
            "rmul",
            "mod",
            "rmod",
            "floordiv",
            "rfloordiv",
            "truediv",
            "rtruediv",
            "divmod",
            "rdivmod",
            "pow",
            "rpow",
            "xor",
            "rxor",
            "or",
            "ror",
            "and",
            "rand",
            "lshift",
            "rlshift",
            "rshift",
            "rrshift",
            "contains",
            "delitem",
            "iadd",
            "iand",
            "ifloordiv",
            "ilshift",
            "imod",
            "imul",
            "ior",
            "ipow",
            "irshift",
            "isub",
            "itruediv",
            "ixor",
        ):
            args = [ArgSig(name="other")]
        elif name in ("neg", "pos", "invert"):
            args = []
        elif name == "get":
            args = [ArgSig(name="instance"), ArgSig(name="owner")]
        elif name == "set":
            args = [ArgSig(name="instance"), ArgSig(name="value")]
        elif name == "reduce_ex":
            args = [ArgSig(name="protocol")]
        elif name == "exit":
            args = [ArgSig(name="type"), ArgSig(name="value"), ArgSig(name="traceback")]
    if args is None:
        args = [ArgSig(name="*args"), ArgSig(name="**kwargs")]
    return [ArgSig(name=self_var or "self")] + args


</t>
<t tx="ekr.20230831011820.1924">def infer_method_ret_type(name: str) -&gt; str:
    if name.startswith("__") and name.endswith("__"):
        name = name[2:-2]
        if name in ("float", "bool", "bytes", "int"):
            return name
        # Note: __eq__ and co may return arbitrary types, but bool is good enough for stubgen.
        elif name in ("eq", "ne", "lt", "le", "gt", "ge", "contains"):
            return "bool"
        elif name in ("len", "hash", "sizeof", "trunc", "floor", "ceil"):
            return "int"
        elif name in ("init", "setitem"):
            return "None"
    return "Any"
</t>
<t tx="ekr.20230831011820.1925">@path mypy
&lt;&lt; stubinfo.py: preamble &gt;&gt;
@others


# Stubs for these third-party packages used to be shipped with mypy.
#
# Map package name to PyPI stub distribution name.
legacy_bundled_packages = {
    "aiofiles": "types-aiofiles",
    "bleach": "types-bleach",
    "boto": "types-boto",
    "cachetools": "types-cachetools",
    "click_spinner": "types-click-spinner",
    "contextvars": "types-contextvars",
    "croniter": "types-croniter",
    "dataclasses": "types-dataclasses",
    "dateparser": "types-dateparser",
    "datetimerange": "types-DateTimeRange",
    "dateutil": "types-python-dateutil",
    "decorator": "types-decorator",
    "deprecated": "types-Deprecated",
    "docutils": "types-docutils",
    "first": "types-first",
    "geoip2": "types-geoip2",
    "gflags": "types-python-gflags",
    "google.protobuf": "types-protobuf",
    "markdown": "types-Markdown",
    "maxminddb": "types-maxminddb",
    "mock": "types-mock",
    "OpenSSL": "types-pyOpenSSL",
    "paramiko": "types-paramiko",
    "pkg_resources": "types-setuptools",
    "polib": "types-polib",
    "pycurl": "types-pycurl",
    "pymysql": "types-PyMySQL",
    "pyrfc3339": "types-pyRFC3339",
    "python2": "types-six",
    "pytz": "types-pytz",
    "pyVmomi": "types-pyvmomi",
    "redis": "types-redis",
    "requests": "types-requests",
    "retry": "types-retry",
    "simplejson": "types-simplejson",
    "singledispatch": "types-singledispatch",
    "six": "types-six",
    "slugify": "types-python-slugify",
    "tabulate": "types-tabulate",
    "toml": "types-toml",
    "typed_ast": "types-typed-ast",
    "tzlocal": "types-tzlocal",
    "ujson": "types-ujson",
    "waitress": "types-waitress",
    "yaml": "types-PyYAML",
}

# Map package name to PyPI stub distribution name from typeshed.
# Stubs for these packages were never bundled with mypy. Don't
# include packages that have a release that includes PEP 561 type
# information.
#
# Package name can have one or two components ('a' or 'a.b').
#
# Note that these packages are omitted for now:
#   pika:       typeshed's stubs are on PyPI as types-pika-ts.
#               types-pika already exists on PyPI, and is more complete in many ways,
#               but is a non-typeshed stubs package.
non_bundled_packages = {
    "MySQLdb": "types-mysqlclient",
    "PIL": "types-Pillow",
    "PyInstaller": "types-pyinstaller",
    "Xlib": "types-python-xlib",
    "annoy": "types-annoy",
    "appdirs": "types-appdirs",
    "aws_xray_sdk": "types-aws-xray-sdk",
    "babel": "types-babel",
    "backports.ssl_match_hostname": "types-backports.ssl_match_hostname",
    "braintree": "types-braintree",
    "bs4": "types-beautifulsoup4",
    "bugbear": "types-flake8-bugbear",
    "caldav": "types-caldav",
    "cffi": "types-cffi",
    "chevron": "types-chevron",
    "colorama": "types-colorama",
    "commonmark": "types-commonmark",
    "consolemenu": "types-console-menu",
    "crontab": "types-python-crontab",
    "d3dshot": "types-D3DShot",
    "dj_database_url": "types-dj-database-url",
    "dockerfile_parse": "types-dockerfile-parse",
    "docopt": "types-docopt",
    "editdistance": "types-editdistance",
    "entrypoints": "types-entrypoints",
    "farmhash": "types-pyfarmhash",
    "flake8_2020": "types-flake8-2020",
    "flake8_builtins": "types-flake8-builtins",
    "flake8_docstrings": "types-flake8-docstrings",
    "flake8_plugin_utils": "types-flake8-plugin-utils",
    "flake8_rst_docstrings": "types-flake8-rst-docstrings",
    "flake8_simplify": "types-flake8-simplify",
    "flake8_typing_imports": "types-flake8-typing-imports",
    "flask_cors": "types-Flask-Cors",
    "flask_migrate": "types-Flask-Migrate",
    "flask_sqlalchemy": "types-Flask-SQLAlchemy",
    "fpdf": "types-fpdf2",
    "gdb": "types-gdb",
    "google.cloud.ndb": "types-google-cloud-ndb",
    "hdbcli": "types-hdbcli",
    "html5lib": "types-html5lib",
    "httplib2": "types-httplib2",
    "humanfriendly": "types-humanfriendly",
    "invoke": "types-invoke",
    "jack": "types-JACK-Client",
    "jmespath": "types-jmespath",
    "jose": "types-python-jose",
    "jsonschema": "types-jsonschema",
    "keyboard": "types-keyboard",
    "ldap3": "types-ldap3",
    "nmap": "types-python-nmap",
    "oauthlib": "types-oauthlib",
    "openpyxl": "types-openpyxl",
    "opentracing": "types-opentracing",
    "paho.mqtt": "types-paho-mqtt",
    "parsimonious": "types-parsimonious",
    "passlib": "types-passlib",
    "passpy": "types-passpy",
    "peewee": "types-peewee",
    "pep8ext_naming": "types-pep8-naming",
    "playsound": "types-playsound",
    "psutil": "types-psutil",
    "psycopg2": "types-psycopg2",
    "pyaudio": "types-pyaudio",
    "pyautogui": "types-PyAutoGUI",
    "pycocotools": "types-pycocotools",
    "pyflakes": "types-pyflakes",
    "pygments": "types-Pygments",
    "pyi_splash": "types-pyinstaller",
    "pynput": "types-pynput",
    "pythoncom": "types-pywin32",
    "pythonwin": "types-pywin32",
    "pyscreeze": "types-PyScreeze",
    "pysftp": "types-pysftp",
    "pytest_lazyfixture": "types-pytest-lazy-fixture",
    "pywintypes": "types-pywin32",
    "regex": "types-regex",
    "send2trash": "types-Send2Trash",
    "slumber": "types-slumber",
    "stdlib_list": "types-stdlib-list",
    "stripe": "types-stripe",
    "toposort": "types-toposort",
    "tqdm": "types-tqdm",
    "tree_sitter": "types-tree-sitter",
    "tree_sitter_languages": "types-tree-sitter-languages",
    "ttkthemes": "types-ttkthemes",
    "urllib3": "types-urllib3",
    "vobject": "types-vobject",
    "whatthepatch": "types-whatthepatch",
    "win32": "types-pywin32",
    "win32api": "types-pywin32",
    "win32con": "types-pywin32",
    "win32com": "types-pywin32",
    "win32comext": "types-pywin32",
    "win32gui": "types-pywin32",
    "xmltodict": "types-xmltodict",
    "xxhash": "types-xxhash",
    "zxcvbn": "types-zxcvbn",
    # Stub packages that are not from typeshed
    # Since these can be installed automatically via --install-types, we have a high trust bar
    # for additions here
    "pandas": "pandas-stubs",  # https://github.com/pandas-dev/pandas-stubs
    "lxml": "lxml-stubs",  # https://github.com/lxml/lxml-stubs
}
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1926">from __future__ import annotations


</t>
<t tx="ekr.20230831011820.1927">def is_legacy_bundled_package(prefix: str) -&gt; bool:
    return prefix in legacy_bundled_packages


</t>
<t tx="ekr.20230831011820.1928">def approved_stub_package_exists(prefix: str) -&gt; bool:
    return is_legacy_bundled_package(prefix) or prefix in non_bundled_packages


</t>
<t tx="ekr.20230831011820.1929">def stub_distribution_name(prefix: str) -&gt; str:
    return legacy_bundled_packages.get(prefix) or non_bundled_packages[prefix]
</t>
<t tx="ekr.20230831011820.193">    def unreachable_statement(self, context: Context) -&gt; None:
        self.fail("Statement is unreachable", context, code=codes.UNREACHABLE)

</t>
<t tx="ekr.20230831011820.1930">@path mypy
"""Tests for stubs.

Verify that various things in stubs are consistent with how things behave at runtime.

"""

&lt;&lt; stubtest.py: declarations &gt;&gt;
@others


if __name__ == "__main__":
    sys.exit(main())
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1932">from __future__ import annotations

import argparse
import collections.abc
import copy
import enum
import importlib
import importlib.machinery
import inspect
import os
import pkgutil
import re
import symtable
import sys
import traceback
import types
import typing
import typing_extensions
import warnings
from contextlib import redirect_stderr, redirect_stdout
from functools import singledispatch
from pathlib import Path
from typing import AbstractSet, Any, Generic, Iterator, TypeVar, Union
from typing_extensions import get_origin, is_typeddict

import mypy.build
import mypy.modulefinder
import mypy.nodes
import mypy.state
import mypy.types
import mypy.version
from mypy import nodes
from mypy.config_parser import parse_config_file
from mypy.evalexpr import UNKNOWN, evaluate_expression
from mypy.options import Options
from mypy.util import FancyFormatter, bytes_to_human_readable_repr, is_dunder, plural_s


</t>
<t tx="ekr.20230831011820.1933">class Missing:
    """Marker object for things that are missing (from a stub or the runtime)."""

    @others
</t>
<t tx="ekr.20230831011820.1934">def __repr__(self) -&gt; str:
    return "MISSING"


</t>
<t tx="ekr.20230831011820.1935">MISSING: typing_extensions.Final = Missing()

T = TypeVar("T")
MaybeMissing: typing_extensions.TypeAlias = Union[T, Missing]

_formatter: typing_extensions.Final = FancyFormatter(sys.stdout, sys.stderr, False)


def _style(message: str, **kwargs: Any) -&gt; str:
    """Wrapper around mypy.util for fancy formatting."""
    kwargs.setdefault("color", "none")
    return _formatter.style(message, **kwargs)


</t>
<t tx="ekr.20230831011820.1936">def _truncate(message: str, length: int) -&gt; str:
    if len(message) &gt; length:
        return message[: length - 3] + "..."
    return message


</t>
<t tx="ekr.20230831011820.1937">class StubtestFailure(Exception):
    pass


</t>
<t tx="ekr.20230831011820.1938">class Error:
    @others
</t>
<t tx="ekr.20230831011820.1939">def __init__(
    self,
    object_path: list[str],
    message: str,
    stub_object: MaybeMissing[nodes.Node],
    runtime_object: MaybeMissing[Any],
    *,
    stub_desc: str | None = None,
    runtime_desc: str | None = None,
) -&gt; None:
    """Represents an error found by stubtest.

    :param object_path: Location of the object with the error,
        e.g. ``["module", "Class", "method"]``
    :param message: Error message
    :param stub_object: The mypy node representing the stub
    :param runtime_object: Actual object obtained from the runtime
    :param stub_desc: Specialised description for the stub object, should you wish
    :param runtime_desc: Specialised description for the runtime object, should you wish

    """
    self.object_path = object_path
    self.object_desc = ".".join(object_path)
    self.message = message
    self.stub_object = stub_object
    self.runtime_object = runtime_object
    self.stub_desc = stub_desc or str(getattr(stub_object, "type", stub_object))
    self.runtime_desc = runtime_desc or _truncate(repr(runtime_object), 100)

</t>
<t tx="ekr.20230831011820.194">    def redundant_left_operand(self, op_name: str, context: Context) -&gt; None:
        """Indicates that the left operand of a boolean expression is redundant:
        it does not change the truth value of the entire condition as a whole.
        'op_name' should either be the string "and" or the string "or".
        """
        self.redundant_expr(f'Left operand of "{op_name}"', op_name == "and", context)

</t>
<t tx="ekr.20230831011820.1940">def is_missing_stub(self) -&gt; bool:
    """Whether or not the error is for something missing from the stub."""
    return isinstance(self.stub_object, Missing)

</t>
<t tx="ekr.20230831011820.1941">def is_positional_only_related(self) -&gt; bool:
    """Whether or not the error is for something being (or not being) positional-only."""
    # TODO: This is hacky, use error codes or something more resilient
    return "leading double underscore" in self.message

</t>
<t tx="ekr.20230831011820.1942">def get_description(self, concise: bool = False) -&gt; str:
    """Returns a description of the error.

    :param concise: Whether to return a concise, one-line description

    """
    if concise:
        return _style(self.object_desc, bold=True) + " " + self.message

    stub_line = None
    stub_file = None
    if not isinstance(self.stub_object, Missing):
        stub_line = self.stub_object.line
    stub_node = get_stub(self.object_path[0])
    if stub_node is not None:
        stub_file = stub_node.path or None

    stub_loc_str = ""
    if stub_file:
        stub_loc_str += f" in file {Path(stub_file)}"
    if stub_line:
        stub_loc_str += f"{':' if stub_file else ' at line '}{stub_line}"

    runtime_line = None
    runtime_file = None
    if not isinstance(self.runtime_object, Missing):
        try:
            runtime_line = inspect.getsourcelines(self.runtime_object)[1]
        except (OSError, TypeError, SyntaxError):
            pass
        try:
            runtime_file = inspect.getsourcefile(self.runtime_object)
        except TypeError:
            pass

    runtime_loc_str = ""
    if runtime_file:
        runtime_loc_str += f" in file {Path(runtime_file)}"
    if runtime_line:
        runtime_loc_str += f"{':' if runtime_file else ' at line '}{runtime_line}"

    output = [
        _style("error: ", color="red", bold=True),
        _style(self.object_desc, bold=True),
        " ",
        self.message,
        "\n",
        "Stub:",
        _style(stub_loc_str, dim=True),
        "\n",
        _style(self.stub_desc + "\n", color="blue", dim=True),
        "Runtime:",
        _style(runtime_loc_str, dim=True),
        "\n",
        _style(self.runtime_desc + "\n", color="blue", dim=True),
    ]
    return "".join(output)


</t>
<t tx="ekr.20230831011820.1943"># ====================
# Core logic
# ====================


def silent_import_module(module_name: str) -&gt; types.ModuleType:
    with open(os.devnull, "w") as devnull:
        with warnings.catch_warnings(), redirect_stdout(devnull), redirect_stderr(devnull):
            warnings.simplefilter("ignore")
            runtime = importlib.import_module(module_name)
            # Also run the equivalent of `from module import *`
            # This could have the additional effect of loading not-yet-loaded submodules
            # mentioned in __all__
            __import__(module_name, fromlist=["*"])
    return runtime


</t>
<t tx="ekr.20230831011820.1944">def test_module(module_name: str) -&gt; Iterator[Error]:
    """Tests a given module's stub against introspecting it at runtime.

    Requires the stub to have been built already, accomplished by a call to ``build_stubs``.

    :param module_name: The module to test

    """
    stub = get_stub(module_name)
    if stub is None:
        if not is_probably_private(module_name.split(".")[-1]):
            runtime_desc = repr(sys.modules[module_name]) if module_name in sys.modules else "N/A"
            yield Error(
                [module_name], "failed to find stubs", MISSING, None, runtime_desc=runtime_desc
            )
        return

    try:
        runtime = silent_import_module(module_name)
    except KeyboardInterrupt:
        raise
    except BaseException as e:
        note = ""
        if isinstance(e, ModuleNotFoundError):
            note = " Maybe install the runtime package or alter PYTHONPATH?"
        yield Error(
            [module_name], f"failed to import.{note} {type(e).__name__}: {e}", stub, MISSING
        )
        return

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        try:
            yield from verify(stub, runtime, [module_name])
        except Exception as e:
            bottom_frame = list(traceback.walk_tb(e.__traceback__))[-1][0]
            bottom_module = bottom_frame.f_globals.get("__name__", "")
            # Pass on any errors originating from stubtest or mypy
            # These can occur expectedly, e.g. StubtestFailure
            if bottom_module == "__main__" or bottom_module.split(".")[0] == "mypy":
                raise
            yield Error(
                [module_name],
                f"encountered unexpected error, {type(e).__name__}: {e}",
                stub,
                runtime,
                stub_desc="N/A",
                runtime_desc=(
                    "This is most likely the fault of something very dynamic in your library. "
                    "It's also possible this is a bug in stubtest.\nIf in doubt, please "
                    "open an issue at https://github.com/python/mypy\n\n"
                    + traceback.format_exc().strip()
                ),
            )


</t>
<t tx="ekr.20230831011820.1945">@singledispatch
def verify(
    stub: MaybeMissing[nodes.Node], runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    """Entry point for comparing a stub to a runtime object.

    We use single dispatch based on the type of ``stub``.

    :param stub: The mypy node representing a part of the stub
    :param runtime: The runtime object corresponding to ``stub``

    """
    yield Error(object_path, "is an unknown mypy node", stub, runtime)


</t>
<t tx="ekr.20230831011820.1946">def _verify_exported_names(
    object_path: list[str], stub: nodes.MypyFile, runtime_all_as_set: set[str]
) -&gt; Iterator[Error]:
    # note that this includes the case the stub simply defines `__all__: list[str]`
    assert "__all__" in stub.names
    public_names_in_stub = {m for m, o in stub.names.items() if o.module_public}
    names_in_stub_not_runtime = sorted(public_names_in_stub - runtime_all_as_set)
    names_in_runtime_not_stub = sorted(runtime_all_as_set - public_names_in_stub)
    if not (names_in_runtime_not_stub or names_in_stub_not_runtime):
        return
    yield Error(
        object_path + ["__all__"],
        (
            "names exported from the stub do not correspond to the names exported at runtime. "
            "This is probably due to things being missing from the stub or an inaccurate `__all__` in the stub"
        ),
        # Pass in MISSING instead of the stub and runtime objects, as the line numbers aren't very
        # relevant here, and it makes for a prettier error message
        # This means this error will be ignored when using `--ignore-missing-stub`, which is
        # desirable in at least the `names_in_runtime_not_stub` case
        stub_object=MISSING,
        runtime_object=MISSING,
        stub_desc=(
            f"Names exported in the stub but not at runtime: " f"{names_in_stub_not_runtime}"
        ),
        runtime_desc=(
            f"Names exported at runtime but not in the stub: " f"{names_in_runtime_not_stub}"
        ),
    )


</t>
<t tx="ekr.20230831011820.1947">def _get_imported_symbol_names(runtime: types.ModuleType) -&gt; frozenset[str] | None:
    """Retrieve the names in the global namespace which are known to be imported.

    1). Use inspect to retrieve the source code of the module
    2). Use symtable to parse the source and retrieve names that are known to be imported
        from other modules.

    If either of the above steps fails, return `None`.

    Note that if a set of names is returned,
    it won't include names imported via `from foo import *` imports.
    """
    try:
        source = inspect.getsource(runtime)
    except (OSError, TypeError, SyntaxError):
        return None

    if not source.strip():
        # The source code for the module was an empty file,
        # no point in parsing it with symtable
        return frozenset()

    try:
        module_symtable = symtable.symtable(source, runtime.__name__, "exec")
    except SyntaxError:
        return None

    return frozenset(sym.get_name() for sym in module_symtable.get_symbols() if sym.is_imported())


</t>
<t tx="ekr.20230831011820.1948">@verify.register(nodes.MypyFile)
def verify_mypyfile(
    stub: nodes.MypyFile, runtime: MaybeMissing[types.ModuleType], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if not isinstance(runtime, types.ModuleType):
        yield Error(object_path, "is not a module", stub, runtime)
        return

    runtime_all_as_set: set[str] | None

    if hasattr(runtime, "__all__"):
        runtime_all_as_set = set(runtime.__all__)
        if "__all__" in stub.names:
            # Only verify the contents of the stub's __all__
            # if the stub actually defines __all__
            yield from _verify_exported_names(object_path, stub, runtime_all_as_set)
    else:
        runtime_all_as_set = None

    # Check things in the stub
    to_check = {
        m
        for m, o in stub.names.items()
        if not o.module_hidden and (not is_probably_private(m) or hasattr(runtime, m))
    }

    imported_symbols = _get_imported_symbol_names(runtime)

    def _belongs_to_runtime(r: types.ModuleType, attr: str) -&gt; bool:
        """Heuristics to determine whether a name originates from another module."""
        obj = getattr(r, attr)
        if isinstance(obj, types.ModuleType):
            return False
        if callable(obj):
            # It's highly likely to be a class or a function if it's callable,
            # so the __module__ attribute will give a good indication of which module it comes from
            try:
                obj_mod = obj.__module__
            except Exception:
                pass
            else:
                if isinstance(obj_mod, str):
                    return bool(obj_mod == r.__name__)
        if imported_symbols is not None:
            return attr not in imported_symbols
        return True

    runtime_public_contents = (
        runtime_all_as_set
        if runtime_all_as_set is not None
        else {
            m
            for m in dir(runtime)
            if not is_probably_private(m)
            # Filter out objects that originate from other modules (best effort). Note that in the
            # absence of __all__, we don't have a way to detect explicit / intentional re-exports
            # at runtime
            and _belongs_to_runtime(runtime, m)
        }
    )
    # Check all things declared in module's __all__, falling back to our best guess
    to_check.update(runtime_public_contents)
    to_check.difference_update(IGNORED_MODULE_DUNDERS)

    for entry in sorted(to_check):
        stub_entry = stub.names[entry].node if entry in stub.names else MISSING
        if isinstance(stub_entry, nodes.MypyFile):
            # Don't recursively check exported modules, since that leads to infinite recursion
            continue
        assert stub_entry is not None
        try:
            runtime_entry = getattr(runtime, entry, MISSING)
        except Exception:
            # Catch all exceptions in case the runtime raises an unexpected exception
            # from __getattr__ or similar.
            continue
        yield from verify(stub_entry, runtime_entry, object_path + [entry])


</t>
<t tx="ekr.20230831011820.1949">def _verify_final(
@others

    except TypeError:
        # Enum classes are implicitly @final
        if not stub.is_final and not issubclass(runtime, enum.Enum):
            yield Error(
                object_path,
                "cannot be subclassed at runtime, but isn't marked with @final in the stub",
                stub,
                runtime,
                stub_desc=repr(stub),
            )
    except Exception:
        # The class probably wants its subclasses to do something special.
        # Examples: ctypes.Array, ctypes._SimpleCData
        pass

    # Runtime class might be annotated with `@final`:
    try:
        runtime_final = getattr(runtime, "__final__", False)
    except Exception:
        runtime_final = False

    if runtime_final and not stub.is_final:
        yield Error(
            object_path,
            "has `__final__` attribute, but isn't marked with @final in the stub",
            stub,
            runtime,
            stub_desc=repr(stub),
        )


</t>
<t tx="ekr.20230831011820.195">    def unreachable_right_operand(self, op_name: str, context: Context) -&gt; None:
        """Indicates that the right operand of a boolean expression is redundant:
        it does not change the truth value of the entire condition as a whole.
        'op_name' should either be the string "and" or the string "or".
        """
        self.fail(
            f'Right operand of "{op_name}" is never evaluated', context, code=codes.UNREACHABLE
        )

</t>
<t tx="ekr.20230831011820.1950">    stub: nodes.TypeInfo, runtime: type[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    try:

        class SubClass(runtime):  # type: ignore[misc]
            pass
</t>
<t tx="ekr.20230831011820.1951">def _verify_metaclass(
    stub: nodes.TypeInfo, runtime: type[Any], object_path: list[str], *, is_runtime_typeddict: bool
) -&gt; Iterator[Error]:
    # We exclude protocols, because of how complex their implementation is in different versions of
    # python. Enums are also hard, as are runtime TypedDicts; ignoring.
    # TODO: check that metaclasses are identical?
    if not stub.is_protocol and not stub.is_enum and not is_runtime_typeddict:
        runtime_metaclass = type(runtime)
        if runtime_metaclass is not type and stub.metaclass_type is None:
            # This means that runtime has a custom metaclass, but a stub does not.
            yield Error(
                object_path,
                "is inconsistent, metaclass differs",
                stub,
                runtime,
                stub_desc="N/A",
                runtime_desc=f"{runtime_metaclass}",
            )
        elif (
            runtime_metaclass is type
            and stub.metaclass_type is not None
            # We ignore extra `ABCMeta` metaclass on stubs, this might be typing hack.
            # We also ignore `builtins.type` metaclass as an implementation detail in mypy.
            and not mypy.types.is_named_instance(
                stub.metaclass_type, ("abc.ABCMeta", "builtins.type")
            )
        ):
            # This means that our stub has a metaclass that is not present at runtime.
            yield Error(
                object_path,
                "metaclass mismatch",
                stub,
                runtime,
                stub_desc=f"{stub.metaclass_type.type.fullname}",
                runtime_desc="N/A",
            )


</t>
<t tx="ekr.20230831011820.1952">@verify.register(nodes.TypeInfo)
def verify_typeinfo(
    stub: nodes.TypeInfo, runtime: MaybeMissing[type[Any]], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime, stub_desc=repr(stub))
        return
    if not isinstance(runtime, type):
        yield Error(object_path, "is not a type", stub, runtime, stub_desc=repr(stub))
        return

    yield from _verify_final(stub, runtime, object_path)
    is_runtime_typeddict = stub.typeddict_type is not None and is_typeddict(runtime)
    yield from _verify_metaclass(
        stub, runtime, object_path, is_runtime_typeddict=is_runtime_typeddict
    )

    # Check everything already defined on the stub class itself (i.e. not inherited)
    #
    # Filter out non-identifier names, as these are (hopefully always?) whacky/fictional things
    # (like __mypy-replace or __mypy-post_init, etc.) that don't exist at runtime,
    # and exist purely for internal mypy reasons
    to_check = {name for name in stub.names if name.isidentifier()}
    # Check all public things on the runtime class
    to_check.update(
        m for m in vars(runtime) if not is_probably_private(m) and m not in IGNORABLE_CLASS_DUNDERS
    )
    # Special-case the __init__ method for Protocols and the __new__ method for TypedDicts
    #
    # TODO: On Python &lt;3.11, __init__ methods on Protocol classes
    # are silently discarded and replaced.
    # However, this is not the case on Python 3.11+.
    # Ideally, we'd figure out a good way of validating Protocol __init__ methods on 3.11+.
    if stub.is_protocol:
        to_check.discard("__init__")
    if is_runtime_typeddict:
        to_check.discard("__new__")

    for entry in sorted(to_check):
        mangled_entry = entry
        if entry.startswith("__") and not entry.endswith("__"):
            mangled_entry = f"_{stub.name.lstrip('_')}{entry}"
        stub_to_verify = next((t.names[entry].node for t in stub.mro if entry in t.names), MISSING)
        assert stub_to_verify is not None
        try:
            try:
                runtime_attr = getattr(runtime, mangled_entry)
            except AttributeError:
                runtime_attr = inspect.getattr_static(runtime, mangled_entry, MISSING)
        except Exception:
            # Catch all exceptions in case the runtime raises an unexpected exception
            # from __getattr__ or similar.
            continue
        # Do not error for an object missing from the stub
        # If the runtime object is a types.WrapperDescriptorType object
        # and has a non-special dunder name.
        # The vast majority of these are false positives.
        if not (
            isinstance(stub_to_verify, Missing)
            and isinstance(runtime_attr, types.WrapperDescriptorType)
            and is_dunder(mangled_entry, exclude_special=True)
        ):
            yield from verify(stub_to_verify, runtime_attr, object_path + [entry])


</t>
<t tx="ekr.20230831011820.1953">def _static_lookup_runtime(object_path: list[str]) -&gt; MaybeMissing[Any]:
    static_runtime = importlib.import_module(object_path[0])
    for entry in object_path[1:]:
        try:
            static_runtime = inspect.getattr_static(static_runtime, entry)
        except AttributeError:
            # This can happen with mangled names, ignore for now.
            # TODO: pass more information about ancestors of nodes/objects to verify, so we don't
            # have to do this hacky lookup. Would be useful in several places.
            return MISSING
    return static_runtime


</t>
<t tx="ekr.20230831011820.1954">def _verify_static_class_methods(
    stub: nodes.FuncBase, runtime: Any, static_runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[str]:
    if stub.name in ("__new__", "__init_subclass__", "__class_getitem__"):
        # Special cased by Python, so don't bother checking
        return
    if inspect.isbuiltin(runtime):
        # The isinstance checks don't work reliably for builtins, e.g. datetime.datetime.now, so do
        # something a little hacky that seems to work well
        probably_class_method = isinstance(getattr(runtime, "__self__", None), type)
        if probably_class_method and not stub.is_class:
            yield "runtime is a classmethod but stub is not"
        if not probably_class_method and stub.is_class:
            yield "stub is a classmethod but runtime is not"
        return

    if static_runtime is MISSING:
        return

    if isinstance(static_runtime, classmethod) and not stub.is_class:
        yield "runtime is a classmethod but stub is not"
    if not isinstance(static_runtime, classmethod) and stub.is_class:
        yield "stub is a classmethod but runtime is not"
    if isinstance(static_runtime, staticmethod) and not stub.is_static:
        yield "runtime is a staticmethod but stub is not"
    if not isinstance(static_runtime, staticmethod) and stub.is_static:
        yield "stub is a staticmethod but runtime is not"


</t>
<t tx="ekr.20230831011820.1955">def _verify_arg_name(
    stub_arg: nodes.Argument, runtime_arg: inspect.Parameter, function_name: str
) -&gt; Iterator[str]:
    """Checks whether argument names match."""
    # Ignore exact names for most dunder methods
    if is_dunder(function_name, exclude_special=True):
        return

    def strip_prefix(s: str, prefix: str) -&gt; str:
        return s[len(prefix) :] if s.startswith(prefix) else s

    if strip_prefix(stub_arg.variable.name, "__") == runtime_arg.name:
        return

    def names_approx_match(a: str, b: str) -&gt; bool:
        a = a.strip("_")
        b = b.strip("_")
        return a.startswith(b) or b.startswith(a) or len(a) == 1 or len(b) == 1

    # Be more permissive about names matching for positional-only arguments
    if runtime_arg.kind == inspect.Parameter.POSITIONAL_ONLY and names_approx_match(
        stub_arg.variable.name, runtime_arg.name
    ):
        return
    # This comes up with namedtuples, so ignore
    if stub_arg.variable.name == "_self":
        return
    yield (
        f'stub argument "{stub_arg.variable.name}" '
        f'differs from runtime argument "{runtime_arg.name}"'
    )


</t>
<t tx="ekr.20230831011820.1956">def _verify_arg_default_value(
    stub_arg: nodes.Argument, runtime_arg: inspect.Parameter
) -&gt; Iterator[str]:
    """Checks whether argument default values are compatible."""
    if runtime_arg.default != inspect.Parameter.empty:
        if stub_arg.kind.is_required():
            yield (
                f'runtime argument "{runtime_arg.name}" '
                "has a default value but stub argument does not"
            )
        else:
            runtime_type = get_mypy_type_of_runtime_value(runtime_arg.default)
            # Fallback to the type annotation type if var type is missing. The type annotation
            # is an UnboundType, but I don't know enough to know what the pros and cons here are.
            # UnboundTypes have ugly question marks following them, so default to var type.
            # Note we do this same fallback when constructing signatures in from_overloadedfuncdef
            stub_type = stub_arg.variable.type or stub_arg.type_annotation
            if isinstance(stub_type, mypy.types.TypeVarType):
                stub_type = stub_type.upper_bound
            if (
                runtime_type is not None
                and stub_type is not None
                # Avoid false positives for marker objects
                and type(runtime_arg.default) != object
                # And ellipsis
                and runtime_arg.default is not ...
                and not is_subtype_helper(runtime_type, stub_type)
            ):
                yield (
                    f'runtime argument "{runtime_arg.name}" '
                    f"has a default value of type {runtime_type}, "
                    f"which is incompatible with stub argument type {stub_type}"
                )
            if stub_arg.initializer is not None:
                stub_default = evaluate_expression(stub_arg.initializer)
                if (
                    stub_default is not UNKNOWN
                    and stub_default is not ...
                    and (
                        stub_default != runtime_arg.default
                        # We want the types to match exactly, e.g. in case the stub has
                        # True and the runtime has 1 (or vice versa).
                        or type(stub_default) is not type(runtime_arg.default)  # noqa: E721
                    )
                ):
                    yield (
                        f'runtime argument "{runtime_arg.name}" '
                        f"has a default value of {runtime_arg.default!r}, "
                        f"which is different from stub argument default {stub_default!r}"
                    )
    else:
        if stub_arg.kind.is_optional():
            yield (
                f'stub argument "{stub_arg.variable.name}" has a default value '
                f"but runtime argument does not"
            )


</t>
<t tx="ekr.20230831011820.1957">def maybe_strip_cls(name: str, args: list[nodes.Argument]) -&gt; list[nodes.Argument]:
    if args and name in ("__init_subclass__", "__class_getitem__"):
        # These are implicitly classmethods. If the stub chooses not to have @classmethod, we
        # should remove the cls argument
        if args[0].variable.name == "cls":
            return args[1:]
    return args


</t>
<t tx="ekr.20230831011820.1958">class Signature(Generic[T]):
    @others
</t>
<t tx="ekr.20230831011820.1959">def __init__(self) -&gt; None:
    self.pos: list[T] = []
    self.kwonly: dict[str, T] = {}
    self.varpos: T | None = None
    self.varkw: T | None = None

</t>
<t tx="ekr.20230831011820.196">    def redundant_condition_in_comprehension(self, truthiness: bool, context: Context) -&gt; None:
        self.redundant_expr("If condition in comprehension", truthiness, context)

</t>
<t tx="ekr.20230831011820.1960">def __str__(self) -&gt; str:
    def get_name(arg: Any) -&gt; str:
        if isinstance(arg, inspect.Parameter):
            return arg.name
        if isinstance(arg, nodes.Argument):
            return arg.variable.name
        raise AssertionError

    def get_type(arg: Any) -&gt; str | None:
        if isinstance(arg, inspect.Parameter):
            return None
        if isinstance(arg, nodes.Argument):
            return str(arg.variable.type or arg.type_annotation)
        raise AssertionError

    def has_default(arg: Any) -&gt; bool:
        if isinstance(arg, inspect.Parameter):
            return bool(arg.default != inspect.Parameter.empty)
        if isinstance(arg, nodes.Argument):
            return arg.kind.is_optional()
        raise AssertionError

    def get_desc(arg: Any) -&gt; str:
        arg_type = get_type(arg)
        return (
            get_name(arg)
            + (f": {arg_type}" if arg_type else "")
            + (" = ..." if has_default(arg) else "")
        )

    kw_only = sorted(self.kwonly.values(), key=lambda a: (has_default(a), get_name(a)))
    ret = "def ("
    ret += ", ".join(
        [get_desc(arg) for arg in self.pos]
        + (["*" + get_name(self.varpos)] if self.varpos else (["*"] if self.kwonly else []))
        + [get_desc(arg) for arg in kw_only]
        + (["**" + get_name(self.varkw)] if self.varkw else [])
    )
    ret += ")"
    return ret

</t>
<t tx="ekr.20230831011820.1961">@staticmethod
def from_funcitem(stub: nodes.FuncItem) -&gt; Signature[nodes.Argument]:
    stub_sig: Signature[nodes.Argument] = Signature()
    stub_args = maybe_strip_cls(stub.name, stub.arguments)
    for stub_arg in stub_args:
        if stub_arg.kind.is_positional():
            stub_sig.pos.append(stub_arg)
        elif stub_arg.kind.is_named():
            stub_sig.kwonly[stub_arg.variable.name] = stub_arg
        elif stub_arg.kind == nodes.ARG_STAR:
            stub_sig.varpos = stub_arg
        elif stub_arg.kind == nodes.ARG_STAR2:
            stub_sig.varkw = stub_arg
        else:
            raise AssertionError
    return stub_sig

</t>
<t tx="ekr.20230831011820.1962">@staticmethod
def from_inspect_signature(signature: inspect.Signature) -&gt; Signature[inspect.Parameter]:
    runtime_sig: Signature[inspect.Parameter] = Signature()
    for runtime_arg in signature.parameters.values():
        if runtime_arg.kind in (
            inspect.Parameter.POSITIONAL_ONLY,
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
        ):
            runtime_sig.pos.append(runtime_arg)
        elif runtime_arg.kind == inspect.Parameter.KEYWORD_ONLY:
            runtime_sig.kwonly[runtime_arg.name] = runtime_arg
        elif runtime_arg.kind == inspect.Parameter.VAR_POSITIONAL:
            runtime_sig.varpos = runtime_arg
        elif runtime_arg.kind == inspect.Parameter.VAR_KEYWORD:
            runtime_sig.varkw = runtime_arg
        else:
            raise AssertionError
    return runtime_sig

</t>
<t tx="ekr.20230831011820.1963">@staticmethod
def from_overloadedfuncdef(stub: nodes.OverloadedFuncDef) -&gt; Signature[nodes.Argument]:
    """Returns a Signature from an OverloadedFuncDef.

    If life were simple, to verify_overloadedfuncdef, we'd just verify_funcitem for each of its
    items. Unfortunately, life isn't simple and overloads are pretty deceitful. So instead, we
    try and combine the overload's items into a single signature that is compatible with any
    lies it might try to tell.

    """
    # For most dunder methods, just assume all args are positional-only
    assume_positional_only = is_dunder(stub.name, exclude_special=True)

    all_args: dict[str, list[tuple[nodes.Argument, int]]] = {}
    for func in map(_resolve_funcitem_from_decorator, stub.items):
        assert func is not None
        args = maybe_strip_cls(stub.name, func.arguments)
        for index, arg in enumerate(args):
            # For positional-only args, we allow overloads to have different names for the same
            # argument. To accomplish this, we just make up a fake index-based name.
            name = (
                f"__{index}"
                if arg.variable.name.startswith("__") or assume_positional_only
                else arg.variable.name
            )
            all_args.setdefault(name, []).append((arg, index))

    def get_position(arg_name: str) -&gt; int:
        # We just need this to return the positional args in the correct order.
        return max(index for _, index in all_args[arg_name])

    def get_type(arg_name: str) -&gt; mypy.types.ProperType:
        with mypy.state.state.strict_optional_set(True):
            all_types = [
                arg.variable.type or arg.type_annotation for arg, _ in all_args[arg_name]
            ]
            return mypy.typeops.make_simplified_union([t for t in all_types if t])

    def get_kind(arg_name: str) -&gt; nodes.ArgKind:
        kinds = {arg.kind for arg, _ in all_args[arg_name]}
        if nodes.ARG_STAR in kinds:
            return nodes.ARG_STAR
        if nodes.ARG_STAR2 in kinds:
            return nodes.ARG_STAR2
        # The logic here is based on two tenets:
        # 1) If an arg is ever optional (or unspecified), it is optional
        # 2) If an arg is ever positional, it is positional
        is_opt = (
            len(all_args[arg_name]) &lt; len(stub.items)
            or nodes.ARG_OPT in kinds
            or nodes.ARG_NAMED_OPT in kinds
        )
        is_pos = nodes.ARG_OPT in kinds or nodes.ARG_POS in kinds
        if is_opt:
            return nodes.ARG_OPT if is_pos else nodes.ARG_NAMED_OPT
        return nodes.ARG_POS if is_pos else nodes.ARG_NAMED

    sig: Signature[nodes.Argument] = Signature()
    for arg_name in sorted(all_args, key=get_position):
        # example_arg_name gives us a real name (in case we had a fake index-based name)
        example_arg_name = all_args[arg_name][0][0].variable.name
        arg = nodes.Argument(
            nodes.Var(example_arg_name, get_type(arg_name)),
            type_annotation=None,
            initializer=None,
            kind=get_kind(arg_name),
        )
        if arg.kind.is_positional():
            sig.pos.append(arg)
        elif arg.kind.is_named():
            sig.kwonly[arg.variable.name] = arg
        elif arg.kind == nodes.ARG_STAR:
            sig.varpos = arg
        elif arg.kind == nodes.ARG_STAR2:
            sig.varkw = arg
        else:
            raise AssertionError
    return sig


</t>
<t tx="ekr.20230831011820.1964">def _verify_signature(
    stub: Signature[nodes.Argument], runtime: Signature[inspect.Parameter], function_name: str
) -&gt; Iterator[str]:
    # Check positional arguments match up
    for stub_arg, runtime_arg in zip(stub.pos, runtime.pos):
        yield from _verify_arg_name(stub_arg, runtime_arg, function_name)
        yield from _verify_arg_default_value(stub_arg, runtime_arg)
        if (
            runtime_arg.kind == inspect.Parameter.POSITIONAL_ONLY
            and not stub_arg.pos_only
            and not stub_arg.variable.name.startswith("__")
            and not stub_arg.variable.name.strip("_") == "self"
            and not is_dunder(function_name, exclude_special=True)  # noisy for dunder methods
        ):
            yield (
                f'stub argument "{stub_arg.variable.name}" should be positional-only '
                f'(rename with a leading double underscore, i.e. "__{runtime_arg.name}")'
            )
        if (
            runtime_arg.kind != inspect.Parameter.POSITIONAL_ONLY
            and (stub_arg.pos_only or stub_arg.variable.name.startswith("__"))
            and not is_dunder(function_name, exclude_special=True)  # noisy for dunder methods
        ):
            yield (
                f'stub argument "{stub_arg.variable.name}" should be positional or keyword '
                "(remove leading double underscore)"
            )

    # Check unmatched positional args
    if len(stub.pos) &gt; len(runtime.pos):
        # There are cases where the stub exhaustively lists out the extra parameters the function
        # would take through *args. Hence, a) if runtime accepts *args, we don't check whether the
        # runtime has all of the stub's parameters, b) below, we don't enforce that the stub takes
        # *args, since runtime logic may prevent arbitrary arguments from actually being accepted.
        if runtime.varpos is None:
            for stub_arg in stub.pos[len(runtime.pos) :]:
                # If the variable is in runtime.kwonly, it's just mislabelled as not a
                # keyword-only argument
                if stub_arg.variable.name not in runtime.kwonly:
                    yield f'runtime does not have argument "{stub_arg.variable.name}"'
                else:
                    yield f'stub argument "{stub_arg.variable.name}" is not keyword-only'
            if stub.varpos is not None:
                yield f'runtime does not have *args argument "{stub.varpos.variable.name}"'
    elif len(stub.pos) &lt; len(runtime.pos):
        for runtime_arg in runtime.pos[len(stub.pos) :]:
            if runtime_arg.name not in stub.kwonly:
                yield f'stub does not have argument "{runtime_arg.name}"'
            else:
                yield f'runtime argument "{runtime_arg.name}" is not keyword-only'

    # Checks involving *args
    if len(stub.pos) &lt;= len(runtime.pos) or runtime.varpos is None:
        if stub.varpos is None and runtime.varpos is not None:
            yield f'stub does not have *args argument "{runtime.varpos.name}"'
        if stub.varpos is not None and runtime.varpos is None:
            yield f'runtime does not have *args argument "{stub.varpos.variable.name}"'

    # Check keyword-only args
    for arg in sorted(set(stub.kwonly) &amp; set(runtime.kwonly)):
        stub_arg, runtime_arg = stub.kwonly[arg], runtime.kwonly[arg]
        yield from _verify_arg_name(stub_arg, runtime_arg, function_name)
        yield from _verify_arg_default_value(stub_arg, runtime_arg)

    # Check unmatched keyword-only args
    if runtime.varkw is None or not set(runtime.kwonly).issubset(set(stub.kwonly)):
        # There are cases where the stub exhaustively lists out the extra parameters the function
        # would take through **kwargs. Hence, a) if runtime accepts **kwargs (and the stub hasn't
        # exhaustively listed out params), we don't check whether the runtime has all of the stub's
        # parameters, b) below, we don't enforce that the stub takes **kwargs, since runtime logic
        # may prevent arbitrary keyword arguments from actually being accepted.
        for arg in sorted(set(stub.kwonly) - set(runtime.kwonly)):
            if arg in {runtime_arg.name for runtime_arg in runtime.pos}:
                # Don't report this if we've reported it before
                if arg not in {runtime_arg.name for runtime_arg in runtime.pos[len(stub.pos) :]}:
                    yield f'runtime argument "{arg}" is not keyword-only'
            else:
                yield f'runtime does not have argument "{arg}"'
    for arg in sorted(set(runtime.kwonly) - set(stub.kwonly)):
        if arg in {stub_arg.variable.name for stub_arg in stub.pos}:
            # Don't report this if we've reported it before
            if not (
                runtime.varpos is None
                and arg in {stub_arg.variable.name for stub_arg in stub.pos[len(runtime.pos) :]}
            ):
                yield f'stub argument "{arg}" is not keyword-only'
        else:
            yield f'stub does not have argument "{arg}"'

    # Checks involving **kwargs
    if stub.varkw is None and runtime.varkw is not None:
        # As mentioned above, don't enforce that the stub takes **kwargs.
        # Also check against positional parameters, to avoid a nitpicky message when an argument
        # isn't marked as keyword-only
        stub_pos_names = {stub_arg.variable.name for stub_arg in stub.pos}
        # Ideally we'd do a strict subset check, but in practice the errors from that aren't useful
        if not set(runtime.kwonly).issubset(set(stub.kwonly) | stub_pos_names):
            yield f'stub does not have **kwargs argument "{runtime.varkw.name}"'
    if stub.varkw is not None and runtime.varkw is None:
        yield f'runtime does not have **kwargs argument "{stub.varkw.variable.name}"'


</t>
<t tx="ekr.20230831011820.1965">@verify.register(nodes.FuncItem)
def verify_funcitem(
    stub: nodes.FuncItem, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if not is_probably_a_function(runtime):
        yield Error(object_path, "is not a function", stub, runtime)
        if not callable(runtime):
            return

    # Look the object up statically, to avoid binding by the descriptor protocol
    static_runtime = _static_lookup_runtime(object_path)

    if isinstance(stub, nodes.FuncDef):
        for error_text in _verify_abstract_status(stub, runtime):
            yield Error(object_path, error_text, stub, runtime)
        for error_text in _verify_final_method(stub, runtime, static_runtime):
            yield Error(object_path, error_text, stub, runtime)

    for message in _verify_static_class_methods(stub, runtime, static_runtime, object_path):
        yield Error(object_path, "is inconsistent, " + message, stub, runtime)

    signature = safe_inspect_signature(runtime)
    runtime_is_coroutine = inspect.iscoroutinefunction(runtime)

    if signature:
        stub_sig = Signature.from_funcitem(stub)
        runtime_sig = Signature.from_inspect_signature(signature)
        runtime_sig_desc = f'{"async " if runtime_is_coroutine else ""}def {signature}'
        stub_desc = str(stub_sig)
    else:
        runtime_sig_desc, stub_desc = None, None

    # Don't raise an error if the stub is a coroutine, but the runtime isn't.
    # That results in false positives.
    # See https://github.com/python/typeshed/issues/7344
    if runtime_is_coroutine and not stub.is_coroutine:
        yield Error(
            object_path,
            'is an "async def" function at runtime, but not in the stub',
            stub,
            runtime,
            stub_desc=stub_desc,
            runtime_desc=runtime_sig_desc,
        )

    if not signature:
        return

    for message in _verify_signature(stub_sig, runtime_sig, function_name=stub.name):
        yield Error(
            object_path,
            "is inconsistent, " + message,
            stub,
            runtime,
            runtime_desc=runtime_sig_desc,
        )


</t>
<t tx="ekr.20230831011820.1966">@verify.register(Missing)
def verify_none(
    stub: Missing, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    yield Error(object_path, "is not present in stub", stub, runtime)


</t>
<t tx="ekr.20230831011820.1967">@verify.register(nodes.Var)
def verify_var(
    stub: nodes.Var, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        # Don't always yield an error here, because we often can't find instance variables
        if len(object_path) &lt;= 2:
            yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if (
        stub.is_initialized_in_class
        and is_read_only_property(runtime)
        and (stub.is_settable_property or not stub.is_property)
    ):
        yield Error(object_path, "is read-only at runtime but not in the stub", stub, runtime)

    runtime_type = get_mypy_type_of_runtime_value(runtime)
    if (
        runtime_type is not None
        and stub.type is not None
        and not is_subtype_helper(runtime_type, stub.type)
    ):
        should_error = True
        # Avoid errors when defining enums, since runtime_type is the enum itself, but we'd
        # annotate it with the type of runtime.value
        if isinstance(runtime, enum.Enum):
            runtime_type = get_mypy_type_of_runtime_value(runtime.value)
            if runtime_type is not None and is_subtype_helper(runtime_type, stub.type):
                should_error = False

        if should_error:
            yield Error(
                object_path, f"variable differs from runtime type {runtime_type}", stub, runtime
            )


</t>
<t tx="ekr.20230831011820.1968">@verify.register(nodes.OverloadedFuncDef)
def verify_overloadedfuncdef(
    stub: nodes.OverloadedFuncDef, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if stub.is_property:
        # Any property with a setter is represented as an OverloadedFuncDef
        if is_read_only_property(runtime):
            yield Error(object_path, "is read-only at runtime but not in the stub", stub, runtime)
        return

    if not is_probably_a_function(runtime):
        yield Error(object_path, "is not a function", stub, runtime)
        if not callable(runtime):
            return

    # mypy doesn't allow overloads where one overload is abstract but another isn't,
    # so it should be okay to just check whether the first overload is abstract or not.
    #
    # TODO: Mypy *does* allow properties where e.g. the getter is abstract but the setter is not;
    # and any property with a setter is represented as an OverloadedFuncDef internally;
    # not sure exactly what (if anything) we should do about that.
    first_part = stub.items[0]
    if isinstance(first_part, nodes.Decorator) and first_part.is_overload:
        for msg in _verify_abstract_status(first_part.func, runtime):
            yield Error(object_path, msg, stub, runtime)

    # Look the object up statically, to avoid binding by the descriptor protocol
    static_runtime = _static_lookup_runtime(object_path)

    for message in _verify_static_class_methods(stub, runtime, static_runtime, object_path):
        yield Error(object_path, "is inconsistent, " + message, stub, runtime)

    # TODO: Should call _verify_final_method here,
    # but overloaded final methods in stubs cause a stubtest crash: see #14950

    signature = safe_inspect_signature(runtime)
    if not signature:
        return

    stub_sig = Signature.from_overloadedfuncdef(stub)
    runtime_sig = Signature.from_inspect_signature(signature)

    for message in _verify_signature(stub_sig, runtime_sig, function_name=stub.name):
        # TODO: This is a little hacky, but the addition here is super useful
        if "has a default value of type" in message:
            message += (
                ". This is often caused by overloads failing to account for explicitly passing "
                "in the default value."
            )
        yield Error(
            object_path,
            "is inconsistent, " + message,
            stub,
            runtime,
            stub_desc=(str(stub.type)) + f"\nInferred signature: {stub_sig}",
            runtime_desc="def " + str(signature),
        )


</t>
<t tx="ekr.20230831011820.1969">@verify.register(nodes.TypeVarExpr)
def verify_typevarexpr(
    stub: nodes.TypeVarExpr, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        # We seem to insert these typevars into NamedTuple stubs, but they
        # don't exist at runtime. Just ignore!
        if stub.name == "_NT":
            return
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if not isinstance(runtime, TypeVar):
        yield Error(object_path, "is not a TypeVar", stub, runtime)
        return


</t>
<t tx="ekr.20230831011820.197">    def redundant_condition_in_if(self, truthiness: bool, context: Context) -&gt; None:
        self.redundant_expr("If condition", truthiness, context)

</t>
<t tx="ekr.20230831011820.1970">@verify.register(nodes.ParamSpecExpr)
def verify_paramspecexpr(
    stub: nodes.ParamSpecExpr, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    maybe_paramspec_types = (
        getattr(typing, "ParamSpec", None),
        getattr(typing_extensions, "ParamSpec", None),
    )
    paramspec_types = tuple(t for t in maybe_paramspec_types if t is not None)
    if not paramspec_types or not isinstance(runtime, paramspec_types):
        yield Error(object_path, "is not a ParamSpec", stub, runtime)
        return


</t>
<t tx="ekr.20230831011820.1971">def _verify_readonly_property(stub: nodes.Decorator, runtime: Any) -&gt; Iterator[str]:
    assert stub.func.is_property
    if isinstance(runtime, property):
        yield from _verify_final_method(stub.func, runtime.fget, MISSING)
        return
    if inspect.isdatadescriptor(runtime):
        # It's enough like a property...
        return
    # Sometimes attributes pretend to be properties, for instance, to express that they
    # are read only. So allowlist if runtime_type matches the return type of stub.
    runtime_type = get_mypy_type_of_runtime_value(runtime)
    func_type = (
        stub.func.type.ret_type if isinstance(stub.func.type, mypy.types.CallableType) else None
    )
    if (
        runtime_type is not None
        and func_type is not None
        and is_subtype_helper(runtime_type, func_type)
    ):
        return
    yield "is inconsistent, cannot reconcile @property on stub with runtime object"


</t>
<t tx="ekr.20230831011820.1972">def _verify_abstract_status(stub: nodes.FuncDef, runtime: Any) -&gt; Iterator[str]:
    stub_abstract = stub.abstract_status == nodes.IS_ABSTRACT
    runtime_abstract = getattr(runtime, "__isabstractmethod__", False)
    # The opposite can exist: some implementations omit `@abstractmethod` decorators
    if runtime_abstract and not stub_abstract:
        item_type = "property" if stub.is_property else "method"
        yield f"is inconsistent, runtime {item_type} is abstract but stub is not"


</t>
<t tx="ekr.20230831011820.1973">def _verify_final_method(
    stub: nodes.FuncDef, runtime: Any, static_runtime: MaybeMissing[Any]
) -&gt; Iterator[str]:
    if stub.is_final:
        return
    if getattr(runtime, "__final__", False) or (
        static_runtime is not MISSING and getattr(static_runtime, "__final__", False)
    ):
        yield "is decorated with @final at runtime, but not in the stub"


</t>
<t tx="ekr.20230831011820.1974">def _resolve_funcitem_from_decorator(dec: nodes.OverloadPart) -&gt; nodes.FuncItem | None:
    """Returns a FuncItem that corresponds to the output of the decorator.

    Returns None if we can't figure out what that would be. For convenience, this function also
    accepts FuncItems.
    """
    if isinstance(dec, nodes.FuncItem):
        return dec
    if dec.func.is_property:
        return None

    def apply_decorator_to_funcitem(
        decorator: nodes.Expression, func: nodes.FuncItem
    ) -&gt; nodes.FuncItem | None:
        if not isinstance(decorator, nodes.RefExpr):
            return None
        if not decorator.fullname:
            # Happens with namedtuple
            return None
        if (
            decorator.fullname in ("builtins.staticmethod", "abc.abstractmethod")
            or decorator.fullname in mypy.types.OVERLOAD_NAMES
        ):
            return func
        if decorator.fullname == "builtins.classmethod":
            if func.arguments[0].variable.name not in ("cls", "mcs", "metacls"):
                raise StubtestFailure(
                    f"unexpected class argument name {func.arguments[0].variable.name!r} "
                    f"in {dec.fullname}"
                )
            # FuncItem is written so that copy.copy() actually works, even when compiled
            ret = copy.copy(func)
            # Remove the cls argument, since it's not present in inspect.signature of classmethods
            ret.arguments = ret.arguments[1:]
            return ret
        # Just give up on any other decorators. After excluding properties, we don't run into
        # anything else when running on typeshed's stdlib.
        return None

    func: nodes.FuncItem = dec.func
    for decorator in dec.original_decorators:
        resulting_func = apply_decorator_to_funcitem(decorator, func)
        if resulting_func is None:
            return None
        func = resulting_func
    return func


</t>
<t tx="ekr.20230831011820.1975">@verify.register(nodes.Decorator)
def verify_decorator(
    stub: nodes.Decorator, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if stub.func.is_property:
        for message in _verify_readonly_property(stub, runtime):
            yield Error(object_path, message, stub, runtime)
        for message in _verify_abstract_status(stub.func, runtime):
            yield Error(object_path, message, stub, runtime)
        return

    func = _resolve_funcitem_from_decorator(stub)
    if func is not None:
        yield from verify(func, runtime, object_path)


</t>
<t tx="ekr.20230831011820.1976">@verify.register(nodes.TypeAlias)
def verify_typealias(
    stub: nodes.TypeAlias, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    stub_target = mypy.types.get_proper_type(stub.target)
    stub_desc = f"Type alias for {stub_target}"
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime, stub_desc=stub_desc)
        return
    runtime_origin = get_origin(runtime) or runtime
    if isinstance(stub_target, mypy.types.Instance):
        if not isinstance(runtime_origin, type):
            yield Error(
                object_path,
                "is inconsistent, runtime is not a type",
                stub,
                runtime,
                stub_desc=stub_desc,
            )
            return

        stub_origin = stub_target.type
        # Do our best to figure out the fullname of the runtime object...
        runtime_name: object
        try:
            runtime_name = runtime_origin.__qualname__
        except AttributeError:
            runtime_name = getattr(runtime_origin, "__name__", MISSING)
        if isinstance(runtime_name, str):
            runtime_module: object = getattr(runtime_origin, "__module__", MISSING)
            if isinstance(runtime_module, str):
                if runtime_module == "collections.abc" or (
                    runtime_module == "re" and runtime_name in {"Match", "Pattern"}
                ):
                    runtime_module = "typing"
                runtime_fullname = f"{runtime_module}.{runtime_name}"
                if re.fullmatch(rf"_?{re.escape(stub_origin.fullname)}", runtime_fullname):
                    # Okay, we're probably fine.
                    return

        # Okay, either we couldn't construct a fullname
        # or the fullname of the stub didn't match the fullname of the runtime.
        # Fallback to a full structural check of the runtime vis-a-vis the stub.
        yield from verify(stub_origin, runtime_origin, object_path)
        return
    if isinstance(stub_target, mypy.types.UnionType):
        # complain if runtime is not a Union or UnionType
        if runtime_origin is not Union and (
            not (sys.version_info &gt;= (3, 10) and isinstance(runtime, types.UnionType))
        ):
            yield Error(object_path, "is not a Union", stub, runtime, stub_desc=str(stub_target))
        # could check Union contents here...
        return
    if isinstance(stub_target, mypy.types.TupleType):
        if tuple not in getattr(runtime_origin, "__mro__", ()):
            yield Error(
                object_path, "is not a subclass of tuple", stub, runtime, stub_desc=stub_desc
            )
        # could check Tuple contents here...
        return
    if isinstance(stub_target, mypy.types.CallableType):
        if runtime_origin is not collections.abc.Callable:
            yield Error(
                object_path, "is not a type alias for Callable", stub, runtime, stub_desc=stub_desc
            )
        # could check Callable contents here...
        return
    if isinstance(stub_target, mypy.types.AnyType):
        return
    yield Error(object_path, "is not a recognised type alias", stub, runtime, stub_desc=stub_desc)


</t>
<t tx="ekr.20230831011820.1977"># ====================
# Helpers
# ====================


IGNORED_MODULE_DUNDERS: typing_extensions.Final = frozenset(
    {
        "__file__",
        "__doc__",
        "__name__",
        "__builtins__",
        "__package__",
        "__cached__",
        "__loader__",
        "__spec__",
        "__annotations__",
        "__path__",  # mypy adds __path__ to packages, but C packages don't have it
        "__getattr__",  # resulting behaviour might be typed explicitly
        # Created by `warnings.warn`, does not make much sense to have in stubs:
        "__warningregistry__",
        # TODO: remove the following from this list
        "__author__",
        "__version__",
        "__copyright__",
    }
)

IGNORABLE_CLASS_DUNDERS: typing_extensions.Final = frozenset(
    {
        # Special attributes
        "__dict__",
        "__annotations__",
        "__text_signature__",
        "__weakref__",
        "__del__",  # Only ever called when an object is being deleted, who cares?
        "__hash__",
        "__getattr__",  # resulting behaviour might be typed explicitly
        "__setattr__",  # defining this on a class can cause worse type checking
        "__vectorcalloffset__",  # undocumented implementation detail of the vectorcall protocol
        # isinstance/issubclass hooks that type-checkers don't usually care about
        "__instancecheck__",
        "__subclasshook__",
        "__subclasscheck__",
        # python2 only magic methods:
        "__cmp__",
        "__nonzero__",
        "__unicode__",
        "__div__",
        # cython methods
        "__pyx_vtable__",
        # Pickle methods
        "__setstate__",
        "__getstate__",
        "__getnewargs__",
        "__getinitargs__",
        "__reduce_ex__",
        "__reduce__",
        # ctypes weirdness
        "__ctype_be__",
        "__ctype_le__",
        "__ctypes_from_outparam__",
        # mypy limitations
        "__abstractmethods__",  # Classes with metaclass=ABCMeta inherit this attribute
        "__new_member__",  # If an enum defines __new__, the method is renamed as __new_member__
        "__dataclass_fields__",  # Generated by dataclasses
        "__dataclass_params__",  # Generated by dataclasses
        "__doc__",  # mypy's semanal for namedtuples assumes this is str, not Optional[str]
        # Added to all protocol classes on 3.12+ (or if using typing_extensions.Protocol)
        "__protocol_attrs__",
        "__callable_proto_members_only__",
        # typing implementation details, consider removing some of these:
        "__parameters__",
        "__origin__",
        "__args__",
        "__orig_bases__",
        "__final__",  # Has a specialized check
        # Consider removing __slots__?
        "__slots__",
    }
)


def is_probably_private(name: str) -&gt; bool:
    return name.startswith("_") and not is_dunder(name)


</t>
<t tx="ekr.20230831011820.1978">def is_probably_a_function(runtime: Any) -&gt; bool:
    return (
        isinstance(runtime, (types.FunctionType, types.BuiltinFunctionType))
        or isinstance(runtime, (types.MethodType, types.BuiltinMethodType))
        or (inspect.ismethoddescriptor(runtime) and callable(runtime))
    )


</t>
<t tx="ekr.20230831011820.1979">def is_read_only_property(runtime: object) -&gt; bool:
    return isinstance(runtime, property) and runtime.fset is None


</t>
<t tx="ekr.20230831011820.198">    def redundant_expr(self, description: str, truthiness: bool, context: Context) -&gt; None:
        self.fail(
            f"{description} is always {str(truthiness).lower()}",
            context,
            code=codes.REDUNDANT_EXPR,
        )

</t>
<t tx="ekr.20230831011820.1980">def safe_inspect_signature(runtime: Any) -&gt; inspect.Signature | None:
    try:
        return inspect.signature(runtime)
    except Exception:
        # inspect.signature throws ValueError all the time
        # catch RuntimeError because of https://bugs.python.org/issue39504
        # catch TypeError because of https://github.com/python/typeshed/pull/5762
        # catch AttributeError because of inspect.signature(_curses.window.border)
        return None


</t>
<t tx="ekr.20230831011820.1981">def is_subtype_helper(left: mypy.types.Type, right: mypy.types.Type) -&gt; bool:
    """Checks whether ``left`` is a subtype of ``right``."""
    left = mypy.types.get_proper_type(left)
    right = mypy.types.get_proper_type(right)
    if (
        isinstance(left, mypy.types.LiteralType)
        and isinstance(left.value, int)
        and left.value in (0, 1)
        and mypy.types.is_named_instance(right, "builtins.bool")
    ):
        # Pretend Literal[0, 1] is a subtype of bool to avoid unhelpful errors.
        return True

    if isinstance(right, mypy.types.TypedDictType) and mypy.types.is_named_instance(
        left, "builtins.dict"
    ):
        # Special case checks against TypedDicts
        return True

    with mypy.state.state.strict_optional_set(True):
        return mypy.subtypes.is_subtype(left, right)


</t>
<t tx="ekr.20230831011820.1982">def get_mypy_type_of_runtime_value(runtime: Any) -&gt; mypy.types.Type | None:
    """Returns a mypy type object representing the type of ``runtime``.

    Returns None if we can't find something that works.

    """
    if runtime is None:
        return mypy.types.NoneType()
    if isinstance(runtime, property):
        # Give up on properties to avoid issues with things that are typed as attributes.
        return None

    def anytype() -&gt; mypy.types.AnyType:
        return mypy.types.AnyType(mypy.types.TypeOfAny.unannotated)

    if isinstance(
        runtime,
        (types.FunctionType, types.BuiltinFunctionType, types.MethodType, types.BuiltinMethodType),
    ):
        builtins = get_stub("builtins")
        assert builtins is not None
        type_info = builtins.names["function"].node
        assert isinstance(type_info, nodes.TypeInfo)
        fallback = mypy.types.Instance(type_info, [anytype()])
        signature = safe_inspect_signature(runtime)
        if signature:
            arg_types = []
            arg_kinds = []
            arg_names = []
            for arg in signature.parameters.values():
                arg_types.append(anytype())
                arg_names.append(
                    None if arg.kind == inspect.Parameter.POSITIONAL_ONLY else arg.name
                )
                has_default = arg.default == inspect.Parameter.empty
                if arg.kind == inspect.Parameter.POSITIONAL_ONLY:
                    arg_kinds.append(nodes.ARG_POS if has_default else nodes.ARG_OPT)
                elif arg.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
                    arg_kinds.append(nodes.ARG_POS if has_default else nodes.ARG_OPT)
                elif arg.kind == inspect.Parameter.KEYWORD_ONLY:
                    arg_kinds.append(nodes.ARG_NAMED if has_default else nodes.ARG_NAMED_OPT)
                elif arg.kind == inspect.Parameter.VAR_POSITIONAL:
                    arg_kinds.append(nodes.ARG_STAR)
                elif arg.kind == inspect.Parameter.VAR_KEYWORD:
                    arg_kinds.append(nodes.ARG_STAR2)
                else:
                    raise AssertionError
        else:
            arg_types = [anytype(), anytype()]
            arg_kinds = [nodes.ARG_STAR, nodes.ARG_STAR2]
            arg_names = [None, None]

        return mypy.types.CallableType(
            arg_types,
            arg_kinds,
            arg_names,
            ret_type=anytype(),
            fallback=fallback,
            is_ellipsis_args=True,
        )

    # Try and look up a stub for the runtime object
    stub = get_stub(type(runtime).__module__)
    if stub is None:
        return None
    type_name = type(runtime).__name__
    if type_name not in stub.names:
        return None
    type_info = stub.names[type_name].node
    if isinstance(type_info, nodes.Var):
        return type_info.type
    if not isinstance(type_info, nodes.TypeInfo):
        return None

    if isinstance(runtime, tuple):
        # Special case tuples so we construct a valid mypy.types.TupleType
        optional_items = [get_mypy_type_of_runtime_value(v) for v in runtime]
        items = [(i if i is not None else anytype()) for i in optional_items]
        fallback = mypy.types.Instance(type_info, [anytype()])
        return mypy.types.TupleType(items, fallback)

    fallback = mypy.types.Instance(type_info, [anytype() for _ in type_info.type_vars])

    value: bool | int | str
    if isinstance(runtime, enum.Enum) and isinstance(runtime.name, str):
        value = runtime.name
    elif isinstance(runtime, bytes):
        value = bytes_to_human_readable_repr(runtime)
    elif isinstance(runtime, (bool, int, str)):
        value = runtime
    else:
        return fallback

    return mypy.types.LiteralType(value=value, fallback=fallback)


</t>
<t tx="ekr.20230831011820.1983"># ====================
# Build and entrypoint
# ====================


_all_stubs: dict[str, nodes.MypyFile] = {}


def build_stubs(modules: list[str], options: Options, find_submodules: bool = False) -&gt; list[str]:
    """Uses mypy to construct stub objects for the given modules.

    This sets global state that ``get_stub`` can access.

    Returns all modules we might want to check. If ``find_submodules`` is False, this is equal
    to ``modules``.

    :param modules: List of modules to build stubs for.
    :param options: Mypy options for finding and building stubs.
    :param find_submodules: Whether to attempt to find submodules of the given modules as well.

    """
    data_dir = mypy.build.default_data_dir()
    search_path = mypy.modulefinder.compute_search_paths([], options, data_dir)
    find_module_cache = mypy.modulefinder.FindModuleCache(
        search_path, fscache=None, options=options
    )

    all_modules = []
    sources = []
    for module in modules:
        all_modules.append(module)
        if not find_submodules:
            module_path = find_module_cache.find_module(module)
            if not isinstance(module_path, str):
                # test_module will yield an error later when it can't find stubs
                continue
            sources.append(mypy.modulefinder.BuildSource(module_path, module, None))
        else:
            found_sources = find_module_cache.find_modules_recursive(module)
            sources.extend(found_sources)
            # find submodules via mypy
            all_modules.extend(s.module for s in found_sources if s.module not in all_modules)
            # find submodules via pkgutil
            try:
                runtime = silent_import_module(module)
                all_modules.extend(
                    m.name
                    for m in pkgutil.walk_packages(runtime.__path__, runtime.__name__ + ".")
                    if m.name not in all_modules
                )
            except KeyboardInterrupt:
                raise
            except BaseException:
                pass

    if sources:
        try:
            res = mypy.build.build(sources=sources, options=options)
        except mypy.errors.CompileError as e:
            raise StubtestFailure(f"failed mypy compile:\n{e}") from e
        if res.errors:
            raise StubtestFailure("mypy build errors:\n" + "\n".join(res.errors))

        global _all_stubs
        _all_stubs = res.files

    return all_modules


</t>
<t tx="ekr.20230831011820.1984">def get_stub(module: str) -&gt; nodes.MypyFile | None:
    """Returns a stub object for the given module, if we've built one."""
    return _all_stubs.get(module)


</t>
<t tx="ekr.20230831011820.1985">def get_typeshed_stdlib_modules(
    custom_typeshed_dir: str | None, version_info: tuple[int, int] | None = None
) -&gt; set[str]:
    """Returns a list of stdlib modules in typeshed (for current Python version)."""
    stdlib_py_versions = mypy.modulefinder.load_stdlib_py_versions(custom_typeshed_dir)
    if version_info is None:
        version_info = sys.version_info[0:2]

    def exists_in_version(module: str) -&gt; bool:
        assert version_info is not None
        parts = module.split(".")
        for i in range(len(parts), 0, -1):
            current_module = ".".join(parts[:i])
            if current_module in stdlib_py_versions:
                minver, maxver = stdlib_py_versions[current_module]
                return version_info &gt;= minver and (maxver is None or version_info &lt;= maxver)
        return False

    if custom_typeshed_dir:
        typeshed_dir = Path(custom_typeshed_dir)
    else:
        typeshed_dir = Path(mypy.build.default_data_dir()) / "typeshed"
    stdlib_dir = typeshed_dir / "stdlib"

    modules: set[str] = set()
    for path in stdlib_dir.rglob("*.pyi"):
        if path.stem == "__init__":
            path = path.parent
        module = ".".join(path.relative_to(stdlib_dir).parts[:-1] + (path.stem,))
        if exists_in_version(module):
            modules.add(module)
    return modules


</t>
<t tx="ekr.20230831011820.1986">def get_importable_stdlib_modules() -&gt; set[str]:
    """Return all importable stdlib modules at runtime."""
    all_stdlib_modules: AbstractSet[str]
    if sys.version_info &gt;= (3, 10):
        all_stdlib_modules = sys.stdlib_module_names
    else:
        all_stdlib_modules = set(sys.builtin_module_names)
        python_exe_dir = Path(sys.executable).parent
        for m in pkgutil.iter_modules():
            finder = m.module_finder
            if isinstance(finder, importlib.machinery.FileFinder):
                finder_path = Path(finder.path)
                if (
                    python_exe_dir in finder_path.parents
                    and "site-packages" not in finder_path.parts
                ):
                    all_stdlib_modules.add(m.name)

    importable_stdlib_modules: set[str] = set()
    for module_name in all_stdlib_modules:
        if module_name in ANNOYING_STDLIB_MODULES:
            continue

        try:
            runtime = silent_import_module(module_name)
        except ImportError:
            continue
        else:
            importable_stdlib_modules.add(module_name)

        try:
            # some stdlib modules (e.g. `nt`) don't have __path__ set...
            runtime_path = runtime.__path__
            runtime_name = runtime.__name__
        except AttributeError:
            continue

        for submodule in pkgutil.walk_packages(runtime_path, runtime_name + "."):
            submodule_name = submodule.name

            # There are many annoying *.__main__ stdlib modules,
            # and including stubs for them isn't really that useful anyway:
            # tkinter.__main__ opens a tkinter windows; unittest.__main__ raises SystemExit; etc.
            #
            # The idlelib.* submodules are similarly annoying in opening random tkinter windows,
            # and we're unlikely to ever add stubs for idlelib in typeshed
            # (see discussion in https://github.com/python/typeshed/pull/9193)
            if submodule_name.endswith(".__main__") or submodule_name.startswith("idlelib."):
                continue

            try:
                silent_import_module(submodule_name)
            # importing multiprocessing.popen_forkserver on Windows raises AttributeError...
            except Exception:
                continue
            else:
                importable_stdlib_modules.add(submodule_name)

    return importable_stdlib_modules


</t>
<t tx="ekr.20230831011820.1987">def get_allowlist_entries(allowlist_file: str) -&gt; Iterator[str]:
    def strip_comments(s: str) -&gt; str:
        try:
            return s[: s.index("#")].strip()
        except ValueError:
            return s.strip()

    with open(allowlist_file) as f:
        for line in f:
            entry = strip_comments(line)
            if entry:
                yield entry


</t>
<t tx="ekr.20230831011820.1988">class _Arguments:
    modules: list[str]
    concise: bool
    ignore_missing_stub: bool
    ignore_positional_only: bool
    allowlist: list[str]
    generate_allowlist: bool
    ignore_unused_allowlist: bool
    mypy_config_file: str
    custom_typeshed_dir: str
    check_typeshed: bool
    version: str


</t>
<t tx="ekr.20230831011820.1989"># typeshed added a stub for __main__, but that causes stubtest to check itself
ANNOYING_STDLIB_MODULES: typing_extensions.Final = frozenset({"antigravity", "this", "__main__"})


def test_stubs(args: _Arguments, use_builtins_fixtures: bool = False) -&gt; int:
    """This is stubtest! It's time to test the stubs!"""
    # Load the allowlist. This is a series of strings corresponding to Error.object_desc
    # Values in the dict will store whether we used the allowlist entry or not.
    allowlist = {
        entry: False
        for allowlist_file in args.allowlist
        for entry in get_allowlist_entries(allowlist_file)
    }
    allowlist_regexes = {entry: re.compile(entry) for entry in allowlist}

    # If we need to generate an allowlist, we store Error.object_desc for each error here.
    generated_allowlist = set()

    modules = args.modules
    if args.check_typeshed:
        if args.modules:
            print(
                _style("error:", color="red", bold=True),
                "cannot pass both --check-typeshed and a list of modules",
            )
            return 1
        typeshed_modules = get_typeshed_stdlib_modules(args.custom_typeshed_dir)
        runtime_modules = get_importable_stdlib_modules()
        modules = sorted((typeshed_modules | runtime_modules) - ANNOYING_STDLIB_MODULES)

    if not modules:
        print(_style("error:", color="red", bold=True), "no modules to check")
        return 1

    options = Options()
    options.incremental = False
    options.custom_typeshed_dir = args.custom_typeshed_dir
    if options.custom_typeshed_dir:
        options.abs_custom_typeshed_dir = os.path.abspath(args.custom_typeshed_dir)
    options.config_file = args.mypy_config_file
    options.use_builtins_fixtures = use_builtins_fixtures

    if options.config_file:

        def set_strict_flags() -&gt; None:  # not needed yet
            return

        parse_config_file(options, set_strict_flags, options.config_file, sys.stdout, sys.stderr)

    try:
        modules = build_stubs(modules, options, find_submodules=not args.check_typeshed)
    except StubtestFailure as stubtest_failure:
        print(
            _style("error:", color="red", bold=True),
            f"not checking stubs due to {stubtest_failure}",
        )
        return 1

    exit_code = 0
    error_count = 0
    for module in modules:
        for error in test_module(module):
            # Filter errors
            if args.ignore_missing_stub and error.is_missing_stub():
                continue
            if args.ignore_positional_only and error.is_positional_only_related():
                continue
            if error.object_desc in allowlist:
                allowlist[error.object_desc] = True
                continue
            is_allowlisted = False
            for w in allowlist:
                if allowlist_regexes[w].fullmatch(error.object_desc):
                    allowlist[w] = True
                    is_allowlisted = True
                    break
            if is_allowlisted:
                continue

            # We have errors, so change exit code, and output whatever necessary
            exit_code = 1
            if args.generate_allowlist:
                generated_allowlist.add(error.object_desc)
                continue
            print(error.get_description(concise=args.concise))
            error_count += 1

    # Print unused allowlist entries
    if not args.ignore_unused_allowlist:
        for w in allowlist:
            # Don't consider an entry unused if it regex-matches the empty string
            # This lets us allowlist errors that don't manifest at all on some systems
            if not allowlist[w] and not allowlist_regexes[w].fullmatch(""):
                exit_code = 1
                error_count += 1
                print(f"note: unused allowlist entry {w}")

    # Print the generated allowlist
    if args.generate_allowlist:
        for e in sorted(generated_allowlist):
            print(e)
        exit_code = 0
    elif not args.concise:
        if error_count:
            print(
                _style(
                    f"Found {error_count} error{plural_s(error_count)}"
                    f" (checked {len(modules)} module{plural_s(modules)})",
                    color="red",
                    bold=True,
                )
            )
        else:
            print(
                _style(
                    f"Success: no issues found in {len(modules)} module{plural_s(modules)}",
                    color="green",
                    bold=True,
                )
            )

    return exit_code


</t>
<t tx="ekr.20230831011820.199">    def impossible_intersection(
        self, formatted_base_class_list: str, reason: str, context: Context
    ) -&gt; None:
        template = "Subclass of {} cannot exist: would have {}"
        self.fail(
            template.format(formatted_base_class_list, reason), context, code=codes.UNREACHABLE
        )

</t>
<t tx="ekr.20230831011820.1990">def parse_options(args: list[str]) -&gt; _Arguments:
    parser = argparse.ArgumentParser(
        description="Compares stubs to objects introspected from the runtime."
    )
    parser.add_argument("modules", nargs="*", help="Modules to test")
    parser.add_argument(
        "--concise",
        action="store_true",
        help="Makes stubtest's output more concise, one line per error",
    )
    parser.add_argument(
        "--ignore-missing-stub",
        action="store_true",
        help="Ignore errors for stub missing things that are present at runtime",
    )
    parser.add_argument(
        "--ignore-positional-only",
        action="store_true",
        help="Ignore errors for whether an argument should or shouldn't be positional-only",
    )
    parser.add_argument(
        "--allowlist",
        "--whitelist",
        action="append",
        metavar="FILE",
        default=[],
        help=(
            "Use file as an allowlist. Can be passed multiple times to combine multiple "
            "allowlists. Allowlists can be created with --generate-allowlist. Allowlists "
            "support regular expressions."
        ),
    )
    parser.add_argument(
        "--generate-allowlist",
        "--generate-whitelist",
        action="store_true",
        help="Print an allowlist (to stdout) to be used with --allowlist",
    )
    parser.add_argument(
        "--ignore-unused-allowlist",
        "--ignore-unused-whitelist",
        action="store_true",
        help="Ignore unused allowlist entries",
    )
    parser.add_argument(
        "--mypy-config-file",
        metavar="FILE",
        help=("Use specified mypy config file to determine mypy plugins and mypy path"),
    )
    parser.add_argument(
        "--custom-typeshed-dir", metavar="DIR", help="Use the custom typeshed in DIR"
    )
    parser.add_argument(
        "--check-typeshed", action="store_true", help="Check all stdlib modules in typeshed"
    )
    parser.add_argument(
        "--version", action="version", version="%(prog)s " + mypy.version.__version__
    )

    return parser.parse_args(args, namespace=_Arguments())


</t>
<t tx="ekr.20230831011820.1991">def main() -&gt; int:
    mypy.util.check_python_version("stubtest")
    return test_stubs(parse_options(sys.argv[1:]))
</t>
<t tx="ekr.20230831011820.1992">@path mypy
"""Utilities for mypy.stubgen, mypy.stubgenc, and mypy.stubdoc modules."""
&lt;&lt; stubutil.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.1993">
from __future__ import annotations

import os.path
import re
import sys
from contextlib import contextmanager
from typing import Iterator
from typing_extensions import overload

from mypy.modulefinder import ModuleNotFoundReason
from mypy.moduleinspect import InspectError, ModuleInspect

# Modules that may fail when imported, or that may have side effects (fully qualified).
NOT_IMPORTABLE_MODULES = ()


</t>
<t tx="ekr.20230831011820.1994">class CantImport(Exception):
    @others
</t>
<t tx="ekr.20230831011820.1995">def __init__(self, module: str, message: str):
    self.module = module
    self.message = message


</t>
<t tx="ekr.20230831011820.1996">def walk_packages(
    inspect: ModuleInspect, packages: list[str], verbose: bool = False
) -&gt; Iterator[str]:
    """Iterates through all packages and sub-packages in the given list.

    This uses runtime imports (in another process) to find both Python and C modules.
    For Python packages we simply pass the __path__ attribute to pkgutil.walk_packages() to
    get the content of the package (all subpackages and modules).  However, packages in C
    extensions do not have this attribute, so we have to roll out our own logic: recursively
    find all modules imported in the package that have matching names.
    """
    for package_name in packages:
        if package_name in NOT_IMPORTABLE_MODULES:
            print(f"{package_name}: Skipped (blacklisted)")
            continue
        if verbose:
            print(f"Trying to import {package_name!r} for runtime introspection")
        try:
            prop = inspect.get_package_properties(package_name)
        except InspectError:
            report_missing(package_name)
            continue
        yield prop.name
        if prop.is_c_module:
            # Recursively iterate through the subpackages
            yield from walk_packages(inspect, prop.subpackages, verbose)
        else:
            yield from prop.subpackages


</t>
<t tx="ekr.20230831011820.1997">def find_module_path_using_sys_path(module: str, sys_path: list[str]) -&gt; str | None:
    relative_candidates = (
        module.replace(".", "/") + ".py",
        os.path.join(module.replace(".", "/"), "__init__.py"),
    )
    for base in sys_path:
        for relative_path in relative_candidates:
            path = os.path.join(base, relative_path)
            if os.path.isfile(path):
                return path
    return None


</t>
<t tx="ekr.20230831011820.1998">def find_module_path_and_all_py3(
    inspect: ModuleInspect, module: str, verbose: bool
) -&gt; tuple[str | None, list[str] | None] | None:
    """Find module and determine __all__ for a Python 3 module.

    Return None if the module is a C module. Return (module_path, __all__) if
    it is a Python module. Raise CantImport if import failed.
    """
    if module in NOT_IMPORTABLE_MODULES:
        raise CantImport(module, "")

    # TODO: Support custom interpreters.
    if verbose:
        print(f"Trying to import {module!r} for runtime introspection")
    try:
        mod = inspect.get_package_properties(module)
    except InspectError as e:
        # Fall back to finding the module using sys.path.
        path = find_module_path_using_sys_path(module, sys.path)
        if path is None:
            raise CantImport(module, str(e)) from e
        return path, None
    if mod.is_c_module:
        return None
    return mod.file, mod.all


</t>
<t tx="ekr.20230831011820.1999">@contextmanager
def generate_guarded(
    mod: str, target: str, ignore_errors: bool = True, verbose: bool = False
) -&gt; Iterator[None]:
    """Ignore or report errors during stub generation.

    Optionally report success.
    """
    if verbose:
        print(f"Processing {mod}")
    try:
        yield
    except Exception as e:
        if not ignore_errors:
            raise e
        else:
            # --ignore-errors was passed
            print("Stub generation failed for", mod, file=sys.stderr)
    else:
        if verbose:
            print(f"Created {target}")


</t>
<t tx="ekr.20230831011820.2">def maybe_write_junit_xml(td: float, serious: bool, messages: list[str], options: Options) -&gt; None:
    if options.junit_xml:
        py_version = f"{options.python_version[0]}_{options.python_version[1]}"
        util.write_junit_xml(
            td, serious, messages, options.junit_xml, py_version, options.platform
        )


</t>
<t tx="ekr.20230831011820.20">def is_literal_in_union(x: ProperType, y: ProperType) -&gt; bool:
    """Return True if x is a Literal and y is an Union that includes x"""
    return (
        isinstance(x, LiteralType)
        and isinstance(y, UnionType)
        and any(x == get_proper_type(z) for z in y.items)
    )


</t>
<t tx="ekr.20230831011820.200">    def report_protocol_problems(
        self,
        subtype: Instance | TupleType | TypedDictType | TypeType | CallableType,
        supertype: Instance,
        context: Context,
        *,
        code: ErrorCode | None,
    ) -&gt; None:
        """Report possible protocol conflicts between 'subtype' and 'supertype'.

        This includes missing members, incompatible types, and incompatible
        attribute flags, such as settable vs read-only or class variable vs
        instance variable.
        """
        OFFSET = 4  # Four spaces, so that notes will look like this:
        # note: 'Cls' is missing following 'Proto' members:
        # note:     method, attr
        MAX_ITEMS = 2  # Maximum number of conflicts, missing members, and overloads shown
        # List of special situations where we don't want to report additional problems
        exclusions: dict[type, list[str]] = {
            TypedDictType: ["typing.Mapping"],
            TupleType: ["typing.Iterable", "typing.Sequence"],
        }
        if supertype.type.fullname in exclusions.get(type(subtype), []):
            return
        if any(isinstance(tp, UninhabitedType) for tp in get_proper_types(supertype.args)):
            # We don't want to add notes for failed inference (e.g. Iterable[&lt;nothing&gt;]).
            # This will be only confusing a user even more.
            return

        class_obj = False
        is_module = False
        skip = []
        if isinstance(subtype, TupleType):
            if not isinstance(subtype.partial_fallback, Instance):
                return
            subtype = subtype.partial_fallback
        elif isinstance(subtype, TypedDictType):
            if not isinstance(subtype.fallback, Instance):
                return
            subtype = subtype.fallback
        elif isinstance(subtype, TypeType):
            if not isinstance(subtype.item, Instance):
                return
            class_obj = True
            subtype = subtype.item
        elif isinstance(subtype, CallableType):
            if subtype.is_type_obj():
                ret_type = get_proper_type(subtype.ret_type)
                if isinstance(ret_type, TupleType):
                    ret_type = ret_type.partial_fallback
                if not isinstance(ret_type, Instance):
                    return
                class_obj = True
                subtype = ret_type
            else:
                subtype = subtype.fallback
                skip = ["__call__"]
        if subtype.extra_attrs and subtype.extra_attrs.mod_name:
            is_module = True

        # Report missing members
        missing = get_missing_protocol_members(subtype, supertype, skip=skip)
        if (
            missing
            and (len(missing) &lt; len(supertype.type.protocol_members) or missing == ["__call__"])
            and len(missing) &lt;= MAX_ITEMS
        ):
            if missing == ["__call__"] and class_obj:
                self.note(
                    '"{}" has constructor incompatible with "__call__" of "{}"'.format(
                        subtype.type.name, supertype.type.name
                    ),
                    context,
                    code=code,
                )
            else:
                self.note(
                    '"{}" is missing following "{}" protocol member{}:'.format(
                        subtype.type.name, supertype.type.name, plural_s(missing)
                    ),
                    context,
                    code=code,
                )
                self.note(", ".join(missing), context, offset=OFFSET, code=code)
        elif len(missing) &gt; MAX_ITEMS or len(missing) == len(supertype.type.protocol_members):
            # This is an obviously wrong type: too many missing members
            return

        # Report member type conflicts
        conflict_types = get_conflict_protocol_types(
            subtype, supertype, class_obj=class_obj, options=self.options
        )
        if conflict_types and (
            not is_subtype(subtype, erase_type(supertype), options=self.options)
            or not subtype.type.defn.type_vars
            or not supertype.type.defn.type_vars
        ):
            type_name = format_type(subtype, self.options, module_names=True)
            self.note(f"Following member(s) of {type_name} have conflicts:", context, code=code)
            for name, got, exp in conflict_types[:MAX_ITEMS]:
                exp = get_proper_type(exp)
                got = get_proper_type(got)
                if not isinstance(exp, (CallableType, Overloaded)) or not isinstance(
                    got, (CallableType, Overloaded)
                ):
                    self.note(
                        "{}: expected {}, got {}".format(
                            name, *format_type_distinctly(exp, got, options=self.options)
                        ),
                        context,
                        offset=OFFSET,
                        code=code,
                    )
                else:
                    self.note("Expected:", context, offset=OFFSET, code=code)
                    if isinstance(exp, CallableType):
                        self.note(
                            pretty_callable(exp, self.options, skip_self=class_obj or is_module),
                            context,
                            offset=2 * OFFSET,
                            code=code,
                        )
                    else:
                        assert isinstance(exp, Overloaded)
                        self.pretty_overload(
                            exp, context, 2 * OFFSET, code=code, skip_self=class_obj or is_module
                        )
                    self.note("Got:", context, offset=OFFSET, code=code)
                    if isinstance(got, CallableType):
                        self.note(
                            pretty_callable(got, self.options, skip_self=class_obj or is_module),
                            context,
                            offset=2 * OFFSET,
                            code=code,
                        )
                    else:
                        assert isinstance(got, Overloaded)
                        self.pretty_overload(
                            got, context, 2 * OFFSET, code=code, skip_self=class_obj or is_module
                        )
            self.print_more(conflict_types, context, OFFSET, MAX_ITEMS, code=code)

        # Report flag conflicts (i.e. settable vs read-only etc.)
        conflict_flags = get_bad_protocol_flags(subtype, supertype, class_obj=class_obj)
        for name, subflags, superflags in conflict_flags[:MAX_ITEMS]:
            if not class_obj and IS_CLASSVAR in subflags and IS_CLASSVAR not in superflags:
                self.note(
                    "Protocol member {}.{} expected instance variable,"
                    " got class variable".format(supertype.type.name, name),
                    context,
                    code=code,
                )
            if not class_obj and IS_CLASSVAR in superflags and IS_CLASSVAR not in subflags:
                self.note(
                    "Protocol member {}.{} expected class variable,"
                    " got instance variable".format(supertype.type.name, name),
                    context,
                    code=code,
                )
            if IS_SETTABLE in superflags and IS_SETTABLE not in subflags:
                self.note(
                    "Protocol member {}.{} expected settable variable,"
                    " got read-only attribute".format(supertype.type.name, name),
                    context,
                    code=code,
                )
            if IS_CLASS_OR_STATIC in superflags and IS_CLASS_OR_STATIC not in subflags:
                self.note(
                    "Protocol member {}.{} expected class or static method".format(
                        supertype.type.name, name
                    ),
                    context,
                    code=code,
                )
            if (
                class_obj
                and IS_VAR in superflags
                and (IS_VAR in subflags and IS_CLASSVAR not in subflags)
            ):
                self.note(
                    "Only class variables allowed for class object access on protocols,"
                    ' {} is an instance variable of "{}"'.format(name, subtype.type.name),
                    context,
                    code=code,
                )
            if class_obj and IS_CLASSVAR in superflags:
                self.note(
                    "ClassVar protocol member {}.{} can never be matched by a class object".format(
                        supertype.type.name, name
                    ),
                    context,
                    code=code,
                )
        self.print_more(conflict_flags, context, OFFSET, MAX_ITEMS, code=code)

</t>
<t tx="ekr.20230831011820.2000">def report_missing(mod: str, message: str | None = "", traceback: str = "") -&gt; None:
    if message:
        message = " with error: " + message
    print(f"{mod}: Failed to import, skipping{message}")


</t>
<t tx="ekr.20230831011820.2001">def fail_missing(mod: str, reason: ModuleNotFoundReason) -&gt; None:
    if reason is ModuleNotFoundReason.NOT_FOUND:
        clarification = "(consider using --search-path)"
    elif reason is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
        clarification = "(module likely exists, but is not PEP 561 compatible)"
    else:
        clarification = f"(unknown reason '{reason}')"
    raise SystemExit(f"Can't find module '{mod}' {clarification}")


</t>
<t tx="ekr.20230831011820.2002">@overload
def remove_misplaced_type_comments(source: bytes) -&gt; bytes:
    ...


</t>
<t tx="ekr.20230831011820.2003">@overload
def remove_misplaced_type_comments(source: str) -&gt; str:
    ...


</t>
<t tx="ekr.20230831011820.2004">def remove_misplaced_type_comments(source: str | bytes) -&gt; str | bytes:
    """Remove comments from source that could be understood as misplaced type comments.

    Normal comments may look like misplaced type comments, and since they cause blocking
    parse errors, we want to avoid them.
    """
    if isinstance(source, bytes):
        # This gives us a 1-1 character code mapping, so it's roundtrippable.
        text = source.decode("latin1")
    else:
        text = source

    # Remove something that looks like a variable type comment but that's by itself
    # on a line, as it will often generate a parse error (unless it's # type: ignore).
    text = re.sub(r'^[ \t]*# +type: +["\'a-zA-Z_].*$', "", text, flags=re.MULTILINE)

    # Remove something that looks like a function type comment after docstring,
    # which will result in a parse error.
    text = re.sub(r'""" *\n[ \t\n]*# +type: +\(.*$', '"""\n', text, flags=re.MULTILINE)
    text = re.sub(r"''' *\n[ \t\n]*# +type: +\(.*$", "'''\n", text, flags=re.MULTILINE)

    # Remove something that looks like a badly formed function type comment.
    text = re.sub(r"^[ \t]*# +type: +\([^()]+(\)[ \t]*)?$", "", text, flags=re.MULTILINE)

    if isinstance(source, bytes):
        return text.encode("latin1")
    else:
        return text


</t>
<t tx="ekr.20230831011820.2005">def common_dir_prefix(paths: list[str]) -&gt; str:
    if not paths:
        return "."
    cur = os.path.dirname(os.path.normpath(paths[0]))
    for path in paths[1:]:
        while True:
            path = os.path.dirname(os.path.normpath(path))
            if (cur + os.sep).startswith(path + os.sep):
                cur = path
                break
    return cur or "."
</t>
<t tx="ekr.20230831011820.2006">@path mypy
&lt;&lt; subtypes.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.2007">from __future__ import annotations

from contextlib import contextmanager
from typing import Any, Callable, Final, Iterator, List, TypeVar, cast
from typing_extensions import TypeAlias as _TypeAlias

import mypy.applytype
import mypy.constraints
import mypy.typeops
from mypy.erasetype import erase_type
from mypy.expandtype import expand_self_type, expand_type_by_instance
from mypy.maptype import map_instance_to_supertype

# Circular import; done in the function instead.
# import mypy.solve
from mypy.nodes import (
    ARG_STAR,
    ARG_STAR2,
    CONTRAVARIANT,
    COVARIANT,
    Decorator,
    FuncBase,
    OverloadedFuncDef,
    TypeInfo,
    Var,
)
from mypy.options import Options
from mypy.state import state
from mypy.types import (
    MYPYC_NATIVE_INT_NAMES,
    TUPLE_LIKE_INSTANCE_NAMES,
    TYPED_NAMEDTUPLE_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    FormalArgument,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    NormalizedCallableType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    is_named_instance,
)
from mypy.types_utils import flatten_types
from mypy.typestate import SubtypeKind, type_state
from mypy.typevars import fill_typevars_with_any
from mypy.typevartuples import extract_unpack, fully_split_with_mapped_and_template

# Flags for detected protocol members
IS_SETTABLE: Final = 1
IS_CLASSVAR: Final = 2
IS_CLASS_OR_STATIC: Final = 3
IS_VAR: Final = 4

TypeParameterChecker: _TypeAlias = Callable[[Type, Type, int, bool, "SubtypeContext"], bool]


</t>
<t tx="ekr.20230831011820.2008">class SubtypeContext:
    @others
</t>
<t tx="ekr.20230831011820.2009">def __init__(
    self,
    *,
    # Non-proper subtype flags
    ignore_type_params: bool = False,
    ignore_pos_arg_names: bool = False,
    ignore_declared_variance: bool = False,
    # Supported for both proper and non-proper
    ignore_promotions: bool = False,
    ignore_uninhabited: bool = False,
    # Proper subtype flags
    erase_instances: bool = False,
    keep_erased_types: bool = False,
    options: Options | None = None,
) -&gt; None:
    self.ignore_type_params = ignore_type_params
    self.ignore_pos_arg_names = ignore_pos_arg_names
    self.ignore_declared_variance = ignore_declared_variance
    self.ignore_promotions = ignore_promotions
    self.ignore_uninhabited = ignore_uninhabited
    self.erase_instances = erase_instances
    self.keep_erased_types = keep_erased_types
    self.options = options

</t>
<t tx="ekr.20230831011820.201">    def pretty_overload(
        self,
        tp: Overloaded,
        context: Context,
        offset: int,
        *,
        add_class_or_static_decorator: bool = False,
        allow_dups: bool = False,
        code: ErrorCode | None = None,
        skip_self: bool = False,
    ) -&gt; None:
        for item in tp.items:
            self.note("@overload", context, offset=offset, allow_dups=allow_dups, code=code)

            if add_class_or_static_decorator:
                decorator = pretty_class_or_static_decorator(item)
                if decorator is not None:
                    self.note(decorator, context, offset=offset, allow_dups=allow_dups, code=code)

            self.note(
                pretty_callable(item, self.options, skip_self=skip_self),
                context,
                offset=offset,
                allow_dups=allow_dups,
                code=code,
            )

</t>
<t tx="ekr.20230831011820.2010">def check_context(self, proper_subtype: bool) -&gt; None:
    # Historically proper and non-proper subtypes were defined using different helpers
    # and different visitors. Check if flag values are such that we definitely support.
    if proper_subtype:
        assert not self.ignore_pos_arg_names and not self.ignore_declared_variance
    else:
        assert not self.erase_instances and not self.keep_erased_types


</t>
<t tx="ekr.20230831011820.2011">def is_subtype(
    left: Type,
    right: Type,
    *,
    subtype_context: SubtypeContext | None = None,
    ignore_type_params: bool = False,
    ignore_pos_arg_names: bool = False,
    ignore_declared_variance: bool = False,
    ignore_promotions: bool = False,
    ignore_uninhabited: bool = False,
    options: Options | None = None,
) -&gt; bool:
    """Is 'left' subtype of 'right'?

    Also consider Any to be a subtype of any type, and vice versa. This
    recursively applies to components of composite types (List[int] is subtype
    of List[Any], for example).

    type_parameter_checker is used to check the type parameters (for example,
    A with B in is_subtype(C[A], C[B]). The default checks for subtype relation
    between the type arguments (e.g., A and B), taking the variance of the
    type var into account.
    """
    if subtype_context is None:
        subtype_context = SubtypeContext(
            ignore_type_params=ignore_type_params,
            ignore_pos_arg_names=ignore_pos_arg_names,
            ignore_declared_variance=ignore_declared_variance,
            ignore_promotions=ignore_promotions,
            ignore_uninhabited=ignore_uninhabited,
            options=options,
        )
    else:
        assert not any(
            {
                ignore_type_params,
                ignore_pos_arg_names,
                ignore_declared_variance,
                ignore_promotions,
                ignore_uninhabited,
                options,
            }
        ), "Don't pass both context and individual flags"
    if type_state.is_assumed_subtype(left, right):
        return True
    if mypy.typeops.is_recursive_pair(left, right):
        # This case requires special care because it may cause infinite recursion.
        # Our view on recursive types is known under a fancy name of iso-recursive mu-types.
        # Roughly this means that a recursive type is defined as an alias where right hand side
        # can refer to the type as a whole, for example:
        #     A = Union[int, Tuple[A, ...]]
        # and an alias unrolled once represents the *same type*, in our case all these represent
        # the same type:
        #    A
        #    Union[int, Tuple[A, ...]]
        #    Union[int, Tuple[Union[int, Tuple[A, ...]], ...]]
        # The algorithm for subtyping is then essentially under the assumption that left &lt;: right,
        # check that get_proper_type(left) &lt;: get_proper_type(right). On the example above,
        # If we start with:
        #     A = Union[int, Tuple[A, ...]]
        #     B = Union[int, Tuple[B, ...]]
        # When checking if A &lt;: B we push pair (A, B) onto 'assuming' stack, then when after few
        # steps we come back to initial call is_subtype(A, B) and immediately return True.
        with pop_on_exit(type_state.get_assumptions(is_proper=False), left, right):
            return _is_subtype(left, right, subtype_context, proper_subtype=False)
    return _is_subtype(left, right, subtype_context, proper_subtype=False)


</t>
<t tx="ekr.20230831011820.2012">def is_proper_subtype(
    left: Type,
    right: Type,
    *,
    subtype_context: SubtypeContext | None = None,
    ignore_promotions: bool = False,
    ignore_uninhabited: bool = False,
    erase_instances: bool = False,
    keep_erased_types: bool = False,
) -&gt; bool:
    """Is left a proper subtype of right?

    For proper subtypes, there's no need to rely on compatibility due to
    Any types. Every usable type is a proper subtype of itself.

    If erase_instances is True, erase left instance *after* mapping it to supertype
    (this is useful for runtime isinstance() checks). If keep_erased_types is True,
    do not consider ErasedType a subtype of all types (used by type inference against unions).
    """
    if subtype_context is None:
        subtype_context = SubtypeContext(
            ignore_promotions=ignore_promotions,
            ignore_uninhabited=ignore_uninhabited,
            erase_instances=erase_instances,
            keep_erased_types=keep_erased_types,
        )
    else:
        assert not any(
            {
                ignore_promotions,
                ignore_uninhabited,
                erase_instances,
                keep_erased_types,
                ignore_uninhabited,
            }
        ), "Don't pass both context and individual flags"
    if type_state.is_assumed_proper_subtype(left, right):
        return True
    if mypy.typeops.is_recursive_pair(left, right):
        # Same as for non-proper subtype, see detailed comment there for explanation.
        with pop_on_exit(type_state.get_assumptions(is_proper=True), left, right):
            return _is_subtype(left, right, subtype_context, proper_subtype=True)
    return _is_subtype(left, right, subtype_context, proper_subtype=True)


</t>
<t tx="ekr.20230831011820.2013">def is_equivalent(
    a: Type,
    b: Type,
    *,
    ignore_type_params: bool = False,
    ignore_pos_arg_names: bool = False,
    options: Options | None = None,
    subtype_context: SubtypeContext | None = None,
) -&gt; bool:
    return is_subtype(
        a,
        b,
        ignore_type_params=ignore_type_params,
        ignore_pos_arg_names=ignore_pos_arg_names,
        options=options,
        subtype_context=subtype_context,
    ) and is_subtype(
        b,
        a,
        ignore_type_params=ignore_type_params,
        ignore_pos_arg_names=ignore_pos_arg_names,
        options=options,
        subtype_context=subtype_context,
    )


</t>
<t tx="ekr.20230831011820.2014">def is_same_type(
    a: Type, b: Type, ignore_promotions: bool = True, subtype_context: SubtypeContext | None = None
) -&gt; bool:
    """Are these types proper subtypes of each other?

    This means types may have different representation (e.g. an alias, or
    a non-simplified union) but are semantically exchangeable in all contexts.
    """
    # Note that using ignore_promotions=True (default) makes types like int and int64
    # considered not the same type (which is the case at runtime).
    # Also Union[bool, int] (if it wasn't simplified before) will be different
    # from plain int, etc.
    return is_proper_subtype(
        a, b, ignore_promotions=ignore_promotions, subtype_context=subtype_context
    ) and is_proper_subtype(
        b, a, ignore_promotions=ignore_promotions, subtype_context=subtype_context
    )


</t>
<t tx="ekr.20230831011820.2015"># This is a common entry point for subtyping checks (both proper and non-proper).
# Never call this private function directly, use the public versions.
def _is_subtype(
    left: Type, right: Type, subtype_context: SubtypeContext, proper_subtype: bool
) -&gt; bool:
    subtype_context.check_context(proper_subtype)
    orig_right = right
    orig_left = left
    left = get_proper_type(left)
    right = get_proper_type(right)

    if not proper_subtype and isinstance(right, (AnyType, UnboundType, ErasedType)):
        # TODO: should we consider all types proper subtypes of UnboundType and/or
        # ErasedType as we do for non-proper subtyping.
        return True

    if isinstance(right, UnionType) and not isinstance(left, UnionType):
        # Normally, when 'left' is not itself a union, the only way
        # 'left' can be a subtype of the union 'right' is if it is a
        # subtype of one of the items making up the union.
        if proper_subtype:
            is_subtype_of_item = any(
                is_proper_subtype(orig_left, item, subtype_context=subtype_context)
                for item in right.items
            )
        else:
            is_subtype_of_item = any(
                is_subtype(orig_left, item, subtype_context=subtype_context)
                for item in right.items
            )
        # Recombine rhs literal types, to make an enum type a subtype
        # of a union of all enum items as literal types. Only do it if
        # the previous check didn't succeed, since recombining can be
        # expensive.
        # `bool` is a special case, because `bool` is `Literal[True, False]`.
        if (
            not is_subtype_of_item
            and isinstance(left, Instance)
            and (left.type.is_enum or left.type.fullname == "builtins.bool")
        ):
            right = UnionType(mypy.typeops.try_contracting_literals_in_union(right.items))
            if proper_subtype:
                is_subtype_of_item = any(
                    is_proper_subtype(orig_left, item, subtype_context=subtype_context)
                    for item in right.items
                )
            else:
                is_subtype_of_item = any(
                    is_subtype(orig_left, item, subtype_context=subtype_context)
                    for item in right.items
                )
        # However, if 'left' is a type variable T, T might also have
        # an upper bound which is itself a union. This case will be
        # handled below by the SubtypeVisitor. We have to check both
        # possibilities, to handle both cases like T &lt;: Union[T, U]
        # and cases like T &lt;: B where B is the upper bound of T and is
        # a union. (See #2314.)
        if not isinstance(left, TypeVarType):
            return is_subtype_of_item
        elif is_subtype_of_item:
            return True
        # otherwise, fall through
    return left.accept(SubtypeVisitor(orig_right, subtype_context, proper_subtype))


</t>
<t tx="ekr.20230831011820.2016">def check_type_parameter(
    left: Type, right: Type, variance: int, proper_subtype: bool, subtype_context: SubtypeContext
) -&gt; bool:
    if variance == COVARIANT:
        if proper_subtype:
            return is_proper_subtype(left, right, subtype_context=subtype_context)
        else:
            return is_subtype(left, right, subtype_context=subtype_context)
    elif variance == CONTRAVARIANT:
        if proper_subtype:
            return is_proper_subtype(right, left, subtype_context=subtype_context)
        else:
            return is_subtype(right, left, subtype_context=subtype_context)
    else:
        if proper_subtype:
            # We pass ignore_promotions=False because it is a default for subtype checks.
            # The actual value will be taken from the subtype_context, and it is whatever
            # the original caller passed.
            return is_same_type(
                left, right, ignore_promotions=False, subtype_context=subtype_context
            )
        else:
            return is_equivalent(left, right, subtype_context=subtype_context)


</t>
<t tx="ekr.20230831011820.2017">class SubtypeVisitor(TypeVisitor[bool]):
    @others
</t>
<t tx="ekr.20230831011820.2018">def __init__(self, right: Type, subtype_context: SubtypeContext, proper_subtype: bool) -&gt; None:
    self.right = get_proper_type(right)
    self.orig_right = right
    self.proper_subtype = proper_subtype
    self.subtype_context = subtype_context
    self.options = subtype_context.options
    self._subtype_kind = SubtypeVisitor.build_subtype_kind(subtype_context, proper_subtype)

</t>
<t tx="ekr.20230831011820.2019">@staticmethod
def build_subtype_kind(subtype_context: SubtypeContext, proper_subtype: bool) -&gt; SubtypeKind:
    return (
        state.strict_optional,
        proper_subtype,
        subtype_context.ignore_type_params,
        subtype_context.ignore_pos_arg_names,
        subtype_context.ignore_declared_variance,
        subtype_context.ignore_promotions,
        subtype_context.erase_instances,
        subtype_context.keep_erased_types,
    )

</t>
<t tx="ekr.20230831011820.202">    def print_more(
        self,
        conflicts: Sequence[Any],
        context: Context,
        offset: int,
        max_items: int,
        *,
        code: ErrorCode | None = None,
    ) -&gt; None:
        if len(conflicts) &gt; max_items:
            self.note(
                f"&lt;{len(conflicts) - max_items} more conflict(s) not shown&gt;",
                context,
                offset=offset,
                code=code,
            )

</t>
<t tx="ekr.20230831011820.2020">def _is_subtype(self, left: Type, right: Type) -&gt; bool:
    if self.proper_subtype:
        return is_proper_subtype(left, right, subtype_context=self.subtype_context)
    return is_subtype(left, right, subtype_context=self.subtype_context)

</t>
<t tx="ekr.20230831011820.2021"># visit_x(left) means: is left (which is an instance of X) a subtype of right?

def visit_unbound_type(self, left: UnboundType) -&gt; bool:
    # This can be called if there is a bad type annotation. The result probably
    # doesn't matter much but by returning True we simplify these bad types away
    # from unions, which could filter out some bogus messages.
    return True

</t>
<t tx="ekr.20230831011820.2022">def visit_any(self, left: AnyType) -&gt; bool:
    return isinstance(self.right, AnyType) if self.proper_subtype else True

</t>
<t tx="ekr.20230831011820.2023">def visit_none_type(self, left: NoneType) -&gt; bool:
    if state.strict_optional:
        if isinstance(self.right, NoneType) or is_named_instance(
            self.right, "builtins.object"
        ):
            return True
        if isinstance(self.right, Instance) and self.right.type.is_protocol:
            members = self.right.type.protocol_members
            # None is compatible with Hashable (and other similar protocols). This is
            # slightly sloppy since we don't check the signature of "__hash__".
            # None is also compatible with `SupportsStr` protocol.
            return not members or all(member in ("__hash__", "__str__") for member in members)
        return False
    else:
        return True

</t>
<t tx="ekr.20230831011820.2024">def visit_uninhabited_type(self, left: UninhabitedType) -&gt; bool:
    # We ignore this for unsafe overload checks, so that and empty list and
    # a list of int will be considered non-overlapping.
    if isinstance(self.right, UninhabitedType):
        return True
    return not self.subtype_context.ignore_uninhabited

</t>
<t tx="ekr.20230831011820.2025">def visit_erased_type(self, left: ErasedType) -&gt; bool:
    # This may be encountered during type inference. The result probably doesn't
    # matter much.
    # TODO: it actually does matter, figure out more principled logic about this.
    return not self.subtype_context.keep_erased_types

</t>
<t tx="ekr.20230831011820.2026">def visit_deleted_type(self, left: DeletedType) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011820.2027">def visit_instance(self, left: Instance) -&gt; bool:
    if left.type.fallback_to_any and not self.proper_subtype:
        # NOTE: `None` is a *non-subclassable* singleton, therefore no class
        # can by a subtype of it, even with an `Any` fallback.
        # This special case is needed to treat descriptors in classes with
        # dynamic base classes correctly, see #5456.
        return not isinstance(self.right, NoneType)
    right = self.right
    if isinstance(right, TupleType) and right.partial_fallback.type.is_enum:
        return self._is_subtype(left, mypy.typeops.tuple_fallback(right))
    if isinstance(right, Instance):
        if type_state.is_cached_subtype_check(self._subtype_kind, left, right):
            return True
        if type_state.is_cached_negative_subtype_check(self._subtype_kind, left, right):
            return False
        if not self.subtype_context.ignore_promotions:
            for base in left.type.mro:
                if base._promote and any(
                    self._is_subtype(p, self.right) for p in base._promote
                ):
                    type_state.record_subtype_cache_entry(self._subtype_kind, left, right)
                    return True
            # Special case: Low-level integer types are compatible with 'int'. We can't
            # use promotions, since 'int' is already promoted to low-level integer types,
            # and we can't have circular promotions.
            if left.type.alt_promote and left.type.alt_promote.type is right.type:
                return True
        rname = right.type.fullname
        # Always try a nominal check if possible,
        # there might be errors that a user wants to silence *once*.
        # NamedTuples are a special case, because `NamedTuple` is not listed
        # in `TypeInfo.mro`, so when `(a: NamedTuple) -&gt; None` is used,
        # we need to check for `is_named_tuple` property
        if (
            left.type.has_base(rname)
            or rname == "builtins.object"
            or (
                rname in TYPED_NAMEDTUPLE_NAMES
                and any(l.is_named_tuple for l in left.type.mro)
            )
        ) and not self.subtype_context.ignore_declared_variance:
            # Map left type to corresponding right instances.
            t = map_instance_to_supertype(left, right.type)
            if self.subtype_context.erase_instances:
                erased = erase_type(t)
                assert isinstance(erased, Instance)
                t = erased
            nominal = True
            if right.type.has_type_var_tuple_type:
                assert left.type.type_var_tuple_prefix is not None
                assert left.type.type_var_tuple_suffix is not None
                assert right.type.type_var_tuple_prefix is not None
                assert right.type.type_var_tuple_suffix is not None
                split_result = fully_split_with_mapped_and_template(
                    left.args,
                    left.type.type_var_tuple_prefix,
                    left.type.type_var_tuple_suffix,
                    right.args,
                    right.type.type_var_tuple_prefix,
                    right.type.type_var_tuple_suffix,
                )
                if split_result is None:
                    return False

                (
                    left_prefix,
                    left_mprefix,
                    left_middle,
                    left_msuffix,
                    left_suffix,
                    right_prefix,
                    right_mprefix,
                    right_middle,
                    right_msuffix,
                    right_suffix,
                ) = split_result

                left_unpacked = extract_unpack(left_middle)
                right_unpacked = extract_unpack(right_middle)

                # Helper for case 2 below so we can treat them the same.
                def check_mixed(
                    unpacked_type: ProperType, compare_to: tuple[Type, ...]
                ) -&gt; bool:
                    if (
                        isinstance(unpacked_type, Instance)
                        and unpacked_type.type.fullname == "builtins.tuple"
                    ):
                        return all(is_equivalent(l, unpacked_type.args[0]) for l in compare_to)
                    if isinstance(unpacked_type, TypeVarTupleType):
                        return False
                    if isinstance(unpacked_type, AnyType):
                        return True
                    if isinstance(unpacked_type, TupleType):
                        if len(unpacked_type.items) != len(compare_to):
                            return False
                        for t1, t2 in zip(unpacked_type.items, compare_to):
                            if not is_equivalent(t1, t2):
                                return False
                        return True
                    return False

                # Case 1: Both are unpacks, in this case we check what is being
                # unpacked is the same.
                if left_unpacked is not None and right_unpacked is not None:
                    if not is_equivalent(left_unpacked, right_unpacked):
                        return False

                # Case 2: Only one of the types is an unpack. The equivalence
                # case is mostly the same but we check some additional
                # things when unpacking on the right.
                elif left_unpacked is not None and right_unpacked is None:
                    if not check_mixed(left_unpacked, right_middle):
                        return False
                elif left_unpacked is None and right_unpacked is not None:
                    if not check_mixed(right_unpacked, left_middle):
                        return False

                # Case 3: Neither type is an unpack. In this case we just compare
                # the items themselves.
                else:
                    if len(left_middle) != len(right_middle):
                        return False
                    for left_t, right_t in zip(left_middle, right_middle):
                        if not is_equivalent(left_t, right_t):
                            return False

                assert len(left_mprefix) == len(right_mprefix)
                assert len(left_msuffix) == len(right_msuffix)

                for left_item, right_item in zip(
                    left_mprefix + left_msuffix, right_mprefix + right_msuffix
                ):
                    if not is_equivalent(left_item, right_item):
                        return False

                left_items = t.args[: right.type.type_var_tuple_prefix]
                right_items = right.args[: right.type.type_var_tuple_prefix]
                if right.type.type_var_tuple_suffix:
                    left_items += t.args[-right.type.type_var_tuple_suffix :]
                    right_items += right.args[-right.type.type_var_tuple_suffix :]
                unpack_index = right.type.type_var_tuple_prefix
                assert unpack_index is not None
                type_params = zip(
                    left_prefix + left_suffix,
                    right_prefix + right_suffix,
                    right.type.defn.type_vars[:unpack_index]
                    + right.type.defn.type_vars[unpack_index + 1 :],
                )
            else:
                type_params = zip(t.args, right.args, right.type.defn.type_vars)
            if not self.subtype_context.ignore_type_params:
                for lefta, righta, tvar in type_params:
                    if isinstance(tvar, TypeVarType):
                        if not check_type_parameter(
                            lefta,
                            righta,
                            tvar.variance,
                            self.proper_subtype,
                            self.subtype_context,
                        ):
                            nominal = False
                    else:
                        # TODO: everywhere else ParamSpecs are handled as invariant.
                        if not check_type_parameter(
                            lefta, righta, COVARIANT, self.proper_subtype, self.subtype_context
                        ):
                            nominal = False
            if nominal:
                type_state.record_subtype_cache_entry(self._subtype_kind, left, right)
            else:
                type_state.record_negative_subtype_cache_entry(self._subtype_kind, left, right)
            return nominal
        if right.type.is_protocol and is_protocol_implementation(
            left, right, proper_subtype=self.proper_subtype, options=self.options
        ):
            return True
        # We record negative cache entry here, and not in the protocol check like we do for
        # positive cache, to avoid accidentally adding a type that is not a structural
        # subtype, but is a nominal subtype (involving type: ignore override).
        type_state.record_negative_subtype_cache_entry(self._subtype_kind, left, right)
        return False
    if isinstance(right, TypeType):
        item = right.item
        if isinstance(item, TupleType):
            item = mypy.typeops.tuple_fallback(item)
        # TODO: this is a bit arbitrary, we should only skip Any-related cases.
        if not self.proper_subtype:
            if is_named_instance(left, "builtins.type"):
                return self._is_subtype(TypeType(AnyType(TypeOfAny.special_form)), right)
            if left.type.is_metaclass():
                if isinstance(item, AnyType):
                    return True
                if isinstance(item, Instance):
                    return is_named_instance(item, "builtins.object")
    if isinstance(right, LiteralType) and left.last_known_value is not None:
        return self._is_subtype(left.last_known_value, right)
    if isinstance(right, CallableType):
        # Special case: Instance can be a subtype of Callable.
        call = find_member("__call__", left, left, is_operator=True)
        if call:
            return self._is_subtype(call, right)
        return False
    else:
        return False

</t>
<t tx="ekr.20230831011820.2028">def visit_type_var(self, left: TypeVarType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeVarType) and left.id == right.id:
        return True
    if left.values and self._is_subtype(UnionType.make_union(left.values), right):
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20230831011820.2029">def visit_param_spec(self, left: ParamSpecType) -&gt; bool:
    right = self.right
    if (
        isinstance(right, ParamSpecType)
        and right.id == left.id
        and right.flavor == left.flavor
    ):
        return self._is_subtype(left.prefix, right.prefix)
    if isinstance(right, Parameters) and are_trivial_parameters(right):
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20230831011820.203">    def try_report_long_tuple_assignment_error(
        self,
        subtype: ProperType,
        supertype: ProperType,
        context: Context,
        msg: message_registry.ErrorMessage,
        subtype_label: str | None = None,
        supertype_label: str | None = None,
    ) -&gt; bool:
        """Try to generate meaningful error message for very long tuple assignment

        Returns a bool: True when generating long tuple assignment error,
        False when no such error reported
        """
        if isinstance(subtype, TupleType):
            if (
                len(subtype.items) &gt; 10
                and isinstance(supertype, Instance)
                and supertype.type.fullname == "builtins.tuple"
            ):
                lhs_type = supertype.args[0]
                lhs_types = [lhs_type] * len(subtype.items)
                self.generate_incompatible_tuple_error(lhs_types, subtype.items, context, msg)
                return True
            elif isinstance(supertype, TupleType) and (
                len(subtype.items) &gt; 10 or len(supertype.items) &gt; 10
            ):
                if len(subtype.items) != len(supertype.items):
                    if supertype_label is not None and subtype_label is not None:
                        msg = msg.with_additional_msg(
                            " ({} {}, {} {})".format(
                                subtype_label,
                                self.format_long_tuple_type(subtype),
                                supertype_label,
                                self.format_long_tuple_type(supertype),
                            )
                        )
                        self.fail(msg.value, context, code=msg.code)
                        return True
                self.generate_incompatible_tuple_error(
                    supertype.items, subtype.items, context, msg
                )
                return True
        return False

</t>
<t tx="ekr.20230831011820.2030">def visit_type_var_tuple(self, left: TypeVarTupleType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeVarTupleType) and right.id == left.id:
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20230831011820.2031">def visit_unpack_type(self, left: UnpackType) -&gt; bool:
    # TODO: Ideally we should not need this (since it is not a real type).
    # Instead callers (upper level types) should handle it when it appears in type list.
    if isinstance(self.right, UnpackType):
        return self._is_subtype(left.type, self.right.type)
    if isinstance(self.right, Instance) and self.right.type.fullname == "builtins.object":
        return True
    return False

</t>
<t tx="ekr.20230831011820.2032">def visit_parameters(self, left: Parameters) -&gt; bool:
    if isinstance(self.right, Parameters):
        # TODO: direction here should be opposite, this function expects
        # order of callables, while parameters are contravariant.
        return are_parameters_compatible(
            left,
            self.right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
        )
    else:
        return False

</t>
<t tx="ekr.20230831011820.2033">def visit_callable_type(self, left: CallableType) -&gt; bool:
    right = self.right
    if isinstance(right, CallableType):
        if left.type_guard is not None and right.type_guard is not None:
            if not self._is_subtype(left.type_guard, right.type_guard):
                return False
        elif right.type_guard is not None and left.type_guard is None:
            # This means that one function has `TypeGuard` and other does not.
            # They are not compatible. See https://github.com/python/mypy/issues/11307
            return False
        return is_callable_compatible(
            left,
            right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
            strict_concatenate=(self.options.extra_checks or self.options.strict_concatenate)
            if self.options
            else False,
        )
    elif isinstance(right, Overloaded):
        return all(self._is_subtype(left, item) for item in right.items)
    elif isinstance(right, Instance):
        if right.type.is_protocol and "__call__" in right.type.protocol_members:
            # OK, a callable can implement a protocol with a `__call__` member.
            # TODO: we should probably explicitly exclude self-types in this case.
            call = find_member("__call__", right, left, is_operator=True)
            assert call is not None
            if self._is_subtype(left, call):
                if len(right.type.protocol_members) == 1:
                    return True
                if is_protocol_implementation(left.fallback, right, skip=["__call__"]):
                    return True
        if right.type.is_protocol and left.is_type_obj():
            ret_type = get_proper_type(left.ret_type)
            if isinstance(ret_type, TupleType):
                ret_type = mypy.typeops.tuple_fallback(ret_type)
            if isinstance(ret_type, Instance) and is_protocol_implementation(
                ret_type, right, proper_subtype=self.proper_subtype, class_obj=True
            ):
                return True
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, TypeType):
        # This is unsound, we don't check the __init__ signature.
        return left.is_type_obj() and self._is_subtype(left.ret_type, right.item)
    else:
        return False

</t>
<t tx="ekr.20230831011820.2034">def visit_tuple_type(self, left: TupleType) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if is_named_instance(right, "typing.Sized"):
            return True
        elif is_named_instance(right, TUPLE_LIKE_INSTANCE_NAMES):
            if right.args:
                iter_type = right.args[0]
            else:
                if self.proper_subtype:
                    return False
                iter_type = AnyType(TypeOfAny.special_form)
            if is_named_instance(right, "builtins.tuple") and isinstance(
                get_proper_type(iter_type), AnyType
            ):
                # TODO: We shouldn't need this special case. This is currently needed
                #       for isinstance(x, tuple), though it's unclear why.
                return True
            for li in left.items:
                if isinstance(li, UnpackType):
                    unpack = get_proper_type(li.type)
                    if isinstance(unpack, Instance):
                        assert unpack.type.fullname == "builtins.tuple"
                        li = unpack.args[0]
                if not self._is_subtype(li, iter_type):
                    return False
            return True
        elif self._is_subtype(left.partial_fallback, right) and self._is_subtype(
            mypy.typeops.tuple_fallback(left), right
        ):
            return True
        return False
    elif isinstance(right, TupleType):
        if len(left.items) != len(right.items):
            # TODO: handle tuple with variadic items better.
            return False
        if any(not self._is_subtype(l, r) for l, r in zip(left.items, right.items)):
            return False
        rfallback = mypy.typeops.tuple_fallback(right)
        if is_named_instance(rfallback, "builtins.tuple"):
            # No need to verify fallback. This is useful since the calculated fallback
            # may be inconsistent due to how we calculate joins between unions vs.
            # non-unions. For example, join(int, str) == object, whereas
            # join(Union[int, C], Union[str, C]) == Union[int, str, C].
            return True
        lfallback = mypy.typeops.tuple_fallback(left)
        return self._is_subtype(lfallback, rfallback)
    else:
        return False

</t>
<t tx="ekr.20230831011820.2035">def visit_typeddict_type(self, left: TypedDictType) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, TypedDictType):
        if not left.names_are_wider_than(right):
            return False
        for name, l, r in left.zip(right):
            # TODO: should we pass on the full subtype_context here and below?
            if self.proper_subtype:
                check = is_same_type(l, r)
            else:
                check = is_equivalent(
                    l,
                    r,
                    ignore_type_params=self.subtype_context.ignore_type_params,
                    options=self.options,
                )
            if not check:
                return False
            # Non-required key is not compatible with a required key since
            # indexing may fail unexpectedly if a required key is missing.
            # Required key is not compatible with a non-required key since
            # the prior doesn't support 'del' but the latter should support
            # it.
            #
            # NOTE: 'del' support is currently not implemented (#3550). We
            #       don't want to have to change subtyping after 'del' support
            #       lands so here we are anticipating that change.
            if (name in left.required_keys) != (name in right.required_keys):
                return False
        # (NOTE: Fallbacks don't matter.)
        return True
    else:
        return False

</t>
<t tx="ekr.20230831011820.2036">def visit_literal_type(self, left: LiteralType) -&gt; bool:
    if isinstance(self.right, LiteralType):
        return left == self.right
    else:
        return self._is_subtype(left.fallback, self.right)

</t>
<t tx="ekr.20230831011820.2037">def visit_overloaded(self, left: Overloaded) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if right.type.is_protocol and "__call__" in right.type.protocol_members:
            # same as for CallableType
            call = find_member("__call__", right, left, is_operator=True)
            assert call is not None
            if self._is_subtype(left, call):
                if len(right.type.protocol_members) == 1:
                    return True
                if is_protocol_implementation(left.fallback, right, skip=["__call__"]):
                    return True
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, CallableType):
        for item in left.items:
            if self._is_subtype(item, right):
                return True
        return False
    elif isinstance(right, Overloaded):
        if left == self.right:
            # When it is the same overload, then the types are equal.
            return True

        # Ensure each overload in the right side (the supertype) is accounted for.
        previous_match_left_index = -1
        matched_overloads = set()

        for right_item in right.items:
            found_match = False

            for left_index, left_item in enumerate(left.items):
                subtype_match = self._is_subtype(left_item, right_item)

                # Order matters: we need to make sure that the index of
                # this item is at least the index of the previous one.
                if subtype_match and previous_match_left_index &lt;= left_index:
                    previous_match_left_index = left_index
                    found_match = True
                    matched_overloads.add(left_index)
                    break
                else:
                    # If this one overlaps with the supertype in any way, but it wasn't
                    # an exact match, then it's a potential error.
                    strict_concat = (
                        (self.options.extra_checks or self.options.strict_concatenate)
                        if self.options
                        else False
                    )
                    if left_index not in matched_overloads and (
                        is_callable_compatible(
                            left_item,
                            right_item,
                            is_compat=self._is_subtype,
                            ignore_return=True,
                            ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
                            strict_concatenate=strict_concat,
                        )
                        or is_callable_compatible(
                            right_item,
                            left_item,
                            is_compat=self._is_subtype,
                            ignore_return=True,
                            ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
                            strict_concatenate=strict_concat,
                        )
                    ):
                        return False

            if not found_match:
                return False
        return True
    elif isinstance(right, UnboundType):
        return True
    elif isinstance(right, TypeType):
        # All the items must have the same type object status, so
        # it's sufficient to query only (any) one of them.
        # This is unsound, we don't check all the __init__ signatures.
        return left.is_type_obj() and self._is_subtype(left.items[0], right)
    else:
        return False

</t>
<t tx="ekr.20230831011820.2038">def visit_union_type(self, left: UnionType) -&gt; bool:
    if isinstance(self.right, Instance):
        literal_types: set[Instance] = set()
        # avoid redundant check for union of literals
        for item in left.relevant_items():
            p_item = get_proper_type(item)
            lit_type = mypy.typeops.simple_literal_type(p_item)
            if lit_type is not None:
                if lit_type in literal_types:
                    continue
                literal_types.add(lit_type)
                item = lit_type
            if not self._is_subtype(item, self.orig_right):
                return False
        return True

    elif isinstance(self.right, UnionType):
        # prune literals early to avoid nasty quadratic behavior which would otherwise arise when checking
        # subtype relationships between slightly different narrowings of an Enum
        # we achieve O(N+M) instead of O(N*M)

        fast_check: set[ProperType] = set()

        for item in flatten_types(self.right.relevant_items()):
            p_item = get_proper_type(item)
            fast_check.add(p_item)
            if isinstance(p_item, Instance) and p_item.last_known_value is not None:
                fast_check.add(p_item.last_known_value)

        for item in left.relevant_items():
            p_item = get_proper_type(item)
            if p_item in fast_check:
                continue
            lit_type = mypy.typeops.simple_literal_type(p_item)
            if lit_type in fast_check:
                continue
            if not self._is_subtype(item, self.orig_right):
                return False
        return True

    return all(self._is_subtype(item, self.orig_right) for item in left.items)

</t>
<t tx="ekr.20230831011820.2039">def visit_partial_type(self, left: PartialType) -&gt; bool:
    # This is indeterminate as we don't really know the complete type yet.
    if self.proper_subtype:
        # TODO: What's the right thing to do here?
        return False
    if left.type is None:
        # Special case, partial `None`. This might happen when defining
        # class-level attributes with explicit `None`.
        # We can still recover from this.
        # https://github.com/python/mypy/issues/11105
        return self.visit_none_type(NoneType())
    raise RuntimeError(f'Partial type "{left}" cannot be checked with "issubtype()"')

</t>
<t tx="ekr.20230831011820.204">    def format_long_tuple_type(self, typ: TupleType) -&gt; str:
        """Format very long tuple type using an ellipsis notation"""
        item_cnt = len(typ.items)
        if item_cnt &gt; 10:
            return "{}[{}, {}, ... &lt;{} more items&gt;]".format(
                "tuple" if self.options.use_lowercase_names() else "Tuple",
                format_type_bare(typ.items[0], self.options),
                format_type_bare(typ.items[1], self.options),
                str(item_cnt - 2),
            )
        else:
            return format_type_bare(typ, self.options)

</t>
<t tx="ekr.20230831011820.2040">def visit_type_type(self, left: TypeType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeType):
        return self._is_subtype(left.item, right.item)
    if isinstance(right, CallableType):
        if self.proper_subtype and not right.is_type_obj():
            # We can't accept `Type[X]` as a *proper* subtype of Callable[P, X]
            # since this will break transitivity of subtyping.
            return False
        # This is unsound, we don't check the __init__ signature.
        return self._is_subtype(left.item, right.ret_type)
    if isinstance(right, Instance):
        if right.type.fullname in ["builtins.object", "builtins.type"]:
            # TODO: Strictly speaking, the type builtins.type is considered equivalent to
            #       Type[Any]. However, this would break the is_proper_subtype check in
            #       conditional_types for cases like isinstance(x, type) when the type
            #       of x is Type[int]. It's unclear what's the right way to address this.
            return True
        item = left.item
        if isinstance(item, TypeVarType):
            item = get_proper_type(item.upper_bound)
        if isinstance(item, Instance):
            if right.type.is_protocol and is_protocol_implementation(
                item, right, proper_subtype=self.proper_subtype, class_obj=True
            ):
                return True
            metaclass = item.type.metaclass_type
            return metaclass is not None and self._is_subtype(metaclass, right)
    return False

</t>
<t tx="ekr.20230831011820.2041">def visit_type_alias_type(self, left: TypeAliasType) -&gt; bool:
    assert False, f"This should be never called, got {left}"


</t>
<t tx="ekr.20230831011820.2042">T = TypeVar("T", bound=Type)


@contextmanager
def pop_on_exit(stack: list[tuple[T, T]], left: T, right: T) -&gt; Iterator[None]:
    stack.append((left, right))
    yield
    stack.pop()


</t>
<t tx="ekr.20230831011820.2043">def is_protocol_implementation(
    left: Instance,
    right: Instance,
    proper_subtype: bool = False,
    class_obj: bool = False,
    skip: list[str] | None = None,
    options: Options | None = None,
) -&gt; bool:
    """Check whether 'left' implements the protocol 'right'.

    If 'proper_subtype' is True, then check for a proper subtype.
    Treat recursive protocols by using the 'assuming' structural subtype matrix
    (in sparse representation, i.e. as a list of pairs (subtype, supertype)),
    see also comment in nodes.TypeInfo. When we enter a check for classes
    (A, P), defined as following::

      class P(Protocol):
          def f(self) -&gt; P: ...
      class A:
          def f(self) -&gt; A: ...

    this results in A being a subtype of P without infinite recursion.
    On every false result, we pop the assumption, thus avoiding an infinite recursion
    as well.
    """
    assert right.type.is_protocol
    if skip is None:
        skip = []
    # We need to record this check to generate protocol fine-grained dependencies.
    type_state.record_protocol_subtype_check(left.type, right.type)
    # nominal subtyping currently ignores '__init__' and '__new__' signatures
    members_not_to_check = {"__init__", "__new__"}
    members_not_to_check.update(skip)
    # Trivial check that circumvents the bug described in issue 9771:
    if left.type.is_protocol:
        members_right = set(right.type.protocol_members) - members_not_to_check
        members_left = set(left.type.protocol_members) - members_not_to_check
        if not members_right.issubset(members_left):
            return False
    assuming = right.type.assuming_proper if proper_subtype else right.type.assuming
    for l, r in reversed(assuming):
        if l == left and r == right:
            return True
    with pop_on_exit(assuming, left, right):
        for member in right.type.protocol_members:
            if member in members_not_to_check:
                continue
            ignore_names = member != "__call__"  # __call__ can be passed kwargs
            # The third argument below indicates to what self type is bound.
            # We always bind self to the subtype. (Similarly to nominal types).
            supertype = get_proper_type(find_member(member, right, left))
            assert supertype is not None

            subtype = mypy.typeops.get_protocol_member(left, member, class_obj)
            # Useful for debugging:
            # print(member, 'of', left, 'has type', subtype)
            # print(member, 'of', right, 'has type', supertype)
            if not subtype:
                return False
            if isinstance(subtype, PartialType):
                subtype = (
                    NoneType()
                    if subtype.type is None
                    else Instance(
                        subtype.type,
                        [AnyType(TypeOfAny.unannotated)] * len(subtype.type.type_vars),
                    )
                )
            if not proper_subtype:
                # Nominal check currently ignores arg names
                # NOTE: If we ever change this, be sure to also change the call to
                # SubtypeVisitor.build_subtype_kind(...) down below.
                is_compat = is_subtype(
                    subtype, supertype, ignore_pos_arg_names=ignore_names, options=options
                )
            else:
                is_compat = is_proper_subtype(subtype, supertype)
            if not is_compat:
                return False
            if isinstance(subtype, NoneType) and isinstance(supertype, CallableType):
                # We want __hash__ = None idiom to work even without --strict-optional
                return False
            subflags = get_member_flags(member, left, class_obj=class_obj)
            superflags = get_member_flags(member, right)
            if IS_SETTABLE in superflags:
                # Check opposite direction for settable attributes.
                if not is_subtype(supertype, subtype, options=options):
                    return False
            if not class_obj:
                if IS_SETTABLE not in superflags:
                    if IS_CLASSVAR in superflags and IS_CLASSVAR not in subflags:
                        return False
                elif (IS_CLASSVAR in subflags) != (IS_CLASSVAR in superflags):
                    return False
            else:
                if IS_VAR in superflags and IS_CLASSVAR not in subflags:
                    # Only class variables are allowed for class object access.
                    return False
                if IS_CLASSVAR in superflags:
                    # This can be never matched by a class object.
                    return False
            if IS_SETTABLE in superflags and IS_SETTABLE not in subflags:
                return False
            # This rule is copied from nominal check in checker.py
            if IS_CLASS_OR_STATIC in superflags and IS_CLASS_OR_STATIC not in subflags:
                return False

    if not proper_subtype:
        # Nominal check currently ignores arg names, but __call__ is special for protocols
        ignore_names = right.type.protocol_members != ["__call__"]
    else:
        ignore_names = False
    subtype_kind = SubtypeVisitor.build_subtype_kind(
        subtype_context=SubtypeContext(ignore_pos_arg_names=ignore_names),
        proper_subtype=proper_subtype,
    )
    type_state.record_subtype_cache_entry(subtype_kind, left, right)
    return True


</t>
<t tx="ekr.20230831011820.2044">def find_member(
    name: str, itype: Instance, subtype: Type, is_operator: bool = False, class_obj: bool = False
) -&gt; Type | None:
    """Find the type of member by 'name' in 'itype's TypeInfo.

    Find the member type after applying type arguments from 'itype', and binding
    'self' to 'subtype'. Return None if member was not found.
    """
    # TODO: this code shares some logic with checkmember.analyze_member_access,
    # consider refactoring.
    info = itype.type
    method = info.get_method(name)
    if method:
        if isinstance(method, Decorator):
            return find_node_type(method.var, itype, subtype, class_obj=class_obj)
        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            return find_node_type(dec.var, itype, subtype, class_obj=class_obj)
        return find_node_type(method, itype, subtype, class_obj=class_obj)
    else:
        # don't have such method, maybe variable or decorator?
        node = info.get(name)
        v = node.node if node else None
        if isinstance(v, Var):
            return find_node_type(v, itype, subtype, class_obj=class_obj)
        if (
            not v
            and name not in ["__getattr__", "__setattr__", "__getattribute__"]
            and not is_operator
            and not class_obj
            and itype.extra_attrs is None  # skip ModuleType.__getattr__
        ):
            for method_name in ("__getattribute__", "__getattr__"):
                # Normally, mypy assumes that instances that define __getattr__ have all
                # attributes with the corresponding return type. If this will produce
                # many false negatives, then this could be prohibited for
                # structural subtyping.
                method = info.get_method(method_name)
                if method and method.info.fullname != "builtins.object":
                    if isinstance(method, Decorator):
                        getattr_type = get_proper_type(find_node_type(method.var, itype, subtype))
                    else:
                        getattr_type = get_proper_type(find_node_type(method, itype, subtype))
                    if isinstance(getattr_type, CallableType):
                        return getattr_type.ret_type
                    return getattr_type
        if itype.type.fallback_to_any or class_obj and itype.type.meta_fallback_to_any:
            return AnyType(TypeOfAny.special_form)
        if isinstance(v, TypeInfo):
            # PEP 544 doesn't specify anything about such use cases. So we just try
            # to do something meaningful (at least we should not crash).
            return TypeType(fill_typevars_with_any(v))
    if itype.extra_attrs and name in itype.extra_attrs.attrs:
        return itype.extra_attrs.attrs[name]
    return None


</t>
<t tx="ekr.20230831011820.2045">def get_member_flags(name: str, itype: Instance, class_obj: bool = False) -&gt; set[int]:
    """Detect whether a member 'name' is settable, whether it is an
    instance or class variable, and whether it is class or static method.

    The flags are defined as following:
    * IS_SETTABLE: whether this attribute can be set, not set for methods and
      non-settable properties;
    * IS_CLASSVAR: set if the variable is annotated as 'x: ClassVar[t]';
    * IS_CLASS_OR_STATIC: set for methods decorated with @classmethod or
      with @staticmethod.
    """
    info = itype.type
    method = info.get_method(name)
    setattr_meth = info.get_method("__setattr__")
    if method:
        if isinstance(method, Decorator):
            if method.var.is_staticmethod or method.var.is_classmethod:
                return {IS_CLASS_OR_STATIC}
            elif method.var.is_property:
                return {IS_VAR}
        elif method.is_property:  # this could be settable property
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            if dec.var.is_settable_property or setattr_meth:
                return {IS_VAR, IS_SETTABLE}
            else:
                return {IS_VAR}
        return set()  # Just a regular method
    node = info.get(name)
    if not node:
        if setattr_meth:
            return {IS_SETTABLE}
        if itype.extra_attrs and name in itype.extra_attrs.attrs:
            flags = set()
            if name not in itype.extra_attrs.immutable:
                flags.add(IS_SETTABLE)
            return flags
        return set()
    v = node.node
    # just a variable
    if isinstance(v, Var):
        if v.is_property:
            return {IS_VAR}
        flags = {IS_VAR}
        if not v.is_final:
            flags.add(IS_SETTABLE)
        if v.is_classvar:
            flags.add(IS_CLASSVAR)
        if class_obj and v.is_inferred:
            flags.add(IS_CLASSVAR)
        return flags
    return set()


</t>
<t tx="ekr.20230831011820.2046">def find_node_type(
    node: Var | FuncBase, itype: Instance, subtype: Type, class_obj: bool = False
) -&gt; Type:
    """Find type of a variable or method 'node' (maybe also a decorated method).
    Apply type arguments from 'itype', and bind 'self' to 'subtype'.
    """
    from mypy.typeops import bind_self

    if isinstance(node, FuncBase):
        typ: Type | None = mypy.typeops.function_type(
            node, fallback=Instance(itype.type.mro[-1], [])
        )
    else:
        typ = node.type
        if typ is not None:
            typ = expand_self_type(node, typ, subtype)
    p_typ = get_proper_type(typ)
    if typ is None:
        return AnyType(TypeOfAny.from_error)
    # We don't need to bind 'self' for static methods, since there is no 'self'.
    if isinstance(node, FuncBase) or (
        isinstance(p_typ, FunctionLike)
        and node.is_initialized_in_class
        and not node.is_staticmethod
    ):
        assert isinstance(p_typ, FunctionLike)
        if class_obj and not (
            node.is_class if isinstance(node, FuncBase) else node.is_classmethod
        ):
            # Don't bind instance methods on class objects.
            signature = p_typ
        else:
            signature = bind_self(
                p_typ, subtype, is_classmethod=isinstance(node, Var) and node.is_classmethod
            )
        if node.is_property and not class_obj:
            assert isinstance(signature, CallableType)
            typ = signature.ret_type
        else:
            typ = signature
    itype = map_instance_to_supertype(itype, node.info)
    typ = expand_type_by_instance(typ, itype)
    return typ


</t>
<t tx="ekr.20230831011820.2047">def non_method_protocol_members(tp: TypeInfo) -&gt; list[str]:
    """Find all non-callable members of a protocol."""

    assert tp.is_protocol
    result: list[str] = []
    anytype = AnyType(TypeOfAny.special_form)
    instance = Instance(tp, [anytype] * len(tp.defn.type_vars))

    for member in tp.protocol_members:
        typ = get_proper_type(find_member(member, instance, instance))
        if not isinstance(typ, (Overloaded, CallableType)):
            result.append(member)
    return result


</t>
<t tx="ekr.20230831011820.2048">def is_callable_compatible(
    left: CallableType,
    right: CallableType,
    *,
    is_compat: Callable[[Type, Type], bool],
    is_compat_return: Callable[[Type, Type], bool] | None = None,
    ignore_return: bool = False,
    ignore_pos_arg_names: bool = False,
    check_args_covariantly: bool = False,
    allow_partial_overlap: bool = False,
    strict_concatenate: bool = False,
    no_unify_none: bool = False,
) -&gt; bool:
    """Is the left compatible with the right, using the provided compatibility check?

    is_compat:
        The check we want to run against the parameters.

    is_compat_return:
        The check we want to run against the return type.
        If None, use the 'is_compat' check.

    check_args_covariantly:
        If true, check if the left's args is compatible with the right's
        instead of the other way around (contravariantly).

        This function is mostly used to check if the left is a subtype of the right which
        is why the default is to check the args contravariantly. However, it's occasionally
        useful to check the args using some other check, so we leave the variance
        configurable.

        For example, when checking the validity of overloads, it's useful to see if
        the first overload alternative has more precise arguments then the second.
        We would want to check the arguments covariantly in that case.

        Note! The following two function calls are NOT equivalent:

            is_callable_compatible(f, g, is_compat=is_subtype, check_args_covariantly=False)
            is_callable_compatible(g, f, is_compat=is_subtype, check_args_covariantly=True)

        The two calls are similar in that they both check the function arguments in
        the same direction: they both run `is_subtype(argument_from_g, argument_from_f)`.

        However, the two calls differ in which direction they check things like
        keyword arguments. For example, suppose f and g are defined like so:

            def f(x: int, *y: int) -&gt; int: ...
            def g(x: int) -&gt; int: ...

        In this case, the first call will succeed and the second will fail: f is a
        valid stand-in for g but not vice-versa.

    allow_partial_overlap:
        By default this function returns True if and only if *all* calls to left are
        also calls to right (with respect to the provided 'is_compat' function).

        If this parameter is set to 'True', we return True if *there exists at least one*
        call to left that's also a call to right.

        In other words, we perform an existential check instead of a universal one;
        we require left to only overlap with right instead of being a subset.

        For example, suppose we set 'is_compat' to some subtype check and compare following:

            f(x: float, y: str = "...", *args: bool) -&gt; str
            g(*args: int) -&gt; str

        This function would normally return 'False': f is not a subtype of g.
        However, we would return True if this parameter is set to 'True': the two
        calls are compatible if the user runs "f_or_g(3)". In the context of that
        specific call, the two functions effectively have signatures of:

            f2(float) -&gt; str
            g2(int) -&gt; str

        Here, f2 is a valid subtype of g2 so we return True.

        Specifically, if this parameter is set this function will:

        -   Ignore optional arguments on either the left or right that have no
            corresponding match.
        -   No longer mandate optional arguments on either side are also optional
            on the other.
        -   No longer mandate that if right has a *arg or **kwarg that left must also
            have the same.

        Note: when this argument is set to True, this function becomes "symmetric" --
        the following calls are equivalent:

            is_callable_compatible(f, g,
                                   is_compat=some_check,
                                   check_args_covariantly=False,
                                   allow_partial_overlap=True)
            is_callable_compatible(g, f,
                                   is_compat=some_check,
                                   check_args_covariantly=True,
                                   allow_partial_overlap=True)

        If the 'some_check' function is also symmetric, the two calls would be equivalent
        whether or not we check the args covariantly.
    """
    # Normalize both types before comparing them.
    left = left.with_unpacked_kwargs().with_normalized_var_args()
    right = right.with_unpacked_kwargs().with_normalized_var_args()

    if is_compat_return is None:
        is_compat_return = is_compat

    # If either function is implicitly typed, ignore positional arg names too
    if left.implicit or right.implicit:
        ignore_pos_arg_names = True

    # Non-type cannot be a subtype of type.
    if right.is_type_obj() and not left.is_type_obj() and not allow_partial_overlap:
        return False

    # A callable L is a subtype of a generic callable R if L is a
    # subtype of every type obtained from R by substituting types for
    # the variables of R. We can check this by simply leaving the
    # generic variables of R as type variables, effectively varying
    # over all possible values.

    # It's okay even if these variables share ids with generic
    # type variables of L, because generating and solving
    # constraints for the variables of L to make L a subtype of R
    # (below) treats type variables on the two sides as independent.
    if left.variables:
        # Apply generic type variables away in left via type inference.
        unified = unify_generic_callable(
            left, right, ignore_return=ignore_return, no_unify_none=no_unify_none
        )
        if unified is None:
            return False
        left = unified

    # If we allow partial overlaps, we don't need to leave R generic:
    # if we can find even just a single typevar assignment which
    # would make these callables compatible, we should return True.

    # So, we repeat the above checks in the opposite direction. This also
    # lets us preserve the 'symmetry' property of allow_partial_overlap.
    if allow_partial_overlap and right.variables:
        unified = unify_generic_callable(
            right, left, ignore_return=ignore_return, no_unify_none=no_unify_none
        )
        if unified is not None:
            right = unified

    # Check return types.
    if not ignore_return and not is_compat_return(left.ret_type, right.ret_type):
        return False

    if check_args_covariantly:
        is_compat = flip_compat_check(is_compat)

    if not strict_concatenate and (left.from_concatenate or right.from_concatenate):
        strict_concatenate_check = False
    else:
        strict_concatenate_check = True

    return are_parameters_compatible(
        left,
        right,
        is_compat=is_compat,
        ignore_pos_arg_names=ignore_pos_arg_names,
        allow_partial_overlap=allow_partial_overlap,
        strict_concatenate_check=strict_concatenate_check,
    )


</t>
<t tx="ekr.20230831011820.2049">def are_trivial_parameters(param: Parameters | NormalizedCallableType) -&gt; bool:
    param_star = param.var_arg()
    param_star2 = param.kw_arg()
    return (
        param.arg_kinds == [ARG_STAR, ARG_STAR2]
        and param_star is not None
        and isinstance(get_proper_type(param_star.typ), AnyType)
        and param_star2 is not None
        and isinstance(get_proper_type(param_star2.typ), AnyType)
    )


</t>
<t tx="ekr.20230831011820.205">    def generate_incompatible_tuple_error(
        self,
        lhs_types: list[Type],
        rhs_types: list[Type],
        context: Context,
        msg: message_registry.ErrorMessage,
    ) -&gt; None:
        """Generate error message for individual incompatible tuple pairs"""
        error_cnt = 0
        notes: list[str] = []
        for i, (lhs_t, rhs_t) in enumerate(zip(lhs_types, rhs_types)):
            if not is_subtype(lhs_t, rhs_t):
                if error_cnt &lt; 3:
                    notes.append(
                        "Expression tuple item {} has type {}; {} expected; ".format(
                            str(i),
                            format_type(rhs_t, self.options),
                            format_type(lhs_t, self.options),
                        )
                    )
                error_cnt += 1

        info = f" ({str(error_cnt)} tuple items are incompatible"
        if error_cnt - 3 &gt; 0:
            info += f"; {str(error_cnt - 3)} items are omitted)"
        else:
            info += ")"
        msg = msg.with_additional_msg(info)
        self.fail(msg.value, context, code=msg.code)
        for note in notes:
            self.note(note, context, code=msg.code)

</t>
<t tx="ekr.20230831011820.2050">def are_parameters_compatible(
    left: Parameters | NormalizedCallableType,
    right: Parameters | NormalizedCallableType,
    *,
    is_compat: Callable[[Type, Type], bool],
    ignore_pos_arg_names: bool = False,
    allow_partial_overlap: bool = False,
    strict_concatenate_check: bool = False,
) -&gt; bool:
    """Helper function for is_callable_compatible, used for Parameter compatibility"""
    if right.is_ellipsis_args:
        return True

    left_star = left.var_arg()
    left_star2 = left.kw_arg()
    right_star = right.var_arg()
    right_star2 = right.kw_arg()

    # Treat "def _(*a: Any, **kw: Any) -&gt; X" similarly to "Callable[..., X]"
    if are_trivial_parameters(right):
        return True

    # Match up corresponding arguments and check them for compatibility. In
    # every pair (argL, argR) of corresponding arguments from L and R, argL must
    # be "more general" than argR if L is to be a subtype of R.

    # Arguments are corresponding if they either share a name, share a position,
    # or both. If L's corresponding argument is ambiguous, L is not a subtype of R.

    # If left has one corresponding argument by name and another by position,
    # consider them to be one "merged" argument (and not ambiguous) if they're
    # both optional, they're name-only and position-only respectively, and they
    # have the same type.  This rule allows functions with (*args, **kwargs) to
    # properly stand in for the full domain of formal arguments that they're
    # used for in practice.

    # Every argument in R must have a corresponding argument in L, and every
    # required argument in L must have a corresponding argument in R.

    # Phase 1: Confirm every argument in R has a corresponding argument in L.

    # Phase 1a: If left and right can both accept an infinite number of args,
    #           their types must be compatible.
    #
    #           Furthermore, if we're checking for compatibility in all cases,
    #           we confirm that if R accepts an infinite number of arguments,
    #           L must accept the same.
    def _incompatible(left_arg: FormalArgument | None, right_arg: FormalArgument | None) -&gt; bool:
        if right_arg is None:
            return False
        if left_arg is None:
            return not allow_partial_overlap
        return not is_compat(right_arg.typ, left_arg.typ)

    if _incompatible(left_star, right_star) or _incompatible(left_star2, right_star2):
        return False

    # Phase 1b: Check non-star args: for every arg right can accept, left must
    #           also accept. The only exception is if we are allowing partial
    #           overlaps: in that case, we ignore optional args on the right.
    for right_arg in right.formal_arguments():
        left_arg = mypy.typeops.callable_corresponding_argument(left, right_arg)
        if left_arg is None:
            if allow_partial_overlap and not right_arg.required:
                continue
            return False
        if not are_args_compatible(
            left_arg, right_arg, ignore_pos_arg_names, allow_partial_overlap, is_compat
        ):
            return False

    # Phase 1c: Check var args. Right has an infinite series of optional positional
    #           arguments. Get all further positional args of left, and make sure
    #           they're more general than the corresponding member in right.
    # TODO: are we handling UnpackType correctly here?
    if right_star is not None:
        # Synthesize an anonymous formal argument for the right
        right_by_position = right.try_synthesizing_arg_from_vararg(None)
        assert right_by_position is not None

        i = right_star.pos
        assert i is not None
        while i &lt; len(left.arg_kinds) and left.arg_kinds[i].is_positional():
            if allow_partial_overlap and left.arg_kinds[i].is_optional():
                break

            left_by_position = left.argument_by_position(i)
            assert left_by_position is not None

            if not are_args_compatible(
                left_by_position,
                right_by_position,
                ignore_pos_arg_names,
                allow_partial_overlap,
                is_compat,
            ):
                return False
            i += 1

    # Phase 1d: Check kw args. Right has an infinite series of optional named
    #           arguments. Get all further named args of left, and make sure
    #           they're more general than the corresponding member in right.
    if right_star2 is not None:
        right_names = {name for name in right.arg_names if name is not None}
        left_only_names = set()
        for name, kind in zip(left.arg_names, left.arg_kinds):
            if (
                name is None
                or kind.is_star()
                or name in right_names
                or not strict_concatenate_check
            ):
                continue
            left_only_names.add(name)

        # Synthesize an anonymous formal argument for the right
        right_by_name = right.try_synthesizing_arg_from_kwarg(None)
        assert right_by_name is not None

        for name in left_only_names:
            left_by_name = left.argument_by_name(name)
            assert left_by_name is not None

            if allow_partial_overlap and not left_by_name.required:
                continue

            if not are_args_compatible(
                left_by_name, right_by_name, ignore_pos_arg_names, allow_partial_overlap, is_compat
            ):
                return False

    # Phase 2: Left must not impose additional restrictions.
    #          (Every required argument in L must have a corresponding argument in R)
    #          Note: we already checked the *arg and **kwarg arguments in phase 1a.
    for left_arg in left.formal_arguments():
        right_by_name = (
            right.argument_by_name(left_arg.name) if left_arg.name is not None else None
        )

        right_by_pos = (
            right.argument_by_position(left_arg.pos) if left_arg.pos is not None else None
        )

        # If the left hand argument corresponds to two right-hand arguments,
        # neither of them can be required.
        if (
            right_by_name is not None
            and right_by_pos is not None
            and right_by_name != right_by_pos
            and (right_by_pos.required or right_by_name.required)
            and strict_concatenate_check
        ):
            return False

        # All *required* left-hand arguments must have a corresponding
        # right-hand argument.  Optional args do not matter.
        if left_arg.required and right_by_pos is None and right_by_name is None:
            return False

    return True


</t>
<t tx="ekr.20230831011820.2051">def are_args_compatible(
    left: FormalArgument,
    right: FormalArgument,
    ignore_pos_arg_names: bool,
    allow_partial_overlap: bool,
    is_compat: Callable[[Type, Type], bool],
) -&gt; bool:
    if left.required and right.required:
        # If both arguments are required allow_partial_overlap has no effect.
        allow_partial_overlap = False

    def is_different(left_item: object | None, right_item: object | None) -&gt; bool:
        """Checks if the left and right items are different.

        If the right item is unspecified (e.g. if the right callable doesn't care
        about what name or position its arg has), we default to returning False.

        If we're allowing partial overlap, we also default to returning False
        if the left callable also doesn't care."""
        if right_item is None:
            return False
        if allow_partial_overlap and left_item is None:
            return False
        return left_item != right_item

    # If right has a specific name it wants this argument to be, left must
    # have the same.
    if is_different(left.name, right.name):
        # But pay attention to whether we're ignoring positional arg names
        if not ignore_pos_arg_names or right.pos is None:
            return False

    # If right is at a specific position, left must have the same:
    if is_different(left.pos, right.pos):
        return False

    # If right's argument is optional, left's must also be
    # (unless we're relaxing the checks to allow potential
    # rather than definite compatibility).
    if not allow_partial_overlap and not right.required and left.required:
        return False

    # If we're allowing partial overlaps and neither arg is required,
    # the types don't actually need to be the same
    if allow_partial_overlap and not left.required and not right.required:
        return True

    # Left must have a more general type
    return is_compat(right.typ, left.typ)


</t>
<t tx="ekr.20230831011820.2052">def flip_compat_check(is_compat: Callable[[Type, Type], bool]) -&gt; Callable[[Type, Type], bool]:
    def new_is_compat(left: Type, right: Type) -&gt; bool:
        return is_compat(right, left)

    return new_is_compat


</t>
<t tx="ekr.20230831011820.2053">def unify_generic_callable(
    type: NormalizedCallableType,
    target: NormalizedCallableType,
    ignore_return: bool,
    return_constraint_direction: int | None = None,
    *,
    no_unify_none: bool = False,
) -&gt; NormalizedCallableType | None:
    """Try to unify a generic callable type with another callable type.

    Return unified CallableType if successful; otherwise, return None.
    """
    import mypy.solve

    if return_constraint_direction is None:
        return_constraint_direction = mypy.constraints.SUBTYPE_OF

    constraints: list[mypy.constraints.Constraint] = []
    # There is some special logic for inference in callables, so better use them
    # as wholes instead of picking separate arguments.
    cs = mypy.constraints.infer_constraints(
        type.copy_modified(ret_type=UninhabitedType()),
        target.copy_modified(ret_type=UninhabitedType()),
        mypy.constraints.SUBTYPE_OF,
        skip_neg_op=True,
    )
    constraints.extend(cs)
    if not ignore_return:
        c = mypy.constraints.infer_constraints(
            type.ret_type, target.ret_type, return_constraint_direction
        )
        constraints.extend(c)
    if no_unify_none:
        constraints = [
            c for c in constraints if not isinstance(get_proper_type(c.target), NoneType)
        ]
    inferred_vars, _ = mypy.solve.solve_constraints(type.variables, constraints)
    if None in inferred_vars:
        return None
    non_none_inferred_vars = cast(List[Type], inferred_vars)
    had_errors = False

    def report(*args: Any) -&gt; None:
        nonlocal had_errors
        had_errors = True

    # This function may be called by the solver, so we need to allow erased types here.
    # We anyway allow checking subtyping between other types containing &lt;Erased&gt;
    # (probably also because solver needs subtyping). See also comment in
    # ExpandTypeVisitor.visit_erased_type().
    applied = mypy.applytype.apply_generic_arguments(
        type, non_none_inferred_vars, report, context=target
    )
    if had_errors:
        return None
    return cast(NormalizedCallableType, applied)


</t>
<t tx="ekr.20230831011820.2054">def try_restrict_literal_union(t: UnionType, s: Type) -&gt; list[Type] | None:
    """Return the items of t, excluding any occurrence of s, if and only if
      - t only contains simple literals
      - s is a simple literal

    Otherwise, returns None
    """
    ps = get_proper_type(s)
    if not mypy.typeops.is_simple_literal(ps):
        return None

    new_items: list[Type] = []
    for i in t.relevant_items():
        pi = get_proper_type(i)
        if not mypy.typeops.is_simple_literal(pi):
            return None
        if pi != ps:
            new_items.append(i)
    return new_items


</t>
<t tx="ekr.20230831011820.2055">def restrict_subtype_away(t: Type, s: Type) -&gt; Type:
    """Return t minus s for runtime type assertions.

    If we can't determine a precise result, return a supertype of the
    ideal result (just t is a valid result).

    This is used for type inference of runtime type checks such as
    isinstance(). Currently, this just removes elements of a union type.
    """
    p_t = get_proper_type(t)
    if isinstance(p_t, UnionType):
        new_items = try_restrict_literal_union(p_t, s)
        if new_items is None:
            new_items = [
                restrict_subtype_away(item, s)
                for item in p_t.relevant_items()
                if (isinstance(get_proper_type(item), AnyType) or not covers_at_runtime(item, s))
            ]
        return UnionType.make_union(new_items)
    elif covers_at_runtime(t, s):
        return UninhabitedType()
    else:
        return t


</t>
<t tx="ekr.20230831011820.2056">def covers_at_runtime(item: Type, supertype: Type) -&gt; bool:
    """Will isinstance(item, supertype) always return True at runtime?"""
    item = get_proper_type(item)
    supertype = get_proper_type(supertype)

    # Since runtime type checks will ignore type arguments, erase the types.
    supertype = erase_type(supertype)
    if is_proper_subtype(
        erase_type(item), supertype, ignore_promotions=True, erase_instances=True
    ):
        return True
    if isinstance(supertype, Instance):
        if supertype.type.is_protocol:
            # TODO: Implement more robust support for runtime isinstance() checks, see issue #3827.
            if is_proper_subtype(item, supertype, ignore_promotions=True):
                return True
        if isinstance(item, TypedDictType):
            # Special case useful for selecting TypedDicts from unions using isinstance(x, dict).
            if supertype.type.fullname == "builtins.dict":
                return True
        elif isinstance(item, TypeVarType):
            if is_proper_subtype(item.upper_bound, supertype, ignore_promotions=True):
                return True
        elif isinstance(item, Instance) and supertype.type.fullname == "builtins.int":
            # "int" covers all native int types
            if item.type.fullname in MYPYC_NATIVE_INT_NAMES:
                return True
    # TODO: Add more special cases.
    return False


</t>
<t tx="ekr.20230831011820.2057">def is_more_precise(left: Type, right: Type, *, ignore_promotions: bool = False) -&gt; bool:
    """Check if left is a more precise type than right.

    A left is a proper subtype of right, left is also more precise than
    right. Also, if right is Any, left is more precise than right, for
    any left.
    """
    # TODO Should List[int] be more precise than List[Any]?
    right = get_proper_type(right)
    if isinstance(right, AnyType):
        return True
    return is_proper_subtype(left, right, ignore_promotions=ignore_promotions)
</t>
<t tx="ekr.20230831011820.2058">@path mypy
"""Mechanisms for inferring function types based on callsites.

Currently works by collecting all argument types at callsites,
synthesizing a list of possible function types from that, trying them
all, and picking the one with the fewest errors that we think is the
"best".

Can return JSON that pyannotate can use to apply the annotations to code.

There are a bunch of TODOs here:
 * Maybe want a way to surface the choices not selected??
 * We can generate an exponential number of type suggestions, and probably want
   a way to not always need to check them all.
 * Our heuristics for what types to try are primitive and not yet
   supported by real practice.
 * More!

Other things:
 * This is super brute force. Could we integrate with the typechecker
   more to understand more about what is going on?
 * Like something with tracking constraints/unification variables?
 * No understanding of type variables at *all*
"""

&lt;&lt; suggestions.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.206">    def add_fixture_note(self, fullname: str, ctx: Context) -&gt; None:
        self.note(f'Maybe your test fixture does not define "{fullname}"?', ctx)
        if fullname in SUGGESTED_TEST_FIXTURES:
            self.note(
                "Consider adding [builtins fixtures/{}] to your test description".format(
                    SUGGESTED_TEST_FIXTURES[fullname]
                ),
                ctx,
            )

</t>
<t tx="ekr.20230831011820.2060">from __future__ import annotations

import itertools
import json
import os
from contextlib import contextmanager
from typing import Callable, Iterator, NamedTuple, TypeVar, cast
from typing_extensions import TypedDict

from mypy.argmap import map_actuals_to_formals
from mypy.build import Graph, State
from mypy.checkexpr import has_any_type
from mypy.find_sources import InvalidSourceList, SourceFinder
from mypy.join import join_type_list
from mypy.meet import meet_type_list
from mypy.modulefinder import PYTHON_EXTENSIONS
from mypy.nodes import (
    ARG_STAR,
    ARG_STAR2,
    ArgKind,
    CallExpr,
    Decorator,
    Expression,
    FuncDef,
    MypyFile,
    RefExpr,
    ReturnStmt,
    SymbolNode,
    SymbolTable,
    TypeInfo,
    reverse_builtin_aliases,
)
from mypy.options import Options
from mypy.plugin import FunctionContext, MethodContext, Plugin
from mypy.server.update import FineGrainedBuildManager
from mypy.state import state
from mypy.traverser import TraverserVisitor
from mypy.typeops import make_simplified_union
from mypy.types import (
    AnyType,
    CallableType,
    FunctionLike,
    Instance,
    NoneType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeStrVisitor,
    TypeTranslator,
    TypeVarType,
    UninhabitedType,
    UnionType,
    get_proper_type,
)
from mypy.types_utils import is_overlapping_none, remove_optional
from mypy.util import split_target


</t>
<t tx="ekr.20230831011820.2061">class PyAnnotateSignature(TypedDict):
    return_type: str
    arg_types: list[str]


</t>
<t tx="ekr.20230831011820.2062">class Callsite(NamedTuple):
    path: str
    line: int
    arg_kinds: list[list[ArgKind]]
    callee_arg_names: list[str | None]
    arg_names: list[list[str | None]]
    arg_types: list[list[Type]]


</t>
<t tx="ekr.20230831011820.2063">class SuggestionPlugin(Plugin):
    """Plugin that records all calls to a given target."""

    @others
</t>
<t tx="ekr.20230831011820.2064">def __init__(self, target: str) -&gt; None:
    if target.endswith((".__new__", ".__init__")):
        target = target.rsplit(".", 1)[0]

    self.target = target
    # List of call sites found by dmypy suggest:
    # (path, line, &lt;arg kinds&gt;, &lt;arg names&gt;, &lt;arg types&gt;)
    self.mystery_hits: list[Callsite] = []

</t>
<t tx="ekr.20230831011820.2065">def get_function_hook(self, fullname: str) -&gt; Callable[[FunctionContext], Type] | None:
    if fullname == self.target:
        return self.log
    else:
        return None

</t>
<t tx="ekr.20230831011820.2066">def get_method_hook(self, fullname: str) -&gt; Callable[[MethodContext], Type] | None:
    if fullname == self.target:
        return self.log
    else:
        return None

</t>
<t tx="ekr.20230831011820.2067">def log(self, ctx: FunctionContext | MethodContext) -&gt; Type:
    self.mystery_hits.append(
        Callsite(
            ctx.api.path,
            ctx.context.line,
            ctx.arg_kinds,
            ctx.callee_arg_names,
            ctx.arg_names,
            ctx.arg_types,
        )
    )
    return ctx.default_return_type


</t>
<t tx="ekr.20230831011820.2068"># NOTE: We could make this a bunch faster by implementing a StatementVisitor that skips
# traversing into expressions
class ReturnFinder(TraverserVisitor):
    """Visitor for finding all types returned from a function."""

    @others
</t>
<t tx="ekr.20230831011820.2069">def __init__(self, typemap: dict[Expression, Type]) -&gt; None:
    self.typemap = typemap
    self.return_types: list[Type] = []

</t>
<t tx="ekr.20230831011820.207">    def annotation_in_unchecked_function(self, context: Context) -&gt; None:
        self.note(
            "By default the bodies of untyped functions are not checked,"
            " consider using --check-untyped-defs",
            context,
            code=codes.ANNOTATION_UNCHECKED,
        )


</t>
<t tx="ekr.20230831011820.2070">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is not None and o.expr in self.typemap:
        self.return_types.append(self.typemap[o.expr])

</t>
<t tx="ekr.20230831011820.2071">def visit_func_def(self, o: FuncDef) -&gt; None:
    # Skip nested functions
    pass


</t>
<t tx="ekr.20230831011820.2072">def get_return_types(typemap: dict[Expression, Type], func: FuncDef) -&gt; list[Type]:
    """Find all the types returned by return statements in func."""
    finder = ReturnFinder(typemap)
    func.body.accept(finder)
    return finder.return_types


</t>
<t tx="ekr.20230831011820.2073">class ArgUseFinder(TraverserVisitor):
    """Visitor for finding all the types of arguments that each arg is passed to.

    This is extremely simple minded but might be effective anyways.
    """

    @others
</t>
<t tx="ekr.20230831011820.2074">def __init__(self, func: FuncDef, typemap: dict[Expression, Type]) -&gt; None:
    self.typemap = typemap
    self.arg_types: dict[SymbolNode, list[Type]] = {arg.variable: [] for arg in func.arguments}

</t>
<t tx="ekr.20230831011820.2075">def visit_call_expr(self, o: CallExpr) -&gt; None:
    if not any(isinstance(e, RefExpr) and e.node in self.arg_types for e in o.args):
        return

    typ = get_proper_type(self.typemap.get(o.callee))
    if not isinstance(typ, CallableType):
        return

    formal_to_actual = map_actuals_to_formals(
        o.arg_kinds,
        o.arg_names,
        typ.arg_kinds,
        typ.arg_names,
        lambda n: AnyType(TypeOfAny.special_form),
    )

    for i, args in enumerate(formal_to_actual):
        for arg_idx in args:
            arg = o.args[arg_idx]
            if isinstance(arg, RefExpr) and arg.node in self.arg_types:
                self.arg_types[arg.node].append(typ.arg_types[i])


</t>
<t tx="ekr.20230831011820.2076">def get_arg_uses(typemap: dict[Expression, Type], func: FuncDef) -&gt; list[list[Type]]:
    """Find all the types of arguments that each arg is passed to.

    For example, given
      def foo(x: int) -&gt; None: ...
      def bar(x: str) -&gt; None: ...
      def test(x, y):
          foo(x)
          bar(y)

    this will return [[int], [str]].
    """
    finder = ArgUseFinder(func, typemap)
    func.body.accept(finder)
    return [finder.arg_types[arg.variable] for arg in func.arguments]


</t>
<t tx="ekr.20230831011820.2077">class SuggestionFailure(Exception):
    pass


</t>
<t tx="ekr.20230831011820.2078">def is_explicit_any(typ: AnyType) -&gt; bool:
    # Originally I wanted to count as explicit anything derived from an explicit any, but that
    # seemed too strict in some testing.
    # return (typ.type_of_any == TypeOfAny.explicit
    #         or (typ.source_any is not None and typ.source_any.type_of_any == TypeOfAny.explicit))
    # Important question: what should we do with source_any stuff? Does that count?
    # And actually should explicit anys count at all?? Maybe not!
    return typ.type_of_any == TypeOfAny.explicit


</t>
<t tx="ekr.20230831011820.2079">def is_implicit_any(typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, AnyType) and not is_explicit_any(typ)


</t>
<t tx="ekr.20230831011820.208">def quote_type_string(type_string: str) -&gt; str:
    """Quotes a type representation for use in messages."""
    no_quote_regex = r"^&lt;(tuple|union): \d+ items&gt;$"
    if (
        type_string in ["Module", "overloaded function", "&lt;nothing&gt;", "&lt;deleted&gt;"]
        or type_string.startswith("Module ")
        or re.match(no_quote_regex, type_string) is not None
        or type_string.endswith("?")
    ):
        # Messages are easier to read if these aren't quoted.  We use a
        # regex to match strings with variable contents.
        return type_string
    return f'"{type_string}"'


</t>
<t tx="ekr.20230831011820.2080">class SuggestionEngine:
    """Engine for finding call sites and suggesting signatures."""

    @others
</t>
<t tx="ekr.20230831011820.2081">def __init__(
    self,
    fgmanager: FineGrainedBuildManager,
    *,
    json: bool,
    no_errors: bool = False,
    no_any: bool = False,
    flex_any: float | None = None,
    use_fixme: str | None = None,
    max_guesses: int | None = None,
) -&gt; None:
    self.fgmanager = fgmanager
    self.manager = fgmanager.manager
    self.plugin = self.manager.plugin
    self.graph = fgmanager.graph
    self.finder = SourceFinder(self.manager.fscache, self.manager.options)

    self.give_json = json
    self.no_errors = no_errors
    self.flex_any = flex_any
    if no_any:
        self.flex_any = 1.0

    self.max_guesses = max_guesses or 64
    self.use_fixme = use_fixme

</t>
<t tx="ekr.20230831011820.2082">def suggest(self, function: str) -&gt; str:
    """Suggest an inferred type for function."""
    mod, func_name, node = self.find_node(function)

    with self.restore_after(mod):
        with self.with_export_types():
            suggestion = self.get_suggestion(mod, node)

    if self.give_json:
        return self.json_suggestion(mod, func_name, node, suggestion)
    else:
        return self.format_signature(suggestion)

</t>
<t tx="ekr.20230831011820.2083">def suggest_callsites(self, function: str) -&gt; str:
    """Find a list of call sites of function."""
    mod, _, node = self.find_node(function)
    with self.restore_after(mod):
        callsites, _ = self.get_callsites(node)

    return "\n".join(
        dedup(
            [
                f"{path}:{line}: {self.format_args(arg_kinds, arg_names, arg_types)}"
                for path, line, arg_kinds, _, arg_names, arg_types in callsites
            ]
        )
    )

</t>
<t tx="ekr.20230831011820.2084">@contextmanager
def restore_after(self, module: str) -&gt; Iterator[None]:
    """Context manager that reloads a module after executing the body.

    This should undo any damage done to the module state while mucking around.
    """
    try:
        yield
    finally:
        self.reload(self.graph[module])

</t>
<t tx="ekr.20230831011820.2085">@contextmanager
def with_export_types(self) -&gt; Iterator[None]:
    """Context manager that enables the export_types flag in the body.

    This causes type information to be exported into the manager's all_types variable.
    """
    old = self.manager.options.export_types
    self.manager.options.export_types = True
    try:
        yield
    finally:
        self.manager.options.export_types = old

</t>
<t tx="ekr.20230831011820.2086">def get_trivial_type(self, fdef: FuncDef) -&gt; CallableType:
    """Generate a trivial callable type from a func def, with all Anys"""
    # The Anys are marked as being from the suggestion engine
    # since they need some special treatment (specifically,
    # constraint generation ignores them.)
    return CallableType(
        [AnyType(TypeOfAny.suggestion_engine) for _ in fdef.arg_kinds],
        fdef.arg_kinds,
        fdef.arg_names,
        AnyType(TypeOfAny.suggestion_engine),
        self.named_type("builtins.function"),
    )

</t>
<t tx="ekr.20230831011820.2087">def get_starting_type(self, fdef: FuncDef) -&gt; CallableType:
    if isinstance(fdef.type, CallableType):
        return make_suggestion_anys(fdef.type)
    else:
        return self.get_trivial_type(fdef)

</t>
<t tx="ekr.20230831011820.2088">def get_args(
    self,
    is_method: bool,
    base: CallableType,
    defaults: list[Type | None],
    callsites: list[Callsite],
    uses: list[list[Type]],
) -&gt; list[list[Type]]:
    """Produce a list of type suggestions for each argument type."""
    types: list[list[Type]] = []
    for i in range(len(base.arg_kinds)):
        # Make self args Any but this will get overridden somewhere in the checker
        if i == 0 and is_method:
            types.append([AnyType(TypeOfAny.suggestion_engine)])
            continue

        all_arg_types = []
        for call in callsites:
            for typ in call.arg_types[i - is_method]:
                # Collect all the types except for implicit anys
                if not is_implicit_any(typ):
                    all_arg_types.append(typ)
        all_use_types = []
        for typ in uses[i]:
            # Collect all the types except for implicit anys
            if not is_implicit_any(typ):
                all_use_types.append(typ)
        # Add in any default argument types
        default = defaults[i]
        if default:
            all_arg_types.append(default)
            if all_use_types:
                all_use_types.append(default)

        arg_types = []

        if all_arg_types and all(
            isinstance(get_proper_type(tp), NoneType) for tp in all_arg_types
        ):
            arg_types.append(
                UnionType.make_union([all_arg_types[0], AnyType(TypeOfAny.explicit)])
            )
        elif all_arg_types:
            arg_types.extend(generate_type_combinations(all_arg_types))
        else:
            arg_types.append(AnyType(TypeOfAny.explicit))

        if all_use_types:
            # This is a meet because the type needs to be compatible with all the uses
            arg_types.append(meet_type_list(all_use_types))

        types.append(arg_types)
    return types

</t>
<t tx="ekr.20230831011820.2089">def get_default_arg_types(self, fdef: FuncDef) -&gt; list[Type | None]:
    return [
        self.manager.all_types[arg.initializer] if arg.initializer else None
        for arg in fdef.arguments
    ]

</t>
<t tx="ekr.20230831011820.209">def format_callable_args(
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: list[str | None],
    format: Callable[[Type], str],
    verbosity: int,
) -&gt; str:
    """Format a bunch of Callable arguments into a string"""
    arg_strings = []
    for arg_name, arg_type, arg_kind in zip(arg_names, arg_types, arg_kinds):
        if arg_kind == ARG_POS and arg_name is None or verbosity == 0 and arg_kind.is_positional():
            arg_strings.append(format(arg_type))
        else:
            constructor = ARG_CONSTRUCTOR_NAMES[arg_kind]
            if arg_kind.is_star() or arg_name is None:
                arg_strings.append(f"{constructor}({format(arg_type)})")
            else:
                arg_strings.append(f"{constructor}({format(arg_type)}, {repr(arg_name)})")

    return ", ".join(arg_strings)


</t>
<t tx="ekr.20230831011820.2090">def get_guesses(
    self,
    is_method: bool,
    base: CallableType,
    defaults: list[Type | None],
    callsites: list[Callsite],
    uses: list[list[Type]],
) -&gt; list[CallableType]:
    """Compute a list of guesses for a function's type.

    This focuses just on the argument types, and doesn't change the provided return type.
    """
    options = self.get_args(is_method, base, defaults, callsites, uses)

    # Take the first `max_guesses` guesses.
    product = itertools.islice(itertools.product(*options), 0, self.max_guesses)
    return [refine_callable(base, base.copy_modified(arg_types=list(x))) for x in product]

</t>
<t tx="ekr.20230831011820.2091">def get_callsites(self, func: FuncDef) -&gt; tuple[list[Callsite], list[str]]:
    """Find all call sites of a function."""
    new_type = self.get_starting_type(func)

    collector_plugin = SuggestionPlugin(func.fullname)

    self.plugin._plugins.insert(0, collector_plugin)
    try:
        errors = self.try_type(func, new_type)
    finally:
        self.plugin._plugins.pop(0)

    return collector_plugin.mystery_hits, errors

</t>
<t tx="ekr.20230831011820.2092">def filter_options(
    self, guesses: list[CallableType], is_method: bool, ignore_return: bool
) -&gt; list[CallableType]:
    """Apply any configured filters to the possible guesses.

    Currently the only option is filtering based on Any prevalance."""
    return [
        t
        for t in guesses
        if self.flex_any is None
        or any_score_callable(t, is_method, ignore_return) &gt;= self.flex_any
    ]

</t>
<t tx="ekr.20230831011820.2093">def find_best(self, func: FuncDef, guesses: list[CallableType]) -&gt; tuple[CallableType, int]:
    """From a list of possible function types, find the best one.

    For best, we want the fewest errors, then the best "score" from score_callable.
    """
    if not guesses:
        raise SuggestionFailure("No guesses that match criteria!")
    errors = {guess: self.try_type(func, guess) for guess in guesses}
    best = min(guesses, key=lambda s: (count_errors(errors[s]), self.score_callable(s)))
    return best, count_errors(errors[best])

</t>
<t tx="ekr.20230831011820.2094">def get_guesses_from_parent(self, node: FuncDef) -&gt; list[CallableType]:
    """Try to get a guess of a method type from a parent class."""
    if not node.info:
        return []

    for parent in node.info.mro[1:]:
        pnode = parent.names.get(node.name)
        if pnode and isinstance(pnode.node, (FuncDef, Decorator)):
            typ = get_proper_type(pnode.node.type)
            # FIXME: Doesn't work right with generic tyeps
            if isinstance(typ, CallableType) and len(typ.arg_types) == len(node.arguments):
                # Return the first thing we find, since it probably doesn't make sense
                # to grab things further up in the chain if an earlier parent has it.
                return [typ]

    return []

</t>
<t tx="ekr.20230831011820.2095">def get_suggestion(self, mod: str, node: FuncDef) -&gt; PyAnnotateSignature:
    """Compute a suggestion for a function.

    Return the type and whether the first argument should be ignored.
    """
    graph = self.graph
    callsites, orig_errors = self.get_callsites(node)
    uses = get_arg_uses(self.manager.all_types, node)

    if self.no_errors and orig_errors:
        raise SuggestionFailure("Function does not typecheck.")

    is_method = bool(node.info) and not node.is_static

    with state.strict_optional_set(graph[mod].options.strict_optional):
        guesses = self.get_guesses(
            is_method,
            self.get_starting_type(node),
            self.get_default_arg_types(node),
            callsites,
            uses,
        )
    guesses += self.get_guesses_from_parent(node)
    guesses = self.filter_options(guesses, is_method, ignore_return=True)
    best, _ = self.find_best(node, guesses)

    # Now try to find the return type!
    self.try_type(node, best)
    returns = get_return_types(self.manager.all_types, node)
    with state.strict_optional_set(graph[mod].options.strict_optional):
        if returns:
            ret_types = generate_type_combinations(returns)
        else:
            ret_types = [NoneType()]

    guesses = [best.copy_modified(ret_type=refine_type(best.ret_type, t)) for t in ret_types]
    guesses = self.filter_options(guesses, is_method, ignore_return=False)
    best, errors = self.find_best(node, guesses)

    if self.no_errors and errors:
        raise SuggestionFailure("No annotation without errors")

    return self.pyannotate_signature(mod, is_method, best)

</t>
<t tx="ekr.20230831011820.2096">def format_args(
    self,
    arg_kinds: list[list[ArgKind]],
    arg_names: list[list[str | None]],
    arg_types: list[list[Type]],
) -&gt; str:
    args: list[str] = []
    for i in range(len(arg_types)):
        for kind, name, typ in zip(arg_kinds[i], arg_names[i], arg_types[i]):
            arg = self.format_type(None, typ)
            if kind == ARG_STAR:
                arg = "*" + arg
            elif kind == ARG_STAR2:
                arg = "**" + arg
            elif kind.is_named():
                if name:
                    arg = f"{name}={arg}"
        args.append(arg)
    return f"({', '.join(args)})"

</t>
<t tx="ekr.20230831011820.2097">def find_node(self, key: str) -&gt; tuple[str, str, FuncDef]:
    """From a target name, return module/target names and the func def.

    The 'key' argument can be in one of two formats:
    * As the function full name, e.g., package.module.Cls.method
    * As the function location as file and line separated by column,
      e.g., path/to/file.py:42
    """
    # TODO: Also return OverloadedFuncDef -- currently these are ignored.
    node: SymbolNode | None = None
    if ":" in key:
        if key.count(":") &gt; 1:
            raise SuggestionFailure(
                "Malformed location for function: {}. Must be either"
                " package.module.Class.method or path/to/file.py:line".format(key)
            )
        file, line = key.split(":")
        if not line.isdigit():
            raise SuggestionFailure(f"Line number must be a number. Got {line}")
        line_number = int(line)
        modname, node = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1 :]  # add one to account for '.'
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure(f"Cannot find module for {key}")
        modname, tail = target
        node = self.find_node_by_module_and_name(modname, tail)

    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(f"Object {key} is a decorator we can't handle")

    if not isinstance(node, FuncDef):
        raise SuggestionFailure(f"Object {key} is not a function")

    return modname, tail, node

</t>
<t tx="ekr.20230831011820.2098">def find_node_by_module_and_name(self, modname: str, tail: str) -&gt; SymbolNode | None:
    """Find symbol node by module id and qualified name.

    Raise SuggestionFailure if can't find one.
    """
    tree = self.ensure_loaded(self.fgmanager.graph[modname])

    # N.B. This is reimplemented from update's lookup_target
    # basically just to produce better error messages.

    names: SymbolTable = tree.names

    # Look through any classes
    components = tail.split(".")
    for i, component in enumerate(components[:-1]):
        if component not in names:
            raise SuggestionFailure(
                "Unknown class {}.{}".format(modname, ".".join(components[: i + 1]))
            )
        node: SymbolNode | None = names[component].node
        if not isinstance(node, TypeInfo):
            raise SuggestionFailure(
                "Object {}.{} is not a class".format(modname, ".".join(components[: i + 1]))
            )
        names = node.names

    # Look for the actual function/method
    funcname = components[-1]
    if funcname not in names:
        key = modname + "." + tail
        raise SuggestionFailure(
            "Unknown {} {}".format("method" if len(components) &gt; 1 else "function", key)
        )
    return names[funcname].node

</t>
<t tx="ekr.20230831011820.2099">def find_node_by_file_and_line(self, file: str, line: int) -&gt; tuple[str, SymbolNode]:
    """Find symbol node by path to file and line number.

    Find the first function declared *before or on* the line number.

    Return module id and the node found. Raise SuggestionFailure if can't find one.
    """
    if not any(file.endswith(ext) for ext in PYTHON_EXTENSIONS):
        raise SuggestionFailure("Source file is not a Python file")
    try:
        modname, _ = self.finder.crawl_up(os.path.normpath(file))
    except InvalidSourceList as e:
        raise SuggestionFailure("Invalid source file name: " + file) from e
    if modname not in self.graph:
        raise SuggestionFailure("Unknown module: " + modname)
    # We must be sure about any edits in this file as this might affect the line numbers.
    tree = self.ensure_loaded(self.fgmanager.graph[modname], force=True)
    node: SymbolNode | None = None
    closest_line: int | None = None
    # TODO: Handle nested functions.
    for _, sym, _ in tree.local_definitions():
        if isinstance(sym.node, (FuncDef, Decorator)):
            sym_line = sym.node.line
        # TODO: add support for OverloadedFuncDef.
        else:
            continue

        # We want the closest function above the specified line
        if sym_line &lt;= line and (closest_line is None or sym_line &gt; closest_line):
            closest_line = sym_line
            node = sym.node
    if not node:
        raise SuggestionFailure(f"Cannot find a function at line {line}")
    return modname, node

</t>
<t tx="ekr.20230831011820.21">def is_overlapping_types(
    left: Type,
    right: Type,
    ignore_promotions: bool = False,
    prohibit_none_typevar_overlap: bool = False,
    ignore_uninhabited: bool = False,
) -&gt; bool:
    """Can a value of type 'left' also be of type 'right' or vice-versa?

    If 'ignore_promotions' is True, we ignore promotions while checking for overlaps.
    If 'prohibit_none_typevar_overlap' is True, we disallow None from overlapping with
    TypeVars (in both strict-optional and non-strict-optional mode).
    """
    if isinstance(left, TypeGuardedType) or isinstance(  # type: ignore[misc]
        right, TypeGuardedType
    ):
        # A type guard forces the new type even if it doesn't overlap the old.
        return True

    left, right = get_proper_types((left, right))

    def _is_overlapping_types(left: Type, right: Type) -&gt; bool:
        """Encode the kind of overlapping check to perform.

        This function mostly exists so we don't have to repeat keyword arguments everywhere."""
        return is_overlapping_types(
            left,
            right,
            ignore_promotions=ignore_promotions,
            prohibit_none_typevar_overlap=prohibit_none_typevar_overlap,
            ignore_uninhabited=ignore_uninhabited,
        )

    # We should never encounter this type.
    if isinstance(left, PartialType) or isinstance(right, PartialType):
        assert False, "Unexpectedly encountered partial type"

    # We should also never encounter these types, but it's possible a few
    # have snuck through due to unrelated bugs. For now, we handle these
    # in the same way we handle 'Any'.
    #
    # TODO: Replace these with an 'assert False' once we are more confident.
    illegal_types = (UnboundType, ErasedType, DeletedType)
    if isinstance(left, illegal_types) or isinstance(right, illegal_types):
        return True

    # When running under non-strict optional mode, simplify away types of
    # the form 'Union[A, B, C, None]' into just 'Union[A, B, C]'.

    if not state.strict_optional:
        if isinstance(left, UnionType):
            left = UnionType.make_union(left.relevant_items())
        if isinstance(right, UnionType):
            right = UnionType.make_union(right.relevant_items())
        left, right = get_proper_types((left, right))

    # 'Any' may or may not be overlapping with the other type
    if isinstance(left, AnyType) or isinstance(right, AnyType):
        return True

    # We check for complete overlaps next as a general-purpose failsafe.
    # If this check fails, we start checking to see if there exists a
    # *partial* overlap between types.
    #
    # These checks will also handle the NoneType and UninhabitedType cases for us.

    # enums are sometimes expanded into an Union of Literals
    # when that happens we want to make sure we treat the two as overlapping
    # and crucially, we want to do that *fast* in case the enum is large
    # so we do it before expanding variants below to avoid O(n**2) behavior
    if (
        is_enum_overlapping_union(left, right)
        or is_enum_overlapping_union(right, left)
        or is_literal_in_union(left, right)
        or is_literal_in_union(right, left)
    ):
        return True

    if is_proper_subtype(
        left, right, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
    ) or is_proper_subtype(
        right, left, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
    ):
        return True

    # See the docstring for 'get_possible_variants' for more info on what the
    # following lines are doing.

    left_possible = get_possible_variants(left)
    right_possible = get_possible_variants(right)

    # First handle special cases relating to PEP 612:
    # - comparing a `Parameters` to a `Parameters`
    # - comparing a `Parameters` to a `ParamSpecType`
    # - comparing a `ParamSpecType` to a `ParamSpecType`
    #
    # These should all always be considered overlapping equality checks.
    # These need to be done before we move on to other TypeVarLike comparisons.
    if isinstance(left, (Parameters, ParamSpecType)) and isinstance(
        right, (Parameters, ParamSpecType)
    ):
        return True
    # A `Parameters` does not overlap with anything else, however
    if isinstance(left, Parameters) or isinstance(right, Parameters):
        return False

    # Now move on to checking multi-variant types like Unions. We also perform
    # the same logic if either type happens to be a TypeVar/ParamSpec/TypeVarTuple.
    #
    # Handling the TypeVarLikes now lets us simulate having them bind to the corresponding
    # type -- if we deferred these checks, the "return-early" logic of the other
    # checks will prevent us from detecting certain overlaps.
    #
    # If both types are singleton variants (and are not TypeVarLikes), we've hit the base case:
    # we skip these checks to avoid infinitely recursing.

    def is_none_typevarlike_overlap(t1: Type, t2: Type) -&gt; bool:
        t1, t2 = get_proper_types((t1, t2))
        return isinstance(t1, NoneType) and isinstance(t2, TypeVarLikeType)

    if prohibit_none_typevar_overlap:
        if is_none_typevarlike_overlap(left, right) or is_none_typevarlike_overlap(right, left):
            return False

    if (
        len(left_possible) &gt; 1
        or len(right_possible) &gt; 1
        or isinstance(left, TypeVarLikeType)
        or isinstance(right, TypeVarLikeType)
    ):
        for l in left_possible:
            for r in right_possible:
                if _is_overlapping_types(l, r):
                    return True
        return False

    # Now that we've finished handling TypeVarLikes, we're free to end early
    # if one one of the types is None and we're running in strict-optional mode.
    # (None only overlaps with None in strict-optional mode).
    #
    # We must perform this check after the TypeVarLike checks because
    # a TypeVar could be bound to None, for example.

    if state.strict_optional and isinstance(left, NoneType) != isinstance(right, NoneType):
        return False

    # Next, we handle single-variant types that may be inherently partially overlapping:
    #
    # - TypedDicts
    # - Tuples
    #
    # If we cannot identify a partial overlap and end early, we degrade these two types
    # into their 'Instance' fallbacks.

    if isinstance(left, TypedDictType) and isinstance(right, TypedDictType):
        return are_typed_dicts_overlapping(left, right, ignore_promotions=ignore_promotions)
    elif typed_dict_mapping_pair(left, right):
        # Overlaps between TypedDicts and Mappings require dedicated logic.
        return typed_dict_mapping_overlap(left, right, overlapping=_is_overlapping_types)
    elif isinstance(left, TypedDictType):
        left = left.fallback
    elif isinstance(right, TypedDictType):
        right = right.fallback

    if is_tuple(left) and is_tuple(right):
        return are_tuples_overlapping(left, right, ignore_promotions=ignore_promotions)
    elif isinstance(left, TupleType):
        left = tuple_fallback(left)
    elif isinstance(right, TupleType):
        right = tuple_fallback(right)

    # Next, we handle single-variant types that cannot be inherently partially overlapping,
    # but do require custom logic to inspect.
    #
    # As before, we degrade into 'Instance' whenever possible.

    if isinstance(left, TypeType) and isinstance(right, TypeType):
        return _is_overlapping_types(left.item, right.item)

    def _type_object_overlap(left: Type, right: Type) -&gt; bool:
        """Special cases for type object types overlaps."""
        # TODO: these checks are a bit in gray area, adjust if they cause problems.
        left, right = get_proper_types((left, right))
        # 1. Type[C] vs Callable[..., C] overlap even if the latter is not class object.
        if isinstance(left, TypeType) and isinstance(right, CallableType):
            return _is_overlapping_types(left.item, right.ret_type)
        # 2. Type[C] vs Meta, where Meta is a metaclass for C.
        if isinstance(left, TypeType) and isinstance(right, Instance):
            if isinstance(left.item, Instance):
                left_meta = left.item.type.metaclass_type
                if left_meta is not None:
                    return _is_overlapping_types(left_meta, right)
                # builtins.type (default metaclass) overlaps with all metaclasses
                return right.type.has_base("builtins.type")
            elif isinstance(left.item, AnyType):
                return right.type.has_base("builtins.type")
        # 3. Callable[..., C] vs Meta is considered below, when we switch to fallbacks.
        return False

    if isinstance(left, TypeType) or isinstance(right, TypeType):
        return _type_object_overlap(left, right) or _type_object_overlap(right, left)

    if isinstance(left, CallableType) and isinstance(right, CallableType):
        return is_callable_compatible(
            left,
            right,
            is_compat=_is_overlapping_types,
            ignore_pos_arg_names=True,
            allow_partial_overlap=True,
        )
    elif isinstance(left, CallableType):
        left = left.fallback
    elif isinstance(right, CallableType):
        right = right.fallback

    if isinstance(left, LiteralType) and isinstance(right, LiteralType):
        if left.value == right.value:
            # If values are the same, we still need to check if fallbacks are overlapping,
            # this is done below.
            left = left.fallback
            right = right.fallback
        else:
            return False
    elif isinstance(left, LiteralType):
        left = left.fallback
    elif isinstance(right, LiteralType):
        right = right.fallback

    # Finally, we handle the case where left and right are instances.

    if isinstance(left, Instance) and isinstance(right, Instance):
        # First we need to handle promotions and structural compatibility for instances
        # that came as fallbacks, so simply call is_subtype() to avoid code duplication.
        if is_subtype(
            left, right, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
        ) or is_subtype(
            right, left, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
        ):
            return True

        if right.type.fullname == "builtins.int" and left.type.fullname in MYPYC_NATIVE_INT_NAMES:
            return True

        # Two unrelated types cannot be partially overlapping: they're disjoint.
        if left.type.has_base(right.type.fullname):
            left = map_instance_to_supertype(left, right.type)
        elif right.type.has_base(left.type.fullname):
            right = map_instance_to_supertype(right, left.type)
        else:
            return False

        if len(left.args) == len(right.args):
            # Note: we don't really care about variance here, since the overlapping check
            # is symmetric and since we want to return 'True' even for partial overlaps.
            #
            # For example, suppose we have two types Wrapper[Parent] and Wrapper[Child].
            # It doesn't matter whether Wrapper is covariant or contravariant since
            # either way, one of the two types will overlap with the other.
            #
            # Similarly, if Wrapper was invariant, the two types could still be partially
            # overlapping -- what if Wrapper[Parent] happened to contain only instances of
            # specifically Child?
            #
            # Or, to use a more concrete example, List[Union[A, B]] and List[Union[B, C]]
            # would be considered partially overlapping since it's possible for both lists
            # to contain only instances of B at runtime.
            if all(
                _is_overlapping_types(left_arg, right_arg)
                for left_arg, right_arg in zip(left.args, right.args)
            ):
                return True

        return False

    # We ought to have handled every case by now: we conclude the
    # two types are not overlapping, either completely or partially.
    #
    # Note: it's unclear however, whether returning False is the right thing
    # to do when inferring reachability -- see  https://github.com/python/mypy/issues/5529

    assert type(left) != type(right), f"{type(left)} vs {type(right)}"
    return False


</t>
<t tx="ekr.20230831011820.210">def format_type_inner(
    typ: Type,
    verbosity: int,
    options: Options,
    fullnames: set[str] | None,
    module_names: bool = False,
) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    Args:
      verbosity: a coarse grained control on the verbosity of the type
      fullnames: a set of names that should be printed in full
    """

    def format(typ: Type) -&gt; str:
        return format_type_inner(typ, verbosity, options, fullnames)

    def format_list(types: Sequence[Type]) -&gt; str:
        return ", ".join(format(typ) for typ in types)

    def format_union(types: Sequence[Type]) -&gt; str:
        formatted = [format(typ) for typ in types if format(typ) != "None"]
        if any(format(typ) == "None" for typ in types):
            formatted.append("None")
        return " | ".join(formatted)

    def format_literal_value(typ: LiteralType) -&gt; str:
        if typ.is_enum_literal():
            underlying_type = format(typ.fallback)
            return f"{underlying_type}.{typ.value}"
        else:
            return typ.value_repr()

    if isinstance(typ, TypeAliasType) and typ.is_recursive:
        # TODO: find balance here, str(typ) doesn't support custom verbosity, and may be
        # too verbose for user messages, OTOH it nicely shows structure of recursive types.
        if verbosity &lt; 2:
            type_str = typ.alias.name if typ.alias else "&lt;alias (unfixed)&gt;"
            if typ.args:
                type_str += f"[{format_list(typ.args)}]"
            return type_str
        return str(typ)

    # TODO: always mention type alias names in errors.
    typ = get_proper_type(typ)

    if isinstance(typ, Instance):
        itype = typ
        # Get the short name of the type.
        if itype.type.fullname == "types.ModuleType":
            # Make some common error messages simpler and tidier.
            base_str = "Module"
            if itype.extra_attrs and itype.extra_attrs.mod_name and module_names:
                return f'{base_str} "{itype.extra_attrs.mod_name}"'
            return base_str
        if itype.type.fullname == "typing._SpecialForm":
            # This is not a real type but used for some typing-related constructs.
            return "&lt;typing special form&gt;"
        if itype.type.fullname in reverse_builtin_aliases and not options.use_lowercase_names():
            alias = reverse_builtin_aliases[itype.type.fullname]
            base_str = alias.split(".")[-1]
        elif verbosity &gt;= 2 or (fullnames and itype.type.fullname in fullnames):
            base_str = itype.type.fullname
        else:
            base_str = itype.type.name
        if not itype.args:
            # No type arguments, just return the type name
            return base_str
        elif itype.type.fullname == "builtins.tuple":
            item_type_str = format(itype.args[0])
            return f"{'tuple' if options.use_lowercase_names() else 'Tuple'}[{item_type_str}, ...]"
        else:
            # There are type arguments. Convert the arguments to strings.
            return f"{base_str}[{format_list(itype.args)}]"
    elif isinstance(typ, UnpackType):
        return f"Unpack[{format(typ.type)}]"
    elif isinstance(typ, TypeVarType):
        # This is similar to non-generic instance types.
        return typ.name
    elif isinstance(typ, TypeVarTupleType):
        # This is similar to non-generic instance types.
        return typ.name
    elif isinstance(typ, ParamSpecType):
        # Concatenate[..., P]
        if typ.prefix.arg_types:
            args = format_callable_args(
                typ.prefix.arg_types, typ.prefix.arg_kinds, typ.prefix.arg_names, format, verbosity
            )

            return f"[{args}, **{typ.name_with_suffix()}]"
        else:
            return typ.name_with_suffix()
    elif isinstance(typ, TupleType):
        # Prefer the name of the fallback class (if not tuple), as it's more informative.
        if typ.partial_fallback.type.fullname != "builtins.tuple":
            return format(typ.partial_fallback)
        type_items = format_list(typ.items) or "()"
        if options.use_lowercase_names():
            s = f"tuple[{type_items}]"
        else:
            s = f"Tuple[{type_items}]"
        return s
    elif isinstance(typ, TypedDictType):
        # If the TypedDictType is named, return the name
        if not typ.is_anonymous():
            return format(typ.fallback)
        items = []
        for item_name, item_type in typ.items.items():
            modifier = "" if item_name in typ.required_keys else "?"
            items.append(f"{item_name!r}{modifier}: {format(item_type)}")
        s = f"TypedDict({{{', '.join(items)}}})"
        return s
    elif isinstance(typ, LiteralType):
        return f"Literal[{format_literal_value(typ)}]"
    elif isinstance(typ, UnionType):
        literal_items, union_items = separate_union_literals(typ)

        # Coalesce multiple Literal[] members. This also changes output order.
        # If there's just one Literal item, retain the original ordering.
        if len(literal_items) &gt; 1:
            literal_str = "Literal[{}]".format(
                ", ".join(format_literal_value(t) for t in literal_items)
            )

            if len(union_items) == 1 and isinstance(get_proper_type(union_items[0]), NoneType):
                return (
                    f"{literal_str} | None"
                    if options.use_or_syntax()
                    else f"Optional[{literal_str}]"
                )
            elif union_items:
                return (
                    f"{literal_str} | {format_union(union_items)}"
                    if options.use_or_syntax()
                    else f"Union[{format_list(union_items)}, {literal_str}]"
                )
            else:
                return literal_str
        else:
            # Only print Union as Optional if the Optional wouldn't have to contain another Union
            print_as_optional = (
                len(typ.items) - sum(isinstance(get_proper_type(t), NoneType) for t in typ.items)
                == 1
            )
            if print_as_optional:
                rest = [t for t in typ.items if not isinstance(get_proper_type(t), NoneType)]
                return (
                    f"{format(rest[0])} | None"
                    if options.use_or_syntax()
                    else f"Optional[{format(rest[0])}]"
                )
            else:
                s = (
                    format_union(typ.items)
                    if options.use_or_syntax()
                    else f"Union[{format_list(typ.items)}]"
                )
            return s
    elif isinstance(typ, NoneType):
        return "None"
    elif isinstance(typ, AnyType):
        return "Any"
    elif isinstance(typ, DeletedType):
        return "&lt;deleted&gt;"
    elif isinstance(typ, UninhabitedType):
        if typ.is_noreturn:
            return "NoReturn"
        else:
            return "&lt;nothing&gt;"
    elif isinstance(typ, TypeType):
        type_name = "type" if options.use_lowercase_names() else "Type"
        return f"{type_name}[{format(typ.item)}]"
    elif isinstance(typ, FunctionLike):
        func = typ
        if func.is_type_obj():
            # The type of a type object type can be derived from the
            # return type (this always works).
            return format(TypeType.make_normalized(erase_type(func.items[0].ret_type)))
        elif isinstance(func, CallableType):
            if func.type_guard is not None:
                return_type = f"TypeGuard[{format(func.type_guard)}]"
            else:
                return_type = format(func.ret_type)
            if func.is_ellipsis_args:
                return f"Callable[..., {return_type}]"
            param_spec = func.param_spec()
            if param_spec is not None:
                return f"Callable[{format(param_spec)}, {return_type}]"
            args = format_callable_args(
                func.arg_types, func.arg_kinds, func.arg_names, format, verbosity
            )
            return f"Callable[[{args}], {return_type}]"
        else:
            # Use a simple representation for function types; proper
            # function types may result in long and difficult-to-read
            # error messages.
            return "overloaded function"
    elif isinstance(typ, UnboundType):
        return typ.accept(TypeStrVisitor(options=options))
    elif isinstance(typ, Parameters):
        args = format_callable_args(typ.arg_types, typ.arg_kinds, typ.arg_names, format, verbosity)
        return f"[{args}]"
    elif typ is None:
        raise RuntimeError("Type is None")
    else:
        # Default case; we simply have to return something meaningful here.
        return "object"


</t>
<t tx="ekr.20230831011820.2100">def extract_from_decorator(self, node: Decorator) -&gt; FuncDef | None:
    for dec in node.decorators:
        typ = None
        if isinstance(dec, RefExpr) and isinstance(dec.node, FuncDef):
            typ = dec.node.type
        elif (
            isinstance(dec, CallExpr)
            and isinstance(dec.callee, RefExpr)
            and isinstance(dec.callee.node, FuncDef)
            and isinstance(dec.callee.node.type, CallableType)
        ):
            typ = get_proper_type(dec.callee.node.type.ret_type)

        if not isinstance(typ, FunctionLike):
            return None
        for ct in typ.items:
            if not (
                len(ct.arg_types) == 1
                and isinstance(ct.arg_types[0], TypeVarType)
                and ct.arg_types[0] == ct.ret_type
            ):
                return None

    return node.func

</t>
<t tx="ekr.20230831011820.2101">def try_type(self, func: FuncDef, typ: ProperType) -&gt; list[str]:
    """Recheck a function while assuming it has type typ.

    Return all error messages.
    """
    old = func.unanalyzed_type
    # During reprocessing, unanalyzed_type gets copied to type (by aststrip).
    # We set type to None to ensure that the type always changes during
    # reprocessing.
    func.type = None
    func.unanalyzed_type = typ
    try:
        res = self.fgmanager.trigger(func.fullname)
        # if res:
        #     print('===', typ)
        #     print('\n'.join(res))
        return res
    finally:
        func.unanalyzed_type = old

</t>
<t tx="ekr.20230831011820.2102">def reload(self, state: State) -&gt; list[str]:
    """Recheck the module given by state."""
    assert state.path is not None
    self.fgmanager.flush_cache()
    return self.fgmanager.update([(state.id, state.path)], [])

</t>
<t tx="ekr.20230831011820.2103">def ensure_loaded(self, state: State, force: bool = False) -&gt; MypyFile:
    """Make sure that the module represented by state is fully loaded."""
    if not state.tree or state.tree.is_cache_skeleton or force:
        self.reload(state)
    assert state.tree is not None
    return state.tree

</t>
<t tx="ekr.20230831011820.2104">def named_type(self, s: str) -&gt; Instance:
    return self.manager.semantic_analyzer.named_type(s)

</t>
<t tx="ekr.20230831011820.2105">def json_suggestion(
    self, mod: str, func_name: str, node: FuncDef, suggestion: PyAnnotateSignature
) -&gt; str:
    """Produce a json blob for a suggestion suitable for application by pyannotate."""
    # pyannotate irritatingly drops class names for class and static methods
    if node.is_class or node.is_static:
        func_name = func_name.split(".", 1)[-1]

    # pyannotate works with either paths relative to where the
    # module is rooted or with absolute paths. We produce absolute
    # paths because it is simpler.
    path = os.path.abspath(self.graph[mod].xpath)

    obj = {
        "signature": suggestion,
        "line": node.line,
        "path": path,
        "func_name": func_name,
        "samples": 0,
    }
    return json.dumps([obj], sort_keys=True)

</t>
<t tx="ekr.20230831011820.2106">def pyannotate_signature(
    self, cur_module: str | None, is_method: bool, typ: CallableType
) -&gt; PyAnnotateSignature:
    """Format a callable type as a pyannotate dict"""
    start = int(is_method)
    return {
        "arg_types": [self.format_type(cur_module, t) for t in typ.arg_types[start:]],
        "return_type": self.format_type(cur_module, typ.ret_type),
    }

</t>
<t tx="ekr.20230831011820.2107">def format_signature(self, sig: PyAnnotateSignature) -&gt; str:
    """Format a callable type in a way suitable as an annotation... kind of"""
    return f"({', '.join(sig['arg_types'])}) -&gt; {sig['return_type']}"

</t>
<t tx="ekr.20230831011820.2108">def format_type(self, cur_module: str | None, typ: Type) -&gt; str:
    if self.use_fixme and isinstance(get_proper_type(typ), AnyType):
        return self.use_fixme
    return typ.accept(TypeFormatter(cur_module, self.graph, self.manager.options))

</t>
<t tx="ekr.20230831011820.2109">def score_type(self, t: Type, arg_pos: bool) -&gt; int:
    """Generate a score for a type that we use to pick which type to use.

    Lower is better, prefer non-union/non-any types. Don't penalize optionals.
    """
    t = get_proper_type(t)
    if isinstance(t, AnyType):
        return 20
    if arg_pos and isinstance(t, NoneType):
        return 20
    if isinstance(t, UnionType):
        if any(isinstance(get_proper_type(x), AnyType) for x in t.items):
            return 20
        if any(has_any_type(x) for x in t.items):
            return 15
        if not is_overlapping_none(t):
            return 10
    if isinstance(t, CallableType) and (has_any_type(t) or is_tricky_callable(t)):
        return 10
    return 0

</t>
<t tx="ekr.20230831011820.211">def collect_all_instances(t: Type) -&gt; list[Instance]:
    """Return all instances that `t` contains (including `t`).

    This is similar to collect_all_inner_types from typeanal but only
    returns instances and will recurse into fallbacks.
    """
    visitor = CollectAllInstancesQuery()
    t.accept(visitor)
    return visitor.instances


</t>
<t tx="ekr.20230831011820.2110">def score_callable(self, t: CallableType) -&gt; int:
    return sum(self.score_type(x, arg_pos=True) for x in t.arg_types) + self.score_type(
        t.ret_type, arg_pos=False
    )


</t>
<t tx="ekr.20230831011820.2111">def any_score_type(ut: Type, arg_pos: bool) -&gt; float:
    """Generate a very made up number representing the Anyness of a type.

    Higher is better, 1.0 is max
    """
    t = get_proper_type(ut)
    if isinstance(t, AnyType) and t.type_of_any != TypeOfAny.suggestion_engine:
        return 0
    if isinstance(t, NoneType) and arg_pos:
        return 0.5
    if isinstance(t, UnionType):
        if any(isinstance(get_proper_type(x), AnyType) for x in t.items):
            return 0.5
        if any(has_any_type(x) for x in t.items):
            return 0.25
    if isinstance(t, CallableType) and is_tricky_callable(t):
        return 0.5
    if has_any_type(t):
        return 0.5

    return 1.0


</t>
<t tx="ekr.20230831011820.2112">def any_score_callable(t: CallableType, is_method: bool, ignore_return: bool) -&gt; float:
    # Ignore the first argument of methods
    scores = [any_score_type(x, arg_pos=True) for x in t.arg_types[int(is_method) :]]
    # Return type counts twice (since it spreads type information), unless it is
    # None in which case it does not count at all. (Though it *does* still count
    # if there are no arguments.)
    if not isinstance(get_proper_type(t.ret_type), NoneType) or not scores:
        ret = 1.0 if ignore_return else any_score_type(t.ret_type, arg_pos=False)
        scores += [ret, ret]

    return sum(scores) / len(scores)


</t>
<t tx="ekr.20230831011820.2113">def is_tricky_callable(t: CallableType) -&gt; bool:
    """Is t a callable that we need to put a ... in for syntax reasons?"""
    return t.is_ellipsis_args or any(k.is_star() or k.is_named() for k in t.arg_kinds)


</t>
<t tx="ekr.20230831011820.2114">class TypeFormatter(TypeStrVisitor):
    """Visitor used to format types"""

    @others
</t>
<t tx="ekr.20230831011820.2115"># TODO: Probably a lot
def __init__(self, module: str | None, graph: Graph, options: Options) -&gt; None:
    super().__init__(options=options)
    self.module = module
    self.graph = graph

</t>
<t tx="ekr.20230831011820.2116">def visit_any(self, t: AnyType) -&gt; str:
    if t.missing_import_name:
        return t.missing_import_name
    else:
        return "Any"

</t>
<t tx="ekr.20230831011820.2117">def visit_instance(self, t: Instance) -&gt; str:
    s = t.type.fullname or t.type.name or None
    if s is None:
        return "&lt;???&gt;"
    if s in reverse_builtin_aliases:
        s = reverse_builtin_aliases[s]

    mod_obj = split_target(self.graph, s)
    assert mod_obj
    mod, obj = mod_obj

    # If a class is imported into the current module, rewrite the reference
    # to point to the current module. This helps the annotation tool avoid
    # inserting redundant imports when a type has been reexported.
    if self.module:
        parts = obj.split(".")  # need to split the object part if it is a nested class
        tree = self.graph[self.module].tree
        if tree and parts[0] in tree.names:
            mod = self.module

    if (mod, obj) == ("builtins", "tuple"):
        mod, obj = "typing", "Tuple[" + t.args[0].accept(self) + ", ...]"
    elif t.args:
        obj += f"[{self.list_str(t.args)}]"

    if mod_obj == ("builtins", "unicode"):
        return "Text"
    elif mod == "builtins":
        return obj
    else:
        delim = "." if "." not in obj else ":"
        return mod + delim + obj

</t>
<t tx="ekr.20230831011820.2118">def visit_tuple_type(self, t: TupleType) -&gt; str:
    if t.partial_fallback and t.partial_fallback.type:
        fallback_name = t.partial_fallback.type.fullname
        if fallback_name != "builtins.tuple":
            return t.partial_fallback.accept(self)
    s = self.list_str(t.items)
    return f"Tuple[{s}]"

</t>
<t tx="ekr.20230831011820.2119">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; str:
    return "Any"

</t>
<t tx="ekr.20230831011820.212">class CollectAllInstancesQuery(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011820.2120">def visit_typeddict_type(self, t: TypedDictType) -&gt; str:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20230831011820.2121">def visit_union_type(self, t: UnionType) -&gt; str:
    if len(t.items) == 2 and is_overlapping_none(t):
        return f"Optional[{remove_optional(t).accept(self)}]"
    else:
        return super().visit_union_type(t)

</t>
<t tx="ekr.20230831011820.2122">def visit_callable_type(self, t: CallableType) -&gt; str:
    # TODO: use extended callables?
    if is_tricky_callable(t):
        arg_str = "..."
    else:
        # Note: for default arguments, we just assume that they
        # are required.  This isn't right, but neither is the
        # other thing, and I suspect this will produce more better
        # results than falling back to `...`
        args = [typ.accept(self) for typ in t.arg_types]
        arg_str = f"[{', '.join(args)}]"

    return f"Callable[{arg_str}, {t.ret_type.accept(self)}]"


</t>
<t tx="ekr.20230831011820.2123">TType = TypeVar("TType", bound=Type)


def make_suggestion_anys(t: TType) -&gt; TType:
    """Make all anys in the type as coming from the suggestion engine.

    This keeps those Anys from influencing constraint generation,
    which allows us to do better when refining types.
    """
    return cast(TType, t.accept(MakeSuggestionAny()))


</t>
<t tx="ekr.20230831011820.2124">class MakeSuggestionAny(TypeTranslator):
    @others
</t>
<t tx="ekr.20230831011820.2125">def visit_any(self, t: AnyType) -&gt; Type:
    if not t.missing_import_name:
        return t.copy_modified(type_of_any=TypeOfAny.suggestion_engine)
    else:
        return t

</t>
<t tx="ekr.20230831011820.2126">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20230831011820.2127">def generate_type_combinations(types: list[Type]) -&gt; list[Type]:
    """Generate possible combinations of a list of types.

    mypy essentially supports two different ways to do this: joining the types
    and unioning the types. We try both.
    """
    joined_type = join_type_list(types)
    union_type = make_simplified_union(types)
    if joined_type == union_type:
        return [joined_type]
    else:
        return [joined_type, union_type]


</t>
<t tx="ekr.20230831011820.2128">def count_errors(msgs: list[str]) -&gt; int:
    return len([x for x in msgs if " error: " in x])


</t>
<t tx="ekr.20230831011820.2129">def refine_type(ti: Type, si: Type) -&gt; Type:
    """Refine `ti` by replacing Anys in it with information taken from `si`

    This basically works by, when the types have the same structure,
    traversing both of them in parallel and replacing Any on the left
    with whatever the type on the right is. If the types don't have the
    same structure (or aren't supported), the left type is chosen.

    For example:
      refine(Any, T) = T,  for all T
      refine(float, int) = float
      refine(List[Any], List[int]) = List[int]
      refine(Dict[int, Any], Dict[Any, int]) = Dict[int, int]
      refine(Tuple[int, Any], Tuple[Any, int]) = Tuple[int, int]

      refine(Callable[[Any], Any], Callable[[int], int]) = Callable[[int], int]
      refine(Callable[..., int], Callable[[int, float], Any]) = Callable[[int, float], int]

      refine(Optional[Any], int) = Optional[int]
      refine(Optional[Any], Optional[int]) = Optional[int]
      refine(Optional[Any], Union[int, str]) = Optional[Union[int, str]]
      refine(Optional[List[Any]], List[int]) = List[int]

    """
    t = get_proper_type(ti)
    s = get_proper_type(si)

    if isinstance(t, AnyType):
        # If s is also an Any, we return if it is a missing_import Any
        return t if isinstance(s, AnyType) and t.missing_import_name else s

    if isinstance(t, Instance) and isinstance(s, Instance) and t.type == s.type:
        return t.copy_modified(args=[refine_type(ta, sa) for ta, sa in zip(t.args, s.args)])

    if (
        isinstance(t, TupleType)
        and isinstance(s, TupleType)
        and t.partial_fallback == s.partial_fallback
        and len(t.items) == len(s.items)
    ):
        return t.copy_modified(items=[refine_type(ta, sa) for ta, sa in zip(t.items, s.items)])

    if isinstance(t, CallableType) and isinstance(s, CallableType):
        return refine_callable(t, s)

    if isinstance(t, UnionType):
        return refine_union(t, s)

    # TODO: Refining of builtins.tuple, Type?

    return t


</t>
<t tx="ekr.20230831011820.213">def __init__(self) -&gt; None:
    self.instances: list[Instance] = []

</t>
<t tx="ekr.20230831011820.2130">def refine_union(t: UnionType, s: ProperType) -&gt; Type:
    """Refine a union type based on another type.

    This is done by refining every component of the union against the
    right hand side type (or every component of its union if it is
    one). If an element of the union is successfully refined, we drop it
    from the union in favor of the refined versions.
    """
    # Don't try to do any union refining if the types are already the
    # same.  This prevents things like refining Optional[Any] against
    # itself and producing None.
    if t == s:
        return t

    rhs_items = s.items if isinstance(s, UnionType) else [s]

    new_items = []
    for lhs in t.items:
        refined = False
        for rhs in rhs_items:
            new = refine_type(lhs, rhs)
            if new != lhs:
                new_items.append(new)
                refined = True
        if not refined:
            new_items.append(lhs)

    # Turn strict optional on when simplifying the union since we
    # don't want to drop Nones.
    with state.strict_optional_set(True):
        return make_simplified_union(new_items)


</t>
<t tx="ekr.20230831011820.2131">def refine_callable(t: CallableType, s: CallableType) -&gt; CallableType:
    """Refine a callable based on another.

    See comments for refine_type.
    """
    if t.fallback != s.fallback:
        return t

    if t.is_ellipsis_args and not is_tricky_callable(s):
        return s.copy_modified(ret_type=refine_type(t.ret_type, s.ret_type))

    if is_tricky_callable(t) or t.arg_kinds != s.arg_kinds:
        return t

    return t.copy_modified(
        arg_types=[refine_type(ta, sa) for ta, sa in zip(t.arg_types, s.arg_types)],
        ret_type=refine_type(t.ret_type, s.ret_type),
    )


</t>
<t tx="ekr.20230831011820.2132">T = TypeVar("T")


def dedup(old: list[T]) -&gt; list[T]:
    new: list[T] = []
    for x in old:
        if x not in new:
            new.append(x)
    return new
</t>
<t tx="ekr.20230831011820.2133">@path mypy
"""Generic node traverser visitor"""
&lt;&lt; traverser.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.214">def visit_instance(self, t: Instance) -&gt; None:
    self.instances.append(t)
    super().visit_instance(t)

</t>
<t tx="ekr.20230831011820.215">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    if t.alias and not t.is_recursive:
        t.alias.target.accept(self)
    super().visit_type_alias_type(t)


</t>
<t tx="ekr.20230831011820.216">def find_type_overlaps(*types: Type) -&gt; set[str]:
    """Return a set of fullnames that share a short name and appear in either type.

    This is used to ensure that distinct types with the same short name are printed
    with their fullname.
    """
    d: dict[str, set[str]] = {}
    for type in types:
        for inst in collect_all_instances(type):
            d.setdefault(inst.type.name, set()).add(inst.type.fullname)
    for shortname in d.keys():
        if f"typing.{shortname}" in TYPES_FOR_UNIMPORTED_HINTS:
            d[shortname].add(f"typing.{shortname}")

    overlaps: set[str] = set()
    for fullnames in d.values():
        if len(fullnames) &gt; 1:
            overlaps.update(fullnames)
    return overlaps


</t>
<t tx="ekr.20230831011820.217">def format_type(
    typ: Type, options: Options, verbosity: int = 0, module_names: bool = False
) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    `verbosity` is a coarse grained control on the verbosity of the type

    This function returns a string appropriate for unmodified use in error
    messages; this means that it will be quoted in most cases.  If
    modification of the formatted string is required, callers should use
    format_type_bare.
    """
    return quote_type_string(format_type_bare(typ, options, verbosity, module_names))


</t>
<t tx="ekr.20230831011820.218">def format_type_bare(
    typ: Type, options: Options, verbosity: int = 0, module_names: bool = False
) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    `verbosity` is a coarse grained control on the verbosity of the type
    `fullnames` specifies a set of names that should be printed in full

    This function will return an unquoted string.  If a caller doesn't need to
    perform post-processing on the string output, format_type should be used
    instead.  (The caller may want to use quote_type_string after
    processing has happened, to maintain consistent quoting in messages.)
    """
    return format_type_inner(typ, verbosity, options, find_type_overlaps(typ), module_names)


</t>
<t tx="ekr.20230831011820.219">def format_type_distinctly(*types: Type, options: Options, bare: bool = False) -&gt; tuple[str, ...]:
    """Jointly format types to distinct strings.

    Increase the verbosity of the type strings until they become distinct
    while also requiring that distinct types with the same short name are
    formatted distinctly.

    By default, the returned strings are created using format_type() and will be
    quoted accordingly. If ``bare`` is True, the returned strings will not
    be quoted; callers who need to do post-processing of the strings before
    quoting them (such as prepending * or **) should use this.
    """
    overlapping = find_type_overlaps(*types)
    for verbosity in range(2):
        strs = [
            format_type_inner(type, verbosity=verbosity, options=options, fullnames=overlapping)
            for type in types
        ]
        if len(set(strs)) == len(strs):
            break
    if bare:
        return tuple(strs)
    else:
        return tuple(quote_type_string(s) for s in strs)


</t>
<t tx="ekr.20230831011820.22">def is_overlapping_erased_types(
    left: Type, right: Type, *, ignore_promotions: bool = False
) -&gt; bool:
    """The same as 'is_overlapping_erased_types', except the types are erased first."""
    return is_overlapping_types(
        erase_type(left),
        erase_type(right),
        ignore_promotions=ignore_promotions,
        prohibit_none_typevar_overlap=True,
    )


</t>
<t tx="ekr.20230831011820.220">def pretty_class_or_static_decorator(tp: CallableType) -&gt; str | None:
    """Return @classmethod or @staticmethod, if any, for the given callable type."""
    if tp.definition is not None and isinstance(tp.definition, SYMBOL_FUNCBASE_TYPES):
        if tp.definition.is_class:
            return "@classmethod"
        if tp.definition.is_static:
            return "@staticmethod"
    return None


</t>
<t tx="ekr.20230831011820.221">def pretty_callable(tp: CallableType, options: Options, skip_self: bool = False) -&gt; str:
    """Return a nice easily-readable representation of a callable type.
    For example:
        def [T &lt;: int] f(self, x: int, y: T) -&gt; None

    If skip_self is True, print an actual callable type, as it would appear
    when bound on an instance/class, rather than how it would appear in the
    defining statement.
    """
    s = ""
    asterisk = False
    slash = False
    for i in range(len(tp.arg_types)):
        if s:
            s += ", "
        if tp.arg_kinds[i].is_named() and not asterisk:
            s += "*, "
            asterisk = True
        if tp.arg_kinds[i] == ARG_STAR:
            s += "*"
            asterisk = True
        if tp.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = tp.arg_names[i]
        if name:
            s += name + ": "
        type_str = format_type_bare(tp.arg_types[i], options)
        if tp.arg_kinds[i] == ARG_STAR2 and tp.unpack_kwargs:
            type_str = f"Unpack[{type_str}]"
        s += type_str
        if tp.arg_kinds[i].is_optional():
            s += " = ..."
        if (
            not slash
            and tp.arg_kinds[i].is_positional()
            and name is None
            and (
                i == len(tp.arg_types) - 1
                or (tp.arg_names[i + 1] is not None or not tp.arg_kinds[i + 1].is_positional())
            )
        ):
            s += ", /"
            slash = True

    # If we got a "special arg" (i.e: self, cls, etc...), prepend it to the arg list
    if (
        isinstance(tp.definition, FuncDef)
        and hasattr(tp.definition, "arguments")
        and not tp.from_concatenate
    ):
        definition_arg_names = [arg.variable.name for arg in tp.definition.arguments]
        if (
            len(definition_arg_names) &gt; len(tp.arg_names)
            and definition_arg_names[0]
            and not skip_self
        ):
            if s:
                s = ", " + s
            s = definition_arg_names[0] + s
        s = f"{tp.definition.name}({s})"
    elif tp.name:
        first_arg = tp.def_extras.get("first_arg")
        if first_arg:
            if s:
                s = ", " + s
            s = first_arg + s
        s = f"{tp.name.split()[0]}({s})"  # skip "of Class" part
    else:
        s = f"({s})"

    s += " -&gt; "
    if tp.type_guard is not None:
        s += f"TypeGuard[{format_type_bare(tp.type_guard, options)}]"
    else:
        s += format_type_bare(tp.ret_type, options)

    if tp.variables:
        tvars = []
        for tvar in tp.variables:
            if isinstance(tvar, TypeVarType):
                upper_bound = get_proper_type(tvar.upper_bound)
                if (
                    isinstance(upper_bound, Instance)
                    and upper_bound.type.fullname != "builtins.object"
                ):
                    tvars.append(f"{tvar.name} &lt;: {format_type_bare(upper_bound, options)}")
                elif tvar.values:
                    tvars.append(
                        "{} in ({})".format(
                            tvar.name,
                            ", ".join([format_type_bare(tp, options) for tp in tvar.values]),
                        )
                    )
                else:
                    tvars.append(tvar.name)
            else:
                # For other TypeVarLikeTypes, just use the repr
                tvars.append(repr(tvar))
        s = f"[{', '.join(tvars)}] {s}"
    return f"def {s}"


</t>
<t tx="ekr.20230831011820.222">def variance_string(variance: int) -&gt; str:
    if variance == COVARIANT:
        return "covariant"
    elif variance == CONTRAVARIANT:
        return "contravariant"
    else:
        return "invariant"


</t>
<t tx="ekr.20230831011820.223">def get_missing_protocol_members(left: Instance, right: Instance, skip: list[str]) -&gt; list[str]:
    """Find all protocol members of 'right' that are not implemented
    (i.e. completely missing) in 'left'.
    """
    assert right.type.is_protocol
    missing: list[str] = []
    for member in right.type.protocol_members:
        if member in skip:
            continue
        if not find_member(member, left, left):
            missing.append(member)
    return missing


</t>
<t tx="ekr.20230831011820.224">def get_conflict_protocol_types(
    left: Instance, right: Instance, class_obj: bool = False, options: Options | None = None
) -&gt; list[tuple[str, Type, Type]]:
    """Find members that are defined in 'left' but have incompatible types.
    Return them as a list of ('member', 'got', 'expected').
    """
    assert right.type.is_protocol
    conflicts: list[tuple[str, Type, Type]] = []
    for member in right.type.protocol_members:
        if member in ("__init__", "__new__"):
            continue
        supertype = find_member(member, right, left)
        assert supertype is not None
        subtype = mypy.typeops.get_protocol_member(left, member, class_obj)
        if not subtype:
            continue
        is_compat = is_subtype(subtype, supertype, ignore_pos_arg_names=True, options=options)
        if IS_SETTABLE in get_member_flags(member, right):
            is_compat = is_compat and is_subtype(supertype, subtype, options=options)
        if not is_compat:
            conflicts.append((member, subtype, supertype))
    return conflicts


</t>
<t tx="ekr.20230831011820.225">def get_bad_protocol_flags(
    left: Instance, right: Instance, class_obj: bool = False
) -&gt; list[tuple[str, set[int], set[int]]]:
    """Return all incompatible attribute flags for members that are present in both
    'left' and 'right'.
    """
    assert right.type.is_protocol
    all_flags: list[tuple[str, set[int], set[int]]] = []
    for member in right.type.protocol_members:
        if find_member(member, left, left):
            item = (member, get_member_flags(member, left), get_member_flags(member, right))
            all_flags.append(item)
    bad_flags = []
    for name, subflags, superflags in all_flags:
        if (
            IS_CLASSVAR in subflags
            and IS_CLASSVAR not in superflags
            and IS_SETTABLE in superflags
            or IS_CLASSVAR in superflags
            and IS_CLASSVAR not in subflags
            or IS_SETTABLE in superflags
            and IS_SETTABLE not in subflags
            or IS_CLASS_OR_STATIC in superflags
            and IS_CLASS_OR_STATIC not in subflags
            or class_obj
            and IS_VAR in superflags
            and IS_CLASSVAR not in subflags
            or class_obj
            and IS_CLASSVAR in superflags
        ):
            bad_flags.append((name, subflags, superflags))
    return bad_flags


</t>
<t tx="ekr.20230831011820.226">def capitalize(s: str) -&gt; str:
    """Capitalize the first character of a string."""
    if s == "":
        return ""
    else:
        return s[0].upper() + s[1:]


</t>
<t tx="ekr.20230831011820.227">def extract_type(name: str) -&gt; str:
    """If the argument is the name of a method (of form C.m), return
    the type portion in quotes (e.g. "y"). Otherwise, return the string
    unmodified.
    """
    name = re.sub('^"[a-zA-Z0-9_]+" of ', "", name)
    return name


</t>
<t tx="ekr.20230831011820.228">def strip_quotes(s: str) -&gt; str:
    """Strip a double quote at the beginning and end of the string, if any."""
    s = re.sub('^"', "", s)
    s = re.sub('"$', "", s)
    return s


</t>
<t tx="ekr.20230831011820.229">def format_string_list(lst: list[str]) -&gt; str:
    assert lst
    if len(lst) == 1:
        return lst[0]
    elif len(lst) &lt;= 5:
        return f"{', '.join(lst[:-1])} and {lst[-1]}"
    else:
        return "%s, ... and %s (%i methods suppressed)" % (
            ", ".join(lst[:2]),
            lst[-1],
            len(lst) - 3,
        )


</t>
<t tx="ekr.20230831011820.23">def are_typed_dicts_overlapping(
    left: TypedDictType,
    right: TypedDictType,
    *,
    ignore_promotions: bool = False,
    prohibit_none_typevar_overlap: bool = False,
) -&gt; bool:
    """Returns 'true' if left and right are overlapping TypeDictTypes."""
    # All required keys in left are present and overlapping with something in right
    for key in left.required_keys:
        if key not in right.items:
            return False
        if not is_overlapping_types(
            left.items[key],
            right.items[key],
            ignore_promotions=ignore_promotions,
            prohibit_none_typevar_overlap=prohibit_none_typevar_overlap,
        ):
            return False

    # Repeat check in the other direction
    for key in right.required_keys:
        if key not in left.items:
            return False
        if not is_overlapping_types(
            left.items[key], right.items[key], ignore_promotions=ignore_promotions
        ):
            return False

    # The presence of any additional optional keys does not affect whether the two
    # TypedDicts are partially overlapping: the dicts would be overlapping if the
    # keys happened to be missing.
    return True


</t>
<t tx="ekr.20230831011820.230">def format_item_name_list(s: Iterable[str]) -&gt; str:
    lst = list(s)
    if len(lst) &lt;= 5:
        return "(" + ", ".join([f'"{name}"' for name in lst]) + ")"
    else:
        return "(" + ", ".join([f'"{name}"' for name in lst[:5]]) + ", ...)"


</t>
<t tx="ekr.20230831011820.231">def callable_name(type: FunctionLike) -&gt; str | None:
    name = type.get_name()
    if name is not None and name[0] != "&lt;":
        return f'"{name}"'.replace(" of ", '" of "')
    return name


</t>
<t tx="ekr.20230831011820.232">def for_function(callee: CallableType) -&gt; str:
    name = callable_name(callee)
    if name is not None:
        return f" for {name}"
    return ""


</t>
<t tx="ekr.20230831011820.233">def wrong_type_arg_count(n: int, act: str, name: str) -&gt; str:
    s = f"{n} type arguments"
    if n == 0:
        s = "no type arguments"
    elif n == 1:
        s = "1 type argument"
    if act == "0":
        act = "none"
    return f'"{name}" expects {s}, but {act} given'


</t>
<t tx="ekr.20230831011820.234">def find_defining_module(modules: dict[str, MypyFile], typ: CallableType) -&gt; MypyFile | None:
    if not typ.definition:
        return None
    fullname = typ.definition.fullname
    if "." in fullname:
        for i in range(fullname.count(".")):
            module_name = fullname.rsplit(".", i + 1)[0]
            try:
                return modules[module_name]
            except KeyError:
                pass
        assert False, "Couldn't determine module from CallableType"
    return None


</t>
<t tx="ekr.20230831011820.235"># For hard-coding suggested missing member alternatives.
COMMON_MISTAKES: Final[dict[str, Sequence[str]]] = {"add": ("append", "extend")}


def _real_quick_ratio(a: str, b: str) -&gt; float:
    # this is an upper bound on difflib.SequenceMatcher.ratio
    # similar to difflib.SequenceMatcher.real_quick_ratio, but faster since we don't instantiate
    al = len(a)
    bl = len(b)
    return 2.0 * min(al, bl) / (al + bl)


</t>
<t tx="ekr.20230831011820.236">def best_matches(current: str, options: Collection[str], n: int) -&gt; list[str]:
    if not current:
        return []
    # narrow down options cheaply
    options = [o for o in options if _real_quick_ratio(current, o) &gt; 0.75]
    if len(options) &gt;= 50:
        options = [o for o in options if abs(len(o) - len(current)) &lt;= 1]

    ratios = {option: difflib.SequenceMatcher(a=current, b=option).ratio() for option in options}
    options = [option for option, ratio in ratios.items() if ratio &gt; 0.75]
    return sorted(options, key=lambda v: (-ratios[v], v))[:n]


</t>
<t tx="ekr.20230831011820.237">def pretty_seq(args: Sequence[str], conjunction: str) -&gt; str:
    quoted = ['"' + a + '"' for a in args]
    if len(quoted) == 1:
        return quoted[0]
    if len(quoted) == 2:
        return f"{quoted[0]} {conjunction} {quoted[1]}"
    last_sep = ", " + conjunction + " "
    return ", ".join(quoted[:-1]) + last_sep + quoted[-1]


</t>
<t tx="ekr.20230831011820.238">def append_invariance_notes(
    notes: list[str], arg_type: Instance, expected_type: Instance
) -&gt; list[str]:
    """Explain that the type is invariant and give notes for how to solve the issue."""
    invariant_type = ""
    covariant_suggestion = ""
    if (
        arg_type.type.fullname == "builtins.list"
        and expected_type.type.fullname == "builtins.list"
        and is_subtype(arg_type.args[0], expected_type.args[0])
    ):
        invariant_type = "List"
        covariant_suggestion = 'Consider using "Sequence" instead, which is covariant'
    elif (
        arg_type.type.fullname == "builtins.dict"
        and expected_type.type.fullname == "builtins.dict"
        and is_same_type(arg_type.args[0], expected_type.args[0])
        and is_subtype(arg_type.args[1], expected_type.args[1])
    ):
        invariant_type = "Dict"
        covariant_suggestion = (
            'Consider using "Mapping" instead, ' "which is covariant in the value type"
        )
    if invariant_type and covariant_suggestion:
        notes.append(
            f'"{invariant_type}" is invariant -- see '
            + "https://mypy.readthedocs.io/en/stable/common_issues.html#variance"
        )
        notes.append(covariant_suggestion)
    return notes


</t>
<t tx="ekr.20230831011820.239">def append_numbers_notes(
    notes: list[str], arg_type: Instance, expected_type: Instance
) -&gt; list[str]:
    """Explain if an unsupported type from "numbers" is used in a subtype check."""
    if expected_type.type.fullname in UNSUPPORTED_NUMBERS_TYPES:
        notes.append('Types from "numbers" aren\'t supported for static type checking')
        notes.append("See https://peps.python.org/pep-0484/#the-numeric-tower")
        notes.append("Consider using a protocol instead, such as typing.SupportsFloat")
    return notes


</t>
<t tx="ekr.20230831011820.24">def are_tuples_overlapping(
    left: Type,
    right: Type,
    *,
    ignore_promotions: bool = False,
    prohibit_none_typevar_overlap: bool = False,
) -&gt; bool:
    """Returns true if left and right are overlapping tuples."""
    left, right = get_proper_types((left, right))
    left = adjust_tuple(left, right) or left
    right = adjust_tuple(right, left) or right
    assert isinstance(left, TupleType), f"Type {left} is not a tuple"
    assert isinstance(right, TupleType), f"Type {right} is not a tuple"
    if len(left.items) != len(right.items):
        return False
    return all(
        is_overlapping_types(
            l,
            r,
            ignore_promotions=ignore_promotions,
            prohibit_none_typevar_overlap=prohibit_none_typevar_overlap,
        )
        for l, r in zip(left.items, right.items)
    )


</t>
<t tx="ekr.20230831011820.240">def make_inferred_type_note(
    context: Context, subtype: Type, supertype: Type, supertype_str: str
) -&gt; str:
    """Explain that the user may have forgotten to type a variable.

    The user does not expect an error if the inferred container type is the same as the return
    type of a function and the argument type(s) are a subtype of the argument type(s) of the
    return type. This note suggests that they add a type annotation with the return type instead
    of relying on the inferred type.
    """
    subtype = get_proper_type(subtype)
    supertype = get_proper_type(supertype)
    if (
        isinstance(subtype, Instance)
        and isinstance(supertype, Instance)
        and subtype.type.fullname == supertype.type.fullname
        and subtype.args
        and supertype.args
        and isinstance(context, ReturnStmt)
        and isinstance(context.expr, NameExpr)
        and isinstance(context.expr.node, Var)
        and context.expr.node.is_inferred
    ):
        for subtype_arg, supertype_arg in zip(subtype.args, supertype.args):
            if not is_subtype(subtype_arg, supertype_arg):
                return ""
        var_name = context.expr.name
        return 'Perhaps you need a type annotation for "{}"? Suggestion: {}'.format(
            var_name, supertype_str
        )
    return ""


</t>
<t tx="ekr.20230831011820.241">def format_key_list(keys: list[str], *, short: bool = False) -&gt; str:
    formatted_keys = [f'"{key}"' for key in keys]
    td = "" if short else "TypedDict "
    if len(keys) == 0:
        return f"no {td}keys"
    elif len(keys) == 1:
        return f"{td}key {formatted_keys[0]}"
    else:
        return f"{td}keys ({', '.join(formatted_keys)})"
</t>
<t tx="ekr.20230831011820.242">@path mypy
"""Interfaces for accessing metadata.

We provide two implementations.
 * The "classic" file system implementation, which uses a directory
   structure of files.
 * A hokey sqlite backed implementation, which basically simulates
   the file system in an effort to work around poor file system performance
   on OS X.
"""

&lt;&lt; metastore.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.244">from __future__ import annotations

import binascii
import os
import time
from abc import abstractmethod
from typing import TYPE_CHECKING, Any, Iterable

if TYPE_CHECKING:
    # We avoid importing sqlite3 unless we are using it so we can mostly work
    # on semi-broken pythons that are missing it.
    import sqlite3


</t>
<t tx="ekr.20230831011820.245">class MetadataStore:
    """Generic interface for metadata storage."""

    @others
</t>
<t tx="ekr.20230831011820.246">@abstractmethod
def getmtime(self, name: str) -&gt; float:
</t>
<t tx="ekr.20230831011820.247">    """Read the mtime of a metadata entry..

    Raises FileNotFound if the entry does not exist.
    """

@abstractmethod
def read(self, name: str) -&gt; str:
</t>
<t tx="ekr.20230831011820.248">    """Read the contents of a metadata entry.

    Raises FileNotFound if the entry does not exist.
    """

@abstractmethod
def write(self, name: str, data: str, mtime: float | None = None) -&gt; bool:
</t>
<t tx="ekr.20230831011820.249">    """Write a metadata entry.

    If mtime is specified, set it as the mtime of the entry. Otherwise,
    the current time is used.

    Returns True if the entry is successfully written, False otherwise.
    """

@abstractmethod
def remove(self, name: str) -&gt; None:
</t>
<t tx="ekr.20230831011820.25">def adjust_tuple(left: ProperType, r: ProperType) -&gt; TupleType | None:
    """Find out if `left` is a Tuple[A, ...], and adjust its length to `right`"""
    if isinstance(left, Instance) and left.type.fullname == "builtins.tuple":
        n = r.length() if isinstance(r, TupleType) else 1
        return TupleType([left.args[0]] * n, left)
    return None


</t>
<t tx="ekr.20230831011820.250">    """Delete a metadata entry"""

@abstractmethod
def commit(self) -&gt; None:
</t>
<t tx="ekr.20230831011820.251">    """If the backing store requires a commit, do it.

    But N.B. that this is not *guaranteed* to do anything, and
    there is no guarantee that changes are not made until it is
    called.
    """

@abstractmethod
def list_all(self) -&gt; Iterable[str]:
    ...


</t>
<t tx="ekr.20230831011820.252">def random_string() -&gt; str:
    return binascii.hexlify(os.urandom(8)).decode("ascii")


</t>
<t tx="ekr.20230831011820.253">class FilesystemMetadataStore(MetadataStore):
    @others
</t>
<t tx="ekr.20230831011820.254">def __init__(self, cache_dir_prefix: str) -&gt; None:
    # We check startswith instead of equality because the version
    # will have already been appended by the time the cache dir is
    # passed here.
    if cache_dir_prefix.startswith(os.devnull):
        self.cache_dir_prefix = None
    else:
        self.cache_dir_prefix = cache_dir_prefix

</t>
<t tx="ekr.20230831011820.255">def getmtime(self, name: str) -&gt; float:
    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    return int(os.path.getmtime(os.path.join(self.cache_dir_prefix, name)))

</t>
<t tx="ekr.20230831011820.256">def read(self, name: str) -&gt; str:
    assert os.path.normpath(name) != os.path.abspath(name), "Don't use absolute paths!"

    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    with open(os.path.join(self.cache_dir_prefix, name)) as f:
        return f.read()

</t>
<t tx="ekr.20230831011820.257">def write(self, name: str, data: str, mtime: float | None = None) -&gt; bool:
    assert os.path.normpath(name) != os.path.abspath(name), "Don't use absolute paths!"

    if not self.cache_dir_prefix:
        return False

    path = os.path.join(self.cache_dir_prefix, name)
    tmp_filename = path + "." + random_string()
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(tmp_filename, "w") as f:
            f.write(data)
        os.replace(tmp_filename, path)
        if mtime is not None:
            os.utime(path, times=(mtime, mtime))

    except os.error:
        return False
    return True

</t>
<t tx="ekr.20230831011820.258">def remove(self, name: str) -&gt; None:
    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    os.remove(os.path.join(self.cache_dir_prefix, name))

</t>
<t tx="ekr.20230831011820.259">def commit(self) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011820.26">def is_tuple(typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, TupleType) or (
        isinstance(typ, Instance) and typ.type.fullname == "builtins.tuple"
    )


</t>
<t tx="ekr.20230831011820.260">def list_all(self) -&gt; Iterable[str]:
    if not self.cache_dir_prefix:
        return

    for dir, _, files in os.walk(self.cache_dir_prefix):
        dir = os.path.relpath(dir, self.cache_dir_prefix)
        for file in files:
            yield os.path.join(dir, file)


</t>
<t tx="ekr.20230831011820.261">SCHEMA = """
CREATE TABLE IF NOT EXISTS files (
    path TEXT UNIQUE NOT NULL,
    mtime REAL,
    data TEXT
);
CREATE INDEX IF NOT EXISTS path_idx on files(path);
"""
# No migrations yet
MIGRATIONS: list[str] = []


def connect_db(db_file: str) -&gt; sqlite3.Connection:
    import sqlite3.dbapi2

    db = sqlite3.dbapi2.connect(db_file)
    db.executescript(SCHEMA)
    for migr in MIGRATIONS:
        try:
            db.executescript(migr)
        except sqlite3.OperationalError:
            pass
    return db


</t>
<t tx="ekr.20230831011820.262">class SqliteMetadataStore(MetadataStore):
    @others
</t>
<t tx="ekr.20230831011820.263">def __init__(self, cache_dir_prefix: str) -&gt; None:
    # We check startswith instead of equality because the version
    # will have already been appended by the time the cache dir is
    # passed here.
    if cache_dir_prefix.startswith(os.devnull):
        self.db = None
        return

    os.makedirs(cache_dir_prefix, exist_ok=True)
    self.db = connect_db(os.path.join(cache_dir_prefix, "cache.db"))

</t>
<t tx="ekr.20230831011820.264">def _query(self, name: str, field: str) -&gt; Any:
    # Raises FileNotFound for consistency with the file system version
    if not self.db:
        raise FileNotFoundError()

    cur = self.db.execute(f"SELECT {field} FROM files WHERE path = ?", (name,))
    results = cur.fetchall()
    if not results:
        raise FileNotFoundError()
    assert len(results) == 1
    return results[0][0]

</t>
<t tx="ekr.20230831011820.265">def getmtime(self, name: str) -&gt; float:
    mtime = self._query(name, "mtime")
    assert isinstance(mtime, float)
    return mtime

</t>
<t tx="ekr.20230831011820.266">def read(self, name: str) -&gt; str:
    data = self._query(name, "data")
    assert isinstance(data, str)
    return data

</t>
<t tx="ekr.20230831011820.267">def write(self, name: str, data: str, mtime: float | None = None) -&gt; bool:
    import sqlite3

    if not self.db:
        return False
    try:
        if mtime is None:
            mtime = time.time()
        self.db.execute(
            "INSERT OR REPLACE INTO files(path, mtime, data) VALUES(?, ?, ?)",
            (name, mtime, data),
        )
    except sqlite3.OperationalError:
        return False
    return True

</t>
<t tx="ekr.20230831011820.268">def remove(self, name: str) -&gt; None:
    if not self.db:
        raise FileNotFoundError()

    self.db.execute("DELETE FROM files WHERE path = ?", (name,))

</t>
<t tx="ekr.20230831011820.269">def commit(self) -&gt; None:
    if self.db:
        self.db.commit()

</t>
<t tx="ekr.20230831011820.27">class TypeMeetVisitor(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20230831011820.270">def list_all(self) -&gt; Iterable[str]:
    if self.db:
        for row in self.db.execute("SELECT path FROM files"):
            yield row[0]
</t>
<t tx="ekr.20230831011820.271">@path mypy
&lt;&lt; mixedtraverser.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.272">from __future__ import annotations

from mypy.nodes import (
    AssertTypeExpr,
    AssignmentStmt,
    CastExpr,
    ClassDef,
    ForStmt,
    FuncItem,
    NamedTupleExpr,
    NewTypeExpr,
    PromoteExpr,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    Var,
    WithStmt,
)
from mypy.traverser import TraverserVisitor
from mypy.types import Type
from mypy.typetraverser import TypeTraverserVisitor


</t>
<t tx="ekr.20230831011820.273">class MixedTraverserVisitor(TraverserVisitor, TypeTraverserVisitor):
    """Recursive traversal of both Node and Type objects."""

    @others
</t>
<t tx="ekr.20230831011820.274">def __init__(self) -&gt; None:
    self.in_type_alias_expr = False

</t>
<t tx="ekr.20230831011820.275"># Symbol nodes

def visit_var(self, var: Var) -&gt; None:
    self.visit_optional_type(var.type)

</t>
<t tx="ekr.20230831011820.276">def visit_func(self, o: FuncItem) -&gt; None:
    super().visit_func(o)
    self.visit_optional_type(o.type)

</t>
<t tx="ekr.20230831011820.277">def visit_class_def(self, o: ClassDef) -&gt; None:
    # TODO: Should we visit generated methods/variables as well, either here or in
    #       TraverserVisitor?
    super().visit_class_def(o)
    info = o.info
    if info:
        for base in info.bases:
            base.accept(self)

</t>
<t tx="ekr.20230831011820.278">def visit_type_alias_expr(self, o: TypeAliasExpr) -&gt; None:
    super().visit_type_alias_expr(o)
    self.in_type_alias_expr = True
    o.node.target.accept(self)
    self.in_type_alias_expr = False

</t>
<t tx="ekr.20230831011820.279">def visit_type_var_expr(self, o: TypeVarExpr) -&gt; None:
    super().visit_type_var_expr(o)
    o.upper_bound.accept(self)
    for value in o.values:
        value.accept(self)

</t>
<t tx="ekr.20230831011820.28">def __init__(self, s: ProperType) -&gt; None:
    self.s = s

</t>
<t tx="ekr.20230831011820.280">def visit_typeddict_expr(self, o: TypedDictExpr) -&gt; None:
    super().visit_typeddict_expr(o)
    self.visit_optional_type(o.info.typeddict_type)

</t>
<t tx="ekr.20230831011820.281">def visit_namedtuple_expr(self, o: NamedTupleExpr) -&gt; None:
    super().visit_namedtuple_expr(o)
    assert o.info.tuple_type
    o.info.tuple_type.accept(self)

</t>
<t tx="ekr.20230831011820.282">def visit__promote_expr(self, o: PromoteExpr) -&gt; None:
    super().visit__promote_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20230831011820.283">def visit_newtype_expr(self, o: NewTypeExpr) -&gt; None:
    super().visit_newtype_expr(o)
    self.visit_optional_type(o.old_type)

</t>
<t tx="ekr.20230831011820.284"># Statements

def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    super().visit_assignment_stmt(o)
    self.visit_optional_type(o.type)

</t>
<t tx="ekr.20230831011820.285">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    super().visit_for_stmt(o)
    self.visit_optional_type(o.index_type)

</t>
<t tx="ekr.20230831011820.286">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    super().visit_with_stmt(o)
    for typ in o.analyzed_types:
        typ.accept(self)

</t>
<t tx="ekr.20230831011820.287"># Expressions

def visit_cast_expr(self, o: CastExpr) -&gt; None:
    super().visit_cast_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20230831011820.288">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    super().visit_assert_type_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20230831011820.289">def visit_type_application(self, o: TypeApplication) -&gt; None:
    super().visit_type_application(o)
    for t in o.types:
        t.accept(self)

</t>
<t tx="ekr.20230831011820.29">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    if isinstance(self.s, NoneType):
        if state.strict_optional:
            return AnyType(TypeOfAny.special_form)
        else:
            return self.s
    elif isinstance(self.s, UninhabitedType):
        return self.s
    else:
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011820.290"># Helpers

def visit_optional_type(self, t: Type | None) -&gt; None:
    if t:
        t.accept(self)
</t>
<t tx="ekr.20230831011820.291">@path mypy
"""Low-level infrastructure to find modules.

This builds on fscache.py; find_sources.py builds on top of this.
"""

&lt;&lt; modulefinder.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.293">from __future__ import annotations

import ast
import collections
import functools
import os
import re
import subprocess
import sys
from enum import Enum, unique

from mypy.errors import CompileError

if sys.version_info &gt;= (3, 11):
    import tomllib
else:
    import tomli as tomllib

from typing import Dict, Final, List, NamedTuple, Optional, Tuple, Union
from typing_extensions import TypeAlias as _TypeAlias

from mypy import pyinfo
from mypy.fscache import FileSystemCache
from mypy.nodes import MypyFile
from mypy.options import Options
from mypy.stubinfo import approved_stub_package_exists


# Paths to be searched in find_module().
</t>
<t tx="ekr.20230831011820.294">class SearchPaths(NamedTuple):
    python_path: tuple[str, ...]  # where user code is found
    mypy_path: tuple[str, ...]  # from $MYPYPATH or config variable
    package_path: tuple[str, ...]  # from get_site_packages_dirs()
    typeshed_path: tuple[str, ...]  # paths in typeshed


</t>
<t tx="ekr.20230831011820.295"># Package dirs are a two-tuple of path to search and whether to verify the module
OnePackageDir = Tuple[str, bool]
PackageDirs = List[OnePackageDir]

# Minimum and maximum Python versions for modules in stdlib as (major, minor)
StdlibVersions: _TypeAlias = Dict[str, Tuple[Tuple[int, int], Optional[Tuple[int, int]]]]

PYTHON_EXTENSIONS: Final = [".pyi", ".py"]


# TODO: Consider adding more reasons here?
# E.g. if we deduce a module would likely be found if the user were
# to set the --namespace-packages flag.
@unique
class ModuleNotFoundReason(Enum):
    @others
</t>
<t tx="ekr.20230831011820.296"># The module was not found: we found neither stubs nor a plausible code
# implementation (with or without a py.typed file).
NOT_FOUND = 0

# The implementation for this module plausibly exists (e.g. we
# found a matching folder or *.py file), but either the parent package
# did not contain a py.typed file or we were unable to find a
# corresponding *-stubs package.
FOUND_WITHOUT_TYPE_HINTS = 1

# The module was not found in the current working directory, but
# was able to be found in the parent directory.
WRONG_WORKING_DIRECTORY = 2

# Stub PyPI package (typically types-pkgname) known to exist but not installed.
APPROVED_STUBS_NOT_INSTALLED = 3

def error_message_templates(self, daemon: bool) -&gt; tuple[str, list[str]]:
    doc_link = "See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports"
    if self is ModuleNotFoundReason.NOT_FOUND:
        msg = 'Cannot find implementation or library stub for module named "{module}"'
        notes = [doc_link]
    elif self is ModuleNotFoundReason.WRONG_WORKING_DIRECTORY:
        msg = 'Cannot find implementation or library stub for module named "{module}"'
        notes = [
            "You may be running mypy in a subpackage, "
            "mypy should be run on the package root"
        ]
    elif self is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
        msg = (
            'Skipping analyzing "{module}": module is installed, but missing library stubs '
            "or py.typed marker"
        )
        notes = [doc_link]
    elif self is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
        msg = 'Library stubs not installed for "{module}"'
        notes = ['Hint: "python3 -m pip install {stub_dist}"']
        if not daemon:
            notes.append(
                '(or run "mypy --install-types" to install all missing stub packages)'
            )
        notes.append(doc_link)
    else:
        assert False
    return msg, notes


</t>
<t tx="ekr.20230831011820.297"># If we found the module, returns the path to the module as a str.
# Otherwise, returns the reason why the module wasn't found.
ModuleSearchResult = Union[str, ModuleNotFoundReason]


class BuildSource:
    """A single source file."""

    @others
</t>
<t tx="ekr.20230831011820.298">def __init__(
    self,
    path: str | None,
    module: str | None,
    text: str | None = None,
    base_dir: str | None = None,
    followed: bool = False,
) -&gt; None:
    self.path = path  # File where it's found (e.g. 'xxx/yyy/foo/bar.py')
    self.module = module or "__main__"  # Module name (e.g. 'foo.bar')
    self.text = text  # Source code, if initially supplied, else None
    self.base_dir = base_dir  # Directory where the package is rooted (e.g. 'xxx/yyy')
    self.followed = followed  # Was this found by following imports?

</t>
<t tx="ekr.20230831011820.299">def __repr__(self) -&gt; str:
    return (
        "BuildSource(path={!r}, module={!r}, has_text={}, base_dir={!r}, followed={})".format(
            self.path, self.module, self.text is not None, self.base_dir, self.followed
        )
    )


</t>
<t tx="ekr.20230831011820.3">def fail(msg: str, stderr: TextIO, options: Options) -&gt; NoReturn:
    """Fail with a serious error."""
    stderr.write(f"{msg}\n")
    maybe_write_junit_xml(0.0, serious=True, messages=[msg], options=options)
    sys.exit(2)


</t>
<t tx="ekr.20230831011820.30">def visit_any(self, t: AnyType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20230831011820.300">class BuildSourceSet:
    """Helper to efficiently test a file's membership in a set of build sources."""

    @others
</t>
<t tx="ekr.20230831011820.301">def __init__(self, sources: list[BuildSource]) -&gt; None:
    self.source_text_present = False
    self.source_modules: dict[str, str] = {}
    self.source_paths: set[str] = set()

    for source in sources:
        if source.text is not None:
            self.source_text_present = True
        if source.path:
            self.source_paths.add(source.path)
        if source.module:
            self.source_modules[source.module] = source.path or ""

</t>
<t tx="ekr.20230831011820.302">def is_source(self, file: MypyFile) -&gt; bool:
    return (
        (file.path and file.path in self.source_paths)
        or file._fullname in self.source_modules
        or self.source_text_present
    )


</t>
<t tx="ekr.20230831011820.303">class FindModuleCache:
    """Module finder with integrated cache.

    Module locations and some intermediate results are cached internally
    and can be cleared with the clear() method.

    All file system accesses are performed through a FileSystemCache,
    which is not ever cleared by this class. If necessary it must be
    cleared by client code.
    """

    @others
</t>
<t tx="ekr.20230831011820.304">def __init__(
    self,
    search_paths: SearchPaths,
    fscache: FileSystemCache | None,
    options: Options | None,
    stdlib_py_versions: StdlibVersions | None = None,
    source_set: BuildSourceSet | None = None,
) -&gt; None:
    self.search_paths = search_paths
    self.source_set = source_set
    self.fscache = fscache or FileSystemCache()
    # Cache for get_toplevel_possibilities:
    # search_paths -&gt; (toplevel_id -&gt; list(package_dirs))
    self.initial_components: dict[tuple[str, ...], dict[str, list[str]]] = {}
    # Cache find_module: id -&gt; result
    self.results: dict[str, ModuleSearchResult] = {}
    self.ns_ancestors: dict[str, str] = {}
    self.options = options
    custom_typeshed_dir = None
    if options:
        custom_typeshed_dir = options.custom_typeshed_dir
    self.stdlib_py_versions = stdlib_py_versions or load_stdlib_py_versions(
        custom_typeshed_dir
    )

</t>
<t tx="ekr.20230831011820.305">def clear(self) -&gt; None:
    self.results.clear()
    self.initial_components.clear()
    self.ns_ancestors.clear()

</t>
<t tx="ekr.20230831011820.306">def find_module_via_source_set(self, id: str) -&gt; ModuleSearchResult | None:
    """Fast path to find modules by looking through the input sources

    This is only used when --fast-module-lookup is passed on the command line."""
    if not self.source_set:
        return None

    p = self.source_set.source_modules.get(id, None)
    if p and self.fscache.isfile(p):
        # We need to make sure we still have __init__.py all the way up
        # otherwise we might have false positives compared to slow path
        # in case of deletion of init files, which is covered by some tests.
        # TODO: are there some combination of flags in which this check should be skipped?
        d = os.path.dirname(p)
        for _ in range(id.count(".")):
            if not any(
                self.fscache.isfile(os.path.join(d, "__init__" + x)) for x in PYTHON_EXTENSIONS
            ):
                return None
            d = os.path.dirname(d)
        return p

    idx = id.rfind(".")
    if idx != -1:
        # When we're looking for foo.bar.baz and can't find a matching module
        # in the source set, look up for a foo.bar module.
        parent = self.find_module_via_source_set(id[:idx])
        if parent is None or not isinstance(parent, str):
            return None

        basename, ext = os.path.splitext(parent)
        if not any(parent.endswith("__init__" + x) for x in PYTHON_EXTENSIONS) and (
            ext in PYTHON_EXTENSIONS and not self.fscache.isdir(basename)
        ):
            # If we do find such a *module* (and crucially, we don't want a package,
            # hence the filtering out of __init__ files, and checking for the presence
            # of a folder with a matching name), then we can be pretty confident that
            # 'baz' will either be a top-level variable in foo.bar, or will not exist.
            #
            # Either way, spelunking in other search paths for another 'foo.bar.baz'
            # module should be avoided because:
            #  1. in the unlikely event that one were found, it's highly likely that
            #     it would be unrelated to the source being typechecked and therefore
            #     more likely to lead to erroneous results
            #  2. as described in _find_module, in some cases the search itself could
            #  potentially waste significant amounts of time
            return ModuleNotFoundReason.NOT_FOUND
    return None

</t>
<t tx="ekr.20230831011820.307">def find_lib_path_dirs(self, id: str, lib_path: tuple[str, ...]) -&gt; PackageDirs:
    """Find which elements of a lib_path have the directory a module needs to exist.

    This is run for the python_path, mypy_path, and typeshed_path search paths.
    """
    components = id.split(".")
    dir_chain = os.sep.join(components[:-1])  # e.g., 'foo/bar'

    dirs = []
    for pathitem in self.get_toplevel_possibilities(lib_path, components[0]):
        # e.g., '/usr/lib/python3.4/foo/bar'
        dir = os.path.normpath(os.path.join(pathitem, dir_chain))
        if self.fscache.isdir(dir):
            dirs.append((dir, True))
    return dirs

</t>
<t tx="ekr.20230831011820.308">def get_toplevel_possibilities(self, lib_path: tuple[str, ...], id: str) -&gt; list[str]:
    """Find which elements of lib_path could contain a particular top-level module.

    In practice, almost all modules can be routed to the correct entry in
    lib_path by looking at just the first component of the module name.

    We take advantage of this by enumerating the contents of all of the
    directories on the lib_path and building a map of which entries in
    the lib_path could contain each potential top-level module that appears.
    """

    if lib_path in self.initial_components:
        return self.initial_components[lib_path].get(id, [])

    # Enumerate all the files in the directories on lib_path and produce the map
    components: dict[str, list[str]] = {}
    for dir in lib_path:
        try:
            contents = self.fscache.listdir(dir)
        except OSError:
            contents = []
        # False positives are fine for correctness here, since we will check
        # precisely later, so we only look at the root of every filename without
        # any concern for the exact details.
        for name in contents:
            name = os.path.splitext(name)[0]
            components.setdefault(name, []).append(dir)

    self.initial_components[lib_path] = components
    return components.get(id, [])

</t>
<t tx="ekr.20230831011820.309">def find_module(self, id: str, *, fast_path: bool = False) -&gt; ModuleSearchResult:
    """Return the path of the module source file or why it wasn't found.

    If fast_path is True, prioritize performance over generating detailed
    error descriptions.
    """
    if id not in self.results:
        top_level = id.partition(".")[0]
        use_typeshed = True
        if id in self.stdlib_py_versions:
            use_typeshed = self._typeshed_has_version(id)
        elif top_level in self.stdlib_py_versions:
            use_typeshed = self._typeshed_has_version(top_level)
        self.results[id] = self._find_module(id, use_typeshed)
        if (
            not (fast_path or (self.options is not None and self.options.fast_module_lookup))
            and self.results[id] is ModuleNotFoundReason.NOT_FOUND
            and self._can_find_module_in_parent_dir(id)
        ):
            self.results[id] = ModuleNotFoundReason.WRONG_WORKING_DIRECTORY
    return self.results[id]

</t>
<t tx="ekr.20230831011820.31">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    if isinstance(self.s, UnionType):
        meets: list[Type] = []
        for x in t.items:
            for y in self.s.items:
                meets.append(meet_types(x, y))
    else:
        meets = [meet_types(x, self.s) for x in t.items]
    return make_simplified_union(meets)

</t>
<t tx="ekr.20230831011820.310">def _typeshed_has_version(self, module: str) -&gt; bool:
    if not self.options:
        return True
    version = typeshed_py_version(self.options)
    min_version, max_version = self.stdlib_py_versions[module]
    return version &gt;= min_version and (max_version is None or version &lt;= max_version)

</t>
<t tx="ekr.20230831011820.311">def _find_module_non_stub_helper(
    self, components: list[str], pkg_dir: str
) -&gt; OnePackageDir | ModuleNotFoundReason:
    plausible_match = False
    dir_path = pkg_dir
    for index, component in enumerate(components):
        dir_path = os.path.join(dir_path, component)
        if self.fscache.isfile(os.path.join(dir_path, "py.typed")):
            return os.path.join(pkg_dir, *components[:-1]), index == 0
        elif not plausible_match and (
            self.fscache.isdir(dir_path) or self.fscache.isfile(dir_path + ".py")
        ):
            plausible_match = True
        # If this is not a directory then we can't traverse further into it
        if not self.fscache.isdir(dir_path):
            break
    for i in range(len(components), 0, -1):
        if approved_stub_package_exists(".".join(components[:i])):
            return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    if plausible_match:
        return ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS
    else:
        return ModuleNotFoundReason.NOT_FOUND

</t>
<t tx="ekr.20230831011820.312">def _update_ns_ancestors(self, components: list[str], match: tuple[str, bool]) -&gt; None:
    path, verify = match
    for i in range(1, len(components)):
        pkg_id = ".".join(components[:-i])
        if pkg_id not in self.ns_ancestors and self.fscache.isdir(path):
            self.ns_ancestors[pkg_id] = path
        path = os.path.dirname(path)

</t>
<t tx="ekr.20230831011820.313">def _can_find_module_in_parent_dir(self, id: str) -&gt; bool:
    """Test if a module can be found by checking the parent directories
    of the current working directory.
    """
    working_dir = os.getcwd()
    parent_search = FindModuleCache(
        SearchPaths((), (), (), ()),
        self.fscache,
        self.options,
        stdlib_py_versions=self.stdlib_py_versions,
    )
    while any(is_init_file(file) for file in os.listdir(working_dir)):
        working_dir = os.path.dirname(working_dir)
        parent_search.search_paths = SearchPaths((working_dir,), (), (), ())
        if not isinstance(parent_search._find_module(id, False), ModuleNotFoundReason):
            return True
    return False

</t>
<t tx="ekr.20230831011820.314">def _find_module(self, id: str, use_typeshed: bool) -&gt; ModuleSearchResult:
    fscache = self.fscache

    # Fast path for any modules in the current source set.
    # This is particularly important when there are a large number of search
    # paths which share the first (few) component(s) due to the use of namespace
    # packages, for instance:
    # foo/
    #    company/
    #        __init__.py
    #        foo/
    # bar/
    #    company/
    #        __init__.py
    #        bar/
    # baz/
    #    company/
    #        __init__.py
    #        baz/
    #
    # mypy gets [foo/company/foo, bar/company/bar, baz/company/baz, ...] as input
    # and computes [foo, bar, baz, ...] as the module search path.
    #
    # This would result in O(n) search for every import of company.*, leading to
    # O(n**2) behavior in load_graph as such imports are unsurprisingly present
    # at least once, and usually many more times than that, in each and every file
    # being parsed.
    #
    # Thankfully, such cases are efficiently handled by looking up the module path
    # via BuildSourceSet.
    p = (
        self.find_module_via_source_set(id)
        if (self.options is not None and self.options.fast_module_lookup)
        else None
    )
    if p:
        return p

    # If we're looking for a module like 'foo.bar.baz', it's likely that most of the
    # many elements of lib_path don't even have a subdirectory 'foo/bar'.  Discover
    # that only once and cache it for when we look for modules like 'foo.bar.blah'
    # that will require the same subdirectory.
    components = id.split(".")
    dir_chain = os.sep.join(components[:-1])  # e.g., 'foo/bar'

    # We have two sets of folders so that we collect *all* stubs folders and
    # put them in the front of the search path
    third_party_inline_dirs: PackageDirs = []
    third_party_stubs_dirs: PackageDirs = []
    found_possible_third_party_missing_type_hints = False
    need_installed_stubs = False
    # Third-party stub/typed packages
    for pkg_dir in self.search_paths.package_path:
        stub_name = components[0] + "-stubs"
        stub_dir = os.path.join(pkg_dir, stub_name)
        if fscache.isdir(stub_dir) and self._is_compatible_stub_package(stub_dir):
            stub_typed_file = os.path.join(stub_dir, "py.typed")
            stub_components = [stub_name] + components[1:]
            path = os.path.join(pkg_dir, *stub_components[:-1])
            if fscache.isdir(path):
                if fscache.isfile(stub_typed_file):
                    # Stub packages can have a py.typed file, which must include
                    # 'partial\n' to make the package partial
                    # Partial here means that mypy should look at the runtime
                    # package if installed.
                    if fscache.read(stub_typed_file).decode().strip() == "partial":
                        runtime_path = os.path.join(pkg_dir, dir_chain)
                        third_party_inline_dirs.append((runtime_path, True))
                        # if the package is partial, we don't verify the module, as
                        # the partial stub package may not have a __init__.pyi
                        third_party_stubs_dirs.append((path, False))
                    else:
                        # handle the edge case where people put a py.typed file
                        # in a stub package, but it isn't partial
                        third_party_stubs_dirs.append((path, True))
                else:
                    third_party_stubs_dirs.append((path, True))
        non_stub_match = self._find_module_non_stub_helper(components, pkg_dir)
        if isinstance(non_stub_match, ModuleNotFoundReason):
            if non_stub_match is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
                found_possible_third_party_missing_type_hints = True
            elif non_stub_match is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
                need_installed_stubs = True
        else:
            third_party_inline_dirs.append(non_stub_match)
            self._update_ns_ancestors(components, non_stub_match)
    if self.options and self.options.use_builtins_fixtures:
        # Everything should be in fixtures.
        third_party_inline_dirs.clear()
        third_party_stubs_dirs.clear()
        found_possible_third_party_missing_type_hints = False
    python_mypy_path = self.search_paths.mypy_path + self.search_paths.python_path
    candidate_base_dirs = self.find_lib_path_dirs(id, python_mypy_path)
    if use_typeshed:
        # Search for stdlib stubs in typeshed before installed
        # stubs to avoid picking up backports (dataclasses, for
        # example) when the library is included in stdlib.
        candidate_base_dirs += self.find_lib_path_dirs(id, self.search_paths.typeshed_path)
    candidate_base_dirs += third_party_stubs_dirs + third_party_inline_dirs

    # If we're looking for a module like 'foo.bar.baz', then candidate_base_dirs now
    # contains just the subdirectories 'foo/bar' that actually exist under the
    # elements of lib_path.  This is probably much shorter than lib_path itself.
    # Now just look for 'baz.pyi', 'baz/__init__.py', etc., inside those directories.
    seplast = os.sep + components[-1]  # so e.g. '/baz'
    sepinit = os.sep + "__init__"
    near_misses = []  # Collect near misses for namespace mode (see below).
    for base_dir, verify in candidate_base_dirs:
        base_path = base_dir + seplast  # so e.g. '/usr/lib/python3.4/foo/bar/baz'
        has_init = False
        dir_prefix = base_dir
        for _ in range(len(components) - 1):
            dir_prefix = os.path.dirname(dir_prefix)
        # Prefer package over module, i.e. baz/__init__.py* over baz.py*.
        for extension in PYTHON_EXTENSIONS:
            path = base_path + sepinit + extension
            path_stubs = base_path + "-stubs" + sepinit + extension
            if fscache.isfile_case(path, dir_prefix):
                has_init = True
                if verify and not verify_module(fscache, id, path, dir_prefix):
                    near_misses.append((path, dir_prefix))
                    continue
                return path
            elif fscache.isfile_case(path_stubs, dir_prefix):
                if verify and not verify_module(fscache, id, path_stubs, dir_prefix):
                    near_misses.append((path_stubs, dir_prefix))
                    continue
                return path_stubs

        # In namespace mode, register a potential namespace package
        if self.options and self.options.namespace_packages:
            if (
                not has_init
                and fscache.exists_case(base_path, dir_prefix)
                and not fscache.isfile_case(base_path, dir_prefix)
            ):
                near_misses.append((base_path, dir_prefix))

        # No package, look for module.
        for extension in PYTHON_EXTENSIONS:
            path = base_path + extension
            if fscache.isfile_case(path, dir_prefix):
                if verify and not verify_module(fscache, id, path, dir_prefix):
                    near_misses.append((path, dir_prefix))
                    continue
                return path

    # In namespace mode, re-check those entries that had 'verify'.
    # Assume search path entries xxx, yyy and zzz, and we're
    # looking for foo.bar.baz.  Suppose near_misses has:
    #
    # - xxx/foo/bar/baz.py
    # - yyy/foo/bar/baz/__init__.py
    # - zzz/foo/bar/baz.pyi
    #
    # If any of the foo directories has __init__.py[i], it wins.
    # Else, we look for foo/bar/__init__.py[i], etc.  If there are
    # none, the first hit wins.  Note that this does not take into
    # account whether the lowest-level module is a file (baz.py),
    # a package (baz/__init__.py), or a stub file (baz.pyi) -- for
    # these the first one encountered along the search path wins.
    #
    # The helper function highest_init_level() returns an int that
    # indicates the highest level at which a __init__.py[i] file
    # is found; if no __init__ was found it returns 0, if we find
    # only foo/bar/__init__.py it returns 1, and if we have
    # foo/__init__.py it returns 2 (regardless of what's in
    # foo/bar).  It doesn't look higher than that.
    if self.options and self.options.namespace_packages and near_misses:
        levels = [
            highest_init_level(fscache, id, path, dir_prefix)
            for path, dir_prefix in near_misses
        ]
        index = levels.index(max(levels))
        return near_misses[index][0]

    # Finally, we may be asked to produce an ancestor for an
    # installed package with a py.typed marker that is a
    # subpackage of a namespace package.  We only fess up to these
    # if we would otherwise return "not found".
    ancestor = self.ns_ancestors.get(id)
    if ancestor is not None:
        return ancestor

    if need_installed_stubs:
        return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    elif found_possible_third_party_missing_type_hints:
        return ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS
    else:
        return ModuleNotFoundReason.NOT_FOUND

</t>
<t tx="ekr.20230831011820.315">def _is_compatible_stub_package(self, stub_dir: str) -&gt; bool:
    """Does a stub package support the target Python version?

    Stub packages may contain a metadata file which specifies
    whether the stubs are compatible with Python 2 and 3.
    """
    metadata_fnam = os.path.join(stub_dir, "METADATA.toml")
    if not os.path.isfile(metadata_fnam):
        return True
    with open(metadata_fnam, "rb") as f:
        metadata = tomllib.load(f)
    return bool(metadata.get("python3", True))

</t>
<t tx="ekr.20230831011820.316">def find_modules_recursive(self, module: str) -&gt; list[BuildSource]:
    module_path = self.find_module(module)
    if isinstance(module_path, ModuleNotFoundReason):
        return []
    sources = [BuildSource(module_path, module, None)]

    package_path = None
    if is_init_file(module_path):
        package_path = os.path.dirname(module_path)
    elif self.fscache.isdir(module_path):
        package_path = module_path
    if package_path is None:
        return sources

    # This logic closely mirrors that in find_sources. One small but important difference is
    # that we do not sort names with keyfunc. The recursive call to find_modules_recursive
    # calls find_module, which will handle the preference between packages, pyi and py.
    # Another difference is it doesn't handle nested search paths / package roots.

    seen: set[str] = set()
    names = sorted(self.fscache.listdir(package_path))
    for name in names:
        # Skip certain names altogether
        if name in ("__pycache__", "site-packages", "node_modules") or name.startswith("."):
            continue
        subpath = os.path.join(package_path, name)

        if self.options and matches_exclude(
            subpath, self.options.exclude, self.fscache, self.options.verbosity &gt;= 2
        ):
            continue

        if self.fscache.isdir(subpath):
            # Only recurse into packages
            if (self.options and self.options.namespace_packages) or (
                self.fscache.isfile(os.path.join(subpath, "__init__.py"))
                or self.fscache.isfile(os.path.join(subpath, "__init__.pyi"))
            ):
                seen.add(name)
                sources.extend(self.find_modules_recursive(module + "." + name))
        else:
            stem, suffix = os.path.splitext(name)
            if stem == "__init__":
                continue
            if stem not in seen and "." not in stem and suffix in PYTHON_EXTENSIONS:
                # (If we sorted names by keyfunc) we could probably just make the BuildSource
                # ourselves, but this ensures compatibility with find_module / the cache
                seen.add(stem)
                sources.extend(self.find_modules_recursive(module + "." + stem))
    return sources


</t>
<t tx="ekr.20230831011820.317">def matches_exclude(
    subpath: str, excludes: list[str], fscache: FileSystemCache, verbose: bool
) -&gt; bool:
    if not excludes:
        return False
    subpath_str = os.path.relpath(subpath).replace(os.sep, "/")
    if fscache.isdir(subpath):
        subpath_str += "/"
    for exclude in excludes:
        if re.search(exclude, subpath_str):
            if verbose:
                print(
                    f"TRACE: Excluding {subpath_str} (matches pattern {exclude})", file=sys.stderr
                )
            return True
    return False


</t>
<t tx="ekr.20230831011820.318">def is_init_file(path: str) -&gt; bool:
    return os.path.basename(path) in ("__init__.py", "__init__.pyi")


</t>
<t tx="ekr.20230831011820.319">def verify_module(fscache: FileSystemCache, id: str, path: str, prefix: str) -&gt; bool:
    """Check that all packages containing id have a __init__ file."""
    if is_init_file(path):
        path = os.path.dirname(path)
    for i in range(id.count(".")):
        path = os.path.dirname(path)
        if not any(
            fscache.isfile_case(os.path.join(path, f"__init__{extension}"), prefix)
            for extension in PYTHON_EXTENSIONS
        ):
            return False
    return True


</t>
<t tx="ekr.20230831011820.32">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    if state.strict_optional:
        if isinstance(self.s, NoneType) or (
            isinstance(self.s, Instance) and self.s.type.fullname == "builtins.object"
        ):
            return t
        else:
            return UninhabitedType()
    else:
        return t

</t>
<t tx="ekr.20230831011820.320">def highest_init_level(fscache: FileSystemCache, id: str, path: str, prefix: str) -&gt; int:
    """Compute the highest level where an __init__ file is found."""
    if is_init_file(path):
        path = os.path.dirname(path)
    level = 0
    for i in range(id.count(".")):
        path = os.path.dirname(path)
        if any(
            fscache.isfile_case(os.path.join(path, f"__init__{extension}"), prefix)
            for extension in PYTHON_EXTENSIONS
        ):
            level = i + 1
    return level


</t>
<t tx="ekr.20230831011820.321">def mypy_path() -&gt; list[str]:
    path_env = os.getenv("MYPYPATH")
    if not path_env:
        return []
    return path_env.split(os.pathsep)


</t>
<t tx="ekr.20230831011820.322">def default_lib_path(
    data_dir: str, pyversion: tuple[int, int], custom_typeshed_dir: str | None
) -&gt; list[str]:
    """Return default standard library search paths."""
    path: list[str] = []

    if custom_typeshed_dir:
        typeshed_dir = os.path.join(custom_typeshed_dir, "stdlib")
        mypy_extensions_dir = os.path.join(custom_typeshed_dir, "stubs", "mypy-extensions")
        versions_file = os.path.join(typeshed_dir, "VERSIONS")
        if not os.path.isdir(typeshed_dir) or not os.path.isfile(versions_file):
            print(
                "error: --custom-typeshed-dir does not point to a valid typeshed ({})".format(
                    custom_typeshed_dir
                )
            )
            sys.exit(2)
    else:
        auto = os.path.join(data_dir, "stubs-auto")
        if os.path.isdir(auto):
            data_dir = auto
        typeshed_dir = os.path.join(data_dir, "typeshed", "stdlib")
        mypy_extensions_dir = os.path.join(data_dir, "typeshed", "stubs", "mypy-extensions")
    path.append(typeshed_dir)

    # Get mypy-extensions stubs from typeshed, since we treat it as an
    # "internal" library, similar to typing and typing-extensions.
    path.append(mypy_extensions_dir)

    # Add fallback path that can be used if we have a broken installation.
    if sys.platform != "win32":
        path.append("/usr/local/lib/mypy")
    if not path:
        print(
            "Could not resolve typeshed subdirectories. Your mypy install is broken.\n"
            "Python executable is located at {}.\nMypy located at {}".format(
                sys.executable, data_dir
            ),
            file=sys.stderr,
        )
        sys.exit(1)
    return path


</t>
<t tx="ekr.20230831011820.323">@functools.lru_cache(maxsize=None)
def get_search_dirs(python_executable: str | None) -&gt; tuple[list[str], list[str]]:
    """Find package directories for given python.

    This runs a subprocess call, which generates a list of the directories in sys.path.
    To avoid repeatedly calling a subprocess (which can be slow!) we
    lru_cache the results.
    """

    if python_executable is None:
        return ([], [])
    elif python_executable == sys.executable:
        # Use running Python's package dirs
        sys_path, site_packages = pyinfo.getsearchdirs()
    else:
        # Use subprocess to get the package directory of given Python
        # executable
        env = {**dict(os.environ), "PYTHONSAFEPATH": "1"}
        try:
            sys_path, site_packages = ast.literal_eval(
                subprocess.check_output(
                    [python_executable, pyinfo.__file__, "getsearchdirs"],
                    env=env,
                    stderr=subprocess.PIPE,
                ).decode()
            )
        except subprocess.CalledProcessError as err:
            print(err.stderr)
            print(err.stdout)
            raise
        except OSError as err:
            reason = os.strerror(err.errno)
            raise CompileError(
                [f"mypy: Invalid python executable '{python_executable}': {reason}"]
            ) from err
    return sys_path, site_packages


</t>
<t tx="ekr.20230831011820.324">def compute_search_paths(
    sources: list[BuildSource], options: Options, data_dir: str, alt_lib_path: str | None = None
) -&gt; SearchPaths:
    """Compute the search paths as specified in PEP 561.

    There are the following 4 members created:
    - User code (from `sources`)
    - MYPYPATH (set either via config or environment variable)
    - installed package directories (which will later be split into stub-only and inline)
    - typeshed
    """
    # Determine the default module search path.
    lib_path = collections.deque(
        default_lib_path(
            data_dir, options.python_version, custom_typeshed_dir=options.custom_typeshed_dir
        )
    )

    if options.use_builtins_fixtures:
        # Use stub builtins (to speed up test cases and to make them easier to
        # debug).  This is a test-only feature, so assume our files are laid out
        # as in the source tree.
        # We also need to allow overriding where to look for it. Argh.
        root_dir = os.getenv("MYPY_TEST_PREFIX", None)
        if not root_dir:
            root_dir = os.path.dirname(os.path.dirname(__file__))
        lib_path.appendleft(os.path.join(root_dir, "test-data", "unit", "lib-stub"))
    # alt_lib_path is used by some tests to bypass the normal lib_path mechanics.
    # If we don't have one, grab directories of source files.
    python_path: list[str] = []
    if not alt_lib_path:
        for source in sources:
            # Include directory of the program file in the module search path.
            if source.base_dir:
                dir = source.base_dir
                if dir not in python_path:
                    python_path.append(dir)

        # Do this even if running as a file, for sanity (mainly because with
        # multiple builds, there could be a mix of files/modules, so its easier
        # to just define the semantics that we always add the current director
        # to the lib_path
        # TODO: Don't do this in some cases; for motivation see see
        # https://github.com/python/mypy/issues/4195#issuecomment-341915031
        if options.bazel:
            dir = "."
        else:
            dir = os.getcwd()
        if dir not in lib_path:
            python_path.insert(0, dir)

    # Start with a MYPYPATH environment variable at the front of the mypy_path, if defined.
    mypypath = mypy_path()

    # Add a config-defined mypy path.
    mypypath.extend(options.mypy_path)

    # If provided, insert the caller-supplied extra module path to the
    # beginning (highest priority) of the search path.
    if alt_lib_path:
        mypypath.insert(0, alt_lib_path)

    sys_path, site_packages = get_search_dirs(options.python_executable)
    # We only use site packages for this check
    for site in site_packages:
        assert site not in lib_path
        if (
            site in mypypath
            or any(p.startswith(site + os.path.sep) for p in mypypath)
            or (os.path.altsep and any(p.startswith(site + os.path.altsep) for p in mypypath))
        ):
            print(f"{site} is in the MYPYPATH. Please remove it.", file=sys.stderr)
            print(
                "See https://mypy.readthedocs.io/en/stable/running_mypy.html"
                "#how-mypy-handles-imports for more info",
                file=sys.stderr,
            )
            sys.exit(1)

    return SearchPaths(
        python_path=tuple(reversed(python_path)),
        mypy_path=tuple(mypypath),
        package_path=tuple(sys_path + site_packages),
        typeshed_path=tuple(lib_path),
    )


</t>
<t tx="ekr.20230831011820.325">def load_stdlib_py_versions(custom_typeshed_dir: str | None) -&gt; StdlibVersions:
    """Return dict with minimum and maximum Python versions of stdlib modules.

    The contents look like
    {..., 'secrets': ((3, 6), None), 'symbol': ((2, 7), (3, 9)), ...}

    None means there is no maximum version.
    """
    typeshed_dir = custom_typeshed_dir or os.path.join(os.path.dirname(__file__), "typeshed")
    stdlib_dir = os.path.join(typeshed_dir, "stdlib")
    result = {}

    versions_path = os.path.join(stdlib_dir, "VERSIONS")
    assert os.path.isfile(versions_path), (custom_typeshed_dir, versions_path, __file__)
    with open(versions_path) as f:
        for line in f:
            line = line.split("#")[0].strip()
            if line == "":
                continue
            module, version_range = line.split(":")
            versions = version_range.split("-")
            min_version = parse_version(versions[0])
            max_version = (
                parse_version(versions[1]) if len(versions) &gt;= 2 and versions[1].strip() else None
            )
            result[module] = min_version, max_version
    return result


</t>
<t tx="ekr.20230831011820.326">def parse_version(version: str) -&gt; tuple[int, int]:
    major, minor = version.strip().split(".")
    return int(major), int(minor)


</t>
<t tx="ekr.20230831011820.327">def typeshed_py_version(options: Options) -&gt; tuple[int, int]:
    """Return Python version used for checking whether module supports typeshed."""
    # Typeshed no longer covers Python 3.x versions before 3.7, so 3.7 is
    # the earliest we can support.
    return max(options.python_version, (3, 7))
</t>
<t tx="ekr.20230831011820.328">@path mypy
"""Basic introspection of modules."""
&lt;&lt; moduleinspect.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.329">
from __future__ import annotations

import importlib
import inspect
import os
import pkgutil
import queue
import sys
from multiprocessing import Process, Queue
from types import ModuleType


</t>
<t tx="ekr.20230831011820.33">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20230831011820.330">class ModuleProperties:
    @others
</t>
<t tx="ekr.20230831011820.331"># Note that all __init__ args must have default values
def __init__(
    self,
    name: str = "",
    file: str | None = None,
    path: list[str] | None = None,
    all: list[str] | None = None,
    is_c_module: bool = False,
    subpackages: list[str] | None = None,
) -&gt; None:
    self.name = name  # __name__ attribute
    self.file = file  # __file__ attribute
    self.path = path  # __path__ attribute
    self.all = all  # __all__ attribute
    self.is_c_module = is_c_module
    self.subpackages = subpackages or []


</t>
<t tx="ekr.20230831011820.332">def is_c_module(module: ModuleType) -&gt; bool:
    if module.__dict__.get("__file__") is None:
        # Could be a namespace package. These must be handled through
        # introspection, since there is no source file.
        return True
    return os.path.splitext(module.__dict__["__file__"])[-1] in [".so", ".pyd", ".dll"]


</t>
<t tx="ekr.20230831011820.333">class InspectError(Exception):
    pass


</t>
<t tx="ekr.20230831011820.334">def get_package_properties(package_id: str) -&gt; ModuleProperties:
    """Use runtime introspection to get information about a module/package."""
    try:
        package = importlib.import_module(package_id)
    except BaseException as e:
        raise InspectError(str(e)) from e
    name = getattr(package, "__name__", package_id)
    file = getattr(package, "__file__", None)
    path: list[str] | None = getattr(package, "__path__", None)
    if not isinstance(path, list):
        path = None
    pkg_all = getattr(package, "__all__", None)
    if pkg_all is not None:
        try:
            pkg_all = list(pkg_all)
        except Exception:
            pkg_all = None
    is_c = is_c_module(package)

    if path is None:
        # Object has no path; this means it's either a module inside a package
        # (and thus no sub-packages), or it could be a C extension package.
        if is_c:
            # This is a C extension module, now get the list of all sub-packages
            # using the inspect module
            subpackages = [
                package.__name__ + "." + name
                for name, val in inspect.getmembers(package)
                if inspect.ismodule(val) and val.__name__ == package.__name__ + "." + name
            ]
        else:
            # It's a module inside a package.  There's nothing else to walk/yield.
            subpackages = []
    else:
        all_packages = pkgutil.walk_packages(
            path, prefix=package.__name__ + ".", onerror=lambda r: None
        )
        subpackages = [qualified_name for importer, qualified_name, ispkg in all_packages]
    return ModuleProperties(
        name=name, file=file, path=path, all=pkg_all, is_c_module=is_c, subpackages=subpackages
    )


</t>
<t tx="ekr.20230831011820.335">def worker(tasks: Queue[str], results: Queue[str | ModuleProperties], sys_path: list[str]) -&gt; None:
    """The main loop of a worker introspection process."""
    sys.path = sys_path
    while True:
        mod = tasks.get()
        try:
            prop = get_package_properties(mod)
        except InspectError as e:
            results.put(str(e))
            continue
        results.put(prop)


</t>
<t tx="ekr.20230831011820.336">class ModuleInspect:
    """Perform runtime introspection of modules in a separate process.

    Reuse the process for multiple modules for efficiency. However, if there is an
    error, retry using a fresh process to avoid cross-contamination of state between
    modules.

    We use a separate process to isolate us from many side effects. For example, the
    import of a module may kill the current process, and we want to recover from that.

    Always use in a with statement for proper clean-up:

      with ModuleInspect() as m:
          p = m.get_package_properties('urllib.parse')
    """

    @others
</t>
<t tx="ekr.20230831011820.337">def __init__(self) -&gt; None:
    self._start()

</t>
<t tx="ekr.20230831011820.338">def _start(self) -&gt; None:
    self.tasks: Queue[str] = Queue()
    self.results: Queue[ModuleProperties | str] = Queue()
    self.proc = Process(target=worker, args=(self.tasks, self.results, sys.path))
    self.proc.start()
    self.counter = 0  # Number of successful roundtrips

</t>
<t tx="ekr.20230831011820.339">def close(self) -&gt; None:
    """Free any resources used."""
    self.proc.terminate()

</t>
<t tx="ekr.20230831011820.34">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    if isinstance(self.s, NoneType):
        if state.strict_optional:
            return t
        else:
            return self.s
    elif isinstance(self.s, UninhabitedType):
        return self.s
    else:
        return t

</t>
<t tx="ekr.20230831011820.340">def get_package_properties(self, package_id: str) -&gt; ModuleProperties:
    """Return some properties of a module/package using runtime introspection.

    Raise InspectError if the target couldn't be imported.
    """
    self.tasks.put(package_id)
    res = self._get_from_queue()
    if res is None:
        # The process died; recover and report error.
        self._start()
        raise InspectError(f"Process died when importing {package_id!r}")
    if isinstance(res, str):
        # Error importing module
        if self.counter &gt; 0:
            # Also try with a fresh process. Maybe one of the previous imports has
            # corrupted some global state.
            self.close()
            self._start()
            return self.get_package_properties(package_id)
        raise InspectError(res)
    self.counter += 1
    return res

</t>
<t tx="ekr.20230831011820.341">def _get_from_queue(self) -&gt; ModuleProperties | str | None:
    """Get value from the queue.

    Return the value read from the queue, or None if the process unexpectedly died.
    """
    max_iter = 600
    n = 0
    while True:
        if n == max_iter:
            raise RuntimeError("Timeout waiting for subprocess")
        try:
            return self.results.get(timeout=0.05)
        except queue.Empty:
            if not self.proc.is_alive():
                return None
        n += 1

</t>
<t tx="ekr.20230831011820.342">def __enter__(self) -&gt; ModuleInspect:
    return self

</t>
<t tx="ekr.20230831011820.343">def __exit__(self, *args: object) -&gt; None:
    self.close()
</t>
<t tx="ekr.20230831011820.344">@path mypy
&lt;&lt; mro.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.345">from __future__ import annotations

from typing import Callable

from mypy.nodes import TypeInfo
from mypy.types import Instance
from mypy.typestate import type_state


</t>
<t tx="ekr.20230831011820.346">def calculate_mro(info: TypeInfo, obj_type: Callable[[], Instance] | None = None) -&gt; None:
    """Calculate and set mro (method resolution order).

    Raise MroError if cannot determine mro.
    """
    mro = linearize_hierarchy(info, obj_type)
    assert mro, f"Could not produce a MRO at all for {info}"
    info.mro = mro
    # The property of falling back to Any is inherited.
    info.fallback_to_any = any(baseinfo.fallback_to_any for baseinfo in info.mro)
    type_state.reset_all_subtype_caches_for(info)


</t>
<t tx="ekr.20230831011820.347">class MroError(Exception):
    """Raised if a consistent mro cannot be determined for a class."""


</t>
<t tx="ekr.20230831011820.348">def linearize_hierarchy(
    info: TypeInfo, obj_type: Callable[[], Instance] | None = None
) -&gt; list[TypeInfo]:
    # TODO describe
    if info.mro:
        return info.mro
    bases = info.direct_base_classes()
    if not bases and info.fullname != "builtins.object" and obj_type is not None:
        # Probably an error, add a dummy `object` base class,
        # otherwise MRO calculation may spuriously fail.
        bases = [obj_type().type]
    lin_bases = []
    for base in bases:
        assert base is not None, f"Cannot linearize bases for {info.fullname} {bases}"
        lin_bases.append(linearize_hierarchy(base, obj_type))
    lin_bases.append(bases)
    return [info] + merge(lin_bases)


</t>
<t tx="ekr.20230831011820.349">def merge(seqs: list[list[TypeInfo]]) -&gt; list[TypeInfo]:
    seqs = [s.copy() for s in seqs]
    result: list[TypeInfo] = []
    while True:
        seqs = [s for s in seqs if s]
        if not seqs:
            return result
        for seq in seqs:
            head = seq[0]
            if not [s for s in seqs if head in s[1:]]:
                break
        else:
            raise MroError()
        result.append(head)
        for s in seqs:
            if s[0] is head:
                del s[0]
</t>
<t tx="ekr.20230831011820.35">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20230831011820.350">@path mypy
"""Abstract syntax tree node classes (i.e. parse tree)."""
&lt;&lt; nodes.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.351">
from __future__ import annotations

import os
from abc import abstractmethod
from collections import defaultdict
from enum import Enum, unique
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Final,
    Iterator,
    List,
    Optional,
    Sequence,
    Tuple,
    TypeVar,
    Union,
    cast,
)
from typing_extensions import TypeAlias as _TypeAlias, TypeGuard

from mypy_extensions import trait

import mypy.strconv
from mypy.options import Options
from mypy.util import short_type
from mypy.visitor import ExpressionVisitor, NodeVisitor, StatementVisitor

if TYPE_CHECKING:
    from mypy.patterns import Pattern


</t>
<t tx="ekr.20230831011820.352">class Context:
    """Base type for objects that are valid as error message locations."""

    @others
</t>
<t tx="ekr.20230831011820.353">__slots__ = ("line", "column", "end_line", "end_column")

def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    self.line = line
    self.column = column
    self.end_line: int | None = None
    self.end_column: int | None = None

</t>
<t tx="ekr.20230831011820.354">def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    """If target is a node, pull line (and column) information
    into this node. If column is specified, this will override any column
    information coming from a node.
    """
    if isinstance(target, int):
        self.line = target
    else:
        self.line = target.line
        self.column = target.column
        self.end_line = target.end_line
        self.end_column = target.end_column

    if column is not None:
        self.column = column

    if end_line is not None:
        self.end_line = end_line

    if end_column is not None:
        self.end_column = end_column


</t>
<t tx="ekr.20230831011820.355">if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.types


T = TypeVar("T")

JsonDict: _TypeAlias = Dict[str, Any]


# Symbol table node kinds
#
# TODO rename to use more descriptive names

LDEF: Final = 0
GDEF: Final = 1
MDEF: Final = 2

# Placeholder for a name imported via 'from ... import'. Second phase of
# semantic will replace this the actual imported reference. This is
# needed so that we can detect whether a name has been imported during
# XXX what?
UNBOUND_IMPORTED: Final = 3

# RevealExpr node kinds
REVEAL_TYPE: Final = 0
REVEAL_LOCALS: Final = 1

LITERAL_YES: Final = 2
LITERAL_TYPE: Final = 1
LITERAL_NO: Final = 0

node_kinds: Final = {LDEF: "Ldef", GDEF: "Gdef", MDEF: "Mdef", UNBOUND_IMPORTED: "UnboundImported"}
inverse_node_kinds: Final = {_kind: _name for _name, _kind in node_kinds.items()}


implicit_module_attrs: Final = {
    "__name__": "__builtins__.str",
    "__doc__": None,  # depends on Python version, see semanal.py
    "__path__": None,  # depends on if the module is a package
    "__file__": "__builtins__.str",
    "__package__": "__builtins__.str",
    "__annotations__": None,  # dict[str, Any] bounded in add_implicit_module_attrs()
}


# These aliases exist because built-in class objects are not subscriptable.
# For example `list[int]` fails at runtime. Instead List[int] should be used.
type_aliases: Final = {
    "typing.List": "builtins.list",
    "typing.Dict": "builtins.dict",
    "typing.Set": "builtins.set",
    "typing.FrozenSet": "builtins.frozenset",
    "typing.ChainMap": "collections.ChainMap",
    "typing.Counter": "collections.Counter",
    "typing.DefaultDict": "collections.defaultdict",
    "typing.Deque": "collections.deque",
    "typing.OrderedDict": "collections.OrderedDict",
    # HACK: a lie in lieu of actual support for PEP 675
    "typing.LiteralString": "builtins.str",
}

# This keeps track of the oldest supported Python version where the corresponding
# alias source is available.
type_aliases_source_versions: Final = {
    "typing.List": (2, 7),
    "typing.Dict": (2, 7),
    "typing.Set": (2, 7),
    "typing.FrozenSet": (2, 7),
    "typing.ChainMap": (3, 3),
    "typing.Counter": (2, 7),
    "typing.DefaultDict": (2, 7),
    "typing.Deque": (2, 7),
    "typing.OrderedDict": (3, 7),
    "typing.LiteralString": (3, 11),
}

# This keeps track of aliases in `typing_extensions`, which we treat specially.
typing_extensions_aliases: Final = {
    # See: https://github.com/python/mypy/issues/11528
    "typing_extensions.OrderedDict": "collections.OrderedDict",
    # HACK: a lie in lieu of actual support for PEP 675
    "typing_extensions.LiteralString": "builtins.str",
}

reverse_builtin_aliases: Final = {
    "builtins.list": "typing.List",
    "builtins.dict": "typing.Dict",
    "builtins.set": "typing.Set",
    "builtins.frozenset": "typing.FrozenSet",
}

_nongen_builtins: Final = {"builtins.tuple": "typing.Tuple", "builtins.enumerate": ""}
_nongen_builtins.update((name, alias) for alias, name in type_aliases.items())
# Drop OrderedDict from this for backward compatibility
del _nongen_builtins["collections.OrderedDict"]
# HACK: consequence of hackily treating LiteralString as an alias for str
del _nongen_builtins["builtins.str"]


def get_nongen_builtins(python_version: tuple[int, int]) -&gt; dict[str, str]:
    # After 3.9 with pep585 generic builtins are allowed
    return _nongen_builtins if python_version &lt; (3, 9) else {}


</t>
<t tx="ekr.20230831011820.356">RUNTIME_PROTOCOL_DECOS: Final = (
    "typing.runtime_checkable",
    "typing_extensions.runtime",
    "typing_extensions.runtime_checkable",
)


class Node(Context):
    """Common base class for all non-type parse tree nodes."""

    @others
</t>
<t tx="ekr.20230831011820.357">__slots__ = ()

def __str__(self) -&gt; str:
    ans = self.accept(mypy.strconv.StrConv(options=Options()))
    if ans is None:
        return repr(self)
    return ans

</t>
<t tx="ekr.20230831011820.358">def str_with_options(self, options: Options) -&gt; str:
    ans = self.accept(mypy.strconv.StrConv(options=options))
    assert ans
    return ans

</t>
<t tx="ekr.20230831011820.359">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented", type(self))


</t>
<t tx="ekr.20230831011820.36">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    if isinstance(self.s, TypeVarType) and self.s.id == t.id:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011820.360">@trait
class Statement(Node):
    """A statement node."""

    @others
</t>
<t tx="ekr.20230831011820.361">__slots__ = ()

def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented", type(self))


</t>
<t tx="ekr.20230831011820.362">@trait
class Expression(Node):
    """An expression node."""

    @others
</t>
<t tx="ekr.20230831011820.363">__slots__ = ()

def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented", type(self))


</t>
<t tx="ekr.20230831011820.364">class FakeExpression(Expression):
    """A dummy expression.

    We need a dummy expression in one place, and can't instantiate Expression
    because it is a trait and mypyc barfs.
    """

    __slots__ = ()


</t>
<t tx="ekr.20230831011820.365"># TODO:
# Lvalue = Union['NameExpr', 'MemberExpr', 'IndexExpr', 'SuperExpr', 'StarExpr'
#                'TupleExpr']; see #1783.
Lvalue: _TypeAlias = Expression


@trait
class SymbolNode(Node):
    """Nodes that can be stored in a symbol table."""

    @others
</t>
<t tx="ekr.20230831011820.366">__slots__ = ()

@property
@abstractmethod
def name(self) -&gt; str:
    pass

</t>
<t tx="ekr.20230831011820.367"># Fully qualified name
@property
@abstractmethod
def fullname(self) -&gt; str:
    pass

</t>
<t tx="ekr.20230831011820.368">@abstractmethod
def serialize(self) -&gt; JsonDict:
    pass

</t>
<t tx="ekr.20230831011820.369">@classmethod
def deserialize(cls, data: JsonDict) -&gt; SymbolNode:
    classname = data[".class"]
    method = deserialize_map.get(classname)
    if method is not None:
        return method(data)
    raise NotImplementedError(f"unexpected .class {classname}")


</t>
<t tx="ekr.20230831011820.37">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    if self.s == t:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011820.370"># Items: fullname, related symbol table node, surrounding type (if any)
Definition: _TypeAlias = Tuple[str, "SymbolTableNode", Optional["TypeInfo"]]


class MypyFile(SymbolNode):
    """The abstract syntax tree of a single source file."""

    @others
</t>
<t tx="ekr.20230831011820.371">__slots__ = (
    "_fullname",
    "path",
    "defs",
    "alias_deps",
    "is_bom",
    "names",
    "imports",
    "ignored_lines",
    "skipped_lines",
    "is_stub",
    "is_cache_skeleton",
    "is_partial_stub_package",
    "plugin_deps",
    "future_import_flags",
)

__match_args__ = ("name", "path", "defs")

# Fully qualified module name
_fullname: str
# Path to the file (empty string if not known)
path: str
# Top-level definitions and statements
defs: list[Statement]
# Type alias dependencies as mapping from target to set of alias full names
alias_deps: defaultdict[str, set[str]]
# Is there a UTF-8 BOM at the start?
is_bom: bool
names: SymbolTable
# All import nodes within the file (also ones within functions etc.)
imports: list[ImportBase]
# Lines on which to ignore certain errors when checking.
# If the value is empty, ignore all errors; otherwise, the list contains all
# error codes to ignore.
ignored_lines: dict[int, list[str]]
# Lines that were skipped during semantic analysis e.g. due to ALWAYS_FALSE, MYPY_FALSE,
# or platform/version checks. Those lines would not be type-checked.
skipped_lines: set[int]
# Is this file represented by a stub file (.pyi)?
is_stub: bool
# Is this loaded from the cache and thus missing the actual body of the file?
is_cache_skeleton: bool
# Does this represent an __init__.pyi stub with a module __getattr__
# (i.e. a partial stub package), for such packages we suppress any missing
# module errors in addition to missing attribute errors.
is_partial_stub_package: bool
# Plugin-created dependencies
plugin_deps: dict[str, set[str]]
# Future imports defined in this file. Populated during semantic analysis.
future_import_flags: set[str]

def __init__(
    self,
    defs: list[Statement],
    imports: list[ImportBase],
    is_bom: bool = False,
    ignored_lines: dict[int, list[str]] | None = None,
) -&gt; None:
    super().__init__()
    self.defs = defs
    self.line = 1  # Dummy line number
    self.column = 0  # Dummy column
    self.imports = imports
    self.is_bom = is_bom
    self.alias_deps = defaultdict(set)
    self.plugin_deps = {}
    if ignored_lines:
        self.ignored_lines = ignored_lines
    else:
        self.ignored_lines = {}
    self.skipped_lines = set()

    self.path = ""
    self.is_stub = False
    self.is_cache_skeleton = False
    self.is_partial_stub_package = False
    self.future_import_flags = set()

</t>
<t tx="ekr.20230831011820.372">def local_definitions(self) -&gt; Iterator[Definition]:
    """Return all definitions within the module (including nested).

    This doesn't include imported definitions.
    """
    return local_definitions(self.names, self.fullname)

</t>
<t tx="ekr.20230831011820.373">@property
def name(self) -&gt; str:
    return "" if not self._fullname else self._fullname.split(".")[-1]

</t>
<t tx="ekr.20230831011820.374">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20230831011820.375">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_mypy_file(self)

</t>
<t tx="ekr.20230831011820.376">def is_package_init_file(self) -&gt; bool:
    return len(self.path) != 0 and os.path.basename(self.path).startswith("__init__.")

</t>
<t tx="ekr.20230831011820.377">def is_future_flag_set(self, flag: str) -&gt; bool:
    return flag in self.future_import_flags

</t>
<t tx="ekr.20230831011820.378">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "MypyFile",
        "_fullname": self._fullname,
        "names": self.names.serialize(self._fullname),
        "is_stub": self.is_stub,
        "path": self.path,
        "is_partial_stub_package": self.is_partial_stub_package,
        "future_import_flags": list(self.future_import_flags),
    }

</t>
<t tx="ekr.20230831011820.379">@classmethod
def deserialize(cls, data: JsonDict) -&gt; MypyFile:
    assert data[".class"] == "MypyFile", data
    tree = MypyFile([], [])
    tree._fullname = data["_fullname"]
    tree.names = SymbolTable.deserialize(data["names"])
    tree.is_stub = data["is_stub"]
    tree.path = data["path"]
    tree.is_partial_stub_package = data["is_partial_stub_package"]
    tree.is_cache_skeleton = True
    tree.future_import_flags = set(data["future_import_flags"])
    return tree


</t>
<t tx="ekr.20230831011820.38">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    if self.s == t:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011820.380">class ImportBase(Statement):
    """Base class for all import statements."""

    @others
</t>
<t tx="ekr.20230831011820.381">__slots__ = ("is_unreachable", "is_top_level", "is_mypy_only", "assignments")

is_unreachable: bool  # Set by semanal.SemanticAnalyzerPass1 if inside `if False` etc.
is_top_level: bool  # Ditto if outside any class or def
is_mypy_only: bool  # Ditto if inside `if TYPE_CHECKING` or `if MYPY`

# If an import replaces existing definitions, we construct dummy assignment
# statements that assign the imported names to the names in the current scope,
# for type checking purposes. Example:
#
#     x = 1
#     from m import x   &lt;-- add assignment representing "x = m.x"
assignments: list[AssignmentStmt]

def __init__(self) -&gt; None:
    super().__init__()
    self.assignments = []
    self.is_unreachable = False
    self.is_top_level = False
    self.is_mypy_only = False


</t>
<t tx="ekr.20230831011820.382">class Import(ImportBase):
    """import m [as n]"""

    @others
</t>
<t tx="ekr.20230831011820.383">__slots__ = ("ids",)

__match_args__ = ("ids",)

ids: list[tuple[str, str | None]]  # (module id, as id)

def __init__(self, ids: list[tuple[str, str | None]]) -&gt; None:
    super().__init__()
    self.ids = ids

</t>
<t tx="ekr.20230831011820.384">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import(self)


</t>
<t tx="ekr.20230831011820.385">class ImportFrom(ImportBase):
    """from m import x [as y], ..."""

    @others
</t>
<t tx="ekr.20230831011820.386">__slots__ = ("id", "names", "relative")

__match_args__ = ("id", "names", "relative")

id: str
relative: int
names: list[tuple[str, str | None]]  # Tuples (name, as name)

def __init__(self, id: str, relative: int, names: list[tuple[str, str | None]]) -&gt; None:
    super().__init__()
    self.id = id
    self.names = names
    self.relative = relative

</t>
<t tx="ekr.20230831011820.387">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import_from(self)


</t>
<t tx="ekr.20230831011820.388">class ImportAll(ImportBase):
    """from m import *"""

    @others
</t>
<t tx="ekr.20230831011820.389">__slots__ = ("id", "relative")

__match_args__ = ("id", "relative")

id: str
relative: int

def __init__(self, id: str, relative: int) -&gt; None:
    super().__init__()
    self.id = id
    self.relative = relative

</t>
<t tx="ekr.20230831011820.39">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.390">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import_all(self)


</t>
<t tx="ekr.20230831011820.391">FUNCBASE_FLAGS: Final = ["is_property", "is_class", "is_static", "is_final"]


class FuncBase(Node):
    """Abstract base class for function-like nodes.

    N.B: Although this has SymbolNode subclasses (FuncDef,
    OverloadedFuncDef), avoid calling isinstance(..., FuncBase) on
    something that is typed as SymbolNode.  This is to work around
    mypy bug #3603, in which mypy doesn't understand multiple
    inheritance very well, and will assume that a SymbolNode
    cannot be a FuncBase.

    Instead, test against SYMBOL_FUNCBASE_TYPES, which enumerates
    SymbolNode subclasses that are also FuncBase subclasses.
    """

    @others
</t>
<t tx="ekr.20230831011820.392">__slots__ = (
    "type",
    "unanalyzed_type",
    "info",
    "is_property",
    "is_class",  # Uses "@classmethod" (explicit or implicit)
    "is_static",  # Uses "@staticmethod" (explicit or implicit)
    "is_final",  # Uses "@final"
    "is_explicit_override",  # Uses "@override"
    "_fullname",
)

def __init__(self) -&gt; None:
    super().__init__()
    # Type signature. This is usually CallableType or Overloaded, but it can be
    # something else for decorated functions.
    self.type: mypy.types.ProperType | None = None
    # Original, not semantically analyzed type (used for reprocessing)
    self.unanalyzed_type: mypy.types.ProperType | None = None
    # If method, reference to TypeInfo
    # TODO: Type should be Optional[TypeInfo]
    self.info = FUNC_NO_INFO
    self.is_property = False
    self.is_class = False
    self.is_static = False
    self.is_final = False
    self.is_explicit_override = False
    # Name with module prefix
    self._fullname = ""

</t>
<t tx="ekr.20230831011820.393">@property
@abstractmethod
def name(self) -&gt; str:
    pass

</t>
<t tx="ekr.20230831011820.394">@property
def fullname(self) -&gt; str:
    return self._fullname


</t>
<t tx="ekr.20230831011820.395">OverloadPart: _TypeAlias = Union["FuncDef", "Decorator"]


class OverloadedFuncDef(FuncBase, SymbolNode, Statement):
    """A logical node representing all the variants of a multi-declaration function.

    A multi-declaration function is often an @overload, but can also be a
    @property with a setter and a/or a deleter.

    This node has no explicit representation in the source program.
    Overloaded variants must be consecutive in the source file.
    """

    @others
</t>
<t tx="ekr.20230831011820.396">__slots__ = ("items", "unanalyzed_items", "impl")

items: list[OverloadPart]
unanalyzed_items: list[OverloadPart]
impl: OverloadPart | None

def __init__(self, items: list[OverloadPart]) -&gt; None:
    super().__init__()
    self.items = items
    self.unanalyzed_items = items.copy()
    self.impl = None
    if items:
        # TODO: figure out how to reliably set end position (we don't know the impl here).
        self.set_line(items[0].line, items[0].column)
    self.is_final = False

</t>
<t tx="ekr.20230831011820.397">@property
def name(self) -&gt; str:
    if self.items:
        return self.items[0].name
    else:
        # This may happen for malformed overload
        assert self.impl is not None
        return self.impl.name

</t>
<t tx="ekr.20230831011820.398">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_overloaded_func_def(self)

</t>
<t tx="ekr.20230831011820.399">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "OverloadedFuncDef",
        "items": [i.serialize() for i in self.items],
        "type": None if self.type is None else self.type.serialize(),
        "fullname": self._fullname,
        "impl": None if self.impl is None else self.impl.serialize(),
        "flags": get_flags(self, FUNCBASE_FLAGS),
    }

</t>
<t tx="ekr.20230831011820.4">def read_types_packages_to_install(cache_dir: str, after_run: bool) -&gt; list[str]:
    if not os.path.isdir(cache_dir):
        if not after_run:
            sys.stderr.write(
                "error: Can't determine which types to install with no files to check "
                + "(and no cache from previous mypy run)\n"
            )
        else:
            sys.stderr.write("error: --install-types failed (no mypy cache directory)\n")
        sys.exit(2)
    fnam = build.missing_stubs_file(cache_dir)
    if not os.path.isfile(fnam):
        # No missing stubs.
        return []
    with open(fnam) as f:
        return [line.strip() for line in f]


</t>
<t tx="ekr.20230831011820.40">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    if isinstance(self.s, Parameters):
        if len(t.arg_types) != len(self.s.arg_types):
            return self.default(self.s)
        return t.copy_modified(
            # Note that since during constraint inference we already treat whole ParamSpec as
            # contravariant, we should meet individual items, not join them like for Callables
            arg_types=[meet_types(s_a, t_a) for s_a, t_a in zip(self.s.arg_types, t.arg_types)]
        )
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011820.400">@classmethod
def deserialize(cls, data: JsonDict) -&gt; OverloadedFuncDef:
    assert data[".class"] == "OverloadedFuncDef"
    res = OverloadedFuncDef(
        [cast(OverloadPart, SymbolNode.deserialize(d)) for d in data["items"]]
    )
    if data.get("impl") is not None:
        res.impl = cast(OverloadPart, SymbolNode.deserialize(data["impl"]))
        # set line for empty overload items, as not set in __init__
        if len(res.items) &gt; 0:
            res.set_line(res.impl.line)
    if data.get("type") is not None:
        typ = mypy.types.deserialize_type(data["type"])
        assert isinstance(typ, mypy.types.ProperType)
        res.type = typ
    res._fullname = data["fullname"]
    set_flags(res, data["flags"])
    # NOTE: res.info will be set in the fixup phase.
    return res


</t>
<t tx="ekr.20230831011820.401">class Argument(Node):
    """A single argument in a FuncItem."""

    @others
</t>
<t tx="ekr.20230831011820.402">__slots__ = ("variable", "type_annotation", "initializer", "kind", "pos_only")

__match_args__ = ("variable", "type_annotation", "initializer", "kind", "pos_only")

def __init__(
    self,
    variable: Var,
    type_annotation: mypy.types.Type | None,
    initializer: Expression | None,
    kind: ArgKind,
    pos_only: bool = False,
) -&gt; None:
    super().__init__()
    self.variable = variable
    self.type_annotation = type_annotation
    self.initializer = initializer
    self.kind = kind  # must be an ARG_* constant
    self.pos_only = pos_only

</t>
<t tx="ekr.20230831011820.403">def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    super().set_line(target, column, end_line, end_column)

    if self.initializer and self.initializer.line &lt; 0:
        self.initializer.set_line(self.line, self.column, self.end_line, self.end_column)

    self.variable.set_line(self.line, self.column, self.end_line, self.end_column)


</t>
<t tx="ekr.20230831011820.404">FUNCITEM_FLAGS: Final = FUNCBASE_FLAGS + [
    "is_overload",
    "is_generator",
    "is_coroutine",
    "is_async_generator",
    "is_awaitable_coroutine",
]


class FuncItem(FuncBase):
    """Base class for nodes usable as overloaded function items."""

    @others
</t>
<t tx="ekr.20230831011820.405">__slots__ = (
    "arguments",  # Note that can be unset if deserialized (type is a lie!)
    "arg_names",  # Names of arguments
    "arg_kinds",  # Kinds of arguments
    "min_args",  # Minimum number of arguments
    "max_pos",  # Maximum number of positional arguments, -1 if no explicit
    # limit (*args not included)
    "body",  # Body of the function
    "is_overload",  # Is this an overload variant of function with more than
    # one overload variant?
    "is_generator",  # Contains a yield statement?
    "is_coroutine",  # Defined using 'async def' syntax?
    "is_async_generator",  # Is an async def generator?
    "is_awaitable_coroutine",  # Decorated with '@{typing,asyncio}.coroutine'?
    "expanded",  # Variants of function with type variables with values expanded
)

__deletable__ = ("arguments", "max_pos", "min_args")

def __init__(
    self,
    arguments: list[Argument] | None = None,
    body: Block | None = None,
    typ: mypy.types.FunctionLike | None = None,
) -&gt; None:
    super().__init__()
    self.arguments = arguments or []
    self.arg_names = [None if arg.pos_only else arg.variable.name for arg in self.arguments]
    self.arg_kinds: list[ArgKind] = [arg.kind for arg in self.arguments]
    self.max_pos: int = self.arg_kinds.count(ARG_POS) + self.arg_kinds.count(ARG_OPT)
    self.body: Block = body or Block([])
    self.type = typ
    self.unanalyzed_type = typ
    self.is_overload: bool = False
    self.is_generator: bool = False
    self.is_coroutine: bool = False
    self.is_async_generator: bool = False
    self.is_awaitable_coroutine: bool = False
    self.expanded: list[FuncItem] = []

    self.min_args = 0
    for i in range(len(self.arguments)):
        if self.arguments[i] is None and i &lt; self.max_fixed_argc():
            self.min_args = i + 1

</t>
<t tx="ekr.20230831011820.406">def max_fixed_argc(self) -&gt; int:
    return self.max_pos

</t>
<t tx="ekr.20230831011820.407">def is_dynamic(self) -&gt; bool:
    return self.type is None


</t>
<t tx="ekr.20230831011820.408">FUNCDEF_FLAGS: Final = FUNCITEM_FLAGS + [
    "is_decorated",
    "is_conditional",
    "is_trivial_body",
    "is_mypy_only",
]

# Abstract status of a function
NOT_ABSTRACT: Final = 0
# Explicitly abstract (with @abstractmethod or overload without implementation)
IS_ABSTRACT: Final = 1
# Implicitly abstract: used for functions with trivial bodies defined in Protocols
IMPLICITLY_ABSTRACT: Final = 2


class FuncDef(FuncItem, SymbolNode, Statement):
    """Function definition.

    This is a non-lambda function defined using 'def'.
    """

    @others
</t>
<t tx="ekr.20230831011820.409">__slots__ = (
    "_name",
    "is_decorated",
    "is_conditional",
    "abstract_status",
    "original_def",
    "deco_line",
    "is_trivial_body",
    "is_mypy_only",
    # Present only when a function is decorated with @typing.datasclass_transform or similar
    "dataclass_transform_spec",
    "docstring",
)

__match_args__ = ("name", "arguments", "type", "body")

# Note that all __init__ args must have default values
def __init__(
    self,
    name: str = "",  # Function name
    arguments: list[Argument] | None = None,
    body: Block | None = None,
    typ: mypy.types.FunctionLike | None = None,
) -&gt; None:
    super().__init__(arguments, body, typ)
    self._name = name
    self.is_decorated = False
    self.is_conditional = False  # Defined conditionally (within block)?
    self.abstract_status = NOT_ABSTRACT
    # Is this an abstract method with trivial body?
    # Such methods can't be called via super().
    self.is_trivial_body = False
    self.is_final = False
    # Original conditional definition
    self.original_def: None | FuncDef | Var | Decorator = None
    # Used for error reporting (to keep backward compatibility with pre-3.8)
    self.deco_line: int | None = None
    # Definitions that appear in if TYPE_CHECKING are marked with this flag.
    self.is_mypy_only = False
    self.dataclass_transform_spec: DataclassTransformSpec | None = None
    self.docstring: str | None = None

</t>
<t tx="ekr.20230831011820.41">def visit_instance(self, t: Instance) -&gt; ProperType:
    if isinstance(self.s, Instance):
        if t.type == self.s.type:
            if is_subtype(t, self.s) or is_subtype(self.s, t):
                # Combine type arguments. We could have used join below
                # equivalently.
                args: list[Type] = []
                # N.B: We use zip instead of indexing because the lengths might have
                # mismatches during daemon reprocessing.
                for ta, sia in zip(t.args, self.s.args):
                    args.append(self.meet(ta, sia))
                return Instance(t.type, args)
            else:
                if state.strict_optional:
                    return UninhabitedType()
                else:
                    return NoneType()
        else:
            alt_promote = t.type.alt_promote
            if alt_promote and alt_promote.type is self.s.type:
                return t
            alt_promote = self.s.type.alt_promote
            if alt_promote and alt_promote.type is t.type:
                return self.s
            if is_subtype(t, self.s):
                return t
            elif is_subtype(self.s, t):
                # See also above comment.
                return self.s
            else:
                if state.strict_optional:
                    return UninhabitedType()
                else:
                    return NoneType()
    elif isinstance(self.s, FunctionLike) and t.type.is_protocol:
        call = join.unpack_callback_protocol(t)
        if call:
            return meet_types(call, self.s)
    elif isinstance(self.s, FunctionLike) and self.s.is_type_obj() and t.type.is_metaclass():
        if is_subtype(self.s.fallback, t):
            return self.s
        return self.default(self.s)
    elif isinstance(self.s, TypeType):
        return meet_types(t, self.s)
    elif isinstance(self.s, TupleType):
        return meet_types(t, self.s)
    elif isinstance(self.s, LiteralType):
        return meet_types(t, self.s)
    elif isinstance(self.s, TypedDictType):
        return meet_types(t, self.s)
    return self.default(self.s)

</t>
<t tx="ekr.20230831011820.410">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20230831011820.411">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_func_def(self)

</t>
<t tx="ekr.20230831011820.412">def serialize(self) -&gt; JsonDict:
    # We're deliberating omitting arguments and storing only arg_names and
    # arg_kinds for space-saving reasons (arguments is not used in later
    # stages of mypy).
    # TODO: After a FuncDef is deserialized, the only time we use `arg_names`
    # and `arg_kinds` is when `type` is None and we need to infer a type. Can
    # we store the inferred type ahead of time?
    return {
        ".class": "FuncDef",
        "name": self._name,
        "fullname": self._fullname,
        "arg_names": self.arg_names,
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "type": None if self.type is None else self.type.serialize(),
        "flags": get_flags(self, FUNCDEF_FLAGS),
        "abstract_status": self.abstract_status,
        # TODO: Do we need expanded, original_def?
        "dataclass_transform_spec": (
            None
            if self.dataclass_transform_spec is None
            else self.dataclass_transform_spec.serialize()
        ),
    }

</t>
<t tx="ekr.20230831011820.413">@classmethod
def deserialize(cls, data: JsonDict) -&gt; FuncDef:
    assert data[".class"] == "FuncDef"
    body = Block([])
    ret = FuncDef(
        data["name"],
        [],
        body,
        (
            None
            if data["type"] is None
            else cast(mypy.types.FunctionLike, mypy.types.deserialize_type(data["type"]))
        ),
    )
    ret._fullname = data["fullname"]
    set_flags(ret, data["flags"])
    # NOTE: ret.info is set in the fixup phase.
    ret.arg_names = data["arg_names"]
    ret.arg_kinds = [ArgKind(x) for x in data["arg_kinds"]]
    ret.abstract_status = data["abstract_status"]
    ret.dataclass_transform_spec = (
        DataclassTransformSpec.deserialize(data["dataclass_transform_spec"])
        if data["dataclass_transform_spec"] is not None
        else None
    )
    # Leave these uninitialized so that future uses will trigger an error
    del ret.arguments
    del ret.max_pos
    del ret.min_args
    return ret


</t>
<t tx="ekr.20230831011820.414"># All types that are both SymbolNodes and FuncBases. See the FuncBase
# docstring for the rationale.
SYMBOL_FUNCBASE_TYPES = (OverloadedFuncDef, FuncDef)


class Decorator(SymbolNode, Statement):
    """A decorated function.

    A single Decorator object can include any number of function decorators.
    """

    @others
</t>
<t tx="ekr.20230831011820.415">__slots__ = ("func", "decorators", "original_decorators", "var", "is_overload")

__match_args__ = ("decorators", "var", "func")

func: FuncDef  # Decorated function
decorators: list[Expression]  # Decorators (may be empty)
# Some decorators are removed by semanal, keep the original here.
original_decorators: list[Expression]
# TODO: This is mostly used for the type; consider replacing with a 'type' attribute
var: Var  # Represents the decorated function obj
is_overload: bool

def __init__(self, func: FuncDef, decorators: list[Expression], var: Var) -&gt; None:
    super().__init__()
    self.func = func
    self.decorators = decorators
    self.original_decorators = decorators.copy()
    self.var = var
    self.is_overload = False

</t>
<t tx="ekr.20230831011820.416">@property
def name(self) -&gt; str:
    return self.func.name

</t>
<t tx="ekr.20230831011820.417">@property
def fullname(self) -&gt; str:
    return self.func.fullname

</t>
<t tx="ekr.20230831011820.418">@property
def is_final(self) -&gt; bool:
    return self.func.is_final

</t>
<t tx="ekr.20230831011820.419">@property
def info(self) -&gt; TypeInfo:
    return self.func.info

</t>
<t tx="ekr.20230831011820.42">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    if isinstance(self.s, CallableType) and join.is_similar_callables(t, self.s):
        if is_equivalent(t, self.s):
            return join.combine_similar_callables(t, self.s)
        result = meet_similar_callables(t, self.s)
        # We set the from_type_type flag to suppress error when a collection of
        # concrete class objects gets inferred as their common abstract superclass.
        if not (
            (t.is_type_obj() and t.type_object().is_abstract)
            or (self.s.is_type_obj() and self.s.type_object().is_abstract)
        ):
            result.from_type_type = True
        if isinstance(get_proper_type(result.ret_type), UninhabitedType):
            # Return a plain None or &lt;uninhabited&gt; instead of a weird function.
            return self.default(self.s)
        return result
    elif isinstance(self.s, TypeType) and t.is_type_obj() and not t.is_generic():
        # In this case we are able to potentially produce a better meet.
        res = meet_types(self.s.item, t.ret_type)
        if not isinstance(res, (NoneType, UninhabitedType)):
            return TypeType.make_normalized(res)
        return self.default(self.s)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = join.unpack_callback_protocol(self.s)
        if call:
            return meet_types(t, call)
    return self.default(self.s)

</t>
<t tx="ekr.20230831011820.420">@property
def type(self) -&gt; mypy.types.Type | None:
    return self.var.type

</t>
<t tx="ekr.20230831011820.421">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_decorator(self)

</t>
<t tx="ekr.20230831011820.422">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "Decorator",
        "func": self.func.serialize(),
        "var": self.var.serialize(),
        "is_overload": self.is_overload,
    }

</t>
<t tx="ekr.20230831011820.423">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Decorator:
    assert data[".class"] == "Decorator"
    dec = Decorator(FuncDef.deserialize(data["func"]), [], Var.deserialize(data["var"]))
    dec.is_overload = data["is_overload"]
    return dec


</t>
<t tx="ekr.20230831011820.424">VAR_FLAGS: Final = [
    "is_self",
    "is_cls",
    "is_initialized_in_class",
    "is_staticmethod",
    "is_classmethod",
    "is_property",
    "is_settable_property",
    "is_suppressed_import",
    "is_classvar",
    "is_abstract_var",
    "is_final",
    "final_unset_in_class",
    "final_set_in_init",
    "explicit_self_type",
    "is_ready",
    "is_inferred",
    "invalid_partial_type",
    "from_module_getattr",
    "has_explicit_value",
    "allow_incompatible_override",
]


class Var(SymbolNode):
    """A variable.

    It can refer to global/local variable or a data attribute.
    """

    @others
</t>
<t tx="ekr.20230831011820.425">__slots__ = (
    "_name",
    "_fullname",
    "info",
    "type",
    "final_value",
    "is_self",
    "is_cls",
    "is_ready",
    "is_inferred",
    "is_initialized_in_class",
    "is_staticmethod",
    "is_classmethod",
    "is_property",
    "is_settable_property",
    "is_classvar",
    "is_abstract_var",
    "is_final",
    "final_unset_in_class",
    "final_set_in_init",
    "is_suppressed_import",
    "explicit_self_type",
    "from_module_getattr",
    "has_explicit_value",
    "allow_incompatible_override",
    "invalid_partial_type",
)

__match_args__ = ("name", "type", "final_value")

def __init__(self, name: str, type: mypy.types.Type | None = None) -&gt; None:
    super().__init__()
    self._name = name  # Name without module prefix
    # TODO: Should be Optional[str]
    self._fullname = ""  # Name with module prefix
    # TODO: Should be Optional[TypeInfo]
    self.info = VAR_NO_INFO
    self.type: mypy.types.Type | None = type  # Declared or inferred type, or None
    # Is this the first argument to an ordinary method (usually "self")?
    self.is_self = False
    # Is this the first argument to a classmethod (typically "cls")?
    self.is_cls = False
    self.is_ready = True  # If inferred, is the inferred type available?
    self.is_inferred = self.type is None
    # Is this initialized explicitly to a non-None value in class body?
    self.is_initialized_in_class = False
    self.is_staticmethod = False
    self.is_classmethod = False
    self.is_property = False
    self.is_settable_property = False
    self.is_classvar = False
    self.is_abstract_var = False
    # Set to true when this variable refers to a module we were unable to
    # parse for some reason (eg a silenced module)
    self.is_suppressed_import = False
    # Was this "variable" (rather a constant) defined as Final[...]?
    self.is_final = False
    # If constant value is a simple literal,
    # store the literal value (unboxed) for the benefit of
    # tools like mypyc.
    self.final_value: int | float | complex | bool | str | None = None
    # Where the value was set (only for class attributes)
    self.final_unset_in_class = False
    self.final_set_in_init = False
    # This is True for a variable that was declared on self with an explicit type:
    #     class C:
    #         def __init__(self) -&gt; None:
    #             self.x: int
    # This case is important because this defines a new Var, even if there is one
    # present in a superclass (without explicit type this doesn't create a new Var).
    # See SemanticAnalyzer.analyze_member_lvalue() for details.
    self.explicit_self_type = False
    # If True, this is an implicit Var created due to module-level __getattr__.
    self.from_module_getattr = False
    # Var can be created with an explicit value `a = 1` or without one `a: int`,
    # we need a way to tell which one is which.
    self.has_explicit_value = False
    # If True, subclasses can override this with an incompatible type.
    self.allow_incompatible_override = False
    # If True, this means we didn't manage to infer full type and fall back to
    # something like list[Any]. We may decide to not use such types as context.
    self.invalid_partial_type = False

</t>
<t tx="ekr.20230831011820.426">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20230831011820.427">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20230831011820.428">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_var(self)

</t>
<t tx="ekr.20230831011820.429">def serialize(self) -&gt; JsonDict:
    # TODO: Leave default values out?
    # NOTE: Sometimes self.is_ready is False here, but we don't care.
    data: JsonDict = {
        ".class": "Var",
        "name": self._name,
        "fullname": self._fullname,
        "type": None if self.type is None else self.type.serialize(),
        "flags": get_flags(self, VAR_FLAGS),
    }
    if self.final_value is not None:
        data["final_value"] = self.final_value
    return data

</t>
<t tx="ekr.20230831011820.43">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    # TODO: Implement a better algorithm that covers at least the same cases
    # as TypeJoinVisitor.visit_overloaded().
    s = self.s
    if isinstance(s, FunctionLike):
        if s.items == t.items:
            return Overloaded(t.items)
        elif is_subtype(s, t):
            return s
        elif is_subtype(t, s):
            return t
        else:
            return meet_types(t.fallback, s.fallback)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = join.unpack_callback_protocol(self.s)
        if call:
            return meet_types(t, call)
    return meet_types(t.fallback, s)

</t>
<t tx="ekr.20230831011820.430">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Var:
    assert data[".class"] == "Var"
    name = data["name"]
    type = None if data["type"] is None else mypy.types.deserialize_type(data["type"])
    v = Var(name, type)
    v.is_ready = False  # Override True default set in __init__
    v._fullname = data["fullname"]
    set_flags(v, data["flags"])
    v.final_value = data.get("final_value")
    return v


</t>
<t tx="ekr.20230831011820.431">class ClassDef(Statement):
    """Class definition"""

    @others
</t>
<t tx="ekr.20230831011820.432">__slots__ = (
    "name",
    "_fullname",
    "defs",
    "type_vars",
    "base_type_exprs",
    "removed_base_type_exprs",
    "info",
    "metaclass",
    "decorators",
    "keywords",
    "analyzed",
    "has_incompatible_baseclass",
    "deco_line",
    "docstring",
    "removed_statements",
)

__match_args__ = ("name", "defs")

name: str  # Name of the class without module prefix
_fullname: str  # Fully qualified name of the class
defs: Block
type_vars: list[mypy.types.TypeVarLikeType]
# Base class expressions (not semantically analyzed -- can be arbitrary expressions)
base_type_exprs: list[Expression]
# Special base classes like Generic[...] get moved here during semantic analysis
removed_base_type_exprs: list[Expression]
info: TypeInfo  # Related TypeInfo
metaclass: Expression | None
decorators: list[Expression]
keywords: dict[str, Expression]
analyzed: Expression | None
has_incompatible_baseclass: bool
# Used by special forms like NamedTuple and TypedDict to store invalid statements
removed_statements: list[Statement]

def __init__(
    self,
    name: str,
    defs: Block,
    type_vars: list[mypy.types.TypeVarLikeType] | None = None,
    base_type_exprs: list[Expression] | None = None,
    metaclass: Expression | None = None,
    keywords: list[tuple[str, Expression]] | None = None,
) -&gt; None:
    super().__init__()
    self.name = name
    self._fullname = ""
    self.defs = defs
    self.type_vars = type_vars or []
    self.base_type_exprs = base_type_exprs or []
    self.removed_base_type_exprs = []
    self.info = CLASSDEF_NO_INFO
    self.metaclass = metaclass
    self.decorators = []
    self.keywords = dict(keywords) if keywords else {}
    self.analyzed = None
    self.has_incompatible_baseclass = False
    # Used for error reporting (to keep backwad compatibility with pre-3.8)
    self.deco_line: int | None = None
    self.docstring: str | None = None
    self.removed_statements = []

</t>
<t tx="ekr.20230831011820.433">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20230831011820.434">@fullname.setter
def fullname(self, v: str) -&gt; None:
    self._fullname = v

</t>
<t tx="ekr.20230831011820.435">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_class_def(self)

</t>
<t tx="ekr.20230831011820.436">def is_generic(self) -&gt; bool:
    return self.info.is_generic()

</t>
<t tx="ekr.20230831011820.437">def serialize(self) -&gt; JsonDict:
    # Not serialized: defs, base_type_exprs, metaclass, decorators,
    # analyzed (for named tuples etc.)
    return {
        ".class": "ClassDef",
        "name": self.name,
        "fullname": self.fullname,
        "type_vars": [v.serialize() for v in self.type_vars],
    }

</t>
<t tx="ekr.20230831011820.438">@classmethod
def deserialize(self, data: JsonDict) -&gt; ClassDef:
    assert data[".class"] == "ClassDef"
    res = ClassDef(
        data["name"],
        Block([]),
        # https://github.com/python/mypy/issues/12257
        [
            cast(mypy.types.TypeVarLikeType, mypy.types.deserialize_type(v))
            for v in data["type_vars"]
        ],
    )
    res.fullname = data["fullname"]
    return res


</t>
<t tx="ekr.20230831011820.439">class GlobalDecl(Statement):
    """Declaration global x, y, ..."""

    @others
</t>
<t tx="ekr.20230831011820.44">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    if isinstance(self.s, TupleType) and self.s.length() == t.length():
        items: list[Type] = []
        for i in range(t.length()):
            items.append(self.meet(t.items[i], self.s.items[i]))
        # TODO: What if the fallbacks are different?
        return TupleType(items, tuple_fallback(t))
    elif isinstance(self.s, Instance):
        # meet(Tuple[t1, t2, &lt;...&gt;], Tuple[s, ...]) == Tuple[meet(t1, s), meet(t2, s), &lt;...&gt;].
        if self.s.type.fullname == "builtins.tuple" and self.s.args:
            return t.copy_modified(items=[meet_types(it, self.s.args[0]) for it in t.items])
        elif is_proper_subtype(t, self.s):
            # A named tuple that inherits from a normal class
            return t
    return self.default(self.s)

</t>
<t tx="ekr.20230831011820.440">__slots__ = ("names",)

__match_args__ = ("names",)

names: list[str]

def __init__(self, names: list[str]) -&gt; None:
    super().__init__()
    self.names = names

</t>
<t tx="ekr.20230831011820.441">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_global_decl(self)


</t>
<t tx="ekr.20230831011820.442">class NonlocalDecl(Statement):
    """Declaration nonlocal x, y, ..."""

    @others
</t>
<t tx="ekr.20230831011820.443">__slots__ = ("names",)

__match_args__ = ("names",)

names: list[str]

def __init__(self, names: list[str]) -&gt; None:
    super().__init__()
    self.names = names

</t>
<t tx="ekr.20230831011820.444">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_nonlocal_decl(self)


</t>
<t tx="ekr.20230831011820.445">class Block(Statement):
    @others
</t>
<t tx="ekr.20230831011820.446">__slots__ = ("body", "is_unreachable")

__match_args__ = ("body", "is_unreachable")

def __init__(self, body: list[Statement]) -&gt; None:
    super().__init__()
    self.body = body
    # True if we can determine that this block is not executed during semantic
    # analysis. For example, this applies to blocks that are protected by
    # something like "if PY3:" when using Python 2. However, some code is
    # only considered unreachable during type checking and this is not true
    # in those cases.
    self.is_unreachable = False

</t>
<t tx="ekr.20230831011820.447">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_block(self)


</t>
<t tx="ekr.20230831011820.448"># Statements


class ExpressionStmt(Statement):
    """An expression as a statement, such as print(s)."""

    @others
</t>
<t tx="ekr.20230831011820.449">__slots__ = ("expr",)

__match_args__ = ("expr",)

expr: Expression

def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20230831011820.45">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    if isinstance(self.s, TypedDictType):
        for name, l, r in self.s.zip(t):
            if not is_equivalent(l, r) or (name in t.required_keys) != (
                name in self.s.required_keys
            ):
                return self.default(self.s)
        item_list: list[tuple[str, Type]] = []
        for item_name, s_item_type, t_item_type in self.s.zipall(t):
            if s_item_type is not None:
                item_list.append((item_name, s_item_type))
            else:
                # at least one of s_item_type and t_item_type is not None
                assert t_item_type is not None
                item_list.append((item_name, t_item_type))
        items = dict(item_list)
        fallback = self.s.create_anonymous_fallback()
        required_keys = t.required_keys | self.s.required_keys
        return TypedDictType(items, required_keys, fallback)
    elif isinstance(self.s, Instance) and is_subtype(t, self.s):
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011820.450">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_expression_stmt(self)


</t>
<t tx="ekr.20230831011820.451">class AssignmentStmt(Statement):
    """Assignment statement.

    The same node class is used for single assignment, multiple assignment
    (e.g. x, y = z) and chained assignment (e.g. x = y = z), assignments
    that define new names, and assignments with explicit types ("# type: t"
    or "x: t [= ...]").

    An lvalue can be NameExpr, TupleExpr, ListExpr, MemberExpr, or IndexExpr.
    """

    @others
</t>
<t tx="ekr.20230831011820.452">__slots__ = (
    "lvalues",
    "rvalue",
    "type",
    "unanalyzed_type",
    "new_syntax",
    "is_alias_def",
    "is_final_def",
    "invalid_recursive_alias",
)

__match_args__ = ("lvalues", "rvalues", "type")

lvalues: list[Lvalue]
# This is a TempNode if and only if no rvalue (x: t).
rvalue: Expression
# Declared type in a comment, may be None.
type: mypy.types.Type | None
# Original, not semantically analyzed type in annotation (used for reprocessing)
unanalyzed_type: mypy.types.Type | None
# This indicates usage of PEP 526 type annotation syntax in assignment.
new_syntax: bool
# Does this assignment define a type alias?
is_alias_def: bool
# Is this a final definition?
# Final attributes can't be re-assigned once set, and can't be overridden
# in a subclass. This flag is not set if an attempted declaration was found to
# be invalid during semantic analysis. It is still set to `True` if
# a final declaration overrides another final declaration (this is checked
# during type checking when MROs are known).
is_final_def: bool
# Stop further processing of this assignment, to prevent flipping back and forth
# during semantic analysis passes.
invalid_recursive_alias: bool

def __init__(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    type: mypy.types.Type | None = None,
    new_syntax: bool = False,
) -&gt; None:
    super().__init__()
    self.lvalues = lvalues
    self.rvalue = rvalue
    self.type = type
    self.unanalyzed_type = type
    self.new_syntax = new_syntax
    self.is_alias_def = False
    self.is_final_def = False
    self.invalid_recursive_alias = False

</t>
<t tx="ekr.20230831011820.453">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_assignment_stmt(self)


</t>
<t tx="ekr.20230831011820.454">class OperatorAssignmentStmt(Statement):
    """Operator assignment statement such as x += 1"""

    @others
</t>
<t tx="ekr.20230831011820.455">__slots__ = ("op", "lvalue", "rvalue")

__match_args__ = ("lvalue", "op", "rvalue")

op: str  # TODO: Enum?
lvalue: Lvalue
rvalue: Expression

def __init__(self, op: str, lvalue: Lvalue, rvalue: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.lvalue = lvalue
    self.rvalue = rvalue

</t>
<t tx="ekr.20230831011820.456">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_operator_assignment_stmt(self)


</t>
<t tx="ekr.20230831011820.457">class WhileStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.458">__slots__ = ("expr", "body", "else_body")

__match_args__ = ("expr", "body", "else_body")

expr: Expression
body: Block
else_body: Block | None

def __init__(self, expr: Expression, body: Block, else_body: Block | None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.body = body
    self.else_body = else_body

</t>
<t tx="ekr.20230831011820.459">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_while_stmt(self)


</t>
<t tx="ekr.20230831011820.46">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    if isinstance(self.s, LiteralType) and self.s == t:
        return t
    elif isinstance(self.s, Instance) and is_subtype(t.fallback, self.s):
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011820.460">class ForStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.461">__slots__ = (
    "index",
    "index_type",
    "unanalyzed_index_type",
    "inferred_item_type",
    "inferred_iterator_type",
    "expr",
    "body",
    "else_body",
    "is_async",
)

__match_args__ = ("index", "index_type", "expr", "body", "else_body")

# Index variables
index: Lvalue
# Type given by type comments for index, can be None
index_type: mypy.types.Type | None
# Original, not semantically analyzed type in annotation (used for reprocessing)
unanalyzed_index_type: mypy.types.Type | None
# Inferred iterable item type
inferred_item_type: mypy.types.Type | None
# Inferred iterator type
inferred_iterator_type: mypy.types.Type | None
# Expression to iterate
expr: Expression
body: Block
else_body: Block | None
is_async: bool  # True if `async for ...` (PEP 492, Python 3.5)

def __init__(
    self,
    index: Lvalue,
    expr: Expression,
    body: Block,
    else_body: Block | None,
    index_type: mypy.types.Type | None = None,
) -&gt; None:
    super().__init__()
    self.index = index
    self.index_type = index_type
    self.unanalyzed_index_type = index_type
    self.inferred_item_type = None
    self.inferred_iterator_type = None
    self.expr = expr
    self.body = body
    self.else_body = else_body
    self.is_async = False

</t>
<t tx="ekr.20230831011820.462">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_for_stmt(self)


</t>
<t tx="ekr.20230831011820.463">class ReturnStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.464">__slots__ = ("expr",)

__match_args__ = ("expr",)

expr: Expression | None

def __init__(self, expr: Expression | None) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20230831011820.465">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_return_stmt(self)


</t>
<t tx="ekr.20230831011820.466">class AssertStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.467">__slots__ = ("expr", "msg")

__match_args__ = ("expr", "msg")

expr: Expression
msg: Expression | None

def __init__(self, expr: Expression, msg: Expression | None = None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.msg = msg

</t>
<t tx="ekr.20230831011820.468">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_assert_stmt(self)


</t>
<t tx="ekr.20230831011820.469">class DelStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.47">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # We can't determine the meet of partial types. We should never get here.
    assert False, "Internal error"

</t>
<t tx="ekr.20230831011820.470">__slots__ = ("expr",)

__match_args__ = ("expr",)

expr: Lvalue

def __init__(self, expr: Lvalue) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20230831011820.471">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_del_stmt(self)


</t>
<t tx="ekr.20230831011820.472">class BreakStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.473">__slots__ = ()

def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_break_stmt(self)


</t>
<t tx="ekr.20230831011820.474">class ContinueStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.475">__slots__ = ()

def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_continue_stmt(self)


</t>
<t tx="ekr.20230831011820.476">class PassStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.477">__slots__ = ()

def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_pass_stmt(self)


</t>
<t tx="ekr.20230831011820.478">class IfStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.479">__slots__ = ("expr", "body", "else_body")

__match_args__ = ("expr", "body", "else_body")

expr: list[Expression]
body: list[Block]
else_body: Block | None

def __init__(self, expr: list[Expression], body: list[Block], else_body: Block | None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.body = body
    self.else_body = else_body

</t>
<t tx="ekr.20230831011820.48">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    if isinstance(self.s, TypeType):
        typ = self.meet(t.item, self.s.item)
        if not isinstance(typ, NoneType):
            typ = TypeType.make_normalized(typ, line=t.line)
        return typ
    elif isinstance(self.s, Instance) and self.s.type.fullname == "builtins.type":
        return t
    elif isinstance(self.s, CallableType):
        return self.meet(t, self.s)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20230831011820.480">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_if_stmt(self)


</t>
<t tx="ekr.20230831011820.481">class RaiseStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.482">__slots__ = ("expr", "from_expr")

__match_args__ = ("expr", "from_expr")

# Plain 'raise' is a valid statement.
expr: Expression | None
from_expr: Expression | None

def __init__(self, expr: Expression | None, from_expr: Expression | None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.from_expr = from_expr

</t>
<t tx="ekr.20230831011820.483">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_raise_stmt(self)


</t>
<t tx="ekr.20230831011820.484">class TryStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.485">__slots__ = ("body", "types", "vars", "handlers", "else_body", "finally_body", "is_star")

__match_args__ = ("body", "types", "vars", "handlers", "else_body", "finally_body", "is_star")

body: Block  # Try body
# Plain 'except:' also possible
types: list[Expression | None]  # Except type expressions
vars: list[NameExpr | None]  # Except variable names
handlers: list[Block]  # Except bodies
else_body: Block | None
finally_body: Block | None
# Whether this is try ... except* (added in Python 3.11)
is_star: bool

def __init__(
    self,
    body: Block,
    vars: list[NameExpr | None],
    types: list[Expression | None],
    handlers: list[Block],
    else_body: Block | None,
    finally_body: Block | None,
) -&gt; None:
    super().__init__()
    self.body = body
    self.vars = vars
    self.types = types
    self.handlers = handlers
    self.else_body = else_body
    self.finally_body = finally_body
    self.is_star = False

</t>
<t tx="ekr.20230831011820.486">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_try_stmt(self)


</t>
<t tx="ekr.20230831011820.487">class WithStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.488">__slots__ = ("expr", "target", "unanalyzed_type", "analyzed_types", "body", "is_async")

__match_args__ = ("expr", "target", "body")

expr: list[Expression]
target: list[Lvalue | None]
# Type given by type comments for target, can be None
unanalyzed_type: mypy.types.Type | None
# Semantically analyzed types from type comment (TypeList type expanded)
analyzed_types: list[mypy.types.Type]
body: Block
is_async: bool  # True if `async with ...` (PEP 492, Python 3.5)

def __init__(
    self,
    expr: list[Expression],
    target: list[Lvalue | None],
    body: Block,
    target_type: mypy.types.Type | None = None,
) -&gt; None:
    super().__init__()
    self.expr = expr
    self.target = target
    self.unanalyzed_type = target_type
    self.analyzed_types = []
    self.body = body
    self.is_async = False

</t>
<t tx="ekr.20230831011820.489">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_with_stmt(self)


</t>
<t tx="ekr.20230831011820.49">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, f"This should be never called, got {t}"

</t>
<t tx="ekr.20230831011820.490">class MatchStmt(Statement):
    @others
</t>
<t tx="ekr.20230831011820.491">__slots__ = ("subject", "patterns", "guards", "bodies")

__match_args__ = ("subject", "patterns", "guards", "bodies")

subject: Expression
patterns: list[Pattern]
guards: list[Expression | None]
bodies: list[Block]

def __init__(
    self,
    subject: Expression,
    patterns: list[Pattern],
    guards: list[Expression | None],
    bodies: list[Block],
) -&gt; None:
    super().__init__()
    assert len(patterns) == len(guards) == len(bodies)
    self.subject = subject
    self.patterns = patterns
    self.guards = guards
    self.bodies = bodies

</t>
<t tx="ekr.20230831011820.492">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_match_stmt(self)


</t>
<t tx="ekr.20230831011820.493"># Expressions


class IntExpr(Expression):
    """Integer literal"""

    @others
</t>
<t tx="ekr.20230831011820.494">__slots__ = ("value",)

__match_args__ = ("value",)

value: int  # 0 by default

def __init__(self, value: int) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20230831011820.495">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_int_expr(self)


</t>
<t tx="ekr.20230831011820.496"># How mypy uses StrExpr and BytesExpr:
#
# b'x' -&gt; BytesExpr
# 'x', u'x' -&gt; StrExpr


class StrExpr(Expression):
    """String literal"""

    @others
</t>
<t tx="ekr.20230831011820.497">__slots__ = ("value",)

__match_args__ = ("value",)

value: str  # '' by default

def __init__(self, value: str) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20230831011820.498">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_str_expr(self)


</t>
<t tx="ekr.20230831011820.499">def is_StrExpr_list(seq: list[Expression]) -&gt; TypeGuard[list[StrExpr]]:
    return all(isinstance(item, StrExpr) for item in seq)


</t>
<t tx="ekr.20230831011820.5">def install_types(
    formatter: util.FancyFormatter,
    options: Options,
    *,
    after_run: bool = False,
    non_interactive: bool = False,
) -&gt; bool:
    """Install stub packages using pip if some missing stubs were detected."""
    packages = read_types_packages_to_install(options.cache_dir, after_run)
    if not packages:
        # If there are no missing stubs, generate no output.
        return False
    if after_run and not non_interactive:
        print()
    print("Installing missing stub packages:")
    assert options.python_executable, "Python executable required to install types"
    cmd = [options.python_executable, "-m", "pip", "install"] + packages
    print(formatter.style(" ".join(cmd), "none", bold=True))
    print()
    if not non_interactive:
        x = input("Install? [yN] ")
        if not x.strip() or not x.lower().startswith("y"):
            print(formatter.style("mypy: Skipping installation", "red", bold=True))
            sys.exit(2)
        print()
    subprocess.run(cmd)
    return True
</t>
<t tx="ekr.20230831011820.50">def meet(self, s: Type, t: Type) -&gt; ProperType:
    return meet_types(s, t)

</t>
<t tx="ekr.20230831011820.500">class BytesExpr(Expression):
    """Bytes literal"""

    @others
</t>
<t tx="ekr.20230831011820.501">__slots__ = ("value",)

__match_args__ = ("value",)

# Note: we deliberately do NOT use bytes here because it ends up
# unnecessarily complicating a lot of the result logic. For example,
# we'd have to worry about converting the bytes into a format we can
# easily serialize/deserialize to and from JSON, would have to worry
# about turning the bytes into a human-readable representation in
# error messages...
#
# It's more convenient to just store the human-readable representation
# from the very start.
value: str

def __init__(self, value: str) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20230831011820.502">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_bytes_expr(self)


</t>
<t tx="ekr.20230831011820.503">class FloatExpr(Expression):
    """Float literal"""

    @others
</t>
<t tx="ekr.20230831011820.504">__slots__ = ("value",)

__match_args__ = ("value",)

value: float  # 0.0 by default

def __init__(self, value: float) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20230831011820.505">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_float_expr(self)


</t>
<t tx="ekr.20230831011820.506">class ComplexExpr(Expression):
    """Complex literal"""

    @others
</t>
<t tx="ekr.20230831011820.507">__slots__ = ("value",)

__match_args__ = ("value",)

value: complex

def __init__(self, value: complex) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20230831011820.508">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_complex_expr(self)


</t>
<t tx="ekr.20230831011820.509">class EllipsisExpr(Expression):
    """Ellipsis (...)"""

    @others
</t>
<t tx="ekr.20230831011820.51">def default(self, typ: Type) -&gt; ProperType:
    if isinstance(typ, UnboundType):
        return AnyType(TypeOfAny.special_form)
    else:
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()


</t>
<t tx="ekr.20230831011820.510">__slots__ = ()

def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_ellipsis(self)


</t>
<t tx="ekr.20230831011820.511">class StarExpr(Expression):
    """Star expression"""

    @others
</t>
<t tx="ekr.20230831011820.512">__slots__ = ("expr", "valid")

__match_args__ = ("expr", "valid")

expr: Expression
valid: bool

def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

    # Whether this starred expression is used in a tuple/list and as lvalue
    self.valid = False

</t>
<t tx="ekr.20230831011820.513">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_star_expr(self)


</t>
<t tx="ekr.20230831011820.514">class RefExpr(Expression):
    """Abstract base class for name-like constructs"""

    @others
</t>
<t tx="ekr.20230831011820.515">__slots__ = (
    "kind",
    "node",
    "_fullname",
    "is_new_def",
    "is_inferred_def",
    "is_alias_rvalue",
    "type_guard",
)

def __init__(self) -&gt; None:
    super().__init__()
    # LDEF/GDEF/MDEF/... (None if not available)
    self.kind: int | None = None
    # Var, FuncDef or TypeInfo that describes this
    self.node: SymbolNode | None = None
    # Fully qualified name (or name if not global)
    self._fullname = ""
    # Does this define a new name?
    self.is_new_def = False
    # Does this define a new name with inferred type?
    #
    # For members, after semantic analysis, this does not take base
    # classes into consideration at all; the type checker deals with these.
    self.is_inferred_def = False
    # Is this expression appears as an rvalue of a valid type alias definition?
    self.is_alias_rvalue = False
    # Cache type guard from callable_type.type_guard
    self.type_guard: mypy.types.Type | None = None

</t>
<t tx="ekr.20230831011820.516">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20230831011820.517">@fullname.setter
def fullname(self, v: str) -&gt; None:
    self._fullname = v


</t>
<t tx="ekr.20230831011820.518">class NameExpr(RefExpr):
    """Name expression

    This refers to a local name, global name or a module.
    """

    @others
</t>
<t tx="ekr.20230831011820.519">__slots__ = ("name", "is_special_form")

__match_args__ = ("name", "node")

def __init__(self, name: str) -&gt; None:
    super().__init__()
    self.name = name  # Name referred to
    # Is this a l.h.s. of a special form assignment like typed dict or type variable?
    self.is_special_form = False

</t>
<t tx="ekr.20230831011820.52">def meet_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    from mypy.join import join_types

    arg_types: list[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(join_types(t.arg_types[i], s.arg_types[i]))
    # TODO in combine_similar_callables also applies here (names and kinds)
    # The fallback type can be either 'function' or 'type'. The result should have 'function' as
    # fallback only if both operands have it as 'function'.
    if t.fallback.type.fullname != "builtins.function":
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(
        arg_types=arg_types,
        ret_type=meet_types(t.ret_type, s.ret_type),
        fallback=fallback,
        name=None,
    )


</t>
<t tx="ekr.20230831011820.520">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_name_expr(self)

</t>
<t tx="ekr.20230831011820.521">def serialize(self) -&gt; JsonDict:
    assert False, f"Serializing NameExpr: {self}"


</t>
<t tx="ekr.20230831011820.522">class MemberExpr(RefExpr):
    """Member access expression x.y"""

    @others
</t>
<t tx="ekr.20230831011820.523">__slots__ = ("expr", "name", "def_var")

__match_args__ = ("expr", "name", "node")

def __init__(self, expr: Expression, name: str) -&gt; None:
    super().__init__()
    self.expr = expr
    self.name = name
    # The variable node related to a definition through 'self.x = &lt;initializer&gt;'.
    # The nodes of other kinds of member expressions are resolved during type checking.
    self.def_var: Var | None = None

</t>
<t tx="ekr.20230831011820.524">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_member_expr(self)


</t>
<t tx="ekr.20230831011820.525"># Kinds of arguments
@unique
class ArgKind(Enum):
    @others
</t>
<t tx="ekr.20230831011820.526"># Positional argument
ARG_POS = 0
# Positional, optional argument (functions only, not calls)
ARG_OPT = 1
# *arg argument
ARG_STAR = 2
# Keyword argument x=y in call, or keyword-only function arg
ARG_NAMED = 3
# **arg argument
ARG_STAR2 = 4
# In an argument list, keyword-only and also optional
ARG_NAMED_OPT = 5

def is_positional(self, star: bool = False) -&gt; bool:
    return self == ARG_POS or self == ARG_OPT or (star and self == ARG_STAR)

</t>
<t tx="ekr.20230831011820.527">def is_named(self, star: bool = False) -&gt; bool:
    return self == ARG_NAMED or self == ARG_NAMED_OPT or (star and self == ARG_STAR2)

</t>
<t tx="ekr.20230831011820.528">def is_required(self) -&gt; bool:
    return self == ARG_POS or self == ARG_NAMED

</t>
<t tx="ekr.20230831011820.529">def is_optional(self) -&gt; bool:
    return self == ARG_OPT or self == ARG_NAMED_OPT

</t>
<t tx="ekr.20230831011820.53">def meet_type_list(types: list[Type]) -&gt; Type:
    if not types:
        # This should probably be builtins.object but that is hard to get and
        # it doesn't matter for any current users.
        return AnyType(TypeOfAny.implementation_artifact)
    met = types[0]
    for t in types[1:]:
        met = meet_types(met, t)
    return met


</t>
<t tx="ekr.20230831011820.530">def is_star(self) -&gt; bool:
    return self == ARG_STAR or self == ARG_STAR2


</t>
<t tx="ekr.20230831011820.531">ARG_POS: Final = ArgKind.ARG_POS
ARG_OPT: Final = ArgKind.ARG_OPT
ARG_STAR: Final = ArgKind.ARG_STAR
ARG_NAMED: Final = ArgKind.ARG_NAMED
ARG_STAR2: Final = ArgKind.ARG_STAR2
ARG_NAMED_OPT: Final = ArgKind.ARG_NAMED_OPT


class CallExpr(Expression):
    """Call expression.

    This can also represent several special forms that are syntactically calls
    such as cast(...) and None  # type: ....
    """

    @others
</t>
<t tx="ekr.20230831011820.532">__slots__ = ("callee", "args", "arg_kinds", "arg_names", "analyzed")

__match_args__ = ("callee", "args", "arg_kinds", "arg_names")

def __init__(
    self,
    callee: Expression,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: list[str | None],
    analyzed: Expression | None = None,
) -&gt; None:
    super().__init__()
    if not arg_names:
        arg_names = [None] * len(args)

    self.callee = callee
    self.args = args
    self.arg_kinds = arg_kinds  # ARG_ constants
    # Each name can be None if not a keyword argument.
    self.arg_names: list[str | None] = arg_names
    # If not None, the node that represents the meaning of the CallExpr. For
    # cast(...) this is a CastExpr.
    self.analyzed = analyzed

</t>
<t tx="ekr.20230831011820.533">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_call_expr(self)


</t>
<t tx="ekr.20230831011820.534">class YieldFromExpr(Expression):
    @others
</t>
<t tx="ekr.20230831011820.535">__slots__ = ("expr",)

__match_args__ = ("expr",)

expr: Expression

def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20230831011820.536">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_yield_from_expr(self)


</t>
<t tx="ekr.20230831011820.537">class YieldExpr(Expression):
    @others
</t>
<t tx="ekr.20230831011820.538">__slots__ = ("expr",)

__match_args__ = ("expr",)

expr: Expression | None

def __init__(self, expr: Expression | None) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20230831011820.539">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_yield_expr(self)


</t>
<t tx="ekr.20230831011820.54">def typed_dict_mapping_pair(left: Type, right: Type) -&gt; bool:
    """Is this a pair where one type is a TypedDict and another one is an instance of Mapping?

    This case requires a precise/principled consideration because there are two use cases
    that push the boundary the opposite ways: we need to avoid spurious overlaps to avoid
    false positives for overloads, but we also need to avoid spuriously non-overlapping types
    to avoid false positives with --strict-equality.
    """
    left, right = get_proper_types((left, right))
    assert not isinstance(left, TypedDictType) or not isinstance(right, TypedDictType)

    if isinstance(left, TypedDictType):
        _, other = left, right
    elif isinstance(right, TypedDictType):
        _, other = right, left
    else:
        return False
    return isinstance(other, Instance) and other.type.has_base("typing.Mapping")


</t>
<t tx="ekr.20230831011820.540">class IndexExpr(Expression):
    """Index expression x[y].

    Also wraps type application such as List[int] as a special form.
    """

    @others
</t>
<t tx="ekr.20230831011820.541">__slots__ = ("base", "index", "method_type", "analyzed")

__match_args__ = ("base", "index")

base: Expression
index: Expression
# Inferred __getitem__ method type
method_type: mypy.types.Type | None
# If not None, this is actually semantically a type application
# Class[type, ...] or a type alias initializer.
analyzed: TypeApplication | TypeAliasExpr | None

def __init__(self, base: Expression, index: Expression) -&gt; None:
    super().__init__()
    self.base = base
    self.index = index
    self.method_type = None
    self.analyzed = None

</t>
<t tx="ekr.20230831011820.542">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_index_expr(self)


</t>
<t tx="ekr.20230831011820.543">class UnaryExpr(Expression):
    """Unary operation"""

    @others
</t>
<t tx="ekr.20230831011820.544">__slots__ = ("op", "expr", "method_type")

__match_args__ = ("op", "expr")

op: str  # TODO: Enum?
expr: Expression
# Inferred operator method type
method_type: mypy.types.Type | None

def __init__(self, op: str, expr: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.expr = expr
    self.method_type = None

</t>
<t tx="ekr.20230831011820.545">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_unary_expr(self)


</t>
<t tx="ekr.20230831011820.546">class AssignmentExpr(Expression):
    """Assignment expressions in Python 3.8+, like "a := 2"."""

    @others
</t>
<t tx="ekr.20230831011820.547">__slots__ = ("target", "value")

__match_args__ = ("target", "value")

def __init__(self, target: Expression, value: Expression) -&gt; None:
    super().__init__()
    self.target = target
    self.value = value

</t>
<t tx="ekr.20230831011820.548">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_assignment_expr(self)


</t>
<t tx="ekr.20230831011820.549">class OpExpr(Expression):
    """Binary operation.

    The dot (.), [] and comparison operators have more specific nodes.
    """

    @others
</t>
<t tx="ekr.20230831011820.55">def typed_dict_mapping_overlap(
    left: Type, right: Type, overlapping: Callable[[Type, Type], bool]
) -&gt; bool:
    """Check if a TypedDict type is overlapping with a Mapping.

    The basic logic here consists of two rules:

    * A TypedDict with some required keys is overlapping with Mapping[str, &lt;some type&gt;]
      if and only if every key type is overlapping with &lt;some type&gt;. For example:

      - TypedDict(x=int, y=str) overlaps with Dict[str, Union[str, int]]
      - TypedDict(x=int, y=str) doesn't overlap with Dict[str, int]

      Note that any additional non-required keys can't change the above result.

    * A TypedDict with no required keys overlaps with Mapping[str, &lt;some type&gt;] if and
      only if at least one of key types overlaps with &lt;some type&gt;. For example:

      - TypedDict(x=str, y=str, total=False) overlaps with Dict[str, str]
      - TypedDict(x=str, y=str, total=False) doesn't overlap with Dict[str, int]
      - TypedDict(x=int, y=str, total=False) overlaps with Dict[str, str]

    As usual empty, dictionaries lie in a gray area. In general, List[str] and List[str]
    are considered non-overlapping despite empty list belongs to both. However, List[int]
    and List[&lt;nothing&gt;] are considered overlapping.

    So here we follow the same logic: a TypedDict with no required keys is considered
    non-overlapping with Mapping[str, &lt;some type&gt;], but is considered overlapping with
    Mapping[&lt;nothing&gt;, &lt;nothing&gt;]. This way we avoid false positives for overloads, and also
    avoid false positives for comparisons like SomeTypedDict == {} under --strict-equality.
    """
    left, right = get_proper_types((left, right))
    assert not isinstance(left, TypedDictType) or not isinstance(right, TypedDictType)

    if isinstance(left, TypedDictType):
        assert isinstance(right, Instance)
        typed, other = left, right
    else:
        assert isinstance(left, Instance)
        assert isinstance(right, TypedDictType)
        typed, other = right, left

    mapping = next(base for base in other.type.mro if base.fullname == "typing.Mapping")
    other = map_instance_to_supertype(other, mapping)
    key_type, value_type = get_proper_types(other.args)

    # TODO: is there a cleaner way to get str_type here?
    fallback = typed.as_anonymous().fallback
    str_type = fallback.type.bases[0].args[0]  # typing._TypedDict inherits Mapping[str, object]

    # Special case: a TypedDict with no required keys overlaps with an empty dict.
    if isinstance(key_type, UninhabitedType) and isinstance(value_type, UninhabitedType):
        return not typed.required_keys

    if typed.required_keys:
        if not overlapping(key_type, str_type):
            return False
        return all(overlapping(typed.items[k], value_type) for k in typed.required_keys)
    else:
        if not overlapping(key_type, str_type):
            return False
        non_required = set(typed.items.keys()) - typed.required_keys
        return any(overlapping(typed.items[k], value_type) for k in non_required)
</t>
<t tx="ekr.20230831011820.550">__slots__ = (
    "op",
    "left",
    "right",
    "method_type",
    "right_always",
    "right_unreachable",
    "analyzed",
)

__match_args__ = ("left", "op", "right")

op: str  # TODO: Enum?
left: Expression
right: Expression
# Inferred type for the operator method type (when relevant).
method_type: mypy.types.Type | None
# Per static analysis only: Is the right side going to be evaluated every time?
right_always: bool
# Per static analysis only: Is the right side unreachable?
right_unreachable: bool
# Used for expressions that represent a type "X | Y" in some contexts
analyzed: TypeAliasExpr | None

def __init__(
    self, op: str, left: Expression, right: Expression, analyzed: TypeAliasExpr | None = None
) -&gt; None:
    super().__init__()
    self.op = op
    self.left = left
    self.right = right
    self.method_type = None
    self.right_always = False
    self.right_unreachable = False
    self.analyzed = analyzed

</t>
<t tx="ekr.20230831011820.551">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_op_expr(self)


</t>
<t tx="ekr.20230831011820.552">class ComparisonExpr(Expression):
    """Comparison expression (e.g. a &lt; b &gt; c &lt; d)."""

    @others
</t>
<t tx="ekr.20230831011820.553">__slots__ = ("operators", "operands", "method_types")

__match_args__ = ("operands", "operators")

operators: list[str]
operands: list[Expression]
# Inferred type for the operator methods (when relevant; None for 'is').
method_types: list[mypy.types.Type | None]

def __init__(self, operators: list[str], operands: list[Expression]) -&gt; None:
    super().__init__()
    self.operators = operators
    self.operands = operands
    self.method_types = []

</t>
<t tx="ekr.20230831011820.554">def pairwise(self) -&gt; Iterator[tuple[str, Expression, Expression]]:
    """If this comparison expr is "a &lt; b is c == d", yields the sequence
    ("&lt;", a, b), ("is", b, c), ("==", c, d)
    """
    for i, operator in enumerate(self.operators):
        yield operator, self.operands[i], self.operands[i + 1]

</t>
<t tx="ekr.20230831011820.555">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_comparison_expr(self)


</t>
<t tx="ekr.20230831011820.556">class SliceExpr(Expression):
    """Slice expression (e.g. 'x:y', 'x:', '::2' or ':').

    This is only valid as index in index expressions.
    """

    @others
</t>
<t tx="ekr.20230831011820.557">__slots__ = ("begin_index", "end_index", "stride")

__match_args__ = ("begin_index", "end_index", "stride")

begin_index: Expression | None
end_index: Expression | None
stride: Expression | None

def __init__(
    self,
    begin_index: Expression | None,
    end_index: Expression | None,
    stride: Expression | None,
) -&gt; None:
    super().__init__()
    self.begin_index = begin_index
    self.end_index = end_index
    self.stride = stride

</t>
<t tx="ekr.20230831011820.558">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_slice_expr(self)


</t>
<t tx="ekr.20230831011820.559">class CastExpr(Expression):
    """Cast expression cast(type, expr)."""

    @others
</t>
<t tx="ekr.20230831011820.56">@path mypy
"""Utility for dumping memory usage stats.

This is tailored to mypy and knows (a little) about which list objects are
owned by particular AST nodes, etc.
"""

&lt;&lt; memprofile.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.560">__slots__ = ("expr", "type")

__match_args__ = ("expr", "type")

expr: Expression
type: mypy.types.Type

def __init__(self, expr: Expression, typ: mypy.types.Type) -&gt; None:
    super().__init__()
    self.expr = expr
    self.type = typ

</t>
<t tx="ekr.20230831011820.561">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_cast_expr(self)


</t>
<t tx="ekr.20230831011820.562">class AssertTypeExpr(Expression):
    """Represents a typing.assert_type(expr, type) call."""

    @others
</t>
<t tx="ekr.20230831011820.563">__slots__ = ("expr", "type")

__match_args__ = ("expr", "type")

expr: Expression
type: mypy.types.Type

def __init__(self, expr: Expression, typ: mypy.types.Type) -&gt; None:
    super().__init__()
    self.expr = expr
    self.type = typ

</t>
<t tx="ekr.20230831011820.564">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_assert_type_expr(self)


</t>
<t tx="ekr.20230831011820.565">class RevealExpr(Expression):
    """Reveal type expression reveal_type(expr) or reveal_locals() expression."""

    @others
</t>
<t tx="ekr.20230831011820.566">__slots__ = ("expr", "kind", "local_nodes")

__match_args__ = ("expr", "kind", "local_nodes")

expr: Expression | None
kind: int
local_nodes: list[Var] | None

def __init__(
    self, kind: int, expr: Expression | None = None, local_nodes: list[Var] | None = None
) -&gt; None:
    super().__init__()
    self.expr = expr
    self.kind = kind
    self.local_nodes = local_nodes

</t>
<t tx="ekr.20230831011820.567">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_reveal_expr(self)


</t>
<t tx="ekr.20230831011820.568">class SuperExpr(Expression):
    """Expression super().name"""

    @others
</t>
<t tx="ekr.20230831011820.569">__slots__ = ("name", "info", "call")

__match_args__ = ("name", "call", "info")

name: str
info: TypeInfo | None  # Type that contains this super expression
call: CallExpr  # The expression super(...)

def __init__(self, name: str, call: CallExpr) -&gt; None:
    super().__init__()
    self.name = name
    self.call = call
    self.info = None

</t>
<t tx="ekr.20230831011820.570">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_super_expr(self)


</t>
<t tx="ekr.20230831011820.571">class LambdaExpr(FuncItem, Expression):
    """Lambda expression"""

    @others
</t>
<t tx="ekr.20230831011820.572">__match_args__ = ("arguments", "arg_names", "arg_kinds", "body")

@property
def name(self) -&gt; str:
    return "&lt;lambda&gt;"

</t>
<t tx="ekr.20230831011820.573">def expr(self) -&gt; Expression:
    """Return the expression (the body) of the lambda."""
    ret = self.body.body[-1]
    assert isinstance(ret, ReturnStmt)
    expr = ret.expr
    assert expr is not None  # lambda can't have empty body
    return expr

</t>
<t tx="ekr.20230831011820.574">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_lambda_expr(self)

</t>
<t tx="ekr.20230831011820.575">def is_dynamic(self) -&gt; bool:
    return False


</t>
<t tx="ekr.20230831011820.576">class ListExpr(Expression):
    """List literal expression [...]."""

    @others
</t>
<t tx="ekr.20230831011820.577">__slots__ = ("items",)

__match_args__ = ("items",)

items: list[Expression]

def __init__(self, items: list[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20230831011820.578">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_list_expr(self)


</t>
<t tx="ekr.20230831011820.579">class DictExpr(Expression):
    """Dictionary literal expression {key: value, ...}."""

    @others
</t>
<t tx="ekr.20230831011820.58">from __future__ import annotations

import gc
import sys
from collections import defaultdict
from typing import Dict, Iterable, cast

from mypy.nodes import FakeInfo, Node
from mypy.types import Type
from mypy.util import get_class_descriptors


</t>
<t tx="ekr.20230831011820.580">__slots__ = ("items",)

__match_args__ = ("items",)

items: list[tuple[Expression | None, Expression]]

def __init__(self, items: list[tuple[Expression | None, Expression]]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20230831011820.581">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_dict_expr(self)


</t>
<t tx="ekr.20230831011820.582">class TupleExpr(Expression):
    """Tuple literal expression (..., ...)

    Also lvalue sequences (..., ...) and [..., ...]"""

    @others
</t>
<t tx="ekr.20230831011820.583">__slots__ = ("items",)

__match_args__ = ("items",)

items: list[Expression]

def __init__(self, items: list[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20230831011820.584">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_tuple_expr(self)


</t>
<t tx="ekr.20230831011820.585">class SetExpr(Expression):
    """Set literal expression {value, ...}."""

    @others
</t>
<t tx="ekr.20230831011820.586">__slots__ = ("items",)

__match_args__ = ("items",)

items: list[Expression]

def __init__(self, items: list[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20230831011820.587">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_set_expr(self)


</t>
<t tx="ekr.20230831011820.588">class GeneratorExpr(Expression):
    """Generator expression ... for ... in ... [ for ...  in ... ] [ if ... ]."""

    @others
</t>
<t tx="ekr.20230831011820.589">__slots__ = ("left_expr", "sequences", "condlists", "is_async", "indices")

__match_args__ = ("left_expr", "indices", "sequences", "condlists")

left_expr: Expression
sequences: list[Expression]
condlists: list[list[Expression]]
is_async: list[bool]
indices: list[Lvalue]

def __init__(
    self,
    left_expr: Expression,
    indices: list[Lvalue],
    sequences: list[Expression],
    condlists: list[list[Expression]],
    is_async: list[bool],
) -&gt; None:
    super().__init__()
    self.left_expr = left_expr
    self.sequences = sequences
    self.condlists = condlists
    self.indices = indices
    self.is_async = is_async

</t>
<t tx="ekr.20230831011820.59">def collect_memory_stats() -&gt; tuple[dict[str, int], dict[str, int]]:
    """Return stats about memory use.

    Return a tuple with these items:
      - Dict from object kind to number of instances of that kind
      - Dict from object kind to total bytes used by all instances of that kind
    """
    objs = gc.get_objects()
    find_recursive_objects(objs)

    inferred = {}
    for obj in objs:
        if type(obj) is FakeInfo:
            # Processing these would cause a crash.
            continue
        n = type(obj).__name__
        if hasattr(obj, "__dict__"):
            # Keep track of which class a particular __dict__ is associated with.
            inferred[id(obj.__dict__)] = f"{n} (__dict__)"
        if isinstance(obj, (Node, Type)):  # type: ignore[misc]
            if hasattr(obj, "__dict__"):
                for x in obj.__dict__.values():
                    if isinstance(x, list):
                        # Keep track of which node a list is associated with.
                        inferred[id(x)] = f"{n} (list)"
                    if isinstance(x, tuple):
                        # Keep track of which node a list is associated with.
                        inferred[id(x)] = f"{n} (tuple)"

            for k in get_class_descriptors(type(obj)):
                x = getattr(obj, k, None)
                if isinstance(x, list):
                    inferred[id(x)] = f"{n} (list)"
                if isinstance(x, tuple):
                    inferred[id(x)] = f"{n} (tuple)"

    freqs: dict[str, int] = {}
    memuse: dict[str, int] = {}
    for obj in objs:
        if id(obj) in inferred:
            name = inferred[id(obj)]
        else:
            name = type(obj).__name__
        freqs[name] = freqs.get(name, 0) + 1
        memuse[name] = memuse.get(name, 0) + sys.getsizeof(obj)

    return freqs, memuse


</t>
<t tx="ekr.20230831011820.590">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_generator_expr(self)


</t>
<t tx="ekr.20230831011820.591">class ListComprehension(Expression):
    """List comprehension (e.g. [x + 1 for x in a])"""

    @others
</t>
<t tx="ekr.20230831011820.592">__slots__ = ("generator",)

__match_args__ = ("generator",)

generator: GeneratorExpr

def __init__(self, generator: GeneratorExpr) -&gt; None:
    super().__init__()
    self.generator = generator

</t>
<t tx="ekr.20230831011820.593">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_list_comprehension(self)


</t>
<t tx="ekr.20230831011820.594">class SetComprehension(Expression):
    """Set comprehension (e.g. {x + 1 for x in a})"""

    @others
</t>
<t tx="ekr.20230831011820.595">__slots__ = ("generator",)

__match_args__ = ("generator",)

generator: GeneratorExpr

def __init__(self, generator: GeneratorExpr) -&gt; None:
    super().__init__()
    self.generator = generator

</t>
<t tx="ekr.20230831011820.596">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_set_comprehension(self)


</t>
<t tx="ekr.20230831011820.597">class DictionaryComprehension(Expression):
    """Dictionary comprehension (e.g. {k: v for k, v in a}"""

    @others
</t>
<t tx="ekr.20230831011820.598">__slots__ = ("key", "value", "sequences", "condlists", "is_async", "indices")

__match_args__ = ("key", "value", "indices", "sequences", "condlists")

key: Expression
value: Expression
sequences: list[Expression]
condlists: list[list[Expression]]
is_async: list[bool]
indices: list[Lvalue]

def __init__(
    self,
    key: Expression,
    value: Expression,
    indices: list[Lvalue],
    sequences: list[Expression],
    condlists: list[list[Expression]],
    is_async: list[bool],
) -&gt; None:
    super().__init__()
    self.key = key
    self.value = value
    self.sequences = sequences
    self.condlists = condlists
    self.indices = indices
    self.is_async = is_async

</t>
<t tx="ekr.20230831011820.599">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_dictionary_comprehension(self)


</t>
<t tx="ekr.20230831011820.6">@path mypy
&lt;&lt; maptype.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.60">def print_memory_profile(run_gc: bool = True) -&gt; None:
    if not sys.platform.startswith("win"):
        import resource

        system_memuse = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    else:
        system_memuse = -1  # TODO: Support this on Windows
    if run_gc:
        gc.collect()
    freqs, memuse = collect_memory_stats()
    print("%7s  %7s  %7s  %s" % ("Freq", "Size(k)", "AvgSize", "Type"))
    print("-------------------------------------------")
    totalmem = 0
    i = 0
    for n, mem in sorted(memuse.items(), key=lambda x: -x[1]):
        f = freqs[n]
        if i &lt; 50:
            print("%7d  %7d  %7.0f  %s" % (f, mem // 1024, mem / f, n))
        i += 1
        totalmem += mem
    print()
    print("Mem usage RSS   ", system_memuse // 1024)
    print("Total reachable ", totalmem // 1024)


</t>
<t tx="ekr.20230831011820.600">class ConditionalExpr(Expression):
    """Conditional expression (e.g. x if y else z)"""

    @others
</t>
<t tx="ekr.20230831011820.601">__slots__ = ("cond", "if_expr", "else_expr")

__match_args__ = ("if_expr", "cond", "else_expr")

cond: Expression
if_expr: Expression
else_expr: Expression

def __init__(self, cond: Expression, if_expr: Expression, else_expr: Expression) -&gt; None:
    super().__init__()
    self.cond = cond
    self.if_expr = if_expr
    self.else_expr = else_expr

</t>
<t tx="ekr.20230831011820.602">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_conditional_expr(self)


</t>
<t tx="ekr.20230831011820.603">class TypeApplication(Expression):
    """Type application expr[type, ...]"""

    @others
</t>
<t tx="ekr.20230831011820.604">__slots__ = ("expr", "types")

__match_args__ = ("expr", "types")

expr: Expression
types: list[mypy.types.Type]

def __init__(self, expr: Expression, types: list[mypy.types.Type]) -&gt; None:
    super().__init__()
    self.expr = expr
    self.types = types

</t>
<t tx="ekr.20230831011820.605">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_application(self)


</t>
<t tx="ekr.20230831011820.606"># Variance of a type variable. For example, T in the definition of
# List[T] is invariant, so List[int] is not a subtype of List[object],
# and also List[object] is not a subtype of List[int].
#
# The T in Iterable[T] is covariant, so Iterable[int] is a subtype of
# Iterable[object], but not vice versa.
#
# If T is contravariant in Foo[T], Foo[object] is a subtype of
# Foo[int], but not vice versa.
INVARIANT: Final = 0
COVARIANT: Final = 1
CONTRAVARIANT: Final = 2


class TypeVarLikeExpr(SymbolNode, Expression):
    """Base class for TypeVarExpr, ParamSpecExpr and TypeVarTupleExpr.

    Note that they are constructed by the semantic analyzer.
    """

    @others
</t>
<t tx="ekr.20230831011820.607">__slots__ = ("_name", "_fullname", "upper_bound", "default", "variance")

_name: str
_fullname: str
# Upper bound: only subtypes of upper_bound are valid as values. By default
# this is 'object', meaning no restriction.
upper_bound: mypy.types.Type
# Default: used to resolve the TypeVar if the default is not explicitly given.
# By default this is 'AnyType(TypeOfAny.from_omitted_generics)'. See PEP 696.
default: mypy.types.Type
# Variance of the type variable. Invariant is the default.
# TypeVar(..., covariant=True) defines a covariant type variable.
# TypeVar(..., contravariant=True) defines a contravariant type
# variable.
variance: int

def __init__(
    self,
    name: str,
    fullname: str,
    upper_bound: mypy.types.Type,
    default: mypy.types.Type,
    variance: int = INVARIANT,
) -&gt; None:
    super().__init__()
    self._name = name
    self._fullname = fullname
    self.upper_bound = upper_bound
    self.default = default
    self.variance = variance

</t>
<t tx="ekr.20230831011820.608">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20230831011820.609">@property
def fullname(self) -&gt; str:
    return self._fullname


</t>
<t tx="ekr.20230831011820.61">def find_recursive_objects(objs: list[object]) -&gt; None:
    """Find additional objects referenced by objs and append them to objs.

    We use this since gc.get_objects() does not return objects without pointers
    in them such as strings.
    """
    seen = {id(o) for o in objs}

    def visit(o: object) -&gt; None:
        if id(o) not in seen:
            objs.append(o)
            seen.add(id(o))

    for obj in objs.copy():
        if type(obj) is FakeInfo:
            # Processing these would cause a crash.
            continue
        if type(obj) in (dict, defaultdict):
            for key, val in cast(Dict[object, object], obj).items():
                visit(key)
                visit(val)
        if type(obj) in (list, tuple, set):
            for x in cast(Iterable[object], obj):
                visit(x)
        if hasattr(obj, "__slots__"):
            for base in type.mro(type(obj)):
                for slot in getattr(base, "__slots__", ()):
                    if hasattr(obj, slot):
                        visit(getattr(obj, slot))
</t>
<t tx="ekr.20230831011820.610">class TypeVarExpr(TypeVarLikeExpr):
    """Type variable expression TypeVar(...).

    This is also used to represent type variables in symbol tables.

    A type variable is not valid as a type unless bound in a TypeVarLikeScope.
    That happens within:

     1. a generic class that uses the type variable as a type argument or
     2. a generic function that refers to the type variable in its signature.
    """

    @others
</t>
<t tx="ekr.20230831011820.611">__slots__ = ("values",)

__match_args__ = ("name", "values", "upper_bound", "default")

# Value restriction: only types in the list are valid as values. If the
# list is empty, there is no restriction.
values: list[mypy.types.Type]

def __init__(
    self,
    name: str,
    fullname: str,
    values: list[mypy.types.Type],
    upper_bound: mypy.types.Type,
    default: mypy.types.Type,
    variance: int = INVARIANT,
) -&gt; None:
    super().__init__(name, fullname, upper_bound, default, variance)
    self.values = values

</t>
<t tx="ekr.20230831011820.612">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_var_expr(self)

</t>
<t tx="ekr.20230831011820.613">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TypeVarExpr",
        "name": self._name,
        "fullname": self._fullname,
        "values": [t.serialize() for t in self.values],
        "upper_bound": self.upper_bound.serialize(),
        "default": self.default.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20230831011820.614">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarExpr:
    assert data[".class"] == "TypeVarExpr"
    return TypeVarExpr(
        data["name"],
        data["fullname"],
        [mypy.types.deserialize_type(v) for v in data["values"]],
        mypy.types.deserialize_type(data["upper_bound"]),
        mypy.types.deserialize_type(data["default"]),
        data["variance"],
    )


</t>
<t tx="ekr.20230831011820.615">class ParamSpecExpr(TypeVarLikeExpr):
    @others
</t>
<t tx="ekr.20230831011820.616">__slots__ = ()

__match_args__ = ("name", "upper_bound", "default")

def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_paramspec_expr(self)

</t>
<t tx="ekr.20230831011820.617">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "ParamSpecExpr",
        "name": self._name,
        "fullname": self._fullname,
        "upper_bound": self.upper_bound.serialize(),
        "default": self.default.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20230831011820.618">@classmethod
def deserialize(cls, data: JsonDict) -&gt; ParamSpecExpr:
    assert data[".class"] == "ParamSpecExpr"
    return ParamSpecExpr(
        data["name"],
        data["fullname"],
        mypy.types.deserialize_type(data["upper_bound"]),
        mypy.types.deserialize_type(data["default"]),
        data["variance"],
    )


</t>
<t tx="ekr.20230831011820.619">class TypeVarTupleExpr(TypeVarLikeExpr):
    """Type variable tuple expression TypeVarTuple(...)."""

    @others
</t>
<t tx="ekr.20230831011820.62">@path mypy
"""Message constants for generating error messages during type checking.

Literal messages should be defined as constants in this module so they won't get out of sync
if used in more than one place, and so that they can be easily introspected. These messages are
ultimately consumed by messages.MessageBuilder.fail(). For more non-trivial message generation,
add a method to MessageBuilder and call this instead.
"""

from __future__ import annotations

from typing import Final, NamedTuple

from mypy import errorcodes as codes


@others


# Invalid types
INVALID_TYPE_RAW_ENUM_VALUE: Final = ErrorMessage(
    "Invalid type: try using Literal[{}.{}] instead?", codes.VALID_TYPE
)

# Type checker error message constants
NO_RETURN_VALUE_EXPECTED: Final = ErrorMessage("No return value expected", codes.RETURN_VALUE)
MISSING_RETURN_STATEMENT: Final = ErrorMessage("Missing return statement", codes.RETURN)
EMPTY_BODY_ABSTRACT: Final = ErrorMessage(
    "If the method is meant to be abstract, use @abc.abstractmethod", codes.EMPTY_BODY
)
INVALID_IMPLICIT_RETURN: Final = ErrorMessage("Implicit return in function which does not return")
INCOMPATIBLE_RETURN_VALUE_TYPE: Final = ErrorMessage(
    "Incompatible return value type", codes.RETURN_VALUE
)
RETURN_VALUE_EXPECTED: Final = ErrorMessage("Return value expected", codes.RETURN_VALUE)
NO_RETURN_EXPECTED: Final = ErrorMessage("Return statement in function which does not return")
INVALID_EXCEPTION: Final = ErrorMessage("Exception must be derived from BaseException")
INVALID_EXCEPTION_TYPE: Final = ErrorMessage(
    "Exception type must be derived from BaseException (or be a tuple of exception classes)"
)
INVALID_EXCEPTION_GROUP: Final = ErrorMessage(
    "Exception type in except* cannot derive from BaseExceptionGroup"
)
RETURN_IN_ASYNC_GENERATOR: Final = ErrorMessage(
    '"return" with value in async generator is not allowed'
)
INVALID_RETURN_TYPE_FOR_GENERATOR: Final = ErrorMessage(
    'The return type of a generator function should be "Generator"' " or one of its supertypes"
)
INVALID_RETURN_TYPE_FOR_ASYNC_GENERATOR: Final = ErrorMessage(
    'The return type of an async generator function should be "AsyncGenerator" or one of its '
    "supertypes"
)
YIELD_VALUE_EXPECTED: Final = ErrorMessage("Yield value expected")
INCOMPATIBLE_TYPES: Final = ErrorMessage("Incompatible types")
INCOMPATIBLE_TYPES_IN_ASSIGNMENT: Final = ErrorMessage(
    "Incompatible types in assignment", code=codes.ASSIGNMENT
)
INCOMPATIBLE_TYPES_IN_AWAIT: Final = ErrorMessage('Incompatible types in "await"')
INCOMPATIBLE_REDEFINITION: Final = ErrorMessage("Incompatible redefinition")
INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AENTER: Final = (
    'Incompatible types in "async with" for "__aenter__"'
)
INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AEXIT: Final = (
    'Incompatible types in "async with" for "__aexit__"'
)
INCOMPATIBLE_TYPES_IN_ASYNC_FOR: Final = 'Incompatible types in "async for"'
INVALID_TYPE_FOR_SLOTS: Final = 'Invalid type for "__slots__"'

ASYNC_FOR_OUTSIDE_COROUTINE: Final = '"async for" outside async function'
ASYNC_WITH_OUTSIDE_COROUTINE: Final = '"async with" outside async function'

INCOMPATIBLE_TYPES_IN_YIELD: Final = ErrorMessage('Incompatible types in "yield"')
INCOMPATIBLE_TYPES_IN_YIELD_FROM: Final = ErrorMessage('Incompatible types in "yield from"')
INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION: Final = "Incompatible types in string interpolation"
INCOMPATIBLE_TYPES_IN_CAPTURE: Final = ErrorMessage("Incompatible types in capture pattern")
MUST_HAVE_NONE_RETURN_TYPE: Final = ErrorMessage('The return type of "{}" must be None')
TUPLE_INDEX_OUT_OF_RANGE: Final = ErrorMessage("Tuple index out of range")
INVALID_SLICE_INDEX: Final = ErrorMessage("Slice index must be an integer, SupportsIndex or None")
CANNOT_INFER_LAMBDA_TYPE: Final = ErrorMessage("Cannot infer type of lambda")
CANNOT_ACCESS_INIT: Final = (
    'Accessing "__init__" on an instance is unsound, since instance.__init__ could be from'
    " an incompatible subclass"
)
NON_INSTANCE_NEW_TYPE: Final = ErrorMessage('"__new__" must return a class instance (got {})')
INVALID_NEW_TYPE: Final = ErrorMessage('Incompatible return type for "__new__"')
BAD_CONSTRUCTOR_TYPE: Final = ErrorMessage("Unsupported decorated constructor type")
CANNOT_ASSIGN_TO_METHOD: Final = "Cannot assign to a method"
CANNOT_ASSIGN_TO_TYPE: Final = "Cannot assign to a type"
INCONSISTENT_ABSTRACT_OVERLOAD: Final = ErrorMessage(
    "Overloaded method has both abstract and non-abstract variants"
)
MULTIPLE_OVERLOADS_REQUIRED: Final = ErrorMessage("Single overload definition, multiple required")
READ_ONLY_PROPERTY_OVERRIDES_READ_WRITE: Final = ErrorMessage(
    "Read-only property cannot override read-write property"
)
FORMAT_REQUIRES_MAPPING: Final = "Format requires a mapping"
RETURN_TYPE_CANNOT_BE_CONTRAVARIANT: Final = ErrorMessage(
    "Cannot use a contravariant type variable as return type"
)
FUNCTION_PARAMETER_CANNOT_BE_COVARIANT: Final = ErrorMessage(
    "Cannot use a covariant type variable as a parameter"
)
INCOMPATIBLE_IMPORT_OF: Final = ErrorMessage('Incompatible import of "{}"', code=codes.ASSIGNMENT)
FUNCTION_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a type annotation", codes.NO_UNTYPED_DEF
)
ONLY_CLASS_APPLICATION: Final = ErrorMessage(
    "Type application is only supported for generic classes"
)
RETURN_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a return type annotation", codes.NO_UNTYPED_DEF
)
ARGUMENT_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a type annotation for one or more arguments", codes.NO_UNTYPED_DEF
)
KEYWORD_ARGUMENT_REQUIRES_STR_KEY_TYPE: Final = ErrorMessage(
    'Keyword argument only valid with "str" key type in call to "dict"'
)
ALL_MUST_BE_SEQ_STR: Final = ErrorMessage("Type of __all__ must be {}, not {}")
INVALID_TYPEDDICT_ARGS: Final = ErrorMessage(
    "Expected keyword arguments, {...}, or dict(...) in TypedDict constructor"
)
TYPEDDICT_KEY_MUST_BE_STRING_LITERAL: Final = ErrorMessage(
    "Expected TypedDict key to be string literal"
)
MALFORMED_ASSERT: Final = ErrorMessage("Assertion is always true, perhaps remove parentheses?")
DUPLICATE_TYPE_SIGNATURES: Final = ErrorMessage("Function has duplicate type signatures")
DESCRIPTOR_SET_NOT_CALLABLE: Final = ErrorMessage("{}.__set__ is not callable")
DESCRIPTOR_GET_NOT_CALLABLE: Final = "{}.__get__ is not callable"
MODULE_LEVEL_GETATTRIBUTE: Final = ErrorMessage(
    "__getattribute__ is not valid at the module level"
)
CLASS_VAR_CONFLICTS_SLOTS: Final = '"{}" in __slots__ conflicts with class variable access'
NAME_NOT_IN_SLOTS: Final = ErrorMessage(
    'Trying to assign name "{}" that is not in "__slots__" of type "{}"'
)
TYPE_ALWAYS_TRUE: Final = ErrorMessage(
    "{} which does not implement __bool__ or __len__ "
    "so it could always be true in boolean context",
    code=codes.TRUTHY_BOOL,
)
TYPE_ALWAYS_TRUE_UNIONTYPE: Final = ErrorMessage(
    "{} of which no members implement __bool__ or __len__ "
    "so it could always be true in boolean context",
    code=codes.TRUTHY_BOOL,
)
FUNCTION_ALWAYS_TRUE: Final = ErrorMessage(
    "Function {} could always be true in boolean context", code=codes.TRUTHY_FUNCTION
)
ITERABLE_ALWAYS_TRUE: Final = ErrorMessage(
    "{} which can always be true in boolean context. Consider using {} instead.",
    code=codes.TRUTHY_ITERABLE,
)
NOT_CALLABLE: Final = "{} not callable"
TYPE_MUST_BE_USED: Final = "Value of type {} must be used"

# Generic
GENERIC_INSTANCE_VAR_CLASS_ACCESS: Final = (
    "Access to generic instance variables via class is ambiguous"
)
GENERIC_CLASS_VAR_ACCESS: Final = "Access to generic class variables is ambiguous"
BARE_GENERIC: Final = "Missing type parameters for generic type {}"
IMPLICIT_GENERIC_ANY_BUILTIN: Final = (
    'Implicit generic "Any". Use "{}" and specify generic parameters'
)
INVALID_UNPACK: Final = "{} cannot be unpacked (must be tuple or TypeVarTuple)"
INVALID_UNPACK_POSITION: Final = "Unpack is only valid in a variadic position"

# TypeVar
INCOMPATIBLE_TYPEVAR_VALUE: Final = 'Value of type variable "{}" of {} cannot be {}'
CANNOT_USE_TYPEVAR_AS_EXPRESSION: Final = 'Type variable "{}.{}" cannot be used as an expression'
INVALID_TYPEVAR_AS_TYPEARG: Final = 'Type variable "{}" not valid as type argument value for "{}"'
INVALID_TYPEVAR_ARG_BOUND: Final = 'Type argument {} of "{}" must be a subtype of {}'
INVALID_TYPEVAR_ARG_VALUE: Final = 'Invalid type argument value for "{}"'
TYPEVAR_VARIANCE_DEF: Final = 'TypeVar "{}" may only be a literal bool'
TYPEVAR_ARG_MUST_BE_TYPE: Final = '{} "{}" must be a type'
TYPEVAR_UNEXPECTED_ARGUMENT: Final = 'Unexpected argument to "TypeVar()"'
UNBOUND_TYPEVAR: Final = (
    "A function returning TypeVar should receive at least "
    "one argument containing the same TypeVar"
)

# Super
TOO_MANY_ARGS_FOR_SUPER: Final = ErrorMessage('Too many arguments for "super"')
SUPER_WITH_SINGLE_ARG_NOT_SUPPORTED: Final = ErrorMessage(
    '"super" with a single argument not supported'
)
UNSUPPORTED_ARG_1_FOR_SUPER: Final = ErrorMessage('Unsupported argument 1 for "super"')
UNSUPPORTED_ARG_2_FOR_SUPER: Final = ErrorMessage('Unsupported argument 2 for "super"')
SUPER_VARARGS_NOT_SUPPORTED: Final = ErrorMessage('Varargs not supported with "super"')
SUPER_POSITIONAL_ARGS_REQUIRED: Final = ErrorMessage('"super" only accepts positional arguments')
SUPER_ARG_2_NOT_INSTANCE_OF_ARG_1: Final = ErrorMessage(
    'Argument 2 for "super" not an instance of argument 1'
)
TARGET_CLASS_HAS_NO_BASE_CLASS: Final = ErrorMessage("Target class has no base class")
SUPER_OUTSIDE_OF_METHOD_NOT_SUPPORTED: Final = ErrorMessage(
    "super() outside of a method is not supported"
)
SUPER_ENCLOSING_POSITIONAL_ARGS_REQUIRED: Final = ErrorMessage(
    "super() requires one or more positional arguments in enclosing function"
)

# Self-type
MISSING_OR_INVALID_SELF_TYPE: Final = ErrorMessage(
    "Self argument missing for a non-static method (or an invalid type for self)"
)
ERASED_SELF_TYPE_NOT_SUPERTYPE: Final = ErrorMessage(
    'The erased type of self "{}" is not a supertype of its class "{}"'
)

# Final
CANNOT_INHERIT_FROM_FINAL: Final = ErrorMessage('Cannot inherit from final class "{}"')
DEPENDENT_FINAL_IN_CLASS_BODY: Final = ErrorMessage(
    "Final name declared in class body cannot depend on type variables"
)
CANNOT_ACCESS_FINAL_INSTANCE_ATTR: Final = (
    'Cannot access final instance attribute "{}" on class object'
)
CANNOT_MAKE_DELETABLE_FINAL: Final = ErrorMessage("Deletable attribute cannot be final")

# Enum
ENUM_MEMBERS_ATTR_WILL_BE_OVERRIDEN: Final = ErrorMessage(
    'Assigned "__members__" will be overridden by "Enum" internally'
)

# ClassVar
CANNOT_OVERRIDE_INSTANCE_VAR: Final = ErrorMessage(
    'Cannot override instance variable (previously declared on base class "{}") with class '
    "variable"
)
CANNOT_OVERRIDE_CLASS_VAR: Final = ErrorMessage(
    'Cannot override class variable (previously declared on base class "{}") with instance '
    "variable"
)
CLASS_VAR_WITH_TYPEVARS: Final = "ClassVar cannot contain type variables"
CLASS_VAR_WITH_GENERIC_SELF: Final = "ClassVar cannot contain Self type in generic classes"
CLASS_VAR_OUTSIDE_OF_CLASS: Final = "ClassVar can only be used for assignments in class body"

# Protocol
RUNTIME_PROTOCOL_EXPECTED: Final = ErrorMessage(
    "Only @runtime_checkable protocols can be used with instance and class checks"
)
CANNOT_INSTANTIATE_PROTOCOL: Final = ErrorMessage('Cannot instantiate protocol class "{}"')
TOO_MANY_UNION_COMBINATIONS: Final = ErrorMessage(
    "Not all union combinations were tried because there are too many unions"
)

CONTIGUOUS_ITERABLE_EXPECTED: Final = ErrorMessage("Contiguous iterable with same type expected")
ITERABLE_TYPE_EXPECTED: Final = ErrorMessage("Invalid type '{}' for *expr (iterable expected)")
TYPE_GUARD_POS_ARG_REQUIRED: Final = ErrorMessage("Type guard requires positional argument")

# Match Statement
MISSING_MATCH_ARGS: Final = 'Class "{}" doesn\'t define "__match_args__"'
OR_PATTERN_ALTERNATIVE_NAMES: Final = "Alternative patterns bind different names"
CLASS_PATTERN_GENERIC_TYPE_ALIAS: Final = (
    "Class pattern class must not be a type alias with type parameters"
)
CLASS_PATTERN_TYPE_REQUIRED: Final = 'Expected type in class pattern; found "{}"'
CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS: Final = "Too many positional patterns for class pattern"
CLASS_PATTERN_KEYWORD_MATCHES_POSITIONAL: Final = (
    'Keyword "{}" already matches a positional pattern'
)
CLASS_PATTERN_DUPLICATE_KEYWORD_PATTERN: Final = 'Duplicate keyword pattern "{}"'
CLASS_PATTERN_UNKNOWN_KEYWORD: Final = 'Class "{}" has no attribute "{}"'
CLASS_PATTERN_CLASS_OR_STATIC_METHOD: Final = "Cannot have both classmethod and staticmethod"
MULTIPLE_ASSIGNMENTS_IN_PATTERN: Final = 'Multiple assignments to name "{}" in pattern'
CANNOT_MODIFY_MATCH_ARGS: Final = 'Cannot assign to "__match_args__"'

DATACLASS_FIELD_ALIAS_MUST_BE_LITERAL: Final = (
    '"alias" argument to dataclass field must be a string literal'
)
DATACLASS_POST_INIT_MUST_BE_A_FUNCTION: Final = '"__post_init__" method must be an instance method'

# fastparse
FAILED_TO_MERGE_OVERLOADS: Final = ErrorMessage(
    "Condition can't be inferred, unable to merge overloads"
)
TYPE_IGNORE_WITH_ERRCODE_ON_MODULE: Final = ErrorMessage(
    "type ignore with error code is not supported for modules; "
    'use `# mypy: disable-error-code="{}"`',
    codes.SYNTAX,
)
INVALID_TYPE_IGNORE: Final = ErrorMessage('Invalid "type: ignore" comment', codes.SYNTAX)
TYPE_COMMENT_SYNTAX_ERROR_VALUE: Final = ErrorMessage(
    'Syntax error in type comment "{}"', codes.SYNTAX
)
ELLIPSIS_WITH_OTHER_TYPEARGS: Final = ErrorMessage(
    "Ellipses cannot accompany other argument types in function type signature", codes.SYNTAX
)
TYPE_SIGNATURE_TOO_MANY_ARGS: Final = ErrorMessage(
    "Type signature has too many arguments", codes.SYNTAX
)
TYPE_SIGNATURE_TOO_FEW_ARGS: Final = ErrorMessage(
    "Type signature has too few arguments", codes.SYNTAX
)
ARG_CONSTRUCTOR_NAME_EXPECTED: Final = ErrorMessage("Expected arg constructor name", codes.SYNTAX)
ARG_CONSTRUCTOR_TOO_MANY_ARGS: Final = ErrorMessage(
    "Too many arguments for argument constructor", codes.SYNTAX
)
MULTIPLE_VALUES_FOR_NAME_KWARG: Final = ErrorMessage(
    '"{}" gets multiple values for keyword argument "name"', codes.SYNTAX
)
MULTIPLE_VALUES_FOR_TYPE_KWARG: Final = ErrorMessage(
    '"{}" gets multiple values for keyword argument "type"', codes.SYNTAX
)
ARG_CONSTRUCTOR_UNEXPECTED_ARG: Final = ErrorMessage(
    'Unexpected argument "{}" for argument constructor', codes.SYNTAX
)
ARG_NAME_EXPECTED_STRING_LITERAL: Final = ErrorMessage(
    "Expected string literal for argument name, got {}", codes.SYNTAX
)
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.620">__slots__ = "tuple_fallback"

tuple_fallback: mypy.types.Instance

__match_args__ = ("name", "upper_bound", "default")

def __init__(
    self,
    name: str,
    fullname: str,
    upper_bound: mypy.types.Type,
    tuple_fallback: mypy.types.Instance,
    default: mypy.types.Type,
    variance: int = INVARIANT,
) -&gt; None:
    super().__init__(name, fullname, upper_bound, default, variance)
    self.tuple_fallback = tuple_fallback

</t>
<t tx="ekr.20230831011820.621">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_var_tuple_expr(self)

</t>
<t tx="ekr.20230831011820.622">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TypeVarTupleExpr",
        "name": self._name,
        "fullname": self._fullname,
        "upper_bound": self.upper_bound.serialize(),
        "tuple_fallback": self.tuple_fallback.serialize(),
        "default": self.default.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20230831011820.623">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarTupleExpr:
    assert data[".class"] == "TypeVarTupleExpr"
    return TypeVarTupleExpr(
        data["name"],
        data["fullname"],
        mypy.types.deserialize_type(data["upper_bound"]),
        mypy.types.Instance.deserialize(data["tuple_fallback"]),
        mypy.types.deserialize_type(data["default"]),
        data["variance"],
    )


</t>
<t tx="ekr.20230831011820.624">class TypeAliasExpr(Expression):
    """Type alias expression (rvalue)."""

    @others
</t>
<t tx="ekr.20230831011820.625">__slots__ = ("node",)

__match_args__ = ("node",)

node: TypeAlias

def __init__(self, node: TypeAlias) -&gt; None:
    super().__init__()
    self.node = node

</t>
<t tx="ekr.20230831011820.626">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_alias_expr(self)


</t>
<t tx="ekr.20230831011820.627">class NamedTupleExpr(Expression):
    """Named tuple expression namedtuple(...) or NamedTuple(...)."""

    @others
</t>
<t tx="ekr.20230831011820.628">__slots__ = ("info", "is_typed")

__match_args__ = ("info",)

# The class representation of this named tuple (its tuple_type attribute contains
# the tuple item types)
info: TypeInfo
is_typed: bool  # whether this class was created with typing(_extensions).NamedTuple

def __init__(self, info: TypeInfo, is_typed: bool = False) -&gt; None:
    super().__init__()
    self.info = info
    self.is_typed = is_typed

</t>
<t tx="ekr.20230831011820.629">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_namedtuple_expr(self)


</t>
<t tx="ekr.20230831011820.630">class TypedDictExpr(Expression):
    """Typed dict expression TypedDict(...)."""

    @others
</t>
<t tx="ekr.20230831011820.631">__slots__ = ("info",)

__match_args__ = ("info",)

# The class representation of this typed dict
info: TypeInfo

def __init__(self, info: TypeInfo) -&gt; None:
    super().__init__()
    self.info = info

</t>
<t tx="ekr.20230831011820.632">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_typeddict_expr(self)


</t>
<t tx="ekr.20230831011820.633">class EnumCallExpr(Expression):
    """Named tuple expression Enum('name', 'val1 val2 ...')."""

    @others
</t>
<t tx="ekr.20230831011820.634">__slots__ = ("info", "items", "values")

__match_args__ = ("info", "items", "values")

# The class representation of this enumerated type
info: TypeInfo
# The item names (for debugging)
items: list[str]
values: list[Expression | None]

def __init__(self, info: TypeInfo, items: list[str], values: list[Expression | None]) -&gt; None:
    super().__init__()
    self.info = info
    self.items = items
    self.values = values

</t>
<t tx="ekr.20230831011820.635">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_enum_call_expr(self)


</t>
<t tx="ekr.20230831011820.636">class PromoteExpr(Expression):
    """Ducktype class decorator expression _promote(...)."""

    @others
</t>
<t tx="ekr.20230831011820.637">__slots__ = ("type",)

type: mypy.types.ProperType

def __init__(self, type: mypy.types.ProperType) -&gt; None:
    super().__init__()
    self.type = type

</t>
<t tx="ekr.20230831011820.638">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit__promote_expr(self)


</t>
<t tx="ekr.20230831011820.639">class NewTypeExpr(Expression):
    """NewType expression NewType(...)."""

    @others
</t>
<t tx="ekr.20230831011820.640">__slots__ = ("name", "old_type", "info")

__match_args__ = ("name", "old_type", "info")

name: str
# The base type (the second argument to NewType)
old_type: mypy.types.Type | None
# The synthesized class representing the new type (inherits old_type)
info: TypeInfo | None

def __init__(
    self, name: str, old_type: mypy.types.Type | None, line: int, column: int
) -&gt; None:
    super().__init__(line=line, column=column)
    self.name = name
    self.old_type = old_type
    self.info = None

</t>
<t tx="ekr.20230831011820.641">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_newtype_expr(self)


</t>
<t tx="ekr.20230831011820.642">class AwaitExpr(Expression):
    """Await expression (await ...)."""

    @others
</t>
<t tx="ekr.20230831011820.643">__slots__ = ("expr",)

__match_args__ = ("expr",)

expr: Expression

def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20230831011820.644">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_await_expr(self)


</t>
<t tx="ekr.20230831011820.645"># Constants


class TempNode(Expression):
    """Temporary dummy node used during type checking.

    This node is not present in the original program; it is just an artifact
    of the type checker implementation. It only represents an opaque node with
    some fixed type.
    """

    @others
</t>
<t tx="ekr.20230831011820.646">__slots__ = ("type", "no_rhs")

type: mypy.types.Type
# Is this TempNode used to indicate absence of a right hand side in an annotated assignment?
# (e.g. for 'x: int' the rvalue is TempNode(AnyType(TypeOfAny.special_form), no_rhs=True))
no_rhs: bool

def __init__(
    self, typ: mypy.types.Type, no_rhs: bool = False, *, context: Context | None = None
) -&gt; None:
    """Construct a dummy node; optionally borrow line/column from context object."""
    super().__init__()
    self.type = typ
    self.no_rhs = no_rhs
    if context is not None:
        self.line = context.line
        self.column = context.column

</t>
<t tx="ekr.20230831011820.647">def __repr__(self) -&gt; str:
    return "TempNode:%d(%s)" % (self.line, str(self.type))

</t>
<t tx="ekr.20230831011820.648">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_temp_node(self)


</t>
<t tx="ekr.20230831011820.649"># Special attributes not collected as protocol members by Python 3.12
# See typing._SPECIAL_NAMES
EXCLUDED_PROTOCOL_ATTRIBUTES: Final = frozenset(
    {
        "__abstractmethods__",
        "__annotations__",
        "__dict__",
        "__doc__",
        "__init__",
        "__module__",
        "__new__",
        "__slots__",
        "__subclasshook__",
        "__weakref__",
        "__class_getitem__",  # Since Python 3.9
    }
)


class TypeInfo(SymbolNode):
    """The type structure of a single class.

    Each TypeInfo corresponds one-to-one to a ClassDef, which
    represents the AST of the class.

    In type-theory terms, this is a "type constructor", and if the
    class is generic then it will be a type constructor of higher kind.
    Where the class is used in an actual type, it's in the form of an
    Instance, which amounts to a type application of the tycon to
    the appropriate number of arguments.
    """

    @others
</t>
<t tx="ekr.20230831011820.65">class ErrorMessage(NamedTuple):
    @others
</t>
<t tx="ekr.20230831011820.650">__slots__ = (
    "_fullname",
    "module_name",
    "defn",
    "mro",
    "_mro_refs",
    "bad_mro",
    "is_final",
    "declared_metaclass",
    "metaclass_type",
    "names",
    "is_abstract",
    "is_protocol",
    "runtime_protocol",
    "abstract_attributes",
    "deletable_attributes",
    "slots",
    "assuming",
    "assuming_proper",
    "inferring",
    "is_enum",
    "fallback_to_any",
    "meta_fallback_to_any",
    "type_vars",
    "has_param_spec_type",
    "bases",
    "_promote",
    "tuple_type",
    "special_alias",
    "is_named_tuple",
    "typeddict_type",
    "is_newtype",
    "is_intersection",
    "metadata",
    "alt_promote",
    "has_type_var_tuple_type",
    "type_var_tuple_prefix",
    "type_var_tuple_suffix",
    "self_type",
    "dataclass_transform_spec",
)

_fullname: str  # Fully qualified name
# Fully qualified name for the module this type was defined in. This
# information is also in the fullname, but is harder to extract in the
# case of nested class definitions.
module_name: str
defn: ClassDef  # Corresponding ClassDef
# Method Resolution Order: the order of looking up attributes. The first
# value always to refers to this class.
mro: list[TypeInfo]
# Used to stash the names of the mro classes temporarily between
# deserialization and fixup. See deserialize() for why.
_mro_refs: list[str] | None
bad_mro: bool  # Could not construct full MRO
is_final: bool

declared_metaclass: mypy.types.Instance | None
metaclass_type: mypy.types.Instance | None

names: SymbolTable  # Names defined directly in this type
is_abstract: bool  # Does the class have any abstract attributes?
is_protocol: bool  # Is this a protocol class?
runtime_protocol: bool  # Does this protocol support isinstance checks?
# List of names of abstract attributes together with their abstract status.
# The abstract status must be one of `NOT_ABSTRACT`, `IS_ABSTRACT`, `IMPLICITLY_ABSTRACT`.
abstract_attributes: list[tuple[str, int]]
deletable_attributes: list[str]  # Used by mypyc only
# Does this type have concrete `__slots__` defined?
# If class does not have `__slots__` defined then it is `None`,
# if it has empty `__slots__` then it is an empty set.
slots: set[str] | None

# The attributes 'assuming' and 'assuming_proper' represent structural subtype matrices.
#
# In languages with structural subtyping, one can keep a global subtype matrix like this:
#   . A B C .
#   A 1 0 0
#   B 1 1 1
#   C 1 0 1
#   .
# where 1 indicates that the type in corresponding row is a subtype of the type
# in corresponding column. This matrix typically starts filled with all 1's and
# a typechecker tries to "disprove" every subtyping relation using atomic (or nominal) types.
# However, we don't want to keep this huge global state. Instead, we keep the subtype
# information in the form of list of pairs (subtype, supertype) shared by all Instances
# with given supertype's TypeInfo. When we enter a subtype check we push a pair in this list
# thus assuming that we started with 1 in corresponding matrix element. Such algorithm allows
# to treat recursive and mutually recursive protocols and other kinds of complex situations.
#
# If concurrent/parallel type checking will be added in future,
# then there should be one matrix per thread/process to avoid false negatives
# during the type checking phase.
assuming: list[tuple[mypy.types.Instance, mypy.types.Instance]]
assuming_proper: list[tuple[mypy.types.Instance, mypy.types.Instance]]
# Ditto for temporary 'inferring' stack of recursive constraint inference.
# It contains Instances of protocol types that appeared as an argument to
# constraints.infer_constraints(). We need 'inferring' to avoid infinite recursion for
# recursive and mutually recursive protocols.
#
# We make 'assuming' and 'inferring' attributes here instead of passing they as kwargs,
# since this would require to pass them in many dozens of calls. In particular,
# there is a dependency infer_constraint -&gt; is_subtype -&gt; is_callable_subtype -&gt;
# -&gt; infer_constraints.
inferring: list[mypy.types.Instance]
# 'inferring' and 'assuming' can't be made sets, since we need to use
# is_same_type to correctly treat unions.

# Classes inheriting from Enum shadow their true members with a __getattr__, so we
# have to treat them as a special case.
is_enum: bool
# If true, any unknown attributes should have type 'Any' instead
# of generating a type error.  This would be true if there is a
# base class with type 'Any', but other use cases may be
# possible. This is similar to having __getattr__ that returns Any
# (and __setattr__), but without the __getattr__ method.
fallback_to_any: bool

# Same as above but for cases where metaclass has type Any. This will suppress
# all attribute errors only for *class object* access.
meta_fallback_to_any: bool

# Information related to type annotations.

# Generic type variable names (full names)
type_vars: list[str]

# Whether this class has a ParamSpec type variable
has_param_spec_type: bool

# Direct base classes.
bases: list[mypy.types.Instance]

# Another type which this type will be treated as a subtype of,
# even though it's not a subclass in Python.  The non-standard
# `@_promote` decorator introduces this, and there are also
# several builtin examples, in particular `int` -&gt; `float`.
_promote: list[mypy.types.ProperType]

# This is used for promoting native integer types such as 'i64' to
# 'int'. (_promote is used for the other direction.) This only
# supports one-step promotions (e.g., i64 -&gt; int, not
# i64 -&gt; int -&gt; float, and this isn't used to promote in joins.
#
# This results in some unintuitive results, such as that even
# though i64 is compatible with int and int is compatible with
# float, i64 is *not* compatible with float.
alt_promote: mypy.types.Instance | None

# Representation of a Tuple[...] base class, if the class has any
# (e.g., for named tuples). If this is not None, the actual Type
# object used for this class is not an Instance but a TupleType;
# the corresponding Instance is set as the fallback type of the
# tuple type.
tuple_type: mypy.types.TupleType | None

# Is this a named tuple type?
is_named_tuple: bool

# If this class is defined by the TypedDict type constructor,
# then this is not None.
typeddict_type: mypy.types.TypedDictType | None

# Is this a newtype type?
is_newtype: bool

# Is this a synthesized intersection type?
is_intersection: bool

# This is a dictionary that will be serialized and un-serialized as is.
# It is useful for plugins to add their data to save in the cache.
metadata: dict[str, JsonDict]

# Store type alias representing this type (for named tuples and TypedDicts).
# Although definitions of these types are stored in symbol tables as TypeInfo,
# when a type analyzer will find them, it should construct a TupleType, or
# a TypedDict type. However, we can't use the plain types, since if the definition
# is recursive, this will create an actual recursive structure of types (i.e. as
# internal Python objects) causing infinite recursions everywhere during type checking.
# To overcome this, we create a TypeAlias node, that will point to these types.
# We store this node in the `special_alias` attribute, because it must be the same node
# in case we are doing multiple semantic analysis passes.
special_alias: TypeAlias | None

# Shared type variable for typing.Self in this class (if used, otherwise None).
self_type: mypy.types.TypeVarType | None

# Added if the corresponding class is directly decorated with `typing.dataclass_transform`
dataclass_transform_spec: DataclassTransformSpec | None

FLAGS: Final = [
    "is_abstract",
    "is_enum",
    "fallback_to_any",
    "meta_fallback_to_any",
    "is_named_tuple",
    "is_newtype",
    "is_protocol",
    "runtime_protocol",
    "is_final",
    "is_intersection",
]

def __init__(self, names: SymbolTable, defn: ClassDef, module_name: str) -&gt; None:
    """Initialize a TypeInfo."""
    super().__init__()
    self._fullname = defn.fullname
    self.names = names
    self.defn = defn
    self.module_name = module_name
    self.type_vars = []
    self.has_param_spec_type = False
    self.has_type_var_tuple_type = False
    self.bases = []
    self.mro = []
    self._mro_refs = None
    self.bad_mro = False
    self.declared_metaclass = None
    self.metaclass_type = None
    self.is_abstract = False
    self.abstract_attributes = []
    self.deletable_attributes = []
    self.slots = None
    self.assuming = []
    self.assuming_proper = []
    self.inferring = []
    self.is_protocol = False
    self.runtime_protocol = False
    self.type_var_tuple_prefix: int | None = None
    self.type_var_tuple_suffix: int | None = None
    self.add_type_vars()
    self.is_final = False
    self.is_enum = False
    self.fallback_to_any = False
    self.meta_fallback_to_any = False
    self._promote = []
    self.alt_promote = None
    self.tuple_type = None
    self.special_alias = None
    self.is_named_tuple = False
    self.typeddict_type = None
    self.is_newtype = False
    self.is_intersection = False
    self.metadata = {}
    self.self_type = None
    self.dataclass_transform_spec = None

</t>
<t tx="ekr.20230831011820.651">def add_type_vars(self) -&gt; None:
    self.has_type_var_tuple_type = False
    if self.defn.type_vars:
        for i, vd in enumerate(self.defn.type_vars):
            if isinstance(vd, mypy.types.ParamSpecType):
                self.has_param_spec_type = True
            if isinstance(vd, mypy.types.TypeVarTupleType):
                assert not self.has_type_var_tuple_type
                self.has_type_var_tuple_type = True
                self.type_var_tuple_prefix = i
                self.type_var_tuple_suffix = len(self.defn.type_vars) - i - 1
            self.type_vars.append(vd.name)
    assert not (
        self.has_param_spec_type and self.has_type_var_tuple_type
    ), "Mixing type var tuples and param specs not supported yet"

</t>
<t tx="ekr.20230831011820.652">@property
def name(self) -&gt; str:
    """Short name."""
    return self.defn.name

</t>
<t tx="ekr.20230831011820.653">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20230831011820.654">def is_generic(self) -&gt; bool:
    """Is the type generic (i.e. does it have type variables)?"""
    return len(self.type_vars) &gt; 0

</t>
<t tx="ekr.20230831011820.655">def get(self, name: str) -&gt; SymbolTableNode | None:
    for cls in self.mro:
        n = cls.names.get(name)
        if n:
            return n
    return None

</t>
<t tx="ekr.20230831011820.656">def get_containing_type_info(self, name: str) -&gt; TypeInfo | None:
    for cls in self.mro:
        if name in cls.names:
            return cls
    return None

</t>
<t tx="ekr.20230831011820.657">@property
def protocol_members(self) -&gt; list[str]:
    # Protocol members are names of all attributes/methods defined in a protocol
    # and in all its supertypes (except for 'object').
    members: set[str] = set()
    assert self.mro, "This property can be only accessed after MRO is (re-)calculated"
    for base in self.mro[:-1]:  # we skip "object" since everyone implements it
        if base.is_protocol:
            for name, node in base.names.items():
                if isinstance(node.node, (TypeAlias, TypeVarExpr, MypyFile)):
                    # These are auxiliary definitions (and type aliases are prohibited).
                    continue
                if name in EXCLUDED_PROTOCOL_ATTRIBUTES:
                    continue
                members.add(name)
    return sorted(list(members))

</t>
<t tx="ekr.20230831011820.658">def __getitem__(self, name: str) -&gt; SymbolTableNode:
    n = self.get(name)
    if n:
        return n
    else:
        raise KeyError(name)

</t>
<t tx="ekr.20230831011820.659">def __repr__(self) -&gt; str:
    return f"&lt;TypeInfo {self.fullname}&gt;"

</t>
<t tx="ekr.20230831011820.66">value: str
code: codes.ErrorCode | None = None

def format(self, *args: object, **kwargs: object) -&gt; ErrorMessage:
    return ErrorMessage(self.value.format(*args, **kwargs), code=self.code)

</t>
<t tx="ekr.20230831011820.660">def __bool__(self) -&gt; bool:
    # We defined this here instead of just overriding it in
    # FakeInfo so that mypyc can generate a direct call instead of
    # using the generic bool handling.
    return not isinstance(self, FakeInfo)

</t>
<t tx="ekr.20230831011820.661">def has_readable_member(self, name: str) -&gt; bool:
    return self.get(name) is not None

</t>
<t tx="ekr.20230831011820.662">def get_method(self, name: str) -&gt; FuncBase | Decorator | None:
    for cls in self.mro:
        if name in cls.names:
            node = cls.names[name].node
            if isinstance(node, FuncBase):
                return node
            elif isinstance(node, Decorator):  # Two `if`s make `mypyc` happy
                return node
            else:
                return None
    return None

</t>
<t tx="ekr.20230831011820.663">def calculate_metaclass_type(self) -&gt; mypy.types.Instance | None:
    declared = self.declared_metaclass
    if declared is not None and not declared.type.has_base("builtins.type"):
        return declared
    if self._fullname == "builtins.type":
        return mypy.types.Instance(self, [])
    candidates = [
        s.declared_metaclass
        for s in self.mro
        if s.declared_metaclass is not None and s.declared_metaclass.type is not None
    ]
    for c in candidates:
        if all(other.type in c.type.mro for other in candidates):
            return c
    return None

</t>
<t tx="ekr.20230831011820.664">def is_metaclass(self) -&gt; bool:
    return (
        self.has_base("builtins.type")
        or self.fullname == "abc.ABCMeta"
        or self.fallback_to_any
    )

</t>
<t tx="ekr.20230831011820.665">def has_base(self, fullname: str) -&gt; bool:
    """Return True if type has a base type with the specified name.

    This can be either via extension or via implementation.
    """
    for cls in self.mro:
        if cls.fullname == fullname:
            return True
    return False

</t>
<t tx="ekr.20230831011820.666">def direct_base_classes(self) -&gt; list[TypeInfo]:
    """Return a direct base classes.

    Omit base classes of other base classes.
    """
    return [base.type for base in self.bases]

</t>
<t tx="ekr.20230831011820.667">def update_tuple_type(self, typ: mypy.types.TupleType) -&gt; None:
    """Update tuple_type and special_alias as needed."""
    self.tuple_type = typ
    alias = TypeAlias.from_tuple_type(self)
    if not self.special_alias:
        self.special_alias = alias
    else:
        self.special_alias.target = alias.target

</t>
<t tx="ekr.20230831011820.668">def update_typeddict_type(self, typ: mypy.types.TypedDictType) -&gt; None:
    """Update typeddict_type and special_alias as needed."""
    self.typeddict_type = typ
    alias = TypeAlias.from_typeddict_type(self)
    if not self.special_alias:
        self.special_alias = alias
    else:
        self.special_alias.target = alias.target

</t>
<t tx="ekr.20230831011820.669">def __str__(self) -&gt; str:
    """Return a string representation of the type.

    This includes the most important information about the type.
    """
    options = Options()
    return self.dump(
        str_conv=mypy.strconv.StrConv(options=options),
        type_str_conv=mypy.types.TypeStrVisitor(options=options),
    )

</t>
<t tx="ekr.20230831011820.67">def with_additional_msg(self, info: str) -&gt; ErrorMessage:
    return ErrorMessage(self.value + info, code=self.code)
</t>
<t tx="ekr.20230831011820.670">def dump(
    self, str_conv: mypy.strconv.StrConv, type_str_conv: mypy.types.TypeStrVisitor
) -&gt; str:
    """Return a string dump of the contents of the TypeInfo."""

    base: str = ""

    def type_str(typ: mypy.types.Type) -&gt; str:
        return typ.accept(type_str_conv)

    head = "TypeInfo" + str_conv.format_id(self)
    if self.bases:
        base = f"Bases({', '.join(type_str(base) for base in self.bases)})"
    mro = "Mro({})".format(
        ", ".join(item.fullname + str_conv.format_id(item) for item in self.mro)
    )
    names = []
    for name in sorted(self.names):
        description = name + str_conv.format_id(self.names[name].node)
        node = self.names[name].node
        if isinstance(node, Var) and node.type:
            description += f" ({type_str(node.type)})"
        names.append(description)
    items = [f"Name({self.fullname})", base, mro, ("Names", names)]
    if self.declared_metaclass:
        items.append(f"DeclaredMetaclass({type_str(self.declared_metaclass)})")
    if self.metaclass_type:
        items.append(f"MetaclassType({type_str(self.metaclass_type)})")
    return mypy.strconv.dump_tagged(items, head, str_conv=str_conv)

</t>
<t tx="ekr.20230831011820.671">def serialize(self) -&gt; JsonDict:
    # NOTE: This is where all ClassDefs originate, so there shouldn't be duplicates.
    data = {
        ".class": "TypeInfo",
        "module_name": self.module_name,
        "fullname": self.fullname,
        "names": self.names.serialize(self.fullname),
        "defn": self.defn.serialize(),
        "abstract_attributes": self.abstract_attributes,
        "type_vars": self.type_vars,
        "has_param_spec_type": self.has_param_spec_type,
        "bases": [b.serialize() for b in self.bases],
        "mro": [c.fullname for c in self.mro],
        "_promote": [p.serialize() for p in self._promote],
        "alt_promote": None if self.alt_promote is None else self.alt_promote.serialize(),
        "declared_metaclass": (
            None if self.declared_metaclass is None else self.declared_metaclass.serialize()
        ),
        "metaclass_type": None
        if self.metaclass_type is None
        else self.metaclass_type.serialize(),
        "tuple_type": None if self.tuple_type is None else self.tuple_type.serialize(),
        "typeddict_type": None
        if self.typeddict_type is None
        else self.typeddict_type.serialize(),
        "flags": get_flags(self, TypeInfo.FLAGS),
        "metadata": self.metadata,
        "slots": list(sorted(self.slots)) if self.slots is not None else None,
        "deletable_attributes": self.deletable_attributes,
        "self_type": self.self_type.serialize() if self.self_type is not None else None,
        "dataclass_transform_spec": (
            self.dataclass_transform_spec.serialize()
            if self.dataclass_transform_spec is not None
            else None
        ),
    }
    return data

</t>
<t tx="ekr.20230831011820.672">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeInfo:
    names = SymbolTable.deserialize(data["names"])
    defn = ClassDef.deserialize(data["defn"])
    module_name = data["module_name"]
    ti = TypeInfo(names, defn, module_name)
    ti._fullname = data["fullname"]
    # TODO: Is there a reason to reconstruct ti.subtypes?
    ti.abstract_attributes = [(attr[0], attr[1]) for attr in data["abstract_attributes"]]
    ti.type_vars = data["type_vars"]
    ti.has_param_spec_type = data["has_param_spec_type"]
    ti.bases = [mypy.types.Instance.deserialize(b) for b in data["bases"]]
    _promote = []
    for p in data["_promote"]:
        t = mypy.types.deserialize_type(p)
        assert isinstance(t, mypy.types.ProperType)
        _promote.append(t)
    ti._promote = _promote
    ti.alt_promote = (
        None
        if data["alt_promote"] is None
        else mypy.types.Instance.deserialize(data["alt_promote"])
    )
    ti.declared_metaclass = (
        None
        if data["declared_metaclass"] is None
        else mypy.types.Instance.deserialize(data["declared_metaclass"])
    )
    ti.metaclass_type = (
        None
        if data["metaclass_type"] is None
        else mypy.types.Instance.deserialize(data["metaclass_type"])
    )
    # NOTE: ti.mro will be set in the fixup phase based on these
    # names.  The reason we need to store the mro instead of just
    # recomputing it from base classes has to do with a subtle
    # point about fine-grained incremental: the cache files might
    # not be loaded until after a class in the mro has changed its
    # bases, which causes the mro to change. If we recomputed our
    # mro, we would compute the *new* mro, which leaves us with no
    # way to detect that the mro has changed! Thus we need to make
    # sure to load the original mro so that once the class is
    # rechecked, it can tell that the mro has changed.
    ti._mro_refs = data["mro"]
    ti.tuple_type = (
        None
        if data["tuple_type"] is None
        else mypy.types.TupleType.deserialize(data["tuple_type"])
    )
    ti.typeddict_type = (
        None
        if data["typeddict_type"] is None
        else mypy.types.TypedDictType.deserialize(data["typeddict_type"])
    )
    ti.metadata = data["metadata"]
    ti.slots = set(data["slots"]) if data["slots"] is not None else None
    ti.deletable_attributes = data["deletable_attributes"]
    set_flags(ti, data["flags"])
    st = data["self_type"]
    ti.self_type = mypy.types.TypeVarType.deserialize(st) if st is not None else None
    if data.get("dataclass_transform_spec") is not None:
        ti.dataclass_transform_spec = DataclassTransformSpec.deserialize(
            data["dataclass_transform_spec"]
        )
    return ti


</t>
<t tx="ekr.20230831011820.673">class FakeInfo(TypeInfo):
    @others
</t>
<t tx="ekr.20230831011820.674">__slots__ = ("msg",)

# types.py defines a single instance of this class, called types.NOT_READY.
# This instance is used as a temporary placeholder in the process of de-serialization
# of 'Instance' types. The de-serialization happens in two steps: In the first step,
# Instance.type is set to NOT_READY. In the second step (in fixup.py) it is replaced by
# an actual TypeInfo. If you see the assertion error below, then most probably something
# went wrong during the second step and an 'Instance' that raised this error was not fixed.
# Note:
# 'None' is not used as a dummy value for two reasons:
# 1. This will require around 80-100 asserts to make 'mypy --strict-optional mypy'
#    pass cleanly.
# 2. If NOT_READY value is accidentally used somewhere, it will be obvious where the value
#    is from, whereas a 'None' value could come from anywhere.
#
# Additionally, this serves as a more general-purpose placeholder
# for missing TypeInfos in a number of places where the excuses
# for not being Optional are a little weaker.
#
# TypeInfo defines a __bool__ method that returns False for FakeInfo
# so that it can be conveniently tested against in the same way that it
# would be if things were properly optional.
def __init__(self, msg: str) -&gt; None:
    self.msg = msg

</t>
<t tx="ekr.20230831011820.675">def __getattribute__(self, attr: str) -&gt; type:
    # Handle __class__ so that isinstance still works...
    if attr == "__class__":
        return object.__getattribute__(self, attr)  # type: ignore[no-any-return]
    raise AssertionError(object.__getattribute__(self, "msg"))


</t>
<t tx="ekr.20230831011820.676">VAR_NO_INFO: Final[TypeInfo] = FakeInfo("Var is lacking info")
CLASSDEF_NO_INFO: Final[TypeInfo] = FakeInfo("ClassDef is lacking info")
FUNC_NO_INFO: Final[TypeInfo] = FakeInfo("FuncBase for non-methods lack info")


class TypeAlias(SymbolNode):
    """
    A symbol node representing a type alias.

    Type alias is a static concept, in contrast to variables with types
    like Type[...]. Namely:
        * type aliases
            - can be used in type context (annotations)
            - cannot be re-assigned
        * variables with type Type[...]
            - cannot be used in type context
            - but can be re-assigned

    An alias can be defined only by an assignment to a name (not any other lvalues).

    Such assignment defines an alias by default. To define a variable,
    an explicit Type[...] annotation is required. As an exception,
    at non-global scope non-subscripted rvalue creates a variable even without
    an annotation. This exception exists to accommodate the common use case of
    class-valued attributes. See SemanticAnalyzerPass2.check_and_set_up_type_alias
    for details.

    Aliases can be generic. We use bound type variables for generic aliases, similar
    to classes. Essentially, type aliases work as macros that expand textually.
    The definition and expansion rules are following:

        1. An alias targeting a generic class without explicit variables act as
        the given class (this doesn't apply to TypedDict, Tuple and Callable, which
        are not proper classes but special type constructors):

            A = List
            AA = List[Any]

            x: A  # same as List[Any]
            x: A[int]  # same as List[int]

            x: AA  # same as List[Any]
            x: AA[int]  # Error!

            C = Callable  # Same as Callable[..., Any]
            T = Tuple  # Same as Tuple[Any, ...]

        2. An alias using explicit type variables in its rvalue expects
        replacements (type arguments) for these variables. If missing, they
        are treated as Any, like for other generics:

            B = List[Tuple[T, T]]

            x: B  # same as List[Tuple[Any, Any]]
            x: B[int]  # same as List[Tuple[int, int]]

            def f(x: B[T]) -&gt; T: ...  # without T, Any would be used here

        3. An alias can be defined using another aliases. In the definition
        rvalue the Any substitution doesn't happen for top level unsubscripted
        generic classes:

            A = List
            B = A  # here A is expanded to List, _not_ List[Any],
                   # to match the Python runtime behaviour
            x: B[int]  # same as List[int]
            C = List[A]  # this expands to List[List[Any]]

            AA = List[T]
            D = AA  # here AA expands to List[Any]
            x: D[int]  # Error!

    Note: the fact that we support aliases like `A = List` means that the target
    type will be initially an instance type with wrong number of type arguments.
    Such instances are all fixed either during or after main semantic analysis passes.
    We therefore store the difference between `List` and `List[Any]` rvalues (targets)
    using the `no_args` flag. See also TypeAliasExpr.no_args.

    Meaning of other fields:

    target: The target type. For generic aliases contains bound type variables
        as nested types (currently TypeVar and ParamSpec are supported).
    _fullname: Qualified name of this type alias. This is used in particular
        to track fine grained dependencies from aliases.
    alias_tvars: Type variables used to define this alias.
    normalized: Used to distinguish between `A = List`, and `A = list`. Both
        are internally stored using `builtins.list` (because `typing.List` is
        itself an alias), while the second cannot be subscripted because of
        Python runtime limitation.
    line and column: Line and column on the original alias definition.
    eager: If True, immediately expand alias when referred to (useful for aliases
        within functions that can't be looked up from the symbol table)
    """

    @others
</t>
<t tx="ekr.20230831011820.677">__slots__ = (
    "target",
    "_fullname",
    "alias_tvars",
    "no_args",
    "normalized",
    "_is_recursive",
    "eager",
    "tvar_tuple_index",
)

__match_args__ = ("name", "target", "alias_tvars", "no_args")

def __init__(
    self,
    target: mypy.types.Type,
    fullname: str,
    line: int,
    column: int,
    *,
    alias_tvars: list[mypy.types.TypeVarLikeType] | None = None,
    no_args: bool = False,
    normalized: bool = False,
    eager: bool = False,
) -&gt; None:
    self._fullname = fullname
    self.target = target
    if alias_tvars is None:
        alias_tvars = []
    self.alias_tvars = alias_tvars
    self.no_args = no_args
    self.normalized = normalized
    # This attribute is manipulated by TypeAliasType. If non-None,
    # it is the cached value.
    self._is_recursive: bool | None = None
    self.eager = eager
    self.tvar_tuple_index = None
    for i, t in enumerate(alias_tvars):
        if isinstance(t, mypy.types.TypeVarTupleType):
            self.tvar_tuple_index = i
    super().__init__(line, column)

</t>
<t tx="ekr.20230831011820.678">@classmethod
def from_tuple_type(cls, info: TypeInfo) -&gt; TypeAlias:
    """Generate an alias to the tuple type described by a given TypeInfo.

    NOTE: this doesn't set type alias type variables (for generic tuple types),
    they must be set by the caller (when fully analyzed).
    """
    assert info.tuple_type
    # TODO: is it possible to refactor this to set the correct type vars here?
    return TypeAlias(
        info.tuple_type.copy_modified(fallback=mypy.types.Instance(info, info.defn.type_vars)),
        info.fullname,
        info.line,
        info.column,
    )

</t>
<t tx="ekr.20230831011820.679">@classmethod
def from_typeddict_type(cls, info: TypeInfo) -&gt; TypeAlias:
    """Generate an alias to the TypedDict type described by a given TypeInfo.

    NOTE: this doesn't set type alias type variables (for generic TypedDicts),
    they must be set by the caller (when fully analyzed).
    """
    assert info.typeddict_type
    # TODO: is it possible to refactor this to set the correct type vars here?
    return TypeAlias(
        info.typeddict_type.copy_modified(
            fallback=mypy.types.Instance(info, info.defn.type_vars)
        ),
        info.fullname,
        info.line,
        info.column,
    )

</t>
<t tx="ekr.20230831011820.68">@path mypy
"""Facilities for generating error messages during type checking.

Don't add any non-trivial message construction logic to the type
checker, as it can compromise clarity and make messages less
consistent. Add such logic to this module instead. Literal messages, including those
with format args, should be defined as constants in mypy.message_registry.

Historically we tried to avoid all message string literals in the type
checker but we are moving away from this convention.
"""

&lt;&lt; messages.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.680">@property
def name(self) -&gt; str:
    return self._fullname.split(".")[-1]

</t>
<t tx="ekr.20230831011820.681">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20230831011820.682">@property
def has_param_spec_type(self) -&gt; bool:
    return any(isinstance(v, mypy.types.ParamSpecType) for v in self.alias_tvars)

</t>
<t tx="ekr.20230831011820.683">def serialize(self) -&gt; JsonDict:
    data: JsonDict = {
        ".class": "TypeAlias",
        "fullname": self._fullname,
        "target": self.target.serialize(),
        "alias_tvars": [v.serialize() for v in self.alias_tvars],
        "no_args": self.no_args,
        "normalized": self.normalized,
        "line": self.line,
        "column": self.column,
    }
    return data

</t>
<t tx="ekr.20230831011820.684">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_type_alias(self)

</t>
<t tx="ekr.20230831011820.685">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeAlias:
    assert data[".class"] == "TypeAlias"
    fullname = data["fullname"]
    alias_tvars = [mypy.types.deserialize_type(v) for v in data["alias_tvars"]]
    assert all(isinstance(t, mypy.types.TypeVarLikeType) for t in alias_tvars)
    target = mypy.types.deserialize_type(data["target"])
    no_args = data["no_args"]
    normalized = data["normalized"]
    line = data["line"]
    column = data["column"]
    return cls(
        target,
        fullname,
        line,
        column,
        alias_tvars=cast(List[mypy.types.TypeVarLikeType], alias_tvars),
        no_args=no_args,
        normalized=normalized,
    )


</t>
<t tx="ekr.20230831011820.686">class PlaceholderNode(SymbolNode):
    """Temporary symbol node that will later become a real SymbolNode.

    These are only present during semantic analysis when using the new
    semantic analyzer. These are created if some essential dependencies
    of a definition are not yet complete.

    A typical use is for names imported from a module which is still
    incomplete (within an import cycle):

      from m import f  # Initially may create PlaceholderNode

    This is particularly important if the imported shadows a name from
    an enclosing scope or builtins:

      from m import int  # Placeholder avoids mixups with builtins.int

    Another case where this is useful is when there is another definition
    or assignment:

      from m import f
      def f() -&gt; None: ...

    In the above example, the presence of PlaceholderNode allows us to
    handle the second definition as a redefinition.

    They are also used to create PlaceholderType instances for types
    that refer to incomplete types. Example:

      class C(Sequence[C]): ...

    We create a PlaceholderNode (with becomes_typeinfo=True) for C so
    that the type C in Sequence[C] can be bound.

    Attributes:

      fullname: Full name of the PlaceholderNode.
      node: AST node that contains the definition that caused this to
          be created. This is useful for tracking order of incomplete definitions
          and for debugging.
      becomes_typeinfo: If True, this refers something that could later
          become a TypeInfo. It can't be used with type variables, in
          particular, as this would cause issues with class type variable
          detection.

    The long-term purpose of placeholder nodes/types is to evolve into
    something that can support general recursive types.
    """

    @others
</t>
<t tx="ekr.20230831011820.687">__slots__ = ("_fullname", "node", "becomes_typeinfo")

def __init__(
    self, fullname: str, node: Node, line: int, *, becomes_typeinfo: bool = False
) -&gt; None:
    self._fullname = fullname
    self.node = node
    self.becomes_typeinfo = becomes_typeinfo
    self.line = line

</t>
<t tx="ekr.20230831011820.688">@property
def name(self) -&gt; str:
    return self._fullname.split(".")[-1]

</t>
<t tx="ekr.20230831011820.689">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20230831011820.690">def serialize(self) -&gt; JsonDict:
    assert False, "PlaceholderNode can't be serialized"

</t>
<t tx="ekr.20230831011820.691">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_placeholder_node(self)


</t>
<t tx="ekr.20230831011820.692">class SymbolTableNode:
    """Description of a name binding in a symbol table.

    These are only used as values in module (global), function (local)
    and class symbol tables (see SymbolTable). The name that is bound is
    the key in SymbolTable.

    Symbol tables don't contain direct references to AST nodes primarily
    because there can be multiple symbol table references to a single
    AST node (due to imports and aliases), and different references can
    behave differently. This class describes the unique properties of
    each reference.

    The most fundamental attribute is 'node', which is the AST node that
    the name refers to.

    The kind is usually one of LDEF, GDEF or MDEF, depending on the scope
    of the definition. These three kinds can usually be used
    interchangeably and the difference between local, global and class
    scopes is mostly descriptive, with no semantic significance.
    However, some tools that consume mypy ASTs may care about these so
    they should be correct.

    Attributes:
        node: AST node of definition. Among others, this can be one of
            FuncDef, Var, TypeInfo, TypeVarExpr or MypyFile -- or None
            for cross_ref that hasn't been fixed up yet.
        kind: Kind of node. Possible values:
               - LDEF: local definition
               - GDEF: global (module-level) definition
               - MDEF: class member definition
               - UNBOUND_IMPORTED: temporary kind for imported names (we
                 don't know the final kind yet)
        module_public: If False, this name won't be imported via
            'from &lt;module&gt; import *'. This has no effect on names within
            classes.
        module_hidden: If True, the name will be never exported (needed for
            stub files)
        cross_ref: For deserialized MypyFile nodes, the referenced module
            name; for other nodes, optionally the name of the referenced object.
        implicit: Was this defined by assignment to self attribute?
        plugin_generated: Was this symbol generated by a plugin?
            (And therefore needs to be removed in aststrip.)
        no_serialize: Do not serialize this node if True. This is used to prevent
            keys in the cache that refer to modules on which this file does not
            depend. Currently this can happen if there is a module not in build
            used e.g. like this:
                import a.b.c # type: ignore
            This will add a submodule symbol to parent module `a` symbol table,
            but `a.b` is _not_ added as its dependency. Therefore, we should
            not serialize these symbols as they may not be found during fixup
            phase, instead they will be re-added during subsequent patch parents
            phase.
            TODO: Refactor build.py to make dependency tracking more transparent
            and/or refactor look-up functions to not require parent patching.

    NOTE: No other attributes should be added to this class unless they
    are shared by all node kinds.
    """

    @others
</t>
<t tx="ekr.20230831011820.693">__slots__ = (
    "kind",
    "node",
    "module_public",
    "module_hidden",
    "cross_ref",
    "implicit",
    "plugin_generated",
    "no_serialize",
)

def __init__(
    self,
    kind: int,
    node: SymbolNode | None,
    module_public: bool = True,
    implicit: bool = False,
    module_hidden: bool = False,
    *,
    plugin_generated: bool = False,
    no_serialize: bool = False,
) -&gt; None:
    self.kind = kind
    self.node = node
    self.module_public = module_public
    self.implicit = implicit
    self.module_hidden = module_hidden
    self.cross_ref: str | None = None
    self.plugin_generated = plugin_generated
    self.no_serialize = no_serialize

</t>
<t tx="ekr.20230831011820.694">@property
def fullname(self) -&gt; str | None:
    if self.node is not None:
        return self.node.fullname
    else:
        return None

</t>
<t tx="ekr.20230831011820.695">@property
def type(self) -&gt; mypy.types.Type | None:
    node = self.node
    if isinstance(node, (Var, SYMBOL_FUNCBASE_TYPES)) and node.type is not None:
        return node.type
    elif isinstance(node, Decorator):
        return node.var.type
    else:
        return None

</t>
<t tx="ekr.20230831011820.696">def copy(self) -&gt; SymbolTableNode:
    new = SymbolTableNode(
        self.kind, self.node, self.module_public, self.implicit, self.module_hidden
    )
    new.cross_ref = self.cross_ref
    return new

</t>
<t tx="ekr.20230831011820.697">def __str__(self) -&gt; str:
    s = f"{node_kinds[self.kind]}/{short_type(self.node)}"
    if isinstance(self.node, SymbolNode):
        s += f" ({self.node.fullname})"
    # Include declared type of variables and functions.
    if self.type is not None:
        s += f" : {self.type}"
    return s

</t>
<t tx="ekr.20230831011820.698">def serialize(self, prefix: str, name: str) -&gt; JsonDict:
    """Serialize a SymbolTableNode.

    Args:
      prefix: full name of the containing module or class; or None
      name: name of this object relative to the containing object
    """
    data: JsonDict = {".class": "SymbolTableNode", "kind": node_kinds[self.kind]}
    if self.module_hidden:
        data["module_hidden"] = True
    if not self.module_public:
        data["module_public"] = False
    if self.implicit:
        data["implicit"] = True
    if self.plugin_generated:
        data["plugin_generated"] = True
    if isinstance(self.node, MypyFile):
        data["cross_ref"] = self.node.fullname
    else:
        assert self.node is not None, f"{prefix}:{name}"
        if prefix is not None:
            fullname = self.node.fullname
            if (
                "." in fullname
                and fullname != prefix + "." + name
                and not (isinstance(self.node, Var) and self.node.from_module_getattr)
            ):
                assert not isinstance(
                    self.node, PlaceholderNode
                ), f"Definition of {fullname} is unexpectedly incomplete"
                data["cross_ref"] = fullname
                return data
        data["node"] = self.node.serialize()
    return data

</t>
<t tx="ekr.20230831011820.699">@classmethod
def deserialize(cls, data: JsonDict) -&gt; SymbolTableNode:
    assert data[".class"] == "SymbolTableNode"
    kind = inverse_node_kinds[data["kind"]]
    if "cross_ref" in data:
        # This will be fixed up later.
        stnode = SymbolTableNode(kind, None)
        stnode.cross_ref = data["cross_ref"]
    else:
        assert "node" in data, data
        node = SymbolNode.deserialize(data["node"])
        stnode = SymbolTableNode(kind, node)
    if "module_hidden" in data:
        stnode.module_hidden = data["module_hidden"]
    if "module_public" in data:
        stnode.module_public = data["module_public"]
    if "implicit" in data:
        stnode.implicit = data["implicit"]
    if "plugin_generated" in data:
        stnode.plugin_generated = data["plugin_generated"]
    return stnode


</t>
<t tx="ekr.20230831011820.7">from __future__ import annotations

from mypy.expandtype import expand_type
from mypy.nodes import TypeInfo
from mypy.types import AnyType, Instance, TupleType, Type, TypeOfAny, TypeVarId, has_type_vars


</t>
<t tx="ekr.20230831011820.70">from __future__ import annotations

import difflib
import itertools
import re
from contextlib import contextmanager
from textwrap import dedent
from typing import Any, Callable, Collection, Final, Iterable, Iterator, List, Sequence, cast

import mypy.typeops
from mypy import errorcodes as codes, message_registry
from mypy.erasetype import erase_type
from mypy.errorcodes import ErrorCode
from mypy.errors import ErrorInfo, Errors, ErrorWatcher
from mypy.nodes import (
    ARG_NAMED,
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    CONTRAVARIANT,
    COVARIANT,
    SYMBOL_FUNCBASE_TYPES,
    ArgKind,
    CallExpr,
    ClassDef,
    Context,
    Expression,
    FuncDef,
    IndexExpr,
    MypyFile,
    NameExpr,
    ReturnStmt,
    StrExpr,
    SymbolNode,
    SymbolTable,
    TypeInfo,
    Var,
    reverse_builtin_aliases,
)
from mypy.operators import op_methods, op_methods_to_symbols
from mypy.options import Options
from mypy.subtypes import (
    IS_CLASS_OR_STATIC,
    IS_CLASSVAR,
    IS_SETTABLE,
    IS_VAR,
    find_member,
    get_member_flags,
    is_same_type,
    is_subtype,
)
from mypy.typeops import separate_union_literals
from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeStrVisitor,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)
from mypy.typetraverser import TypeTraverserVisitor
from mypy.util import plural_s, unmangle

TYPES_FOR_UNIMPORTED_HINTS: Final = {
    "typing.Any",
    "typing.Callable",
    "typing.Dict",
    "typing.Iterable",
    "typing.Iterator",
    "typing.List",
    "typing.Optional",
    "typing.Set",
    "typing.Tuple",
    "typing.TypeVar",
    "typing.Union",
    "typing.cast",
}


ARG_CONSTRUCTOR_NAMES: Final = {
    ARG_POS: "Arg",
    ARG_OPT: "DefaultArg",
    ARG_NAMED: "NamedArg",
    ARG_NAMED_OPT: "DefaultNamedArg",
    ARG_STAR: "VarArg",
    ARG_STAR2: "KwArg",
}


# Map from the full name of a missing definition to the test fixture (under
# test-data/unit/fixtures/) that provides the definition. This is used for
# generating better error messages when running mypy tests only.
SUGGESTED_TEST_FIXTURES: Final = {
    "builtins.set": "set.pyi",
    "builtins.tuple": "tuple.pyi",
    "builtins.bool": "bool.pyi",
    "builtins.Exception": "exception.pyi",
    "builtins.BaseException": "exception.pyi",
    "builtins.isinstance": "isinstancelist.pyi",
    "builtins.property": "property.pyi",
    "builtins.classmethod": "classmethod.pyi",
    "typing._SpecialForm": "typing-medium.pyi",
}

UNSUPPORTED_NUMBERS_TYPES: Final = {
    "numbers.Number",
    "numbers.Complex",
    "numbers.Real",
    "numbers.Rational",
    "numbers.Integral",
}


</t>
<t tx="ekr.20230831011820.700">class SymbolTable(Dict[str, SymbolTableNode]):
    """Static representation of a namespace dictionary.

    This is used for module, class and function namespaces.
    """

    @others
</t>
<t tx="ekr.20230831011820.701">__slots__ = ()

def __str__(self) -&gt; str:
    a: list[str] = []
    for key, value in self.items():
        # Filter out the implicit import of builtins.
        if isinstance(value, SymbolTableNode):
            if (
                value.fullname != "builtins"
                and (value.fullname or "").split(".")[-1] not in implicit_module_attrs
            ):
                a.append("  " + str(key) + " : " + str(value))
        else:
            a.append("  &lt;invalid item&gt;")
    a = sorted(a)
    a.insert(0, "SymbolTable(")
    a[-1] += ")"
    return "\n".join(a)

</t>
<t tx="ekr.20230831011820.702">def copy(self) -&gt; SymbolTable:
    return SymbolTable([(key, node.copy()) for key, node in self.items()])

</t>
<t tx="ekr.20230831011820.703">def serialize(self, fullname: str) -&gt; JsonDict:
    data: JsonDict = {".class": "SymbolTable"}
    for key, value in self.items():
        # Skip __builtins__: it's a reference to the builtins
        # module that gets added to every module by
        # SemanticAnalyzerPass2.visit_file(), but it shouldn't be
        # accessed by users of the module.
        if key == "__builtins__" or value.no_serialize:
            continue
        data[key] = value.serialize(fullname, key)
    return data

</t>
<t tx="ekr.20230831011820.704">@classmethod
def deserialize(cls, data: JsonDict) -&gt; SymbolTable:
    assert data[".class"] == "SymbolTable"
    st = SymbolTable()
    for key, value in data.items():
        if key != ".class":
            st[key] = SymbolTableNode.deserialize(value)
    return st


</t>
<t tx="ekr.20230831011820.705">class DataclassTransformSpec:
    """Specifies how a dataclass-like transform should be applied. The fields here are based on the
    parameters accepted by `typing.dataclass_transform`."""

    @others
</t>
<t tx="ekr.20230831011820.706">__slots__ = (
    "eq_default",
    "order_default",
    "kw_only_default",
    "frozen_default",
    "field_specifiers",
)

def __init__(
    self,
    *,
    eq_default: bool | None = None,
    order_default: bool | None = None,
    kw_only_default: bool | None = None,
    field_specifiers: tuple[str, ...] | None = None,
    # Specified outside of PEP 681:
    # frozen_default was added to CPythonin https://github.com/python/cpython/pull/99958 citing
    # positive discussion in typing-sig
    frozen_default: bool | None = None,
):
    self.eq_default = eq_default if eq_default is not None else True
    self.order_default = order_default if order_default is not None else False
    self.kw_only_default = kw_only_default if kw_only_default is not None else False
    self.frozen_default = frozen_default if frozen_default is not None else False
    self.field_specifiers = field_specifiers if field_specifiers is not None else ()

</t>
<t tx="ekr.20230831011820.707">def serialize(self) -&gt; JsonDict:
    return {
        "eq_default": self.eq_default,
        "order_default": self.order_default,
        "kw_only_default": self.kw_only_default,
        "frozen_default": self.frozen_default,
        "field_specifiers": list(self.field_specifiers),
    }

</t>
<t tx="ekr.20230831011820.708">@classmethod
def deserialize(cls, data: JsonDict) -&gt; DataclassTransformSpec:
    return DataclassTransformSpec(
        eq_default=data.get("eq_default"),
        order_default=data.get("order_default"),
        kw_only_default=data.get("kw_only_default"),
        frozen_default=data.get("frozen_default"),
        field_specifiers=tuple(data.get("field_specifiers", [])),
    )


</t>
<t tx="ekr.20230831011820.709">def get_flags(node: Node, names: list[str]) -&gt; list[str]:
    return [name for name in names if getattr(node, name)]


</t>
<t tx="ekr.20230831011820.71">class MessageBuilder:
    """Helper class for reporting type checker error messages with parameters.

    The methods of this class need to be provided with the context within a
    file; the errors member manages the wider context.

    IDEA: Support a 'verbose mode' that includes full information about types
          in error messages and that may otherwise produce more detailed error
          messages.
    """

    @others
</t>
<t tx="ekr.20230831011820.710">def set_flags(node: Node, flags: list[str]) -&gt; None:
    for name in flags:
        setattr(node, name, True)


</t>
<t tx="ekr.20230831011820.711">def get_member_expr_fullname(expr: MemberExpr) -&gt; str | None:
    """Return the qualified name representation of a member expression.

    Return a string of form foo.bar, foo.bar.baz, or similar, or None if the
    argument cannot be represented in this form.
    """
    initial: str | None = None
    if isinstance(expr.expr, NameExpr):
        initial = expr.expr.name
    elif isinstance(expr.expr, MemberExpr):
        initial = get_member_expr_fullname(expr.expr)
    else:
        return None
    return f"{initial}.{expr.name}"


</t>
<t tx="ekr.20230831011820.712">deserialize_map: Final = {
    key: obj.deserialize
    for key, obj in globals().items()
    if type(obj) is not FakeInfo
    and isinstance(obj, type)
    and issubclass(obj, SymbolNode)
    and obj is not SymbolNode
}


def check_arg_kinds(
    arg_kinds: list[ArgKind], nodes: list[T], fail: Callable[[str, T], None]
) -&gt; None:
    is_var_arg = False
    is_kw_arg = False
    seen_named = False
    seen_opt = False
    for kind, node in zip(arg_kinds, nodes):
        if kind == ARG_POS:
            if is_var_arg or is_kw_arg or seen_named or seen_opt:
                fail(
                    "Required positional args may not appear after default, named or var args",
                    node,
                )
                break
        elif kind == ARG_OPT:
            if is_var_arg or is_kw_arg or seen_named:
                fail("Positional default args may not appear after named or var args", node)
                break
            seen_opt = True
        elif kind == ARG_STAR:
            if is_var_arg or is_kw_arg or seen_named:
                fail("Var args may not appear after named or var args", node)
                break
            is_var_arg = True
        elif kind == ARG_NAMED or kind == ARG_NAMED_OPT:
            seen_named = True
            if is_kw_arg:
                fail("A **kwargs argument must be the last argument", node)
                break
        elif kind == ARG_STAR2:
            if is_kw_arg:
                fail("You may only have one **kwargs argument", node)
                break
            is_kw_arg = True


</t>
<t tx="ekr.20230831011820.713">def check_arg_names(
    names: Sequence[str | None],
    nodes: list[T],
    fail: Callable[[str, T], None],
    description: str = "function definition",
) -&gt; None:
    seen_names: set[str | None] = set()
    for name, node in zip(names, nodes):
        if name is not None and name in seen_names:
            fail(f'Duplicate argument "{name}" in {description}', node)
            break
        seen_names.add(name)


</t>
<t tx="ekr.20230831011820.714">def is_class_var(expr: NameExpr) -&gt; bool:
    """Return whether the expression is ClassVar[...]"""
    if isinstance(expr.node, Var):
        return expr.node.is_classvar
    return False


</t>
<t tx="ekr.20230831011820.715">def is_final_node(node: SymbolNode | None) -&gt; bool:
    """Check whether `node` corresponds to a final attribute."""
    return isinstance(node, (Var, FuncDef, OverloadedFuncDef, Decorator)) and node.is_final


</t>
<t tx="ekr.20230831011820.716">def local_definitions(
    names: SymbolTable, name_prefix: str, info: TypeInfo | None = None
) -&gt; Iterator[Definition]:
    """Iterate over local definitions (not imported) in a symbol table.

    Recursively iterate over class members and nested classes.
    """
    # TODO: What should the name be? Or maybe remove it?
    for name, symnode in names.items():
        shortname = name
        if "-redef" in name:
            # Restore original name from mangled name of multiply defined function
            shortname = name.split("-redef")[0]
        fullname = name_prefix + "." + shortname
        node = symnode.node
        if node and node.fullname == fullname:
            yield fullname, symnode, info
            if isinstance(node, TypeInfo):
                yield from local_definitions(node.names, fullname, node)
</t>
<t tx="ekr.20230831011820.717">@path mypy
"""Information about Python operators"""

from __future__ import annotations

from typing import Final

# Map from binary operator id to related method name (in Python 3).
op_methods: Final = {
    "+": "__add__",
    "-": "__sub__",
    "*": "__mul__",
    "/": "__truediv__",
    "%": "__mod__",
    "divmod": "__divmod__",
    "//": "__floordiv__",
    "**": "__pow__",
    "@": "__matmul__",
    "&amp;": "__and__",
    "|": "__or__",
    "^": "__xor__",
    "&lt;&lt;": "__lshift__",
    "&gt;&gt;": "__rshift__",
    "==": "__eq__",
    "!=": "__ne__",
    "&lt;": "__lt__",
    "&gt;=": "__ge__",
    "&gt;": "__gt__",
    "&lt;=": "__le__",
    "in": "__contains__",
}

op_methods_to_symbols: Final = {v: k for (k, v) in op_methods.items()}

ops_falling_back_to_cmp: Final = {"__ne__", "__eq__", "__lt__", "__le__", "__gt__", "__ge__"}


ops_with_inplace_method: Final = {
    "+",
    "-",
    "*",
    "/",
    "%",
    "//",
    "**",
    "@",
    "&amp;",
    "|",
    "^",
    "&lt;&lt;",
    "&gt;&gt;",
}

inplace_operator_methods: Final = {"__i" + op_methods[op][2:] for op in ops_with_inplace_method}

reverse_op_methods: Final = {
    "__add__": "__radd__",
    "__sub__": "__rsub__",
    "__mul__": "__rmul__",
    "__truediv__": "__rtruediv__",
    "__mod__": "__rmod__",
    "__divmod__": "__rdivmod__",
    "__floordiv__": "__rfloordiv__",
    "__pow__": "__rpow__",
    "__matmul__": "__rmatmul__",
    "__and__": "__rand__",
    "__or__": "__ror__",
    "__xor__": "__rxor__",
    "__lshift__": "__rlshift__",
    "__rshift__": "__rrshift__",
    "__eq__": "__eq__",
    "__ne__": "__ne__",
    "__lt__": "__gt__",
    "__ge__": "__le__",
    "__gt__": "__lt__",
    "__le__": "__ge__",
}

reverse_op_method_names: Final = set(reverse_op_methods.values())

# Suppose we have some class A. When we do A() + A(), Python will only check
# the output of A().__add__(A()) and skip calling the __radd__ method entirely.
# This shortcut is used only for the following methods:
op_methods_that_shortcut: Final = {
    "__add__",
    "__sub__",
    "__mul__",
    "__truediv__",
    "__mod__",
    "__divmod__",
    "__floordiv__",
    "__pow__",
    "__matmul__",
    "__and__",
    "__or__",
    "__xor__",
    "__lshift__",
    "__rshift__",
}

normal_from_reverse_op: Final = {m: n for n, m in reverse_op_methods.items()}
reverse_op_method_set: Final = set(reverse_op_methods.values())

unary_op_methods: Final = {"-": "__neg__", "+": "__pos__", "~": "__invert__"}
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.718">@path mypy
&lt;&lt; options.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.719">from __future__ import annotations

import pprint
import re
import sys
import sysconfig
from typing import Any, Callable, Final, Mapping, Pattern

from mypy import defaults
from mypy.errorcodes import ErrorCode, error_codes
from mypy.util import get_class_descriptors, replace_object_state


</t>
<t tx="ekr.20230831011820.72"># Report errors using this instance. It knows about the current file and
# import context.
errors: Errors

modules: dict[str, MypyFile]

# Hack to deduplicate error messages from union types
_disable_type_names: list[bool]

def __init__(self, errors: Errors, modules: dict[str, MypyFile]) -&gt; None:
    self.errors = errors
    self.options = errors.options
    self.modules = modules
    self._disable_type_names = []

</t>
<t tx="ekr.20230831011820.720">class BuildType:
    STANDARD: Final = 0
    MODULE: Final = 1
    PROGRAM_TEXT: Final = 2


</t>
<t tx="ekr.20230831011820.721">PER_MODULE_OPTIONS: Final = {
    # Please keep this list sorted
    "allow_redefinition",
    "allow_untyped_globals",
    "always_false",
    "always_true",
    "check_untyped_defs",
    "debug_cache",
    "disable_error_code",
    "disabled_error_codes",
    "disallow_any_decorated",
    "disallow_any_explicit",
    "disallow_any_expr",
    "disallow_any_generics",
    "disallow_any_unimported",
    "disallow_incomplete_defs",
    "disallow_subclassing_any",
    "disallow_untyped_calls",
    "disallow_untyped_decorators",
    "disallow_untyped_defs",
    "enable_error_code",
    "enabled_error_codes",
    "extra_checks",
    "follow_imports_for_stubs",
    "follow_imports",
    "ignore_errors",
    "ignore_missing_imports",
    "implicit_optional",
    "implicit_reexport",
    "local_partial_types",
    "mypyc",
    "strict_concatenate",
    "strict_equality",
    "strict_optional",
    "warn_no_return",
    "warn_return_any",
    "warn_unreachable",
    "warn_unused_ignores",
}

OPTIONS_AFFECTING_CACHE: Final = (
    PER_MODULE_OPTIONS
    | {
        "platform",
        "bazel",
        "plugins",
        "disable_bytearray_promotion",
        "disable_memoryview_promotion",
    }
) - {"debug_cache"}

# Features that are currently incomplete/experimental
TYPE_VAR_TUPLE: Final = "TypeVarTuple"
UNPACK: Final = "Unpack"
INCOMPLETE_FEATURES: Final = frozenset((TYPE_VAR_TUPLE, UNPACK))


class Options:
    """Options collected from flags."""

    @others
</t>
<t tx="ekr.20230831011820.722">def __init__(self) -&gt; None:
    # Cache for clone_for_module()
    self._per_module_cache: dict[str, Options] | None = None

    # -- build options --
    self.build_type = BuildType.STANDARD
    self.python_version: tuple[int, int] = sys.version_info[:2]
    # The executable used to search for PEP 561 packages. If this is None,
    # then mypy does not search for PEP 561 packages.
    self.python_executable: str | None = sys.executable

    # When cross compiling to emscripten, we need to rely on MACHDEP because
    # sys.platform is the host build platform, not emscripten.
    MACHDEP = sysconfig.get_config_var("MACHDEP")
    if MACHDEP == "emscripten":
        self.platform = MACHDEP
    else:
        self.platform = sys.platform

    self.custom_typing_module: str | None = None
    self.custom_typeshed_dir: str | None = None
    # The abspath() version of the above, we compute it once as an optimization.
    self.abs_custom_typeshed_dir: str | None = None
    self.mypy_path: list[str] = []
    self.report_dirs: dict[str, str] = {}
    # Show errors in PEP 561 packages/site-packages modules
    self.no_silence_site_packages = False
    self.no_site_packages = False
    self.ignore_missing_imports = False
    # Is ignore_missing_imports set in a per-module section
    self.ignore_missing_imports_per_module = False
    self.follow_imports = "normal"  # normal|silent|skip|error
    # Whether to respect the follow_imports setting even for stub files.
    # Intended to be used for disabling specific stubs.
    self.follow_imports_for_stubs = False
    # PEP 420 namespace packages
    # This allows definitions of packages without __init__.py and allows packages to span
    # multiple directories. This flag affects both import discovery and the association of
    # input files/modules/packages to the relevant file and fully qualified module name.
    self.namespace_packages = True
    # Use current directory and MYPYPATH to determine fully qualified module names of files
    # passed by automatically considering their subdirectories as packages. This is only
    # relevant if namespace packages are enabled, since otherwise examining __init__.py's is
    # sufficient to determine module names for files. As a possible alternative, add a single
    # top-level __init__.py to your packages.
    self.explicit_package_bases = False
    # File names, directory names or subpaths to avoid checking
    self.exclude: list[str] = []

    # disallow_any options
    self.disallow_any_generics = False
    self.disallow_any_unimported = False
    self.disallow_any_expr = False
    self.disallow_any_decorated = False
    self.disallow_any_explicit = False

    # Disallow calling untyped functions from typed ones
    self.disallow_untyped_calls = False

    # Always allow untyped calls for function coming from modules/packages
    # in this list (each item effectively acts as a prefix match)
    self.untyped_calls_exclude: list[str] = []

    # Disallow defining untyped (or incompletely typed) functions
    self.disallow_untyped_defs = False

    # Disallow defining incompletely typed functions
    self.disallow_incomplete_defs = False

    # Type check unannotated functions
    self.check_untyped_defs = False

    # Disallow decorating typed functions with untyped decorators
    self.disallow_untyped_decorators = False

    # Disallow subclassing values of type 'Any'
    self.disallow_subclassing_any = False

    # Also check typeshed for missing annotations
    self.warn_incomplete_stub = False

    # Warn about casting an expression to its inferred type
    self.warn_redundant_casts = False

    # Warn about falling off the end of a function returning non-None
    self.warn_no_return = True

    # Warn about returning objects of type Any when the function is
    # declared with a precise type
    self.warn_return_any = False

    # Warn about unused '# type: ignore' comments
    self.warn_unused_ignores = False

    # Warn about unused '[mypy-&lt;pattern&gt;]'  or '[[tool.mypy.overrides]]' config sections
    self.warn_unused_configs = False

    # Files in which to ignore all non-fatal errors
    self.ignore_errors = False

    # Apply strict None checking
    self.strict_optional = True

    # Show "note: In function "foo":" messages.
    self.show_error_context = False

    # Use nicer output (when possible).
    self.color_output = True
    self.error_summary = True

    # Assume arguments with default values of None are Optional
    self.implicit_optional = False

    # Don't re-export names unless they are imported with `from ... as ...`
    self.implicit_reexport = True

    # Suppress toplevel errors caused by missing annotations
    self.allow_untyped_globals = False

    # Allow variable to be redefined with an arbitrary type in the same block
    # and the same nesting level as the initialization
    self.allow_redefinition = False

    # Prohibit equality, identity, and container checks for non-overlapping types.
    # This makes 1 == '1', 1 in ['1'], and 1 is '1' errors.
    self.strict_equality = False

    # Deprecated, use extra_checks instead.
    self.strict_concatenate = False

    # Enable additional checks that are technically correct but impractical.
    self.extra_checks = False

    # Report an error for any branches inferred to be unreachable as a result of
    # type analysis.
    self.warn_unreachable = False

    # Variable names considered True
    self.always_true: list[str] = []

    # Variable names considered False
    self.always_false: list[str] = []

    # Error codes to disable
    self.disable_error_code: list[str] = []
    self.disabled_error_codes: set[ErrorCode] = set()

    # Error codes to enable
    self.enable_error_code: list[str] = []
    self.enabled_error_codes: set[ErrorCode] = set()

    # Use script name instead of __main__
    self.scripts_are_modules = False

    # Config file name
    self.config_file: str | None = None

    # A filename containing a JSON mapping from filenames to
    # mtime/size/hash arrays, used to avoid having to recalculate
    # source hashes as often.
    self.quickstart_file: str | None = None

    # A comma-separated list of files/directories for mypy to type check;
    # supports globbing
    self.files: list[str] | None = None

    # A list of packages for mypy to type check
    self.packages: list[str] | None = None

    # A list of modules for mypy to type check
    self.modules: list[str] | None = None

    # Write junit.xml to given file
    self.junit_xml: str | None = None

    # Caching and incremental checking options
    self.incremental = True
    self.cache_dir = defaults.CACHE_DIR
    self.sqlite_cache = False
    self.debug_cache = False
    self.skip_version_check = False
    self.skip_cache_mtime_checks = False
    self.fine_grained_incremental = False
    # Include fine-grained dependencies in written cache files
    self.cache_fine_grained = False
    # Read cache files in fine-grained incremental mode (cache must include dependencies)
    self.use_fine_grained_cache = False

    # Run tree.serialize() even if cache generation is disabled
    self.debug_serialize = False

    # Tune certain behaviors when being used as a front-end to mypyc. Set per-module
    # in modules being compiled. Not in the config file or command line.
    self.mypyc = False

    # An internal flag to modify some type-checking logic while
    # running inspections (e.g. don't expand function definitions).
    # Not in the config file or command line.
    self.inspections = False

    # Disable the memory optimization of freeing ASTs when
    # possible. This isn't exposed as a command line option
    # because it is intended for software integrating with
    # mypy. (Like mypyc.)
    self.preserve_asts = False

    # If True, function and class docstrings will be extracted and retained.
    # This isn't exposed as a command line option
    # because it is intended for software integrating with
    # mypy. (Like stubgen.)
    self.include_docstrings = False

    # Paths of user plugins
    self.plugins: list[str] = []

    # Per-module options (raw)
    self.per_module_options: dict[str, dict[str, object]] = {}
    self._glob_options: list[tuple[str, Pattern[str]]] = []
    self.unused_configs: set[str] = set()

    # -- development options --
    self.verbosity = 0  # More verbose messages (for troubleshooting)
    self.pdb = False
    self.show_traceback = False
    self.raise_exceptions = False
    self.dump_type_stats = False
    self.dump_inference_stats = False
    self.dump_build_stats = False
    self.enable_incomplete_features = False  # deprecated
    self.enable_incomplete_feature: list[str] = []
    self.timing_stats: str | None = None
    self.line_checking_stats: str | None = None

    # -- test options --
    # Stop after the semantic analysis phase
    self.semantic_analysis_only = False

    # Use stub builtins fixtures to speed up tests
    self.use_builtins_fixtures = False

    # -- experimental options --
    self.shadow_file: list[list[str]] | None = None
    self.show_column_numbers: bool = False
    self.show_error_end: bool = False
    self.hide_error_codes = False
    self.show_error_code_links = False
    # Use soft word wrap and show trimmed source snippets with error location markers.
    self.pretty = False
    self.dump_graph = False
    self.dump_deps = False
    self.logical_deps = False
    # If True, partial types can't span a module top level and a function
    self.local_partial_types = False
    # Some behaviors are changed when using Bazel (https://bazel.build).
    self.bazel = False
    # If True, export inferred types for all expressions as BuildResult.types
    self.export_types = False
    # List of package roots -- directories under these are packages even
    # if they don't have __init__.py.
    self.package_root: list[str] = []
    self.cache_map: dict[str, tuple[str, str]] = {}
    # Don't properly free objects on exit, just kill the current process.
    self.fast_exit = True
    # fast path for finding modules from source set
    self.fast_module_lookup = False
    # Allow empty function bodies even if it is not safe, used for testing only.
    self.allow_empty_bodies = False
    # Used to transform source code before parsing if not None
    # TODO: Make the type precise (AnyStr -&gt; AnyStr)
    self.transform_source: Callable[[Any], Any] | None = None
    # Print full path to each file in the report.
    self.show_absolute_path: bool = False
    # Install missing stub packages if True
    self.install_types = False
    # Install missing stub packages in non-interactive mode (don't prompt for
    # confirmation, and don't show any errors)
    self.non_interactive = False
    # When we encounter errors that may cause many additional errors,
    # skip most errors after this many messages have been reported.
    # -1 means unlimited.
    self.many_errors_threshold = defaults.MANY_ERRORS_THRESHOLD
    # Enable new experimental type inference algorithm.
    self.new_type_inference = False
    # Disable recursive type aliases (currently experimental)
    self.disable_recursive_aliases = False
    # Deprecated reverse version of the above, do not use.
    self.enable_recursive_aliases = False
    # Export line-level, limited, fine-grained dependency information in cache data
    # (undocumented feature).
    self.export_ref_info = False

    self.disable_bytearray_promotion = False
    self.disable_memoryview_promotion = False

    self.force_uppercase_builtins = False
    self.force_union_syntax = False

</t>
<t tx="ekr.20230831011820.723">def use_lowercase_names(self) -&gt; bool:
    if self.python_version &gt;= (3, 9):
        return not self.force_uppercase_builtins
    return False

</t>
<t tx="ekr.20230831011820.724">def use_or_syntax(self) -&gt; bool:
    if self.python_version &gt;= (3, 10):
        return not self.force_union_syntax
    return False

</t>
<t tx="ekr.20230831011820.725"># To avoid breaking plugin compatibility, keep providing new_semantic_analyzer
@property
def new_semantic_analyzer(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011820.726">def snapshot(self) -&gt; dict[str, object]:
    """Produce a comparable snapshot of this Option"""
    # Under mypyc, we don't have a __dict__, so we need to do worse things.
    d = dict(getattr(self, "__dict__", ()))
    for k in get_class_descriptors(Options):
        if hasattr(self, k) and k != "new_semantic_analyzer":
            d[k] = getattr(self, k)
    # Remove private attributes from snapshot
    d = {k: v for k, v in d.items() if not k.startswith("_")}
    return d

</t>
<t tx="ekr.20230831011820.727">def __repr__(self) -&gt; str:
    return f"Options({pprint.pformat(self.snapshot())})"

</t>
<t tx="ekr.20230831011820.728">def apply_changes(self, changes: dict[str, object]) -&gt; Options:
    # Note: effects of this method *must* be idempotent.
    new_options = Options()
    # Under mypyc, we don't have a __dict__, so we need to do worse things.
    replace_object_state(new_options, self, copy_dict=True)
    for key, value in changes.items():
        setattr(new_options, key, value)
    if changes.get("ignore_missing_imports"):
        # This is the only option for which a per-module and a global
        # option sometimes beheave differently.
        new_options.ignore_missing_imports_per_module = True

    # These two act as overrides, so apply them when cloning.
    # Similar to global codes enabling overrides disabling, so we start from latter.
    new_options.disabled_error_codes = self.disabled_error_codes.copy()
    new_options.enabled_error_codes = self.enabled_error_codes.copy()
    for code_str in new_options.disable_error_code:
        code = error_codes[code_str]
        new_options.disabled_error_codes.add(code)
        new_options.enabled_error_codes.discard(code)
    for code_str in new_options.enable_error_code:
        code = error_codes[code_str]
        new_options.enabled_error_codes.add(code)
        new_options.disabled_error_codes.discard(code)

    return new_options

</t>
<t tx="ekr.20230831011820.729">def compare_stable(self, other_snapshot: dict[str, object]) -&gt; bool:
    """Compare options in a way that is stable for snapshot() -&gt; apply_changes() roundtrip.

    This is needed because apply_changes() has non-trivial effects for some flags, so
    Options().apply_changes(options.snapshot()) may result in a (slightly) different object.
    """
    return (
        Options().apply_changes(self.snapshot()).snapshot()
        == Options().apply_changes(other_snapshot).snapshot()
    )

</t>
<t tx="ekr.20230831011820.73">#
# Helpers
#

def filter_errors(
    self,
    *,
    filter_errors: bool | Callable[[str, ErrorInfo], bool] = True,
    save_filtered_errors: bool = False,
) -&gt; ErrorWatcher:
    return ErrorWatcher(
        self.errors, filter_errors=filter_errors, save_filtered_errors=save_filtered_errors
    )

</t>
<t tx="ekr.20230831011820.730">def build_per_module_cache(self) -&gt; None:
    self._per_module_cache = {}

    # Config precedence is as follows:
    #  1. Concrete section names: foo.bar.baz
    #  2. "Unstructured" glob patterns: foo.*.baz, in the order
    #     they appear in the file (last wins)
    #  3. "Well-structured" wildcard patterns: foo.bar.*, in specificity order.

    # Since structured configs inherit from structured configs above them in the hierarchy,
    # we need to process per-module configs in a careful order.
    # We have to process foo.* before foo.bar.* before foo.bar,
    # and we need to apply *.bar to foo.bar but not to foo.bar.*.
    # To do this, process all well-structured glob configs before non-glob configs and
    # exploit the fact that foo.* sorts earlier ASCIIbetically (unicodebetically?)
    # than foo.bar.*.
    # (A section being "processed last" results in its config "winning".)
    # Unstructured glob configs are stored and are all checked for each module.
    unstructured_glob_keys = [k for k in self.per_module_options.keys() if "*" in k[:-1]]
    structured_keys = [k for k in self.per_module_options.keys() if "*" not in k[:-1]]
    wildcards = sorted(k for k in structured_keys if k.endswith(".*"))
    concrete = [k for k in structured_keys if not k.endswith(".*")]

    for glob in unstructured_glob_keys:
        self._glob_options.append((glob, self.compile_glob(glob)))

    # We (for ease of implementation) treat unstructured glob
    # sections as used if any real modules use them or if any
    # concrete config sections use them. This means we need to
    # track which get used while constructing.
    self.unused_configs = set(unstructured_glob_keys)

    for key in wildcards + concrete:
        # Find what the options for this key would be, just based
        # on inheriting from parent configs.
        options = self.clone_for_module(key)
        # And then update it with its per-module options.
        self._per_module_cache[key] = options.apply_changes(self.per_module_options[key])

    # Add the more structured sections into unused configs, since
    # they only count as used if actually used by a real module.
    self.unused_configs.update(structured_keys)

</t>
<t tx="ekr.20230831011820.731">def clone_for_module(self, module: str) -&gt; Options:
    """Create an Options object that incorporates per-module options.

    NOTE: Once this method is called all Options objects should be
    considered read-only, else the caching might be incorrect.
    """
    if self._per_module_cache is None:
        self.build_per_module_cache()
    assert self._per_module_cache is not None

    # If the module just directly has a config entry, use it.
    if module in self._per_module_cache:
        self.unused_configs.discard(module)
        return self._per_module_cache[module]

    # If not, search for glob paths at all the parents. So if we are looking for
    # options for foo.bar.baz, we search foo.bar.baz.*, foo.bar.*, foo.*,
    # in that order, looking for an entry.
    # This is technically quadratic in the length of the path, but module paths
    # don't actually get all that long.
    options = self
    path = module.split(".")
    for i in range(len(path), 0, -1):
        key = ".".join(path[:i] + ["*"])
        if key in self._per_module_cache:
            self.unused_configs.discard(key)
            options = self._per_module_cache[key]
            break

    # OK and *now* we need to look for unstructured glob matches.
    # We only do this for concrete modules, not structured wildcards.
    if not module.endswith(".*"):
        for key, pattern in self._glob_options:
            if pattern.match(module):
                self.unused_configs.discard(key)
                options = options.apply_changes(self.per_module_options[key])

    # We could update the cache to directly point to modules once
    # they have been looked up, but in testing this made things
    # slower and not faster, so we don't bother.

    return options

</t>
<t tx="ekr.20230831011820.732">def compile_glob(self, s: str) -&gt; Pattern[str]:
    # Compile one of the glob patterns to a regex so that '.*' can
    # match *zero or more* module sections. This means we compile
    # '.*' into '(\..*)?'.
    parts = s.split(".")
    expr = re.escape(parts[0]) if parts[0] != "*" else ".*"
    for part in parts[1:]:
        expr += re.escape("." + part) if part != "*" else r"(\..*)?"
    return re.compile(expr + "\\Z")

</t>
<t tx="ekr.20230831011820.733">def select_options_affecting_cache(self) -&gt; Mapping[str, object]:
    result: dict[str, object] = {}
    for opt in OPTIONS_AFFECTING_CACHE:
        val = getattr(self, opt)
        if opt in ("disabled_error_codes", "enabled_error_codes"):
            val = sorted([code.code for code in val])
        result[opt] = val
    return result
</t>
<t tx="ekr.20230831011820.734">@path mypy
&lt;&lt; parse.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.735">from __future__ import annotations

from mypy.errors import Errors
from mypy.nodes import MypyFile
from mypy.options import Options


</t>
<t tx="ekr.20230831011820.736">def parse(
    source: str | bytes, fnam: str, module: str | None, errors: Errors | None, options: Options
) -&gt; MypyFile:
    """Parse a source file, without doing any semantic analysis.

    Return the parse tree. If errors is not provided, raise ParseError
    on failure. Otherwise, use the errors object to report parse errors.

    The python_version (major, minor) option determines the Python syntax variant.
    """
    if options.transform_source is not None:
        source = options.transform_source(source)
    import mypy.fastparse

    return mypy.fastparse.parse(source, fnam=fnam, module=module, errors=errors, options=options)
</t>
<t tx="ekr.20230831011820.737">@path mypy
&lt;&lt; partially_defined.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.738">from __future__ import annotations

from enum import Enum

from mypy import checker, errorcodes
from mypy.messages import MessageBuilder
from mypy.nodes import (
    AssertStmt,
    AssignmentExpr,
    AssignmentStmt,
    BreakStmt,
    ClassDef,
    Context,
    ContinueStmt,
    DictionaryComprehension,
    Expression,
    ExpressionStmt,
    ForStmt,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportFrom,
    LambdaExpr,
    ListExpr,
    Lvalue,
    MatchStmt,
    MypyFile,
    NameExpr,
    NonlocalDecl,
    RaiseStmt,
    ReturnStmt,
    StarExpr,
    SymbolTable,
    TryStmt,
    TupleExpr,
    WhileStmt,
    WithStmt,
    implicit_module_attrs,
)
from mypy.options import Options
from mypy.patterns import AsPattern, StarredPattern
from mypy.reachability import ALWAYS_TRUE, infer_pattern_value
from mypy.traverser import ExtendedTraverserVisitor
from mypy.types import Type, UninhabitedType


</t>
<t tx="ekr.20230831011820.739">class BranchState:
    """BranchState contains information about variable definition at the end of a branching statement.
    `if` and `match` are examples of branching statements.

    `may_be_defined` contains variables that were defined in only some branches.
    `must_be_defined` contains variables that were defined in all branches.
    """

    @others
</t>
<t tx="ekr.20230831011820.74">def add_errors(self, errors: list[ErrorInfo]) -&gt; None:
    """Add errors in messages to this builder."""
    for info in errors:
        self.errors.add_error_info(info)

</t>
<t tx="ekr.20230831011820.740">def __init__(
    self,
    must_be_defined: set[str] | None = None,
    may_be_defined: set[str] | None = None,
    skipped: bool = False,
) -&gt; None:
    if may_be_defined is None:
        may_be_defined = set()
    if must_be_defined is None:
        must_be_defined = set()

    self.may_be_defined = set(may_be_defined)
    self.must_be_defined = set(must_be_defined)
    self.skipped = skipped

</t>
<t tx="ekr.20230831011820.741">def copy(self) -&gt; BranchState:
    return BranchState(
        must_be_defined=set(self.must_be_defined),
        may_be_defined=set(self.may_be_defined),
        skipped=self.skipped,
    )


</t>
<t tx="ekr.20230831011820.742">class BranchStatement:
    @others
</t>
<t tx="ekr.20230831011820.743">def __init__(self, initial_state: BranchState | None = None) -&gt; None:
    if initial_state is None:
        initial_state = BranchState()
    self.initial_state = initial_state
    self.branches: list[BranchState] = [
        BranchState(
            must_be_defined=self.initial_state.must_be_defined,
            may_be_defined=self.initial_state.may_be_defined,
        )
    ]

</t>
<t tx="ekr.20230831011820.744">def copy(self) -&gt; BranchStatement:
    result = BranchStatement(self.initial_state)
    result.branches = [b.copy() for b in self.branches]
    return result

</t>
<t tx="ekr.20230831011820.745">def next_branch(self) -&gt; None:
    self.branches.append(
        BranchState(
            must_be_defined=self.initial_state.must_be_defined,
            may_be_defined=self.initial_state.may_be_defined,
        )
    )

</t>
<t tx="ekr.20230831011820.746">def record_definition(self, name: str) -&gt; None:
    assert len(self.branches) &gt; 0
    self.branches[-1].must_be_defined.add(name)
    self.branches[-1].may_be_defined.discard(name)

</t>
<t tx="ekr.20230831011820.747">def delete_var(self, name: str) -&gt; None:
    assert len(self.branches) &gt; 0
    self.branches[-1].must_be_defined.discard(name)
    self.branches[-1].may_be_defined.discard(name)

</t>
<t tx="ekr.20230831011820.748">def record_nested_branch(self, state: BranchState) -&gt; None:
    assert len(self.branches) &gt; 0
    current_branch = self.branches[-1]
    if state.skipped:
        current_branch.skipped = True
        return
    current_branch.must_be_defined.update(state.must_be_defined)
    current_branch.may_be_defined.update(state.may_be_defined)
    current_branch.may_be_defined.difference_update(current_branch.must_be_defined)

</t>
<t tx="ekr.20230831011820.749">def skip_branch(self) -&gt; None:
    assert len(self.branches) &gt; 0
    self.branches[-1].skipped = True

</t>
<t tx="ekr.20230831011820.75">@contextmanager
def disable_type_names(self) -&gt; Iterator[None]:
    self._disable_type_names.append(True)
    try:
        yield
    finally:
        self._disable_type_names.pop()

</t>
<t tx="ekr.20230831011820.750">def is_possibly_undefined(self, name: str) -&gt; bool:
    assert len(self.branches) &gt; 0
    return name in self.branches[-1].may_be_defined

</t>
<t tx="ekr.20230831011820.751">def is_undefined(self, name: str) -&gt; bool:
    assert len(self.branches) &gt; 0
    branch = self.branches[-1]
    return name not in branch.may_be_defined and name not in branch.must_be_defined

</t>
<t tx="ekr.20230831011820.752">def is_defined_in_a_branch(self, name: str) -&gt; bool:
    assert len(self.branches) &gt; 0
    for b in self.branches:
        if name in b.must_be_defined or name in b.may_be_defined:
            return True
    return False

</t>
<t tx="ekr.20230831011820.753">def done(self) -&gt; BranchState:
    # First, compute all vars, including skipped branches. We include skipped branches
    # because our goal is to capture all variables that semantic analyzer would
    # consider defined.
    all_vars = set()
    for b in self.branches:
        all_vars.update(b.may_be_defined)
        all_vars.update(b.must_be_defined)
    # For the rest of the things, we only care about branches that weren't skipped.
    non_skipped_branches = [b for b in self.branches if not b.skipped]
    if non_skipped_branches:
        must_be_defined = non_skipped_branches[0].must_be_defined
        for b in non_skipped_branches[1:]:
            must_be_defined.intersection_update(b.must_be_defined)
    else:
        must_be_defined = set()
    # Everything that wasn't defined in all branches but was defined
    # in at least one branch should be in `may_be_defined`!
    may_be_defined = all_vars.difference(must_be_defined)
    return BranchState(
        must_be_defined=must_be_defined,
        may_be_defined=may_be_defined,
        skipped=len(non_skipped_branches) == 0,
    )


</t>
<t tx="ekr.20230831011820.754">class ScopeType(Enum):
    Global = 1
    Class = 2
    Func = 3
    Generator = 4


</t>
<t tx="ekr.20230831011820.755">class Scope:
    @others
</t>
<t tx="ekr.20230831011820.756">def __init__(self, stmts: list[BranchStatement], scope_type: ScopeType) -&gt; None:
    self.branch_stmts: list[BranchStatement] = stmts
    self.scope_type = scope_type
    self.undefined_refs: dict[str, set[NameExpr]] = {}

</t>
<t tx="ekr.20230831011820.757">def copy(self) -&gt; Scope:
    result = Scope([s.copy() for s in self.branch_stmts], self.scope_type)
    result.undefined_refs = self.undefined_refs.copy()
    return result

</t>
<t tx="ekr.20230831011820.758">def record_undefined_ref(self, o: NameExpr) -&gt; None:
    if o.name not in self.undefined_refs:
        self.undefined_refs[o.name] = set()
    self.undefined_refs[o.name].add(o)

</t>
<t tx="ekr.20230831011820.759">def pop_undefined_ref(self, name: str) -&gt; set[NameExpr]:
    return self.undefined_refs.pop(name, set())


</t>
<t tx="ekr.20230831011820.76">def are_type_names_disabled(self) -&gt; bool:
    return len(self._disable_type_names) &gt; 0 and self._disable_type_names[-1]

</t>
<t tx="ekr.20230831011820.760">class DefinedVariableTracker:
    """DefinedVariableTracker manages the state and scope for the UndefinedVariablesVisitor."""

    @others
</t>
<t tx="ekr.20230831011820.761">def __init__(self) -&gt; None:
    # There's always at least one scope. Within each scope, there's at least one "global" BranchingStatement.
    self.scopes: list[Scope] = [Scope([BranchStatement()], ScopeType.Global)]
    # disable_branch_skip is used to disable skipping a branch due to a return/raise/etc. This is useful
    # in things like try/except/finally statements.
    self.disable_branch_skip = False

</t>
<t tx="ekr.20230831011820.762">def copy(self) -&gt; DefinedVariableTracker:
    result = DefinedVariableTracker()
    result.scopes = [s.copy() for s in self.scopes]
    result.disable_branch_skip = self.disable_branch_skip
    return result

</t>
<t tx="ekr.20230831011820.763">def _scope(self) -&gt; Scope:
    assert len(self.scopes) &gt; 0
    return self.scopes[-1]

</t>
<t tx="ekr.20230831011820.764">def enter_scope(self, scope_type: ScopeType) -&gt; None:
    assert len(self._scope().branch_stmts) &gt; 0
    initial_state = None
    if scope_type == ScopeType.Generator:
        # Generators are special because they inherit the outer scope.
        initial_state = self._scope().branch_stmts[-1].branches[-1]
    self.scopes.append(Scope([BranchStatement(initial_state)], scope_type))

</t>
<t tx="ekr.20230831011820.765">def exit_scope(self) -&gt; None:
    self.scopes.pop()

</t>
<t tx="ekr.20230831011820.766">def in_scope(self, scope_type: ScopeType) -&gt; bool:
    return self._scope().scope_type == scope_type

</t>
<t tx="ekr.20230831011820.767">def start_branch_statement(self) -&gt; None:
    assert len(self._scope().branch_stmts) &gt; 0
    self._scope().branch_stmts.append(
        BranchStatement(self._scope().branch_stmts[-1].branches[-1])
    )

</t>
<t tx="ekr.20230831011820.768">def next_branch(self) -&gt; None:
    assert len(self._scope().branch_stmts) &gt; 1
    self._scope().branch_stmts[-1].next_branch()

</t>
<t tx="ekr.20230831011820.769">def end_branch_statement(self) -&gt; None:
    assert len(self._scope().branch_stmts) &gt; 1
    result = self._scope().branch_stmts.pop().done()
    self._scope().branch_stmts[-1].record_nested_branch(result)

</t>
<t tx="ekr.20230831011820.77">def prefer_simple_messages(self) -&gt; bool:
    """Should we generate simple/fast error messages?

    If errors aren't shown to the user, we don't want to waste cyles producing
    complex error messages.
    """
    return self.errors.prefer_simple_messages()

</t>
<t tx="ekr.20230831011820.770">def skip_branch(self) -&gt; None:
    # Only skip branch if we're outside of "root" branch statement.
    if len(self._scope().branch_stmts) &gt; 1 and not self.disable_branch_skip:
        self._scope().branch_stmts[-1].skip_branch()

</t>
<t tx="ekr.20230831011820.771">def record_definition(self, name: str) -&gt; None:
    assert len(self.scopes) &gt; 0
    assert len(self.scopes[-1].branch_stmts) &gt; 0
    self._scope().branch_stmts[-1].record_definition(name)

</t>
<t tx="ekr.20230831011820.772">def delete_var(self, name: str) -&gt; None:
    assert len(self.scopes) &gt; 0
    assert len(self.scopes[-1].branch_stmts) &gt; 0
    self._scope().branch_stmts[-1].delete_var(name)

</t>
<t tx="ekr.20230831011820.773">def record_undefined_ref(self, o: NameExpr) -&gt; None:
    """Records an undefined reference. These can later be retrieved via `pop_undefined_ref`."""
    assert len(self.scopes) &gt; 0
    self._scope().record_undefined_ref(o)

</t>
<t tx="ekr.20230831011820.774">def pop_undefined_ref(self, name: str) -&gt; set[NameExpr]:
    """If name has previously been reported as undefined, the NameExpr that was called will be returned."""
    assert len(self.scopes) &gt; 0
    return self._scope().pop_undefined_ref(name)

</t>
<t tx="ekr.20230831011820.775">def is_possibly_undefined(self, name: str) -&gt; bool:
    assert len(self._scope().branch_stmts) &gt; 0
    # A variable is undefined if it's in a set of `may_be_defined` but not in `must_be_defined`.
    return self._scope().branch_stmts[-1].is_possibly_undefined(name)

</t>
<t tx="ekr.20230831011820.776">def is_defined_in_different_branch(self, name: str) -&gt; bool:
    """This will return true if a variable is defined in a branch that's not the current branch."""
    assert len(self._scope().branch_stmts) &gt; 0
    stmt = self._scope().branch_stmts[-1]
    if not stmt.is_undefined(name):
        return False
    for stmt in self._scope().branch_stmts:
        if stmt.is_defined_in_a_branch(name):
            return True
    return False

</t>
<t tx="ekr.20230831011820.777">def is_undefined(self, name: str) -&gt; bool:
    assert len(self._scope().branch_stmts) &gt; 0
    return self._scope().branch_stmts[-1].is_undefined(name)


</t>
<t tx="ekr.20230831011820.778">class Loop:
    @others
</t>
<t tx="ekr.20230831011820.779">def __init__(self) -&gt; None:
    self.has_break = False


</t>
<t tx="ekr.20230831011820.78">def report(
    self,
    msg: str,
    context: Context | None,
    severity: str,
    *,
    code: ErrorCode | None = None,
    file: str | None = None,
    origin: Context | None = None,
    offset: int = 0,
    allow_dups: bool = False,
    secondary_context: Context | None = None,
) -&gt; None:
    """Report an error or note (unless disabled).

    Note that context controls where error is reported, while origin controls
    where # type: ignore comments have effect.
    """

    def span_from_context(ctx: Context) -&gt; Iterable[int]:
        """This determines where a type: ignore for a given context has effect.

        Current logic is a bit tricky, to keep as much backwards compatibility as
        possible. We may reconsider this to always be a single line (or otherwise
        simplify it) when we drop Python 3.7.

        TODO: address this in follow up PR
        """
        if isinstance(ctx, (ClassDef, FuncDef)):
            return range(ctx.deco_line or ctx.line, ctx.line + 1)
        elif not isinstance(ctx, Expression):
            return [ctx.line]
        else:
            return range(ctx.line, (ctx.end_line or ctx.line) + 1)

    origin_span: Iterable[int] | None
    if origin is not None:
        origin_span = span_from_context(origin)
    elif context is not None:
        origin_span = span_from_context(context)
    else:
        origin_span = None

    if secondary_context is not None:
        assert origin_span is not None
        origin_span = itertools.chain(origin_span, span_from_context(secondary_context))

    self.errors.report(
        context.line if context else -1,
        context.column if context else -1,
        msg,
        severity=severity,
        file=file,
        offset=offset,
        origin_span=origin_span,
        end_line=context.end_line if context else -1,
        end_column=context.end_column if context else -1,
        code=code,
        allow_dups=allow_dups,
    )

</t>
<t tx="ekr.20230831011820.780">class PossiblyUndefinedVariableVisitor(ExtendedTraverserVisitor):
    """Detects the following cases:
    - A variable that's defined only part of the time.
    - If a variable is used before definition

    An example of a partial definition:
    if foo():
        x = 1
    print(x)  # Error: "x" may be undefined.

    Example of a used before definition:
    x = y
    y: int = 2

    Note that this code does not detect variables not defined in any of the branches -- that is
    handled by the semantic analyzer.
    """

    @others
</t>
<t tx="ekr.20230831011820.781">def __init__(
    self,
    msg: MessageBuilder,
    type_map: dict[Expression, Type],
    options: Options,
    names: SymbolTable,
) -&gt; None:
    self.msg = msg
    self.type_map = type_map
    self.options = options
    self.builtins = SymbolTable()
    builtins_mod = names.get("__builtins__", None)
    if builtins_mod:
        assert isinstance(builtins_mod.node, MypyFile)
        self.builtins = builtins_mod.node.names
    self.loops: list[Loop] = []
    self.try_depth = 0
    self.tracker = DefinedVariableTracker()
    for name in implicit_module_attrs:
        self.tracker.record_definition(name)

</t>
<t tx="ekr.20230831011820.782">def var_used_before_def(self, name: str, context: Context) -&gt; None:
    if self.msg.errors.is_error_code_enabled(errorcodes.USED_BEFORE_DEF):
        self.msg.var_used_before_def(name, context)

</t>
<t tx="ekr.20230831011820.783">def variable_may_be_undefined(self, name: str, context: Context) -&gt; None:
    if self.msg.errors.is_error_code_enabled(errorcodes.POSSIBLY_UNDEFINED):
        self.msg.variable_may_be_undefined(name, context)

</t>
<t tx="ekr.20230831011820.784">def process_definition(self, name: str) -&gt; None:
    # Was this name previously used? If yes, it's a used-before-definition error.
    if not self.tracker.in_scope(ScopeType.Class):
        refs = self.tracker.pop_undefined_ref(name)
        for ref in refs:
            if self.loops:
                self.variable_may_be_undefined(name, ref)
            else:
                self.var_used_before_def(name, ref)
    else:
        # Errors in class scopes are caught by the semantic analyzer.
        pass
    self.tracker.record_definition(name)

</t>
<t tx="ekr.20230831011820.785">def visit_global_decl(self, o: GlobalDecl) -&gt; None:
    for name in o.names:
        self.process_definition(name)
    super().visit_global_decl(o)

</t>
<t tx="ekr.20230831011820.786">def visit_nonlocal_decl(self, o: NonlocalDecl) -&gt; None:
    for name in o.names:
        self.process_definition(name)
    super().visit_nonlocal_decl(o)

</t>
<t tx="ekr.20230831011820.787">def process_lvalue(self, lvalue: Lvalue | None) -&gt; None:
    if isinstance(lvalue, NameExpr):
        self.process_definition(lvalue.name)
    elif isinstance(lvalue, StarExpr):
        self.process_lvalue(lvalue.expr)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.process_lvalue(item)

</t>
<t tx="ekr.20230831011820.788">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    for lvalue in o.lvalues:
        self.process_lvalue(lvalue)
    super().visit_assignment_stmt(o)

</t>
<t tx="ekr.20230831011820.789">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    o.value.accept(self)
    self.process_lvalue(o.target)

</t>
<t tx="ekr.20230831011820.79">def fail(
    self,
    msg: str,
    context: Context | None,
    *,
    code: ErrorCode | None = None,
    file: str | None = None,
    allow_dups: bool = False,
    secondary_context: Context | None = None,
) -&gt; None:
    """Report an error message (unless disabled)."""
    self.report(
        msg,
        context,
        "error",
        code=code,
        file=file,
        allow_dups=allow_dups,
        secondary_context=secondary_context,
    )

</t>
<t tx="ekr.20230831011820.790">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    for e in o.expr:
        e.accept(self)
    self.tracker.start_branch_statement()
    for b in o.body:
        if b.is_unreachable:
            continue
        b.accept(self)
        self.tracker.next_branch()
    if o.else_body:
        if not o.else_body.is_unreachable:
            o.else_body.accept(self)
        else:
            self.tracker.skip_branch()
    self.tracker.end_branch_statement()

</t>
<t tx="ekr.20230831011820.791">def visit_match_stmt(self, o: MatchStmt) -&gt; None:
    o.subject.accept(self)
    self.tracker.start_branch_statement()
    for i in range(len(o.patterns)):
        pattern = o.patterns[i]
        pattern.accept(self)
        guard = o.guards[i]
        if guard is not None:
            guard.accept(self)
        if not o.bodies[i].is_unreachable:
            o.bodies[i].accept(self)
        else:
            self.tracker.skip_branch()
        is_catchall = infer_pattern_value(pattern) == ALWAYS_TRUE
        if not is_catchall:
            self.tracker.next_branch()
    self.tracker.end_branch_statement()

</t>
<t tx="ekr.20230831011820.792">def visit_func_def(self, o: FuncDef) -&gt; None:
    self.process_definition(o.name)
    super().visit_func_def(o)

</t>
<t tx="ekr.20230831011820.793">def visit_func(self, o: FuncItem) -&gt; None:
    if o.is_dynamic() and not self.options.check_untyped_defs:
        return

    args = o.arguments or []
    # Process initializers (defaults) outside the function scope.
    for arg in args:
        if arg.initializer is not None:
            arg.initializer.accept(self)

    self.tracker.enter_scope(ScopeType.Func)
    for arg in args:
        self.process_definition(arg.variable.name)
        super().visit_var(arg.variable)
    o.body.accept(self)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20230831011820.794">def visit_generator_expr(self, o: GeneratorExpr) -&gt; None:
    self.tracker.enter_scope(ScopeType.Generator)
    for idx in o.indices:
        self.process_lvalue(idx)
    super().visit_generator_expr(o)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20230831011820.795">def visit_dictionary_comprehension(self, o: DictionaryComprehension) -&gt; None:
    self.tracker.enter_scope(ScopeType.Generator)
    for idx in o.indices:
        self.process_lvalue(idx)
    super().visit_dictionary_comprehension(o)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20230831011820.796">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    o.expr.accept(self)
    self.process_lvalue(o.index)
    o.index.accept(self)
    self.tracker.start_branch_statement()
    loop = Loop()
    self.loops.append(loop)
    o.body.accept(self)
    self.tracker.next_branch()
    self.tracker.end_branch_statement()
    if o.else_body is not None:
        # If the loop has a `break` inside, `else` is executed conditionally.
        # If the loop doesn't have a `break` either the function will return or
        # execute the `else`.
        has_break = loop.has_break
        if has_break:
            self.tracker.start_branch_statement()
            self.tracker.next_branch()
        o.else_body.accept(self)
        if has_break:
            self.tracker.end_branch_statement()
    self.loops.pop()

</t>
<t tx="ekr.20230831011820.797">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    super().visit_return_stmt(o)
    self.tracker.skip_branch()

</t>
<t tx="ekr.20230831011820.798">def visit_lambda_expr(self, o: LambdaExpr) -&gt; None:
    self.tracker.enter_scope(ScopeType.Func)
    super().visit_lambda_expr(o)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20230831011820.799">def visit_assert_stmt(self, o: AssertStmt) -&gt; None:
    super().visit_assert_stmt(o)
    if checker.is_false_literal(o.expr):
        self.tracker.skip_branch()

</t>
<t tx="ekr.20230831011820.8">def map_instance_to_supertype(instance: Instance, superclass: TypeInfo) -&gt; Instance:
    """Produce a supertype of `instance` that is an Instance
    of `superclass`, mapping type arguments up the chain of bases.

    If `superclass` is not a nominal superclass of `instance.type`,
    then all type arguments are mapped to 'Any'.
    """
    if instance.type == superclass:
        # Fast path: `instance` already belongs to `superclass`.
        return instance

    if superclass.fullname == "builtins.tuple" and instance.type.tuple_type:
        if has_type_vars(instance.type.tuple_type):
            # We special case mapping generic tuple types to tuple base, because for
            # such tuples fallback can't be calculated before applying type arguments.
            alias = instance.type.special_alias
            assert alias is not None
            if not alias._is_recursive:
                # Unfortunately we can't support this for generic recursive tuples.
                # If we skip this special casing we will fall back to tuple[Any, ...].
                env = instance_to_type_environment(instance)
                tuple_type = expand_type(instance.type.tuple_type, env)
                if isinstance(tuple_type, TupleType):
                    # Make the import here to avoid cyclic imports.
                    import mypy.typeops

                    return mypy.typeops.tuple_fallback(tuple_type)

    if not superclass.type_vars:
        # Fast path: `superclass` has no type variables to map to.
        return Instance(superclass, [])

    return map_instance_to_supertypes(instance, superclass)[0]


</t>
<t tx="ekr.20230831011820.80">def note(
    self,
    msg: str,
    context: Context,
    file: str | None = None,
    origin: Context | None = None,
    offset: int = 0,
    allow_dups: bool = False,
    *,
    code: ErrorCode | None = None,
    secondary_context: Context | None = None,
) -&gt; None:
    """Report a note (unless disabled)."""
    self.report(
        msg,
        context,
        "note",
        file=file,
        origin=origin,
        offset=offset,
        allow_dups=allow_dups,
        code=code,
        secondary_context=secondary_context,
    )

</t>
<t tx="ekr.20230831011820.800">def visit_raise_stmt(self, o: RaiseStmt) -&gt; None:
    super().visit_raise_stmt(o)
    self.tracker.skip_branch()

</t>
<t tx="ekr.20230831011820.801">def visit_continue_stmt(self, o: ContinueStmt) -&gt; None:
    super().visit_continue_stmt(o)
    self.tracker.skip_branch()

</t>
<t tx="ekr.20230831011820.802">def visit_break_stmt(self, o: BreakStmt) -&gt; None:
    super().visit_break_stmt(o)
    if self.loops:
        self.loops[-1].has_break = True
    self.tracker.skip_branch()

</t>
<t tx="ekr.20230831011820.803">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    if isinstance(self.type_map.get(o.expr, None), UninhabitedType):
        self.tracker.skip_branch()
    super().visit_expression_stmt(o)

</t>
<t tx="ekr.20230831011820.804">def visit_try_stmt(self, o: TryStmt) -&gt; None:
    """
    Note that finding undefined vars in `finally` requires different handling from
    the rest of the code. In particular, we want to disallow skipping branches due to jump
    statements in except/else clauses for finally but not for other cases. Imagine a case like:
    def f() -&gt; int:
        try:
            x = 1
        except:
            # This jump statement needs to be handled differently depending on whether or
            # not we're trying to process `finally` or not.
            return 0
        finally:
            # `x` may be undefined here.
            pass
        # `x` is always defined here.
        return x
    """
    self.try_depth += 1
    if o.finally_body is not None:
        # In order to find undefined vars in `finally`, we need to
        # process try/except with branch skipping disabled. However, for the rest of the code
        # after finally, we need to process try/except with branch skipping enabled.
        # Therefore, we need to process try/finally twice.
        # Because processing is not idempotent, we should make a copy of the tracker.
        old_tracker = self.tracker.copy()
        self.tracker.disable_branch_skip = True
        self.process_try_stmt(o)
        self.tracker = old_tracker
    self.process_try_stmt(o)
    self.try_depth -= 1

</t>
<t tx="ekr.20230831011820.805">def process_try_stmt(self, o: TryStmt) -&gt; None:
    """
    Processes try statement decomposing it into the following:
    if ...:
        body
        else_body
    elif ...:
        except 1
    elif ...:
        except 2
    else:
        except n
    finally
    """
    self.tracker.start_branch_statement()
    o.body.accept(self)
    if o.else_body is not None:
        o.else_body.accept(self)
    if len(o.handlers) &gt; 0:
        assert len(o.handlers) == len(o.vars) == len(o.types)
        for i in range(len(o.handlers)):
            self.tracker.next_branch()
            exc_type = o.types[i]
            if exc_type is not None:
                exc_type.accept(self)
            var = o.vars[i]
            if var is not None:
                self.process_definition(var.name)
                var.accept(self)
            o.handlers[i].accept(self)
            if var is not None:
                self.tracker.delete_var(var.name)
    self.tracker.end_branch_statement()

    if o.finally_body is not None:
        o.finally_body.accept(self)

</t>
<t tx="ekr.20230831011820.806">def visit_while_stmt(self, o: WhileStmt) -&gt; None:
    o.expr.accept(self)
    self.tracker.start_branch_statement()
    loop = Loop()
    self.loops.append(loop)
    o.body.accept(self)
    has_break = loop.has_break
    if not checker.is_true_literal(o.expr):
        # If this is a loop like `while True`, we can consider the body to be
        # a single branch statement (we're guaranteed that the body is executed at least once).
        # If not, call next_branch() to make all variables defined there conditional.
        self.tracker.next_branch()
    self.tracker.end_branch_statement()
    if o.else_body is not None:
        # If the loop has a `break` inside, `else` is executed conditionally.
        # If the loop doesn't have a `break` either the function will return or
        # execute the `else`.
        if has_break:
            self.tracker.start_branch_statement()
            self.tracker.next_branch()
        if o.else_body:
            o.else_body.accept(self)
        if has_break:
            self.tracker.end_branch_statement()
    self.loops.pop()

</t>
<t tx="ekr.20230831011820.807">def visit_as_pattern(self, o: AsPattern) -&gt; None:
    if o.name is not None:
        self.process_lvalue(o.name)
    super().visit_as_pattern(o)

</t>
<t tx="ekr.20230831011820.808">def visit_starred_pattern(self, o: StarredPattern) -&gt; None:
    if o.capture is not None:
        self.process_lvalue(o.capture)
    super().visit_starred_pattern(o)

</t>
<t tx="ekr.20230831011820.809">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if o.name in self.builtins and self.tracker.in_scope(ScopeType.Global):
        return
    if self.tracker.is_possibly_undefined(o.name):
        # A variable is only defined in some branches.
        self.variable_may_be_undefined(o.name, o)
        # We don't want to report the error on the same variable multiple times.
        self.tracker.record_definition(o.name)
    elif self.tracker.is_defined_in_different_branch(o.name):
        # A variable is defined in one branch but used in a different branch.
        if self.loops or self.try_depth &gt; 0:
            # If we're in a loop or in a try, we can't be sure that this variable
            # is undefined. Report it as "may be undefined".
            self.variable_may_be_undefined(o.name, o)
        else:
            self.var_used_before_def(o.name, o)
    elif self.tracker.is_undefined(o.name):
        # A variable is undefined. It could be due to two things:
        # 1. A variable is just totally undefined
        # 2. The variable is defined later in the code.
        # Case (1) will be caught by semantic analyzer. Case (2) is a forward ref that should
        # be caught by this visitor. Save the ref for later, so that if we see a definition,
        # we know it's a used-before-definition scenario.
        self.tracker.record_undefined_ref(o)
    super().visit_name_expr(o)

</t>
<t tx="ekr.20230831011820.81">def note_multiline(
    self,
    messages: str,
    context: Context,
    file: str | None = None,
    offset: int = 0,
    allow_dups: bool = False,
    code: ErrorCode | None = None,
    *,
    secondary_context: Context | None = None,
) -&gt; None:
    """Report as many notes as lines in the message (unless disabled)."""
    for msg in messages.splitlines():
        self.report(
            msg,
            context,
            "note",
            file=file,
            offset=offset,
            allow_dups=allow_dups,
            code=code,
            secondary_context=secondary_context,
        )

</t>
<t tx="ekr.20230831011820.810">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    for expr, idx in zip(o.expr, o.target):
        expr.accept(self)
        self.process_lvalue(idx)
    o.body.accept(self)

</t>
<t tx="ekr.20230831011820.811">def visit_class_def(self, o: ClassDef) -&gt; None:
    self.process_definition(o.name)
    self.tracker.enter_scope(ScopeType.Class)
    super().visit_class_def(o)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20230831011820.812">def visit_import(self, o: Import) -&gt; None:
    for mod, alias in o.ids:
        if alias is not None:
            self.tracker.record_definition(alias)
        else:
            # When you do `import x.y`, only `x` becomes defined.
            names = mod.split(".")
            if names:
                # `names` should always be nonempty, but we don't want mypy
                # to crash on invalid code.
                self.tracker.record_definition(names[0])
    super().visit_import(o)

</t>
<t tx="ekr.20230831011820.813">def visit_import_from(self, o: ImportFrom) -&gt; None:
    for mod, alias in o.names:
        name = alias
        if name is None:
            name = mod
        self.tracker.record_definition(name)
    super().visit_import_from(o)
</t>
<t tx="ekr.20230831011820.814">@path mypy
&lt;&lt; patterns.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.815"></t>
<t tx="ekr.20230831011820.816">    @others
</t>
<t tx="ekr.20230831011820.817"></t>
<t tx="ekr.20230831011820.818">    @others
</t>
<t tx="ekr.20230831011820.819"></t>
<t tx="ekr.20230831011820.82">#
# Specific operations
#

# The following operations are for generating specific error messages. They
# get some information as arguments, and they build an error message based
# on them.

def has_no_attr(
    self,
    original_type: Type,
    typ: Type,
    member: str,
    context: Context,
    module_symbol_table: SymbolTable | None = None,
) -&gt; Type:
    """Report a missing or non-accessible member.

    original_type is the top-level type on which the error occurred.
    typ is the actual type that is missing the member. These can be
    different, e.g., in a union, original_type will be the union and typ
    will be the specific item in the union that does not have the member
    attribute.

    'module_symbol_table' is passed to this function if the type for which we
    are trying to get a member was originally a module. The SymbolTable allows
    us to look up and suggests attributes of the module since they are not
    directly available on original_type

    If member corresponds to an operator, use the corresponding operator
    name in the messages. Return type Any.
    """
    original_type = get_proper_type(original_type)
    typ = get_proper_type(typ)

    if isinstance(original_type, Instance) and original_type.type.has_readable_member(member):
        self.fail(f'Member "{member}" is not assignable', context)
    elif member == "__contains__":
        self.fail(
            f"Unsupported right operand type for in ({format_type(original_type, self.options)})",
            context,
            code=codes.OPERATOR,
        )
    elif member in op_methods.values():
        # Access to a binary operator member (e.g. _add). This case does
        # not handle indexing operations.
        for op, method in op_methods.items():
            if method == member:
                self.unsupported_left_operand(op, original_type, context)
                break
    elif member == "__neg__":
        self.fail(
            f"Unsupported operand type for unary - ({format_type(original_type, self.options)})",
            context,
            code=codes.OPERATOR,
        )
    elif member == "__pos__":
        self.fail(
            f"Unsupported operand type for unary + ({format_type(original_type, self.options)})",
            context,
            code=codes.OPERATOR,
        )
    elif member == "__invert__":
        self.fail(
            f"Unsupported operand type for ~ ({format_type(original_type, self.options)})",
            context,
            code=codes.OPERATOR,
        )
    elif member == "__getitem__":
        # Indexed get.
        # TODO: Fix this consistently in format_type
        if isinstance(original_type, CallableType) and original_type.is_type_obj():
            self.fail(
                "The type {} is not generic and not indexable".format(
                    format_type(original_type, self.options)
                ),
                context,
            )
        else:
            self.fail(
                f"Value of type {format_type(original_type, self.options)} is not indexable",
                context,
                code=codes.INDEX,
            )
    elif member == "__setitem__":
        # Indexed set.
        self.fail(
            "Unsupported target for indexed assignment ({})".format(
                format_type(original_type, self.options)
            ),
            context,
            code=codes.INDEX,
        )
    elif member == "__call__":
        if isinstance(original_type, Instance) and (
            original_type.type.fullname == "builtins.function"
        ):
            # "'function' not callable" is a confusing error message.
            # Explain that the problem is that the type of the function is not known.
            self.fail("Cannot call function of unknown type", context, code=codes.OPERATOR)
        else:
            self.fail(
                message_registry.NOT_CALLABLE.format(format_type(original_type, self.options)),
                context,
                code=codes.OPERATOR,
            )
    else:
        # The non-special case: a missing ordinary attribute.
        extra = ""
        if member == "__iter__":
            extra = " (not iterable)"
        elif member == "__aiter__":
            extra = " (not async iterable)"
        if not self.are_type_names_disabled():
            failed = False
            if isinstance(original_type, Instance) and original_type.type.names:
                if (
                    module_symbol_table is not None
                    and member in module_symbol_table
                    and not module_symbol_table[member].module_public
                ):
                    self.fail(
                        f"{format_type(original_type, self.options, module_names=True)} does not "
                        f'explicitly export attribute "{member}"',
                        context,
                        code=codes.ATTR_DEFINED,
                    )
                    failed = True
                else:
                    alternatives = set(original_type.type.names.keys())
                    if module_symbol_table is not None:
                        alternatives |= {
                            k for k, v in module_symbol_table.items() if v.module_public
                        }
                    # Rare but possible, see e.g. testNewAnalyzerCyclicDefinitionCrossModule
                    alternatives.discard(member)

                    matches = [m for m in COMMON_MISTAKES.get(member, []) if m in alternatives]
                    matches.extend(best_matches(member, alternatives, n=3))
                    if member == "__aiter__" and matches == ["__iter__"]:
                        matches = []  # Avoid misleading suggestion
                    if matches:
                        self.fail(
                            '{} has no attribute "{}"; maybe {}?{}'.format(
                                format_type(original_type, self.options),
                                member,
                                pretty_seq(matches, "or"),
                                extra,
                            ),
                            context,
                            code=codes.ATTR_DEFINED,
                        )
                        failed = True
            if not failed:
                self.fail(
                    '{} has no attribute "{}"{}'.format(
                        format_type(original_type, self.options), member, extra
                    ),
                    context,
                    code=codes.ATTR_DEFINED,
                )
        elif isinstance(original_type, UnionType):
            # The checker passes "object" in lieu of "None" for attribute
            # checks, so we manually convert it back.
            typ_format, orig_type_format = format_type_distinctly(
                typ, original_type, options=self.options
            )
            if typ_format == '"object"' and any(
                type(item) == NoneType for item in original_type.items
            ):
                typ_format = '"None"'
            self.fail(
                'Item {} of {} has no attribute "{}"{}'.format(
                    typ_format, orig_type_format, member, extra
                ),
                context,
                code=codes.UNION_ATTR,
            )
        elif isinstance(original_type, TypeVarType):
            bound = get_proper_type(original_type.upper_bound)
            if isinstance(bound, UnionType):
                typ_fmt, bound_fmt = format_type_distinctly(typ, bound, options=self.options)
                original_type_fmt = format_type(original_type, self.options)
                self.fail(
                    "Item {} of the upper bound {} of type variable {} has no "
                    'attribute "{}"{}'.format(
                        typ_fmt, bound_fmt, original_type_fmt, member, extra
                    ),
                    context,
                    code=codes.UNION_ATTR,
                )
        else:
            self.fail(
                '{} has no attribute "{}"{}'.format(
                    format_type(original_type, self.options), member, extra
                ),
                context,
                code=codes.ATTR_DEFINED,
            )
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011820.820"></t>
<t tx="ekr.20230831011820.821">    @others
</t>
<t tx="ekr.20230831011820.822"></t>
<t tx="ekr.20230831011820.823"></t>
<t tx="ekr.20230831011820.824">    @others
</t>
<t tx="ekr.20230831011820.825"></t>
<t tx="ekr.20230831011820.826"></t>
<t tx="ekr.20230831011820.827">    @others
</t>
<t tx="ekr.20230831011820.828"></t>
<t tx="ekr.20230831011820.829"></t>
<t tx="ekr.20230831011820.83">def unsupported_operand_types(
    self,
    op: str,
    left_type: Any,
    right_type: Any,
    context: Context,
    *,
    code: ErrorCode = codes.OPERATOR,
) -&gt; None:
    """Report unsupported operand types for a binary operation.

    Types can be Type objects or strings.
    """
    left_str = ""
    if isinstance(left_type, str):
        left_str = left_type
    else:
        left_str = format_type(left_type, self.options)

    right_str = ""
    if isinstance(right_type, str):
        right_str = right_type
    else:
        right_str = format_type(right_type, self.options)

    if self.are_type_names_disabled():
        msg = f"Unsupported operand types for {op} (likely involving Union)"
    else:
        msg = f"Unsupported operand types for {op} ({left_str} and {right_str})"
    self.fail(msg, context, code=code)

</t>
<t tx="ekr.20230831011820.830">    @others
</t>
<t tx="ekr.20230831011820.831"></t>
<t tx="ekr.20230831011820.832"></t>
<t tx="ekr.20230831011820.833">    @others
</t>
<t tx="ekr.20230831011820.834"></t>
<t tx="ekr.20230831011820.835"></t>
<t tx="ekr.20230831011820.836">    @others
</t>
<t tx="ekr.20230831011820.837"></t>
<t tx="ekr.20230831011820.838"></t>
<t tx="ekr.20230831011820.839">    @others
</t>
<t tx="ekr.20230831011820.84">def unsupported_left_operand(self, op: str, typ: Type, context: Context) -&gt; None:
    if self.are_type_names_disabled():
        msg = f"Unsupported left operand type for {op} (some union)"
    else:
        msg = f"Unsupported left operand type for {op} ({format_type(typ, self.options)})"
    self.fail(msg, context, code=codes.OPERATOR)

</t>
<t tx="ekr.20230831011820.840"></t>
<t tx="ekr.20230831011820.841"></t>
<t tx="ekr.20230831011820.842">@path mypy
"""Plugin system for extending mypy.

At large scale the plugin system works as following:

* Plugins are collected from the corresponding mypy config file option
  (either via paths to Python files, or installed Python modules)
  and imported using importlib.

* Every module should get an entry point function (called 'plugin' by default,
  but may be overridden in the config file) that should accept a single string
  argument that is a full mypy version (includes git commit hash for dev
  versions) and return a subclass of mypy.plugins.Plugin.

* All plugin class constructors should match the signature of mypy.plugin.Plugin
  (i.e. should accept an mypy.options.Options object), and *must* call
  super().__init__().

* At several steps during semantic analysis and type checking mypy calls
  special `get_xxx` methods on user plugins with a single string argument that
  is a fully qualified name (full name) of a relevant definition
  (see mypy.plugin.Plugin method docstrings for details).

* The plugins are called in the order they are passed in the config option.
  Every plugin must decide whether to act on a given full name. The first
  plugin that returns non-None object will be used.

* The above decision should be made using the limited common API specified by
  mypy.plugin.CommonPluginApi.

* The callback returned by the plugin will be called with a larger context that
  includes relevant current state (e.g. a default return type, or a default
  attribute type) and a wider relevant API provider (e.g.
  SemanticAnalyzerPluginInterface or CheckerPluginInterface).

* The result of this is used for further processing. See various `XxxContext`
  named tuples for details about which information is given to each hook.

Plugin developers should ensure that their plugins work well in incremental and
daemon modes. In particular, plugins should not hold global state, and should
always call add_plugin_dependency() in plugin hooks called during semantic
analysis. See the method docstring for more details.

There is no dedicated cache storage for plugins, but plugins can store
per-TypeInfo data in a special .metadata attribute that is serialized to the
mypy caches between incremental runs. To avoid collisions between plugins, they
are encouraged to store their state under a dedicated key coinciding with
plugin name in the metadata dictionary. Every value stored there must be
JSON-serializable.

## Notes about the semantic analyzer

Mypy 0.710 introduced a new semantic analyzer that changed how plugins are
expected to work in several notable ways (from mypy 0.730 the old semantic
analyzer is no longer available):

1. The order of processing AST nodes in modules is different. The old semantic
   analyzer processed modules in textual order, one module at a time. The new
   semantic analyzer first processes the module top levels, including bodies of
   any top-level classes and classes nested within classes. ("Top-level" here
   means "not nested within a function/method".) Functions and methods are
   processed only after module top levels have been finished. If there is an
   import cycle, all module top levels in the cycle are processed before
   processing any functions or methods. Each unit of processing (a module top
   level or a function/method) is called a *target*.

   This also means that function signatures in the same module have not been
   analyzed yet when analyzing the module top level. If you need access to
   a function signature, you'll need to explicitly analyze the signature first
   using `anal_type()`.

2. Each target can be processed multiple times. This may happen if some forward
   references are not ready yet, for example. This means that semantic analyzer
   related plugin hooks can be called multiple times for the same full name.
   These plugin methods must thus be idempotent.

3. The `anal_type` API function returns None if some part of the type is not
   available yet. If this happens, the current target being analyzed will be
   *deferred*, which means that it will be processed again soon, in the hope
   that additional dependencies will be available. This may happen if there are
   forward references to types or inter-module references to types within an
   import cycle.

   Note that if there is a circular definition, mypy may decide to stop
   processing to avoid an infinite number of iterations. When this happens,
   `anal_type` will generate an error and return an `AnyType` type object
   during the final iteration (instead of None).

4. There is a new API method `defer()`. This can be used to explicitly request
   the current target to be reprocessed one more time. You don't need this
   to call this if `anal_type` returns None, however.

5. There is a new API property `final_iteration`, which is true once mypy
   detected no progress during the previous iteration or if the maximum
   semantic analysis iteration count has been reached. You must never
   defer during the final iteration, as it will cause a crash.

6. The `node` attribute of SymbolTableNode objects may contain a reference to
   a PlaceholderNode object. This object means that this definition has not
   been fully processed yet. If you encounter a PlaceholderNode, you should
   defer unless it's the final iteration. If it's the final iteration, you
   should generate an error message. It usually means that there's a cyclic
   definition that cannot be resolved by mypy. PlaceholderNodes can only refer
   to references inside an import cycle. If you are looking up things from
   another module, such as the builtins, that is outside the current module or
   import cycle, you can safely assume that you won't receive a placeholder.

When testing your plugin, you should have a test case that forces a module top
level to be processed multiple times. The easiest way to do this is to include
a forward reference to a class in a top-level annotation. Example:

    c: C  # Forward reference causes second analysis pass
    class C: pass

Note that a forward reference in a function signature won't trigger another
pass, since all functions are processed only after the top level has been fully
analyzed.

You can use `api.options.new_semantic_analyzer` to check whether the new
semantic analyzer is enabled (it's always true in mypy 0.730 and later).
"""

&lt;&lt; plugin.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.844">from __future__ import annotations

from abc import abstractmethod
from typing import Any, Callable, NamedTuple, TypeVar

from mypy_extensions import mypyc_attr, trait

from mypy.errorcodes import ErrorCode
from mypy.lookup import lookup_fully_qualified
from mypy.message_registry import ErrorMessage
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ArgKind,
    CallExpr,
    ClassDef,
    Context,
    Expression,
    MypyFile,
    SymbolTableNode,
    TypeInfo,
)
from mypy.options import Options
from mypy.tvar_scope import TypeVarLikeScope
from mypy.types import (
    CallableType,
    FunctionLike,
    Instance,
    ProperType,
    Type,
    TypeList,
    UnboundType,
)


@trait
</t>
<t tx="ekr.20230831011820.845">class TypeAnalyzerPluginInterface:
    """Interface for accessing semantic analyzer functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in typeanal.py for more details.
    """

    @others
</t>
<t tx="ekr.20230831011820.846"># An options object. Note: these are the cloned options for the current file.
# This might be different from Plugin.options (that contains default/global options)
# if there are per-file options in the config. This applies to all other interfaces
# in this file.
options: Options

@abstractmethod
def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.847">@abstractmethod
def named_type(self, name: str, args: list[Type]) -&gt; Instance:
    """Construct an instance of a builtin type with given name."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.848">@abstractmethod
def analyze_type(self, typ: Type) -&gt; Type:
    """Analyze an unbound type using the default mypy logic."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.849">@abstractmethod
def analyze_callable_args(
    self, arglist: TypeList
) -&gt; tuple[list[Type], list[ArgKind], list[str | None]] | None:
    """Find types, kinds, and names of arguments from extended callable syntax."""
    raise NotImplementedError


</t>
<t tx="ekr.20230831011820.85">def not_callable(self, typ: Type, context: Context) -&gt; Type:
    self.fail(message_registry.NOT_CALLABLE.format(format_type(typ, self.options)), context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011820.850"># A context for a hook that semantically analyzes an unbound type.
class AnalyzeTypeContext(NamedTuple):
    type: UnboundType  # Type to analyze
    context: Context  # Relevant location context (e.g. for error messages)
    api: TypeAnalyzerPluginInterface


</t>
<t tx="ekr.20230831011820.851">@mypyc_attr(allow_interpreted_subclasses=True)
class CommonPluginApi:
    """
    A common plugin API (shared between semantic analysis and type checking phases)
    that all plugin hooks get independently of the context.
    """

    @others
</t>
<t tx="ekr.20230831011820.852"># Global mypy options.
# Per-file options can be only accessed on various
# XxxPluginInterface classes.
options: Options

@abstractmethod
def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode | None:
    """Lookup a symbol by its full name (including module).

    This lookup function available for all plugins. Return None if a name
    is not found. This function doesn't support lookup from current scope.
    Use SemanticAnalyzerPluginInterface.lookup_qualified() for this."""
    raise NotImplementedError


</t>
<t tx="ekr.20230831011820.853">@trait
class CheckerPluginInterface:
    """Interface for accessing type checker functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in checker.py for more details.
    """

    @others
</t>
<t tx="ekr.20230831011820.854">msg: MessageBuilder
options: Options
path: str

# Type context for type inference
@property
@abstractmethod
def type_context(self) -&gt; list[Type | None]:
    """Return the type context of the plugin"""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.855">@abstractmethod
def fail(
    self, msg: str | ErrorMessage, ctx: Context, *, code: ErrorCode | None = None
) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.856">@abstractmethod
def named_generic_type(self, name: str, args: list[Type]) -&gt; Instance:
    """Construct an instance of a builtin type with given type arguments."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.857">@abstractmethod
def get_expression_type(self, node: Expression, type_context: Type | None = None) -&gt; Type:
    """Checks the type of the given expression."""
    raise NotImplementedError


</t>
<t tx="ekr.20230831011820.858">@trait
class SemanticAnalyzerPluginInterface:
    """Interface for accessing semantic analyzer functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in semanal.py for more details.

    # TODO: clean-up lookup functions.
    """

    @others
</t>
<t tx="ekr.20230831011820.859">modules: dict[str, MypyFile]
# Options for current file.
options: Options
cur_mod_id: str
msg: MessageBuilder

@abstractmethod
def named_type(self, fullname: str, args: list[Type] | None = None) -&gt; Instance:
    """Construct an instance of a builtin type with given type arguments."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.86">def untyped_function_call(self, callee: CallableType, context: Context) -&gt; Type:
    name = callable_name(callee) or "(unknown)"
    self.fail(
        f"Call to untyped function {name} in typed context",
        context,
        code=codes.NO_UNTYPED_CALL,
    )
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011820.860">@abstractmethod
def builtin_type(self, fully_qualified_name: str) -&gt; Instance:
    """Legacy function -- use named_type() instead."""
    # NOTE: Do not delete this since many plugins may still use it.
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.861">@abstractmethod
def named_type_or_none(self, fullname: str, args: list[Type] | None = None) -&gt; Instance | None:
    """Construct an instance of a type with given type arguments.

    Return None if a type could not be constructed for the qualified
    type name. This is possible when the qualified name includes a
    module name and the module has not been imported.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.862">@abstractmethod
def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.863">@abstractmethod
def parse_bool(self, expr: Expression) -&gt; bool | None:
    """Parse True/False literals."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.864">@abstractmethod
def parse_str_literal(self, expr: Expression) -&gt; str | None:
</t>
<t tx="ekr.20230831011820.865">    """Parse string literals."""

@abstractmethod
def fail(
    self,
    msg: str,
    ctx: Context,
    serious: bool = False,
    *,
    blocker: bool = False,
    code: ErrorCode | None = None,
) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.866">@abstractmethod
def anal_type(
    self,
    t: Type,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    report_invalid_types: bool = True,
    third_pass: bool = False,
) -&gt; Type | None:
    """Analyze an unbound type.

    Return None if some part of the type is not ready yet. In this
    case the current target being analyzed will be deferred and
    analyzed again.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.867">@abstractmethod
def class_type(self, self_type: Type) -&gt; Type:
    """Generate type of first argument of class methods from type of self."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.868">@abstractmethod
def lookup_fully_qualified(self, name: str) -&gt; SymbolTableNode:
    """Lookup a symbol by its fully qualified name.

    Raise an error if not found.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.869">@abstractmethod
def lookup_fully_qualified_or_none(self, name: str) -&gt; SymbolTableNode | None:
    """Lookup a symbol by its fully qualified name.

    Return None if not found.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.87">def incompatible_argument(
    self,
    n: int,
    m: int,
    callee: CallableType,
    arg_type: Type,
    arg_kind: ArgKind,
    object_type: Type | None,
    context: Context,
    outer_context: Context,
) -&gt; ErrorCode | None:
    """Report an error about an incompatible argument type.

    The argument type is arg_type, argument number is n and the
    callee type is 'callee'. If the callee represents a method
    that corresponds to an operator, use the corresponding
    operator name in the messages.

    Return the error code that used for the argument (multiple error
    codes are possible).
    """
    arg_type = get_proper_type(arg_type)

    target = ""
    callee_name = callable_name(callee)
    if callee_name is not None:
        name = callee_name
        if callee.bound_args and callee.bound_args[0] is not None:
            base = format_type(callee.bound_args[0], self.options)
        else:
            base = extract_type(name)

        for method, op in op_methods_to_symbols.items():
            for variant in method, "__r" + method[2:]:
                # FIX: do not rely on textual formatting
                if name.startswith(f'"{variant}" of'):
                    if op == "in" or variant != method:
                        # Reversed order of base/argument.
                        self.unsupported_operand_types(
                            op, arg_type, base, context, code=codes.OPERATOR
                        )
                    else:
                        self.unsupported_operand_types(
                            op, base, arg_type, context, code=codes.OPERATOR
                        )
                    return codes.OPERATOR

        if name.startswith('"__getitem__" of'):
            self.invalid_index_type(
                arg_type, callee.arg_types[n - 1], base, context, code=codes.INDEX
            )
            return codes.INDEX

        if name.startswith('"__setitem__" of'):
            if n == 1:
                self.invalid_index_type(
                    arg_type, callee.arg_types[n - 1], base, context, code=codes.INDEX
                )
                return codes.INDEX
            else:
                arg_type_str, callee_type_str = format_type_distinctly(
                    arg_type, callee.arg_types[n - 1], options=self.options
                )
                info = (
                    f" (expression has type {arg_type_str}, "
                    f"target has type {callee_type_str})"
                )
                error_msg = (
                    message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT.with_additional_msg(info)
                )
                self.fail(error_msg.value, context, code=error_msg.code)
                return error_msg.code

        target = f"to {name} "

    msg = ""
    code = codes.MISC
    notes: list[str] = []
    if callee_name == "&lt;list&gt;":
        name = callee_name[1:-1]
        n -= 1
        actual_type_str, expected_type_str = format_type_distinctly(
            arg_type, callee.arg_types[0], options=self.options
        )
        msg = "{} item {} has incompatible type {}; expected {}".format(
            name.title(), n, actual_type_str, expected_type_str
        )
        code = codes.LIST_ITEM
    elif callee_name == "&lt;dict&gt;" and isinstance(
        get_proper_type(callee.arg_types[n - 1]), TupleType
    ):
        name = callee_name[1:-1]
        n -= 1
        key_type, value_type = cast(TupleType, arg_type).items
        expected_key_type, expected_value_type = cast(TupleType, callee.arg_types[n]).items

        # don't increase verbosity unless there is need to do so
        if is_subtype(key_type, expected_key_type):
            key_type_str = format_type(key_type, self.options)
            expected_key_type_str = format_type(expected_key_type, self.options)
        else:
            key_type_str, expected_key_type_str = format_type_distinctly(
                key_type, expected_key_type, options=self.options
            )
        if is_subtype(value_type, expected_value_type):
            value_type_str = format_type(value_type, self.options)
            expected_value_type_str = format_type(expected_value_type, self.options)
        else:
            value_type_str, expected_value_type_str = format_type_distinctly(
                value_type, expected_value_type, options=self.options
            )

        msg = "{} entry {} has incompatible type {}: {}; expected {}: {}".format(
            name.title(),
            n,
            key_type_str,
            value_type_str,
            expected_key_type_str,
            expected_value_type_str,
        )
        code = codes.DICT_ITEM
    elif callee_name == "&lt;dict&gt;":
        value_type_str, expected_value_type_str = format_type_distinctly(
            arg_type, callee.arg_types[n - 1], options=self.options
        )
        msg = "Unpacked dict entry {} has incompatible type {}; expected {}".format(
            n - 1, value_type_str, expected_value_type_str
        )
        code = codes.DICT_ITEM
    elif callee_name == "&lt;list-comprehension&gt;":
        actual_type_str, expected_type_str = map(
            strip_quotes,
            format_type_distinctly(arg_type, callee.arg_types[0], options=self.options),
        )
        msg = "List comprehension has incompatible type List[{}]; expected List[{}]".format(
            actual_type_str, expected_type_str
        )
    elif callee_name == "&lt;set-comprehension&gt;":
        actual_type_str, expected_type_str = map(
            strip_quotes,
            format_type_distinctly(arg_type, callee.arg_types[0], options=self.options),
        )
        msg = "Set comprehension has incompatible type Set[{}]; expected Set[{}]".format(
            actual_type_str, expected_type_str
        )
    elif callee_name == "&lt;dictionary-comprehension&gt;":
        actual_type_str, expected_type_str = format_type_distinctly(
            arg_type, callee.arg_types[n - 1], options=self.options
        )
        msg = (
            "{} expression in dictionary comprehension has incompatible type {}; "
            "expected type {}"
        ).format("Key" if n == 1 else "Value", actual_type_str, expected_type_str)
    elif callee_name == "&lt;generator&gt;":
        actual_type_str, expected_type_str = format_type_distinctly(
            arg_type, callee.arg_types[0], options=self.options
        )
        msg = "Generator has incompatible item type {}; expected {}".format(
            actual_type_str, expected_type_str
        )
    else:
        if self.prefer_simple_messages():
            msg = "Argument has incompatible type"
        else:
            try:
                expected_type = callee.arg_types[m - 1]
            except IndexError:  # Varargs callees
                expected_type = callee.arg_types[-1]
            arg_type_str, expected_type_str = format_type_distinctly(
                arg_type, expected_type, bare=True, options=self.options
            )
            if arg_kind == ARG_STAR:
                arg_type_str = "*" + arg_type_str
            elif arg_kind == ARG_STAR2:
                arg_type_str = "**" + arg_type_str

            # For function calls with keyword arguments, display the argument name rather
            # than the number.
            arg_label = str(n)
            if isinstance(outer_context, CallExpr) and len(outer_context.arg_names) &gt;= n:
                arg_name = outer_context.arg_names[n - 1]
                if arg_name is not None:
                    arg_label = f'"{arg_name}"'
            if (
                arg_kind == ARG_STAR2
                and isinstance(arg_type, TypedDictType)
                and m &lt;= len(callee.arg_names)
                and callee.arg_names[m - 1] is not None
                and callee.arg_kinds[m - 1] != ARG_STAR2
            ):
                arg_name = callee.arg_names[m - 1]
                assert arg_name is not None
                arg_type_str, expected_type_str = format_type_distinctly(
                    arg_type.items[arg_name], expected_type, bare=True, options=self.options
                )
                arg_label = f'"{arg_name}"'
            if isinstance(outer_context, IndexExpr) and isinstance(
                outer_context.index, StrExpr
            ):
                msg = 'Value of "{}" has incompatible type {}; expected {}'.format(
                    outer_context.index.value,
                    quote_type_string(arg_type_str),
                    quote_type_string(expected_type_str),
                )
            else:
                msg = "Argument {} {}has incompatible type {}; expected {}".format(
                    arg_label,
                    target,
                    quote_type_string(arg_type_str),
                    quote_type_string(expected_type_str),
                )
            expected_type = get_proper_type(expected_type)
            if isinstance(expected_type, UnionType):
                expected_types = list(expected_type.items)
            else:
                expected_types = [expected_type]
            for type in get_proper_types(expected_types):
                if isinstance(arg_type, Instance) and isinstance(type, Instance):
                    notes = append_invariance_notes(notes, arg_type, type)
                    notes = append_numbers_notes(notes, arg_type, type)
        object_type = get_proper_type(object_type)
        if isinstance(object_type, TypedDictType):
            code = codes.TYPEDDICT_ITEM
        else:
            code = codes.ARG_TYPE
    self.fail(msg, context, code=code)
    if notes:
        for note_msg in notes:
            self.note(note_msg, context, code=code)
    return code

</t>
<t tx="ekr.20230831011820.870">@abstractmethod
def lookup_qualified(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    """Lookup symbol using a name in current scope.

    This follows Python local-&gt;non-local-&gt;global-&gt;builtins rules.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.871">@abstractmethod
def add_plugin_dependency(self, trigger: str, target: str | None = None) -&gt; None:
    """Specify semantic dependencies for generated methods/variables.

    If the symbol with full name given by trigger is found to be stale by mypy,
    then the body of node with full name given by target will be re-checked.
    By default, this is the node that is currently analyzed.

    For example, the dataclass plugin adds a generated __init__ method with
    a signature that depends on types of attributes in ancestor classes. If any
    attribute in an ancestor class gets stale (modified), we need to reprocess
    the subclasses (and thus regenerate __init__ methods).

    This is used by fine-grained incremental mode (mypy daemon). See mypy/server/deps.py
    for more details.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.872">@abstractmethod
def add_symbol_table_node(self, name: str, stnode: SymbolTableNode) -&gt; Any:
    """Add node to global symbol table (or to nearest class if there is one)."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.873">@abstractmethod
def qualified_name(self, n: str) -&gt; str:
    """Make qualified name using current module and enclosing class (if any)."""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.874">@abstractmethod
def defer(self) -&gt; None:
    """Call this to defer the processing of the current node.

    This will request an additional iteration of semantic analysis.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.875">@property
@abstractmethod
def final_iteration(self) -&gt; bool:
    """Is this the final iteration of semantic analysis?"""
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.876">@property
@abstractmethod
def is_stub_file(self) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011820.877">@abstractmethod
def analyze_simple_literal_type(self, rvalue: Expression, is_final: bool) -&gt; Type | None:
    raise NotImplementedError


</t>
<t tx="ekr.20230831011820.878"># A context for querying for configuration data about a module for
# cache invalidation purposes.
class ReportConfigContext(NamedTuple):
    id: str  # Module name
    path: str  # Module file path
    is_check: bool  # Is this invocation for checking whether the config matches


</t>
<t tx="ekr.20230831011820.879"># A context for a function signature hook that infers a better signature for a
# function.  Note that argument types aren't available yet.  If you need them,
# you have to use a method hook instead.
class FunctionSigContext(NamedTuple):
    args: list[list[Expression]]  # Actual expressions for each formal argument
    default_signature: CallableType  # Original signature of the method
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20230831011820.88">def incompatible_argument_note(
    self,
    original_caller_type: ProperType,
    callee_type: ProperType,
    context: Context,
    code: ErrorCode | None,
) -&gt; None:
    if self.prefer_simple_messages():
        return
    if isinstance(
        original_caller_type, (Instance, TupleType, TypedDictType, TypeType, CallableType)
    ):
        if isinstance(callee_type, Instance) and callee_type.type.is_protocol:
            self.report_protocol_problems(
                original_caller_type, callee_type, context, code=code
            )
        if isinstance(callee_type, UnionType):
            for item in callee_type.items:
                item = get_proper_type(item)
                if isinstance(item, Instance) and item.type.is_protocol:
                    self.report_protocol_problems(
                        original_caller_type, item, context, code=code
                    )
    if isinstance(callee_type, CallableType) and isinstance(original_caller_type, Instance):
        call = find_member(
            "__call__", original_caller_type, original_caller_type, is_operator=True
        )
        if call:
            self.note_call(original_caller_type, call, context, code=code)

    self.maybe_note_concatenate_pos_args(original_caller_type, callee_type, context, code)

</t>
<t tx="ekr.20230831011820.880"># A context for a function hook that infers the return type of a function with
# a special signature.
#
# A no-op callback would just return the inferred return type, but a useful
# callback at least sometimes can infer a more precise type.
class FunctionContext(NamedTuple):
    arg_types: list[list[Type]]  # List of actual caller types for each formal argument
    arg_kinds: list[list[ArgKind]]  # Ditto for argument kinds, see nodes.ARG_* constants
    # Names of formal parameters from the callee definition,
    # these will be sufficient in most cases.
    callee_arg_names: list[str | None]
    # Names of actual arguments in the call expression. For example,
    # in a situation like this:
    #     def func(**kwargs) -&gt; None:
    #         pass
    #     func(kw1=1, kw2=2)
    # callee_arg_names will be ['kwargs'] and arg_names will be [['kw1', 'kw2']].
    arg_names: list[list[str | None]]
    default_return_type: Type  # Return type inferred from signature
    args: list[list[Expression]]  # Actual expressions for each formal argument
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20230831011820.881"># A context for a method signature hook that infers a better signature for a
# method.  Note that argument types aren't available yet.  If you need them,
# you have to use a method hook instead.
# TODO: document ProperType in the plugin changelog/update issue.
class MethodSigContext(NamedTuple):
    type: ProperType  # Base object type for method call
    args: list[list[Expression]]  # Actual expressions for each formal argument
    default_signature: CallableType  # Original signature of the method
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20230831011820.882"># A context for a method hook that infers the return type of a method with a
# special signature.
#
# This is very similar to FunctionContext (only differences are documented).
class MethodContext(NamedTuple):
    type: ProperType  # Base object type for method call
    arg_types: list[list[Type]]  # List of actual caller types for each formal argument
    # see FunctionContext for details about names and kinds
    arg_kinds: list[list[ArgKind]]
    callee_arg_names: list[str | None]
    arg_names: list[list[str | None]]
    default_return_type: Type  # Return type inferred by mypy
    args: list[list[Expression]]  # Lists of actual expressions for every formal argument
    context: Context
    api: CheckerPluginInterface


</t>
<t tx="ekr.20230831011820.883"># A context for an attribute type hook that infers the type of an attribute.
class AttributeContext(NamedTuple):
    type: ProperType  # Type of object with attribute
    default_attr_type: Type  # Original attribute type
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20230831011820.884"># A context for a class hook that modifies the class definition.
class ClassDefContext(NamedTuple):
    cls: ClassDef  # The class definition
    reason: Expression  # The expression being applied (decorator, metaclass, base class)
    api: SemanticAnalyzerPluginInterface


</t>
<t tx="ekr.20230831011820.885"># A context for dynamic class definitions like
# Base = declarative_base()
class DynamicClassDefContext(NamedTuple):
    call: CallExpr  # The r.h.s. of dynamic class definition
    name: str  # The name this class is being assigned to
    api: SemanticAnalyzerPluginInterface


</t>
<t tx="ekr.20230831011820.886">@mypyc_attr(allow_interpreted_subclasses=True)
class Plugin(CommonPluginApi):
    """Base class of all type checker plugins.

    This defines a no-op plugin.  Subclasses can override some methods to
    provide some actual functionality.

    All get_ methods are treated as pure functions (you should assume that
    results might be cached). A plugin should return None from a get_ method
    to give way to other plugins.

    Look at the comments of various *Context objects for additional information on
    various hooks.
    """

    @others
</t>
<t tx="ekr.20230831011820.887">def __init__(self, options: Options) -&gt; None:
    self.options = options
    self.python_version = options.python_version
    # This can't be set in __init__ because it is executed too soon in build.py.
    # Therefore, build.py *must* set it later before graph processing starts
    # by calling set_modules().
    self._modules: dict[str, MypyFile] | None = None

</t>
<t tx="ekr.20230831011820.888">def set_modules(self, modules: dict[str, MypyFile]) -&gt; None:
    self._modules = modules

</t>
<t tx="ekr.20230831011820.889">def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode | None:
    assert self._modules is not None
    return lookup_fully_qualified(fullname, self._modules)

</t>
<t tx="ekr.20230831011820.89">def maybe_note_concatenate_pos_args(
    self,
    original_caller_type: ProperType,
    callee_type: ProperType,
    context: Context,
    code: ErrorCode | None = None,
) -&gt; None:
    # pos-only vs positional can be confusing, with Concatenate
    if (
        isinstance(callee_type, CallableType)
        and isinstance(original_caller_type, CallableType)
        and (original_caller_type.from_concatenate or callee_type.from_concatenate)
    ):
        names: list[str] = []
        for c, o in zip(
            callee_type.formal_arguments(), original_caller_type.formal_arguments()
        ):
            if None in (c.pos, o.pos):
                # non-positional
                continue
            if c.name != o.name and c.name is None and o.name is not None:
                names.append(o.name)

        if names:
            missing_arguments = '"' + '", "'.join(names) + '"'
            self.note(
                f'This is likely because "{original_caller_type.name}" has named arguments: '
                f"{missing_arguments}. Consider marking them positional-only",
                context,
                code=code,
            )

</t>
<t tx="ekr.20230831011820.890">def report_config_data(self, ctx: ReportConfigContext) -&gt; Any:
    """Get representation of configuration data for a module.

    The data must be encodable as JSON and will be stored in the
    cache metadata for the module. A mismatch between the cached
    values and the returned will result in that module's cache
    being invalidated and the module being rechecked.

    This can be called twice for each module, once after loading
    the cache to check if it is valid and once while writing new
    cache information.

    If is_check in the context is true, then the return of this
    call will be checked against the cached version. Otherwise the
    call is being made to determine what to put in the cache. This
    can be used to allow consulting extra cache files in certain
    complex situations.

    This can be used to incorporate external configuration information
    that might require changes to typechecking.
    """
    return None

</t>
<t tx="ekr.20230831011820.891">def get_additional_deps(self, file: MypyFile) -&gt; list[tuple[int, str, int]]:
    """Customize dependencies for a module.

    This hook allows adding in new dependencies for a module. It
    is called after parsing a file but before analysis. This can
    be useful if a library has dependencies that are dynamic based
    on configuration information, for example.

    Returns a list of (priority, module name, line number) tuples.

    The line number can be -1 when there is not a known real line number.

    Priorities are defined in mypy.build (but maybe shouldn't be).
    10 is a good choice for priority.
    """
    return []

</t>
<t tx="ekr.20230831011820.892">def get_type_analyze_hook(self, fullname: str) -&gt; Callable[[AnalyzeTypeContext], Type] | None:
    """Customize behaviour of the type analyzer for given full names.

    This method is called during the semantic analysis pass whenever mypy sees an
    unbound type. For example, while analysing this code:

        from lib import Special, Other

        var: Special
        def func(x: Other[int]) -&gt; None:
            ...

    this method will be called with 'lib.Special', and then with 'lib.Other'.
    The callback returned by plugin must return an analyzed type,
    i.e. an instance of `mypy.types.Type`.
    """
    return None

</t>
<t tx="ekr.20230831011820.893">def get_function_signature_hook(
    self, fullname: str
) -&gt; Callable[[FunctionSigContext], FunctionLike] | None:
    """Adjust the signature of a function.

    This method is called before type checking a function call. Plugin
    may infer a better type for the function.

        from lib import Class, do_stuff

        do_stuff(42)
        Class()

    This method will be called with 'lib.do_stuff' and then with 'lib.Class'.
    """
    return None

</t>
<t tx="ekr.20230831011820.894">def get_function_hook(self, fullname: str) -&gt; Callable[[FunctionContext], Type] | None:
    """Adjust the return type of a function call.

    This method is called after type checking a call. Plugin may adjust the return
    type inferred by mypy, and/or emit some error messages. Note, this hook is also
    called for class instantiation calls, so that in this example:

        from lib import Class, do_stuff

        do_stuff(42)
        Class()

    This method will be called with 'lib.do_stuff' and then with 'lib.Class'.
    """
    return None

</t>
<t tx="ekr.20230831011820.895">def get_method_signature_hook(
    self, fullname: str
) -&gt; Callable[[MethodSigContext], FunctionLike] | None:
    """Adjust the signature of a method.

    This method is called before type checking a method call. Plugin
    may infer a better type for the method. The hook is also called for special
    Python dunder methods except __init__ and __new__ (use get_function_hook to customize
    class instantiation). This function is called with the method full name using
    the class where it was _defined_. For example, in this code:

        from lib import Special

        class Base:
            def method(self, arg: Any) -&gt; Any:
                ...
        class Derived(Base):
            ...

        var: Derived
        var.method(42)

        x: Special
        y = x[0]

    this method is called with '__main__.Base.method', and then with
    'lib.Special.__getitem__'.
    """
    return None

</t>
<t tx="ekr.20230831011820.896">def get_method_hook(self, fullname: str) -&gt; Callable[[MethodContext], Type] | None:
    """Adjust return type of a method call.

    This is the same as get_function_hook(), but is called with the
    method full name (again, using the class where the method is defined).
    """
    return None

</t>
<t tx="ekr.20230831011820.897">def get_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    """Adjust type of an instance attribute.

    This method is called with attribute full name using the class of the instance where
    the attribute was defined (or Var.info.fullname for generated attributes).

    For classes without __getattr__ or __getattribute__, this hook is only called for
    names of fields/properties (but not methods) that exist in the instance MRO.

    For classes that implement __getattr__ or __getattribute__, this hook is called
    for all fields/properties, including nonexistent ones (but still not methods).

    For example:

        class Base:
            x: Any
            def __getattr__(self, attr: str) -&gt; Any: ...

        class Derived(Base):
            ...

        var: Derived
        var.x
        var.y

    get_attribute_hook is called with '__main__.Base.x' and '__main__.Base.y'.
    However, if we had not implemented __getattr__ on Base, you would only get
    the callback for 'var.x'; 'var.y' would produce an error without calling the hook.
    """
    return None

</t>
<t tx="ekr.20230831011820.898">def get_class_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    """
    Adjust type of a class attribute.

    This method is called with attribute full name using the class where the attribute was
    defined (or Var.info.fullname for generated attributes).

    For example:

        class Cls:
            x: Any

        Cls.x

    get_class_attribute_hook is called with '__main__.Cls.x' as fullname.
    """
    return None

</t>
<t tx="ekr.20230831011820.899">def get_class_decorator_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    """Update class definition for given class decorators.

    The plugin can modify a TypeInfo _in place_ (for example add some generated
    methods to the symbol table). This hook is called after the class body was
    semantically analyzed, but *there may still be placeholders* (typically
    caused by forward references).

    NOTE: Usually get_class_decorator_hook_2 is the better option, since it
          guarantees that there are no placeholders.

    The hook is called with full names of all class decorators.

    The hook can be called multiple times per class, so it must be
    idempotent.
    """
    return None

</t>
<t tx="ekr.20230831011820.9">def map_instance_to_supertypes(instance: Instance, supertype: TypeInfo) -&gt; list[Instance]:
    # FIX: Currently we should only have one supertype per interface, so no
    #      need to return an array
    result: list[Instance] = []
    for path in class_derivation_paths(instance.type, supertype):
        types = [instance]
        for sup in path:
            a: list[Instance] = []
            for t in types:
                a.extend(map_instance_to_direct_supertypes(t, sup))
            types = a
        result.extend(types)
    if result:
        return result
    else:
        # Nothing. Presumably due to an error. Construct a dummy using Any.
        any_type = AnyType(TypeOfAny.from_error)
        return [Instance(supertype, [any_type] * len(supertype.type_vars))]


</t>
<t tx="ekr.20230831011820.90">def invalid_index_type(
    self,
    index_type: Type,
    expected_type: Type,
    base_str: str,
    context: Context,
    *,
    code: ErrorCode,
) -&gt; None:
    index_str, expected_str = format_type_distinctly(
        index_type, expected_type, options=self.options
    )
    self.fail(
        "Invalid index type {} for {}; expected type {}".format(
            index_str, base_str, expected_str
        ),
        context,
        code=code,
    )

</t>
<t tx="ekr.20230831011820.900">def get_class_decorator_hook_2(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], bool] | None:
    """Update class definition for given class decorators.

    Similar to get_class_decorator_hook, but this runs in a later pass when
    placeholders have been resolved.

    The hook can return False if some base class hasn't been
    processed yet using class hooks. It causes all class hooks
    (that are run in this same pass) to be invoked another time for
    the file(s) currently being processed.

    The hook can be called multiple times per class, so it must be
    idempotent.
    """
    return None

</t>
<t tx="ekr.20230831011820.901">def get_metaclass_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    """Update class definition for given declared metaclasses.

    Same as get_class_decorator_hook() but for metaclasses. Note:
    this hook will be only called for explicit metaclasses, not for
    inherited ones.

    TODO: probably it should also be called on inherited metaclasses.
    """
    return None

</t>
<t tx="ekr.20230831011820.902">def get_base_class_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    """Update class definition for given base classes.

    Same as get_class_decorator_hook() but for base classes. Base classes
    don't need to refer to TypeInfos, if a base class refers to a variable with
    Any type, this hook will still be called.
    """
    return None

</t>
<t tx="ekr.20230831011820.903">def get_customize_class_mro_hook(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], None] | None:
    """Customize MRO for given classes.

    The plugin can modify the class MRO _in place_. This method is called
    with the class full name before its body was semantically analyzed.
    """
    return None

</t>
<t tx="ekr.20230831011820.904">def get_dynamic_class_hook(
    self, fullname: str
) -&gt; Callable[[DynamicClassDefContext], None] | None:
    """Semantically analyze a dynamic class definition.

    This plugin hook allows one to semantically analyze dynamic class definitions like:

        from lib import dynamic_class

        X = dynamic_class('X', [])

    For such definition, this hook will be called with 'lib.dynamic_class'.
    The plugin should create the corresponding TypeInfo, and place it into a relevant
    symbol table, e.g. using ctx.api.add_symbol_table_node().
    """
    return None


</t>
<t tx="ekr.20230831011820.905">T = TypeVar("T")


class ChainedPlugin(Plugin):
    """A plugin that represents a sequence of chained plugins.

    Each lookup method returns the hook for the first plugin that
    reports a match.

    This class should not be subclassed -- use Plugin as the base class
    for all plugins.
    """

    @others
</t>
<t tx="ekr.20230831011820.906"># TODO: Support caching of lookup results (through a LRU cache, for example).

def __init__(self, options: Options, plugins: list[Plugin]) -&gt; None:
    """Initialize chained plugin.

    Assume that the child plugins aren't mutated (results may be cached).
    """
    super().__init__(options)
    self._plugins = plugins

</t>
<t tx="ekr.20230831011820.907">def set_modules(self, modules: dict[str, MypyFile]) -&gt; None:
    for plugin in self._plugins:
        plugin.set_modules(modules)

</t>
<t tx="ekr.20230831011820.908">def report_config_data(self, ctx: ReportConfigContext) -&gt; Any:
    config_data = [plugin.report_config_data(ctx) for plugin in self._plugins]
    return config_data if any(x is not None for x in config_data) else None

</t>
<t tx="ekr.20230831011820.909">def get_additional_deps(self, file: MypyFile) -&gt; list[tuple[int, str, int]]:
    deps = []
    for plugin in self._plugins:
        deps.extend(plugin.get_additional_deps(file))
    return deps

</t>
<t tx="ekr.20230831011820.91">def too_few_arguments(
    self, callee: CallableType, context: Context, argument_names: Sequence[str | None] | None
) -&gt; None:
    if self.prefer_simple_messages():
        msg = "Too few arguments"
    elif argument_names is not None:
        num_positional_args = sum(k is None for k in argument_names)
        arguments_left = callee.arg_names[num_positional_args : callee.min_args]
        diff = [k for k in arguments_left if k not in argument_names]
        if len(diff) == 1:
            msg = "Missing positional argument"
        else:
            msg = "Missing positional arguments"
        callee_name = callable_name(callee)
        if callee_name is not None and diff and all(d is not None for d in diff):
            args = '", "'.join(cast(List[str], diff))
            msg += f' "{args}" in call to {callee_name}'
        else:
            msg = "Too few arguments" + for_function(callee)

    else:
        msg = "Too few arguments" + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)

</t>
<t tx="ekr.20230831011820.910">def get_type_analyze_hook(self, fullname: str) -&gt; Callable[[AnalyzeTypeContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_type_analyze_hook(fullname))

</t>
<t tx="ekr.20230831011820.911">def get_function_signature_hook(
    self, fullname: str
) -&gt; Callable[[FunctionSigContext], FunctionLike] | None:
    return self._find_hook(lambda plugin: plugin.get_function_signature_hook(fullname))

</t>
<t tx="ekr.20230831011820.912">def get_function_hook(self, fullname: str) -&gt; Callable[[FunctionContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_function_hook(fullname))

</t>
<t tx="ekr.20230831011820.913">def get_method_signature_hook(
    self, fullname: str
) -&gt; Callable[[MethodSigContext], FunctionLike] | None:
    return self._find_hook(lambda plugin: plugin.get_method_signature_hook(fullname))

</t>
<t tx="ekr.20230831011820.914">def get_method_hook(self, fullname: str) -&gt; Callable[[MethodContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_method_hook(fullname))

</t>
<t tx="ekr.20230831011820.915">def get_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_attribute_hook(fullname))

</t>
<t tx="ekr.20230831011820.916">def get_class_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_class_attribute_hook(fullname))

</t>
<t tx="ekr.20230831011820.917">def get_class_decorator_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_class_decorator_hook(fullname))

</t>
<t tx="ekr.20230831011820.918">def get_class_decorator_hook_2(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], bool] | None:
    return self._find_hook(lambda plugin: plugin.get_class_decorator_hook_2(fullname))

</t>
<t tx="ekr.20230831011820.919">def get_metaclass_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_metaclass_hook(fullname))

</t>
<t tx="ekr.20230831011820.92">def missing_named_argument(self, callee: CallableType, context: Context, name: str) -&gt; None:
    msg = f'Missing named argument "{name}"' + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)

</t>
<t tx="ekr.20230831011820.920">def get_base_class_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_base_class_hook(fullname))

</t>
<t tx="ekr.20230831011820.921">def get_customize_class_mro_hook(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_customize_class_mro_hook(fullname))

</t>
<t tx="ekr.20230831011820.922">def get_dynamic_class_hook(
    self, fullname: str
) -&gt; Callable[[DynamicClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_dynamic_class_hook(fullname))

</t>
<t tx="ekr.20230831011820.923">def _find_hook(self, lookup: Callable[[Plugin], T]) -&gt; T | None:
    for plugin in self._plugins:
        hook = lookup(plugin)
        if hook:
            return hook
    return None
</t>
<t tx="ekr.20230831011820.924">@path mypy
&lt;&lt; pyinfo.py: docstring &gt;&gt;
&lt;&lt; pyinfo.py: declarations &gt;&gt;
@others


if __name__ == "__main__":
    if sys.argv[-1] == "getsearchdirs":
        print(repr(getsearchdirs()))
    else:
        print("ERROR: incorrect argument to pyinfo.py.", file=sys.stderr)
        sys.exit(1)
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.925">from __future__ import annotations

"""Utilities to find the site and prefix information of a Python executable.

This file MUST remain compatible with all Python 3.8+ versions. Since we cannot make any
assumptions about the Python being executed, this module should not use *any* dependencies outside
of the standard library found in Python 3.8. This file is run each mypy run, so it should be kept
as fast as possible.
"""
</t>
<t tx="ekr.20230831011820.926">import sys

if __name__ == "__main__":
    # HACK: We don't want to pick up mypy.types as the top-level types
    #       module. This could happen if this file is run as a script.
    #       This workaround fixes this for Python versions before 3.11.
    if sys.version_info &lt; (3, 11):
        old_sys_path = sys.path
        sys.path = sys.path[1:]
        import types  # noqa: F401

        sys.path = old_sys_path

import os
import site
import sysconfig


</t>
<t tx="ekr.20230831011820.927">def getsitepackages() -&gt; list[str]:
    res = []
    if hasattr(site, "getsitepackages"):
        res.extend(site.getsitepackages())

        if hasattr(site, "getusersitepackages") and site.ENABLE_USER_SITE:
            res.insert(0, site.getusersitepackages())
    else:
        res = [sysconfig.get_paths()["purelib"]]
    return res


</t>
<t tx="ekr.20230831011820.928">def getsyspath() -&gt; list[str]:
    # Do not include things from the standard library
    # because those should come from typeshed.
    stdlib_zip = os.path.join(
        sys.base_exec_prefix,
        getattr(sys, "platlibdir", "lib"),
        f"python{sys.version_info.major}{sys.version_info.minor}.zip",
    )
    stdlib = sysconfig.get_path("stdlib")
    stdlib_ext = os.path.join(stdlib, "lib-dynload")
    excludes = {stdlib_zip, stdlib, stdlib_ext}

    # Drop the first entry of sys.path
    # - If pyinfo.py is executed as a script (in a subprocess), this is the directory
    #   containing pyinfo.py
    # - Otherwise, if mypy launched via console script, this is the directory of the script
    # - Otherwise, if mypy launched via python -m mypy, this is the current directory
    # In all these cases, it is desirable to drop the first entry
    # Note that mypy adds the cwd to SearchPaths.python_path, so we still find things on the
    # cwd consistently (the return value here sets SearchPaths.package_path)

    # Python 3.11 adds a "safe_path" flag wherein Python won't automatically prepend
    # anything to sys.path. In this case, the first entry of sys.path is no longer special.
    offset = 0 if sys.version_info &gt;= (3, 11) and sys.flags.safe_path else 1

    abs_sys_path = (os.path.abspath(p) for p in sys.path[offset:])
    return [p for p in abs_sys_path if p not in excludes]


</t>
<t tx="ekr.20230831011820.929">def getsearchdirs() -&gt; tuple[list[str], list[str]]:
    return (getsyspath(), getsitepackages())
</t>
<t tx="ekr.20230831011820.93">def too_many_arguments(self, callee: CallableType, context: Context) -&gt; None:
    if self.prefer_simple_messages():
        msg = "Too many arguments"
    else:
        msg = "Too many arguments" + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)
    self.maybe_note_about_special_args(callee, context)

</t>
<t tx="ekr.20230831011820.930">@path mypy
"""Utilities related to determining the reachability of code (in semantic analysis)."""
&lt;&lt; reachability.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.931">
from __future__ import annotations

from typing import Final, Tuple, TypeVar

from mypy.literals import literal
from mypy.nodes import (
    LITERAL_YES,
    AssertStmt,
    Block,
    CallExpr,
    ComparisonExpr,
    Expression,
    FuncDef,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    MatchStmt,
    MemberExpr,
    NameExpr,
    OpExpr,
    SliceExpr,
    StrExpr,
    TupleExpr,
    UnaryExpr,
)
from mypy.options import Options
from mypy.patterns import AsPattern, OrPattern, Pattern
from mypy.traverser import TraverserVisitor

# Inferred truth value of an expression.
ALWAYS_TRUE: Final = 1
MYPY_TRUE: Final = 2  # True in mypy, False at runtime
ALWAYS_FALSE: Final = 3
MYPY_FALSE: Final = 4  # False in mypy, True at runtime
TRUTH_VALUE_UNKNOWN: Final = 5

inverted_truth_mapping: Final = {
    ALWAYS_TRUE: ALWAYS_FALSE,
    ALWAYS_FALSE: ALWAYS_TRUE,
    TRUTH_VALUE_UNKNOWN: TRUTH_VALUE_UNKNOWN,
    MYPY_TRUE: MYPY_FALSE,
    MYPY_FALSE: MYPY_TRUE,
}

reverse_op: Final = {"==": "==", "!=": "!=", "&lt;": "&gt;", "&gt;": "&lt;", "&lt;=": "&gt;=", "&gt;=": "&lt;="}


</t>
<t tx="ekr.20230831011820.932">def infer_reachability_of_if_statement(s: IfStmt, options: Options) -&gt; None:
    for i in range(len(s.expr)):
        result = infer_condition_value(s.expr[i], options)
        if result in (ALWAYS_FALSE, MYPY_FALSE):
            # The condition is considered always false, so we skip the if/elif body.
            mark_block_unreachable(s.body[i])
        elif result in (ALWAYS_TRUE, MYPY_TRUE):
            # This condition is considered always true, so all of the remaining
            # elif/else bodies should not be checked.
            if result == MYPY_TRUE:
                # This condition is false at runtime; this will affect
                # import priorities.
                mark_block_mypy_only(s.body[i])
            for body in s.body[i + 1 :]:
                mark_block_unreachable(body)

            # Make sure else body always exists and is marked as
            # unreachable so the type checker always knows that
            # all control flow paths will flow through the if
            # statement body.
            if not s.else_body:
                s.else_body = Block([])
            mark_block_unreachable(s.else_body)
            break


</t>
<t tx="ekr.20230831011820.933">def infer_reachability_of_match_statement(s: MatchStmt, options: Options) -&gt; None:
    for i, guard in enumerate(s.guards):
        pattern_value = infer_pattern_value(s.patterns[i])

        if guard is not None:
            guard_value = infer_condition_value(guard, options)
        else:
            guard_value = ALWAYS_TRUE

        if pattern_value in (ALWAYS_FALSE, MYPY_FALSE) or guard_value in (
            ALWAYS_FALSE,
            MYPY_FALSE,
        ):
            # The case is considered always false, so we skip the case body.
            mark_block_unreachable(s.bodies[i])
        elif pattern_value in (ALWAYS_FALSE, MYPY_TRUE) and guard_value in (
            ALWAYS_TRUE,
            MYPY_TRUE,
        ):
            for body in s.bodies[i + 1 :]:
                mark_block_unreachable(body)

        if guard_value == MYPY_TRUE:
            # This condition is false at runtime; this will affect
            # import priorities.
            mark_block_mypy_only(s.bodies[i])


</t>
<t tx="ekr.20230831011820.934">def assert_will_always_fail(s: AssertStmt, options: Options) -&gt; bool:
    return infer_condition_value(s.expr, options) in (ALWAYS_FALSE, MYPY_FALSE)


</t>
<t tx="ekr.20230831011820.935">def infer_condition_value(expr: Expression, options: Options) -&gt; int:
    """Infer whether the given condition is always true/false.

    Return ALWAYS_TRUE if always true, ALWAYS_FALSE if always false,
    MYPY_TRUE if true under mypy and false at runtime, MYPY_FALSE if
    false under mypy and true at runtime, else TRUTH_VALUE_UNKNOWN.
    """
    pyversion = options.python_version
    name = ""
    negated = False
    alias = expr
    if isinstance(alias, UnaryExpr):
        if alias.op == "not":
            expr = alias.expr
            negated = True
    result = TRUTH_VALUE_UNKNOWN
    if isinstance(expr, NameExpr):
        name = expr.name
    elif isinstance(expr, MemberExpr):
        name = expr.name
    elif isinstance(expr, OpExpr) and expr.op in ("and", "or"):
        left = infer_condition_value(expr.left, options)
        if (left in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "and") or (
            left in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "or"
        ):
            # Either `True and &lt;other&gt;` or `False or &lt;other&gt;`: the result will
            # always be the right-hand-side.
            return infer_condition_value(expr.right, options)
        else:
            # The result will always be the left-hand-side (e.g. ALWAYS_* or
            # TRUTH_VALUE_UNKNOWN).
            return left
    else:
        result = consider_sys_version_info(expr, pyversion)
        if result == TRUTH_VALUE_UNKNOWN:
            result = consider_sys_platform(expr, options.platform)
    if result == TRUTH_VALUE_UNKNOWN:
        if name == "PY2":
            result = ALWAYS_FALSE
        elif name == "PY3":
            result = ALWAYS_TRUE
        elif name == "MYPY" or name == "TYPE_CHECKING":
            result = MYPY_TRUE
        elif name in options.always_true:
            result = ALWAYS_TRUE
        elif name in options.always_false:
            result = ALWAYS_FALSE
    if negated:
        result = inverted_truth_mapping[result]
    return result


</t>
<t tx="ekr.20230831011820.936">def infer_pattern_value(pattern: Pattern) -&gt; int:
    if isinstance(pattern, AsPattern) and pattern.pattern is None:
        return ALWAYS_TRUE
    elif isinstance(pattern, OrPattern) and any(
        infer_pattern_value(p) == ALWAYS_TRUE for p in pattern.patterns
    ):
        return ALWAYS_TRUE
    else:
        return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20230831011820.937">def consider_sys_version_info(expr: Expression, pyversion: tuple[int, ...]) -&gt; int:
    """Consider whether expr is a comparison involving sys.version_info.

    Return ALWAYS_TRUE, ALWAYS_FALSE, or TRUTH_VALUE_UNKNOWN.
    """
    # Cases supported:
    # - sys.version_info[&lt;int&gt;] &lt;compare_op&gt; &lt;int&gt;
    # - sys.version_info[:&lt;int&gt;] &lt;compare_op&gt; &lt;tuple_of_n_ints&gt;
    # - sys.version_info &lt;compare_op&gt; &lt;tuple_of_1_or_2_ints&gt;
    #   (in this case &lt;compare_op&gt; must be &gt;, &gt;=, &lt;, &lt;=, but cannot be ==, !=)
    if not isinstance(expr, ComparisonExpr):
        return TRUTH_VALUE_UNKNOWN
    # Let's not yet support chained comparisons.
    if len(expr.operators) &gt; 1:
        return TRUTH_VALUE_UNKNOWN
    op = expr.operators[0]
    if op not in ("==", "!=", "&lt;=", "&gt;=", "&lt;", "&gt;"):
        return TRUTH_VALUE_UNKNOWN

    index = contains_sys_version_info(expr.operands[0])
    thing = contains_int_or_tuple_of_ints(expr.operands[1])
    if index is None or thing is None:
        index = contains_sys_version_info(expr.operands[1])
        thing = contains_int_or_tuple_of_ints(expr.operands[0])
        op = reverse_op[op]
    if isinstance(index, int) and isinstance(thing, int):
        # sys.version_info[i] &lt;compare_op&gt; k
        if 0 &lt;= index &lt;= 1:
            return fixed_comparison(pyversion[index], op, thing)
        else:
            return TRUTH_VALUE_UNKNOWN
    elif isinstance(index, tuple) and isinstance(thing, tuple):
        lo, hi = index
        if lo is None:
            lo = 0
        if hi is None:
            hi = 2
        if 0 &lt;= lo &lt; hi &lt;= 2:
            val = pyversion[lo:hi]
            if len(val) == len(thing) or len(val) &gt; len(thing) and op not in ("==", "!="):
                return fixed_comparison(val, op, thing)
    return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20230831011820.938">def consider_sys_platform(expr: Expression, platform: str) -&gt; int:
    """Consider whether expr is a comparison involving sys.platform.

    Return ALWAYS_TRUE, ALWAYS_FALSE, or TRUTH_VALUE_UNKNOWN.
    """
    # Cases supported:
    # - sys.platform == 'posix'
    # - sys.platform != 'win32'
    # - sys.platform.startswith('win')
    if isinstance(expr, ComparisonExpr):
        # Let's not yet support chained comparisons.
        if len(expr.operators) &gt; 1:
            return TRUTH_VALUE_UNKNOWN
        op = expr.operators[0]
        if op not in ("==", "!="):
            return TRUTH_VALUE_UNKNOWN
        if not is_sys_attr(expr.operands[0], "platform"):
            return TRUTH_VALUE_UNKNOWN
        right = expr.operands[1]
        if not isinstance(right, StrExpr):
            return TRUTH_VALUE_UNKNOWN
        return fixed_comparison(platform, op, right.value)
    elif isinstance(expr, CallExpr):
        if not isinstance(expr.callee, MemberExpr):
            return TRUTH_VALUE_UNKNOWN
        if len(expr.args) != 1 or not isinstance(expr.args[0], StrExpr):
            return TRUTH_VALUE_UNKNOWN
        if not is_sys_attr(expr.callee.expr, "platform"):
            return TRUTH_VALUE_UNKNOWN
        if expr.callee.name != "startswith":
            return TRUTH_VALUE_UNKNOWN
        if platform.startswith(expr.args[0].value):
            return ALWAYS_TRUE
        else:
            return ALWAYS_FALSE
    else:
        return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20230831011820.939">Targ = TypeVar("Targ", int, str, Tuple[int, ...])


def fixed_comparison(left: Targ, op: str, right: Targ) -&gt; int:
    rmap = {False: ALWAYS_FALSE, True: ALWAYS_TRUE}
    if op == "==":
        return rmap[left == right]
    if op == "!=":
        return rmap[left != right]
    if op == "&lt;=":
        return rmap[left &lt;= right]
    if op == "&gt;=":
        return rmap[left &gt;= right]
    if op == "&lt;":
        return rmap[left &lt; right]
    if op == "&gt;":
        return rmap[left &gt; right]
    return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20230831011820.94">def too_many_arguments_from_typed_dict(
    self, callee: CallableType, arg_type: TypedDictType, context: Context
) -&gt; None:
    # Try to determine the name of the extra argument.
    for key in arg_type.items:
        if key not in callee.arg_names:
            msg = f'Extra argument "{key}" from **args' + for_function(callee)
            break
    else:
        self.too_many_arguments(callee, context)
        return
    self.fail(msg, context)

</t>
<t tx="ekr.20230831011820.940">def contains_int_or_tuple_of_ints(expr: Expression) -&gt; None | int | tuple[int, ...]:
    if isinstance(expr, IntExpr):
        return expr.value
    if isinstance(expr, TupleExpr):
        if literal(expr) == LITERAL_YES:
            thing = []
            for x in expr.items:
                if not isinstance(x, IntExpr):
                    return None
                thing.append(x.value)
            return tuple(thing)
    return None


</t>
<t tx="ekr.20230831011820.941">def contains_sys_version_info(expr: Expression) -&gt; None | int | tuple[int | None, int | None]:
    if is_sys_attr(expr, "version_info"):
        return (None, None)  # Same as sys.version_info[:]
    if isinstance(expr, IndexExpr) and is_sys_attr(expr.base, "version_info"):
        index = expr.index
        if isinstance(index, IntExpr):
            return index.value
        if isinstance(index, SliceExpr):
            if index.stride is not None:
                if not isinstance(index.stride, IntExpr) or index.stride.value != 1:
                    return None
            begin = end = None
            if index.begin_index is not None:
                if not isinstance(index.begin_index, IntExpr):
                    return None
                begin = index.begin_index.value
            if index.end_index is not None:
                if not isinstance(index.end_index, IntExpr):
                    return None
                end = index.end_index.value
            return (begin, end)
    return None


</t>
<t tx="ekr.20230831011820.942">def is_sys_attr(expr: Expression, name: str) -&gt; bool:
    # TODO: This currently doesn't work with code like this:
    # - import sys as _sys
    # - from sys import version_info
    if isinstance(expr, MemberExpr) and expr.name == name:
        if isinstance(expr.expr, NameExpr) and expr.expr.name == "sys":
            # TODO: Guard against a local named sys, etc.
            # (Though later passes will still do most checking.)
            return True
    return False


</t>
<t tx="ekr.20230831011820.943">def mark_block_unreachable(block: Block) -&gt; None:
    block.is_unreachable = True
    block.accept(MarkImportsUnreachableVisitor())


</t>
<t tx="ekr.20230831011820.944">class MarkImportsUnreachableVisitor(TraverserVisitor):
    """Visitor that flags all imports nested within a node as unreachable."""

    @others
</t>
<t tx="ekr.20230831011820.945">def visit_import(self, node: Import) -&gt; None:
    node.is_unreachable = True

</t>
<t tx="ekr.20230831011820.946">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_unreachable = True

</t>
<t tx="ekr.20230831011820.947">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_unreachable = True


</t>
<t tx="ekr.20230831011820.948">def mark_block_mypy_only(block: Block) -&gt; None:
    block.accept(MarkImportsMypyOnlyVisitor())


</t>
<t tx="ekr.20230831011820.949">class MarkImportsMypyOnlyVisitor(TraverserVisitor):
    """Visitor that sets is_mypy_only (which affects priority)."""

    @others
</t>
<t tx="ekr.20230831011820.95">def too_many_positional_arguments(self, callee: CallableType, context: Context) -&gt; None:
    if self.prefer_simple_messages():
        msg = "Too many positional arguments"
    else:
        msg = "Too many positional arguments" + for_function(callee)
    self.fail(msg, context)
    self.maybe_note_about_special_args(callee, context)

</t>
<t tx="ekr.20230831011820.950">def visit_import(self, node: Import) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20230831011820.951">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20230831011820.952">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20230831011820.953">def visit_func_def(self, node: FuncDef) -&gt; None:
    node.is_mypy_only = True
</t>
<t tx="ekr.20230831011820.954">@path mypy
"""Find line-level reference information from a mypy AST (undocumented feature)"""
&lt;&lt; refinfo.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.955">
from __future__ import annotations

from mypy.nodes import (
    LDEF,
    Expression,
    FuncDef,
    MemberExpr,
    MypyFile,
    NameExpr,
    RefExpr,
    SymbolNode,
    TypeInfo,
)
from mypy.traverser import TraverserVisitor
from mypy.typeops import tuple_fallback
from mypy.types import (
    FunctionLike,
    Instance,
    TupleType,
    Type,
    TypeType,
    TypeVarLikeType,
    get_proper_type,
)


</t>
<t tx="ekr.20230831011820.956">class RefInfoVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011820.957">def __init__(self, type_map: dict[Expression, Type]) -&gt; None:
    super().__init__()
    self.type_map = type_map
    self.data: list[dict[str, object]] = []

</t>
<t tx="ekr.20230831011820.958">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    super().visit_name_expr(expr)
    self.record_ref_expr(expr)

</t>
<t tx="ekr.20230831011820.959">def visit_member_expr(self, expr: MemberExpr) -&gt; None:
    super().visit_member_expr(expr)
    self.record_ref_expr(expr)

</t>
<t tx="ekr.20230831011820.96">def maybe_note_about_special_args(self, callee: CallableType, context: Context) -&gt; None:
    if self.prefer_simple_messages():
        return
    # https://github.com/python/mypy/issues/11309
    first_arg = callee.def_extras.get("first_arg")
    if first_arg and first_arg not in {"self", "cls", "mcs"}:
        self.note(
            "Looks like the first special argument in a method "
            'is not named "self", "cls", or "mcs", '
            "maybe it is missing?",
            context,
        )

</t>
<t tx="ekr.20230831011820.960">def visit_func_def(self, func: FuncDef) -&gt; None:
    if func.expanded:
        for item in func.expanded:
            if isinstance(item, FuncDef):
                super().visit_func_def(item)
    else:
        super().visit_func_def(func)

</t>
<t tx="ekr.20230831011820.961">def record_ref_expr(self, expr: RefExpr) -&gt; None:
    fullname = None
    if expr.kind != LDEF and "." in expr.fullname:
        fullname = expr.fullname
    elif isinstance(expr, MemberExpr):
        typ = self.type_map.get(expr.expr)
        sym = None
        if isinstance(expr.expr, RefExpr):
            sym = expr.expr.node
        if typ:
            tfn = type_fullname(typ, sym)
            if tfn:
                fullname = f"{tfn}.{expr.name}"
        if not fullname:
            fullname = f"*.{expr.name}"
    if fullname is not None:
        self.data.append({"line": expr.line, "column": expr.column, "target": fullname})


</t>
<t tx="ekr.20230831011820.962">def type_fullname(typ: Type, node: SymbolNode | None = None) -&gt; str | None:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return typ.type.fullname
    elif isinstance(typ, TypeType):
        return type_fullname(typ.item)
    elif isinstance(typ, FunctionLike) and typ.is_type_obj():
        if isinstance(node, TypeInfo):
            return node.fullname
        return type_fullname(typ.fallback)
    elif isinstance(typ, TupleType):
        return type_fullname(tuple_fallback(typ))
    elif isinstance(typ, TypeVarLikeType):
        return type_fullname(typ.upper_bound)
    return None


</t>
<t tx="ekr.20230831011820.963">def get_undocumented_ref_info_json(
    tree: MypyFile, type_map: dict[Expression, Type]
) -&gt; list[dict[str, object]]:
    visitor = RefInfoVisitor(type_map)
    tree.accept(visitor)
    return visitor.data
</t>
<t tx="ekr.20230831011820.964">@path mypy
&lt;&lt; renaming.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011820.965">from __future__ import annotations

from contextlib import contextmanager
from typing import Final, Iterator

from mypy.nodes import (
    AssignmentStmt,
    Block,
    BreakStmt,
    ClassDef,
    ContinueStmt,
    ForStmt,
    FuncDef,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    ListExpr,
    Lvalue,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NameExpr,
    StarExpr,
    TryStmt,
    TupleExpr,
    WhileStmt,
    WithStmt,
)
from mypy.patterns import AsPattern
from mypy.traverser import TraverserVisitor

# Scope kinds
FILE: Final = 0
FUNCTION: Final = 1
CLASS: Final = 2


</t>
<t tx="ekr.20230831011820.966">class VariableRenameVisitor(TraverserVisitor):
    """Rename variables to allow redefinition of variables.

    For example, consider this code:

      x = 0
      f(x)

      x = "a"
      g(x)

    It will be transformed like this:

      x' = 0
      f(x')

      x = "a"
      g(x)

    There will be two independent variables (x' and x) that will have separate
    inferred types. The publicly exposed variant will get the non-suffixed name.
    This is the last definition at module top level and the first definition
    (argument) within a function.

    Renaming only happens for assignments within the same block. Renaming is
    performed before semantic analysis, immediately after parsing.

    The implementation performs a rudimentary static analysis. The analysis is
    overly conservative to keep things simple.
    """

    @others
</t>
<t tx="ekr.20230831011820.967">def __init__(self) -&gt; None:
    # Counter for labeling new blocks
    self.block_id = 0
    # Number of surrounding try statements that disallow variable redefinition
    self.disallow_redef_depth = 0
    # Number of surrounding loop statements
    self.loop_depth = 0
    # Map block id to loop depth.
    self.block_loop_depth: dict[int, int] = {}
    # Stack of block ids being processed.
    self.blocks: list[int] = []
    # List of scopes; each scope maps short (unqualified) name to block id.
    self.var_blocks: list[dict[str, int]] = []

    # References to variables that we may need to rename. List of
    # scopes; each scope is a mapping from name to list of collections
    # of names that refer to the same logical variable.
    self.refs: list[dict[str, list[list[NameExpr]]]] = []
    # Number of reads of the most recent definition of a variable (per scope)
    self.num_reads: list[dict[str, int]] = []
    # Kinds of nested scopes (FILE, FUNCTION or CLASS)
    self.scope_kinds: list[int] = []

</t>
<t tx="ekr.20230831011820.968">def visit_mypy_file(self, file_node: MypyFile) -&gt; None:
    """Rename variables within a file.

    This is the main entry point to this class.
    """
    self.clear()
    with self.enter_scope(FILE), self.enter_block():
        for d in file_node.defs:
            d.accept(self)

</t>
<t tx="ekr.20230831011820.969">def visit_func_def(self, fdef: FuncDef) -&gt; None:
    # Conservatively do not allow variable defined before a function to
    # be redefined later, since function could refer to either definition.
    self.reject_redefinition_of_vars_in_scope()

    with self.enter_scope(FUNCTION), self.enter_block():
        for arg in fdef.arguments:
            name = arg.variable.name
            # 'self' can't be redefined since it's special as it allows definition of
            # attributes. 'cls' can't be used to define attributes so we can ignore it.
            can_be_redefined = name != "self"  # TODO: Proper check
            self.record_assignment(arg.variable.name, can_be_redefined)
            self.handle_arg(name)

        for stmt in fdef.body.body:
            stmt.accept(self)

</t>
<t tx="ekr.20230831011820.97">def unexpected_keyword_argument(
    self, callee: CallableType, name: str, arg_type: Type, context: Context
) -&gt; None:
    msg = f'Unexpected keyword argument "{name}"' + for_function(callee)
    # Suggest intended keyword, look for type match else fallback on any match.
    matching_type_args = []
    not_matching_type_args = []
    for i, kwarg_type in enumerate(callee.arg_types):
        callee_arg_name = callee.arg_names[i]
        if callee_arg_name is not None and callee.arg_kinds[i] != ARG_STAR:
            if is_subtype(arg_type, kwarg_type):
                matching_type_args.append(callee_arg_name)
            else:
                not_matching_type_args.append(callee_arg_name)
    matches = best_matches(name, matching_type_args, n=3)
    if not matches:
        matches = best_matches(name, not_matching_type_args, n=3)
    if matches:
        msg += f"; did you mean {pretty_seq(matches, 'or')}?"
    self.fail(msg, context, code=codes.CALL_ARG)
    module = find_defining_module(self.modules, callee)
    if module:
        assert callee.definition is not None
        fname = callable_name(callee)
        if not fname:  # an alias to function with a different name
            fname = "Called function"
        self.note(
            f"{fname} defined here",
            callee.definition,
            file=module.path,
            origin=context,
            code=codes.CALL_ARG,
        )

</t>
<t tx="ekr.20230831011820.970">def visit_class_def(self, cdef: ClassDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope(CLASS):
        super().visit_class_def(cdef)

</t>
<t tx="ekr.20230831011820.971">def visit_block(self, block: Block) -&gt; None:
    with self.enter_block():
        super().visit_block(block)

</t>
<t tx="ekr.20230831011820.972">def visit_while_stmt(self, stmt: WhileStmt) -&gt; None:
    with self.enter_loop():
        super().visit_while_stmt(stmt)

</t>
<t tx="ekr.20230831011820.973">def visit_for_stmt(self, stmt: ForStmt) -&gt; None:
    stmt.expr.accept(self)
    self.analyze_lvalue(stmt.index, True)
    # Also analyze as non-lvalue so that every for loop index variable is assumed to be read.
    stmt.index.accept(self)
    with self.enter_loop():
        stmt.body.accept(self)
    if stmt.else_body:
        stmt.else_body.accept(self)

</t>
<t tx="ekr.20230831011820.974">def visit_break_stmt(self, stmt: BreakStmt) -&gt; None:
    self.reject_redefinition_of_vars_in_loop()

</t>
<t tx="ekr.20230831011820.975">def visit_continue_stmt(self, stmt: ContinueStmt) -&gt; None:
    self.reject_redefinition_of_vars_in_loop()

</t>
<t tx="ekr.20230831011820.976">def visit_try_stmt(self, stmt: TryStmt) -&gt; None:
    # Variables defined by a try statement get special treatment in the
    # type checker which allows them to be always redefined, so no need to
    # do renaming here.
    with self.enter_try():
        super().visit_try_stmt(stmt)

</t>
<t tx="ekr.20230831011820.977">def visit_with_stmt(self, stmt: WithStmt) -&gt; None:
    for expr in stmt.expr:
        expr.accept(self)
    for target in stmt.target:
        if target is not None:
            self.analyze_lvalue(target)
    # We allow redefinitions in the body of a with statement for
    # convenience.  This is unsafe since with statements can affect control
    # flow by catching exceptions, but this is rare except for
    # assertRaises() and other similar functions, where the exception is
    # raised by the last statement in the body, which usually isn't a
    # problem.
    stmt.body.accept(self)

</t>
<t tx="ekr.20230831011820.978">def visit_import(self, imp: Import) -&gt; None:
    for id, as_id in imp.ids:
        self.record_assignment(as_id or id, False)

</t>
<t tx="ekr.20230831011820.979">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    for id, as_id in imp.names:
        self.record_assignment(as_id or id, False)

</t>
<t tx="ekr.20230831011820.98">def duplicate_argument_value(self, callee: CallableType, index: int, context: Context) -&gt; None:
    self.fail(
        '{} gets multiple values for keyword argument "{}"'.format(
            callable_name(callee) or "Function", callee.arg_names[index]
        ),
        context,
    )

</t>
<t tx="ekr.20230831011820.980">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    s.rvalue.accept(self)
    for lvalue in s.lvalues:
        self.analyze_lvalue(lvalue)

</t>
<t tx="ekr.20230831011820.981">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    s.subject.accept(self)
    for i in range(len(s.patterns)):
        with self.enter_block():
            s.patterns[i].accept(self)
            guard = s.guards[i]
            if guard is not None:
                guard.accept(self)
            # We already entered a block, so visit this block's statements directly
            for stmt in s.bodies[i].body:
                stmt.accept(self)

</t>
<t tx="ekr.20230831011820.982">def visit_capture_pattern(self, p: AsPattern) -&gt; None:
    if p.name is not None:
        self.analyze_lvalue(p.name)

</t>
<t tx="ekr.20230831011820.983">def analyze_lvalue(self, lvalue: Lvalue, is_nested: bool = False) -&gt; None:
    """Process assignment; in particular, keep track of (re)defined names.

    Args:
        is_nested: True for non-outermost Lvalue in a multiple assignment such as
            "x, y = ..."
    """
    if isinstance(lvalue, NameExpr):
        name = lvalue.name
        is_new = self.record_assignment(name, True)
        if is_new:
            self.handle_def(lvalue)
        else:
            self.handle_refine(lvalue)
        if is_nested:
            # This allows these to be redefined freely even if never read. Multiple
            # assignment like "x, _ _ = y" defines dummy variables that are never read.
            self.handle_ref(lvalue)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.analyze_lvalue(item, is_nested=True)
    elif isinstance(lvalue, MemberExpr):
        lvalue.expr.accept(self)
    elif isinstance(lvalue, IndexExpr):
        lvalue.base.accept(self)
        lvalue.index.accept(self)
    elif isinstance(lvalue, StarExpr):
        # Propagate is_nested since in a typical use case like "x, *rest = ..." 'rest' may
        # be freely reused.
        self.analyze_lvalue(lvalue.expr, is_nested=is_nested)

</t>
<t tx="ekr.20230831011820.984">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    self.handle_ref(expr)

</t>
<t tx="ekr.20230831011820.985"># Helpers for renaming references

def handle_arg(self, name: str) -&gt; None:
    """Store function argument."""
    self.refs[-1][name] = [[]]
    self.num_reads[-1][name] = 0

</t>
<t tx="ekr.20230831011820.986">def handle_def(self, expr: NameExpr) -&gt; None:
    """Store new name definition."""
    name = expr.name
    names = self.refs[-1].setdefault(name, [])
    names.append([expr])
    self.num_reads[-1][name] = 0

</t>
<t tx="ekr.20230831011820.987">def handle_refine(self, expr: NameExpr) -&gt; None:
    """Store assignment to an existing name (that replaces previous value, if any)."""
    name = expr.name
    if name in self.refs[-1]:
        names = self.refs[-1][name]
        if not names:
            names.append([])
        names[-1].append(expr)

</t>
<t tx="ekr.20230831011820.988">def handle_ref(self, expr: NameExpr) -&gt; None:
    """Store reference to defined name."""
    name = expr.name
    if name in self.refs[-1]:
        names = self.refs[-1][name]
        if not names:
            names.append([])
        names[-1].append(expr)
    num_reads = self.num_reads[-1]
    num_reads[name] = num_reads.get(name, 0) + 1

</t>
<t tx="ekr.20230831011820.989">def flush_refs(self) -&gt; None:
    """Rename all references within the current scope.

    This will be called at the end of a scope.
    """
    is_func = self.scope_kinds[-1] == FUNCTION
    for name, refs in self.refs[-1].items():
        if len(refs) == 1:
            # Only one definition -- no renaming needed.
            continue
        if is_func:
            # In a function, don't rename the first definition, as it
            # may be an argument that must preserve the name.
            to_rename = refs[1:]
        else:
            # At module top level, don't rename the final definition,
            # as it will be publicly visible outside the module.
            to_rename = refs[:-1]
        for i, item in enumerate(to_rename):
            rename_refs(item, i)
    self.refs.pop()

</t>
<t tx="ekr.20230831011820.99">def does_not_return_value(self, callee_type: Type | None, context: Context) -&gt; None:
    """Report an error about use of an unusable type."""
    name: str | None = None
    callee_type = get_proper_type(callee_type)
    if isinstance(callee_type, FunctionLike):
        name = callable_name(callee_type)
    if name is not None:
        self.fail(
            f"{capitalize(name)} does not return a value",
            context,
            code=codes.FUNC_RETURNS_VALUE,
        )
    else:
        self.fail("Function does not return a value", context, code=codes.FUNC_RETURNS_VALUE)

</t>
<t tx="ekr.20230831011820.990"># Helpers for determining which assignments define new variables

def clear(self) -&gt; None:
    self.blocks = []
    self.var_blocks = []

</t>
<t tx="ekr.20230831011820.991">@contextmanager
def enter_block(self) -&gt; Iterator[None]:
    self.block_id += 1
    self.blocks.append(self.block_id)
    self.block_loop_depth[self.block_id] = self.loop_depth
    try:
        yield
    finally:
        self.blocks.pop()

</t>
<t tx="ekr.20230831011820.992">@contextmanager
def enter_try(self) -&gt; Iterator[None]:
    self.disallow_redef_depth += 1
    try:
        yield
    finally:
        self.disallow_redef_depth -= 1

</t>
<t tx="ekr.20230831011820.993">@contextmanager
def enter_loop(self) -&gt; Iterator[None]:
    self.loop_depth += 1
    try:
        yield
    finally:
        self.loop_depth -= 1

</t>
<t tx="ekr.20230831011820.994">def current_block(self) -&gt; int:
    return self.blocks[-1]

</t>
<t tx="ekr.20230831011820.995">@contextmanager
def enter_scope(self, kind: int) -&gt; Iterator[None]:
    self.var_blocks.append({})
    self.refs.append({})
    self.num_reads.append({})
    self.scope_kinds.append(kind)
    try:
        yield
    finally:
        self.flush_refs()
        self.var_blocks.pop()
        self.num_reads.pop()
        self.scope_kinds.pop()

</t>
<t tx="ekr.20230831011820.996">def is_nested(self) -&gt; int:
    return len(self.var_blocks) &gt; 1

</t>
<t tx="ekr.20230831011820.997">def reject_redefinition_of_vars_in_scope(self) -&gt; None:
    """Make it impossible to redefine defined variables in the current scope.

    This is used if we encounter a function definition that
    can make it ambiguous which definition is live. Example:

      x = 0

      def f() -&gt; int:
          return x

      x = ''  # Error -- cannot redefine x across function definition
    """
    var_blocks = self.var_blocks[-1]
    for key in var_blocks:
        var_blocks[key] = -1

</t>
<t tx="ekr.20230831011820.998">def reject_redefinition_of_vars_in_loop(self) -&gt; None:
    """Reject redefinition of variables in the innermost loop.

    If there is an early exit from a loop, there may be ambiguity about which
    value may escape the loop. Example where this matters:

      while f():
          x = 0
          if g():
              break
          x = ''  # Error -- not a redefinition
      reveal_type(x)  # int

    This method ensures that the second assignment to 'x' doesn't introduce a new
    variable.
    """
    var_blocks = self.var_blocks[-1]
    for key, block in var_blocks.items():
        if self.block_loop_depth.get(block) == self.loop_depth:
            var_blocks[key] = -1

</t>
<t tx="ekr.20230831011820.999">def record_assignment(self, name: str, can_be_redefined: bool) -&gt; bool:
    """Record assignment to given name and return True if it defines a new variable.

    Args:
        can_be_redefined: If True, allows assignment in the same block to redefine
            this name (if this is a new definition)
    """
    if self.num_reads[-1].get(name, -1) == 0:
        # Only set, not read, so no reason to redefine
        return False
    if self.disallow_redef_depth &gt; 0:
        # Can't redefine within try/with a block.
        can_be_redefined = False
    block = self.current_block()
    var_blocks = self.var_blocks[-1]
    if name not in var_blocks:
        # New definition in this scope.
        if can_be_redefined:
            # Store the block where this was defined to allow redefinition in
            # the same block only.
            var_blocks[name] = block
        else:
            # This doesn't support arbitrary redefinition.
            var_blocks[name] = -1
        return True
    elif var_blocks[name] == block:
        # Redefinition -- defines a new variable with the same name.
        return True
    else:
        # Assigns to an existing variable.
        return False


</t>
<t tx="ekr.20230831011821.1">
from __future__ import annotations

from mypy_extensions import mypyc_attr

from mypy.nodes import (
    REVEAL_TYPE,
    AssertStmt,
    AssertTypeExpr,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    CastExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    ContinueStmt,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    ExpressionStmt,
    FloatExpr,
    ForStmt,
    FuncBase,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    ParamSpecExpr,
    PassStmt,
    RaiseStmt,
    ReturnStmt,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    StrExpr,
    SuperExpr,
    TryStmt,
    TupleExpr,
    TypeAlias,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
)
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.visitor import NodeVisitor


@mypyc_attr(allow_interpreted_subclasses=True)
</t>
<t tx="ekr.20230831011821.10">def visit_decorator(self, o: Decorator) -&gt; None:
    o.func.accept(self)
    o.var.accept(self)
    for decorator in o.decorators:
        decorator.accept(self)

</t>
<t tx="ekr.20230831011821.100">def visit_yield_expr(self, o: YieldExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_yield_expr(o)

</t>
<t tx="ekr.20230831011821.1000">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    # TODO: sometimes we want to traverse target as well
    # We need to find a way to indicate explicitly the intent,
    # maybe make this method abstract (like for TypeTranslator)?
    self.traverse_types(t.args)

</t>
<t tx="ekr.20230831011821.1001">def visit_unpack_type(self, t: UnpackType) -&gt; None:
    t.type.accept(self)

</t>
<t tx="ekr.20230831011821.1002"># Helpers

def traverse_types(self, types: Iterable[Type]) -&gt; None:
    for typ in types:
        typ.accept(self)
</t>
<t tx="ekr.20230831011821.1003">@path mypy
&lt;&lt; typevars.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.1004">from __future__ import annotations

from mypy.erasetype import erase_typevars
from mypy.nodes import TypeInfo
from mypy.types import (
    AnyType,
    Instance,
    ParamSpecType,
    TupleType,
    Type,
    TypeOfAny,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnpackType,
)


</t>
<t tx="ekr.20230831011821.1005">def fill_typevars(typ: TypeInfo) -&gt; Instance | TupleType:
    """For a non-generic type, return instance type representing the type.

    For a generic G type with parameters T1, .., Tn, return G[T1, ..., Tn].
    """
    tvs: list[Type] = []
    # TODO: why do we need to keep both typ.type_vars and typ.defn.type_vars?
    for i in range(len(typ.defn.type_vars)):
        tv: TypeVarLikeType | UnpackType = typ.defn.type_vars[i]
        # Change the line number
        if isinstance(tv, TypeVarType):
            tv = tv.copy_modified(line=-1, column=-1)
        elif isinstance(tv, TypeVarTupleType):
            tv = UnpackType(
                TypeVarTupleType(
                    tv.name,
                    tv.fullname,
                    tv.id,
                    tv.upper_bound,
                    tv.tuple_fallback,
                    tv.default,
                    line=-1,
                    column=-1,
                )
            )
        else:
            assert isinstance(tv, ParamSpecType)
            tv = ParamSpecType(
                tv.name,
                tv.fullname,
                tv.id,
                tv.flavor,
                tv.upper_bound,
                tv.default,
                line=-1,
                column=-1,
            )
        tvs.append(tv)
    inst = Instance(typ, tvs)
    if typ.tuple_type is None:
        return inst
    return typ.tuple_type.copy_modified(fallback=inst)


</t>
<t tx="ekr.20230831011821.1006">def fill_typevars_with_any(typ: TypeInfo) -&gt; Instance | TupleType:
    """Apply a correct number of Any's as type arguments to a type."""
    inst = Instance(typ, [AnyType(TypeOfAny.special_form)] * len(typ.defn.type_vars))
    if typ.tuple_type is None:
        return inst
    return typ.tuple_type.copy_modified(fallback=inst)


</t>
<t tx="ekr.20230831011821.1007">def has_no_typevars(typ: Type) -&gt; bool:
    # We test if a type contains type variables by erasing all type variables
    # and comparing the result to the original type. We use comparison by equality that
    # in turn uses `__eq__` defined for types. Note: we can't use `is_same_type` because
    # it is not safe with unresolved forward references, while this function may be called
    # before forward references resolution patch pass. Note also that it is not safe to use
    # `is` comparison because `erase_typevars` doesn't preserve type identity.
    return typ == erase_typevars(typ)
</t>
<t tx="ekr.20230831011821.1008">@path mypy
"""Helpers for interacting with type var tuples."""
&lt;&lt; typevartuples.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.1009">
from __future__ import annotations

from typing import Sequence

from mypy.types import (
    Instance,
    ProperType,
    Type,
    UnpackType,
    find_unpack_in_list,
    get_proper_type,
    split_with_prefix_and_suffix,
)


</t>
<t tx="ekr.20230831011821.101">def visit_call_expr(self, o: CallExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_call_expr(o)

</t>
<t tx="ekr.20230831011821.1010">def split_with_instance(
    typ: Instance,
) -&gt; tuple[tuple[Type, ...], tuple[Type, ...], tuple[Type, ...]]:
    assert typ.type.type_var_tuple_prefix is not None
    assert typ.type.type_var_tuple_suffix is not None
    return split_with_prefix_and_suffix(
        typ.args, typ.type.type_var_tuple_prefix, typ.type.type_var_tuple_suffix
    )


</t>
<t tx="ekr.20230831011821.1011">def split_with_mapped_and_template(
    mapped: tuple[Type, ...],
    mapped_prefix_len: int | None,
    mapped_suffix_len: int | None,
    template: tuple[Type, ...],
    template_prefix_len: int,
    template_suffix_len: int,
) -&gt; (
    tuple[
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
    ]
    | None
</t>
<t tx="ekr.20230831011821.1012">):
    split_result = fully_split_with_mapped_and_template(
        mapped,
        mapped_prefix_len,
        mapped_suffix_len,
        template,
        template_prefix_len,
        template_suffix_len,
    )
    if split_result is None:
        return None

    (
        mapped_prefix,
        mapped_middle_prefix,
        mapped_middle_middle,
        mapped_middle_suffix,
        mapped_suffix,
        template_prefix,
        template_middle_prefix,
        template_middle_middle,
        template_middle_suffix,
        template_suffix,
    ) = split_result

    return (
        mapped_prefix + mapped_middle_prefix,
        mapped_middle_middle,
        mapped_middle_suffix + mapped_suffix,
        template_prefix + template_middle_prefix,
        template_middle_middle,
        template_middle_suffix + template_suffix,
    )


def fully_split_with_mapped_and_template(
    mapped: tuple[Type, ...],
    mapped_prefix_len: int | None,
    mapped_suffix_len: int | None,
    template: tuple[Type, ...],
    template_prefix_len: int,
    template_suffix_len: int,
) -&gt; (
    tuple[
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
        tuple[Type, ...],
    ]
    | None
</t>
<t tx="ekr.20230831011821.1013">):
    if mapped_prefix_len is not None:
        assert mapped_suffix_len is not None
        mapped_prefix, mapped_middle, mapped_suffix = split_with_prefix_and_suffix(
            tuple(mapped), mapped_prefix_len, mapped_suffix_len
        )
    else:
        mapped_prefix = tuple()
        mapped_suffix = tuple()
        mapped_middle = mapped

    template_prefix, template_middle, template_suffix = split_with_prefix_and_suffix(
        tuple(template), template_prefix_len, template_suffix_len
    )

    unpack_prefix = find_unpack_in_list(template_middle)
    if unpack_prefix is None:
        return (
            mapped_prefix,
            (),
            mapped_middle,
            (),
            mapped_suffix,
            template_prefix,
            (),
            template_middle,
            (),
            template_suffix,
        )

    unpack_suffix = len(template_middle) - unpack_prefix - 1
    # mapped_middle is too short to do the unpack
    if unpack_prefix + unpack_suffix &gt; len(mapped_middle):
        return None

    (
        mapped_middle_prefix,
        mapped_middle_middle,
        mapped_middle_suffix,
    ) = split_with_prefix_and_suffix(mapped_middle, unpack_prefix, unpack_suffix)
    (
        template_middle_prefix,
        template_middle_middle,
        template_middle_suffix,
    ) = split_with_prefix_and_suffix(template_middle, unpack_prefix, unpack_suffix)

    return (
        mapped_prefix,
        mapped_middle_prefix,
        mapped_middle_middle,
        mapped_middle_suffix,
        mapped_suffix,
        template_prefix,
        template_middle_prefix,
        template_middle_middle,
        template_middle_suffix,
        template_suffix,
    )


def extract_unpack(types: Sequence[Type]) -&gt; ProperType | None:
    """Given a list of types, extracts either a single type from an unpack, or returns None."""
    if len(types) == 1:
        if isinstance(types[0], UnpackType):
            return get_proper_type(types[0].type)
    return None
</t>
<t tx="ekr.20230831011821.1014">@path mypy
"""Utility functions with no non-trivial dependencies."""
&lt;&lt; util.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.1015">
from __future__ import annotations

import hashlib
import io
import os
import pathlib
import re
import shutil
import sys
import time
from importlib import resources as importlib_resources
from typing import IO, Callable, Container, Final, Iterable, Sequence, Sized, TypeVar
from typing_extensions import Literal

try:
    import curses

    import _curses  # noqa: F401

    CURSES_ENABLED = True
except ImportError:
    CURSES_ENABLED = False

T = TypeVar("T")

if sys.version_info &gt;= (3, 9):
    TYPESHED_DIR: Final = str(importlib_resources.files("mypy") / "typeshed")
else:
    with importlib_resources.path(
        "mypy",  # mypy-c doesn't support __package__
        "py.typed",  # a marker file for type information, we assume typeshed to live in the same dir
    ) as _resource:
        TYPESHED_DIR = str(_resource.parent / "typeshed")


ENCODING_RE: Final = re.compile(rb"([ \t\v]*#.*(\r\n?|\n))??[ \t\v]*#.*coding[:=][ \t]*([-\w.]+)")

DEFAULT_SOURCE_OFFSET: Final = 4
DEFAULT_COLUMNS: Final = 80

# At least this number of columns will be shown on each side of
# error location when printing source code snippet.
MINIMUM_WIDTH: Final = 20

# VT100 color code processing was added in Windows 10, but only the second major update,
# Threshold 2. Fortunately, everyone (even on LTSB, Long Term Support Branch) should
# have a version of Windows 10 newer than this. Note that Windows 8 and below are not
# supported, but are either going out of support, or make up only a few % of the market.
MINIMUM_WINDOWS_MAJOR_VT100: Final = 10
MINIMUM_WINDOWS_BUILD_VT100: Final = 10586

SPECIAL_DUNDERS: Final = frozenset(
    ("__init__", "__new__", "__call__", "__init_subclass__", "__class_getitem__")
)


</t>
<t tx="ekr.20230831011821.1016">def is_dunder(name: str, exclude_special: bool = False) -&gt; bool:
    """Returns whether name is a dunder name.

    Args:
        exclude_special: Whether to return False for a couple special dunder methods.

    """
    if exclude_special and name in SPECIAL_DUNDERS:
        return False
    return name.startswith("__") and name.endswith("__")


</t>
<t tx="ekr.20230831011821.1017">def is_sunder(name: str) -&gt; bool:
    return not is_dunder(name) and name.startswith("_") and name.endswith("_")


</t>
<t tx="ekr.20230831011821.1018">def split_module_names(mod_name: str) -&gt; list[str]:
    """Return the module and all parent module names.

    So, if `mod_name` is 'a.b.c', this function will return
    ['a.b.c', 'a.b', and 'a'].
    """
    out = [mod_name]
    while "." in mod_name:
        mod_name = mod_name.rsplit(".", 1)[0]
        out.append(mod_name)
    return out


</t>
<t tx="ekr.20230831011821.1019">def module_prefix(modules: Iterable[str], target: str) -&gt; str | None:
    result = split_target(modules, target)
    if result is None:
        return None
    return result[0]


</t>
<t tx="ekr.20230831011821.102">def visit_op_expr(self, o: OpExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_op_expr(o)

</t>
<t tx="ekr.20230831011821.1020">def split_target(modules: Iterable[str], target: str) -&gt; tuple[str, str] | None:
    remaining: list[str] = []
    while True:
        if target in modules:
            return target, ".".join(remaining)
        components = target.rsplit(".", 1)
        if len(components) == 1:
            return None
        target = components[0]
        remaining.insert(0, components[1])


</t>
<t tx="ekr.20230831011821.1021">def short_type(obj: object) -&gt; str:
    """Return the last component of the type name of an object.

    If obj is None, return 'nil'. For example, if obj is 1, return 'int'.
    """
    if obj is None:
        return "nil"
    t = str(type(obj))
    return t.split(".")[-1].rstrip("'&gt;")


</t>
<t tx="ekr.20230831011821.1022">def find_python_encoding(text: bytes) -&gt; tuple[str, int]:
    """PEP-263 for detecting Python file encoding"""
    result = ENCODING_RE.match(text)
    if result:
        line = 2 if result.group(1) else 1
        encoding = result.group(3).decode("ascii")
        # Handle some aliases that Python is happy to accept and that are used in the wild.
        if encoding.startswith(("iso-latin-1-", "latin-1-")) or encoding == "iso-latin-1":
            encoding = "latin-1"
        return encoding, line
    else:
        default_encoding = "utf8"
        return default_encoding, -1


</t>
<t tx="ekr.20230831011821.1023">def bytes_to_human_readable_repr(b: bytes) -&gt; str:
    """Converts bytes into some human-readable representation. Unprintable
    bytes such as the nul byte are escaped. For example:

        &gt;&gt;&gt; b = bytes([102, 111, 111, 10, 0])
        &gt;&gt;&gt; s = bytes_to_human_readable_repr(b)
        &gt;&gt;&gt; print(s)
        foo\n\x00
        &gt;&gt;&gt; print(repr(s))
        'foo\\n\\x00'
    """
    return repr(b)[2:-1]


</t>
<t tx="ekr.20230831011821.1024">class DecodeError(Exception):
    """Exception raised when a file cannot be decoded due to an unknown encoding type.

    Essentially a wrapper for the LookupError raised by `bytearray.decode`
    """


</t>
<t tx="ekr.20230831011821.1025">def decode_python_encoding(source: bytes) -&gt; str:
    """Read the Python file with while obeying PEP-263 encoding detection.

    Returns the source as a string.
    """
    # check for BOM UTF-8 encoding and strip it out if present
    if source.startswith(b"\xef\xbb\xbf"):
        encoding = "utf8"
        source = source[3:]
    else:
        # look at first two lines and check if PEP-263 coding is present
        encoding, _ = find_python_encoding(source)

    try:
        source_text = source.decode(encoding)
    except LookupError as lookuperr:
        raise DecodeError(str(lookuperr)) from lookuperr
    return source_text


</t>
<t tx="ekr.20230831011821.1026">def read_py_file(path: str, read: Callable[[str], bytes]) -&gt; list[str] | None:
    """Try reading a Python file as list of source lines.

    Return None if something goes wrong.
    """
    try:
        source = read(path)
    except OSError:
        return None
    else:
        try:
            source_lines = decode_python_encoding(source).splitlines()
        except DecodeError:
            return None
        return source_lines


</t>
<t tx="ekr.20230831011821.1027">def trim_source_line(line: str, max_len: int, col: int, min_width: int) -&gt; tuple[str, int]:
    """Trim a line of source code to fit into max_len.

    Show 'min_width' characters on each side of 'col' (an error location). If either
    start or end is trimmed, this is indicated by adding '...' there.
    A typical result looks like this:
        ...some_variable = function_to_call(one_arg, other_arg) or...

    Return the trimmed string and the column offset to to adjust error location.
    """
    if max_len &lt; 2 * min_width + 1:
        # In case the window is too tiny it is better to still show something.
        max_len = 2 * min_width + 1

    # Trivial case: line already fits in.
    if len(line) &lt;= max_len:
        return line, 0

    # If column is not too large so that there is still min_width after it,
    # the line doesn't need to be trimmed at the start.
    if col + min_width &lt; max_len:
        return line[:max_len] + "...", 0

    # Otherwise, if the column is not too close to the end, trim both sides.
    if col &lt; len(line) - min_width - 1:
        offset = col - max_len + min_width + 1
        return "..." + line[offset : col + min_width + 1] + "...", offset - 3

    # Finally, if the column is near the end, just trim the start.
    return "..." + line[-max_len:], len(line) - max_len - 3


</t>
<t tx="ekr.20230831011821.1028">def get_mypy_comments(source: str) -&gt; list[tuple[int, str]]:
    PREFIX = "# mypy: "
    # Don't bother splitting up the lines unless we know it is useful
    if PREFIX not in source:
        return []
    lines = source.split("\n")
    results = []
    for i, line in enumerate(lines):
        if line.startswith(PREFIX):
            results.append((i + 1, line[len(PREFIX) :]))

    return results


</t>
<t tx="ekr.20230831011821.1029">PASS_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="0" failures="0" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""

FAIL_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="0" failures="1" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
    &lt;failure message="mypy produced messages"&gt;{text}&lt;/failure&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""

ERROR_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="1" failures="0" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
    &lt;error message="mypy produced errors"&gt;{text}&lt;/error&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""


def write_junit_xml(
    dt: float, serious: bool, messages: list[str], path: str, version: str, platform: str
) -&gt; None:
    from xml.sax.saxutils import escape

    if not messages and not serious:
        xml = PASS_TEMPLATE.format(time=dt, ver=version, platform=platform)
    elif not serious:
        xml = FAIL_TEMPLATE.format(
            text=escape("\n".join(messages)), time=dt, ver=version, platform=platform
        )
    else:
        xml = ERROR_TEMPLATE.format(
            text=escape("\n".join(messages)), time=dt, ver=version, platform=platform
        )

    # checks for a directory structure in path and creates folders if needed
    xml_dirs = os.path.dirname(os.path.abspath(path))
    if not os.path.isdir(xml_dirs):
        os.makedirs(xml_dirs)

    with open(path, "wb") as f:
        f.write(xml.encode("utf-8"))


</t>
<t tx="ekr.20230831011821.103">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_comparison_expr(o)

</t>
<t tx="ekr.20230831011821.1030">class IdMapper:
    """Generate integer ids for objects.

    Unlike id(), these start from 0 and increment by 1, and ids won't
    get reused across the life-time of IdMapper.

    Assume objects don't redefine __eq__ or __hash__.
    """

    @others
</t>
<t tx="ekr.20230831011821.1031">def __init__(self) -&gt; None:
    self.id_map: dict[object, int] = {}
    self.next_id = 0

</t>
<t tx="ekr.20230831011821.1032">def id(self, o: object) -&gt; int:
    if o not in self.id_map:
        self.id_map[o] = self.next_id
        self.next_id += 1
    return self.id_map[o]


</t>
<t tx="ekr.20230831011821.1033">def get_prefix(fullname: str) -&gt; str:
    """Drop the final component of a qualified name (e.g. ('x.y' -&gt; 'x')."""
    return fullname.rsplit(".", 1)[0]


</t>
<t tx="ekr.20230831011821.1034">def correct_relative_import(
    cur_mod_id: str, relative: int, target: str, is_cur_package_init_file: bool
) -&gt; tuple[str, bool]:
    if relative == 0:
        return target, True
    parts = cur_mod_id.split(".")
    rel = relative
    if is_cur_package_init_file:
        rel -= 1
    ok = len(parts) &gt;= rel
    if rel != 0:
        cur_mod_id = ".".join(parts[:-rel])
    return cur_mod_id + (("." + target) if target else ""), ok


</t>
<t tx="ekr.20230831011821.1035">fields_cache: Final[dict[type[object], list[str]]] = {}


def get_class_descriptors(cls: type[object]) -&gt; Sequence[str]:
    import inspect  # Lazy import for minor startup speed win

    # Maintain a cache of type -&gt; attributes defined by descriptors in the class
    # (that is, attributes from __slots__ and C extension classes)
    if cls not in fields_cache:
        members = inspect.getmembers(
            cls, lambda o: inspect.isgetsetdescriptor(o) or inspect.ismemberdescriptor(o)
        )
        fields_cache[cls] = [x for x, y in members if x != "__weakref__" and x != "__dict__"]
    return fields_cache[cls]


</t>
<t tx="ekr.20230831011821.1036">def replace_object_state(
    new: object, old: object, copy_dict: bool = False, skip_slots: tuple[str, ...] = ()
</t>
<t tx="ekr.20230831011821.1037">) -&gt; None:
    """Copy state of old node to the new node.

    This handles cases where there is __dict__ and/or attribute descriptors
    (either from slots or because the type is defined in a C extension module).

    Assume that both objects have the same __class__.
    """
    if hasattr(old, "__dict__"):
        if copy_dict:
            new.__dict__ = dict(old.__dict__)
        else:
            new.__dict__ = old.__dict__

    for attr in get_class_descriptors(old.__class__):
        if attr in skip_slots:
            continue
        try:
            if hasattr(old, attr):
                setattr(new, attr, getattr(old, attr))
            elif hasattr(new, attr):
                delattr(new, attr)
        # There is no way to distinguish getsetdescriptors that allow
        # writes from ones that don't (I think?), so we just ignore
        # AttributeErrors if we need to.
        # TODO: What about getsetdescriptors that act like properties???
        except AttributeError:
            pass


def is_sub_path(path1: str, path2: str) -&gt; bool:
    """Given two paths, return if path1 is a sub-path of path2."""
    return pathlib.Path(path2) in pathlib.Path(path1).parents


</t>
<t tx="ekr.20230831011821.1038">def hard_exit(status: int = 0) -&gt; None:
    """Kill the current process without fully cleaning up.

    This can be quite a bit faster than a normal exit() since objects are not freed.
    """
    sys.stdout.flush()
    sys.stderr.flush()
    os._exit(status)


</t>
<t tx="ekr.20230831011821.1039">def unmangle(name: str) -&gt; str:
    """Remove internal suffixes from a short name."""
    return name.rstrip("'")


</t>
<t tx="ekr.20230831011821.104">def visit_cast_expr(self, o: CastExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_cast_expr(o)

</t>
<t tx="ekr.20230831011821.1040">def get_unique_redefinition_name(name: str, existing: Container[str]) -&gt; str:
    """Get a simple redefinition name not present among existing.

    For example, for name 'foo' we try 'foo-redefinition', 'foo-redefinition2',
    'foo-redefinition3', etc. until we find one that is not in existing.
    """
    r_name = name + "-redefinition"
    if r_name not in existing:
        return r_name

    i = 2
    while r_name + str(i) in existing:
        i += 1
    return r_name + str(i)


</t>
<t tx="ekr.20230831011821.1041">def check_python_version(program: str) -&gt; None:
    """Report issues with the Python used to run mypy, dmypy, or stubgen"""
    # Check for known bad Python versions.
    if sys.version_info[:2] &lt; (3, 8):
        sys.exit(
            "Running {name} with Python 3.7 or lower is not supported; "
            "please upgrade to 3.8 or newer".format(name=program)
        )


</t>
<t tx="ekr.20230831011821.1042">def count_stats(messages: list[str]) -&gt; tuple[int, int, int]:
    """Count total number of errors, notes and error_files in message list."""
    errors = [e for e in messages if ": error:" in e]
    error_files = {e.split(":")[0] for e in errors}
    notes = [e for e in messages if ": note:" in e]
    return len(errors), len(notes), len(error_files)


</t>
<t tx="ekr.20230831011821.1043">def split_words(msg: str) -&gt; list[str]:
    """Split line of text into words (but not within quoted groups)."""
    next_word = ""
    res: list[str] = []
    allow_break = True
    for c in msg:
        if c == " " and allow_break:
            res.append(next_word)
            next_word = ""
            continue
        if c == '"':
            allow_break = not allow_break
        next_word += c
    res.append(next_word)
    return res


</t>
<t tx="ekr.20230831011821.1044">def get_terminal_width() -&gt; int:
    """Get current terminal width if possible, otherwise return the default one."""
    return (
        int(os.getenv("MYPY_FORCE_TERMINAL_WIDTH", "0"))
        or shutil.get_terminal_size().columns
        or DEFAULT_COLUMNS
    )


</t>
<t tx="ekr.20230831011821.1045">def soft_wrap(msg: str, max_len: int, first_offset: int, num_indent: int = 0) -&gt; str:
    """Wrap a long error message into few lines.

    Breaks will only happen between words, and never inside a quoted group
    (to avoid breaking types such as "Union[int, str]"). The 'first_offset' is
    the width before the start of first line.

    Pad every next line with 'num_indent' spaces. Every line will be at most 'max_len'
    characters, except if it is a single word or quoted group.

    For example:
               first_offset
        ------------------------
        path/to/file: error: 58: Some very long error message
            that needs to be split in separate lines.
            "Long[Type, Names]" are never split.
        ^^^^--------------------------------------------------
        num_indent           max_len
    """
    words = split_words(msg)
    next_line = words.pop(0)
    lines: list[str] = []
    while words:
        next_word = words.pop(0)
        max_line_len = max_len - num_indent if lines else max_len - first_offset
        # Add 1 to account for space between words.
        if len(next_line) + len(next_word) + 1 &lt;= max_line_len:
            next_line += " " + next_word
        else:
            lines.append(next_line)
            next_line = next_word
    lines.append(next_line)
    padding = "\n" + " " * num_indent
    return padding.join(lines)


</t>
<t tx="ekr.20230831011821.1046">def hash_digest(data: bytes) -&gt; str:
    """Compute a hash digest of some data.

    We use a cryptographic hash because we want a low probability of
    accidental collision, but we don't really care about any of the
    cryptographic properties.
    """
    # Once we drop Python 3.5 support, we should consider using
    # blake2b, which is faster.
    return hashlib.sha256(data).hexdigest()


</t>
<t tx="ekr.20230831011821.1047">def parse_gray_color(cup: bytes) -&gt; str:
    """Reproduce a gray color in ANSI escape sequence"""
    if sys.platform == "win32":
        assert False, "curses is not available on Windows"
    set_color = "".join([cup[:-1].decode(), "m"])
    gray = curses.tparm(set_color.encode("utf-8"), 1, 9).decode()
    return gray


</t>
<t tx="ekr.20230831011821.1048">def should_force_color() -&gt; bool:
    env_var = os.getenv("MYPY_FORCE_COLOR", os.getenv("FORCE_COLOR", "0"))
    try:
        return bool(int(env_var))
    except ValueError:
        return bool(env_var)


</t>
<t tx="ekr.20230831011821.1049">class FancyFormatter:
    """Apply color and bold font to terminal output.

    This currently only works on Linux and Mac.
    """

    @others
</t>
<t tx="ekr.20230831011821.105">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assert_type_expr(o)

</t>
<t tx="ekr.20230831011821.1050">def __init__(self, f_out: IO[str], f_err: IO[str], hide_error_codes: bool) -&gt; None:
    self.hide_error_codes = hide_error_codes
    # Check if we are in a human-facing terminal on a supported platform.
    if sys.platform not in ("linux", "darwin", "win32", "emscripten"):
        self.dummy_term = True
        return
    if not should_force_color() and (not f_out.isatty() or not f_err.isatty()):
        self.dummy_term = True
        return
    if sys.platform == "win32":
        self.dummy_term = not self.initialize_win_colors()
    elif sys.platform == "emscripten":
        self.dummy_term = not self.initialize_vt100_colors()
    else:
        self.dummy_term = not self.initialize_unix_colors()
    if not self.dummy_term:
        self.colors = {
            "red": self.RED,
            "green": self.GREEN,
            "blue": self.BLUE,
            "yellow": self.YELLOW,
            "none": "",
        }

</t>
<t tx="ekr.20230831011821.1051">def initialize_vt100_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    # Windows and Emscripten can both use ANSI/VT100 escape sequences for color
    assert sys.platform in ("win32", "emscripten")
    self.BOLD = "\033[1m"
    self.UNDER = "\033[4m"
    self.BLUE = "\033[94m"
    self.GREEN = "\033[92m"
    self.RED = "\033[91m"
    self.YELLOW = "\033[93m"
    self.NORMAL = "\033[0m"
    self.DIM = "\033[2m"
    return True

</t>
<t tx="ekr.20230831011821.1052">def initialize_win_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    # Windows ANSI escape sequences are only supported on Threshold 2 and above.
    # we check with an assert at runtime and an if check for mypy, as asserts do not
    # yet narrow platform
    assert sys.platform == "win32"
    if sys.platform == "win32":
        winver = sys.getwindowsversion()
        if (
            winver.major &lt; MINIMUM_WINDOWS_MAJOR_VT100
            or winver.build &lt; MINIMUM_WINDOWS_BUILD_VT100
        ):
            return False
        import ctypes

        kernel32 = ctypes.windll.kernel32
        ENABLE_PROCESSED_OUTPUT = 0x1
        ENABLE_WRAP_AT_EOL_OUTPUT = 0x2
        ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x4
        STD_OUTPUT_HANDLE = -11
        kernel32.SetConsoleMode(
            kernel32.GetStdHandle(STD_OUTPUT_HANDLE),
            ENABLE_PROCESSED_OUTPUT
            | ENABLE_WRAP_AT_EOL_OUTPUT
            | ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        )
        self.initialize_vt100_colors()
        return True
    return False

</t>
<t tx="ekr.20230831011821.1053">def initialize_unix_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    if sys.platform == "win32" or not CURSES_ENABLED:
        return False
    try:
        # setupterm wants a fd to potentially write an "initialization sequence".
        # We override sys.stdout for the daemon API so if stdout doesn't have an fd,
        # just give it /dev/null.
        try:
            fd = sys.stdout.fileno()
        except io.UnsupportedOperation:
            with open("/dev/null", "rb") as f:
                curses.setupterm(fd=f.fileno())
        else:
            curses.setupterm(fd=fd)
    except curses.error:
        # Most likely terminfo not found.
        return False
    bold = curses.tigetstr("bold")
    under = curses.tigetstr("smul")
    set_color = curses.tigetstr("setaf")
    set_eseq = curses.tigetstr("cup")
    normal = curses.tigetstr("sgr0")

    if not (bold and under and set_color and set_eseq and normal):
        return False

    self.NORMAL = normal.decode()
    self.BOLD = bold.decode()
    self.UNDER = under.decode()
    self.DIM = parse_gray_color(set_eseq)
    self.BLUE = curses.tparm(set_color, curses.COLOR_BLUE).decode()
    self.GREEN = curses.tparm(set_color, curses.COLOR_GREEN).decode()
    self.RED = curses.tparm(set_color, curses.COLOR_RED).decode()
    self.YELLOW = curses.tparm(set_color, curses.COLOR_YELLOW).decode()
    return True

</t>
<t tx="ekr.20230831011821.1054">def style(
    self,
    text: str,
    color: Literal["red", "green", "blue", "yellow", "none"],
    bold: bool = False,
    underline: bool = False,
    dim: bool = False,
) -&gt; str:
    """Apply simple color and style (underlined or bold)."""
    if self.dummy_term:
        return text
    if bold:
        start = self.BOLD
    else:
        start = ""
    if underline:
        start += self.UNDER
    if dim:
        start += self.DIM
    return start + self.colors[color] + text + self.NORMAL

</t>
<t tx="ekr.20230831011821.1055">def fit_in_terminal(
    self, messages: list[str], fixed_terminal_width: int | None = None
) -&gt; list[str]:
    """Improve readability by wrapping error messages and trimming source code."""
    width = fixed_terminal_width or get_terminal_width()
    new_messages = messages.copy()
    for i, error in enumerate(messages):
        if ": error:" in error:
            loc, msg = error.split("error:", maxsplit=1)
            msg = soft_wrap(msg, width, first_offset=len(loc) + len("error: "))
            new_messages[i] = loc + "error:" + msg
        if error.startswith(" " * DEFAULT_SOURCE_OFFSET) and "^" not in error:
            # TODO: detecting source code highlights through an indent can be surprising.
            # Restore original error message and error location.
            error = error[DEFAULT_SOURCE_OFFSET:]
            marker_line = messages[i + 1]
            marker_column = marker_line.index("^")
            column = marker_column - DEFAULT_SOURCE_OFFSET
            if "~" not in marker_line:
                marker = "^"
            else:
                # +1 because both ends are included
                marker = marker_line[marker_column : marker_line.rindex("~") + 1]

            # Let source have some space also on the right side, plus 6
            # to accommodate ... on each side.
            max_len = width - DEFAULT_SOURCE_OFFSET - 6
            source_line, offset = trim_source_line(error, max_len, column, MINIMUM_WIDTH)

            new_messages[i] = " " * DEFAULT_SOURCE_OFFSET + source_line
            # Also adjust the error marker position and trim error marker is needed.
            new_marker_line = " " * (DEFAULT_SOURCE_OFFSET + column - offset) + marker
            if len(new_marker_line) &gt; len(new_messages[i]) and len(marker) &gt; 3:
                new_marker_line = new_marker_line[: len(new_messages[i]) - 3] + "..."
            new_messages[i + 1] = new_marker_line
    return new_messages

</t>
<t tx="ekr.20230831011821.1056">def colorize(self, error: str) -&gt; str:
    """Colorize an output line by highlighting the status and error code."""
    if ": error:" in error:
        loc, msg = error.split("error:", maxsplit=1)
        if self.hide_error_codes:
            return (
                loc + self.style("error:", "red", bold=True) + self.highlight_quote_groups(msg)
            )
        codepos = msg.rfind("[")
        if codepos != -1:
            code = msg[codepos:]
            msg = msg[:codepos]
        else:
            code = ""  # no error code specified
        return (
            loc
            + self.style("error:", "red", bold=True)
            + self.highlight_quote_groups(msg)
            + self.style(code, "yellow")
        )
    elif ": note:" in error:
        loc, msg = error.split("note:", maxsplit=1)
        formatted = self.highlight_quote_groups(self.underline_link(msg))
        return loc + self.style("note:", "blue") + formatted
    elif error.startswith(" " * DEFAULT_SOURCE_OFFSET):
        # TODO: detecting source code highlights through an indent can be surprising.
        if "^" not in error:
            return self.style(error, "none", dim=True)
        return self.style(error, "red")
    else:
        return error

</t>
<t tx="ekr.20230831011821.1057">def highlight_quote_groups(self, msg: str) -&gt; str:
    """Make groups quoted with double quotes bold (including quotes).

    This is used to highlight types, attribute names etc.
    """
    if msg.count('"') % 2:
        # Broken error message, don't do any formatting.
        return msg
    parts = msg.split('"')
    out = ""
    for i, part in enumerate(parts):
        if i % 2 == 0:
            out += self.style(part, "none")
        else:
            out += self.style('"' + part + '"', "none", bold=True)
    return out

</t>
<t tx="ekr.20230831011821.1058">def underline_link(self, note: str) -&gt; str:
    """Underline a link in a note message (if any).

    This assumes there is at most one link in the message.
    """
    match = re.search(r"https?://\S*", note)
    if not match:
        return note
    start = match.start()
    end = match.end()
    return note[:start] + self.style(note[start:end], "none", underline=True) + note[end:]

</t>
<t tx="ekr.20230831011821.1059">def format_success(self, n_sources: int, use_color: bool = True) -&gt; str:
    """Format short summary in case of success.

    n_sources is total number of files passed directly on command line,
    i.e. excluding stubs and followed imports.
    """
    msg = f"Success: no issues found in {n_sources} source file{plural_s(n_sources)}"
    if not use_color:
        return msg
    return self.style(msg, "green", bold=True)

</t>
<t tx="ekr.20230831011821.106">def visit_reveal_expr(self, o: RevealExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_reveal_expr(o)

</t>
<t tx="ekr.20230831011821.1060">def format_error(
    self,
    n_errors: int,
    n_files: int,
    n_sources: int,
    *,
    blockers: bool = False,
    use_color: bool = True,
) -&gt; str:
    """Format a short summary in case of errors."""
    msg = f"Found {n_errors} error{plural_s(n_errors)} in {n_files} file{plural_s(n_files)}"
    if blockers:
        msg += " (errors prevented further checking)"
    else:
        msg += f" (checked {n_sources} source file{plural_s(n_sources)})"
    if not use_color:
        return msg
    return self.style(msg, "red", bold=True)


</t>
<t tx="ekr.20230831011821.1061">def is_typeshed_file(typeshed_dir: str | None, file: str) -&gt; bool:
    typeshed_dir = typeshed_dir if typeshed_dir is not None else TYPESHED_DIR
    try:
        return os.path.commonpath((typeshed_dir, os.path.abspath(file))) == typeshed_dir
    except ValueError:  # Different drives on Windows
        return False


</t>
<t tx="ekr.20230831011821.1062">def is_stub_package_file(file: str) -&gt; bool:
    # Use hacky heuristics to check whether file is part of a PEP 561 stub package.
    if not file.endswith(".pyi"):
        return False
    return any(component.endswith("-stubs") for component in os.path.split(os.path.abspath(file)))


</t>
<t tx="ekr.20230831011821.1063">def unnamed_function(name: str | None) -&gt; bool:
    return name is not None and name == "_"


</t>
<t tx="ekr.20230831011821.1064">time_ref = time.perf_counter_ns


def time_spent_us(t0: int) -&gt; int:
    return int((time.perf_counter_ns() - t0) / 1000)


</t>
<t tx="ekr.20230831011821.1065">def plural_s(s: int | Sized) -&gt; str:
    count = s if isinstance(s, int) else len(s)
    if count != 1:
        return "s"
    else:
        return ""


</t>
<t tx="ekr.20230831011821.1066">def quote_docstring(docstr: str) -&gt; str:
    """Returns docstring correctly encapsulated in a single or double quoted form."""
    # Uses repr to get hint on the correct quotes and escape everything properly.
    # Creating multiline string for prettier output.
    docstr_repr = "\n".join(re.split(r"(?&lt;=[^\\])\\n", repr(docstr)))

    if docstr_repr.startswith("'"):
        # Enforce double quotes when it's safe to do so.
        # That is when double quotes are not in the string
        # or when it doesn't end with a single quote.
        if '"' not in docstr_repr[1:-1] and docstr_repr[-2] != "'":
            return f'"""{docstr_repr[1:-1]}"""'
        return f"''{docstr_repr}''"
    else:
        return f'""{docstr_repr}""'
</t>
<t tx="ekr.20230831011821.1067">@path mypy
from __future__ import annotations

import os

from mypy import git

# Base version.
# - Release versions have the form "1.2.3".
# - Dev versions have the form "1.2.3+dev" (PLUS sign to conform to PEP 440).
# - Before 1.0 we had the form "0.NNN".
__version__ = "1.6.0+dev"
base_version = __version__

mypy_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
if __version__.endswith("+dev") and git.is_git_repo(mypy_dir) and git.have_git():
    __version__ += "." + git.git_revision(mypy_dir).decode("utf-8")
    if git.is_dirty(mypy_dir):
        __version__ += ".dirty"
del mypy_dir
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.1068">@path mypy
"""Generic abstract syntax tree node visitor"""
&lt;&lt; visitor.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.1069">
from __future__ import annotations

from abc import abstractmethod
from typing import TYPE_CHECKING, Generic, TypeVar

from mypy_extensions import mypyc_attr, trait

if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.nodes
    import mypy.patterns


T = TypeVar("T")


@trait
@mypyc_attr(allow_interpreted_subclasses=True)
</t>
<t tx="ekr.20230831011821.107">def visit_super_expr(self, o: SuperExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_super_expr(o)

</t>
<t tx="ekr.20230831011821.1070">class ExpressionVisitor(Generic[T]):
    @others
</t>
<t tx="ekr.20230831011821.1071">@abstractmethod
def visit_int_expr(self, o: mypy.nodes.IntExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1072">@abstractmethod
def visit_str_expr(self, o: mypy.nodes.StrExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1073">@abstractmethod
def visit_bytes_expr(self, o: mypy.nodes.BytesExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1074">@abstractmethod
def visit_float_expr(self, o: mypy.nodes.FloatExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1075">@abstractmethod
def visit_complex_expr(self, o: mypy.nodes.ComplexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1076">@abstractmethod
def visit_ellipsis(self, o: mypy.nodes.EllipsisExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1077">@abstractmethod
def visit_star_expr(self, o: mypy.nodes.StarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1078">@abstractmethod
def visit_name_expr(self, o: mypy.nodes.NameExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1079">@abstractmethod
def visit_member_expr(self, o: mypy.nodes.MemberExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.108">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assignment_expr(o)

</t>
<t tx="ekr.20230831011821.1080">@abstractmethod
def visit_yield_from_expr(self, o: mypy.nodes.YieldFromExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1081">@abstractmethod
def visit_yield_expr(self, o: mypy.nodes.YieldExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1082">@abstractmethod
def visit_call_expr(self, o: mypy.nodes.CallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1083">@abstractmethod
def visit_op_expr(self, o: mypy.nodes.OpExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1084">@abstractmethod
def visit_comparison_expr(self, o: mypy.nodes.ComparisonExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1085">@abstractmethod
def visit_cast_expr(self, o: mypy.nodes.CastExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1086">@abstractmethod
def visit_assert_type_expr(self, o: mypy.nodes.AssertTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1087">@abstractmethod
def visit_reveal_expr(self, o: mypy.nodes.RevealExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1088">@abstractmethod
def visit_super_expr(self, o: mypy.nodes.SuperExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1089">@abstractmethod
def visit_unary_expr(self, o: mypy.nodes.UnaryExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.109">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_unary_expr(o)

</t>
<t tx="ekr.20230831011821.1090">@abstractmethod
def visit_assignment_expr(self, o: mypy.nodes.AssignmentExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1091">@abstractmethod
def visit_list_expr(self, o: mypy.nodes.ListExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1092">@abstractmethod
def visit_dict_expr(self, o: mypy.nodes.DictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1093">@abstractmethod
def visit_tuple_expr(self, o: mypy.nodes.TupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1094">@abstractmethod
def visit_set_expr(self, o: mypy.nodes.SetExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1095">@abstractmethod
def visit_index_expr(self, o: mypy.nodes.IndexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1096">@abstractmethod
def visit_type_application(self, o: mypy.nodes.TypeApplication) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1097">@abstractmethod
def visit_lambda_expr(self, o: mypy.nodes.LambdaExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1098">@abstractmethod
def visit_list_comprehension(self, o: mypy.nodes.ListComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1099">@abstractmethod
def visit_set_comprehension(self, o: mypy.nodes.SetComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.11">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.110">def visit_list_expr(self, o: ListExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_list_expr(o)

</t>
<t tx="ekr.20230831011821.1100">@abstractmethod
def visit_dictionary_comprehension(self, o: mypy.nodes.DictionaryComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1101">@abstractmethod
def visit_generator_expr(self, o: mypy.nodes.GeneratorExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1102">@abstractmethod
def visit_slice_expr(self, o: mypy.nodes.SliceExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1103">@abstractmethod
def visit_conditional_expr(self, o: mypy.nodes.ConditionalExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1104">@abstractmethod
def visit_type_var_expr(self, o: mypy.nodes.TypeVarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1105">@abstractmethod
def visit_paramspec_expr(self, o: mypy.nodes.ParamSpecExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1106">@abstractmethod
def visit_type_var_tuple_expr(self, o: mypy.nodes.TypeVarTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1107">@abstractmethod
def visit_type_alias_expr(self, o: mypy.nodes.TypeAliasExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1108">@abstractmethod
def visit_namedtuple_expr(self, o: mypy.nodes.NamedTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1109">@abstractmethod
def visit_enum_call_expr(self, o: mypy.nodes.EnumCallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.111">def visit_dict_expr(self, o: DictExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_dict_expr(o)

</t>
<t tx="ekr.20230831011821.1110">@abstractmethod
def visit_typeddict_expr(self, o: mypy.nodes.TypedDictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1111">@abstractmethod
def visit_newtype_expr(self, o: mypy.nodes.NewTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1112">@abstractmethod
def visit__promote_expr(self, o: mypy.nodes.PromoteExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1113">@abstractmethod
def visit_await_expr(self, o: mypy.nodes.AwaitExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1114">@abstractmethod
def visit_temp_node(self, o: mypy.nodes.TempNode) -&gt; T:
    pass


</t>
<t tx="ekr.20230831011821.1115">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class StatementVisitor(Generic[T]):
    @others
</t>
<t tx="ekr.20230831011821.1116"># Definitions

@abstractmethod
def visit_assignment_stmt(self, o: mypy.nodes.AssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1117">@abstractmethod
def visit_for_stmt(self, o: mypy.nodes.ForStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1118">@abstractmethod
def visit_with_stmt(self, o: mypy.nodes.WithStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1119">@abstractmethod
def visit_del_stmt(self, o: mypy.nodes.DelStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.112">def visit_tuple_expr(self, o: TupleExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_tuple_expr(o)

</t>
<t tx="ekr.20230831011821.1120">@abstractmethod
def visit_func_def(self, o: mypy.nodes.FuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1121">@abstractmethod
def visit_overloaded_func_def(self, o: mypy.nodes.OverloadedFuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1122">@abstractmethod
def visit_class_def(self, o: mypy.nodes.ClassDef) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1123">@abstractmethod
def visit_global_decl(self, o: mypy.nodes.GlobalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1124">@abstractmethod
def visit_nonlocal_decl(self, o: mypy.nodes.NonlocalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1125">@abstractmethod
def visit_decorator(self, o: mypy.nodes.Decorator) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1126"># Module structure

@abstractmethod
def visit_import(self, o: mypy.nodes.Import) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1127">@abstractmethod
def visit_import_from(self, o: mypy.nodes.ImportFrom) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1128">@abstractmethod
def visit_import_all(self, o: mypy.nodes.ImportAll) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1129"># Statements

@abstractmethod
def visit_block(self, o: mypy.nodes.Block) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.113">def visit_set_expr(self, o: SetExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_set_expr(o)

</t>
<t tx="ekr.20230831011821.1130">@abstractmethod
def visit_expression_stmt(self, o: mypy.nodes.ExpressionStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1131">@abstractmethod
def visit_operator_assignment_stmt(self, o: mypy.nodes.OperatorAssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1132">@abstractmethod
def visit_while_stmt(self, o: mypy.nodes.WhileStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1133">@abstractmethod
def visit_return_stmt(self, o: mypy.nodes.ReturnStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1134">@abstractmethod
def visit_assert_stmt(self, o: mypy.nodes.AssertStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1135">@abstractmethod
def visit_if_stmt(self, o: mypy.nodes.IfStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1136">@abstractmethod
def visit_break_stmt(self, o: mypy.nodes.BreakStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1137">@abstractmethod
def visit_continue_stmt(self, o: mypy.nodes.ContinueStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1138">@abstractmethod
def visit_pass_stmt(self, o: mypy.nodes.PassStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1139">@abstractmethod
def visit_raise_stmt(self, o: mypy.nodes.RaiseStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.114">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_index_expr(o)

</t>
<t tx="ekr.20230831011821.1140">@abstractmethod
def visit_try_stmt(self, o: mypy.nodes.TryStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1141">@abstractmethod
def visit_match_stmt(self, o: mypy.nodes.MatchStmt) -&gt; T:
    pass


</t>
<t tx="ekr.20230831011821.1142">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class PatternVisitor(Generic[T]):
    @others
</t>
<t tx="ekr.20230831011821.1143">@abstractmethod
def visit_as_pattern(self, o: mypy.patterns.AsPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1144">@abstractmethod
def visit_or_pattern(self, o: mypy.patterns.OrPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1145">@abstractmethod
def visit_value_pattern(self, o: mypy.patterns.ValuePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1146">@abstractmethod
def visit_singleton_pattern(self, o: mypy.patterns.SingletonPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1147">@abstractmethod
def visit_sequence_pattern(self, o: mypy.patterns.SequencePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1148">@abstractmethod
def visit_starred_pattern(self, o: mypy.patterns.StarredPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1149">@abstractmethod
def visit_mapping_pattern(self, o: mypy.patterns.MappingPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.115">def visit_type_application(self, o: TypeApplication) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_application(o)

</t>
<t tx="ekr.20230831011821.1150">@abstractmethod
def visit_class_pattern(self, o: mypy.patterns.ClassPattern) -&gt; T:
    pass


</t>
<t tx="ekr.20230831011821.1151">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class NodeVisitor(Generic[T], ExpressionVisitor[T], StatementVisitor[T], PatternVisitor[T]):
    """Empty base class for parse tree node visitors.

    The T type argument specifies the return type of the visit
    methods. As all methods defined here return None by default,
    subclasses do not always need to override all the methods.

    TODO: make the default return value explicit, then turn on
          empty body checking in mypy_self_check.ini.
    """

    @others
</t>
<t tx="ekr.20230831011821.1152"># Not in superclasses:

def visit_mypy_file(self, o: mypy.nodes.MypyFile) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1153"># TODO: We have a visit_var method, but no visit_typeinfo or any
# other non-Statement SymbolNode (accepting those will raise a
# runtime error). Maybe this should be resolved in some direction.
def visit_var(self, o: mypy.nodes.Var) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1154"># Module structure

def visit_import(self, o: mypy.nodes.Import) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1155">def visit_import_from(self, o: mypy.nodes.ImportFrom) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1156">def visit_import_all(self, o: mypy.nodes.ImportAll) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1157"># Definitions

def visit_func_def(self, o: mypy.nodes.FuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1158">def visit_overloaded_func_def(self, o: mypy.nodes.OverloadedFuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1159">def visit_class_def(self, o: mypy.nodes.ClassDef) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.116">def visit_lambda_expr(self, o: LambdaExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_lambda_expr(o)

</t>
<t tx="ekr.20230831011821.1160">def visit_global_decl(self, o: mypy.nodes.GlobalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1161">def visit_nonlocal_decl(self, o: mypy.nodes.NonlocalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1162">def visit_decorator(self, o: mypy.nodes.Decorator) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1163">def visit_type_alias(self, o: mypy.nodes.TypeAlias) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1164">def visit_placeholder_node(self, o: mypy.nodes.PlaceholderNode) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1165"># Statements

def visit_block(self, o: mypy.nodes.Block) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1166">def visit_expression_stmt(self, o: mypy.nodes.ExpressionStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1167">def visit_assignment_stmt(self, o: mypy.nodes.AssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1168">def visit_operator_assignment_stmt(self, o: mypy.nodes.OperatorAssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1169">def visit_while_stmt(self, o: mypy.nodes.WhileStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.117">def visit_list_comprehension(self, o: ListComprehension) -&gt; None:
    if not self.visit(o):
        return
    super().visit_list_comprehension(o)

</t>
<t tx="ekr.20230831011821.1170">def visit_for_stmt(self, o: mypy.nodes.ForStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1171">def visit_return_stmt(self, o: mypy.nodes.ReturnStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1172">def visit_assert_stmt(self, o: mypy.nodes.AssertStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1173">def visit_del_stmt(self, o: mypy.nodes.DelStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1174">def visit_if_stmt(self, o: mypy.nodes.IfStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1175">def visit_break_stmt(self, o: mypy.nodes.BreakStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1176">def visit_continue_stmt(self, o: mypy.nodes.ContinueStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1177">def visit_pass_stmt(self, o: mypy.nodes.PassStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1178">def visit_raise_stmt(self, o: mypy.nodes.RaiseStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1179">def visit_try_stmt(self, o: mypy.nodes.TryStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.118">def visit_set_comprehension(self, o: SetComprehension) -&gt; None:
    if not self.visit(o):
        return
    super().visit_set_comprehension(o)

</t>
<t tx="ekr.20230831011821.1180">def visit_with_stmt(self, o: mypy.nodes.WithStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1181">def visit_match_stmt(self, o: mypy.nodes.MatchStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1182"># Expressions (default no-op implementation)

def visit_int_expr(self, o: mypy.nodes.IntExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1183">def visit_str_expr(self, o: mypy.nodes.StrExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1184">def visit_bytes_expr(self, o: mypy.nodes.BytesExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1185">def visit_float_expr(self, o: mypy.nodes.FloatExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1186">def visit_complex_expr(self, o: mypy.nodes.ComplexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1187">def visit_ellipsis(self, o: mypy.nodes.EllipsisExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1188">def visit_star_expr(self, o: mypy.nodes.StarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1189">def visit_name_expr(self, o: mypy.nodes.NameExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.119">def visit_dictionary_comprehension(self, o: DictionaryComprehension) -&gt; None:
    if not self.visit(o):
        return
    super().visit_dictionary_comprehension(o)

</t>
<t tx="ekr.20230831011821.1190">def visit_member_expr(self, o: mypy.nodes.MemberExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1191">def visit_yield_from_expr(self, o: mypy.nodes.YieldFromExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1192">def visit_yield_expr(self, o: mypy.nodes.YieldExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1193">def visit_call_expr(self, o: mypy.nodes.CallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1194">def visit_op_expr(self, o: mypy.nodes.OpExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1195">def visit_comparison_expr(self, o: mypy.nodes.ComparisonExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1196">def visit_cast_expr(self, o: mypy.nodes.CastExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1197">def visit_assert_type_expr(self, o: mypy.nodes.AssertTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1198">def visit_reveal_expr(self, o: mypy.nodes.RevealExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1199">def visit_super_expr(self, o: mypy.nodes.SuperExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.12">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    o.rvalue.accept(self)
    for l in o.lvalues:
        l.accept(self)

</t>
<t tx="ekr.20230831011821.120">def visit_generator_expr(self, o: GeneratorExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_generator_expr(o)

</t>
<t tx="ekr.20230831011821.1200">def visit_assignment_expr(self, o: mypy.nodes.AssignmentExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1201">def visit_unary_expr(self, o: mypy.nodes.UnaryExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1202">def visit_list_expr(self, o: mypy.nodes.ListExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1203">def visit_dict_expr(self, o: mypy.nodes.DictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1204">def visit_tuple_expr(self, o: mypy.nodes.TupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1205">def visit_set_expr(self, o: mypy.nodes.SetExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1206">def visit_index_expr(self, o: mypy.nodes.IndexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1207">def visit_type_application(self, o: mypy.nodes.TypeApplication) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1208">def visit_lambda_expr(self, o: mypy.nodes.LambdaExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1209">def visit_list_comprehension(self, o: mypy.nodes.ListComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.121">def visit_slice_expr(self, o: SliceExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_slice_expr(o)

</t>
<t tx="ekr.20230831011821.1210">def visit_set_comprehension(self, o: mypy.nodes.SetComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1211">def visit_dictionary_comprehension(self, o: mypy.nodes.DictionaryComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1212">def visit_generator_expr(self, o: mypy.nodes.GeneratorExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1213">def visit_slice_expr(self, o: mypy.nodes.SliceExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1214">def visit_conditional_expr(self, o: mypy.nodes.ConditionalExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1215">def visit_type_var_expr(self, o: mypy.nodes.TypeVarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1216">def visit_paramspec_expr(self, o: mypy.nodes.ParamSpecExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1217">def visit_type_var_tuple_expr(self, o: mypy.nodes.TypeVarTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1218">def visit_type_alias_expr(self, o: mypy.nodes.TypeAliasExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1219">def visit_namedtuple_expr(self, o: mypy.nodes.NamedTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.122">def visit_conditional_expr(self, o: ConditionalExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_conditional_expr(o)

</t>
<t tx="ekr.20230831011821.1220">def visit_enum_call_expr(self, o: mypy.nodes.EnumCallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1221">def visit_typeddict_expr(self, o: mypy.nodes.TypedDictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1222">def visit_newtype_expr(self, o: mypy.nodes.NewTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1223">def visit__promote_expr(self, o: mypy.nodes.PromoteExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1224">def visit_await_expr(self, o: mypy.nodes.AwaitExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1225">def visit_temp_node(self, o: mypy.nodes.TempNode) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1226"># Patterns

def visit_as_pattern(self, o: mypy.patterns.AsPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1227">def visit_or_pattern(self, o: mypy.patterns.OrPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1228">def visit_value_pattern(self, o: mypy.patterns.ValuePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1229">def visit_singleton_pattern(self, o: mypy.patterns.SingletonPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.123">def visit_type_var_expr(self, o: TypeVarExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_var_expr(o)

</t>
<t tx="ekr.20230831011821.1230">def visit_sequence_pattern(self, o: mypy.patterns.SequencePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1231">def visit_starred_pattern(self, o: mypy.patterns.StarredPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1232">def visit_mapping_pattern(self, o: mypy.patterns.MappingPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.1233">def visit_class_pattern(self, o: mypy.patterns.ClassPattern) -&gt; T:
    pass
</t>
<t tx="ekr.20230831011821.124">def visit_paramspec_expr(self, o: ParamSpecExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_paramspec_expr(o)

</t>
<t tx="ekr.20230831011821.125">def visit_type_var_tuple_expr(self, o: TypeVarTupleExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_var_tuple_expr(o)

</t>
<t tx="ekr.20230831011821.126">def visit_type_alias_expr(self, o: TypeAliasExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_alias_expr(o)

</t>
<t tx="ekr.20230831011821.127">def visit_namedtuple_expr(self, o: NamedTupleExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_namedtuple_expr(o)

</t>
<t tx="ekr.20230831011821.128">def visit_enum_call_expr(self, o: EnumCallExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_enum_call_expr(o)

</t>
<t tx="ekr.20230831011821.129">def visit_typeddict_expr(self, o: TypedDictExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_typeddict_expr(o)

</t>
<t tx="ekr.20230831011821.13">def visit_operator_assignment_stmt(self, o: OperatorAssignmentStmt) -&gt; None:
    o.rvalue.accept(self)
    o.lvalue.accept(self)

</t>
<t tx="ekr.20230831011821.130">def visit_newtype_expr(self, o: NewTypeExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_newtype_expr(o)

</t>
<t tx="ekr.20230831011821.131">def visit_await_expr(self, o: AwaitExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_await_expr(o)

</t>
<t tx="ekr.20230831011821.132"># Patterns

def visit_as_pattern(self, o: AsPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_as_pattern(o)

</t>
<t tx="ekr.20230831011821.133">def visit_or_pattern(self, o: OrPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_or_pattern(o)

</t>
<t tx="ekr.20230831011821.134">def visit_value_pattern(self, o: ValuePattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_value_pattern(o)

</t>
<t tx="ekr.20230831011821.135">def visit_singleton_pattern(self, o: SingletonPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_singleton_pattern(o)

</t>
<t tx="ekr.20230831011821.136">def visit_sequence_pattern(self, o: SequencePattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_sequence_pattern(o)

</t>
<t tx="ekr.20230831011821.137">def visit_starred_pattern(self, o: StarredPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_starred_pattern(o)

</t>
<t tx="ekr.20230831011821.138">def visit_mapping_pattern(self, o: MappingPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_mapping_pattern(o)

</t>
<t tx="ekr.20230831011821.139">def visit_class_pattern(self, o: ClassPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_class_pattern(o)


</t>
<t tx="ekr.20230831011821.14">def visit_while_stmt(self, o: WhileStmt) -&gt; None:
    o.expr.accept(self)
    o.body.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20230831011821.140">class ReturnSeeker(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011821.141">def __init__(self) -&gt; None:
    self.found = False

</t>
<t tx="ekr.20230831011821.142">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is None or isinstance(o.expr, NameExpr) and o.expr.name == "None":
        return
    self.found = True


</t>
<t tx="ekr.20230831011821.143">def has_return_statement(fdef: FuncBase) -&gt; bool:
    """Find if a function has a non-trivial return statement.

    Plain 'return' and 'return None' don't count.
    """
    seeker = ReturnSeeker()
    fdef.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20230831011821.144">class FuncCollectorBase(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011821.145">def __init__(self) -&gt; None:
    self.inside_func = False

</t>
<t tx="ekr.20230831011821.146">def visit_func_def(self, defn: FuncDef) -&gt; None:
    if not self.inside_func:
        self.inside_func = True
        super().visit_func_def(defn)
        self.inside_func = False


</t>
<t tx="ekr.20230831011821.147">class YieldSeeker(FuncCollectorBase):
    @others
</t>
<t tx="ekr.20230831011821.148">def __init__(self) -&gt; None:
    super().__init__()
    self.found = False

</t>
<t tx="ekr.20230831011821.149">def visit_yield_expr(self, o: YieldExpr) -&gt; None:
    self.found = True


</t>
<t tx="ekr.20230831011821.15">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    o.index.accept(self)
    o.expr.accept(self)
    o.body.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20230831011821.150">def has_yield_expression(fdef: FuncBase) -&gt; bool:
    seeker = YieldSeeker()
    fdef.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20230831011821.151">class YieldFromSeeker(FuncCollectorBase):
    @others
</t>
<t tx="ekr.20230831011821.152">def __init__(self) -&gt; None:
    super().__init__()
    self.found = False

</t>
<t tx="ekr.20230831011821.153">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    self.found = True


</t>
<t tx="ekr.20230831011821.154">def has_yield_from_expression(fdef: FuncBase) -&gt; bool:
    seeker = YieldFromSeeker()
    fdef.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20230831011821.155">class AwaitSeeker(TraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011821.156">def __init__(self) -&gt; None:
    super().__init__()
    self.found = False

</t>
<t tx="ekr.20230831011821.157">def visit_await_expr(self, o: AwaitExpr) -&gt; None:
    self.found = True


</t>
<t tx="ekr.20230831011821.158">def has_await_expression(expr: Expression) -&gt; bool:
    seeker = AwaitSeeker()
    expr.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20230831011821.159">class ReturnCollector(FuncCollectorBase):
    @others
</t>
<t tx="ekr.20230831011821.16">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.160">def __init__(self) -&gt; None:
    super().__init__()
    self.return_statements: list[ReturnStmt] = []

</t>
<t tx="ekr.20230831011821.161">def visit_return_stmt(self, stmt: ReturnStmt) -&gt; None:
    self.return_statements.append(stmt)


</t>
<t tx="ekr.20230831011821.162">def all_return_statements(node: Node) -&gt; list[ReturnStmt]:
    v = ReturnCollector()
    node.accept(v)
    return v.return_statements


</t>
<t tx="ekr.20230831011821.163">class YieldCollector(FuncCollectorBase):
    @others
</t>
<t tx="ekr.20230831011821.164">def __init__(self) -&gt; None:
    super().__init__()
    self.in_assignment = False
    self.yield_expressions: list[tuple[YieldExpr, bool]] = []

</t>
<t tx="ekr.20230831011821.165">def visit_assignment_stmt(self, stmt: AssignmentStmt) -&gt; None:
    self.in_assignment = True
    super().visit_assignment_stmt(stmt)
    self.in_assignment = False

</t>
<t tx="ekr.20230831011821.166">def visit_yield_expr(self, expr: YieldExpr) -&gt; None:
    self.yield_expressions.append((expr, self.in_assignment))


</t>
<t tx="ekr.20230831011821.167">def all_yield_expressions(node: Node) -&gt; list[tuple[YieldExpr, bool]]:
    v = YieldCollector()
    node.accept(v)
    return v.yield_expressions


</t>
<t tx="ekr.20230831011821.168">class YieldFromCollector(FuncCollectorBase):
    @others
</t>
<t tx="ekr.20230831011821.169">def __init__(self) -&gt; None:
    super().__init__()
    self.in_assignment = False
    self.yield_from_expressions: list[tuple[YieldFromExpr, bool]] = []

</t>
<t tx="ekr.20230831011821.17">def visit_assert_stmt(self, o: AssertStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)
    if o.msg is not None:
        o.msg.accept(self)

</t>
<t tx="ekr.20230831011821.170">def visit_assignment_stmt(self, stmt: AssignmentStmt) -&gt; None:
    self.in_assignment = True
    super().visit_assignment_stmt(stmt)
    self.in_assignment = False

</t>
<t tx="ekr.20230831011821.171">def visit_yield_from_expr(self, expr: YieldFromExpr) -&gt; None:
    self.yield_from_expressions.append((expr, self.in_assignment))


</t>
<t tx="ekr.20230831011821.172">def all_yield_from_expressions(node: Node) -&gt; list[tuple[YieldFromExpr, bool]]:
    v = YieldFromCollector()
    node.accept(v)
    return v.yield_from_expressions
</t>
<t tx="ekr.20230831011821.173">@path mypy
"""Base visitor that implements an identity AST transform.

Subclass TransformVisitor to perform non-trivial transformations.
"""

&lt;&lt; treetransform.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.175">from __future__ import annotations

from typing import Iterable, Optional, cast

from mypy.nodes import (
    GDEF,
    REVEAL_TYPE,
    Argument,
    AssertStmt,
    AssertTypeExpr,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    CastExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    ContinueStmt,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    ExpressionStmt,
    FloatExpr,
    ForStmt,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    OverloadPart,
    ParamSpecExpr,
    PassStmt,
    PromoteExpr,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    Statement,
    StrExpr,
    SuperExpr,
    SymbolTable,
    TempNode,
    TryStmt,
    TupleExpr,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
)
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    Pattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.traverser import TraverserVisitor
from mypy.types import FunctionLike, ProperType, Type
from mypy.util import replace_object_state
from mypy.visitor import NodeVisitor


</t>
<t tx="ekr.20230831011821.176">class TransformVisitor(NodeVisitor[Node]):
    """Transform a semantically analyzed AST (or subtree) to an identical copy.

    Use the node() method to transform an AST node.

    Subclass to perform a non-identity transform.

    Notes:

     * This can only be used to transform functions or classes, not top-level
       statements, and/or modules as a whole.
     * Do not duplicate TypeInfo nodes. This would generally not be desirable.
     * Only update some name binding cross-references, but only those that
       refer to Var, Decorator or FuncDef nodes, not those targeting ClassDef or
       TypeInfo nodes.
     * Types are not transformed, but you can override type() to also perform
       type transformation.

    TODO nested classes and functions have not been tested well enough
    """

    @others
</t>
<t tx="ekr.20230831011821.177">def __init__(self) -&gt; None:
    # To simplify testing, set this flag to True if you want to transform
    # all statements in a file (this is prohibited in normal mode).
    self.test_only = False
    # There may be multiple references to a Var node. Keep track of
    # Var translations using a dictionary.
    self.var_map: dict[Var, Var] = {}
    # These are uninitialized placeholder nodes used temporarily for nested
    # functions while we are transforming a top-level function. This maps an
    # untransformed node to a placeholder (which will later become the
    # transformed node).
    self.func_placeholder_map: dict[FuncDef, FuncDef] = {}

</t>
<t tx="ekr.20230831011821.178">def visit_mypy_file(self, node: MypyFile) -&gt; MypyFile:
    assert self.test_only, "This visitor should not be used for whole files."
    # NOTE: The 'names' and 'imports' instance variables will be empty!
    ignored_lines = {line: codes.copy() for line, codes in node.ignored_lines.items()}
    new = MypyFile(self.statements(node.defs), [], node.is_bom, ignored_lines=ignored_lines)
    new._fullname = node._fullname
    new.path = node.path
    new.names = SymbolTable()
    return new

</t>
<t tx="ekr.20230831011821.179">def visit_import(self, node: Import) -&gt; Import:
    return Import(node.ids.copy())

</t>
<t tx="ekr.20230831011821.18">def visit_del_stmt(self, o: DelStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.180">def visit_import_from(self, node: ImportFrom) -&gt; ImportFrom:
    return ImportFrom(node.id, node.relative, node.names.copy())

</t>
<t tx="ekr.20230831011821.181">def visit_import_all(self, node: ImportAll) -&gt; ImportAll:
    return ImportAll(node.id, node.relative)

</t>
<t tx="ekr.20230831011821.182">def copy_argument(self, argument: Argument) -&gt; Argument:
    arg = Argument(
        self.visit_var(argument.variable),
        argument.type_annotation,
        argument.initializer,
        argument.kind,
    )

    # Refresh lines of the inner things
    arg.set_line(argument)

    return arg

</t>
<t tx="ekr.20230831011821.183">def visit_func_def(self, node: FuncDef) -&gt; FuncDef:
    # Note that a FuncDef must be transformed to a FuncDef.

    # These contortions are needed to handle the case of recursive
    # references inside the function being transformed.
    # Set up placeholder nodes for references within this function
    # to other functions defined inside it.
    # Don't create an entry for this function itself though,
    # since we want self-references to point to the original
    # function if this is the top-level node we are transforming.
    init = FuncMapInitializer(self)
    for stmt in node.body.body:
        stmt.accept(init)

    new = FuncDef(
        node.name,
        [self.copy_argument(arg) for arg in node.arguments],
        self.block(node.body),
        cast(Optional[FunctionLike], self.optional_type(node.type)),
    )

    self.copy_function_attributes(new, node)

    new._fullname = node._fullname
    new.is_decorated = node.is_decorated
    new.is_conditional = node.is_conditional
    new.abstract_status = node.abstract_status
    new.is_static = node.is_static
    new.is_class = node.is_class
    new.is_property = node.is_property
    new.is_final = node.is_final
    new.original_def = node.original_def

    if node in self.func_placeholder_map:
        # There is a placeholder definition for this function. Replace
        # the attributes of the placeholder with those form the transformed
        # function. We know that the classes will be identical (otherwise
        # this wouldn't work).
        result = self.func_placeholder_map[node]
        replace_object_state(result, new)
        return result
    else:
        return new

</t>
<t tx="ekr.20230831011821.184">def visit_lambda_expr(self, node: LambdaExpr) -&gt; LambdaExpr:
    new = LambdaExpr(
        [self.copy_argument(arg) for arg in node.arguments],
        self.block(node.body),
        cast(Optional[FunctionLike], self.optional_type(node.type)),
    )
    self.copy_function_attributes(new, node)
    return new

</t>
<t tx="ekr.20230831011821.185">def copy_function_attributes(self, new: FuncItem, original: FuncItem) -&gt; None:
    new.info = original.info
    new.min_args = original.min_args
    new.max_pos = original.max_pos
    new.is_overload = original.is_overload
    new.is_generator = original.is_generator
    new.is_coroutine = original.is_coroutine
    new.is_async_generator = original.is_async_generator
    new.is_awaitable_coroutine = original.is_awaitable_coroutine
    new.line = original.line

</t>
<t tx="ekr.20230831011821.186">def visit_overloaded_func_def(self, node: OverloadedFuncDef) -&gt; OverloadedFuncDef:
    items = [cast(OverloadPart, item.accept(self)) for item in node.items]
    for newitem, olditem in zip(items, node.items):
        newitem.line = olditem.line
    new = OverloadedFuncDef(items)
    new._fullname = node._fullname
    new_type = self.optional_type(node.type)
    assert isinstance(new_type, ProperType)
    new.type = new_type
    new.info = node.info
    new.is_static = node.is_static
    new.is_class = node.is_class
    new.is_property = node.is_property
    new.is_final = node.is_final
    if node.impl:
        new.impl = cast(OverloadPart, node.impl.accept(self))
    return new

</t>
<t tx="ekr.20230831011821.187">def visit_class_def(self, node: ClassDef) -&gt; ClassDef:
    new = ClassDef(
        node.name,
        self.block(node.defs),
        node.type_vars,
        self.expressions(node.base_type_exprs),
        self.optional_expr(node.metaclass),
    )
    new.fullname = node.fullname
    new.info = node.info
    new.decorators = [self.expr(decorator) for decorator in node.decorators]
    return new

</t>
<t tx="ekr.20230831011821.188">def visit_global_decl(self, node: GlobalDecl) -&gt; GlobalDecl:
    return GlobalDecl(node.names.copy())

</t>
<t tx="ekr.20230831011821.189">def visit_nonlocal_decl(self, node: NonlocalDecl) -&gt; NonlocalDecl:
    return NonlocalDecl(node.names.copy())

</t>
<t tx="ekr.20230831011821.19">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    for e in o.expr:
        e.accept(self)
    for b in o.body:
        b.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20230831011821.190">def visit_block(self, node: Block) -&gt; Block:
    return Block(self.statements(node.body))

</t>
<t tx="ekr.20230831011821.191">def visit_decorator(self, node: Decorator) -&gt; Decorator:
    # Note that a Decorator must be transformed to a Decorator.
    func = self.visit_func_def(node.func)
    func.line = node.func.line
    new = Decorator(func, self.expressions(node.decorators), self.visit_var(node.var))
    new.is_overload = node.is_overload
    return new

</t>
<t tx="ekr.20230831011821.192">def visit_var(self, node: Var) -&gt; Var:
    # Note that a Var must be transformed to a Var.
    if node in self.var_map:
        return self.var_map[node]
    new = Var(node.name, self.optional_type(node.type))
    new.line = node.line
    new._fullname = node._fullname
    new.info = node.info
    new.is_self = node.is_self
    new.is_ready = node.is_ready
    new.is_initialized_in_class = node.is_initialized_in_class
    new.is_staticmethod = node.is_staticmethod
    new.is_classmethod = node.is_classmethod
    new.is_property = node.is_property
    new.is_final = node.is_final
    new.final_value = node.final_value
    new.final_unset_in_class = node.final_unset_in_class
    new.final_set_in_init = node.final_set_in_init
    new.set_line(node)
    self.var_map[node] = new
    return new

</t>
<t tx="ekr.20230831011821.193">def visit_expression_stmt(self, node: ExpressionStmt) -&gt; ExpressionStmt:
    return ExpressionStmt(self.expr(node.expr))

</t>
<t tx="ekr.20230831011821.194">def visit_assignment_stmt(self, node: AssignmentStmt) -&gt; AssignmentStmt:
    return self.duplicate_assignment(node)

</t>
<t tx="ekr.20230831011821.195">def duplicate_assignment(self, node: AssignmentStmt) -&gt; AssignmentStmt:
    new = AssignmentStmt(
        self.expressions(node.lvalues),
        self.expr(node.rvalue),
        self.optional_type(node.unanalyzed_type),
    )
    new.line = node.line
    new.is_final_def = node.is_final_def
    new.type = self.optional_type(node.type)
    return new

</t>
<t tx="ekr.20230831011821.196">def visit_operator_assignment_stmt(
    self, node: OperatorAssignmentStmt
) -&gt; OperatorAssignmentStmt:
    return OperatorAssignmentStmt(node.op, self.expr(node.lvalue), self.expr(node.rvalue))

</t>
<t tx="ekr.20230831011821.197">def visit_while_stmt(self, node: WhileStmt) -&gt; WhileStmt:
    return WhileStmt(
        self.expr(node.expr), self.block(node.body), self.optional_block(node.else_body)
    )

</t>
<t tx="ekr.20230831011821.198">def visit_for_stmt(self, node: ForStmt) -&gt; ForStmt:
    new = ForStmt(
        self.expr(node.index),
        self.expr(node.expr),
        self.block(node.body),
        self.optional_block(node.else_body),
        self.optional_type(node.unanalyzed_index_type),
    )
    new.is_async = node.is_async
    new.index_type = self.optional_type(node.index_type)
    return new

</t>
<t tx="ekr.20230831011821.199">def visit_return_stmt(self, node: ReturnStmt) -&gt; ReturnStmt:
    return ReturnStmt(self.optional_expr(node.expr))

</t>
<t tx="ekr.20230831011821.2">class TraverserVisitor(NodeVisitor[None]):
    """A parse tree visitor that traverses the parse tree during visiting.

    It does not perform any actions outside the traversal. Subclasses
    should override visit methods to perform actions during
    traversal. Calling the superclass method allows reusing the
    traversal implementation.
    """

    @others
</t>
<t tx="ekr.20230831011821.20">def visit_raise_stmt(self, o: RaiseStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)
    if o.from_expr is not None:
        o.from_expr.accept(self)

</t>
<t tx="ekr.20230831011821.200">def visit_assert_stmt(self, node: AssertStmt) -&gt; AssertStmt:
    return AssertStmt(self.expr(node.expr), self.optional_expr(node.msg))

</t>
<t tx="ekr.20230831011821.201">def visit_del_stmt(self, node: DelStmt) -&gt; DelStmt:
    return DelStmt(self.expr(node.expr))

</t>
<t tx="ekr.20230831011821.202">def visit_if_stmt(self, node: IfStmt) -&gt; IfStmt:
    return IfStmt(
        self.expressions(node.expr),
        self.blocks(node.body),
        self.optional_block(node.else_body),
    )

</t>
<t tx="ekr.20230831011821.203">def visit_break_stmt(self, node: BreakStmt) -&gt; BreakStmt:
    return BreakStmt()

</t>
<t tx="ekr.20230831011821.204">def visit_continue_stmt(self, node: ContinueStmt) -&gt; ContinueStmt:
    return ContinueStmt()

</t>
<t tx="ekr.20230831011821.205">def visit_pass_stmt(self, node: PassStmt) -&gt; PassStmt:
    return PassStmt()

</t>
<t tx="ekr.20230831011821.206">def visit_raise_stmt(self, node: RaiseStmt) -&gt; RaiseStmt:
    return RaiseStmt(self.optional_expr(node.expr), self.optional_expr(node.from_expr))

</t>
<t tx="ekr.20230831011821.207">def visit_try_stmt(self, node: TryStmt) -&gt; TryStmt:
    new = TryStmt(
        self.block(node.body),
        self.optional_names(node.vars),
        self.optional_expressions(node.types),
        self.blocks(node.handlers),
        self.optional_block(node.else_body),
        self.optional_block(node.finally_body),
    )
    new.is_star = node.is_star
    return new

</t>
<t tx="ekr.20230831011821.208">def visit_with_stmt(self, node: WithStmt) -&gt; WithStmt:
    new = WithStmt(
        self.expressions(node.expr),
        self.optional_expressions(node.target),
        self.block(node.body),
        self.optional_type(node.unanalyzed_type),
    )
    new.is_async = node.is_async
    new.analyzed_types = [self.type(typ) for typ in node.analyzed_types]
    return new

</t>
<t tx="ekr.20230831011821.209">def visit_as_pattern(self, p: AsPattern) -&gt; AsPattern:
    return AsPattern(
        pattern=self.pattern(p.pattern) if p.pattern is not None else None,
        name=self.duplicate_name(p.name) if p.name is not None else None,
    )

</t>
<t tx="ekr.20230831011821.21">def visit_try_stmt(self, o: TryStmt) -&gt; None:
    o.body.accept(self)
    for i in range(len(o.types)):
        tp = o.types[i]
        if tp is not None:
            tp.accept(self)
        o.handlers[i].accept(self)
    for v in o.vars:
        if v is not None:
            v.accept(self)
    if o.else_body is not None:
        o.else_body.accept(self)
    if o.finally_body is not None:
        o.finally_body.accept(self)

</t>
<t tx="ekr.20230831011821.210">def visit_or_pattern(self, p: OrPattern) -&gt; OrPattern:
    return OrPattern([self.pattern(pat) for pat in p.patterns])

</t>
<t tx="ekr.20230831011821.211">def visit_value_pattern(self, p: ValuePattern) -&gt; ValuePattern:
    return ValuePattern(self.expr(p.expr))

</t>
<t tx="ekr.20230831011821.212">def visit_singleton_pattern(self, p: SingletonPattern) -&gt; SingletonPattern:
    return SingletonPattern(p.value)

</t>
<t tx="ekr.20230831011821.213">def visit_sequence_pattern(self, p: SequencePattern) -&gt; SequencePattern:
    return SequencePattern([self.pattern(pat) for pat in p.patterns])

</t>
<t tx="ekr.20230831011821.214">def visit_starred_pattern(self, p: StarredPattern) -&gt; StarredPattern:
    return StarredPattern(self.duplicate_name(p.capture) if p.capture is not None else None)

</t>
<t tx="ekr.20230831011821.215">def visit_mapping_pattern(self, p: MappingPattern) -&gt; MappingPattern:
    return MappingPattern(
        keys=[self.expr(expr) for expr in p.keys],
        values=[self.pattern(pat) for pat in p.values],
        rest=self.duplicate_name(p.rest) if p.rest is not None else None,
    )

</t>
<t tx="ekr.20230831011821.216">def visit_class_pattern(self, p: ClassPattern) -&gt; ClassPattern:
    class_ref = p.class_ref.accept(self)
    assert isinstance(class_ref, RefExpr)
    return ClassPattern(
        class_ref=class_ref,
        positionals=[self.pattern(pat) for pat in p.positionals],
        keyword_keys=list(p.keyword_keys),
        keyword_values=[self.pattern(pat) for pat in p.keyword_values],
    )

</t>
<t tx="ekr.20230831011821.217">def visit_match_stmt(self, o: MatchStmt) -&gt; MatchStmt:
    return MatchStmt(
        subject=self.expr(o.subject),
        patterns=[self.pattern(p) for p in o.patterns],
        guards=self.optional_expressions(o.guards),
        bodies=self.blocks(o.bodies),
    )

</t>
<t tx="ekr.20230831011821.218">def visit_star_expr(self, node: StarExpr) -&gt; StarExpr:
    return StarExpr(node.expr)

</t>
<t tx="ekr.20230831011821.219">def visit_int_expr(self, node: IntExpr) -&gt; IntExpr:
    return IntExpr(node.value)

</t>
<t tx="ekr.20230831011821.22">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    for i in range(len(o.expr)):
        o.expr[i].accept(self)
        targ = o.target[i]
        if targ is not None:
            targ.accept(self)
    o.body.accept(self)

</t>
<t tx="ekr.20230831011821.220">def visit_str_expr(self, node: StrExpr) -&gt; StrExpr:
    return StrExpr(node.value)

</t>
<t tx="ekr.20230831011821.221">def visit_bytes_expr(self, node: BytesExpr) -&gt; BytesExpr:
    return BytesExpr(node.value)

</t>
<t tx="ekr.20230831011821.222">def visit_float_expr(self, node: FloatExpr) -&gt; FloatExpr:
    return FloatExpr(node.value)

</t>
<t tx="ekr.20230831011821.223">def visit_complex_expr(self, node: ComplexExpr) -&gt; ComplexExpr:
    return ComplexExpr(node.value)

</t>
<t tx="ekr.20230831011821.224">def visit_ellipsis(self, node: EllipsisExpr) -&gt; EllipsisExpr:
    return EllipsisExpr()

</t>
<t tx="ekr.20230831011821.225">def visit_name_expr(self, node: NameExpr) -&gt; NameExpr:
    return self.duplicate_name(node)

</t>
<t tx="ekr.20230831011821.226">def duplicate_name(self, node: NameExpr) -&gt; NameExpr:
    # This method is used when the transform result must be a NameExpr.
    # visit_name_expr() is used when there is no such restriction.
    new = NameExpr(node.name)
    self.copy_ref(new, node)
    new.is_special_form = node.is_special_form
    return new

</t>
<t tx="ekr.20230831011821.227">def visit_member_expr(self, node: MemberExpr) -&gt; MemberExpr:
    member = MemberExpr(self.expr(node.expr), node.name)
    if node.def_var:
        # This refers to an attribute and we don't transform attributes by default,
        # just normal variables.
        member.def_var = node.def_var
    self.copy_ref(member, node)
    return member

</t>
<t tx="ekr.20230831011821.228">def copy_ref(self, new: RefExpr, original: RefExpr) -&gt; None:
    new.kind = original.kind
    new.fullname = original.fullname
    target = original.node
    if isinstance(target, Var):
        # Do not transform references to global variables. See
        # testGenericFunctionAliasExpand for an example where this is important.
        if original.kind != GDEF:
            target = self.visit_var(target)
    elif isinstance(target, Decorator):
        target = self.visit_var(target.var)
    elif isinstance(target, FuncDef):
        # Use a placeholder node for the function if it exists.
        target = self.func_placeholder_map.get(target, target)
    new.node = target
    new.is_new_def = original.is_new_def
    new.is_inferred_def = original.is_inferred_def

</t>
<t tx="ekr.20230831011821.229">def visit_yield_from_expr(self, node: YieldFromExpr) -&gt; YieldFromExpr:
    return YieldFromExpr(self.expr(node.expr))

</t>
<t tx="ekr.20230831011821.23">def visit_match_stmt(self, o: MatchStmt) -&gt; None:
    o.subject.accept(self)
    for i in range(len(o.patterns)):
        o.patterns[i].accept(self)
        guard = o.guards[i]
        if guard is not None:
            guard.accept(self)
        o.bodies[i].accept(self)

</t>
<t tx="ekr.20230831011821.230">def visit_yield_expr(self, node: YieldExpr) -&gt; YieldExpr:
    return YieldExpr(self.optional_expr(node.expr))

</t>
<t tx="ekr.20230831011821.231">def visit_await_expr(self, node: AwaitExpr) -&gt; AwaitExpr:
    return AwaitExpr(self.expr(node.expr))

</t>
<t tx="ekr.20230831011821.232">def visit_call_expr(self, node: CallExpr) -&gt; CallExpr:
    return CallExpr(
        self.expr(node.callee),
        self.expressions(node.args),
        node.arg_kinds.copy(),
        node.arg_names.copy(),
        self.optional_expr(node.analyzed),
    )

</t>
<t tx="ekr.20230831011821.233">def visit_op_expr(self, node: OpExpr) -&gt; OpExpr:
    new = OpExpr(
        node.op,
        self.expr(node.left),
        self.expr(node.right),
        cast(Optional[TypeAliasExpr], self.optional_expr(node.analyzed)),
    )
    new.method_type = self.optional_type(node.method_type)
    return new

</t>
<t tx="ekr.20230831011821.234">def visit_comparison_expr(self, node: ComparisonExpr) -&gt; ComparisonExpr:
    new = ComparisonExpr(node.operators, self.expressions(node.operands))
    new.method_types = [self.optional_type(t) for t in node.method_types]
    return new

</t>
<t tx="ekr.20230831011821.235">def visit_cast_expr(self, node: CastExpr) -&gt; CastExpr:
    return CastExpr(self.expr(node.expr), self.type(node.type))

</t>
<t tx="ekr.20230831011821.236">def visit_assert_type_expr(self, node: AssertTypeExpr) -&gt; AssertTypeExpr:
    return AssertTypeExpr(self.expr(node.expr), self.type(node.type))

</t>
<t tx="ekr.20230831011821.237">def visit_reveal_expr(self, node: RevealExpr) -&gt; RevealExpr:
    if node.kind == REVEAL_TYPE:
        assert node.expr is not None
        return RevealExpr(kind=REVEAL_TYPE, expr=self.expr(node.expr))
    else:
        # Reveal locals expressions don't have any sub expressions
        return node

</t>
<t tx="ekr.20230831011821.238">def visit_super_expr(self, node: SuperExpr) -&gt; SuperExpr:
    call = self.expr(node.call)
    assert isinstance(call, CallExpr)
    new = SuperExpr(node.name, call)
    new.info = node.info
    return new

</t>
<t tx="ekr.20230831011821.239">def visit_assignment_expr(self, node: AssignmentExpr) -&gt; AssignmentExpr:
    return AssignmentExpr(self.expr(node.target), self.expr(node.value))

</t>
<t tx="ekr.20230831011821.24">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.240">def visit_unary_expr(self, node: UnaryExpr) -&gt; UnaryExpr:
    new = UnaryExpr(node.op, self.expr(node.expr))
    new.method_type = self.optional_type(node.method_type)
    return new

</t>
<t tx="ekr.20230831011821.241">def visit_list_expr(self, node: ListExpr) -&gt; ListExpr:
    return ListExpr(self.expressions(node.items))

</t>
<t tx="ekr.20230831011821.242">def visit_dict_expr(self, node: DictExpr) -&gt; DictExpr:
    return DictExpr(
        [(self.expr(key) if key else None, self.expr(value)) for key, value in node.items]
    )

</t>
<t tx="ekr.20230831011821.243">def visit_tuple_expr(self, node: TupleExpr) -&gt; TupleExpr:
    return TupleExpr(self.expressions(node.items))

</t>
<t tx="ekr.20230831011821.244">def visit_set_expr(self, node: SetExpr) -&gt; SetExpr:
    return SetExpr(self.expressions(node.items))

</t>
<t tx="ekr.20230831011821.245">def visit_index_expr(self, node: IndexExpr) -&gt; IndexExpr:
    new = IndexExpr(self.expr(node.base), self.expr(node.index))
    if node.method_type:
        new.method_type = self.type(node.method_type)
    if node.analyzed:
        if isinstance(node.analyzed, TypeApplication):
            new.analyzed = self.visit_type_application(node.analyzed)
        else:
            new.analyzed = self.visit_type_alias_expr(node.analyzed)
        new.analyzed.set_line(node.analyzed)
    return new

</t>
<t tx="ekr.20230831011821.246">def visit_type_application(self, node: TypeApplication) -&gt; TypeApplication:
    return TypeApplication(self.expr(node.expr), self.types(node.types))

</t>
<t tx="ekr.20230831011821.247">def visit_list_comprehension(self, node: ListComprehension) -&gt; ListComprehension:
    generator = self.duplicate_generator(node.generator)
    generator.set_line(node.generator)
    return ListComprehension(generator)

</t>
<t tx="ekr.20230831011821.248">def visit_set_comprehension(self, node: SetComprehension) -&gt; SetComprehension:
    generator = self.duplicate_generator(node.generator)
    generator.set_line(node.generator)
    return SetComprehension(generator)

</t>
<t tx="ekr.20230831011821.249">def visit_dictionary_comprehension(
    self, node: DictionaryComprehension
) -&gt; DictionaryComprehension:
    return DictionaryComprehension(
        self.expr(node.key),
        self.expr(node.value),
        [self.expr(index) for index in node.indices],
        [self.expr(s) for s in node.sequences],
        [[self.expr(cond) for cond in conditions] for conditions in node.condlists],
        node.is_async,
    )

</t>
<t tx="ekr.20230831011821.25">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.250">def visit_generator_expr(self, node: GeneratorExpr) -&gt; GeneratorExpr:
    return self.duplicate_generator(node)

</t>
<t tx="ekr.20230831011821.251">def duplicate_generator(self, node: GeneratorExpr) -&gt; GeneratorExpr:
    return GeneratorExpr(
        self.expr(node.left_expr),
        [self.expr(index) for index in node.indices],
        [self.expr(s) for s in node.sequences],
        [[self.expr(cond) for cond in conditions] for conditions in node.condlists],
        node.is_async,
    )

</t>
<t tx="ekr.20230831011821.252">def visit_slice_expr(self, node: SliceExpr) -&gt; SliceExpr:
    return SliceExpr(
        self.optional_expr(node.begin_index),
        self.optional_expr(node.end_index),
        self.optional_expr(node.stride),
    )

</t>
<t tx="ekr.20230831011821.253">def visit_conditional_expr(self, node: ConditionalExpr) -&gt; ConditionalExpr:
    return ConditionalExpr(
        self.expr(node.cond), self.expr(node.if_expr), self.expr(node.else_expr)
    )

</t>
<t tx="ekr.20230831011821.254">def visit_type_var_expr(self, node: TypeVarExpr) -&gt; TypeVarExpr:
    return TypeVarExpr(
        node.name,
        node.fullname,
        self.types(node.values),
        self.type(node.upper_bound),
        self.type(node.default),
        variance=node.variance,
    )

</t>
<t tx="ekr.20230831011821.255">def visit_paramspec_expr(self, node: ParamSpecExpr) -&gt; ParamSpecExpr:
    return ParamSpecExpr(
        node.name,
        node.fullname,
        self.type(node.upper_bound),
        self.type(node.default),
        variance=node.variance,
    )

</t>
<t tx="ekr.20230831011821.256">def visit_type_var_tuple_expr(self, node: TypeVarTupleExpr) -&gt; TypeVarTupleExpr:
    return TypeVarTupleExpr(
        node.name,
        node.fullname,
        self.type(node.upper_bound),
        node.tuple_fallback,
        self.type(node.default),
        variance=node.variance,
    )

</t>
<t tx="ekr.20230831011821.257">def visit_type_alias_expr(self, node: TypeAliasExpr) -&gt; TypeAliasExpr:
    return TypeAliasExpr(node.node)

</t>
<t tx="ekr.20230831011821.258">def visit_newtype_expr(self, node: NewTypeExpr) -&gt; NewTypeExpr:
    res = NewTypeExpr(node.name, node.old_type, line=node.line, column=node.column)
    res.info = node.info
    return res

</t>
<t tx="ekr.20230831011821.259">def visit_namedtuple_expr(self, node: NamedTupleExpr) -&gt; NamedTupleExpr:
    return NamedTupleExpr(node.info)

</t>
<t tx="ekr.20230831011821.26">def visit_yield_expr(self, o: YieldExpr) -&gt; None:
    if o.expr:
        o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.260">def visit_enum_call_expr(self, node: EnumCallExpr) -&gt; EnumCallExpr:
    return EnumCallExpr(node.info, node.items, node.values)

</t>
<t tx="ekr.20230831011821.261">def visit_typeddict_expr(self, node: TypedDictExpr) -&gt; Node:
    return TypedDictExpr(node.info)

</t>
<t tx="ekr.20230831011821.262">def visit__promote_expr(self, node: PromoteExpr) -&gt; PromoteExpr:
    return PromoteExpr(node.type)

</t>
<t tx="ekr.20230831011821.263">def visit_temp_node(self, node: TempNode) -&gt; TempNode:
    return TempNode(self.type(node.type))

</t>
<t tx="ekr.20230831011821.264">def node(self, node: Node) -&gt; Node:
    new = node.accept(self)
    new.set_line(node)
    return new

</t>
<t tx="ekr.20230831011821.265">def mypyfile(self, node: MypyFile) -&gt; MypyFile:
    new = node.accept(self)
    assert isinstance(new, MypyFile)
    new.set_line(node)
    return new

</t>
<t tx="ekr.20230831011821.266">def expr(self, expr: Expression) -&gt; Expression:
    new = expr.accept(self)
    assert isinstance(new, Expression)
    new.set_line(expr)
    return new

</t>
<t tx="ekr.20230831011821.267">def stmt(self, stmt: Statement) -&gt; Statement:
    new = stmt.accept(self)
    assert isinstance(new, Statement)
    new.set_line(stmt)
    return new

</t>
<t tx="ekr.20230831011821.268">def pattern(self, pattern: Pattern) -&gt; Pattern:
    new = pattern.accept(self)
    assert isinstance(new, Pattern)
    new.set_line(pattern)
    return new

</t>
<t tx="ekr.20230831011821.269"># Helpers
#
# All the node helpers also propagate line numbers.

def optional_expr(self, expr: Expression | None) -&gt; Expression | None:
    if expr:
        return self.expr(expr)
    else:
        return None

</t>
<t tx="ekr.20230831011821.27">def visit_call_expr(self, o: CallExpr) -&gt; None:
    o.callee.accept(self)
    for a in o.args:
        a.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20230831011821.270">def block(self, block: Block) -&gt; Block:
    new = self.visit_block(block)
    new.line = block.line
    return new

</t>
<t tx="ekr.20230831011821.271">def optional_block(self, block: Block | None) -&gt; Block | None:
    if block:
        return self.block(block)
    else:
        return None

</t>
<t tx="ekr.20230831011821.272">def statements(self, statements: list[Statement]) -&gt; list[Statement]:
    return [self.stmt(stmt) for stmt in statements]

</t>
<t tx="ekr.20230831011821.273">def expressions(self, expressions: list[Expression]) -&gt; list[Expression]:
    return [self.expr(expr) for expr in expressions]

</t>
<t tx="ekr.20230831011821.274">def optional_expressions(
    self, expressions: Iterable[Expression | None]
) -&gt; list[Expression | None]:
    return [self.optional_expr(expr) for expr in expressions]

</t>
<t tx="ekr.20230831011821.275">def blocks(self, blocks: list[Block]) -&gt; list[Block]:
    return [self.block(block) for block in blocks]

</t>
<t tx="ekr.20230831011821.276">def names(self, names: list[NameExpr]) -&gt; list[NameExpr]:
    return [self.duplicate_name(name) for name in names]

</t>
<t tx="ekr.20230831011821.277">def optional_names(self, names: Iterable[NameExpr | None]) -&gt; list[NameExpr | None]:
    result: list[NameExpr | None] = []
    for name in names:
        if name:
            result.append(self.duplicate_name(name))
        else:
            result.append(None)
    return result

</t>
<t tx="ekr.20230831011821.278">def type(self, type: Type) -&gt; Type:
    # Override this method to transform types.
    return type

</t>
<t tx="ekr.20230831011821.279">def optional_type(self, type: Type | None) -&gt; Type | None:
    if type:
        return self.type(type)
    else:
        return None

</t>
<t tx="ekr.20230831011821.28">def visit_op_expr(self, o: OpExpr) -&gt; None:
    o.left.accept(self)
    o.right.accept(self)
    if o.analyzed is not None:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20230831011821.280">def types(self, types: list[Type]) -&gt; list[Type]:
    return [self.type(type) for type in types]


</t>
<t tx="ekr.20230831011821.281">class FuncMapInitializer(TraverserVisitor):
    """This traverser creates mappings from nested FuncDefs to placeholder FuncDefs.

    The placeholders will later be replaced with transformed nodes.
    """

    @others
</t>
<t tx="ekr.20230831011821.282">def __init__(self, transformer: TransformVisitor) -&gt; None:
    self.transformer = transformer

</t>
<t tx="ekr.20230831011821.283">def visit_func_def(self, node: FuncDef) -&gt; None:
    if node not in self.transformer.func_placeholder_map:
        # Haven't seen this FuncDef before, so create a placeholder node.
        self.transformer.func_placeholder_map[node] = FuncDef(
            node.name, node.arguments, node.body, None
        )
    super().visit_func_def(node)
</t>
<t tx="ekr.20230831011821.284">@path mypy
&lt;&lt; tvar_scope.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.285">from __future__ import annotations

from mypy.nodes import (
    ParamSpecExpr,
    SymbolTableNode,
    TypeVarExpr,
    TypeVarLikeExpr,
    TypeVarTupleExpr,
)
from mypy.types import (
    ParamSpecFlavor,
    ParamSpecType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
)


</t>
<t tx="ekr.20230831011821.286">class TypeVarLikeScope:
    """Scope that holds bindings for type variables and parameter specifications.

    Node fullname -&gt; TypeVarLikeType.
    """

    @others
</t>
<t tx="ekr.20230831011821.287">def __init__(
    self,
    parent: TypeVarLikeScope | None = None,
    is_class_scope: bool = False,
    prohibited: TypeVarLikeScope | None = None,
    namespace: str = "",
) -&gt; None:
    """Initializer for TypeVarLikeScope

    Parameters:
      parent: the outer scope for this scope
      is_class_scope: True if this represents a generic class
      prohibited: Type variables that aren't strictly in scope exactly,
                  but can't be bound because they're part of an outer class's scope.
    """
    self.scope: dict[str, TypeVarLikeType] = {}
    self.parent = parent
    self.func_id = 0
    self.class_id = 0
    self.is_class_scope = is_class_scope
    self.prohibited = prohibited
    self.namespace = namespace
    if parent is not None:
        self.func_id = parent.func_id
        self.class_id = parent.class_id

</t>
<t tx="ekr.20230831011821.288">def get_function_scope(self) -&gt; TypeVarLikeScope | None:
    """Get the nearest parent that's a function scope, not a class scope"""
    it: TypeVarLikeScope | None = self
    while it is not None and it.is_class_scope:
        it = it.parent
    return it

</t>
<t tx="ekr.20230831011821.289">def allow_binding(self, fullname: str) -&gt; bool:
    if fullname in self.scope:
        return False
    elif self.parent and not self.parent.allow_binding(fullname):
        return False
    elif self.prohibited and not self.prohibited.allow_binding(fullname):
        return False
    return True

</t>
<t tx="ekr.20230831011821.29">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    for operand in o.operands:
        operand.accept(self)

</t>
<t tx="ekr.20230831011821.290">def method_frame(self) -&gt; TypeVarLikeScope:
    """A new scope frame for binding a method"""
    return TypeVarLikeScope(self, False, None)

</t>
<t tx="ekr.20230831011821.291">def class_frame(self, namespace: str) -&gt; TypeVarLikeScope:
    """A new scope frame for binding a class. Prohibits *this* class's tvars"""
    return TypeVarLikeScope(self.get_function_scope(), True, self, namespace=namespace)

</t>
<t tx="ekr.20230831011821.292">def new_unique_func_id(self) -&gt; int:
    """Used by plugin-like code that needs to make synthetic generic functions."""
    self.func_id -= 1
    return self.func_id

</t>
<t tx="ekr.20230831011821.293">def bind_new(self, name: str, tvar_expr: TypeVarLikeExpr) -&gt; TypeVarLikeType:
    if self.is_class_scope:
        self.class_id += 1
        i = self.class_id
        namespace = self.namespace
    else:
        self.func_id -= 1
        i = self.func_id
        # TODO: Consider also using namespaces for functions
        namespace = ""
    if isinstance(tvar_expr, TypeVarExpr):
        tvar_def: TypeVarLikeType = TypeVarType(
            name=name,
            fullname=tvar_expr.fullname,
            id=TypeVarId(i, namespace=namespace),
            values=tvar_expr.values,
            upper_bound=tvar_expr.upper_bound,
            default=tvar_expr.default,
            variance=tvar_expr.variance,
            line=tvar_expr.line,
            column=tvar_expr.column,
        )
    elif isinstance(tvar_expr, ParamSpecExpr):
        tvar_def = ParamSpecType(
            name,
            tvar_expr.fullname,
            i,
            flavor=ParamSpecFlavor.BARE,
            upper_bound=tvar_expr.upper_bound,
            default=tvar_expr.default,
            line=tvar_expr.line,
            column=tvar_expr.column,
        )
    elif isinstance(tvar_expr, TypeVarTupleExpr):
        tvar_def = TypeVarTupleType(
            name,
            tvar_expr.fullname,
            i,
            upper_bound=tvar_expr.upper_bound,
            tuple_fallback=tvar_expr.tuple_fallback,
            default=tvar_expr.default,
            line=tvar_expr.line,
            column=tvar_expr.column,
        )
    else:
        assert False
    self.scope[tvar_expr.fullname] = tvar_def
    return tvar_def

</t>
<t tx="ekr.20230831011821.294">def bind_existing(self, tvar_def: TypeVarLikeType) -&gt; None:
    self.scope[tvar_def.fullname] = tvar_def

</t>
<t tx="ekr.20230831011821.295">def get_binding(self, item: str | SymbolTableNode) -&gt; TypeVarLikeType | None:
    fullname = item.fullname if isinstance(item, SymbolTableNode) else item
    assert fullname
    if fullname in self.scope:
        return self.scope[fullname]
    elif self.parent is not None:
        return self.parent.get_binding(fullname)
    else:
        return None

</t>
<t tx="ekr.20230831011821.296">def __str__(self) -&gt; str:
    me = ", ".join(f"{k}: {v.name}`{v.id}" for k, v in self.scope.items())
    if self.parent is None:
        return me
    return f"{self.parent} &lt;- {me}"
</t>
<t tx="ekr.20230831011821.297">@path mypy
"""Type visitor classes.

This module defines the type visitors that are intended to be
subclassed by other code.  They have been separated out into their own
module to ease converting mypy to run under mypyc, since currently
mypyc-extension classes can extend interpreted classes but not the
other way around. Separating them out, then, allows us to compile
types before we can compile everything that uses a TypeVisitor.

The visitors are all re-exported from mypy.types and that is how
other modules refer to them.
"""

&lt;&lt; type_visitor.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.299">from __future__ import annotations

from abc import abstractmethod
from typing import Any, Callable, Final, Generic, Iterable, Sequence, TypeVar, cast

from mypy_extensions import mypyc_attr, trait

from mypy.types import (
    AnyType,
    CallableArgument,
    CallableType,
    DeletedType,
    EllipsisType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    PlaceholderType,
    RawExpressionType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
)

T = TypeVar("T")


@trait
@mypyc_attr(allow_interpreted_subclasses=True)
</t>
<t tx="ekr.20230831011821.3">def __init__(self) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.30">def visit_slice_expr(self, o: SliceExpr) -&gt; None:
    if o.begin_index is not None:
        o.begin_index.accept(self)
    if o.end_index is not None:
        o.end_index.accept(self)
    if o.stride is not None:
        o.stride.accept(self)

</t>
<t tx="ekr.20230831011821.300">class TypeVisitor(Generic[T]):
    """Visitor class for types (Type subclasses).

    The parameter T is the return type of the visit methods.
    """

    @others
</t>
<t tx="ekr.20230831011821.301">@abstractmethod
def visit_unbound_type(self, t: UnboundType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.302">@abstractmethod
def visit_any(self, t: AnyType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.303">@abstractmethod
def visit_none_type(self, t: NoneType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.304">@abstractmethod
def visit_uninhabited_type(self, t: UninhabitedType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.305">@abstractmethod
def visit_erased_type(self, t: ErasedType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.306">@abstractmethod
def visit_deleted_type(self, t: DeletedType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.307">@abstractmethod
def visit_type_var(self, t: TypeVarType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.308">@abstractmethod
def visit_param_spec(self, t: ParamSpecType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.309">@abstractmethod
def visit_parameters(self, t: Parameters) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.31">def visit_cast_expr(self, o: CastExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.310">@abstractmethod
def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.311">@abstractmethod
def visit_instance(self, t: Instance) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.312">@abstractmethod
def visit_callable_type(self, t: CallableType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.313">@abstractmethod
def visit_overloaded(self, t: Overloaded) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.314">@abstractmethod
def visit_tuple_type(self, t: TupleType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.315">@abstractmethod
def visit_typeddict_type(self, t: TypedDictType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.316">@abstractmethod
def visit_literal_type(self, t: LiteralType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.317">@abstractmethod
def visit_union_type(self, t: UnionType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.318">@abstractmethod
def visit_partial_type(self, t: PartialType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.319">@abstractmethod
def visit_type_type(self, t: TypeType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.32">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.320">@abstractmethod
def visit_type_alias_type(self, t: TypeAliasType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.321">@abstractmethod
def visit_unpack_type(self, t: UnpackType) -&gt; T:
    pass


</t>
<t tx="ekr.20230831011821.322">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class SyntheticTypeVisitor(TypeVisitor[T]):
    """A TypeVisitor that also knows how to visit synthetic AST constructs.

    Not just real types.
    """

    @others
</t>
<t tx="ekr.20230831011821.323">@abstractmethod
def visit_type_list(self, t: TypeList) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.324">@abstractmethod
def visit_callable_argument(self, t: CallableArgument) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.325">@abstractmethod
def visit_ellipsis_type(self, t: EllipsisType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.326">@abstractmethod
def visit_raw_expression_type(self, t: RawExpressionType) -&gt; T:
    pass

</t>
<t tx="ekr.20230831011821.327">@abstractmethod
def visit_placeholder_type(self, t: PlaceholderType) -&gt; T:
    pass


</t>
<t tx="ekr.20230831011821.328">@mypyc_attr(allow_interpreted_subclasses=True)
class TypeTranslator(TypeVisitor[Type]):
    """Identity type transformation.

    Subclass this and override some methods to implement a non-trivial
    transformation.
    """

    @others
</t>
<t tx="ekr.20230831011821.329">def visit_unbound_type(self, t: UnboundType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.33">def visit_reveal_expr(self, o: RevealExpr) -&gt; None:
    if o.kind == REVEAL_TYPE:
        assert o.expr is not None
        o.expr.accept(self)
    else:
        # RevealLocalsExpr doesn't have an inner expression
        pass

</t>
<t tx="ekr.20230831011821.330">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.331">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.332">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.333">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.334">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.335">def visit_instance(self, t: Instance) -&gt; Type:
    last_known_value: LiteralType | None = None
    if t.last_known_value is not None:
        raw_last_known_value = t.last_known_value.accept(self)
        assert isinstance(raw_last_known_value, LiteralType)  # type: ignore[misc]
        last_known_value = raw_last_known_value
    return Instance(
        typ=t.type,
        args=self.translate_types(t.args),
        line=t.line,
        column=t.column,
        last_known_value=last_known_value,
    )

</t>
<t tx="ekr.20230831011821.336">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.337">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.338">def visit_parameters(self, t: Parameters) -&gt; Type:
    return t.copy_modified(arg_types=self.translate_types(t.arg_types))

</t>
<t tx="ekr.20230831011821.339">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.34">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    o.target.accept(self)
    o.value.accept(self)

</t>
<t tx="ekr.20230831011821.340">def visit_partial_type(self, t: PartialType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.341">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    return UnpackType(t.type.accept(self))

</t>
<t tx="ekr.20230831011821.342">def visit_callable_type(self, t: CallableType) -&gt; Type:
    return t.copy_modified(
        arg_types=self.translate_types(t.arg_types),
        ret_type=t.ret_type.accept(self),
        variables=self.translate_variables(t.variables),
    )

</t>
<t tx="ekr.20230831011821.343">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    return TupleType(
        self.translate_types(t.items),
        # TODO: This appears to be unsafe.
        cast(Any, t.partial_fallback.accept(self)),
        t.line,
        t.column,
    )

</t>
<t tx="ekr.20230831011821.344">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = {item_name: item_type.accept(self) for (item_name, item_type) in t.items.items()}
    return TypedDictType(
        items,
        t.required_keys,
        # TODO: This appears to be unsafe.
        cast(Any, t.fallback.accept(self)),
        t.line,
        t.column,
    )

</t>
<t tx="ekr.20230831011821.345">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    fallback = t.fallback.accept(self)
    assert isinstance(fallback, Instance)  # type: ignore[misc]
    return LiteralType(value=t.value, fallback=fallback, line=t.line, column=t.column)

</t>
<t tx="ekr.20230831011821.346">def visit_union_type(self, t: UnionType) -&gt; Type:
    return UnionType(self.translate_types(t.items), t.line, t.column)

</t>
<t tx="ekr.20230831011821.347">def translate_types(self, types: Iterable[Type]) -&gt; list[Type]:
    return [t.accept(self) for t in types]

</t>
<t tx="ekr.20230831011821.348">def translate_variables(
    self, variables: Sequence[TypeVarLikeType]
) -&gt; Sequence[TypeVarLikeType]:
    return variables

</t>
<t tx="ekr.20230831011821.349">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    items: list[CallableType] = []
    for item in t.items:
        new = item.accept(self)
        assert isinstance(new, CallableType)  # type: ignore[misc]
        items.append(new)
    return Overloaded(items=items)

</t>
<t tx="ekr.20230831011821.35">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.350">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(t.item.accept(self), line=t.line, column=t.column)

</t>
<t tx="ekr.20230831011821.351">@abstractmethod
def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # This method doesn't have a default implementation for type translators,
    # because type aliases are special: some information is contained in the
    # TypeAlias node, and we normally don't generate new nodes. Every subclass
    # must implement this depending on its semantics.
    pass


</t>
<t tx="ekr.20230831011821.352">@mypyc_attr(allow_interpreted_subclasses=True)
class TypeQuery(SyntheticTypeVisitor[T]):
    """Visitor for performing queries of types.

    strategy is used to combine results for a series of types,
    common use cases involve a boolean query using `any` or `all`.

    Note: this visitor keeps an internal state (tracks type aliases to avoid
    recursion), so it should *never* be re-used for querying different types,
    create a new visitor instance instead.

    # TODO: check that we don't have existing violations of this rule.
    """

    @others
</t>
<t tx="ekr.20230831011821.353">def __init__(self, strategy: Callable[[list[T]], T]) -&gt; None:
    self.strategy = strategy
    # Keep track of the type aliases already visited. This is needed to avoid
    # infinite recursion on types like A = Union[int, List[A]].
    self.seen_aliases: set[TypeAliasType] = set()
    # By default, we eagerly expand type aliases, and query also types in the
    # alias target. In most cases this is a desired behavior, but we may want
    # to skip targets in some cases (e.g. when collecting type variables).
    self.skip_alias_target = False

</t>
<t tx="ekr.20230831011821.354">def visit_unbound_type(self, t: UnboundType) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20230831011821.355">def visit_type_list(self, t: TypeList) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20230831011821.356">def visit_callable_argument(self, t: CallableArgument) -&gt; T:
    return t.typ.accept(self)

</t>
<t tx="ekr.20230831011821.357">def visit_any(self, t: AnyType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.358">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.359">def visit_none_type(self, t: NoneType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.36">def visit_list_expr(self, o: ListExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20230831011821.360">def visit_erased_type(self, t: ErasedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.361">def visit_deleted_type(self, t: DeletedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.362">def visit_type_var(self, t: TypeVarType) -&gt; T:
    return self.query_types([t.upper_bound, t.default] + t.values)

</t>
<t tx="ekr.20230831011821.363">def visit_param_spec(self, t: ParamSpecType) -&gt; T:
    return self.query_types([t.upper_bound, t.default, t.prefix])

</t>
<t tx="ekr.20230831011821.364">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; T:
    return self.query_types([t.upper_bound, t.default])

</t>
<t tx="ekr.20230831011821.365">def visit_unpack_type(self, t: UnpackType) -&gt; T:
    return self.query_types([t.type])

</t>
<t tx="ekr.20230831011821.366">def visit_parameters(self, t: Parameters) -&gt; T:
    return self.query_types(t.arg_types)

</t>
<t tx="ekr.20230831011821.367">def visit_partial_type(self, t: PartialType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.368">def visit_instance(self, t: Instance) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20230831011821.369">def visit_callable_type(self, t: CallableType) -&gt; T:
    # FIX generics
    return self.query_types(t.arg_types + [t.ret_type])

</t>
<t tx="ekr.20230831011821.37">def visit_tuple_expr(self, o: TupleExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20230831011821.370">def visit_tuple_type(self, t: TupleType) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20230831011821.371">def visit_typeddict_type(self, t: TypedDictType) -&gt; T:
    return self.query_types(t.items.values())

</t>
<t tx="ekr.20230831011821.372">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.373">def visit_literal_type(self, t: LiteralType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.374">def visit_union_type(self, t: UnionType) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20230831011821.375">def visit_overloaded(self, t: Overloaded) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20230831011821.376">def visit_type_type(self, t: TypeType) -&gt; T:
    return t.item.accept(self)

</t>
<t tx="ekr.20230831011821.377">def visit_ellipsis_type(self, t: EllipsisType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20230831011821.378">def visit_placeholder_type(self, t: PlaceholderType) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20230831011821.379">def visit_type_alias_type(self, t: TypeAliasType) -&gt; T:
    # Skip type aliases already visited types to avoid infinite recursion.
    # TODO: Ideally we should fire subvisitors here (or use caching) if we care
    #       about duplicates.
    if t in self.seen_aliases:
        return self.strategy([])
    self.seen_aliases.add(t)
    if self.skip_alias_target:
        return self.query_types(t.args)
    return get_proper_type(t).accept(self)

</t>
<t tx="ekr.20230831011821.38">def visit_dict_expr(self, o: DictExpr) -&gt; None:
    for k, v in o.items:
        if k is not None:
            k.accept(self)
        v.accept(self)

</t>
<t tx="ekr.20230831011821.380">def query_types(self, types: Iterable[Type]) -&gt; T:
    """Perform a query for a list of types using the strategy to combine the results."""
    return self.strategy([t.accept(self) for t in types])


</t>
<t tx="ekr.20230831011821.381"># Return True if at least one type component returns True
ANY_STRATEGY: Final = 0
# Return True if no type component returns False
ALL_STRATEGY: Final = 1


class BoolTypeQuery(SyntheticTypeVisitor[bool]):
    """Visitor for performing recursive queries of types with a bool result.

    Use TypeQuery if you need non-bool results.

    'strategy' is used to combine results for a series of types. It must
    be ANY_STRATEGY or ALL_STRATEGY.

    Note: This visitor keeps an internal state (tracks type aliases to avoid
    recursion), so it should *never* be re-used for querying different types
    unless you call reset() first.
    """

    @others
</t>
<t tx="ekr.20230831011821.382">def __init__(self, strategy: int) -&gt; None:
    self.strategy = strategy
    if strategy == ANY_STRATEGY:
        self.default = False
    else:
        assert strategy == ALL_STRATEGY
        self.default = True
    # Keep track of the type aliases already visited. This is needed to avoid
    # infinite recursion on types like A = Union[int, List[A]]. An empty set is
    # represented as None as a micro-optimization.
    self.seen_aliases: set[TypeAliasType] | None = None
    # By default, we eagerly expand type aliases, and query also types in the
    # alias target. In most cases this is a desired behavior, but we may want
    # to skip targets in some cases (e.g. when collecting type variables).
    self.skip_alias_target = False

</t>
<t tx="ekr.20230831011821.383">def reset(self) -&gt; None:
    """Clear mutable state (but preserve strategy).

    This *must* be called if you want to reuse the visitor.
    """
    self.seen_aliases = None

</t>
<t tx="ekr.20230831011821.384">def visit_unbound_type(self, t: UnboundType) -&gt; bool:
    return self.query_types(t.args)

</t>
<t tx="ekr.20230831011821.385">def visit_type_list(self, t: TypeList) -&gt; bool:
    return self.query_types(t.items)

</t>
<t tx="ekr.20230831011821.386">def visit_callable_argument(self, t: CallableArgument) -&gt; bool:
    return t.typ.accept(self)

</t>
<t tx="ekr.20230831011821.387">def visit_any(self, t: AnyType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.388">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.389">def visit_none_type(self, t: NoneType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.39">def visit_set_expr(self, o: SetExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20230831011821.390">def visit_erased_type(self, t: ErasedType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.391">def visit_deleted_type(self, t: DeletedType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.392">def visit_type_var(self, t: TypeVarType) -&gt; bool:
    return self.query_types([t.upper_bound, t.default] + t.values)

</t>
<t tx="ekr.20230831011821.393">def visit_param_spec(self, t: ParamSpecType) -&gt; bool:
    return self.query_types([t.upper_bound, t.default])

</t>
<t tx="ekr.20230831011821.394">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; bool:
    return self.query_types([t.upper_bound, t.default])

</t>
<t tx="ekr.20230831011821.395">def visit_unpack_type(self, t: UnpackType) -&gt; bool:
    return self.query_types([t.type])

</t>
<t tx="ekr.20230831011821.396">def visit_parameters(self, t: Parameters) -&gt; bool:
    return self.query_types(t.arg_types)

</t>
<t tx="ekr.20230831011821.397">def visit_partial_type(self, t: PartialType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.398">def visit_instance(self, t: Instance) -&gt; bool:
    return self.query_types(t.args)

</t>
<t tx="ekr.20230831011821.399">def visit_callable_type(self, t: CallableType) -&gt; bool:
    # FIX generics
    # Avoid allocating any objects here as an optimization.
    args = self.query_types(t.arg_types)
    ret = t.ret_type.accept(self)
    if self.strategy == ANY_STRATEGY:
        return args or ret
    else:
        return args and ret

</t>
<t tx="ekr.20230831011821.4"># Visit methods

def visit_mypy_file(self, o: MypyFile) -&gt; None:
    for d in o.defs:
        d.accept(self)

</t>
<t tx="ekr.20230831011821.40">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    o.base.accept(self)
    o.index.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20230831011821.400">def visit_tuple_type(self, t: TupleType) -&gt; bool:
    return self.query_types(t.items)

</t>
<t tx="ekr.20230831011821.401">def visit_typeddict_type(self, t: TypedDictType) -&gt; bool:
    return self.query_types(list(t.items.values()))

</t>
<t tx="ekr.20230831011821.402">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.403">def visit_literal_type(self, t: LiteralType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.404">def visit_union_type(self, t: UnionType) -&gt; bool:
    return self.query_types(t.items)

</t>
<t tx="ekr.20230831011821.405">def visit_overloaded(self, t: Overloaded) -&gt; bool:
    return self.query_types(t.items)  # type: ignore[arg-type]

</t>
<t tx="ekr.20230831011821.406">def visit_type_type(self, t: TypeType) -&gt; bool:
    return t.item.accept(self)

</t>
<t tx="ekr.20230831011821.407">def visit_ellipsis_type(self, t: EllipsisType) -&gt; bool:
    return self.default

</t>
<t tx="ekr.20230831011821.408">def visit_placeholder_type(self, t: PlaceholderType) -&gt; bool:
    return self.query_types(t.args)

</t>
<t tx="ekr.20230831011821.409">def visit_type_alias_type(self, t: TypeAliasType) -&gt; bool:
    # Skip type aliases already visited types to avoid infinite recursion.
    # TODO: Ideally we should fire subvisitors here (or use caching) if we care
    #       about duplicates.
    if self.seen_aliases is None:
        self.seen_aliases = set()
    elif t in self.seen_aliases:
        return self.default
    self.seen_aliases.add(t)
    if self.skip_alias_target:
        return self.query_types(t.args)
    return get_proper_type(t).accept(self)

</t>
<t tx="ekr.20230831011821.41">def visit_generator_expr(self, o: GeneratorExpr) -&gt; None:
    for index, sequence, conditions in zip(o.indices, o.sequences, o.condlists):
        sequence.accept(self)
        index.accept(self)
        for cond in conditions:
            cond.accept(self)
    o.left_expr.accept(self)

</t>
<t tx="ekr.20230831011821.410">def query_types(self, types: list[Type] | tuple[Type, ...]) -&gt; bool:
    """Perform a query for a sequence of types using the strategy to combine the results."""
    # Special-case for lists and tuples to allow mypyc to produce better code.
    if isinstance(types, list):
        if self.strategy == ANY_STRATEGY:
            return any(t.accept(self) for t in types)
        else:
            return all(t.accept(self) for t in types)
    else:
        if self.strategy == ANY_STRATEGY:
            return any(t.accept(self) for t in types)
        else:
            return all(t.accept(self) for t in types)
</t>
<t tx="ekr.20230831011821.411">@path mypy
"""Semantic analysis of types"""
&lt;&lt; typeanal.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.412">
from __future__ import annotations

import itertools
from contextlib import contextmanager
from typing import Callable, Final, Iterable, Iterator, List, Sequence, Tuple, TypeVar
from typing_extensions import Protocol

from mypy import errorcodes as codes, message_registry, nodes
from mypy.errorcodes import ErrorCode
from mypy.messages import MessageBuilder, format_type_bare, quote_type_string, wrong_type_arg_count
from mypy.nodes import (
    ARG_NAMED,
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    SYMBOL_FUNCBASE_TYPES,
    ArgKind,
    Context,
    Decorator,
    MypyFile,
    ParamSpecExpr,
    PlaceholderNode,
    SymbolTableNode,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    TypeVarLikeExpr,
    TypeVarTupleExpr,
    Var,
    check_arg_kinds,
    check_arg_names,
    get_nongen_builtins,
)
from mypy.options import UNPACK, Options
from mypy.plugin import AnalyzeTypeContext, Plugin, TypeAnalyzerPluginInterface
from mypy.semanal_shared import SemanticAnalyzerCoreInterface, paramspec_args, paramspec_kwargs
from mypy.tvar_scope import TypeVarLikeScope
from mypy.types import (
    ANNOTATED_TYPE_NAMES,
    ANY_STRATEGY,
    FINAL_TYPE_NAMES,
    LITERAL_TYPE_NAMES,
    NEVER_NAMES,
    TYPE_ALIAS_NAMES,
    AnyType,
    BoolTypeQuery,
    CallableArgument,
    CallableType,
    DeletedType,
    EllipsisType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecFlavor,
    ParamSpecType,
    PartialType,
    PlaceholderType,
    RawExpressionType,
    RequiredType,
    SyntheticTypeVisitor,
    TrivialSyntheticTypeTranslator,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeOfAny,
    TypeQuery,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    callable_with_ellipsis,
    flatten_nested_unions,
    get_proper_type,
    has_type_vars,
)
from mypy.types_utils import is_bad_type_type_item
from mypy.typetraverser import TypeTraverserVisitor
from mypy.typevars import fill_typevars

T = TypeVar("T")

type_constructors: Final = {
    "typing.Callable",
    "typing.Optional",
    "typing.Tuple",
    "typing.Type",
    "typing.Union",
    *LITERAL_TYPE_NAMES,
    *ANNOTATED_TYPE_NAMES,
}

ARG_KINDS_BY_CONSTRUCTOR: Final = {
    "mypy_extensions.Arg": ARG_POS,
    "mypy_extensions.DefaultArg": ARG_OPT,
    "mypy_extensions.NamedArg": ARG_NAMED,
    "mypy_extensions.DefaultNamedArg": ARG_NAMED_OPT,
    "mypy_extensions.VarArg": ARG_STAR,
    "mypy_extensions.KwArg": ARG_STAR2,
}

GENERIC_STUB_NOT_AT_RUNTIME_TYPES: Final = {
    "queue.Queue",
    "builtins._PathLike",
    "asyncio.futures.Future",
}

SELF_TYPE_NAMES: Final = {"typing.Self", "typing_extensions.Self"}


</t>
<t tx="ekr.20230831011821.413">def analyze_type_alias(
    type: Type,
    api: SemanticAnalyzerCoreInterface,
    tvar_scope: TypeVarLikeScope,
    plugin: Plugin,
    options: Options,
    is_typeshed_stub: bool,
    allow_placeholder: bool = False,
    in_dynamic_func: bool = False,
    global_scope: bool = True,
    allowed_alias_tvars: list[TypeVarLikeType] | None = None,
) -&gt; tuple[Type, set[str]]:
    """Analyze r.h.s. of a (potential) type alias definition.

    If `node` is valid as a type alias rvalue, return the resulting type and a set of
    full names of type aliases it depends on (directly or indirectly).
    'node' must have been semantically analyzed.
    """
    analyzer = TypeAnalyser(
        api,
        tvar_scope,
        plugin,
        options,
        is_typeshed_stub,
        defining_alias=True,
        allow_placeholder=allow_placeholder,
        prohibit_self_type="type alias target",
        allowed_alias_tvars=allowed_alias_tvars,
    )
    analyzer.in_dynamic_func = in_dynamic_func
    analyzer.global_scope = global_scope
    res = type.accept(analyzer)
    return res, analyzer.aliases_used


</t>
<t tx="ekr.20230831011821.414">def no_subscript_builtin_alias(name: str, propose_alt: bool = True) -&gt; str:
    class_name = name.split(".")[-1]
    msg = f'"{class_name}" is not subscriptable'
    # This should never be called if the python_version is 3.9 or newer
    nongen_builtins = get_nongen_builtins((3, 8))
    replacement = nongen_builtins[name]
    if replacement and propose_alt:
        msg += f', use "{replacement}" instead'
    return msg


</t>
<t tx="ekr.20230831011821.415">class TypeAnalyser(SyntheticTypeVisitor[Type], TypeAnalyzerPluginInterface):
    """Semantic analyzer for types.

    Converts unbound types into bound types. This is a no-op for already
    bound types.

    If an incomplete reference is encountered, this does a defer. The
    caller never needs to defer.
    """

    @others
</t>
<t tx="ekr.20230831011821.416"># Is this called from an untyped function definition?
in_dynamic_func: bool = False
# Is this called from global scope?
global_scope: bool = True

def __init__(
    self,
    api: SemanticAnalyzerCoreInterface,
    tvar_scope: TypeVarLikeScope,
    plugin: Plugin,
    options: Options,
    is_typeshed_stub: bool,
    *,
    defining_alias: bool = False,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_placeholder: bool = False,
    allow_required: bool = False,
    allow_param_spec_literals: bool = False,
    allow_unpack: bool = False,
    report_invalid_types: bool = True,
    prohibit_self_type: str | None = None,
    allowed_alias_tvars: list[TypeVarLikeType] | None = None,
    allow_type_any: bool = False,
) -&gt; None:
    self.api = api
    self.fail_func = api.fail
    self.note_func = api.note
    self.tvar_scope = tvar_scope
    # Are we analysing a type alias definition rvalue?
    self.defining_alias = defining_alias
    self.allow_tuple_literal = allow_tuple_literal
    # Positive if we are analyzing arguments of another (outer) type
    self.nesting_level = 0
    # Should we allow new type syntax when targeting older Python versions
    # like 'list[int]' or 'X | Y' (allowed in stubs and with `__future__` import)?
    self.always_allow_new_syntax = self.api.is_stub_file or self.api.is_future_flag_set(
        "annotations"
    )
    # Should we accept unbound type variables? This is currently used for class bases,
    # and alias right hand sides (before they are analyzed as type aliases).
    self.allow_unbound_tvars = allow_unbound_tvars
    if allowed_alias_tvars is None:
        allowed_alias_tvars = []
    self.allowed_alias_tvars = allowed_alias_tvars
    # If false, record incomplete ref if we generate PlaceholderType.
    self.allow_placeholder = allow_placeholder
    # Are we in a context where Required[] is allowed?
    self.allow_required = allow_required
    # Are we in a context where ParamSpec literals are allowed?
    self.allow_param_spec_literals = allow_param_spec_literals
    # Are we in context where literal "..." specifically is allowed?
    self.allow_ellipsis = False
    # Should we report an error whenever we encounter a RawExpressionType outside
    # of a Literal context: e.g. whenever we encounter an invalid type? Normally,
    # we want to report an error, but the caller may want to do more specialized
    # error handling.
    self.report_invalid_types = report_invalid_types
    self.plugin = plugin
    self.options = options
    self.is_typeshed_stub = is_typeshed_stub
    # Names of type aliases encountered while analysing a type will be collected here.
    self.aliases_used: set[str] = set()
    self.prohibit_self_type = prohibit_self_type
    # Allow variables typed as Type[Any] and type (useful for base classes).
    self.allow_type_any = allow_type_any
    self.allow_type_var_tuple = False
    self.allow_unpack = allow_unpack

</t>
<t tx="ekr.20230831011821.417">def lookup_qualified(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    return self.api.lookup_qualified(name, ctx, suppress_errors)

</t>
<t tx="ekr.20230831011821.418">def lookup_fully_qualified(self, name: str) -&gt; SymbolTableNode:
    return self.api.lookup_fully_qualified(name)

</t>
<t tx="ekr.20230831011821.419">def visit_unbound_type(self, t: UnboundType, defining_literal: bool = False) -&gt; Type:
    typ = self.visit_unbound_type_nonoptional(t, defining_literal)
    if t.optional:
        # We don't need to worry about double-wrapping Optionals or
        # wrapping Anys: Union simplification will take care of that.
        return make_optional_type(typ)
    return typ

</t>
<t tx="ekr.20230831011821.42">def visit_dictionary_comprehension(self, o: DictionaryComprehension) -&gt; None:
    for index, sequence, conditions in zip(o.indices, o.sequences, o.condlists):
        sequence.accept(self)
        index.accept(self)
        for cond in conditions:
            cond.accept(self)
    o.key.accept(self)
    o.value.accept(self)

</t>
<t tx="ekr.20230831011821.420">def visit_unbound_type_nonoptional(self, t: UnboundType, defining_literal: bool) -&gt; Type:
    sym = self.lookup_qualified(t.name, t)
    if sym is not None:
        node = sym.node
        if isinstance(node, PlaceholderNode):
            if node.becomes_typeinfo:
                # Reference to placeholder type.
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                elif self.allow_placeholder:
                    self.api.defer()
                else:
                    self.api.record_incomplete_ref()
                # Always allow ParamSpec for placeholders, if they are actually not valid,
                # they will be reported later, after we resolve placeholders.
                return PlaceholderType(
                    node.fullname,
                    self.anal_array(
                        t.args,
                        allow_param_spec=True,
                        allow_param_spec_literals=True,
                        allow_unpack=True,
                    ),
                    t.line,
                )
            else:
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                else:
                    # Reference to an unknown placeholder node.
                    self.api.record_incomplete_ref()
                    return AnyType(TypeOfAny.special_form)
        if node is None:
            self.fail(f"Internal error (node is None, kind={sym.kind})", t)
            return AnyType(TypeOfAny.special_form)
        fullname = node.fullname
        hook = self.plugin.get_type_analyze_hook(fullname)
        if hook is not None:
            return hook(AnalyzeTypeContext(t, t, self))
        if (
            fullname in get_nongen_builtins(self.options.python_version)
            and t.args
            and not self.always_allow_new_syntax
        ):
            self.fail(
                no_subscript_builtin_alias(fullname, propose_alt=not self.defining_alias), t
            )
        tvar_def = self.tvar_scope.get_binding(sym)
        if isinstance(sym.node, ParamSpecExpr):
            if tvar_def is None:
                if self.allow_unbound_tvars:
                    return t
                self.fail(f'ParamSpec "{t.name}" is unbound', t, code=codes.VALID_TYPE)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, ParamSpecType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'ParamSpec "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return ParamSpecType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.flavor,
                tvar_def.upper_bound,
                tvar_def.default,
                line=t.line,
                column=t.column,
            )
        if (
            isinstance(sym.node, TypeVarExpr)
            and self.defining_alias
            and not defining_literal
            and (tvar_def is None or tvar_def not in self.allowed_alias_tvars)
        ):
            self.fail(
                f'Can\'t use bound type variable "{t.name}" to define generic alias',
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarExpr) and tvar_def is not None:
            assert isinstance(tvar_def, TypeVarType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'Type variable "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return tvar_def.copy_modified(line=t.line, column=t.column)
        if isinstance(sym.node, TypeVarTupleExpr) and (
            tvar_def is not None
            and self.defining_alias
            and tvar_def not in self.allowed_alias_tvars
        ):
            self.fail(
                f'Can\'t use bound type variable "{t.name}" to define generic alias',
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarTupleExpr):
            if tvar_def is None:
                if self.allow_unbound_tvars:
                    return t
                self.fail(f'TypeVarTuple "{t.name}" is unbound', t, code=codes.VALID_TYPE)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, TypeVarTupleType)
            if not self.allow_type_var_tuple:
                self.fail(
                    f'TypeVarTuple "{t.name}" is only valid with an unpack',
                    t,
                    code=codes.VALID_TYPE,
                )
                return AnyType(TypeOfAny.from_error)
            if len(t.args) &gt; 0:
                self.fail(
                    f'Type variable "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )

            # Change the line number
            return TypeVarTupleType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.upper_bound,
                sym.node.tuple_fallback,
                tvar_def.default,
                line=t.line,
                column=t.column,
            )
        special = self.try_analyze_special_unbound_type(t, fullname)
        if special is not None:
            return special
        if isinstance(node, TypeAlias):
            self.aliases_used.add(fullname)
            an_args = self.anal_array(
                t.args,
                allow_param_spec=True,
                allow_param_spec_literals=node.has_param_spec_type,
                allow_unpack=node.tvar_tuple_index is not None,
            )
            if node.has_param_spec_type and len(node.alias_tvars) == 1:
                an_args = self.pack_paramspec_args(an_args)

            disallow_any = self.options.disallow_any_generics and not self.is_typeshed_stub
            res = instantiate_type_alias(
                node,
                an_args,
                self.fail,
                node.no_args,
                t,
                self.options,
                unexpanded_type=t,
                disallow_any=disallow_any,
                empty_tuple_index=t.empty_tuple_index,
            )
            # The only case where instantiate_type_alias() can return an incorrect instance is
            # when it is top-level instance, so no need to recurse.
            if (
                isinstance(res, Instance)  # type: ignore[misc]
                and len(res.args) != len(res.type.type_vars)
                and not self.defining_alias
                and not res.type.has_type_var_tuple_type
            ):
                fix_instance(
                    res,
                    self.fail,
                    self.note,
                    disallow_any=disallow_any,
                    options=self.options,
                    use_generic_error=True,
                    unexpanded_type=t,
                )
            if node.eager:
                res = get_proper_type(res)
            return res
        elif isinstance(node, TypeInfo):
            return self.analyze_type_with_type_info(node, t.args, t)
        elif node.fullname in TYPE_ALIAS_NAMES:
            return AnyType(TypeOfAny.special_form)
        # Concatenate is an operator, no need for a proper type
        elif node.fullname in ("typing_extensions.Concatenate", "typing.Concatenate"):
            # We check the return type further up the stack for valid use locations
            return self.apply_concatenate_operator(t)
        else:
            return self.analyze_unbound_type_without_type_info(t, sym, defining_literal)
    else:  # sym is None
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20230831011821.421">def pack_paramspec_args(self, an_args: Sequence[Type]) -&gt; list[Type]:
    # "Aesthetic" ParamSpec literals for single ParamSpec: C[int, str] -&gt; C[[int, str]].
    # These do not support mypy_extensions VarArgs, etc. as they were already analyzed
    # TODO: should these be re-analyzed to get rid of this inconsistency?
    count = len(an_args)
    if count &gt; 0:
        first_arg = get_proper_type(an_args[0])
        if not (count == 1 and isinstance(first_arg, (Parameters, ParamSpecType, AnyType))):
            return [Parameters(an_args, [ARG_POS] * count, [None] * count)]
    return list(an_args)

</t>
<t tx="ekr.20230831011821.422">def cannot_resolve_type(self, t: UnboundType) -&gt; None:
    # TODO: Move error message generation to messages.py. We'd first
    #       need access to MessageBuilder here. Also move the similar
    #       message generation logic in semanal.py.
    self.api.fail(f'Cannot resolve name "{t.name}" (possible cyclic definition)', t)
    if not self.options.disable_recursive_aliases and self.api.is_func_scope():
        self.note("Recursive types are not allowed at function scope", t)

</t>
<t tx="ekr.20230831011821.423">def apply_concatenate_operator(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.api.fail("Concatenate needs type arguments", t, code=codes.VALID_TYPE)
        return AnyType(TypeOfAny.from_error)

    # Last argument has to be ParamSpec or Ellipsis.
    ps = self.anal_type(t.args[-1], allow_param_spec=True, allow_ellipsis=True)
    if not isinstance(ps, (ParamSpecType, Parameters)):
        if isinstance(ps, UnboundType) and self.allow_unbound_tvars:
            sym = self.lookup_qualified(ps.name, t)
            if sym is not None and isinstance(sym.node, ParamSpecExpr):
                return ps
        self.api.fail(
            "The last parameter to Concatenate needs to be a ParamSpec",
            t,
            code=codes.VALID_TYPE,
        )
        return AnyType(TypeOfAny.from_error)

    # TODO: this may not work well with aliases, if those worked.
    #   Those should be special-cased.
    elif isinstance(ps, ParamSpecType) and ps.prefix.arg_types:
        self.api.fail("Nested Concatenates are invalid", t, code=codes.VALID_TYPE)

    args = self.anal_array(t.args[:-1])
    pre = ps.prefix if isinstance(ps, ParamSpecType) else ps

    # mypy can't infer this :(
    names: list[str | None] = [None] * len(args)

    pre = Parameters(
        args + pre.arg_types, [ARG_POS] * len(args) + pre.arg_kinds, names + pre.arg_names
    )
    return ps.copy_modified(prefix=pre) if isinstance(ps, ParamSpecType) else pre

</t>
<t tx="ekr.20230831011821.424">def try_analyze_special_unbound_type(self, t: UnboundType, fullname: str) -&gt; Type | None:
    """Bind special type that is recognized through magic name such as 'typing.Any'.

    Return the bound type if successful, and return None if the type is a normal type.
    """
    if fullname == "builtins.None":
        return NoneType()
    elif fullname == "typing.Any" or fullname == "builtins.Any":
        return AnyType(TypeOfAny.explicit, line=t.line, column=t.column)
    elif fullname in FINAL_TYPE_NAMES:
        self.fail(
            "Final can be only used as an outermost qualifier in a variable annotation",
            t,
            code=codes.VALID_TYPE,
        )
        return AnyType(TypeOfAny.from_error)
    elif fullname == "typing.Tuple" or (
        fullname == "builtins.tuple"
        and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9))
    ):
        # Tuple is special because it is involved in builtin import cycle
        # and may be not ready when used.
        sym = self.api.lookup_fully_qualified_or_none("builtins.tuple")
        if not sym or isinstance(sym.node, PlaceholderNode):
            if self.api.is_incomplete_namespace("builtins"):
                self.api.record_incomplete_ref()
            else:
                self.fail('Name "tuple" is not defined', t)
            return AnyType(TypeOfAny.special_form)
        if len(t.args) == 0 and not t.empty_tuple_index:
            # Bare 'Tuple' is same as 'tuple'
            any_type = self.get_omitted_any(t)
            return self.named_type("builtins.tuple", [any_type], line=t.line, column=t.column)
        if len(t.args) == 2 and isinstance(t.args[1], EllipsisType):
            # Tuple[T, ...] (uniform, variable-length tuple)
            instance = self.named_type("builtins.tuple", [self.anal_type(t.args[0])])
            instance.line = t.line
            return instance
        return self.tuple_type(self.anal_array(t.args, allow_unpack=True))
    elif fullname == "typing.Union":
        items = self.anal_array(t.args)
        return UnionType.make_union(items)
    elif fullname == "typing.Optional":
        if len(t.args) != 1:
            self.fail(
                "Optional[...] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        item = self.anal_type(t.args[0])
        return make_optional_type(item)
    elif fullname == "typing.Callable":
        return self.analyze_callable_type(t)
    elif fullname == "typing.Type" or (
        fullname == "builtins.type"
        and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9))
    ):
        if len(t.args) == 0:
            if fullname == "typing.Type":
                any_type = self.get_omitted_any(t)
                return TypeType(any_type, line=t.line, column=t.column)
            else:
                # To prevent assignment of 'builtins.type' inferred as 'builtins.object'
                # See https://github.com/python/mypy/issues/9476 for more information
                return None
        if len(t.args) != 1:
            type_str = "Type[...]" if fullname == "typing.Type" else "type[...]"
            self.fail(
                type_str + " must have exactly one type argument", t, code=codes.VALID_TYPE
            )
        item = self.anal_type(t.args[0])
        if is_bad_type_type_item(item):
            self.fail("Type[...] can't contain another Type[...]", t, code=codes.VALID_TYPE)
            item = AnyType(TypeOfAny.from_error)
        return TypeType.make_normalized(item, line=t.line, column=t.column)
    elif fullname == "typing.ClassVar":
        if self.nesting_level &gt; 0:
            self.fail(
                "Invalid type: ClassVar nested inside other type", t, code=codes.VALID_TYPE
            )
        if len(t.args) == 0:
            return AnyType(TypeOfAny.from_omitted_generics, line=t.line, column=t.column)
        if len(t.args) != 1:
            self.fail(
                "ClassVar[...] must have at most one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in NEVER_NAMES:
        return UninhabitedType(is_noreturn=True)
    elif fullname in LITERAL_TYPE_NAMES:
        return self.analyze_literal_type(t)
    elif fullname in ANNOTATED_TYPE_NAMES:
        if len(t.args) &lt; 2:
            self.fail(
                "Annotated[...] must have exactly one type argument"
                " and at least one annotation",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in ("typing_extensions.Required", "typing.Required"):
        if not self.allow_required:
            self.fail(
                "Required[] can be only used in a TypedDict definition",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail(
                "Required[] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=True)
    elif fullname in ("typing_extensions.NotRequired", "typing.NotRequired"):
        if not self.allow_required:
            self.fail(
                "NotRequired[] can be only used in a TypedDict definition",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail(
                "NotRequired[] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=False)
    elif self.anal_type_guard_arg(t, fullname) is not None:
        # In most contexts, TypeGuard[...] acts as an alias for bool (ignoring its args)
        return self.named_type("builtins.bool")
    elif fullname in ("typing.Unpack", "typing_extensions.Unpack"):
        if not self.api.incomplete_feature_enabled(UNPACK, t):
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail("Unpack[...] requires exactly one type argument", t)
            return AnyType(TypeOfAny.from_error)
        if not self.allow_unpack:
            self.fail(message_registry.INVALID_UNPACK_POSITION, t, code=codes.VALID_TYPE)
            return AnyType(TypeOfAny.from_error)
        self.allow_type_var_tuple = True
        result = UnpackType(self.anal_type(t.args[0]), line=t.line, column=t.column)
        self.allow_type_var_tuple = False
        return result
    elif fullname in SELF_TYPE_NAMES:
        if t.args:
            self.fail("Self type cannot have type arguments", t)
        if self.prohibit_self_type is not None:
            self.fail(f"Self type cannot be used in {self.prohibit_self_type}", t)
            return AnyType(TypeOfAny.from_error)
        if self.api.type is None:
            self.fail("Self type is only allowed in annotations within class definition", t)
            return AnyType(TypeOfAny.from_error)
        if self.api.type.has_base("builtins.type"):
            self.fail("Self type cannot be used in a metaclass", t)
        if self.api.type.self_type is not None:
            if self.api.type.is_final:
                return fill_typevars(self.api.type)
            return self.api.type.self_type.copy_modified(line=t.line, column=t.column)
        # TODO: verify this is unreachable and replace with an assert?
        self.fail("Unexpected Self type", t)
        return AnyType(TypeOfAny.from_error)
    return None

</t>
<t tx="ekr.20230831011821.425">def get_omitted_any(self, typ: Type, fullname: str | None = None) -&gt; AnyType:
    disallow_any = not self.is_typeshed_stub and self.options.disallow_any_generics
    return get_omitted_any(disallow_any, self.fail, self.note, typ, self.options, fullname)

</t>
<t tx="ekr.20230831011821.426">def analyze_type_with_type_info(
    self, info: TypeInfo, args: Sequence[Type], ctx: Context
) -&gt; Type:
    """Bind unbound type when were able to find target TypeInfo.

    This handles simple cases like 'int', 'modname.UserClass[str]', etc.
    """

    if len(args) &gt; 0 and info.fullname == "builtins.tuple":
        fallback = Instance(info, [AnyType(TypeOfAny.special_form)], ctx.line)
        return TupleType(self.anal_array(args, allow_unpack=True), fallback, ctx.line)

    # Analyze arguments and (usually) construct Instance type. The
    # number of type arguments and their values are
    # checked only later, since we do not always know the
    # valid count at this point. Thus we may construct an
    # Instance with an invalid number of type arguments.
    #
    # We allow ParamSpec literals based on a heuristic: it will be
    # checked later anyways but the error message may be worse.
    instance = Instance(
        info,
        self.anal_array(
            args,
            allow_param_spec=True,
            allow_param_spec_literals=info.has_param_spec_type,
            allow_unpack=info.has_type_var_tuple_type,
        ),
        ctx.line,
        ctx.column,
    )
    if len(info.type_vars) == 1 and info.has_param_spec_type:
        instance.args = tuple(self.pack_paramspec_args(instance.args))

    if info.has_type_var_tuple_type:
        if instance.args:
            # -1 to account for empty tuple
            valid_arg_length = len(instance.args) &gt;= len(info.type_vars) - 1
        # Empty case is special cased and we want to infer a Tuple[Any, ...]
        # instead of the empty tuple, so no - 1 here.
        else:
            valid_arg_length = False
    else:
        valid_arg_length = len(instance.args) == len(info.type_vars)

    # Check type argument count.
    if not valid_arg_length and not self.defining_alias:
        fix_instance(
            instance,
            self.fail,
            self.note,
            disallow_any=self.options.disallow_any_generics and not self.is_typeshed_stub,
            options=self.options,
        )

    tup = info.tuple_type
    if tup is not None:
        # The class has a Tuple[...] base class so it will be
        # represented as a tuple type.
        if info.special_alias:
            return instantiate_type_alias(
                info.special_alias,
                # TODO: should we allow NamedTuples generic in ParamSpec and TypeVarTuple?
                self.anal_array(args),
                self.fail,
                False,
                ctx,
                self.options,
                use_standard_error=True,
            )
        return tup.copy_modified(
            items=self.anal_array(tup.items, allow_unpack=True), fallback=instance
        )
    td = info.typeddict_type
    if td is not None:
        # The class has a TypedDict[...] base class so it will be
        # represented as a typeddict type.
        if info.special_alias:
            return instantiate_type_alias(
                info.special_alias,
                # TODO: should we allow TypedDicts generic in ParamSpec?
                self.anal_array(args),
                self.fail,
                False,
                ctx,
                self.options,
                use_standard_error=True,
            )
        # Create a named TypedDictType
        return td.copy_modified(
            item_types=self.anal_array(list(td.items.values())), fallback=instance
        )

    if info.fullname == "types.NoneType":
        self.fail(
            "NoneType should not be used as a type, please use None instead",
            ctx,
            code=codes.VALID_TYPE,
        )
        return NoneType(ctx.line, ctx.column)

    return instance

</t>
<t tx="ekr.20230831011821.427">def analyze_unbound_type_without_type_info(
    self, t: UnboundType, sym: SymbolTableNode, defining_literal: bool
) -&gt; Type:
    """Figure out what an unbound type that doesn't refer to a TypeInfo node means.

    This is something unusual. We try our best to find out what it is.
    """
    name = sym.fullname
    if name is None:
        assert sym.node is not None
        name = sym.node.name
    # Option 1:
    # Something with an Any type -- make it an alias for Any in a type
    # context. This is slightly problematic as it allows using the type 'Any'
    # as a base class -- however, this will fail soon at runtime so the problem
    # is pretty minor.
    if isinstance(sym.node, Var):
        typ = get_proper_type(sym.node.type)
        if isinstance(typ, AnyType):
            return AnyType(
                TypeOfAny.from_unimported_type, missing_import_name=typ.missing_import_name
            )
        elif self.allow_type_any:
            if isinstance(typ, Instance) and typ.type.fullname == "builtins.type":
                return AnyType(TypeOfAny.special_form)
            if isinstance(typ, TypeType) and isinstance(typ.item, AnyType):
                return AnyType(TypeOfAny.from_another_any, source_any=typ.item)
    # Option 2:
    # Unbound type variable. Currently these may be still valid,
    # for example when defining a generic type alias.
    unbound_tvar = (
        isinstance(sym.node, (TypeVarExpr, TypeVarTupleExpr))
        and self.tvar_scope.get_binding(sym) is None
    )
    if self.allow_unbound_tvars and unbound_tvar:
        return t

    # Option 3:
    # Enum value. Note: we only want to return a LiteralType when
    # we're using this enum value specifically within context of
    # a "Literal[...]" type. So, if `defining_literal` is not set,
    # we bail out early with an error.
    #
    # If, in the distant future, we decide to permit things like
    # `def foo(x: Color.RED) -&gt; None: ...`, we can remove that
    # check entirely.
    if isinstance(sym.node, Var) and sym.node.info and sym.node.info.is_enum:
        value = sym.node.name
        base_enum_short_name = sym.node.info.name
        if not defining_literal:
            msg = message_registry.INVALID_TYPE_RAW_ENUM_VALUE.format(
                base_enum_short_name, value
            )
            self.fail(msg.value, t, code=msg.code)
            return AnyType(TypeOfAny.from_error)
        return LiteralType(
            value=value,
            fallback=Instance(sym.node.info, [], line=t.line, column=t.column),
            line=t.line,
            column=t.column,
        )

    # None of the above options worked. We parse the args (if there are any)
    # to make sure there are no remaining semanal-only types, then give up.
    t = t.copy_modified(args=self.anal_array(t.args))
    # TODO: Move this message building logic to messages.py.
    notes: list[str] = []
    if isinstance(sym.node, Var):
        notes.append(
            "See https://mypy.readthedocs.io/en/"
            "stable/common_issues.html#variables-vs-type-aliases"
        )
        message = 'Variable "{}" is not valid as a type'
    elif isinstance(sym.node, (SYMBOL_FUNCBASE_TYPES, Decorator)):
        message = 'Function "{}" is not valid as a type'
        if name == "builtins.any":
            notes.append('Perhaps you meant "typing.Any" instead of "any"?')
        elif name == "builtins.callable":
            notes.append('Perhaps you meant "typing.Callable" instead of "callable"?')
        else:
            notes.append('Perhaps you need "Callable[...]" or a callback protocol?')
    elif isinstance(sym.node, MypyFile):
        message = 'Module "{}" is not valid as a type'
        notes.append("Perhaps you meant to use a protocol matching the module structure?")
    elif unbound_tvar:
        message = 'Type variable "{}" is unbound'
        short = name.split(".")[-1]
        notes.append(
            (
                '(Hint: Use "Generic[{}]" or "Protocol[{}]" base class'
                ' to bind "{}" inside a class)'
            ).format(short, short, short)
        )
        notes.append(
            '(Hint: Use "{}" in function signature to bind "{}"'
            " inside a function)".format(short, short)
        )
    else:
        message = 'Cannot interpret reference "{}" as a type'
    if not defining_literal:
        # Literal check already gives a custom error. Avoid duplicating errors.
        self.fail(message.format(name), t, code=codes.VALID_TYPE)
        for note in notes:
            self.note(note, t, code=codes.VALID_TYPE)

    # TODO: Would it be better to always return Any instead of UnboundType
    # in case of an error? On one hand, UnboundType has a name so error messages
    # are more detailed, on the other hand, some of them may be bogus,
    # see https://github.com/python/mypy/issues/4987.
    return t

</t>
<t tx="ekr.20230831011821.428">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.429">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.43">def visit_list_comprehension(self, o: ListComprehension) -&gt; None:
    o.generator.accept(self)

</t>
<t tx="ekr.20230831011821.430">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.431">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    # This type should exist only temporarily during type inference
    assert False, "Internal error: Unexpected erased type"

</t>
<t tx="ekr.20230831011821.432">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.433">def visit_type_list(self, t: TypeList) -&gt; Type:
    # Parameters literal (Z[[int, str, Whatever]])
    if self.allow_param_spec_literals:
        params = self.analyze_callable_args(t)
        if params:
            ts, kinds, names = params
            # bind these types
            return Parameters(self.anal_array(ts), kinds, names)
        else:
            return AnyType(TypeOfAny.from_error)
    else:
        self.fail(
            'Bracketed expression "[...]" is not valid as a type', t, code=codes.VALID_TYPE
        )
        if len(t.items) == 1:
            self.note('Did you mean "List[...]"?', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011821.434">def visit_callable_argument(self, t: CallableArgument) -&gt; Type:
    self.fail("Invalid type", t, code=codes.VALID_TYPE)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011821.435">def visit_instance(self, t: Instance) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.436">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # TODO: should we do something here?
    return t

</t>
<t tx="ekr.20230831011821.437">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.438">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.439">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.44">def visit_set_comprehension(self, o: SetComprehension) -&gt; None:
    o.generator.accept(self)

</t>
<t tx="ekr.20230831011821.440">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011821.441">def visit_parameters(self, t: Parameters) -&gt; Type:
    raise NotImplementedError("ParamSpec literals cannot have unbound TypeVars")

</t>
<t tx="ekr.20230831011821.442">def visit_callable_type(self, t: CallableType, nested: bool = True) -&gt; Type:
    # Every Callable can bind its own type variables, if they're not in the outer scope
    with self.tvar_scope_frame():
        if self.defining_alias:
            variables = t.variables
        else:
            variables, _ = self.bind_function_type_variables(t, t)
        special = self.anal_type_guard(t.ret_type)
        arg_kinds = t.arg_kinds
        if len(arg_kinds) &gt;= 2 and arg_kinds[-2] == ARG_STAR and arg_kinds[-1] == ARG_STAR2:
            arg_types = self.anal_array(t.arg_types[:-2], nested=nested) + [
                self.anal_star_arg_type(t.arg_types[-2], ARG_STAR, nested=nested),
                self.anal_star_arg_type(t.arg_types[-1], ARG_STAR2, nested=nested),
            ]
        else:
            arg_types = self.anal_array(t.arg_types, nested=nested, allow_unpack=True)
            star_index = None
            if ARG_STAR in arg_kinds:
                star_index = arg_kinds.index(ARG_STAR)
            star2_index = None
            if ARG_STAR2 in arg_kinds:
                star2_index = arg_kinds.index(ARG_STAR2)
            validated_args: list[Type] = []
            for i, at in enumerate(arg_types):
                if isinstance(at, UnpackType) and i not in (star_index, star2_index):
                    self.fail(
                        message_registry.INVALID_UNPACK_POSITION, at, code=codes.VALID_TYPE
                    )
                    validated_args.append(AnyType(TypeOfAny.from_error))
                else:
                    validated_args.append(at)
            arg_types = validated_args
        # If there were multiple (invalid) unpacks, the arg types list will become shorter,
        # we need to trim the kinds/names as well to avoid crashes.
        arg_kinds = t.arg_kinds[: len(arg_types)]
        arg_names = t.arg_names[: len(arg_types)]

        ret = t.copy_modified(
            arg_types=arg_types,
            arg_kinds=arg_kinds,
            arg_names=arg_names,
            ret_type=self.anal_type(t.ret_type, nested=nested),
            # If the fallback isn't filled in yet,
            # its type will be the falsey FakeInfo
            fallback=(t.fallback if t.fallback.type else self.named_type("builtins.function")),
            variables=self.anal_var_defs(variables),
            type_guard=special,
        )
    return ret

</t>
<t tx="ekr.20230831011821.443">def anal_type_guard(self, t: Type) -&gt; Type | None:
    if isinstance(t, UnboundType):
        sym = self.lookup_qualified(t.name, t)
        if sym is not None and sym.node is not None:
            return self.anal_type_guard_arg(t, sym.node.fullname)
    # TODO: What if it's an Instance? Then use t.type.fullname?
    return None

</t>
<t tx="ekr.20230831011821.444">def anal_type_guard_arg(self, t: UnboundType, fullname: str) -&gt; Type | None:
    if fullname in ("typing_extensions.TypeGuard", "typing.TypeGuard"):
        if len(t.args) != 1:
            self.fail(
                "TypeGuard must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    return None

</t>
<t tx="ekr.20230831011821.445">def anal_star_arg_type(self, t: Type, kind: ArgKind, nested: bool) -&gt; Type:
    """Analyze signature argument type for *args and **kwargs argument."""
    if isinstance(t, UnboundType) and t.name and "." in t.name and not t.args:
        components = t.name.split(".")
        tvar_name = ".".join(components[:-1])
        sym = self.lookup_qualified(tvar_name, t)
        if sym is not None and isinstance(sym.node, ParamSpecExpr):
            tvar_def = self.tvar_scope.get_binding(sym)
            if isinstance(tvar_def, ParamSpecType):
                if kind == ARG_STAR:
                    make_paramspec = paramspec_args
                    if components[-1] != "args":
                        self.fail(
                            f'Use "{tvar_name}.args" for variadic "*" parameter',
                            t,
                            code=codes.VALID_TYPE,
                        )
                elif kind == ARG_STAR2:
                    make_paramspec = paramspec_kwargs
                    if components[-1] != "kwargs":
                        self.fail(
                            f'Use "{tvar_name}.kwargs" for variadic "**" parameter',
                            t,
                            code=codes.VALID_TYPE,
                        )
                else:
                    assert False, kind
                return make_paramspec(
                    tvar_def.name,
                    tvar_def.fullname,
                    tvar_def.id,
                    named_type_func=self.named_type,
                    line=t.line,
                    column=t.column,
                )
    return self.anal_type(t, nested=nested, allow_unpack=True)

</t>
<t tx="ekr.20230831011821.446">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    # Overloaded types are manually constructed in semanal.py by analyzing the
    # AST and combining together the Callable types this visitor converts.
    #
    # So if we're ever asked to reanalyze an Overloaded type, we know it's
    # fine to just return it as-is.
    return t

</t>
<t tx="ekr.20230831011821.447">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    # Types such as (t1, t2, ...) only allowed in assignment statements. They'll
    # generate errors elsewhere, and Tuple[t1, t2, ...] must be used instead.
    if t.implicit and not self.allow_tuple_literal:
        self.fail("Syntax error in type annotation", t, code=codes.SYNTAX)
        if len(t.items) == 0:
            self.note(
                "Suggestion: Use Tuple[()] instead of () for an empty tuple, or "
                "None for a function without a return value",
                t,
                code=codes.SYNTAX,
            )
        elif len(t.items) == 1:
            self.note("Suggestion: Is there a spurious trailing comma?", t, code=codes.SYNTAX)
        else:
            self.note(
                "Suggestion: Use Tuple[T1, ..., Tn] instead of (T1, ..., Tn)",
                t,
                code=codes.SYNTAX,
            )
        return AnyType(TypeOfAny.from_error)

    any_type = AnyType(TypeOfAny.special_form)
    # If the fallback isn't filled in yet, its type will be the falsey FakeInfo
    fallback = (
        t.partial_fallback
        if t.partial_fallback.type
        else self.named_type("builtins.tuple", [any_type])
    )
    return TupleType(self.anal_array(t.items, allow_unpack=True), fallback, t.line)

</t>
<t tx="ekr.20230831011821.448">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = {
        item_name: self.anal_type(item_type) for (item_name, item_type) in t.items.items()
    }
    return TypedDictType(items, set(t.required_keys), t.fallback)

</t>
<t tx="ekr.20230831011821.449">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; Type:
    # We should never see a bare Literal. We synthesize these raw literals
    # in the earlier stages of semantic analysis, but those
    # "fake literals" should always be wrapped in an UnboundType
    # corresponding to 'Literal'.
    #
    # Note: if at some point in the distant future, we decide to
    # make signatures like "foo(x: 20) -&gt; None" legal, we can change
    # this method so it generates and returns an actual LiteralType
    # instead.

    if self.report_invalid_types:
        if t.base_type_name in ("builtins.int", "builtins.bool"):
            # The only time it makes sense to use an int or bool is inside of
            # a literal type.
            msg = f"Invalid type: try using Literal[{repr(t.literal_value)}] instead?"
        elif t.base_type_name in ("builtins.float", "builtins.complex"):
            # We special-case warnings for floats and complex numbers.
            msg = f"Invalid type: {t.simple_name()} literals cannot be used as a type"
        else:
            # And in all other cases, we default to a generic error message.
            # Note: the reason why we use a generic error message for strings
            # but not ints or bools is because whenever we see an out-of-place
            # string, it's unclear if the user meant to construct a literal type
            # or just misspelled a regular type. So we avoid guessing.
            msg = "Invalid type comment or annotation"

        self.fail(msg, t, code=codes.VALID_TYPE)
        if t.note is not None:
            self.note(t.note, t, code=codes.VALID_TYPE)

    return AnyType(TypeOfAny.from_error, line=t.line, column=t.column)

</t>
<t tx="ekr.20230831011821.45">def visit_conditional_expr(self, o: ConditionalExpr) -&gt; None:
    o.cond.accept(self)
    o.if_expr.accept(self)
    o.else_expr.accept(self)

</t>
<t tx="ekr.20230831011821.450">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.451">def visit_union_type(self, t: UnionType) -&gt; Type:
    if (
        t.uses_pep604_syntax is True
        and t.is_evaluated is True
        and not self.always_allow_new_syntax
        and not self.options.python_version &gt;= (3, 10)
    ):
        self.fail("X | Y syntax for unions requires Python 3.10", t, code=codes.SYNTAX)
    return UnionType(self.anal_array(t.items), t.line)

</t>
<t tx="ekr.20230831011821.452">def visit_partial_type(self, t: PartialType) -&gt; Type:
    assert False, "Internal error: Unexpected partial type"

</t>
<t tx="ekr.20230831011821.453">def visit_ellipsis_type(self, t: EllipsisType) -&gt; Type:
    if self.allow_ellipsis or self.allow_param_spec_literals:
        any_type = AnyType(TypeOfAny.explicit)
        return Parameters(
            [any_type, any_type], [ARG_STAR, ARG_STAR2], [None, None], is_ellipsis_args=True
        )
    else:
        self.fail('Unexpected "..."', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20230831011821.454">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(self.anal_type(t.item), line=t.line)

</t>
<t tx="ekr.20230831011821.455">def visit_placeholder_type(self, t: PlaceholderType) -&gt; Type:
    n = (
        None
        # No dot in fullname indicates we are at function scope, and recursive
        # types are not supported there anyway, so we just give up.
        if not t.fullname or "." not in t.fullname
        else self.api.lookup_fully_qualified(t.fullname)
    )
    if not n or isinstance(n.node, PlaceholderNode):
        self.api.defer()  # Still incomplete
        return t
    else:
        # TODO: Handle non-TypeInfo
        assert isinstance(n.node, TypeInfo)
        return self.analyze_type_with_type_info(n.node, t.args, t)

</t>
<t tx="ekr.20230831011821.456">def analyze_callable_args_for_paramspec(
    self, callable_args: Type, ret_type: Type, fallback: Instance
) -&gt; CallableType | None:
    """Construct a 'Callable[P, RET]', where P is ParamSpec, return None if we cannot."""
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    tvar_def = self.tvar_scope.get_binding(sym)
    if not isinstance(tvar_def, ParamSpecType):
        if (
            tvar_def is None
            and self.allow_unbound_tvars
            and isinstance(sym.node, ParamSpecExpr)
        ):
            # We are analyzing this type in runtime context (e.g. as type application).
            # If it is not valid as a type in this position an error will be given later.
            return callable_with_ellipsis(
                AnyType(TypeOfAny.explicit), ret_type=ret_type, fallback=fallback
            )
        return None

    return CallableType(
        [
            paramspec_args(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
            paramspec_kwargs(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
        ],
        [nodes.ARG_STAR, nodes.ARG_STAR2],
        [None, None],
        ret_type=ret_type,
        fallback=fallback,
    )

</t>
<t tx="ekr.20230831011821.457">def analyze_callable_args_for_concatenate(
    self, callable_args: Type, ret_type: Type, fallback: Instance
) -&gt; CallableType | AnyType | None:
    """Construct a 'Callable[C, RET]', where C is Concatenate[..., P], returning None if we
    cannot.
    """
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    if sym.node is None:
        return None
    if sym.node.fullname not in ("typing_extensions.Concatenate", "typing.Concatenate"):
        return None

    tvar_def = self.anal_type(callable_args, allow_param_spec=True)
    if not isinstance(tvar_def, (ParamSpecType, Parameters)):
        if self.allow_unbound_tvars and isinstance(tvar_def, UnboundType):
            sym = self.lookup_qualified(tvar_def.name, callable_args)
            if sym is not None and isinstance(sym.node, ParamSpecExpr):
                # We are analyzing this type in runtime context (e.g. as type application).
                # If it is not valid as a type in this position an error will be given later.
                return callable_with_ellipsis(
                    AnyType(TypeOfAny.explicit), ret_type=ret_type, fallback=fallback
                )
        # Error was already given, so prevent further errors.
        return AnyType(TypeOfAny.from_error)
    if isinstance(tvar_def, Parameters):
        # This comes from Concatenate[int, ...]
        return CallableType(
            arg_types=tvar_def.arg_types,
            arg_names=tvar_def.arg_names,
            arg_kinds=tvar_def.arg_kinds,
            ret_type=ret_type,
            fallback=fallback,
            from_concatenate=True,
        )

    # ick, CallableType should take ParamSpecType
    prefix = tvar_def.prefix
    # we don't set the prefix here as generic arguments will get updated at some point
    # in the future. CallableType.param_spec() accounts for this.
    return CallableType(
        [
            *prefix.arg_types,
            paramspec_args(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
            paramspec_kwargs(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
        ],
        [*prefix.arg_kinds, nodes.ARG_STAR, nodes.ARG_STAR2],
        [*prefix.arg_names, None, None],
        ret_type=ret_type,
        fallback=fallback,
        from_concatenate=True,
    )

</t>
<t tx="ekr.20230831011821.458">def analyze_callable_type(self, t: UnboundType) -&gt; Type:
    fallback = self.named_type("builtins.function")
    if len(t.args) == 0:
        # Callable (bare). Treat as Callable[..., Any].
        any_type = self.get_omitted_any(t)
        ret = callable_with_ellipsis(any_type, any_type, fallback)
    elif len(t.args) == 2:
        callable_args = t.args[0]
        ret_type = t.args[1]
        if isinstance(callable_args, TypeList):
            # Callable[[ARG, ...], RET] (ordinary callable type)
            analyzed_args = self.analyze_callable_args(callable_args)
            if analyzed_args is None:
                return AnyType(TypeOfAny.from_error)
            args, kinds, names = analyzed_args
            ret = CallableType(args, kinds, names, ret_type=ret_type, fallback=fallback)
        elif isinstance(callable_args, EllipsisType):
            # Callable[..., RET] (with literal ellipsis; accept arbitrary arguments)
            ret = callable_with_ellipsis(
                AnyType(TypeOfAny.explicit), ret_type=ret_type, fallback=fallback
            )
        else:
            # Callable[P, RET] (where P is ParamSpec)
            with self.tvar_scope_frame():
                # Temporarily bind ParamSpecs to allow code like this:
                #     my_fun: Callable[Q, Foo[Q]]
                # We usually do this later in visit_callable_type(), but the analysis
                # below happens at very early stage.
                variables = []
                for name, tvar_expr in self.find_type_var_likes(callable_args):
                    variables.append(self.tvar_scope.bind_new(name, tvar_expr))
                maybe_ret = self.analyze_callable_args_for_paramspec(
                    callable_args, ret_type, fallback
                ) or self.analyze_callable_args_for_concatenate(
                    callable_args, ret_type, fallback
                )
                if isinstance(maybe_ret, CallableType):
                    maybe_ret = maybe_ret.copy_modified(
                        ret_type=ret_type.accept(self), variables=variables
                    )
            if maybe_ret is None:
                # Callable[?, RET] (where ? is something invalid)
                self.fail(
                    "The first argument to Callable must be a "
                    'list of types, parameter specification, or "..."',
                    t,
                    code=codes.VALID_TYPE,
                )
                self.note(
                    "See https://mypy.readthedocs.io/en/stable/kinds_of_types.html#callable-types-and-lambdas",
                    t,
                )
                return AnyType(TypeOfAny.from_error)
            elif isinstance(maybe_ret, AnyType):
                return maybe_ret
            ret = maybe_ret
    else:
        if self.options.disallow_any_generics:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]"', t)
        else:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]" or "Callable"', t)
        return AnyType(TypeOfAny.from_error)
    assert isinstance(ret, CallableType)
    return ret.accept(self)

</t>
<t tx="ekr.20230831011821.459">def analyze_callable_args(
    self, arglist: TypeList
) -&gt; tuple[list[Type], list[ArgKind], list[str | None]] | None:
    args: list[Type] = []
    kinds: list[ArgKind] = []
    names: list[str | None] = []
    for arg in arglist.items:
        if isinstance(arg, CallableArgument):
            args.append(arg.typ)
            names.append(arg.name)
            if arg.constructor is None:
                return None
            found = self.lookup_qualified(arg.constructor, arg)
            if found is None:
                # Looking it up already put an error message in
                return None
            elif found.fullname not in ARG_KINDS_BY_CONSTRUCTOR:
                self.fail(f'Invalid argument constructor "{found.fullname}"', arg)
                return None
            else:
                assert found.fullname is not None
                kind = ARG_KINDS_BY_CONSTRUCTOR[found.fullname]
                kinds.append(kind)
                if arg.name is not None and kind.is_star():
                    self.fail(f"{arg.constructor} arguments should not have names", arg)
                    return None
        elif isinstance(arg, UnboundType):
            kind = ARG_POS
            # Potentially a unpack.
            sym = self.lookup_qualified(arg.name, arg)
            if sym is not None:
                if sym.fullname in ("typing_extensions.Unpack", "typing.Unpack"):
                    kind = ARG_STAR
            args.append(arg)
            kinds.append(kind)
            names.append(None)
        else:
            args.append(arg)
            kinds.append(ARG_POS)
            names.append(None)
    # Note that arglist below is only used for error context.
    check_arg_names(names, [arglist] * len(args), self.fail, "Callable")
    check_arg_kinds(kinds, [arglist] * len(args), self.fail)
    return args, kinds, names

</t>
<t tx="ekr.20230831011821.46">def visit_type_application(self, o: TypeApplication) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.460">def analyze_literal_type(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.fail("Literal[...] must have at least one parameter", t, code=codes.VALID_TYPE)
        return AnyType(TypeOfAny.from_error)

    output: list[Type] = []
    for i, arg in enumerate(t.args):
        analyzed_types = self.analyze_literal_param(i + 1, arg, t)
        if analyzed_types is None:
            return AnyType(TypeOfAny.from_error)
        else:
            output.extend(analyzed_types)
    return UnionType.make_union(output, line=t.line)

</t>
<t tx="ekr.20230831011821.461">def analyze_literal_param(self, idx: int, arg: Type, ctx: Context) -&gt; list[Type] | None:
    # This UnboundType was originally defined as a string.
    if isinstance(arg, UnboundType) and arg.original_str_expr is not None:
        assert arg.original_str_fallback is not None
        return [
            LiteralType(
                value=arg.original_str_expr,
                fallback=self.named_type(arg.original_str_fallback),
                line=arg.line,
                column=arg.column,
            )
        ]

    # If arg is an UnboundType that was *not* originally defined as
    # a string, try expanding it in case it's a type alias or something.
    if isinstance(arg, UnboundType):
        self.nesting_level += 1
        try:
            arg = self.visit_unbound_type(arg, defining_literal=True)
        finally:
            self.nesting_level -= 1

    # Literal[...] cannot contain Any. Give up and add an error message
    # (if we haven't already).
    arg = get_proper_type(arg)
    if isinstance(arg, AnyType):
        # Note: We can encounter Literals containing 'Any' under three circumstances:
        #
        # 1. If the user attempts use an explicit Any as a parameter
        # 2. If the user is trying to use an enum value imported from a module with
        #    no type hints, giving it an implicit type of 'Any'
        # 3. If there's some other underlying problem with the parameter.
        #
        # We report an error in only the first two cases. In the third case, we assume
        # some other region of the code has already reported a more relevant error.
        #
        # TODO: Once we start adding support for enums, make sure we report a custom
        # error for case 2 as well.
        if arg.type_of_any not in (TypeOfAny.from_error, TypeOfAny.special_form):
            self.fail(
                f'Parameter {idx} of Literal[...] cannot be of type "Any"',
                ctx,
                code=codes.VALID_TYPE,
            )
        return None
    elif isinstance(arg, RawExpressionType):
        # A raw literal. Convert it directly into a literal if we can.
        if arg.literal_value is None:
            name = arg.simple_name()
            if name in ("float", "complex"):
                msg = f'Parameter {idx} of Literal[...] cannot be of type "{name}"'
            else:
                msg = "Invalid type: Literal[...] cannot contain arbitrary expressions"
            self.fail(msg, ctx, code=codes.VALID_TYPE)
            # Note: we deliberately ignore arg.note here: the extra info might normally be
            # helpful, but it generally won't make sense in the context of a Literal[...].
            return None

        # Remap bytes and unicode into the appropriate type for the correct Python version
        fallback = self.named_type(arg.base_type_name)
        assert isinstance(fallback, Instance)
        return [LiteralType(arg.literal_value, fallback, line=arg.line, column=arg.column)]
    elif isinstance(arg, (NoneType, LiteralType)):
        # Types that we can just add directly to the literal/potential union of literals.
        return [arg]
    elif isinstance(arg, Instance) and arg.last_known_value is not None:
        # Types generated from declarations like "var: Final = 4".
        return [arg.last_known_value]
    elif isinstance(arg, UnionType):
        out = []
        for union_arg in arg.items:
            union_result = self.analyze_literal_param(idx, union_arg, ctx)
            if union_result is None:
                return None
            out.extend(union_result)
        return out
    else:
        self.fail(f"Parameter {idx} of Literal[...] is invalid", ctx, code=codes.VALID_TYPE)
        return None

</t>
<t tx="ekr.20230831011821.462">def analyze_type(self, t: Type) -&gt; Type:
    return t.accept(self)

</t>
<t tx="ekr.20230831011821.463">def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.fail_func(msg, ctx, code=code)

</t>
<t tx="ekr.20230831011821.464">def note(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.note_func(msg, ctx, code=code)

</t>
<t tx="ekr.20230831011821.465">@contextmanager
def tvar_scope_frame(self) -&gt; Iterator[None]:
    old_scope = self.tvar_scope
    self.tvar_scope = self.tvar_scope.method_frame()
    yield
    self.tvar_scope = old_scope

</t>
<t tx="ekr.20230831011821.466">def find_type_var_likes(self, t: Type, include_callables: bool = True) -&gt; TypeVarLikeList:
    return t.accept(
        TypeVarLikeQuery(self.api, self.tvar_scope, include_callables=include_callables)
    )

</t>
<t tx="ekr.20230831011821.467">def infer_type_variables(self, type: CallableType) -&gt; list[tuple[str, TypeVarLikeExpr]]:
    """Return list of unique type variables referred to in a callable."""
    names: list[str] = []
    tvars: list[TypeVarLikeExpr] = []
    for arg in type.arg_types:
        for name, tvar_expr in self.find_type_var_likes(arg):
            if name not in names:
                names.append(name)
                tvars.append(tvar_expr)
    # When finding type variables in the return type of a function, don't
    # look inside Callable types.  Type variables only appearing in
    # functions in the return type belong to those functions, not the
    # function we're currently analyzing.
    for name, tvar_expr in self.find_type_var_likes(type.ret_type, include_callables=False):
        if name not in names:
            names.append(name)
            tvars.append(tvar_expr)

    if not names:
        return []  # Fast path
    return list(zip(names, tvars))

</t>
<t tx="ekr.20230831011821.468">def bind_function_type_variables(
    self, fun_type: CallableType, defn: Context
) -&gt; tuple[Sequence[TypeVarLikeType], bool]:
    """Find the type variables of the function type and bind them in our tvar_scope"""
    has_self_type = False
    if fun_type.variables:
        defs = []
        for var in fun_type.variables:
            if self.api.type and self.api.type.self_type and var == self.api.type.self_type:
                has_self_type = True
                continue
            var_node = self.lookup_qualified(var.name, defn)
            assert var_node, "Binding for function type variable not found within function"
            var_expr = var_node.node
            assert isinstance(var_expr, TypeVarLikeExpr)
            binding = self.tvar_scope.bind_new(var.name, var_expr)
            defs.append(binding)
        return defs, has_self_type
    typevars = self.infer_type_variables(fun_type)
    has_self_type = find_self_type(
        fun_type, lambda name: self.api.lookup_qualified(name, defn, suppress_errors=True)
    )
    # Do not define a new type variable if already defined in scope.
    typevars = [
        (name, tvar) for name, tvar in typevars if not self.is_defined_type_var(name, defn)
    ]
    defs = []
    for name, tvar in typevars:
        if not self.tvar_scope.allow_binding(tvar.fullname):
            self.fail(
                f'Type variable "{name}" is bound by an outer class',
                defn,
                code=codes.VALID_TYPE,
            )
        binding = self.tvar_scope.bind_new(name, tvar)
        defs.append(binding)

    return defs, has_self_type

</t>
<t tx="ekr.20230831011821.469">def is_defined_type_var(self, tvar: str, context: Context) -&gt; bool:
    tvar_node = self.lookup_qualified(tvar, context)
    if not tvar_node:
        return False
    return self.tvar_scope.get_binding(tvar_node) is not None

</t>
<t tx="ekr.20230831011821.47">def visit_lambda_expr(self, o: LambdaExpr) -&gt; None:
    self.visit_func(o)

</t>
<t tx="ekr.20230831011821.470">def anal_array(
    self,
    a: Iterable[Type],
    nested: bool = True,
    *,
    allow_param_spec: bool = False,
    allow_param_spec_literals: bool = False,
    allow_unpack: bool = False,
) -&gt; list[Type]:
    old_allow_param_spec_literals = self.allow_param_spec_literals
    self.allow_param_spec_literals = allow_param_spec_literals
    res: list[Type] = []
    for t in a:
        res.append(
            self.anal_type(
                t, nested, allow_param_spec=allow_param_spec, allow_unpack=allow_unpack
            )
        )
    self.allow_param_spec_literals = old_allow_param_spec_literals
    return self.check_unpacks_in_list(res)

</t>
<t tx="ekr.20230831011821.471">def anal_type(
    self,
    t: Type,
    nested: bool = True,
    *,
    allow_param_spec: bool = False,
    allow_unpack: bool = False,
    allow_ellipsis: bool = False,
) -&gt; Type:
    if nested:
        self.nesting_level += 1
    old_allow_required = self.allow_required
    self.allow_required = False
    old_allow_ellipsis = self.allow_ellipsis
    self.allow_ellipsis = allow_ellipsis
    old_allow_unpack = self.allow_unpack
    self.allow_unpack = allow_unpack
    try:
        analyzed = t.accept(self)
    finally:
        if nested:
            self.nesting_level -= 1
        self.allow_required = old_allow_required
        self.allow_ellipsis = old_allow_ellipsis
        self.allow_unpack = old_allow_unpack
    if (
        not allow_param_spec
        and isinstance(analyzed, ParamSpecType)
        and analyzed.flavor == ParamSpecFlavor.BARE
    ):
        if analyzed.prefix.arg_types:
            self.fail("Invalid location for Concatenate", t, code=codes.VALID_TYPE)
            self.note("You can use Concatenate as the first argument to Callable", t)
            analyzed = AnyType(TypeOfAny.from_error)
        else:
            self.fail(
                f'Invalid location for ParamSpec "{analyzed.name}"', t, code=codes.VALID_TYPE
            )
            self.note(
                "You can use ParamSpec as the first argument to Callable, e.g., "
                "'Callable[{}, int]'".format(analyzed.name),
                t,
            )
            analyzed = AnyType(TypeOfAny.from_error)
    return analyzed

</t>
<t tx="ekr.20230831011821.472">def anal_var_def(self, var_def: TypeVarLikeType) -&gt; TypeVarLikeType:
    if isinstance(var_def, TypeVarType):
        return TypeVarType(
            name=var_def.name,
            fullname=var_def.fullname,
            id=var_def.id.raw_id,
            values=self.anal_array(var_def.values),
            upper_bound=var_def.upper_bound.accept(self),
            default=var_def.default.accept(self),
            variance=var_def.variance,
            line=var_def.line,
            column=var_def.column,
        )
    else:
        return var_def

</t>
<t tx="ekr.20230831011821.473">def anal_var_defs(self, var_defs: Sequence[TypeVarLikeType]) -&gt; list[TypeVarLikeType]:
    return [self.anal_var_def(vd) for vd in var_defs]

</t>
<t tx="ekr.20230831011821.474">def named_type(
    self,
    fully_qualified_name: str,
    args: list[Type] | None = None,
    line: int = -1,
    column: int = -1,
) -&gt; Instance:
    node = self.lookup_fully_qualified(fully_qualified_name)
    assert isinstance(node.node, TypeInfo)
    any_type = AnyType(TypeOfAny.special_form)
    if args is not None:
        args = self.check_unpacks_in_list(args)
    return Instance(
        node.node, args or [any_type] * len(node.node.defn.type_vars), line=line, column=column
    )

</t>
<t tx="ekr.20230831011821.475">def check_unpacks_in_list(self, items: list[Type]) -&gt; list[Type]:
    new_items: list[Type] = []
    num_unpacks = 0
    final_unpack = None
    for item in items:
        if isinstance(item, UnpackType) and not isinstance(
            get_proper_type(item.type), TupleType
        ):
            if not num_unpacks:
                new_items.append(item)
            num_unpacks += 1
            final_unpack = item
        else:
            new_items.append(item)

    if num_unpacks &gt; 1:
        assert final_unpack is not None
        self.fail("More than one Unpack in a type is not allowed", final_unpack)
    return new_items

</t>
<t tx="ekr.20230831011821.476">def tuple_type(self, items: list[Type]) -&gt; TupleType:
    any_type = AnyType(TypeOfAny.special_form)
    return TupleType(items, fallback=self.named_type("builtins.tuple", [any_type]))


</t>
<t tx="ekr.20230831011821.477">TypeVarLikeList = List[Tuple[str, TypeVarLikeExpr]]


class MsgCallback(Protocol):
    @others
</t>
<t tx="ekr.20230831011821.478">def __call__(self, __msg: str, __ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    ...


</t>
<t tx="ekr.20230831011821.479">def get_omitted_any(
    disallow_any: bool,
    fail: MsgCallback,
    note: MsgCallback,
    orig_type: Type,
    options: Options,
    fullname: str | None = None,
    unexpanded_type: Type | None = None,
) -&gt; AnyType:
    if disallow_any:
        nongen_builtins = get_nongen_builtins(options.python_version)
        if fullname in nongen_builtins:
            typ = orig_type
            # We use a dedicated error message for builtin generics (as the most common case).
            alternative = nongen_builtins[fullname]
            fail(
                message_registry.IMPLICIT_GENERIC_ANY_BUILTIN.format(alternative),
                typ,
                code=codes.TYPE_ARG,
            )
        else:
            typ = unexpanded_type or orig_type
            type_str = typ.name if isinstance(typ, UnboundType) else format_type_bare(typ, options)

            fail(
                message_registry.BARE_GENERIC.format(quote_type_string(type_str)),
                typ,
                code=codes.TYPE_ARG,
            )
            base_type = get_proper_type(orig_type)
            base_fullname = (
                base_type.type.fullname if isinstance(base_type, Instance) else fullname
            )
            # Ideally, we'd check whether the type is quoted or `from __future__ annotations`
            # is set before issuing this note
            if (
                options.python_version &lt; (3, 9)
                and base_fullname in GENERIC_STUB_NOT_AT_RUNTIME_TYPES
            ):
                # Recommend `from __future__ import annotations` or to put type in quotes
                # (string literal escaping) for classes not generic at runtime
                note(
                    "Subscripting classes that are not generic at runtime may require "
                    "escaping, see https://mypy.readthedocs.io/en/stable/runtime_troubles.html"
                    "#not-generic-runtime",
                    typ,
                    code=codes.TYPE_ARG,
                )

        any_type = AnyType(TypeOfAny.from_error, line=typ.line, column=typ.column)
    else:
        any_type = AnyType(
            TypeOfAny.from_omitted_generics, line=orig_type.line, column=orig_type.column
        )
    return any_type


</t>
<t tx="ekr.20230831011821.48">def visit_star_expr(self, o: StarExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.480">def fix_type_var_tuple_argument(any_type: Type, t: Instance) -&gt; None:
    if t.type.has_type_var_tuple_type:
        args = list(t.args)
        assert t.type.type_var_tuple_prefix is not None
        tvt = t.type.defn.type_vars[t.type.type_var_tuple_prefix]
        assert isinstance(tvt, TypeVarTupleType)
        args[t.type.type_var_tuple_prefix] = UnpackType(
            Instance(tvt.tuple_fallback.type, [any_type])
        )
        t.args = tuple(args)


</t>
<t tx="ekr.20230831011821.481">def fix_instance(
    t: Instance,
    fail: MsgCallback,
    note: MsgCallback,
    disallow_any: bool,
    options: Options,
    use_generic_error: bool = False,
    unexpanded_type: Type | None = None,
) -&gt; None:
    """Fix a malformed instance by replacing all type arguments with Any.

    Also emit a suitable error if this is not due to implicit Any's.
    """
    if len(t.args) == 0:
        if use_generic_error:
            fullname: str | None = None
        else:
            fullname = t.type.fullname
        any_type = get_omitted_any(disallow_any, fail, note, t, options, fullname, unexpanded_type)
        t.args = (any_type,) * len(t.type.type_vars)
        fix_type_var_tuple_argument(any_type, t)

        return

    if t.type.has_type_var_tuple_type:
        # This can be only correctly analyzed when all arguments are fully
        # analyzed, because there may be a variadic item among them, so we
        # do this in semanal_typeargs.py.
        return

    # Invalid number of type parameters.
    fail(
        wrong_type_arg_count(len(t.type.type_vars), str(len(t.args)), t.type.name),
        t,
        code=codes.TYPE_ARG,
    )
    # Construct the correct number of type arguments, as
    # otherwise the type checker may crash as it expects
    # things to be right.
    t.args = tuple(AnyType(TypeOfAny.from_error) for _ in t.type.type_vars)
    t.invalid = True


</t>
<t tx="ekr.20230831011821.482">def instantiate_type_alias(
    node: TypeAlias,
    args: list[Type],
    fail: MsgCallback,
    no_args: bool,
    ctx: Context,
    options: Options,
    *,
    unexpanded_type: Type | None = None,
    disallow_any: bool = False,
    use_standard_error: bool = False,
    empty_tuple_index: bool = False,
) -&gt; Type:
    """Create an instance of a (generic) type alias from alias node and type arguments.

    We are following the rules outlined in TypeAlias docstring.
    Here:
        node: type alias node (definition)
        args: type arguments (types to be substituted in place of type variables
              when expanding the alias)
        fail: error reporter callback
        no_args: whether original definition used a bare generic `A = List`
        ctx: context where expansion happens
        unexpanded_type, disallow_any, use_standard_error: used to customize error messages
    """
    exp_len = len(node.alias_tvars)
    act_len = len(args)
    if (
        exp_len &gt; 0
        and act_len == 0
        and not (empty_tuple_index and node.tvar_tuple_index is not None)
    ):
        # Interpret bare Alias same as normal generic, i.e., Alias[Any, Any, ...]
        return set_any_tvars(
            node,
            ctx.line,
            ctx.column,
            options,
            disallow_any=disallow_any,
            fail=fail,
            unexpanded_type=unexpanded_type,
        )
    if exp_len == 0 and act_len == 0:
        if no_args:
            assert isinstance(node.target, Instance)  # type: ignore[misc]
            # Note: this is the only case where we use an eager expansion. See more info about
            # no_args aliases like L = List in the docstring for TypeAlias class.
            return Instance(node.target.type, [], line=ctx.line, column=ctx.column)
        return TypeAliasType(node, [], line=ctx.line, column=ctx.column)
    if (
        exp_len == 0
        and act_len &gt; 0
        and isinstance(node.target, Instance)  # type: ignore[misc]
        and no_args
    ):
        tp = Instance(node.target.type, args)
        tp.line = ctx.line
        tp.column = ctx.column
        return tp
    if act_len != exp_len and node.tvar_tuple_index is None:
        if use_standard_error:
            # This is used if type alias is an internal representation of another type,
            # for example a generic TypedDict or NamedTuple.
            msg = wrong_type_arg_count(exp_len, str(act_len), node.name)
        else:
            msg = f"Bad number of arguments for type alias, expected: {exp_len}, given: {act_len}"
        fail(msg, ctx, code=codes.TYPE_ARG)
        return set_any_tvars(node, ctx.line, ctx.column, options, from_error=True)
    # TODO: we need to check args validity w.r.t alias.alias_tvars.
    # Otherwise invalid instantiations will be allowed in runtime context.
    # Note: in type context, these will be still caught by semanal_typeargs.
    typ = TypeAliasType(node, args, ctx.line, ctx.column)
    assert typ.alias is not None
    # HACK: Implement FlexibleAlias[T, typ] by expanding it to typ here.
    if (
        isinstance(typ.alias.target, Instance)  # type: ignore[misc]
        and typ.alias.target.type.fullname == "mypy_extensions.FlexibleAlias"
    ):
        exp = get_proper_type(typ)
        assert isinstance(exp, Instance)
        return exp.args[-1]
    return typ


</t>
<t tx="ekr.20230831011821.483">def set_any_tvars(
    node: TypeAlias,
    newline: int,
    newcolumn: int,
    options: Options,
    *,
    from_error: bool = False,
    disallow_any: bool = False,
    fail: MsgCallback | None = None,
    unexpanded_type: Type | None = None,
) -&gt; TypeAliasType:
    if from_error or disallow_any:
        type_of_any = TypeOfAny.from_error
    else:
        type_of_any = TypeOfAny.from_omitted_generics
    if disallow_any and node.alias_tvars:
        assert fail is not None
        if unexpanded_type:
            type_str = (
                unexpanded_type.name
                if isinstance(unexpanded_type, UnboundType)
                else format_type_bare(unexpanded_type, options)
            )
        else:
            type_str = node.name

        fail(
            message_registry.BARE_GENERIC.format(quote_type_string(type_str)),
            Context(newline, newcolumn),
            code=codes.TYPE_ARG,
        )
    any_type = AnyType(type_of_any, line=newline, column=newcolumn)

    args: list[Type] = []
    for tv in node.alias_tvars:
        if isinstance(tv, TypeVarTupleType):
            args.append(UnpackType(Instance(tv.tuple_fallback.type, [any_type])))
        else:
            args.append(any_type)
    return TypeAliasType(node, args, newline, newcolumn)


</t>
<t tx="ekr.20230831011821.484">def flatten_tvars(lists: list[list[T]]) -&gt; list[T]:
    result: list[T] = []
    for lst in lists:
        for item in lst:
            if item not in result:
                result.append(item)
    return result


</t>
<t tx="ekr.20230831011821.485">class TypeVarLikeQuery(TypeQuery[TypeVarLikeList]):
    """Find TypeVar and ParamSpec references in an unbound type."""

    @others
</t>
<t tx="ekr.20230831011821.486">def __init__(
    self,
    api: SemanticAnalyzerCoreInterface,
    scope: TypeVarLikeScope,
    *,
    include_callables: bool = True,
) -&gt; None:
    super().__init__(flatten_tvars)
    self.api = api
    self.scope = scope
    self.include_callables = include_callables
    # Only include type variables in type aliases args. This would be anyway
    # that case if we expand (as target variables would be overridden with args)
    # and it may cause infinite recursion on invalid (diverging) recursive aliases.
    self.skip_alias_target = True

</t>
<t tx="ekr.20230831011821.487">def _seems_like_callable(self, type: UnboundType) -&gt; bool:
    if not type.args:
        return False
    return isinstance(type.args[0], (EllipsisType, TypeList, ParamSpecType))

</t>
<t tx="ekr.20230831011821.488">def visit_unbound_type(self, t: UnboundType) -&gt; TypeVarLikeList:
    name = t.name
    node = None
    # Special case P.args and P.kwargs for ParamSpecs only.
    if name.endswith("args"):
        if name.endswith(".args") or name.endswith(".kwargs"):
            base = ".".join(name.split(".")[:-1])
            n = self.api.lookup_qualified(base, t)
            if n is not None and isinstance(n.node, ParamSpecExpr):
                node = n
                name = base
    if node is None:
        node = self.api.lookup_qualified(name, t)
    if (
        node
        and isinstance(node.node, TypeVarLikeExpr)
        and self.scope.get_binding(node) is None
    ):
        assert isinstance(node.node, TypeVarLikeExpr)
        return [(name, node.node)]
    elif not self.include_callables and self._seems_like_callable(t):
        return []
    elif node and node.fullname in LITERAL_TYPE_NAMES:
        return []
    elif node and node.fullname in ANNOTATED_TYPE_NAMES and t.args:
        # Don't query the second argument to Annotated for TypeVars
        return self.query_types([t.args[0]])
    else:
        return super().visit_unbound_type(t)

</t>
<t tx="ekr.20230831011821.489">def visit_callable_type(self, t: CallableType) -&gt; TypeVarLikeList:
    if self.include_callables:
        return super().visit_callable_type(t)
    else:
        return []


</t>
<t tx="ekr.20230831011821.49">def visit_await_expr(self, o: AwaitExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.490">class DivergingAliasDetector(TrivialSyntheticTypeTranslator):
    """See docstring of detect_diverging_alias() for details."""

    @others
</t>
<t tx="ekr.20230831011821.491"># TODO: this doesn't really need to be a translator, but we don't have a trivial visitor.
def __init__(
    self,
    seen_nodes: set[TypeAlias],
    lookup: Callable[[str, Context], SymbolTableNode | None],
    scope: TypeVarLikeScope,
) -&gt; None:
    self.seen_nodes = seen_nodes
    self.lookup = lookup
    self.scope = scope
    self.diverging = False

</t>
<t tx="ekr.20230831011821.492">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    assert t.alias is not None, f"Unfixed type alias {t.type_ref}"
    if t.alias in self.seen_nodes:
        for arg in t.args:
            if not (
                isinstance(arg, TypeVarLikeType)
                or isinstance(arg, UnpackType)
                and isinstance(arg.type, TypeVarLikeType)
            ) and has_type_vars(arg):
                self.diverging = True
                return t
        # All clear for this expansion chain.
        return t
    new_nodes = self.seen_nodes | {t.alias}
    visitor = DivergingAliasDetector(new_nodes, self.lookup, self.scope)
    _ = get_proper_type(t).accept(visitor)
    if visitor.diverging:
        self.diverging = True
    return t


</t>
<t tx="ekr.20230831011821.493">def detect_diverging_alias(
    node: TypeAlias,
    target: Type,
    lookup: Callable[[str, Context], SymbolTableNode | None],
    scope: TypeVarLikeScope,
) -&gt; bool:
    """This detects type aliases that will diverge during type checking.

    For example F = Something[..., F[List[T]]]. At each expansion step this will produce
    *new* type aliases: e.g. F[List[int]], F[List[List[int]]], etc. So we can't detect
    recursion. It is a known problem in the literature, recursive aliases and generic types
    don't always go well together. It looks like there is no known systematic solution yet.

    # TODO: should we handle such aliases using type_recursion counter and some large limit?
    They may be handy in rare cases, e.g. to express a union of non-mixed nested lists:
    Nested = Union[T, Nested[List[T]]] ~&gt; Union[T, List[T], List[List[T]], ...]
    """
    visitor = DivergingAliasDetector({node}, lookup, scope)
    _ = target.accept(visitor)
    return visitor.diverging


</t>
<t tx="ekr.20230831011821.494">def check_for_explicit_any(
    typ: Type | None,
    options: Options,
    is_typeshed_stub: bool,
    msg: MessageBuilder,
    context: Context,
) -&gt; None:
    if options.disallow_any_explicit and not is_typeshed_stub and typ and has_explicit_any(typ):
        msg.explicit_any(context)


</t>
<t tx="ekr.20230831011821.495">def has_explicit_any(t: Type) -&gt; bool:
    """
    Whether this type is or type it contains is an Any coming from explicit type annotation
    """
    return t.accept(HasExplicitAny())


</t>
<t tx="ekr.20230831011821.496">class HasExplicitAny(TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20230831011821.497">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20230831011821.498">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any == TypeOfAny.explicit

</t>
<t tx="ekr.20230831011821.499">def visit_typeddict_type(self, t: TypedDictType) -&gt; bool:
    # typeddict is checked during TypedDict declaration, so don't typecheck it here.
    return False


</t>
<t tx="ekr.20230831011821.5">def visit_block(self, block: Block) -&gt; None:
    for s in block.body:
        s.accept(self)

</t>
<t tx="ekr.20230831011821.50">def visit_super_expr(self, o: SuperExpr) -&gt; None:
    o.call.accept(self)

</t>
<t tx="ekr.20230831011821.500">def has_any_from_unimported_type(t: Type) -&gt; bool:
    """Return true if this type is Any because an import was not followed.

    If type t is such Any type or has type arguments that contain such Any type
    this function will return true.
    """
    return t.accept(HasAnyFromUnimportedType())


</t>
<t tx="ekr.20230831011821.501">class HasAnyFromUnimportedType(BoolTypeQuery):
    @others
</t>
<t tx="ekr.20230831011821.502">def __init__(self) -&gt; None:
    super().__init__(ANY_STRATEGY)

</t>
<t tx="ekr.20230831011821.503">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any == TypeOfAny.from_unimported_type

</t>
<t tx="ekr.20230831011821.504">def visit_typeddict_type(self, t: TypedDictType) -&gt; bool:
    # typeddict is checked during TypedDict declaration, so don't typecheck it here
    return False


</t>
<t tx="ekr.20230831011821.505">def collect_all_inner_types(t: Type) -&gt; list[Type]:
    """
    Return all types that `t` contains
    """
    return t.accept(CollectAllInnerTypesQuery())


</t>
<t tx="ekr.20230831011821.506">class CollectAllInnerTypesQuery(TypeQuery[List[Type]]):
    @others
</t>
<t tx="ekr.20230831011821.507">def __init__(self) -&gt; None:
    super().__init__(self.combine_lists_strategy)

</t>
<t tx="ekr.20230831011821.508">def query_types(self, types: Iterable[Type]) -&gt; list[Type]:
    return self.strategy([t.accept(self) for t in types]) + list(types)

</t>
<t tx="ekr.20230831011821.509">@classmethod
def combine_lists_strategy(cls, it: Iterable[list[Type]]) -&gt; list[Type]:
    return list(itertools.chain.from_iterable(it))


</t>
<t tx="ekr.20230831011821.51">def visit_as_pattern(self, o: AsPattern) -&gt; None:
    if o.pattern is not None:
        o.pattern.accept(self)
    if o.name is not None:
        o.name.accept(self)

</t>
<t tx="ekr.20230831011821.510">def make_optional_type(t: Type) -&gt; Type:
    """Return the type corresponding to Optional[t].

    Note that we can't use normal union simplification, since this function
    is called during semantic analysis and simplification only works during
    type checking.
    """
    p_t = get_proper_type(t)
    if isinstance(p_t, NoneType):
        return t
    elif isinstance(p_t, UnionType):
        # Eagerly expanding aliases is not safe during semantic analysis.
        items = [
            item
            for item in flatten_nested_unions(p_t.items, handle_type_alias_type=False)
            if not isinstance(get_proper_type(item), NoneType)
        ]
        return UnionType(items + [NoneType()], t.line, t.column)
    else:
        return UnionType([t, NoneType()], t.line, t.column)


</t>
<t tx="ekr.20230831011821.511">def fix_instance_types(t: Type, fail: MsgCallback, note: MsgCallback, options: Options) -&gt; None:
    """Recursively fix all instance types (type argument count) in a given type.

    For example 'Union[Dict, List[str, int]]' will be transformed into
    'Union[Dict[Any, Any], List[Any]]' in place.
    """
    t.accept(InstanceFixer(fail, note, options))


</t>
<t tx="ekr.20230831011821.512">class InstanceFixer(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011821.513">def __init__(self, fail: MsgCallback, note: MsgCallback, options: Options) -&gt; None:
    self.fail = fail
    self.note = note
    self.options = options

</t>
<t tx="ekr.20230831011821.514">def visit_instance(self, typ: Instance) -&gt; None:
    super().visit_instance(typ)
    if len(typ.args) != len(typ.type.type_vars) and not typ.type.has_type_var_tuple_type:
        fix_instance(
            typ,
            self.fail,
            self.note,
            disallow_any=False,
            options=self.options,
            use_generic_error=True,
        )


</t>
<t tx="ekr.20230831011821.515">def find_self_type(typ: Type, lookup: Callable[[str], SymbolTableNode | None]) -&gt; bool:
    return typ.accept(HasSelfType(lookup))


</t>
<t tx="ekr.20230831011821.516">class HasSelfType(BoolTypeQuery):
    @others
</t>
<t tx="ekr.20230831011821.517">def __init__(self, lookup: Callable[[str], SymbolTableNode | None]) -&gt; None:
    self.lookup = lookup
    super().__init__(ANY_STRATEGY)

</t>
<t tx="ekr.20230831011821.518">def visit_unbound_type(self, t: UnboundType) -&gt; bool:
    sym = self.lookup(t.name)
    if sym and sym.fullname in SELF_TYPE_NAMES:
        return True
    return super().visit_unbound_type(t)
</t>
<t tx="ekr.20230831011821.519">@path mypy
"""Miscellaneous type operations and helpers for use during type checking.

NOTE: These must not be accessed from mypy.nodes or mypy.types to avoid import
      cycles. These must not be called from the semantic analysis main pass
      since these may assume that MROs are ready.
"""

&lt;&lt; typeops.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.52">def visit_or_pattern(self, o: OrPattern) -&gt; None:
    for p in o.patterns:
        p.accept(self)

</t>
<t tx="ekr.20230831011821.521">from __future__ import annotations

import itertools
from typing import Any, Iterable, List, Sequence, TypeVar, cast

from mypy.copytype import copy_type
from mypy.expandtype import expand_type, expand_type_by_instance
from mypy.maptype import map_instance_to_supertype
from mypy.nodes import (
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    SYMBOL_FUNCBASE_TYPES,
    Decorator,
    Expression,
    FuncBase,
    FuncDef,
    FuncItem,
    OverloadedFuncDef,
    StrExpr,
    TypeInfo,
    Var,
)
from mypy.state import state
from mypy.types import (
    ENUM_REMOVED_PROPS,
    AnyType,
    CallableType,
    ExtraAttrs,
    FormalArgument,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeQuery,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UninhabitedType,
    UnionType,
    UnpackType,
    flatten_nested_unions,
    get_proper_type,
    get_proper_types,
)
from mypy.typevars import fill_typevars


</t>
<t tx="ekr.20230831011821.522">def is_recursive_pair(s: Type, t: Type) -&gt; bool:
    """Is this a pair of recursive types?

    There may be more cases, and we may be forced to use e.g. has_recursive_types()
    here, but this function is called in very hot code, so we try to keep it simple
    and return True only in cases we know may have problems.
    """
    if isinstance(s, TypeAliasType) and s.is_recursive:
        return (
            isinstance(get_proper_type(t), (Instance, UnionType))
            or isinstance(t, TypeAliasType)
            and t.is_recursive
            # Tuple types are special, they can cause an infinite recursion even if
            # the other type is not recursive, because of the tuple fallback that is
            # calculated "on the fly".
            or isinstance(get_proper_type(s), TupleType)
        )
    if isinstance(t, TypeAliasType) and t.is_recursive:
        return (
            isinstance(get_proper_type(s), (Instance, UnionType))
            or isinstance(s, TypeAliasType)
            and s.is_recursive
            # Same as above.
            or isinstance(get_proper_type(t), TupleType)
        )
    return False


</t>
<t tx="ekr.20230831011821.523">def tuple_fallback(typ: TupleType) -&gt; Instance:
    """Return fallback type for a tuple."""
    from mypy.join import join_type_list

    info = typ.partial_fallback.type
    if info.fullname != "builtins.tuple":
        return typ.partial_fallback
    items = []
    for item in typ.items:
        if isinstance(item, UnpackType):
            unpacked_type = get_proper_type(item.type)
            if isinstance(unpacked_type, TypeVarTupleType):
                items.append(unpacked_type.upper_bound)
            elif (
                isinstance(unpacked_type, Instance)
                and unpacked_type.type.fullname == "builtins.tuple"
            ):
                items.append(unpacked_type.args[0])
            elif isinstance(unpacked_type, (AnyType, UninhabitedType)):
                continue
            else:
                raise NotImplementedError(unpacked_type)
        else:
            items.append(item)
    # TODO: we should really use a union here, tuple types are special.
    return Instance(info, [join_type_list(items)], extra_attrs=typ.partial_fallback.extra_attrs)


</t>
<t tx="ekr.20230831011821.524">def get_self_type(func: CallableType, default_self: Instance | TupleType) -&gt; Type | None:
    if isinstance(get_proper_type(func.ret_type), UninhabitedType):
        return func.ret_type
    elif func.arg_types and func.arg_types[0] != default_self and func.arg_kinds[0] == ARG_POS:
        return func.arg_types[0]
    else:
        return None


</t>
<t tx="ekr.20230831011821.525">def type_object_type_from_function(
    signature: FunctionLike, info: TypeInfo, def_info: TypeInfo, fallback: Instance, is_new: bool
) -&gt; FunctionLike:
    # We first need to record all non-trivial (explicit) self types in __init__,
    # since they will not be available after we bind them. Note, we use explicit
    # self-types only in the defining class, similar to __new__ (but not exactly the same,
    # see comment in class_callable below). This is mostly useful for annotating library
    # classes such as subprocess.Popen.
    default_self = fill_typevars(info)
    if not is_new and not info.is_newtype:
        orig_self_types = [get_self_type(it, default_self) for it in signature.items]
    else:
        orig_self_types = [None] * len(signature.items)

    # The __init__ method might come from a generic superclass 'def_info'
    # with type variables that do not map identically to the type variables of
    # the class 'info' being constructed. For example:
    #
    #   class A(Generic[T]):
    #       def __init__(self, x: T) -&gt; None: ...
    #   class B(A[List[T]]):
    #      ...
    #
    # We need to map B's __init__ to the type (List[T]) -&gt; None.
    signature = bind_self(signature, original_type=default_self, is_classmethod=is_new)
    signature = cast(FunctionLike, map_type_from_supertype(signature, info, def_info))

    special_sig: str | None = None
    if def_info.fullname == "builtins.dict":
        # Special signature!
        special_sig = "dict"

    if isinstance(signature, CallableType):
        return class_callable(signature, info, fallback, special_sig, is_new, orig_self_types[0])
    else:
        # Overloaded __init__/__new__.
        assert isinstance(signature, Overloaded)
        items: list[CallableType] = []
        for item, orig_self in zip(signature.items, orig_self_types):
            items.append(class_callable(item, info, fallback, special_sig, is_new, orig_self))
        return Overloaded(items)


</t>
<t tx="ekr.20230831011821.526">def class_callable(
    init_type: CallableType,
    info: TypeInfo,
    type_type: Instance,
    special_sig: str | None,
    is_new: bool,
    orig_self_type: Type | None = None,
) -&gt; CallableType:
    """Create a type object type based on the signature of __init__."""
    variables: list[TypeVarLikeType] = []
    variables.extend(info.defn.type_vars)
    variables.extend(init_type.variables)

    from mypy.subtypes import is_subtype

    init_ret_type = get_proper_type(init_type.ret_type)
    orig_self_type = get_proper_type(orig_self_type)
    default_ret_type = fill_typevars(info)
    explicit_type = init_ret_type if is_new else orig_self_type
    if (
        isinstance(explicit_type, (Instance, TupleType, UninhabitedType))
        # We have to skip protocols, because it can be a subtype of a return type
        # by accident. Like `Hashable` is a subtype of `object`. See #11799
        and isinstance(default_ret_type, Instance)
        and not default_ret_type.type.is_protocol
        # Only use the declared return type from __new__ or declared self in __init__
        # if it is actually returning a subtype of what we would return otherwise.
        and is_subtype(explicit_type, default_ret_type, ignore_type_params=True)
    ):
        ret_type: Type = explicit_type
    else:
        ret_type = default_ret_type

    callable_type = init_type.copy_modified(
        ret_type=ret_type,
        fallback=type_type,
        name=None,
        variables=variables,
        special_sig=special_sig,
    )
    c = callable_type.with_name(info.name)
    return c


</t>
<t tx="ekr.20230831011821.527">def map_type_from_supertype(typ: Type, sub_info: TypeInfo, super_info: TypeInfo) -&gt; Type:
    """Map type variables in a type defined in a supertype context to be valid
    in the subtype context. Assume that the result is unique; if more than
    one type is possible, return one of the alternatives.

    For example, assume

      class D(Generic[S]): ...
      class C(D[E[T]], Generic[T]): ...

    Now S in the context of D would be mapped to E[T] in the context of C.
    """
    # Create the type of self in subtype, of form t[a1, ...].
    inst_type = fill_typevars(sub_info)
    if isinstance(inst_type, TupleType):
        inst_type = tuple_fallback(inst_type)
    # Map the type of self to supertype. This gets us a description of the
    # supertype type variables in terms of subtype variables, i.e. t[t1, ...]
    # so that any type variables in tN are to be interpreted in subtype
    # context.
    inst_type = map_instance_to_supertype(inst_type, super_info)
    # Finally expand the type variables in type with those in the previously
    # constructed type. Note that both type and inst_type may have type
    # variables, but in type they are interpreted in supertype context while
    # in inst_type they are interpreted in subtype context. This works even if
    # the names of type variables in supertype and subtype overlap.
    return expand_type_by_instance(typ, inst_type)


</t>
<t tx="ekr.20230831011821.528">def supported_self_type(typ: ProperType) -&gt; bool:
    """Is this a supported kind of explicit self-types?

    Currently, this means a X or Type[X], where X is an instance or
    a type variable with an instance upper bound.
    """
    if isinstance(typ, TypeType):
        return supported_self_type(typ.item)
    return isinstance(typ, TypeVarType) or (
        isinstance(typ, Instance) and typ != fill_typevars(typ.type)
    )


</t>
<t tx="ekr.20230831011821.529">F = TypeVar("F", bound=FunctionLike)


def bind_self(method: F, original_type: Type | None = None, is_classmethod: bool = False) -&gt; F:
    """Return a copy of `method`, with the type of its first parameter (usually
    self or cls) bound to original_type.

    If the type of `self` is a generic type (T, or Type[T] for classmethods),
    instantiate every occurrence of type with original_type in the rest of the
    signature and in the return type.

    original_type is the type of E in the expression E.copy(). It is None in
    compatibility checks. In this case we treat it as the erasure of the
    declared type of self.

    This way we can express "the type of self". For example:

    T = TypeVar('T', bound='A')
    class A:
        def copy(self: T) -&gt; T: ...

    class B(A): pass

    b = B().copy()  # type: B

    """
    if isinstance(method, Overloaded):
        return cast(
            F, Overloaded([bind_self(c, original_type, is_classmethod) for c in method.items])
        )
    assert isinstance(method, CallableType)
    func = method
    if not func.arg_types:
        # Invalid method, return something.
        return cast(F, func)
    if func.arg_kinds[0] == ARG_STAR:
        # The signature is of the form 'def foo(*args, ...)'.
        # In this case we shouldn't drop the first arg,
        # since func will be absorbed by the *args.

        # TODO: infer bounds on the type of *args?
        return cast(F, func)
    self_param_type = get_proper_type(func.arg_types[0])

    variables: Sequence[TypeVarLikeType]
    if func.variables and supported_self_type(self_param_type):
        from mypy.infer import infer_type_arguments

        if original_type is None:
            # TODO: type check method override (see #7861).
            original_type = erase_to_bound(self_param_type)
        original_type = get_proper_type(original_type)

        # Find which of method type variables appear in the type of "self".
        self_ids = {tv.id for tv in get_all_type_vars(self_param_type)}
        self_vars = [tv for tv in func.variables if tv.id in self_ids]

        # Solve for these type arguments using the actual class or instance type.
        typeargs = infer_type_arguments(
            self_vars, self_param_type, original_type, is_supertype=True
        )
        if (
            is_classmethod
            and any(isinstance(get_proper_type(t), UninhabitedType) for t in typeargs)
            and isinstance(original_type, (Instance, TypeVarType, TupleType))
        ):
            # In case we call a classmethod through an instance x, fallback to type(x).
            typeargs = infer_type_arguments(
                self_vars, self_param_type, TypeType(original_type), is_supertype=True
            )

        # Update the method signature with the solutions found.
        # Technically, some constraints might be unsolvable, make them &lt;nothing&gt;.
        to_apply = [t if t is not None else UninhabitedType() for t in typeargs]
        func = expand_type(func, {tv.id: arg for tv, arg in zip(self_vars, to_apply)})
        variables = [v for v in func.variables if v not in self_vars]
    else:
        variables = func.variables

    original_type = get_proper_type(original_type)
    if isinstance(original_type, CallableType) and original_type.is_type_obj():
        original_type = TypeType.make_normalized(original_type.ret_type)
    res = func.copy_modified(
        arg_types=func.arg_types[1:],
        arg_kinds=func.arg_kinds[1:],
        arg_names=func.arg_names[1:],
        variables=variables,
        bound_args=[original_type],
    )
    return cast(F, res)


</t>
<t tx="ekr.20230831011821.53">def visit_value_pattern(self, o: ValuePattern) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20230831011821.530">def erase_to_bound(t: Type) -&gt; Type:
    # TODO: use value restrictions to produce a union?
    t = get_proper_type(t)
    if isinstance(t, TypeVarType):
        return t.upper_bound
    if isinstance(t, TypeType):
        if isinstance(t.item, TypeVarType):
            return TypeType.make_normalized(t.item.upper_bound)
    return t


</t>
<t tx="ekr.20230831011821.531">def callable_corresponding_argument(
    typ: CallableType | Parameters, model: FormalArgument
) -&gt; FormalArgument | None:
    """Return the argument a function that corresponds to `model`"""

    by_name = typ.argument_by_name(model.name)
    by_pos = typ.argument_by_position(model.pos)
    if by_name is None and by_pos is None:
        return None
    if by_name is not None and by_pos is not None:
        if by_name == by_pos:
            return by_name
        # If we're dealing with an optional pos-only and an optional
        # name-only arg, merge them.  This is the case for all functions
        # taking both *args and **args, or a pair of functions like so:

        # def right(a: int = ...) -&gt; None: ...
        # def left(__a: int = ..., *, a: int = ...) -&gt; None: ...
        from mypy.subtypes import is_equivalent

        if (
            not (by_name.required or by_pos.required)
            and by_pos.name is None
            and by_name.pos is None
            and is_equivalent(by_name.typ, by_pos.typ)
        ):
            return FormalArgument(by_name.name, by_pos.pos, by_name.typ, False)
    return by_name if by_name is not None else by_pos


</t>
<t tx="ekr.20230831011821.532">def simple_literal_type(t: ProperType | None) -&gt; Instance | None:
    """Extract the underlying fallback Instance type for a simple Literal"""
    if isinstance(t, Instance) and t.last_known_value is not None:
        t = t.last_known_value
    if isinstance(t, LiteralType):
        return t.fallback
    return None


</t>
<t tx="ekr.20230831011821.533">def is_simple_literal(t: ProperType) -&gt; bool:
    if isinstance(t, LiteralType):
        return t.fallback.type.is_enum or t.fallback.type.fullname == "builtins.str"
    if isinstance(t, Instance):
        return t.last_known_value is not None and isinstance(t.last_known_value.value, str)
    return False


</t>
<t tx="ekr.20230831011821.534">def make_simplified_union(
    items: Sequence[Type],
    line: int = -1,
    column: int = -1,
    *,
    keep_erased: bool = False,
    contract_literals: bool = True,
) -&gt; ProperType:
    """Build union type with redundant union items removed.

    If only a single item remains, this may return a non-union type.

    Examples:

    * [int, str] -&gt; Union[int, str]
    * [int, object] -&gt; object
    * [int, int] -&gt; int
    * [int, Any] -&gt; Union[int, Any] (Any types are not simplified away!)
    * [Any, Any] -&gt; Any
    * [int, Union[bytes, str]] -&gt; Union[int, bytes, str]

    Note: This must NOT be used during semantic analysis, since TypeInfos may not
          be fully initialized.

    The keep_erased flag is used for type inference against union types
    containing type variables. If set to True, keep all ErasedType items.

    The contract_literals flag indicates whether we need to contract literal types
    back into a sum type. Set it to False when called by try_expanding_sum_type_
    to_union().
    """
    # Step 1: expand all nested unions
    items = flatten_nested_unions(items)

    # Step 2: fast path for single item
    if len(items) == 1:
        return get_proper_type(items[0])

    # Step 3: remove redundant unions
    simplified_set: Sequence[Type] = _remove_redundant_union_items(items, keep_erased)

    # Step 4: If more than one literal exists in the union, try to simplify
    if (
        contract_literals
        and sum(isinstance(get_proper_type(item), LiteralType) for item in simplified_set) &gt; 1
    ):
        simplified_set = try_contracting_literals_in_union(simplified_set)

    result = get_proper_type(UnionType.make_union(simplified_set, line, column))

    nitems = len(items)
    if nitems &gt; 1 and (
        nitems &gt; 2 or not (type(items[0]) is NoneType or type(items[1]) is NoneType)
    ):
        # Step 5: At last, we erase any (inconsistent) extra attributes on instances.

        # Initialize with None instead of an empty set as a micro-optimization. The set
        # is needed very rarely, so we try to avoid constructing it.
        extra_attrs_set: set[ExtraAttrs] | None = None
        for item in items:
            instance = try_getting_instance_fallback(item)
            if instance and instance.extra_attrs:
                if extra_attrs_set is None:
                    extra_attrs_set = {instance.extra_attrs}
                else:
                    extra_attrs_set.add(instance.extra_attrs)

        if extra_attrs_set is not None and len(extra_attrs_set) &gt; 1:
            fallback = try_getting_instance_fallback(result)
            if fallback:
                fallback.extra_attrs = None

    return result


</t>
<t tx="ekr.20230831011821.535">def _remove_redundant_union_items(items: list[Type], keep_erased: bool) -&gt; list[Type]:
    from mypy.subtypes import is_proper_subtype

    # The first pass through this loop, we check if later items are subtypes of earlier items.
    # The second pass through this loop, we check if earlier items are subtypes of later items
    # (by reversing the remaining items)
    for _direction in range(2):
        new_items: list[Type] = []
        # seen is a map from a type to its index in new_items
        seen: dict[ProperType, int] = {}
        unduplicated_literal_fallbacks: set[Instance] | None = None
        for ti in items:
            proper_ti = get_proper_type(ti)

            # UninhabitedType is always redundant
            if isinstance(proper_ti, UninhabitedType):
                continue

            duplicate_index = -1
            # Quickly check if we've seen this type
            if proper_ti in seen:
                duplicate_index = seen[proper_ti]
            elif (
                isinstance(proper_ti, LiteralType)
                and unduplicated_literal_fallbacks is not None
                and proper_ti.fallback in unduplicated_literal_fallbacks
            ):
                # This is an optimisation for unions with many LiteralType
                # We've already checked for exact duplicates. This means that any super type of
                # the LiteralType must be a super type of its fallback. If we've gone through
                # the expensive loop below and found no super type for a previous LiteralType
                # with the same fallback, we can skip doing that work again and just add the type
                # to new_items
                pass
            else:
                # If not, check if we've seen a supertype of this type
                for j, tj in enumerate(new_items):
                    tj = get_proper_type(tj)
                    # If tj is an Instance with a last_known_value, do not remove proper_ti
                    # (unless it's an instance with the same last_known_value)
                    if (
                        isinstance(tj, Instance)
                        and tj.last_known_value is not None
                        and not (
                            isinstance(proper_ti, Instance)
                            and tj.last_known_value == proper_ti.last_known_value
                        )
                    ):
                        continue

                    if is_proper_subtype(
                        ti, tj, keep_erased_types=keep_erased, ignore_promotions=True
                    ):
                        duplicate_index = j
                        break
            if duplicate_index != -1:
                # If deleted subtypes had more general truthiness, use that
                orig_item = new_items[duplicate_index]
                if not orig_item.can_be_true and ti.can_be_true:
                    new_items[duplicate_index] = true_or_false(orig_item)
                elif not orig_item.can_be_false and ti.can_be_false:
                    new_items[duplicate_index] = true_or_false(orig_item)
            else:
                # We have a non-duplicate item, add it to new_items
                seen[proper_ti] = len(new_items)
                new_items.append(ti)
                if isinstance(proper_ti, LiteralType):
                    if unduplicated_literal_fallbacks is None:
                        unduplicated_literal_fallbacks = set()
                    unduplicated_literal_fallbacks.add(proper_ti.fallback)

        items = new_items
        if len(items) &lt;= 1:
            break
        items.reverse()

    return items


</t>
<t tx="ekr.20230831011821.536">def _get_type_special_method_bool_ret_type(t: Type) -&gt; Type | None:
    t = get_proper_type(t)

    if isinstance(t, Instance):
        bool_method = t.type.get("__bool__")
        if bool_method:
            callee = get_proper_type(bool_method.type)
            if isinstance(callee, CallableType):
                return callee.ret_type

    return None


</t>
<t tx="ekr.20230831011821.537">def true_only(t: Type) -&gt; ProperType:
    """
    Restricted version of t with only True-ish values
    """
    t = get_proper_type(t)

    if not t.can_be_true:
        # All values of t are False-ish, so there are no true values in it
        return UninhabitedType(line=t.line, column=t.column)
    elif not t.can_be_false:
        # All values of t are already True-ish, so true_only is idempotent in this case
        return t
    elif isinstance(t, UnionType):
        # The true version of a union type is the union of the true versions of its components
        new_items = [true_only(item) for item in t.items]
        can_be_true_items = [item for item in new_items if item.can_be_true]
        return make_simplified_union(can_be_true_items, line=t.line, column=t.column)
    else:
        ret_type = _get_type_special_method_bool_ret_type(t)

        if ret_type and not ret_type.can_be_true:
            return UninhabitedType(line=t.line, column=t.column)

        new_t = copy_type(t)
        new_t.can_be_false = False
        return new_t


</t>
<t tx="ekr.20230831011821.538">def false_only(t: Type) -&gt; ProperType:
    """
    Restricted version of t with only False-ish values
    """
    t = get_proper_type(t)

    if not t.can_be_false:
        if state.strict_optional:
            # All values of t are True-ish, so there are no false values in it
            return UninhabitedType(line=t.line)
        else:
            # When strict optional checking is disabled, everything can be
            # False-ish since anything can be None
            return NoneType(line=t.line)
    elif not t.can_be_true:
        # All values of t are already False-ish, so false_only is idempotent in this case
        return t
    elif isinstance(t, UnionType):
        # The false version of a union type is the union of the false versions of its components
        new_items = [false_only(item) for item in t.items]
        can_be_false_items = [item for item in new_items if item.can_be_false]
        return make_simplified_union(can_be_false_items, line=t.line, column=t.column)
    else:
        ret_type = _get_type_special_method_bool_ret_type(t)

        if ret_type and not ret_type.can_be_false:
            return UninhabitedType(line=t.line)

        new_t = copy_type(t)
        new_t.can_be_true = False
        return new_t


</t>
<t tx="ekr.20230831011821.539">def true_or_false(t: Type) -&gt; ProperType:
    """
    Unrestricted version of t with both True-ish and False-ish values
    """
    t = get_proper_type(t)

    if isinstance(t, UnionType):
        new_items = [true_or_false(item) for item in t.items]
        return make_simplified_union(new_items, line=t.line, column=t.column)

    new_t = copy_type(t)
    new_t.can_be_true = new_t.can_be_true_default()
    new_t.can_be_false = new_t.can_be_false_default()
    return new_t


</t>
<t tx="ekr.20230831011821.54">def visit_sequence_pattern(self, o: SequencePattern) -&gt; None:
    for p in o.patterns:
        p.accept(self)

</t>
<t tx="ekr.20230831011821.540">def erase_def_to_union_or_bound(tdef: TypeVarLikeType) -&gt; Type:
    # TODO(PEP612): fix for ParamSpecType
    if isinstance(tdef, ParamSpecType):
        return AnyType(TypeOfAny.from_error)
    assert isinstance(tdef, TypeVarType)
    if tdef.values:
        return make_simplified_union(tdef.values)
    else:
        return tdef.upper_bound


</t>
<t tx="ekr.20230831011821.541">def erase_to_union_or_bound(typ: TypeVarType) -&gt; ProperType:
    if typ.values:
        return make_simplified_union(typ.values)
    else:
        return get_proper_type(typ.upper_bound)


</t>
<t tx="ekr.20230831011821.542">def function_type(func: FuncBase, fallback: Instance) -&gt; FunctionLike:
    if func.type:
        assert isinstance(func.type, FunctionLike)
        return func.type
    else:
        # Implicit type signature with dynamic types.
        if isinstance(func, FuncItem):
            return callable_type(func, fallback)
        else:
            # Broken overloads can have self.type set to None.
            # TODO: should we instead always set the type in semantic analyzer?
            assert isinstance(func, OverloadedFuncDef)
            any_type = AnyType(TypeOfAny.from_error)
            dummy = CallableType(
                [any_type, any_type],
                [ARG_STAR, ARG_STAR2],
                [None, None],
                any_type,
                fallback,
                line=func.line,
                is_ellipsis_args=True,
            )
            # Return an Overloaded, because some callers may expect that
            # an OverloadedFuncDef has an Overloaded type.
            return Overloaded([dummy])


</t>
<t tx="ekr.20230831011821.543">def callable_type(
    fdef: FuncItem, fallback: Instance, ret_type: Type | None = None
) -&gt; CallableType:
    # TODO: somewhat unfortunate duplication with prepare_method_signature in semanal
    if fdef.info and (not fdef.is_static or fdef.name == "__new__") and fdef.arg_names:
        self_type: Type = fill_typevars(fdef.info)
        if fdef.is_class or fdef.name == "__new__":
            self_type = TypeType.make_normalized(self_type)
        args = [self_type] + [AnyType(TypeOfAny.unannotated)] * (len(fdef.arg_names) - 1)
    else:
        args = [AnyType(TypeOfAny.unannotated)] * len(fdef.arg_names)

    return CallableType(
        args,
        fdef.arg_kinds,
        fdef.arg_names,
        ret_type or AnyType(TypeOfAny.unannotated),
        fallback,
        name=fdef.name,
        line=fdef.line,
        column=fdef.column,
        implicit=True,
        # We need this for better error messages, like missing `self` note:
        definition=fdef if isinstance(fdef, FuncDef) else None,
    )


</t>
<t tx="ekr.20230831011821.544">def try_getting_str_literals(expr: Expression, typ: Type) -&gt; list[str] | None:
    """If the given expression or type corresponds to a string literal
    or a union of string literals, returns a list of the underlying strings.
    Otherwise, returns None.

    Specifically, this function is guaranteed to return a list with
    one or more strings if one of the following is true:

    1. 'expr' is a StrExpr
    2. 'typ' is a LiteralType containing a string
    3. 'typ' is a UnionType containing only LiteralType of strings
    """
    if isinstance(expr, StrExpr):
        return [expr.value]

    # TODO: See if we can eliminate this function and call the below one directly
    return try_getting_str_literals_from_type(typ)


</t>
<t tx="ekr.20230831011821.545">def try_getting_str_literals_from_type(typ: Type) -&gt; list[str] | None:
    """If the given expression or type corresponds to a string Literal
    or a union of string Literals, returns a list of the underlying strings.
    Otherwise, returns None.

    For example, if we had the type 'Literal["foo", "bar"]' as input, this function
    would return a list of strings ["foo", "bar"].
    """
    return try_getting_literals_from_type(typ, str, "builtins.str")


</t>
<t tx="ekr.20230831011821.546">def try_getting_int_literals_from_type(typ: Type) -&gt; list[int] | None:
    """If the given expression or type corresponds to an int Literal
    or a union of int Literals, returns a list of the underlying ints.
    Otherwise, returns None.

    For example, if we had the type 'Literal[1, 2, 3]' as input, this function
    would return a list of ints [1, 2, 3].
    """
    return try_getting_literals_from_type(typ, int, "builtins.int")


</t>
<t tx="ekr.20230831011821.547">T = TypeVar("T")


def try_getting_literals_from_type(
    typ: Type, target_literal_type: type[T], target_fullname: str
) -&gt; list[T] | None:
    """If the given expression or type corresponds to a Literal or
    union of Literals where the underlying values correspond to the given
    target type, returns a list of those underlying values. Otherwise,
    returns None.
    """
    typ = get_proper_type(typ)

    if isinstance(typ, Instance) and typ.last_known_value is not None:
        possible_literals: list[Type] = [typ.last_known_value]
    elif isinstance(typ, UnionType):
        possible_literals = list(typ.items)
    else:
        possible_literals = [typ]

    literals: list[T] = []
    for lit in get_proper_types(possible_literals):
        if isinstance(lit, LiteralType) and lit.fallback.type.fullname == target_fullname:
            val = lit.value
            if isinstance(val, target_literal_type):
                literals.append(val)
            else:
                return None
        else:
            return None
    return literals


</t>
<t tx="ekr.20230831011821.548">def is_literal_type_like(t: Type | None) -&gt; bool:
    """Returns 'true' if the given type context is potentially either a LiteralType,
    a Union of LiteralType, or something similar.
    """
    t = get_proper_type(t)
    if t is None:
        return False
    elif isinstance(t, LiteralType):
        return True
    elif isinstance(t, UnionType):
        return any(is_literal_type_like(item) for item in t.items)
    elif isinstance(t, TypeVarType):
        return is_literal_type_like(t.upper_bound) or any(
            is_literal_type_like(item) for item in t.values
        )
    else:
        return False


</t>
<t tx="ekr.20230831011821.549">def is_singleton_type(typ: Type) -&gt; bool:
    """Returns 'true' if this type is a "singleton type" -- if there exists
    exactly only one runtime value associated with this type.

    That is, given two values 'a' and 'b' that have the same type 't',
    'is_singleton_type(t)' returns True if and only if the expression 'a is b' is
    always true.

    Currently, this returns True when given NoneTypes, enum LiteralTypes,
    enum types with a single value and ... (Ellipses).

    Note that other kinds of LiteralTypes cannot count as singleton types. For
    example, suppose we do 'a = 100000 + 1' and 'b = 100001'. It is not guaranteed
    that 'a is b' will always be true -- some implementations of Python will end up
    constructing two distinct instances of 100001.
    """
    typ = get_proper_type(typ)
    return typ.is_singleton_type()


</t>
<t tx="ekr.20230831011821.55">def visit_starred_pattern(self, o: StarredPattern) -&gt; None:
    if o.capture is not None:
        o.capture.accept(self)

</t>
<t tx="ekr.20230831011821.550">def try_expanding_sum_type_to_union(typ: Type, target_fullname: str) -&gt; ProperType:
    """Attempts to recursively expand any enum Instances with the given target_fullname
    into a Union of all of its component LiteralTypes.

    For example, if we have:

        class Color(Enum):
            RED = 1
            BLUE = 2
            YELLOW = 3

        class Status(Enum):
            SUCCESS = 1
            FAILURE = 2
            UNKNOWN = 3

    ...and if we call `try_expanding_enum_to_union(Union[Color, Status], 'module.Color')`,
    this function will return Literal[Color.RED, Color.BLUE, Color.YELLOW, Status].
    """
    typ = get_proper_type(typ)

    if isinstance(typ, UnionType):
        items = [
            try_expanding_sum_type_to_union(item, target_fullname) for item in typ.relevant_items()
        ]
        return make_simplified_union(items, contract_literals=False)
    elif isinstance(typ, Instance) and typ.type.fullname == target_fullname:
        if typ.type.is_enum:
            new_items = []
            for name, symbol in typ.type.names.items():
                if not isinstance(symbol.node, Var):
                    continue
                # Skip these since Enum will remove it
                if name in ENUM_REMOVED_PROPS:
                    continue
                new_items.append(LiteralType(name, typ))
            return make_simplified_union(new_items, contract_literals=False)
        elif typ.type.fullname == "builtins.bool":
            return make_simplified_union(
                [LiteralType(True, typ), LiteralType(False, typ)], contract_literals=False
            )

    return typ


</t>
<t tx="ekr.20230831011821.551">def try_contracting_literals_in_union(types: Sequence[Type]) -&gt; list[ProperType]:
    """Contracts any literal types back into a sum type if possible.

    Will replace the first instance of the literal with the sum type and
    remove all others.

    If we call `try_contracting_union(Literal[Color.RED, Color.BLUE, Color.YELLOW])`,
    this function will return Color.

    We also treat `Literal[True, False]` as `bool`.
    """
    proper_types = [get_proper_type(typ) for typ in types]
    sum_types: dict[str, tuple[set[Any], list[int]]] = {}
    marked_for_deletion = set()
    for idx, typ in enumerate(proper_types):
        if isinstance(typ, LiteralType):
            fullname = typ.fallback.type.fullname
            if typ.fallback.type.is_enum or isinstance(typ.value, bool):
                if fullname not in sum_types:
                    sum_types[fullname] = (
                        set(typ.fallback.get_enum_values())
                        if typ.fallback.type.is_enum
                        else {True, False},
                        [],
                    )
                literals, indexes = sum_types[fullname]
                literals.discard(typ.value)
                indexes.append(idx)
                if not literals:
                    first, *rest = indexes
                    proper_types[first] = typ.fallback
                    marked_for_deletion |= set(rest)
    return list(
        itertools.compress(
            proper_types, [(i not in marked_for_deletion) for i in range(len(proper_types))]
        )
    )


</t>
<t tx="ekr.20230831011821.552">def coerce_to_literal(typ: Type) -&gt; Type:
    """Recursively converts any Instances that have a last_known_value or are
    instances of enum types with a single value into the corresponding LiteralType.
    """
    original_type = typ
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        new_items = [coerce_to_literal(item) for item in typ.items]
        return UnionType.make_union(new_items)
    elif isinstance(typ, Instance):
        if typ.last_known_value:
            return typ.last_known_value
        elif typ.type.is_enum:
            enum_values = typ.get_enum_values()
            if len(enum_values) == 1:
                return LiteralType(value=enum_values[0], fallback=typ)
    return original_type


</t>
<t tx="ekr.20230831011821.553">def get_type_vars(tp: Type) -&gt; list[TypeVarType]:
    return cast("list[TypeVarType]", tp.accept(TypeVarExtractor()))


</t>
<t tx="ekr.20230831011821.554">def get_all_type_vars(tp: Type) -&gt; list[TypeVarLikeType]:
    # TODO: should we always use this function instead of get_type_vars() above?
    return tp.accept(TypeVarExtractor(include_all=True))


</t>
<t tx="ekr.20230831011821.555">class TypeVarExtractor(TypeQuery[List[TypeVarLikeType]]):
    @others
</t>
<t tx="ekr.20230831011821.556">def __init__(self, include_all: bool = False) -&gt; None:
    super().__init__(self._merge)
    self.include_all = include_all

</t>
<t tx="ekr.20230831011821.557">def _merge(self, iter: Iterable[list[TypeVarLikeType]]) -&gt; list[TypeVarLikeType]:
    out = []
    for item in iter:
        out.extend(item)
    return out

</t>
<t tx="ekr.20230831011821.558">def visit_type_var(self, t: TypeVarType) -&gt; list[TypeVarLikeType]:
    return [t]

</t>
<t tx="ekr.20230831011821.559">def visit_param_spec(self, t: ParamSpecType) -&gt; list[TypeVarLikeType]:
    return [t] if self.include_all else []

</t>
<t tx="ekr.20230831011821.56">def visit_mapping_pattern(self, o: MappingPattern) -&gt; None:
    for key in o.keys:
        key.accept(self)
    for value in o.values:
        value.accept(self)
    if o.rest is not None:
        o.rest.accept(self)

</t>
<t tx="ekr.20230831011821.560">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; list[TypeVarLikeType]:
    return [t] if self.include_all else []


</t>
<t tx="ekr.20230831011821.561">def custom_special_method(typ: Type, name: str, check_all: bool = False) -&gt; bool:
    """Does this type have a custom special method such as __format__() or __eq__()?

    If check_all is True ensure all items of a union have a custom method, not just some.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        method = typ.type.get(name)
        if method and isinstance(method.node, (SYMBOL_FUNCBASE_TYPES, Decorator, Var)):
            if method.node.info:
                return not method.node.info.fullname.startswith("builtins.")
        return False
    if isinstance(typ, UnionType):
        if check_all:
            return all(custom_special_method(t, name, check_all) for t in typ.items)
        return any(custom_special_method(t, name) for t in typ.items)
    if isinstance(typ, TupleType):
        return custom_special_method(tuple_fallback(typ), name, check_all)
    if isinstance(typ, CallableType) and typ.is_type_obj():
        # Look up __method__ on the metaclass for class objects.
        return custom_special_method(typ.fallback, name, check_all)
    if isinstance(typ, AnyType):
        # Avoid false positives in uncertain cases.
        return True
    # TODO: support other types (see ExpressionChecker.has_member())?
    return False


</t>
<t tx="ekr.20230831011821.562">def separate_union_literals(t: UnionType) -&gt; tuple[Sequence[LiteralType], Sequence[Type]]:
    """Separate literals from other members in a union type."""
    literal_items = []
    union_items = []

    for item in t.items:
        proper = get_proper_type(item)
        if isinstance(proper, LiteralType):
            literal_items.append(proper)
        else:
            union_items.append(item)

    return literal_items, union_items


</t>
<t tx="ekr.20230831011821.563">def try_getting_instance_fallback(typ: Type) -&gt; Instance | None:
    """Returns the Instance fallback for this type if one exists or None."""
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return typ
    elif isinstance(typ, LiteralType):
        return typ.fallback
    elif isinstance(typ, NoneType):
        return None  # Fast path for None, which is common
    elif isinstance(typ, FunctionLike):
        return typ.fallback
    elif isinstance(typ, TupleType):
        return typ.partial_fallback
    elif isinstance(typ, TypedDictType):
        return typ.fallback
    elif isinstance(typ, TypeVarType):
        return try_getting_instance_fallback(typ.upper_bound)
    return None


</t>
<t tx="ekr.20230831011821.564">def fixup_partial_type(typ: Type) -&gt; Type:
    """Convert a partial type that we couldn't resolve into something concrete.

    This means, for None we make it Optional[Any], and for anything else we
    fill in all of the type arguments with Any.
    """
    if not isinstance(typ, PartialType):
        return typ
    if typ.type is None:
        return UnionType.make_union([AnyType(TypeOfAny.unannotated), NoneType()])
    else:
        return Instance(typ.type, [AnyType(TypeOfAny.unannotated)] * len(typ.type.type_vars))


</t>
<t tx="ekr.20230831011821.565">def get_protocol_member(left: Instance, member: str, class_obj: bool) -&gt; ProperType | None:
    if member == "__call__" and class_obj:
        # Special case: class objects always have __call__ that is just the constructor.
        from mypy.checkmember import type_object_type

        def named_type(fullname: str) -&gt; Instance:
            return Instance(left.type.mro[-1], [])

        return type_object_type(left.type, named_type)

    if member == "__call__" and left.type.is_metaclass():
        # Special case: we want to avoid falling back to metaclass __call__
        # if constructor signature didn't match, this can cause many false negatives.
        return None

    from mypy.subtypes import find_member

    return get_proper_type(find_member(member, left, left, class_obj=class_obj))
</t>
<t tx="ekr.20230831011821.566">@path mypy
"""Classes for representing mypy types."""
&lt;&lt; types.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.567">
from __future__ import annotations

import sys
from abc import abstractmethod
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Dict,
    Final,
    Iterable,
    NamedTuple,
    NewType,
    Sequence,
    TypeVar,
    Union,
    cast,
)
from typing_extensions import Self, TypeAlias as _TypeAlias, TypeGuard, overload

import mypy.nodes
from mypy.bogus_type import Bogus
from mypy.nodes import (
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    INVARIANT,
    ArgKind,
    FakeInfo,
    FuncDef,
    SymbolNode,
)
from mypy.options import Options
from mypy.state import state
from mypy.util import IdMapper

T = TypeVar("T")

JsonDict: _TypeAlias = Dict[str, Any]

# The set of all valid expressions that can currently be contained
# inside of a Literal[...].
#
# Literals can contain bytes and enum-values: we special-case both of these
# and store the value as a string. We rely on the fallback type that's also
# stored with the Literal to determine how a string is being used.
#
# TODO: confirm that we're happy with representing enums (and the
# other types) in the manner described above.
#
# Note: if we change the set of types included below, we must also
# make sure to audit the following methods:
#
# 1. types.LiteralType's serialize and deserialize methods: this method
#    needs to make sure it can convert the below types into JSON and back.
#
# 2. types.LiteralType's 'alue_repr` method: this method is ultimately used
#    by TypeStrVisitor's visit_literal_type to generate a reasonable
#    repr-able output.
#
# 3. server.astdiff.SnapshotTypeVisitor's visit_literal_type_method: this
#    method assumes that the following types supports equality checks and
#    hashability.
#
# Note: Although "Literal[None]" is a valid type, we internally always convert
# such a type directly into "None". So, "None" is not a valid parameter of
# LiteralType and is omitted from this list.
#
# Note: Float values are only used internally. They are not accepted within
# Literal[...].
LiteralValue: _TypeAlias = Union[int, str, bool, float]


# If we only import type_visitor in the middle of the file, mypy
# breaks, and if we do it at the top, it breaks at runtime because of
# import cycle issues, so we do it at the top while typechecking and
# then again in the middle at runtime.
# We should be able to remove this once we are switched to the new
# semantic analyzer!
if TYPE_CHECKING:
    from mypy.type_visitor import (
        SyntheticTypeVisitor as SyntheticTypeVisitor,
        TypeVisitor as TypeVisitor,
    )

TYPED_NAMEDTUPLE_NAMES: Final = ("typing.NamedTuple", "typing_extensions.NamedTuple")

# Supported names of TypedDict type constructors.
TPDICT_NAMES: Final = (
    "typing.TypedDict",
    "typing_extensions.TypedDict",
    "mypy_extensions.TypedDict",
)

# Supported fallback instance type names for TypedDict types.
TPDICT_FB_NAMES: Final = (
    "typing._TypedDict",
    "typing_extensions._TypedDict",
    "mypy_extensions._TypedDict",
)

# Supported names of Protocol base class.
PROTOCOL_NAMES: Final = ("typing.Protocol", "typing_extensions.Protocol")

# Supported TypeAlias names.
TYPE_ALIAS_NAMES: Final = ("typing.TypeAlias", "typing_extensions.TypeAlias")

# Supported Final type names.
FINAL_TYPE_NAMES: Final = ("typing.Final", "typing_extensions.Final")

# Supported @final decorator names.
FINAL_DECORATOR_NAMES: Final = ("typing.final", "typing_extensions.final")

# Supported Literal type names.
LITERAL_TYPE_NAMES: Final = ("typing.Literal", "typing_extensions.Literal")

# Supported Annotated type names.
ANNOTATED_TYPE_NAMES: Final = ("typing.Annotated", "typing_extensions.Annotated")

# We use this constant in various places when checking `tuple` subtyping:
TUPLE_LIKE_INSTANCE_NAMES: Final = (
    "builtins.tuple",
    "typing.Iterable",
    "typing.Container",
    "typing.Sequence",
    "typing.Reversible",
)

REVEAL_TYPE_NAMES: Final = (
    "builtins.reveal_type",
    "typing.reveal_type",
    "typing_extensions.reveal_type",
)

ASSERT_TYPE_NAMES: Final = ("typing.assert_type", "typing_extensions.assert_type")

OVERLOAD_NAMES: Final = ("typing.overload", "typing_extensions.overload")

# Attributes that can optionally be defined in the body of a subclass of
# enum.Enum but are removed from the class __dict__ by EnumMeta.
ENUM_REMOVED_PROPS: Final = ("_ignore_", "_order_", "__order__")

NEVER_NAMES: Final = (
    "typing.NoReturn",
    "typing_extensions.NoReturn",
    "mypy_extensions.NoReturn",
    "typing.Never",
    "typing_extensions.Never",
)

# Mypyc fixed-width native int types (compatible with builtins.int)
MYPYC_NATIVE_INT_NAMES: Final = (
    "mypy_extensions.i64",
    "mypy_extensions.i32",
    "mypy_extensions.i16",
    "mypy_extensions.u8",
)

DATACLASS_TRANSFORM_NAMES: Final = (
    "typing.dataclass_transform",
    "typing_extensions.dataclass_transform",
)
# Supported @override decorator names.
OVERRIDE_DECORATOR_NAMES: Final = ("typing.override", "typing_extensions.override")

# A placeholder used for Bogus[...] parameters
_dummy: Final[Any] = object()

# A placeholder for int parameters
_dummy_int: Final = -999999


</t>
<t tx="ekr.20230831011821.568">class TypeOfAny:
    """
    This class describes different types of Any. Each 'Any' can be of only one type at a time.
    """

    __slots__ = ()

    # Was this Any type inferred without a type annotation?
    unannotated: Final = 1
    # Does this Any come from an explicit type annotation?
    explicit: Final = 2
    # Does this come from an unfollowed import? See --disallow-any-unimported option
    from_unimported_type: Final = 3
    # Does this Any type come from omitted generics?
    from_omitted_generics: Final = 4
    # Does this Any come from an error?
    from_error: Final = 5
    # Is this a type that can't be represented in mypy's type system? For instance, type of
    # call to NewType(...). Even though these types aren't real Anys, we treat them as such.
    # Also used for variables named '_'.
    special_form: Final = 6
    # Does this Any come from interaction with another Any?
    from_another_any: Final = 7
    # Does this Any come from an implementation limitation/bug?
    implementation_artifact: Final = 8
    # Does this Any come from use in the suggestion engine?  This is
    # used to ignore Anys inserted by the suggestion engine when
    # generating constraints.
    suggestion_engine: Final = 9


</t>
<t tx="ekr.20230831011821.569">def deserialize_type(data: JsonDict | str) -&gt; Type:
    if isinstance(data, str):
        return Instance.deserialize(data)
    classname = data[".class"]
    method = deserialize_map.get(classname)
    if method is not None:
        return method(data)
    raise NotImplementedError(f"unexpected .class {classname}")


</t>
<t tx="ekr.20230831011821.57">def visit_class_pattern(self, o: ClassPattern) -&gt; None:
    o.class_ref.accept(self)
    for p in o.positionals:
        p.accept(self)
    for v in o.keyword_values:
        v.accept(self)

</t>
<t tx="ekr.20230831011821.570">class Type(mypy.nodes.Context):
    """Abstract base class for all types."""

    @others
</t>
<t tx="ekr.20230831011821.571">__slots__ = ("_can_be_true", "_can_be_false")
# 'can_be_true' and 'can_be_false' mean whether the value of the
# expression can be true or false in a boolean context. They are useful
# when inferring the type of logic expressions like `x and y`.
#
# For example:
#   * the literal `False` can't be true while `True` can.
#   * a value with type `bool` can be true or false.
#   * `None` can't be true
#   * ...

def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    # Value of these can be -1 (use the default, lazy init), 0 (false) or 1 (true)
    self._can_be_true = -1
    self._can_be_false = -1

</t>
<t tx="ekr.20230831011821.572">@property
def can_be_true(self) -&gt; bool:
    if self._can_be_true == -1:  # Lazy init helps mypyc
        self._can_be_true = self.can_be_true_default()
    return bool(self._can_be_true)

</t>
<t tx="ekr.20230831011821.573">@can_be_true.setter
def can_be_true(self, v: bool) -&gt; None:
    self._can_be_true = v

</t>
<t tx="ekr.20230831011821.574">@property
def can_be_false(self) -&gt; bool:
    if self._can_be_false == -1:  # Lazy init helps mypyc
        self._can_be_false = self.can_be_false_default()
    return bool(self._can_be_false)

</t>
<t tx="ekr.20230831011821.575">@can_be_false.setter
def can_be_false(self, v: bool) -&gt; None:
    self._can_be_false = v

</t>
<t tx="ekr.20230831011821.576">def can_be_true_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011821.577">def can_be_false_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011821.578">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented", type(self))

</t>
<t tx="ekr.20230831011821.579">def __repr__(self) -&gt; str:
    return self.accept(TypeStrVisitor(options=Options()))

</t>
<t tx="ekr.20230831011821.58">def visit_import(self, o: Import) -&gt; None:
    for a in o.assignments:
        a.accept(self)

</t>
<t tx="ekr.20230831011821.580">def str_with_options(self, options: Options) -&gt; str:
    return self.accept(TypeStrVisitor(options=options))

</t>
<t tx="ekr.20230831011821.581">def serialize(self) -&gt; JsonDict | str:
    raise NotImplementedError(f"Cannot serialize {self.__class__.__name__} instance")

</t>
<t tx="ekr.20230831011821.582">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Type:
    raise NotImplementedError(f"Cannot deserialize {cls.__name__} instance")

</t>
<t tx="ekr.20230831011821.583">def is_singleton_type(self) -&gt; bool:
    return False


</t>
<t tx="ekr.20230831011821.584">class TypeAliasType(Type):
    """A type alias to another type.

    To support recursive type aliases we don't immediately expand a type alias
    during semantic analysis, but create an instance of this type that records the target alias
    definition node (mypy.nodes.TypeAlias) and type arguments (for generic aliases).

    This is very similar to how TypeInfo vs Instance interact, where a recursive class-based
    structure like
        class Node:
            value: int
            children: List[Node]
    can be represented in a tree-like manner.
    """

    @others
</t>
<t tx="ekr.20230831011821.585">__slots__ = ("alias", "args", "type_ref")

def __init__(
    self,
    alias: mypy.nodes.TypeAlias | None,
    args: list[Type],
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.alias = alias
    self.args = args
    self.type_ref: str | None = None

</t>
<t tx="ekr.20230831011821.586">def _expand_once(self) -&gt; Type:
    """Expand to the target type exactly once.

    This doesn't do full expansion, i.e. the result can contain another
    (or even this same) type alias. Use this internal helper only when really needed,
    its public wrapper mypy.types.get_proper_type() is preferred.
    """
    assert self.alias is not None
    if self.alias.no_args:
        # We know that no_args=True aliases like L = List must have an instance
        # as their target.
        assert isinstance(self.alias.target, Instance)  # type: ignore[misc]
        return self.alias.target.copy_modified(args=self.args)

    # TODO: this logic duplicates the one in expand_type_by_instance().
    if self.alias.tvar_tuple_index is None:
        mapping = {v.id: s for (v, s) in zip(self.alias.alias_tvars, self.args)}
    else:
        prefix = self.alias.tvar_tuple_index
        suffix = len(self.alias.alias_tvars) - self.alias.tvar_tuple_index - 1
        start, middle, end = split_with_prefix_and_suffix(tuple(self.args), prefix, suffix)
        tvar = self.alias.alias_tvars[prefix]
        assert isinstance(tvar, TypeVarTupleType)
        mapping = {tvar.id: TupleType(list(middle), tvar.tuple_fallback)}
        for tvar, sub in zip(
            self.alias.alias_tvars[:prefix] + self.alias.alias_tvars[prefix + 1 :], start + end
        ):
            mapping[tvar.id] = sub

    new_tp = self.alias.target.accept(InstantiateAliasVisitor(mapping))
    new_tp.accept(LocationSetter(self.line, self.column))
    new_tp.line = self.line
    new_tp.column = self.column
    return new_tp

</t>
<t tx="ekr.20230831011821.587">def _partial_expansion(self, nothing_args: bool = False) -&gt; tuple[ProperType, bool]:
    # Private method mostly for debugging and testing.
    unroller = UnrollAliasVisitor(set())
    if nothing_args:
        alias = self.copy_modified(args=[UninhabitedType()] * len(self.args))
    else:
        alias = self
    unrolled = alias.accept(unroller)
    assert isinstance(unrolled, ProperType)
    return unrolled, unroller.recursed

</t>
<t tx="ekr.20230831011821.588">def expand_all_if_possible(self, nothing_args: bool = False) -&gt; ProperType | None:
    """Attempt a full expansion of the type alias (including nested aliases).

    If the expansion is not possible, i.e. the alias is (mutually-)recursive,
    return None. If nothing_args is True, replace all type arguments with an
    UninhabitedType() (used to detect recursively defined aliases).
    """
    unrolled, recursed = self._partial_expansion(nothing_args=nothing_args)
    if recursed:
        return None
    return unrolled

</t>
<t tx="ekr.20230831011821.589">@property
def is_recursive(self) -&gt; bool:
    """Whether this type alias is recursive.

    Note this doesn't check generic alias arguments, but only if this alias
    *definition* is recursive. The property value thus can be cached on the
    underlying TypeAlias node. If you want to include all nested types, use
    has_recursive_types() function.
    """
    assert self.alias is not None, "Unfixed type alias"
    is_recursive = self.alias._is_recursive
    if is_recursive is None:
        is_recursive = self.expand_all_if_possible(nothing_args=True) is None
        # We cache the value on the underlying TypeAlias node as an optimization,
        # since the value is the same for all instances of the same alias.
        self.alias._is_recursive = is_recursive
    return is_recursive

</t>
<t tx="ekr.20230831011821.59">def visit_import_from(self, o: ImportFrom) -&gt; None:
    for a in o.assignments:
        a.accept(self)


</t>
<t tx="ekr.20230831011821.590">def can_be_true_default(self) -&gt; bool:
    if self.alias is not None:
        return self.alias.target.can_be_true
    return super().can_be_true_default()

</t>
<t tx="ekr.20230831011821.591">def can_be_false_default(self) -&gt; bool:
    if self.alias is not None:
        return self.alias.target.can_be_false
    return super().can_be_false_default()

</t>
<t tx="ekr.20230831011821.592">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_alias_type(self)

</t>
<t tx="ekr.20230831011821.593">def __hash__(self) -&gt; int:
    return hash((self.alias, tuple(self.args)))

</t>
<t tx="ekr.20230831011821.594">def __eq__(self, other: object) -&gt; bool:
    # Note: never use this to determine subtype relationships, use is_subtype().
    if not isinstance(other, TypeAliasType):
        return NotImplemented
    return self.alias == other.alias and self.args == other.args

</t>
<t tx="ekr.20230831011821.595">def serialize(self) -&gt; JsonDict:
    assert self.alias is not None
    data: JsonDict = {
        ".class": "TypeAliasType",
        "type_ref": self.alias.fullname,
        "args": [arg.serialize() for arg in self.args],
    }
    return data

</t>
<t tx="ekr.20230831011821.596">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeAliasType:
    assert data[".class"] == "TypeAliasType"
    args: list[Type] = []
    if "args" in data:
        args_list = data["args"]
        assert isinstance(args_list, list)
        args = [deserialize_type(arg) for arg in args_list]
    alias = TypeAliasType(None, args)
    alias.type_ref = data["type_ref"]
    return alias

</t>
<t tx="ekr.20230831011821.597">def copy_modified(self, *, args: list[Type] | None = None) -&gt; TypeAliasType:
    return TypeAliasType(
        self.alias, args if args is not None else self.args.copy(), self.line, self.column
    )


</t>
<t tx="ekr.20230831011821.598">class TypeGuardedType(Type):
    """Only used by find_isinstance_check() etc."""

    @others
</t>
<t tx="ekr.20230831011821.599">__slots__ = ("type_guard",)

def __init__(self, type_guard: Type):
    super().__init__(line=type_guard.line, column=type_guard.column)
    self.type_guard = type_guard

</t>
<t tx="ekr.20230831011821.6">def visit_func(self, o: FuncItem) -&gt; None:
    if o.arguments is not None:
        for arg in o.arguments:
            init = arg.initializer
            if init is not None:
                init.accept(self)

        for arg in o.arguments:
            self.visit_var(arg.variable)

    o.body.accept(self)

</t>
<t tx="ekr.20230831011821.60">class ExtendedTraverserVisitor(TraverserVisitor):
    """This is a more flexible traverser.

    In addition to the base traverser it:
        * has visit_ methods for leaf nodes
        * has common method that is called for all nodes
        * allows to skip recursing into a node

    Note that this traverser still doesn't visit some internal
    mypy constructs like _promote expression and Var.
    """

    @others
</t>
<t tx="ekr.20230831011821.600">def __repr__(self) -&gt; str:
    return f"TypeGuard({self.type_guard})"


</t>
<t tx="ekr.20230831011821.601">class RequiredType(Type):
    """Required[T] or NotRequired[T]. Only usable at top-level of a TypedDict definition."""

    @others
</t>
<t tx="ekr.20230831011821.602">def __init__(self, item: Type, *, required: bool) -&gt; None:
    super().__init__(line=item.line, column=item.column)
    self.item = item
    self.required = required

</t>
<t tx="ekr.20230831011821.603">def __repr__(self) -&gt; str:
    if self.required:
        return f"Required[{self.item}]"
    else:
        return f"NotRequired[{self.item}]"

</t>
<t tx="ekr.20230831011821.604">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return self.item.accept(visitor)


</t>
<t tx="ekr.20230831011821.605">class ProperType(Type):
    """Not a type alias.

    Every type except TypeAliasType must inherit from this type.
    """

    __slots__ = ()


</t>
<t tx="ekr.20230831011821.606">class TypeVarId:
    @others
</t>
<t tx="ekr.20230831011821.607"># A type variable is uniquely identified by its raw id and meta level.

# For plain variables (type parameters of generic classes and
# functions) raw ids are allocated by semantic analysis, using
# positive ids 1, 2, ... for generic class parameters and negative
# ids -1, ... for generic function type arguments. A special value 0
# is reserved for Self type variable (autogenerated). This convention
# is only used to keep type variable ids distinct when allocating
# them; the type checker makes no distinction between class and
# function type variables.

# Metavariables are allocated unique ids starting from 1.
raw_id: int = 0

# Level of the variable in type inference. Currently either 0 for
# declared types, or 1 for type inference metavariables.
meta_level: int = 0

# Class variable used for allocating fresh ids for metavariables.
next_raw_id: ClassVar[int] = 1

# Fullname of class (or potentially function in the future) which
# declares this type variable (not the fullname of the TypeVar
# definition!), or ''
namespace: str

def __init__(self, raw_id: int, meta_level: int = 0, *, namespace: str = "") -&gt; None:
    self.raw_id = raw_id
    self.meta_level = meta_level
    self.namespace = namespace

</t>
<t tx="ekr.20230831011821.608">@staticmethod
def new(meta_level: int) -&gt; TypeVarId:
    raw_id = TypeVarId.next_raw_id
    TypeVarId.next_raw_id += 1
    return TypeVarId(raw_id, meta_level)

</t>
<t tx="ekr.20230831011821.609">def __repr__(self) -&gt; str:
    return self.raw_id.__repr__()

</t>
<t tx="ekr.20230831011821.61">def visit(self, o: Node) -&gt; bool:
    # If returns True, will continue to nested nodes.
    return True

</t>
<t tx="ekr.20230831011821.610">def __eq__(self, other: object) -&gt; bool:
    return (
        isinstance(other, TypeVarId)
        and self.raw_id == other.raw_id
        and self.meta_level == other.meta_level
        and self.namespace == other.namespace
    )

</t>
<t tx="ekr.20230831011821.611">def __ne__(self, other: object) -&gt; bool:
    return not (self == other)

</t>
<t tx="ekr.20230831011821.612">def __hash__(self) -&gt; int:
    return hash((self.raw_id, self.meta_level, self.namespace))

</t>
<t tx="ekr.20230831011821.613">def is_meta_var(self) -&gt; bool:
    return self.meta_level &gt; 0


</t>
<t tx="ekr.20230831011821.614">class TypeVarLikeType(ProperType):
    @others
</t>
<t tx="ekr.20230831011821.615">__slots__ = ("name", "fullname", "id", "upper_bound", "default")

name: str  # Name (may be qualified)
fullname: str  # Fully qualified name
id: TypeVarId
upper_bound: Type
default: Type

def __init__(
    self,
    name: str,
    fullname: str,
    id: TypeVarId | int,
    upper_bound: Type,
    default: Type,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.name = name
    self.fullname = fullname
    if isinstance(id, int):
        id = TypeVarId(id)
    self.id = id
    self.upper_bound = upper_bound
    self.default = default

</t>
<t tx="ekr.20230831011821.616">def serialize(self) -&gt; JsonDict:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011821.617">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarLikeType:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011821.618">def copy_modified(self, *, id: TypeVarId, **kwargs: Any) -&gt; Self:
    raise NotImplementedError

</t>
<t tx="ekr.20230831011821.619">@classmethod
def new_unification_variable(cls, old: Self) -&gt; Self:
    new_id = TypeVarId.new(meta_level=1)
    return old.copy_modified(id=new_id)

</t>
<t tx="ekr.20230831011821.62">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    if not self.visit(o):
        return
    super().visit_mypy_file(o)

</t>
<t tx="ekr.20230831011821.620">def has_default(self) -&gt; bool:
    t = get_proper_type(self.default)
    return not (isinstance(t, AnyType) and t.type_of_any == TypeOfAny.from_omitted_generics)


</t>
<t tx="ekr.20230831011821.621">class TypeVarType(TypeVarLikeType):
    """Type that refers to a type variable."""

    @others
</t>
<t tx="ekr.20230831011821.622">__slots__ = ("values", "variance")

values: list[Type]  # Value restriction, empty list if no restriction
variance: int

def __init__(
    self,
    name: str,
    fullname: str,
    id: TypeVarId | int,
    values: list[Type],
    upper_bound: Type,
    default: Type,
    variance: int = INVARIANT,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(name, fullname, id, upper_bound, default, line, column)
    assert values is not None, "No restrictions must be represented by empty list"
    self.values = values
    self.variance = variance

</t>
<t tx="ekr.20230831011821.623">def copy_modified(
    self,
    *,
    values: Bogus[list[Type]] = _dummy,
    upper_bound: Bogus[Type] = _dummy,
    default: Bogus[Type] = _dummy,
    id: Bogus[TypeVarId | int] = _dummy,
    line: int = _dummy_int,
    column: int = _dummy_int,
    **kwargs: Any,
) -&gt; TypeVarType:
    return TypeVarType(
        name=self.name,
        fullname=self.fullname,
        id=self.id if id is _dummy else id,
        values=self.values if values is _dummy else values,
        upper_bound=self.upper_bound if upper_bound is _dummy else upper_bound,
        default=self.default if default is _dummy else default,
        variance=self.variance,
        line=self.line if line == _dummy_int else line,
        column=self.column if column == _dummy_int else column,
    )

</t>
<t tx="ekr.20230831011821.624">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_var(self)

</t>
<t tx="ekr.20230831011821.625">def __hash__(self) -&gt; int:
    return hash((self.id, self.upper_bound, tuple(self.values)))

</t>
<t tx="ekr.20230831011821.626">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeVarType):
        return NotImplemented
    return (
        self.id == other.id
        and self.upper_bound == other.upper_bound
        and self.values == other.values
    )

</t>
<t tx="ekr.20230831011821.627">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {
        ".class": "TypeVarType",
        "name": self.name,
        "fullname": self.fullname,
        "id": self.id.raw_id,
        "namespace": self.id.namespace,
        "values": [v.serialize() for v in self.values],
        "upper_bound": self.upper_bound.serialize(),
        "default": self.default.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20230831011821.628">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarType:
    assert data[".class"] == "TypeVarType"
    return TypeVarType(
        name=data["name"],
        fullname=data["fullname"],
        id=TypeVarId(data["id"], namespace=data["namespace"]),
        values=[deserialize_type(v) for v in data["values"]],
        upper_bound=deserialize_type(data["upper_bound"]),
        default=deserialize_type(data["default"]),
        variance=data["variance"],
    )


</t>
<t tx="ekr.20230831011821.629">class ParamSpecFlavor:
    # Simple ParamSpec reference such as "P"
    BARE: Final = 0
    # P.args
    ARGS: Final = 1
    # P.kwargs
    KWARGS: Final = 2


</t>
<t tx="ekr.20230831011821.63"># Module structure

def visit_import(self, o: Import) -&gt; None:
    if not self.visit(o):
        return
    super().visit_import(o)

</t>
<t tx="ekr.20230831011821.630">class ParamSpecType(TypeVarLikeType):
    """Type that refers to a ParamSpec.

    A ParamSpec is a type variable that represents the parameter
    types, names and kinds of a callable (i.e., the signature without
    the return type).

    This can be one of these forms
     * P (ParamSpecFlavor.BARE)
     * P.args (ParamSpecFlavor.ARGS)
     * P.kwargs (ParamSpecFLavor.KWARGS)

    The upper_bound is really used as a fallback type -- it's shared
    with TypeVarType for simplicity. It can't be specified by the user
    and the value is directly derived from the flavor (currently
    always just 'object').
    """

    @others
</t>
<t tx="ekr.20230831011821.631">__slots__ = ("flavor", "prefix")

flavor: int
prefix: Parameters

def __init__(
    self,
    name: str,
    fullname: str,
    id: TypeVarId | int,
    flavor: int,
    upper_bound: Type,
    default: Type,
    *,
    line: int = -1,
    column: int = -1,
    prefix: Parameters | None = None,
) -&gt; None:
    super().__init__(name, fullname, id, upper_bound, default, line=line, column=column)
    self.flavor = flavor
    self.prefix = prefix or Parameters([], [], [])

</t>
<t tx="ekr.20230831011821.632">def with_flavor(self, flavor: int) -&gt; ParamSpecType:
    return ParamSpecType(
        self.name,
        self.fullname,
        self.id,
        flavor,
        upper_bound=self.upper_bound,
        default=self.default,
        prefix=self.prefix,
    )

</t>
<t tx="ekr.20230831011821.633">def copy_modified(
    self,
    *,
    id: Bogus[TypeVarId | int] = _dummy,
    flavor: int = _dummy_int,
    prefix: Bogus[Parameters] = _dummy,
    default: Bogus[Type] = _dummy,
    **kwargs: Any,
) -&gt; ParamSpecType:
    return ParamSpecType(
        self.name,
        self.fullname,
        id if id is not _dummy else self.id,
        flavor if flavor != _dummy_int else self.flavor,
        self.upper_bound,
        default=default if default is not _dummy else self.default,
        line=self.line,
        column=self.column,
        prefix=prefix if prefix is not _dummy else self.prefix,
    )

</t>
<t tx="ekr.20230831011821.634">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_param_spec(self)

</t>
<t tx="ekr.20230831011821.635">def name_with_suffix(self) -&gt; str:
    n = self.name
    if self.flavor == ParamSpecFlavor.ARGS:
        return f"{n}.args"
    elif self.flavor == ParamSpecFlavor.KWARGS:
        return f"{n}.kwargs"
    return n

</t>
<t tx="ekr.20230831011821.636">def __hash__(self) -&gt; int:
    return hash((self.id, self.flavor, self.prefix))

</t>
<t tx="ekr.20230831011821.637">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, ParamSpecType):
        return NotImplemented
    # Upper bound can be ignored, since it's determined by flavor.
    return self.id == other.id and self.flavor == other.flavor and self.prefix == other.prefix

</t>
<t tx="ekr.20230831011821.638">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {
        ".class": "ParamSpecType",
        "name": self.name,
        "fullname": self.fullname,
        "id": self.id.raw_id,
        "flavor": self.flavor,
        "upper_bound": self.upper_bound.serialize(),
        "default": self.default.serialize(),
        "prefix": self.prefix.serialize(),
    }

</t>
<t tx="ekr.20230831011821.639">@classmethod
def deserialize(cls, data: JsonDict) -&gt; ParamSpecType:
    assert data[".class"] == "ParamSpecType"
    return ParamSpecType(
        data["name"],
        data["fullname"],
        data["id"],
        data["flavor"],
        deserialize_type(data["upper_bound"]),
        deserialize_type(data["default"]),
        prefix=Parameters.deserialize(data["prefix"]),
    )


</t>
<t tx="ekr.20230831011821.64">def visit_import_from(self, o: ImportFrom) -&gt; None:
    if not self.visit(o):
        return
    super().visit_import_from(o)

</t>
<t tx="ekr.20230831011821.640">class TypeVarTupleType(TypeVarLikeType):
    """Type that refers to a TypeVarTuple.

    See PEP646 for more information.
    """

    @others
</t>
<t tx="ekr.20230831011821.641">def __init__(
    self,
    name: str,
    fullname: str,
    id: TypeVarId | int,
    upper_bound: Type,
    tuple_fallback: Instance,
    default: Type,
    *,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(name, fullname, id, upper_bound, default, line=line, column=column)
    self.tuple_fallback = tuple_fallback

</t>
<t tx="ekr.20230831011821.642">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {
        ".class": "TypeVarTupleType",
        "name": self.name,
        "fullname": self.fullname,
        "id": self.id.raw_id,
        "upper_bound": self.upper_bound.serialize(),
        "tuple_fallback": self.tuple_fallback.serialize(),
        "default": self.default.serialize(),
    }

</t>
<t tx="ekr.20230831011821.643">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarTupleType:
    assert data[".class"] == "TypeVarTupleType"
    return TypeVarTupleType(
        data["name"],
        data["fullname"],
        data["id"],
        deserialize_type(data["upper_bound"]),
        Instance.deserialize(data["tuple_fallback"]),
        deserialize_type(data["default"]),
    )

</t>
<t tx="ekr.20230831011821.644">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_var_tuple(self)

</t>
<t tx="ekr.20230831011821.645">def __hash__(self) -&gt; int:
    return hash(self.id)

</t>
<t tx="ekr.20230831011821.646">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeVarTupleType):
        return NotImplemented
    return self.id == other.id

</t>
<t tx="ekr.20230831011821.647">def copy_modified(
    self,
    *,
    id: Bogus[TypeVarId | int] = _dummy,
    upper_bound: Bogus[Type] = _dummy,
    default: Bogus[Type] = _dummy,
    **kwargs: Any,
) -&gt; TypeVarTupleType:
    return TypeVarTupleType(
        self.name,
        self.fullname,
        self.id if id is _dummy else id,
        self.upper_bound if upper_bound is _dummy else upper_bound,
        self.tuple_fallback,
        self.default if default is _dummy else default,
        line=self.line,
        column=self.column,
    )


</t>
<t tx="ekr.20230831011821.648">class UnboundType(ProperType):
    """Instance type that has not been bound during semantic analysis."""

    @others
</t>
<t tx="ekr.20230831011821.649">__slots__ = (
    "name",
    "args",
    "optional",
    "empty_tuple_index",
    "original_str_expr",
    "original_str_fallback",
)

def __init__(
    self,
    name: str | None,
    args: Sequence[Type] | None = None,
    line: int = -1,
    column: int = -1,
    optional: bool = False,
    empty_tuple_index: bool = False,
    original_str_expr: str | None = None,
    original_str_fallback: str | None = None,
) -&gt; None:
    super().__init__(line, column)
    if not args:
        args = []
    assert name is not None
    self.name = name
    self.args = tuple(args)
    # Should this type be wrapped in an Optional?
    self.optional = optional
    # Special case for X[()]
    self.empty_tuple_index = empty_tuple_index
    # If this UnboundType was originally defined as a str or bytes, keep track of
    # the original contents of that string-like thing. This way, if this UnboundExpr
    # ever shows up inside of a LiteralType, we can determine whether that
    # Literal[...] is valid or not. E.g. Literal[foo] is most likely invalid
    # (unless 'foo' is an alias for another literal or something) and
    # Literal["foo"] most likely is.
    #
    # We keep track of the entire string instead of just using a boolean flag
    # so we can distinguish between things like Literal["foo"] vs
    # Literal["    foo   "].
    #
    # We also keep track of what the original base fallback type was supposed to be
    # so we don't have to try and recompute it later
    self.original_str_expr = original_str_expr
    self.original_str_fallback = original_str_fallback

</t>
<t tx="ekr.20230831011821.65">def visit_import_all(self, o: ImportAll) -&gt; None:
    if not self.visit(o):
        return
    super().visit_import_all(o)

</t>
<t tx="ekr.20230831011821.650">def copy_modified(self, args: Bogus[Sequence[Type] | None] = _dummy) -&gt; UnboundType:
    if args is _dummy:
        args = self.args
    return UnboundType(
        name=self.name,
        args=args,
        line=self.line,
        column=self.column,
        optional=self.optional,
        empty_tuple_index=self.empty_tuple_index,
        original_str_expr=self.original_str_expr,
        original_str_fallback=self.original_str_fallback,
    )

</t>
<t tx="ekr.20230831011821.651">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_unbound_type(self)

</t>
<t tx="ekr.20230831011821.652">def __hash__(self) -&gt; int:
    return hash((self.name, self.optional, tuple(self.args), self.original_str_expr))

</t>
<t tx="ekr.20230831011821.653">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, UnboundType):
        return NotImplemented
    return (
        self.name == other.name
        and self.optional == other.optional
        and self.args == other.args
        and self.original_str_expr == other.original_str_expr
        and self.original_str_fallback == other.original_str_fallback
    )

</t>
<t tx="ekr.20230831011821.654">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "UnboundType",
        "name": self.name,
        "args": [a.serialize() for a in self.args],
        "expr": self.original_str_expr,
        "expr_fallback": self.original_str_fallback,
    }

</t>
<t tx="ekr.20230831011821.655">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UnboundType:
    assert data[".class"] == "UnboundType"
    return UnboundType(
        data["name"],
        [deserialize_type(a) for a in data["args"]],
        original_str_expr=data["expr"],
        original_str_fallback=data["expr_fallback"],
    )


</t>
<t tx="ekr.20230831011821.656">class CallableArgument(ProperType):
    """Represents a Arg(type, 'name') inside a Callable's type list.

    Note that this is a synthetic type for helping parse ASTs, not a real type.
    """

    @others
</t>
<t tx="ekr.20230831011821.657">__slots__ = ("typ", "name", "constructor")

typ: Type
name: str | None
constructor: str | None

def __init__(
    self,
    typ: Type,
    name: str | None,
    constructor: str | None,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.typ = typ
    self.name = name
    self.constructor = constructor

</t>
<t tx="ekr.20230831011821.658">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    ret: T = visitor.visit_callable_argument(self)
    return ret

</t>
<t tx="ekr.20230831011821.659">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20230831011821.66"># Definitions

def visit_func_def(self, o: FuncDef) -&gt; None:
    if not self.visit(o):
        return
    super().visit_func_def(o)

</t>
<t tx="ekr.20230831011821.660">class TypeList(ProperType):
    """Information about argument types and names [...].

    This is used for the arguments of a Callable type, i.e. for
    [arg, ...] in Callable[[arg, ...], ret]. This is not a real type
    but a syntactic AST construct. UnboundTypes can also have TypeList
    types before they are processed into Callable types.
    """

    @others
</t>
<t tx="ekr.20230831011821.661">__slots__ = ("items",)

items: list[Type]

def __init__(self, items: list[Type], line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.items = items

</t>
<t tx="ekr.20230831011821.662">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    ret: T = visitor.visit_type_list(self)
    return ret

</t>
<t tx="ekr.20230831011821.663">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"

</t>
<t tx="ekr.20230831011821.664">def __hash__(self) -&gt; int:
    return hash(tuple(self.items))

</t>
<t tx="ekr.20230831011821.665">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, TypeList) and self.items == other.items


</t>
<t tx="ekr.20230831011821.666">class UnpackType(ProperType):
    """Type operator Unpack from PEP646. Can be either with Unpack[]
    or unpacking * syntax.

    The inner type should be either a TypeVarTuple, a constant size
    tuple, or a variable length tuple. Type aliases to these are not allowed,
    except during semantic analysis.
    """

    @others
</t>
<t tx="ekr.20230831011821.667">__slots__ = ["type"]

def __init__(self, typ: Type, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.type = typ

</t>
<t tx="ekr.20230831011821.668">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_unpack_type(self)

</t>
<t tx="ekr.20230831011821.669">def serialize(self) -&gt; JsonDict:
    return {".class": "UnpackType", "type": self.type.serialize()}

</t>
<t tx="ekr.20230831011821.67">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    if not self.visit(o):
        return
    super().visit_overloaded_func_def(o)

</t>
<t tx="ekr.20230831011821.670">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UnpackType:
    assert data[".class"] == "UnpackType"
    typ = data["type"]
    return UnpackType(deserialize_type(typ))

</t>
<t tx="ekr.20230831011821.671">def __hash__(self) -&gt; int:
    return hash(self.type)

</t>
<t tx="ekr.20230831011821.672">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, UnpackType) and self.type == other.type


</t>
<t tx="ekr.20230831011821.673">class AnyType(ProperType):
    """The type 'Any'."""

    @others
</t>
<t tx="ekr.20230831011821.674">__slots__ = ("type_of_any", "source_any", "missing_import_name")

def __init__(
    self,
    type_of_any: int,
    source_any: AnyType | None = None,
    missing_import_name: str | None = None,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.type_of_any = type_of_any
    # If this Any was created as a result of interacting with another 'Any', record the source
    # and use it in reports.
    self.source_any = source_any
    if source_any and source_any.source_any:
        self.source_any = source_any.source_any

    if source_any is None:
        self.missing_import_name = missing_import_name
    else:
        self.missing_import_name = source_any.missing_import_name

    # Only unimported type anys and anys from other anys should have an import name
    assert missing_import_name is None or type_of_any in (
        TypeOfAny.from_unimported_type,
        TypeOfAny.from_another_any,
    )
    # Only Anys that come from another Any can have source_any.
    assert type_of_any != TypeOfAny.from_another_any or source_any is not None
    # We should not have chains of Anys.
    assert not self.source_any or self.source_any.type_of_any != TypeOfAny.from_another_any

</t>
<t tx="ekr.20230831011821.675">@property
def is_from_error(self) -&gt; bool:
    return self.type_of_any == TypeOfAny.from_error

</t>
<t tx="ekr.20230831011821.676">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_any(self)

</t>
<t tx="ekr.20230831011821.677">def copy_modified(
    self,
    # Mark with Bogus because _dummy is just an object (with type Any)
    type_of_any: int = _dummy_int,
    original_any: Bogus[AnyType | None] = _dummy,
) -&gt; AnyType:
    if type_of_any == _dummy_int:
        type_of_any = self.type_of_any
    if original_any is _dummy:
        original_any = self.source_any
    return AnyType(
        type_of_any=type_of_any,
        source_any=original_any,
        missing_import_name=self.missing_import_name,
        line=self.line,
        column=self.column,
    )

</t>
<t tx="ekr.20230831011821.678">def __hash__(self) -&gt; int:
    return hash(AnyType)

</t>
<t tx="ekr.20230831011821.679">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, AnyType)

</t>
<t tx="ekr.20230831011821.68">def visit_class_def(self, o: ClassDef) -&gt; None:
    if not self.visit(o):
        return
    super().visit_class_def(o)

</t>
<t tx="ekr.20230831011821.680">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "AnyType",
        "type_of_any": self.type_of_any,
        "source_any": self.source_any.serialize() if self.source_any is not None else None,
        "missing_import_name": self.missing_import_name,
    }

</t>
<t tx="ekr.20230831011821.681">@classmethod
def deserialize(cls, data: JsonDict) -&gt; AnyType:
    assert data[".class"] == "AnyType"
    source = data["source_any"]
    return AnyType(
        data["type_of_any"],
        AnyType.deserialize(source) if source is not None else None,
        data["missing_import_name"],
    )


</t>
<t tx="ekr.20230831011821.682">class UninhabitedType(ProperType):
    """This type has no members.

    This type is the bottom type.
    With strict Optional checking, it is the only common subtype between all
    other types, which allows `meet` to be well defined.  Without strict
    Optional checking, NoneType fills this role.

    In general, for any type T:
        join(UninhabitedType, T) = T
        meet(UninhabitedType, T) = UninhabitedType
        is_subtype(UninhabitedType, T) = True
    """

    @others
</t>
<t tx="ekr.20230831011821.683">__slots__ = ("ambiguous", "is_noreturn")

is_noreturn: bool  # Does this come from a NoReturn?  Purely for error messages.
# It is important to track whether this is an actual NoReturn type, or just a result
# of ambiguous type inference, in the latter case we don't want to mark a branch as
# unreachable in binder.
ambiguous: bool  # Is this a result of inference for a variable without constraints?

def __init__(self, is_noreturn: bool = False, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.is_noreturn = is_noreturn
    self.ambiguous = False

</t>
<t tx="ekr.20230831011821.684">def can_be_true_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20230831011821.685">def can_be_false_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20230831011821.686">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_uninhabited_type(self)

</t>
<t tx="ekr.20230831011821.687">def __hash__(self) -&gt; int:
    return hash(UninhabitedType)

</t>
<t tx="ekr.20230831011821.688">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, UninhabitedType)

</t>
<t tx="ekr.20230831011821.689">def serialize(self) -&gt; JsonDict:
    return {".class": "UninhabitedType", "is_noreturn": self.is_noreturn}

</t>
<t tx="ekr.20230831011821.69">def visit_global_decl(self, o: GlobalDecl) -&gt; None:
    if not self.visit(o):
        return
    super().visit_global_decl(o)

</t>
<t tx="ekr.20230831011821.690">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UninhabitedType:
    assert data[".class"] == "UninhabitedType"
    return UninhabitedType(is_noreturn=data["is_noreturn"])


</t>
<t tx="ekr.20230831011821.691">class NoneType(ProperType):
    """The type of 'None'.

    This type can be written by users as 'None'.
    """

    @others
</t>
<t tx="ekr.20230831011821.692">__slots__ = ()

def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)

</t>
<t tx="ekr.20230831011821.693">def can_be_true_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20230831011821.694">def __hash__(self) -&gt; int:
    return hash(NoneType)

</t>
<t tx="ekr.20230831011821.695">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, NoneType)

</t>
<t tx="ekr.20230831011821.696">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_none_type(self)

</t>
<t tx="ekr.20230831011821.697">def serialize(self) -&gt; JsonDict:
    return {".class": "NoneType"}

</t>
<t tx="ekr.20230831011821.698">@classmethod
def deserialize(cls, data: JsonDict) -&gt; NoneType:
    assert data[".class"] == "NoneType"
    return NoneType()

</t>
<t tx="ekr.20230831011821.699">def is_singleton_type(self) -&gt; bool:
    return True


</t>
<t tx="ekr.20230831011821.7">def visit_func_def(self, o: FuncDef) -&gt; None:
    self.visit_func(o)

</t>
<t tx="ekr.20230831011821.70">def visit_nonlocal_decl(self, o: NonlocalDecl) -&gt; None:
    if not self.visit(o):
        return
    super().visit_nonlocal_decl(o)

</t>
<t tx="ekr.20230831011821.700"># NoneType used to be called NoneTyp so to avoid needlessly breaking
# external plugins we keep that alias here.
NoneTyp = NoneType


class ErasedType(ProperType):
    """Placeholder for an erased type.

    This is used during type inference. This has the special property that
    it is ignored during type inference.
    """

    @others
</t>
<t tx="ekr.20230831011821.701">__slots__ = ()

def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_erased_type(self)


</t>
<t tx="ekr.20230831011821.702">class DeletedType(ProperType):
    """Type of deleted variables.

    These can be used as lvalues but not rvalues.
    """

    @others
</t>
<t tx="ekr.20230831011821.703">__slots__ = ("source",)

source: str | None  # May be None; name that generated this value

def __init__(self, source: str | None = None, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.source = source

</t>
<t tx="ekr.20230831011821.704">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_deleted_type(self)

</t>
<t tx="ekr.20230831011821.705">def serialize(self) -&gt; JsonDict:
    return {".class": "DeletedType", "source": self.source}

</t>
<t tx="ekr.20230831011821.706">@classmethod
def deserialize(cls, data: JsonDict) -&gt; DeletedType:
    assert data[".class"] == "DeletedType"
    return DeletedType(data["source"])


</t>
<t tx="ekr.20230831011821.707"># Fake TypeInfo to be used as a placeholder during Instance de-serialization.
NOT_READY: Final = mypy.nodes.FakeInfo("De-serialization failure: TypeInfo not fixed")


class ExtraAttrs:
    """Summary of module attributes and types.

    This is used for instances of types.ModuleType, because they can have different
    attributes per instance, and for type narrowing with hasattr() checks.
    """

    @others
</t>
<t tx="ekr.20230831011821.708">def __init__(
    self,
    attrs: dict[str, Type],
    immutable: set[str] | None = None,
    mod_name: str | None = None,
) -&gt; None:
    self.attrs = attrs
    if immutable is None:
        immutable = set()
    self.immutable = immutable
    self.mod_name = mod_name

</t>
<t tx="ekr.20230831011821.709">def __hash__(self) -&gt; int:
    return hash((tuple(self.attrs.items()), tuple(sorted(self.immutable))))

</t>
<t tx="ekr.20230831011821.71">def visit_decorator(self, o: Decorator) -&gt; None:
    if not self.visit(o):
        return
    super().visit_decorator(o)

</t>
<t tx="ekr.20230831011821.710">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, ExtraAttrs):
        return NotImplemented
    return self.attrs == other.attrs and self.immutable == other.immutable

</t>
<t tx="ekr.20230831011821.711">def copy(self) -&gt; ExtraAttrs:
    return ExtraAttrs(self.attrs.copy(), self.immutable.copy(), self.mod_name)

</t>
<t tx="ekr.20230831011821.712">def __repr__(self) -&gt; str:
    return f"ExtraAttrs({self.attrs!r}, {self.immutable!r}, {self.mod_name!r})"


</t>
<t tx="ekr.20230831011821.713">class Instance(ProperType):
    """An instance type of form C[T1, ..., Tn].

    The list of type variables may be empty.

    Several types have fallbacks to `Instance`, because in Python everything is an object
    and this concept is impossible to express without intersection types. We therefore use
    fallbacks for all "non-special" (like UninhabitedType, ErasedType etc) types.
    """

    @others
</t>
<t tx="ekr.20230831011821.714">__slots__ = ("type", "args", "invalid", "type_ref", "last_known_value", "_hash", "extra_attrs")

def __init__(
    self,
    typ: mypy.nodes.TypeInfo,
    args: Sequence[Type],
    line: int = -1,
    column: int = -1,
    *,
    last_known_value: LiteralType | None = None,
    extra_attrs: ExtraAttrs | None = None,
) -&gt; None:
    super().__init__(line, column)
    self.type = typ
    self.args = tuple(args)
    self.type_ref: str | None = None

    # True if recovered after incorrect number of type arguments error
    self.invalid = False

    # This field keeps track of the underlying Literal[...] value associated with
    # this instance, if one is known.
    #
    # This field is set whenever possible within expressions, but is erased upon
    # variable assignment (see erasetype.remove_instance_last_known_values) unless
    # the variable is declared to be final.
    #
    # For example, consider the following program:
    #
    #     a = 1
    #     b: Final[int] = 2
    #     c: Final = 3
    #     print(a + b + c + 4)
    #
    # The 'Instance' objects associated with the expressions '1', '2', '3', and '4' will
    # have last_known_values of type Literal[1], Literal[2], Literal[3], and Literal[4]
    # respectively. However, the Instance object assigned to 'a' and 'b' will have their
    # last_known_value erased: variable 'a' is mutable; variable 'b' was declared to be
    # specifically an int.
    #
    # Or more broadly, this field lets this Instance "remember" its original declaration
    # when applicable. We want this behavior because we want implicit Final declarations
    # to act pretty much identically with constants: we should be able to replace any
    # places where we use some Final variable with the original value and get the same
    # type-checking behavior. For example, we want this program:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    var: Final = 3
    #    expects_literal(var)
    #
    # ...to type-check in the exact same way as if we had written the program like this:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    expects_literal(3)
    #
    # In order to make this work (especially with literal types), we need var's type
    # (an Instance) to remember the "original" value.
    #
    # Preserving this value within expressions is useful for similar reasons.
    #
    # Currently most of mypy will ignore this field and will continue to treat this type like
    # a regular Instance. We end up using this field only when we are explicitly within a
    # Literal context.
    self.last_known_value = last_known_value

    # Cached hash value
    self._hash = -1

    # Additional attributes defined per instance of this type. For example modules
    # have different attributes per instance of types.ModuleType. This is intended
    # to be "short-lived", we don't serialize it, and even don't store as variable type.
    self.extra_attrs = extra_attrs

</t>
<t tx="ekr.20230831011821.715">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_instance(self)

</t>
<t tx="ekr.20230831011821.716">def __hash__(self) -&gt; int:
    if self._hash == -1:
        self._hash = hash((self.type, self.args, self.last_known_value, self.extra_attrs))
    return self._hash

</t>
<t tx="ekr.20230831011821.717">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Instance):
        return NotImplemented
    return (
        self.type == other.type
        and self.args == other.args
        and self.last_known_value == other.last_known_value
        and self.extra_attrs == other.extra_attrs
    )

</t>
<t tx="ekr.20230831011821.718">def serialize(self) -&gt; JsonDict | str:
    assert self.type is not None
    type_ref = self.type.fullname
    if not self.args and not self.last_known_value:
        return type_ref
    data: JsonDict = {".class": "Instance"}
    data["type_ref"] = type_ref
    data["args"] = [arg.serialize() for arg in self.args]
    if self.last_known_value is not None:
        data["last_known_value"] = self.last_known_value.serialize()
    return data

</t>
<t tx="ekr.20230831011821.719">@classmethod
def deserialize(cls, data: JsonDict | str) -&gt; Instance:
    if isinstance(data, str):
        inst = Instance(NOT_READY, [])
        inst.type_ref = data
        return inst
    assert data[".class"] == "Instance"
    args: list[Type] = []
    if "args" in data:
        args_list = data["args"]
        assert isinstance(args_list, list)
        args = [deserialize_type(arg) for arg in args_list]
    inst = Instance(NOT_READY, args)
    inst.type_ref = data["type_ref"]  # Will be fixed up by fixup.py later.
    if "last_known_value" in data:
        inst.last_known_value = LiteralType.deserialize(data["last_known_value"])
    return inst

</t>
<t tx="ekr.20230831011821.72">def visit_type_alias(self, o: TypeAlias) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_alias(o)

</t>
<t tx="ekr.20230831011821.720">def copy_modified(
    self,
    *,
    args: Bogus[list[Type]] = _dummy,
    last_known_value: Bogus[LiteralType | None] = _dummy,
) -&gt; Instance:
    new = Instance(
        self.type,
        args if args is not _dummy else self.args,
        self.line,
        self.column,
        last_known_value=last_known_value
        if last_known_value is not _dummy
        else self.last_known_value,
    )
    # We intentionally don't copy the extra_attrs here, so they will be erased.
    new.can_be_true = self.can_be_true
    new.can_be_false = self.can_be_false
    return new

</t>
<t tx="ekr.20230831011821.721">def copy_with_extra_attr(self, name: str, typ: Type) -&gt; Instance:
    if self.extra_attrs:
        existing_attrs = self.extra_attrs.copy()
    else:
        existing_attrs = ExtraAttrs({}, set(), None)
    existing_attrs.attrs[name] = typ
    new = self.copy_modified()
    new.extra_attrs = existing_attrs
    return new

</t>
<t tx="ekr.20230831011821.722">def is_singleton_type(self) -&gt; bool:
    # TODO:
    # Also make this return True if the type corresponds to NotImplemented?
    return (
        self.type.is_enum
        and len(self.get_enum_values()) == 1
        or self.type.fullname == "builtins.ellipsis"
    )

</t>
<t tx="ekr.20230831011821.723">def get_enum_values(self) -&gt; list[str]:
    """Return the list of values for an Enum."""
    return [
        name for name, sym in self.type.names.items() if isinstance(sym.node, mypy.nodes.Var)
    ]


</t>
<t tx="ekr.20230831011821.724">class FunctionLike(ProperType):
    """Abstract base class for function types."""

    @others
</t>
<t tx="ekr.20230831011821.725">__slots__ = ("fallback",)

fallback: Instance

def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self._can_be_false = False

</t>
<t tx="ekr.20230831011821.726">@abstractmethod
def is_type_obj(self) -&gt; bool:
    pass

</t>
<t tx="ekr.20230831011821.727">@abstractmethod
def type_object(self) -&gt; mypy.nodes.TypeInfo:
    pass

</t>
<t tx="ekr.20230831011821.728">@property
@abstractmethod
def items(self) -&gt; list[CallableType]:
    pass

</t>
<t tx="ekr.20230831011821.729">@abstractmethod
def with_name(self, name: str) -&gt; FunctionLike:
    pass

</t>
<t tx="ekr.20230831011821.73"># Statements

def visit_block(self, block: Block) -&gt; None:
    if not self.visit(block):
        return
    super().visit_block(block)

</t>
<t tx="ekr.20230831011821.730">@abstractmethod
def get_name(self) -&gt; str | None:
    pass


</t>
<t tx="ekr.20230831011821.731">class FormalArgument(NamedTuple):
    name: str | None
    pos: int | None
    typ: Type
    required: bool


</t>
<t tx="ekr.20230831011821.732">class Parameters(ProperType):
    """Type that represents the parameters to a function.

    Used for ParamSpec analysis."""

    @others
</t>
<t tx="ekr.20230831011821.733">__slots__ = (
    "arg_types",
    "arg_kinds",
    "arg_names",
    "min_args",
    "is_ellipsis_args",
    # TODO: variables don't really belong here, but they are used to allow hacky support
    # for forall . Foo[[x: T], T] by capturing generic callable with ParamSpec, see #15909
    "variables",
)

def __init__(
    self,
    arg_types: Sequence[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None],
    *,
    variables: Sequence[TypeVarLikeType] | None = None,
    is_ellipsis_args: bool = False,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.arg_types = list(arg_types)
    self.arg_kinds = arg_kinds
    self.arg_names = list(arg_names)
    assert len(arg_types) == len(arg_kinds) == len(arg_names)
    assert not any(isinstance(t, Parameters) for t in arg_types)
    self.min_args = arg_kinds.count(ARG_POS)
    self.is_ellipsis_args = is_ellipsis_args
    self.variables = variables or []

</t>
<t tx="ekr.20230831011821.734">def copy_modified(
    self,
    arg_types: Bogus[Sequence[Type]] = _dummy,
    arg_kinds: Bogus[list[ArgKind]] = _dummy,
    arg_names: Bogus[Sequence[str | None]] = _dummy,
    *,
    variables: Bogus[Sequence[TypeVarLikeType]] = _dummy,
    is_ellipsis_args: Bogus[bool] = _dummy,
) -&gt; Parameters:
    return Parameters(
        arg_types=arg_types if arg_types is not _dummy else self.arg_types,
        arg_kinds=arg_kinds if arg_kinds is not _dummy else self.arg_kinds,
        arg_names=arg_names if arg_names is not _dummy else self.arg_names,
        is_ellipsis_args=(
            is_ellipsis_args if is_ellipsis_args is not _dummy else self.is_ellipsis_args
        ),
        variables=variables if variables is not _dummy else self.variables,
    )

</t>
<t tx="ekr.20230831011821.735"># TODO: here is a lot of code duplication with Callable type, fix this.
def var_arg(self) -&gt; FormalArgument | None:
    """The formal argument for *args."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20230831011821.736">def kw_arg(self) -&gt; FormalArgument | None:
    """The formal argument for **kwargs."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR2:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20230831011821.737">def formal_arguments(self, include_star_args: bool = False) -&gt; list[FormalArgument]:
    """Yields the formal arguments corresponding to this callable, ignoring *arg and **kwargs.

    To handle *args and **kwargs, use the 'callable.var_args' and 'callable.kw_args' fields,
    if they are not None.

    If you really want to include star args in the yielded output, set the
    'include_star_args' parameter to 'True'."""
    args = []
    done_with_positional = False
    for i in range(len(self.arg_types)):
        kind = self.arg_kinds[i]
        if kind.is_named() or kind.is_star():
            done_with_positional = True
        if not include_star_args and kind.is_star():
            continue

        required = kind.is_required()
        pos = None if done_with_positional else i
        arg = FormalArgument(self.arg_names[i], pos, self.arg_types[i], required)
        args.append(arg)
    return args

</t>
<t tx="ekr.20230831011821.738">def argument_by_name(self, name: str | None) -&gt; FormalArgument | None:
    if name is None:
        return None
    seen_star = False
    for i, (arg_name, kind, typ) in enumerate(
        zip(self.arg_names, self.arg_kinds, self.arg_types)
    ):
        # No more positional arguments after these.
        if kind.is_named() or kind.is_star():
            seen_star = True
        if kind.is_star():
            continue
        if arg_name == name:
            position = None if seen_star else i
            return FormalArgument(name, position, typ, kind.is_required())
    return self.try_synthesizing_arg_from_kwarg(name)

</t>
<t tx="ekr.20230831011821.739">def argument_by_position(self, position: int | None) -&gt; FormalArgument | None:
    if position is None:
        return None
    if position &gt;= len(self.arg_names):
        return self.try_synthesizing_arg_from_vararg(position)
    name, kind, typ = (
        self.arg_names[position],
        self.arg_kinds[position],
        self.arg_types[position],
    )
    if kind.is_positional():
        return FormalArgument(name, position, typ, kind == ARG_POS)
    else:
        return self.try_synthesizing_arg_from_vararg(position)

</t>
<t tx="ekr.20230831011821.74">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_expression_stmt(o)

</t>
<t tx="ekr.20230831011821.740">def try_synthesizing_arg_from_kwarg(self, name: str | None) -&gt; FormalArgument | None:
    kw_arg = self.kw_arg()
    if kw_arg is not None:
        return FormalArgument(name, None, kw_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20230831011821.741">def try_synthesizing_arg_from_vararg(self, position: int | None) -&gt; FormalArgument | None:
    var_arg = self.var_arg()
    if var_arg is not None:
        return FormalArgument(None, position, var_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20230831011821.742">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_parameters(self)

</t>
<t tx="ekr.20230831011821.743">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "Parameters",
        "arg_types": [t.serialize() for t in self.arg_types],
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "arg_names": self.arg_names,
        "variables": [tv.serialize() for tv in self.variables],
    }

</t>
<t tx="ekr.20230831011821.744">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Parameters:
    assert data[".class"] == "Parameters"
    return Parameters(
        [deserialize_type(t) for t in data["arg_types"]],
        [ArgKind(x) for x in data["arg_kinds"]],
        data["arg_names"],
        variables=[cast(TypeVarLikeType, deserialize_type(v)) for v in data["variables"]],
    )

</t>
<t tx="ekr.20230831011821.745">def __hash__(self) -&gt; int:
    return hash(
        (
            self.is_ellipsis_args,
            tuple(self.arg_types),
            tuple(self.arg_names),
            tuple(self.arg_kinds),
        )
    )

</t>
<t tx="ekr.20230831011821.746">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, (Parameters, CallableType)):
        return (
            self.arg_types == other.arg_types
            and self.arg_names == other.arg_names
            and self.arg_kinds == other.arg_kinds
            and self.is_ellipsis_args == other.is_ellipsis_args
        )
    else:
        return NotImplemented


</t>
<t tx="ekr.20230831011821.747">CT = TypeVar("CT", bound="CallableType")


class CallableType(FunctionLike):
    """Type of a non-overloaded callable object (such as function)."""

    @others
</t>
<t tx="ekr.20230831011821.748">__slots__ = (
    "arg_types",  # Types of function arguments
    "arg_kinds",  # ARG_ constants
    "arg_names",  # Argument names; None if not a keyword argument
    "min_args",  # Minimum number of arguments; derived from arg_kinds
    "ret_type",  # Return value type
    "name",  # Name (may be None; for error messages and plugins)
    "definition",  # For error messages.  May be None.
    "variables",  # Type variables for a generic function
    "is_ellipsis_args",  # Is this Callable[..., t] (with literal '...')?
    "is_classmethod_class",  # Is this callable constructed for the benefit
    # of a classmethod's 'cls' argument?
    "implicit",  # Was this type implicitly generated instead of explicitly
    # specified by the user?
    "special_sig",  # Non-None for signatures that require special handling
    # (currently only value is 'dict' for a signature similar to
    # 'dict')
    "from_type_type",  # Was this callable generated by analyzing Type[...]
    # instantiation?
    "bound_args",  # Bound type args, mostly unused but may be useful for
    # tools that consume mypy ASTs
    "def_extras",  # Information about original definition we want to serialize.
    # This is used for more detailed error messages.
    "type_guard",  # T, if -&gt; TypeGuard[T] (ret_type is bool in this case).
    "from_concatenate",  # whether this callable is from a concatenate object
    # (this is used for error messages)
    "unpack_kwargs",  # Was an Unpack[...] with **kwargs used to define this callable?
)

def __init__(
    self,
    # maybe this should be refactored to take a Parameters object
    arg_types: Sequence[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None],
    ret_type: Type,
    fallback: Instance,
    name: str | None = None,
    definition: SymbolNode | None = None,
    variables: Sequence[TypeVarLikeType] | None = None,
    line: int = -1,
    column: int = -1,
    is_ellipsis_args: bool = False,
    implicit: bool = False,
    special_sig: str | None = None,
    from_type_type: bool = False,
    bound_args: Sequence[Type | None] = (),
    def_extras: dict[str, Any] | None = None,
    type_guard: Type | None = None,
    from_concatenate: bool = False,
    unpack_kwargs: bool = False,
</t>
<t tx="ekr.20230831011821.749">) -&gt; None:
    super().__init__(line, column)
    assert len(arg_types) == len(arg_kinds) == len(arg_names)
    for t, k in zip(arg_types, arg_kinds):
        if isinstance(t, ParamSpecType):
            assert not t.prefix.arg_types
            # TODO: should we assert that only ARG_STAR contain ParamSpecType?
            # See testParamSpecJoin, that relies on passing e.g `P.args` as plain argument.
    if variables is None:
        variables = []
    self.arg_types = list(arg_types)
    self.arg_kinds = arg_kinds
    self.arg_names = list(arg_names)
    self.min_args = arg_kinds.count(ARG_POS)
    self.ret_type = ret_type
    self.fallback = fallback
    assert not name or "&lt;bound method" not in name
    self.name = name
    self.definition = definition
    self.variables = variables
    self.is_ellipsis_args = is_ellipsis_args
    self.implicit = implicit
    self.special_sig = special_sig
    self.from_type_type = from_type_type
    self.from_concatenate = from_concatenate
    if not bound_args:
        bound_args = ()
    self.bound_args = bound_args
    if def_extras:
        self.def_extras = def_extras
    elif isinstance(definition, FuncDef):
        # This information would be lost if we don't have definition
        # after serialization, but it is useful in error messages.
        # TODO: decide how to add more info here (file, line, column)
        # without changing interface hash.
        first_arg: str | None = None
        if definition.arg_names and definition.info and not definition.is_static:
            if getattr(definition, "arguments", None):
                first_arg = definition.arguments[0].variable.name
            else:
                first_arg = definition.arg_names[0]
        self.def_extras = {"first_arg": first_arg}
    else:
        self.def_extras = {}
    self.type_guard = type_guard
    self.unpack_kwargs = unpack_kwargs

def copy_modified(
    self: CT,
    arg_types: Bogus[Sequence[Type]] = _dummy,
    arg_kinds: Bogus[list[ArgKind]] = _dummy,
    arg_names: Bogus[Sequence[str | None]] = _dummy,
    ret_type: Bogus[Type] = _dummy,
    fallback: Bogus[Instance] = _dummy,
    name: Bogus[str | None] = _dummy,
    definition: Bogus[SymbolNode] = _dummy,
    variables: Bogus[Sequence[TypeVarLikeType]] = _dummy,
    line: int = _dummy_int,
    column: int = _dummy_int,
    is_ellipsis_args: Bogus[bool] = _dummy,
    implicit: Bogus[bool] = _dummy,
    special_sig: Bogus[str | None] = _dummy,
    from_type_type: Bogus[bool] = _dummy,
    bound_args: Bogus[list[Type | None]] = _dummy,
    def_extras: Bogus[dict[str, Any]] = _dummy,
    type_guard: Bogus[Type | None] = _dummy,
    from_concatenate: Bogus[bool] = _dummy,
    unpack_kwargs: Bogus[bool] = _dummy,
) -&gt; CT:
    modified = CallableType(
        arg_types=arg_types if arg_types is not _dummy else self.arg_types,
        arg_kinds=arg_kinds if arg_kinds is not _dummy else self.arg_kinds,
        arg_names=arg_names if arg_names is not _dummy else self.arg_names,
        ret_type=ret_type if ret_type is not _dummy else self.ret_type,
        fallback=fallback if fallback is not _dummy else self.fallback,
        name=name if name is not _dummy else self.name,
        definition=definition if definition is not _dummy else self.definition,
        variables=variables if variables is not _dummy else self.variables,
        line=line if line != _dummy_int else self.line,
        column=column if column != _dummy_int else self.column,
        is_ellipsis_args=(
            is_ellipsis_args if is_ellipsis_args is not _dummy else self.is_ellipsis_args
        ),
        implicit=implicit if implicit is not _dummy else self.implicit,
        special_sig=special_sig if special_sig is not _dummy else self.special_sig,
        from_type_type=from_type_type if from_type_type is not _dummy else self.from_type_type,
        bound_args=bound_args if bound_args is not _dummy else self.bound_args,
        def_extras=def_extras if def_extras is not _dummy else dict(self.def_extras),
        type_guard=type_guard if type_guard is not _dummy else self.type_guard,
        from_concatenate=(
            from_concatenate if from_concatenate is not _dummy else self.from_concatenate
        ),
        unpack_kwargs=unpack_kwargs if unpack_kwargs is not _dummy else self.unpack_kwargs,
    )
    # Optimization: Only NewTypes are supported as subtypes since
    # the class is effectively final, so we can use a cast safely.
    return cast(CT, modified)

</t>
<t tx="ekr.20230831011821.75">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assignment_stmt(o)

</t>
<t tx="ekr.20230831011821.750">def var_arg(self) -&gt; FormalArgument | None:
    """The formal argument for *args."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20230831011821.751">def kw_arg(self) -&gt; FormalArgument | None:
    """The formal argument for **kwargs."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR2:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20230831011821.752">@property
def is_var_arg(self) -&gt; bool:
    """Does this callable have a *args argument?"""
    return ARG_STAR in self.arg_kinds

</t>
<t tx="ekr.20230831011821.753">@property
def is_kw_arg(self) -&gt; bool:
    """Does this callable have a **kwargs argument?"""
    return ARG_STAR2 in self.arg_kinds

</t>
<t tx="ekr.20230831011821.754">def is_type_obj(self) -&gt; bool:
    return self.fallback.type.is_metaclass() and not isinstance(
        get_proper_type(self.ret_type), UninhabitedType
    )

</t>
<t tx="ekr.20230831011821.755">def type_object(self) -&gt; mypy.nodes.TypeInfo:
    assert self.is_type_obj()
    ret = get_proper_type(self.ret_type)
    if isinstance(ret, TypeVarType):
        ret = get_proper_type(ret.upper_bound)
    if isinstance(ret, TupleType):
        ret = ret.partial_fallback
    if isinstance(ret, TypedDictType):
        ret = ret.fallback
    assert isinstance(ret, Instance)
    return ret.type

</t>
<t tx="ekr.20230831011821.756">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_callable_type(self)

</t>
<t tx="ekr.20230831011821.757">def with_name(self, name: str) -&gt; CallableType:
    """Return a copy of this type with the specified name."""
    return self.copy_modified(ret_type=self.ret_type, name=name)

</t>
<t tx="ekr.20230831011821.758">def get_name(self) -&gt; str | None:
    return self.name

</t>
<t tx="ekr.20230831011821.759">def max_possible_positional_args(self) -&gt; int:
    """Returns maximum number of positional arguments this method could possibly accept.

    This takes into account *arg and **kwargs but excludes keyword-only args."""
    if self.is_var_arg or self.is_kw_arg:
        return sys.maxsize
    return sum(kind.is_positional() for kind in self.arg_kinds)

</t>
<t tx="ekr.20230831011821.76">def visit_operator_assignment_stmt(self, o: OperatorAssignmentStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_operator_assignment_stmt(o)

</t>
<t tx="ekr.20230831011821.760">def formal_arguments(self, include_star_args: bool = False) -&gt; list[FormalArgument]:
    """Return a list of the formal arguments of this callable, ignoring *arg and **kwargs.

    To handle *args and **kwargs, use the 'callable.var_args' and 'callable.kw_args' fields,
    if they are not None.

    If you really want to include star args in the yielded output, set the
    'include_star_args' parameter to 'True'."""
    args = []
    done_with_positional = False
    for i in range(len(self.arg_types)):
        kind = self.arg_kinds[i]
        if kind.is_named() or kind.is_star():
            done_with_positional = True
        if not include_star_args and kind.is_star():
            continue

        required = kind.is_required()
        pos = None if done_with_positional else i
        arg = FormalArgument(self.arg_names[i], pos, self.arg_types[i], required)
        args.append(arg)
    return args

</t>
<t tx="ekr.20230831011821.761">def argument_by_name(self, name: str | None) -&gt; FormalArgument | None:
    if name is None:
        return None
    seen_star = False
    for i, (arg_name, kind, typ) in enumerate(
        zip(self.arg_names, self.arg_kinds, self.arg_types)
    ):
        # No more positional arguments after these.
        if kind.is_named() or kind.is_star():
            seen_star = True
        if kind.is_star():
            continue
        if arg_name == name:
            position = None if seen_star else i
            return FormalArgument(name, position, typ, kind.is_required())
    return self.try_synthesizing_arg_from_kwarg(name)

</t>
<t tx="ekr.20230831011821.762">def argument_by_position(self, position: int | None) -&gt; FormalArgument | None:
    if position is None:
        return None
    if position &gt;= len(self.arg_names):
        return self.try_synthesizing_arg_from_vararg(position)
    name, kind, typ = (
        self.arg_names[position],
        self.arg_kinds[position],
        self.arg_types[position],
    )
    if kind.is_positional():
        return FormalArgument(name, position, typ, kind == ARG_POS)
    else:
        return self.try_synthesizing_arg_from_vararg(position)

</t>
<t tx="ekr.20230831011821.763">def try_synthesizing_arg_from_kwarg(self, name: str | None) -&gt; FormalArgument | None:
    kw_arg = self.kw_arg()
    if kw_arg is not None:
        return FormalArgument(name, None, kw_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20230831011821.764">def try_synthesizing_arg_from_vararg(self, position: int | None) -&gt; FormalArgument | None:
    var_arg = self.var_arg()
    if var_arg is not None:
        return FormalArgument(None, position, var_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20230831011821.765">@property
def items(self) -&gt; list[CallableType]:
    return [self]

</t>
<t tx="ekr.20230831011821.766">def is_generic(self) -&gt; bool:
    return bool(self.variables)

</t>
<t tx="ekr.20230831011821.767">def type_var_ids(self) -&gt; list[TypeVarId]:
    a: list[TypeVarId] = []
    for tv in self.variables:
        a.append(tv.id)
    return a

</t>
<t tx="ekr.20230831011821.768">def param_spec(self) -&gt; ParamSpecType | None:
    """Return ParamSpec if callable can be called with one.

    A Callable accepting ParamSpec P args (*args, **kwargs) must have the
    two final parameters like this: *args: P.args, **kwargs: P.kwargs.
    """
    if len(self.arg_types) &lt; 2:
        return None
    if self.arg_kinds[-2] != ARG_STAR or self.arg_kinds[-1] != ARG_STAR2:
        return None
    arg_type = self.arg_types[-2]
    if not isinstance(arg_type, ParamSpecType):
        return None

    # Prepend prefix for def f(prefix..., *args: P.args, **kwargs: P.kwargs) -&gt; ...
    # TODO: confirm that all arg kinds are positional
    prefix = Parameters(self.arg_types[:-2], self.arg_kinds[:-2], self.arg_names[:-2])
    return arg_type.copy_modified(flavor=ParamSpecFlavor.BARE, prefix=prefix)

</t>
<t tx="ekr.20230831011821.769">def expand_param_spec(self, c: Parameters) -&gt; CallableType:
    variables = c.variables
    return self.copy_modified(
        arg_types=self.arg_types[:-2] + c.arg_types,
        arg_kinds=self.arg_kinds[:-2] + c.arg_kinds,
        arg_names=self.arg_names[:-2] + c.arg_names,
        is_ellipsis_args=c.is_ellipsis_args,
        variables=[*variables, *self.variables],
    )

</t>
<t tx="ekr.20230831011821.77">def visit_while_stmt(self, o: WhileStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_while_stmt(o)

</t>
<t tx="ekr.20230831011821.770">def with_unpacked_kwargs(self) -&gt; NormalizedCallableType:
    if not self.unpack_kwargs:
        return cast(NormalizedCallableType, self)
    last_type = get_proper_type(self.arg_types[-1])
    assert isinstance(last_type, TypedDictType)
    extra_kinds = [
        ArgKind.ARG_NAMED if name in last_type.required_keys else ArgKind.ARG_NAMED_OPT
        for name in last_type.items
    ]
    new_arg_kinds = self.arg_kinds[:-1] + extra_kinds
    new_arg_names = self.arg_names[:-1] + list(last_type.items)
    new_arg_types = self.arg_types[:-1] + list(last_type.items.values())
    return NormalizedCallableType(
        self.copy_modified(
            arg_kinds=new_arg_kinds,
            arg_names=new_arg_names,
            arg_types=new_arg_types,
            unpack_kwargs=False,
        )
    )

</t>
<t tx="ekr.20230831011821.771">def with_normalized_var_args(self) -&gt; Self:
    var_arg = self.var_arg()
    if not var_arg or not isinstance(var_arg.typ, UnpackType):
        return self
    unpacked = get_proper_type(var_arg.typ.type)
    if not isinstance(unpacked, TupleType):
        # Note that we don't normalize *args: *tuple[X, ...] -&gt; *args: X,
        # this should be done once in semanal_typeargs.py for user-defined types,
        # and we ourselves should never construct such type.
        return self
    unpack_index = find_unpack_in_list(unpacked.items)
    if unpack_index == 0 and len(unpacked.items) &gt; 1:
        # Already normalized.
        return self

    # Boilerplate:
    var_arg_index = self.arg_kinds.index(ARG_STAR)
    types_prefix = self.arg_types[:var_arg_index]
    kinds_prefix = self.arg_kinds[:var_arg_index]
    names_prefix = self.arg_names[:var_arg_index]
    types_suffix = self.arg_types[var_arg_index + 1 :]
    kinds_suffix = self.arg_kinds[var_arg_index + 1 :]
    names_suffix = self.arg_names[var_arg_index + 1 :]
    no_name: str | None = None  # to silence mypy

    # Now we have something non-trivial to do.
    if unpack_index is None:
        # Plain *Tuple[X, Y, Z] -&gt; replace with ARG_POS completely
        types_middle = unpacked.items
        kinds_middle = [ARG_POS] * len(unpacked.items)
        names_middle = [no_name] * len(unpacked.items)
    else:
        # *Tuple[X, *Ts, Y, Z] or *Tuple[X, *tuple[T, ...], X, Z], here
        # we replace the prefix by ARG_POS (this is how some places expect
        # Callables to be represented)
        nested_unpack = unpacked.items[unpack_index]
        assert isinstance(nested_unpack, UnpackType)
        nested_unpacked = get_proper_type(nested_unpack.type)
        if unpack_index == len(unpacked.items) - 1:
            # Normalize also single item tuples like
            #   *args: *Tuple[*tuple[X, ...]] -&gt; *args: X
            #   *args: *Tuple[*Ts] -&gt; *args: *Ts
            # This may be not strictly necessary, but these are very verbose.
            if isinstance(nested_unpacked, Instance):
                assert nested_unpacked.type.fullname == "builtins.tuple"
                new_unpack = nested_unpacked.args[0]
            else:
                assert isinstance(nested_unpacked, TypeVarTupleType)
                new_unpack = nested_unpack
        else:
            new_unpack = UnpackType(
                unpacked.copy_modified(items=unpacked.items[unpack_index:])
            )
        types_middle = unpacked.items[:unpack_index] + [new_unpack]
        kinds_middle = [ARG_POS] * unpack_index + [ARG_STAR]
        names_middle = [no_name] * unpack_index + [self.arg_names[var_arg_index]]
    return self.copy_modified(
        arg_types=types_prefix + types_middle + types_suffix,
        arg_kinds=kinds_prefix + kinds_middle + kinds_suffix,
        arg_names=names_prefix + names_middle + names_suffix,
    )

</t>
<t tx="ekr.20230831011821.772">def __hash__(self) -&gt; int:
    # self.is_type_obj() will fail if self.fallback.type is a FakeInfo
    if isinstance(self.fallback.type, FakeInfo):
        is_type_obj = 2
    else:
        is_type_obj = self.is_type_obj()
    return hash(
        (
            self.ret_type,
            is_type_obj,
            self.is_ellipsis_args,
            self.name,
            tuple(self.arg_types),
            tuple(self.arg_names),
            tuple(self.arg_kinds),
            self.fallback,
        )
    )

</t>
<t tx="ekr.20230831011821.773">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, CallableType):
        return (
            self.ret_type == other.ret_type
            and self.arg_types == other.arg_types
            and self.arg_names == other.arg_names
            and self.arg_kinds == other.arg_kinds
            and self.name == other.name
            and self.is_type_obj() == other.is_type_obj()
            and self.is_ellipsis_args == other.is_ellipsis_args
            and self.fallback == other.fallback
        )
    else:
        return NotImplemented

</t>
<t tx="ekr.20230831011821.774">def serialize(self) -&gt; JsonDict:
    # TODO: As an optimization, leave out everything related to
    # generic functions for non-generic functions.
    return {
        ".class": "CallableType",
        "arg_types": [t.serialize() for t in self.arg_types],
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "arg_names": self.arg_names,
        "ret_type": self.ret_type.serialize(),
        "fallback": self.fallback.serialize(),
        "name": self.name,
        # We don't serialize the definition (only used for error messages).
        "variables": [v.serialize() for v in self.variables],
        "is_ellipsis_args": self.is_ellipsis_args,
        "implicit": self.implicit,
        "bound_args": [(None if t is None else t.serialize()) for t in self.bound_args],
        "def_extras": dict(self.def_extras),
        "type_guard": self.type_guard.serialize() if self.type_guard is not None else None,
        "from_concatenate": self.from_concatenate,
        "unpack_kwargs": self.unpack_kwargs,
    }

</t>
<t tx="ekr.20230831011821.775">@classmethod
def deserialize(cls, data: JsonDict) -&gt; CallableType:
    assert data[".class"] == "CallableType"
    # TODO: Set definition to the containing SymbolNode?
    return CallableType(
        [deserialize_type(t) for t in data["arg_types"]],
        [ArgKind(x) for x in data["arg_kinds"]],
        data["arg_names"],
        deserialize_type(data["ret_type"]),
        Instance.deserialize(data["fallback"]),
        name=data["name"],
        variables=[cast(TypeVarLikeType, deserialize_type(v)) for v in data["variables"]],
        is_ellipsis_args=data["is_ellipsis_args"],
        implicit=data["implicit"],
        bound_args=[(None if t is None else deserialize_type(t)) for t in data["bound_args"]],
        def_extras=data["def_extras"],
        type_guard=(
            deserialize_type(data["type_guard"]) if data["type_guard"] is not None else None
        ),
        from_concatenate=data["from_concatenate"],
        unpack_kwargs=data["unpack_kwargs"],
    )


</t>
<t tx="ekr.20230831011821.776"># This is a little safety net to prevent reckless special-casing of callables
# that can potentially break Unpack[...] with **kwargs.
# TODO: use this in more places in checkexpr.py etc?
NormalizedCallableType = NewType("NormalizedCallableType", CallableType)


class Overloaded(FunctionLike):
    """Overloaded function type T1, ... Tn, where each Ti is CallableType.

    The variant to call is chosen based on static argument
    types. Overloaded function types can only be defined in stub
    files, and thus there is no explicit runtime dispatch
    implementation.
    """

    @others
</t>
<t tx="ekr.20230831011821.777">__slots__ = ("_items",)

_items: list[CallableType]  # Must not be empty

def __init__(self, items: list[CallableType]) -&gt; None:
    super().__init__(items[0].line, items[0].column)
    self._items = items
    self.fallback = items[0].fallback

</t>
<t tx="ekr.20230831011821.778">@property
def items(self) -&gt; list[CallableType]:
    return self._items

</t>
<t tx="ekr.20230831011821.779">def name(self) -&gt; str | None:
    return self.get_name()

</t>
<t tx="ekr.20230831011821.78">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_for_stmt(o)

</t>
<t tx="ekr.20230831011821.780">def is_type_obj(self) -&gt; bool:
    # All the items must have the same type object status, so it's
    # sufficient to query only (any) one of them.
    return self._items[0].is_type_obj()

</t>
<t tx="ekr.20230831011821.781">def type_object(self) -&gt; mypy.nodes.TypeInfo:
    # All the items must have the same type object, so it's sufficient to
    # query only (any) one of them.
    return self._items[0].type_object()

</t>
<t tx="ekr.20230831011821.782">def with_name(self, name: str) -&gt; Overloaded:
    ni: list[CallableType] = []
    for it in self._items:
        ni.append(it.with_name(name))
    return Overloaded(ni)

</t>
<t tx="ekr.20230831011821.783">def get_name(self) -&gt; str | None:
    return self._items[0].name

</t>
<t tx="ekr.20230831011821.784">def with_unpacked_kwargs(self) -&gt; Overloaded:
    if any(i.unpack_kwargs for i in self.items):
        return Overloaded([i.with_unpacked_kwargs() for i in self.items])
    return self

</t>
<t tx="ekr.20230831011821.785">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_overloaded(self)

</t>
<t tx="ekr.20230831011821.786">def __hash__(self) -&gt; int:
    return hash(tuple(self.items))

</t>
<t tx="ekr.20230831011821.787">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Overloaded):
        return NotImplemented
    return self.items == other.items

</t>
<t tx="ekr.20230831011821.788">def serialize(self) -&gt; JsonDict:
    return {".class": "Overloaded", "items": [t.serialize() for t in self.items]}

</t>
<t tx="ekr.20230831011821.789">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Overloaded:
    assert data[".class"] == "Overloaded"
    return Overloaded([CallableType.deserialize(t) for t in data["items"]])


</t>
<t tx="ekr.20230831011821.79">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_return_stmt(o)

</t>
<t tx="ekr.20230831011821.790">class TupleType(ProperType):
    """The tuple type Tuple[T1, ..., Tn] (at least one type argument).

    Instance variables:
        items: Tuple item types
        partial_fallback: The (imprecise) underlying instance type that is used
            for non-tuple methods. This is generally builtins.tuple[Any, ...] for
            regular tuples, but it's different for named tuples and classes with
            a tuple base class. Use mypy.typeops.tuple_fallback to calculate the
            precise fallback type derived from item types.
        implicit: If True, derived from a tuple expression (t,....) instead of Tuple[t, ...]
    """

    @others
</t>
<t tx="ekr.20230831011821.791">__slots__ = ("items", "partial_fallback", "implicit")

items: list[Type]
partial_fallback: Instance
implicit: bool

def __init__(
    self,
    items: list[Type],
    fallback: Instance,
    line: int = -1,
    column: int = -1,
    implicit: bool = False,
) -&gt; None:
    super().__init__(line, column)
    self.partial_fallback = fallback
    self.items = items
    self.implicit = implicit

</t>
<t tx="ekr.20230831011821.792">def can_be_true_default(self) -&gt; bool:
    if self.can_be_any_bool():
        # Corner case: it is a `NamedTuple` with `__bool__` method defined.
        # It can be anything: both `True` and `False`.
        return True
    return self.length() &gt; 0

</t>
<t tx="ekr.20230831011821.793">def can_be_false_default(self) -&gt; bool:
    if self.can_be_any_bool():
        # Corner case: it is a `NamedTuple` with `__bool__` method defined.
        # It can be anything: both `True` and `False`.
        return True
    return self.length() == 0

</t>
<t tx="ekr.20230831011821.794">def can_be_any_bool(self) -&gt; bool:
    return bool(
        self.partial_fallback.type
        and self.partial_fallback.type.fullname != "builtins.tuple"
        and self.partial_fallback.type.names.get("__bool__")
    )

</t>
<t tx="ekr.20230831011821.795">def length(self) -&gt; int:
    return len(self.items)

</t>
<t tx="ekr.20230831011821.796">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_tuple_type(self)

</t>
<t tx="ekr.20230831011821.797">def __hash__(self) -&gt; int:
    return hash((tuple(self.items), self.partial_fallback))

</t>
<t tx="ekr.20230831011821.798">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TupleType):
        return NotImplemented
    return self.items == other.items and self.partial_fallback == other.partial_fallback

</t>
<t tx="ekr.20230831011821.799">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TupleType",
        "items": [t.serialize() for t in self.items],
        "partial_fallback": self.partial_fallback.serialize(),
        "implicit": self.implicit,
    }

</t>
<t tx="ekr.20230831011821.8">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    for item in o.items:
        item.accept(self)
    if o.impl:
        o.impl.accept(self)

</t>
<t tx="ekr.20230831011821.80">def visit_assert_stmt(self, o: AssertStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assert_stmt(o)

</t>
<t tx="ekr.20230831011821.800">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TupleType:
    assert data[".class"] == "TupleType"
    return TupleType(
        [deserialize_type(t) for t in data["items"]],
        Instance.deserialize(data["partial_fallback"]),
        implicit=data["implicit"],
    )

</t>
<t tx="ekr.20230831011821.801">def copy_modified(
    self, *, fallback: Instance | None = None, items: list[Type] | None = None
) -&gt; TupleType:
    if fallback is None:
        fallback = self.partial_fallback
    if items is None:
        items = self.items
    return TupleType(items, fallback, self.line, self.column)

</t>
<t tx="ekr.20230831011821.802">def slice(self, begin: int | None, end: int | None, stride: int | None) -&gt; TupleType:
    return TupleType(
        self.items[begin:end:stride],
        self.partial_fallback,
        self.line,
        self.column,
        self.implicit,
    )


</t>
<t tx="ekr.20230831011821.803">class TypedDictType(ProperType):
    """Type of TypedDict object {'k1': v1, ..., 'kn': vn}.

    A TypedDict object is a dictionary with specific string (literal) keys. Each
    key has a value with a distinct type that depends on the key. TypedDict objects
    are normal dict objects at runtime.

    A TypedDictType can be either named or anonymous. If it's anonymous, its
    fallback will be typing_extensions._TypedDict (Instance). _TypedDict is a subclass
    of Mapping[str, object] and defines all non-mapping dict methods that TypedDict
    supports. Some dict methods are unsafe and not supported. _TypedDict isn't defined
    at runtime.

    If a TypedDict is named, its fallback will be an Instance of the named type
    (ex: "Point") whose TypeInfo has a typeddict_type that is anonymous. This
    is similar to how named tuples work.

    TODO: The fallback structure is perhaps overly complicated.
    """

    @others
</t>
<t tx="ekr.20230831011821.804">__slots__ = ("items", "required_keys", "fallback")

items: dict[str, Type]  # item_name -&gt; item_type
required_keys: set[str]
fallback: Instance

def __init__(
    self,
    items: dict[str, Type],
    required_keys: set[str],
    fallback: Instance,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.items = items
    self.required_keys = required_keys
    self.fallback = fallback
    self.can_be_true = len(self.items) &gt; 0
    self.can_be_false = len(self.required_keys) == 0

</t>
<t tx="ekr.20230831011821.805">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_typeddict_type(self)

</t>
<t tx="ekr.20230831011821.806">def __hash__(self) -&gt; int:
    return hash((frozenset(self.items.items()), self.fallback, frozenset(self.required_keys)))

</t>
<t tx="ekr.20230831011821.807">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypedDictType):
        return NotImplemented

    return (
        frozenset(self.items.keys()) == frozenset(other.items.keys())
        and all(
            left_item_type == right_item_type
            for (_, left_item_type, right_item_type) in self.zip(other)
        )
        and self.fallback == other.fallback
        and self.required_keys == other.required_keys
    )

</t>
<t tx="ekr.20230831011821.808">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TypedDictType",
        "items": [[n, t.serialize()] for (n, t) in self.items.items()],
        "required_keys": sorted(self.required_keys),
        "fallback": self.fallback.serialize(),
    }

</t>
<t tx="ekr.20230831011821.809">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypedDictType:
    assert data[".class"] == "TypedDictType"
    return TypedDictType(
        {n: deserialize_type(t) for (n, t) in data["items"]},
        set(data["required_keys"]),
        Instance.deserialize(data["fallback"]),
    )

</t>
<t tx="ekr.20230831011821.81">def visit_del_stmt(self, o: DelStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_del_stmt(o)

</t>
<t tx="ekr.20230831011821.810">@property
def is_final(self) -&gt; bool:
    return self.fallback.type.is_final

</t>
<t tx="ekr.20230831011821.811">def is_anonymous(self) -&gt; bool:
    return self.fallback.type.fullname in TPDICT_FB_NAMES

</t>
<t tx="ekr.20230831011821.812">def as_anonymous(self) -&gt; TypedDictType:
    if self.is_anonymous():
        return self
    assert self.fallback.type.typeddict_type is not None
    return self.fallback.type.typeddict_type.as_anonymous()

</t>
<t tx="ekr.20230831011821.813">def copy_modified(
    self,
    *,
    fallback: Instance | None = None,
    item_types: list[Type] | None = None,
    item_names: list[str] | None = None,
    required_keys: set[str] | None = None,
) -&gt; TypedDictType:
    if fallback is None:
        fallback = self.fallback
    if item_types is None:
        items = self.items
    else:
        items = dict(zip(self.items, item_types))
    if required_keys is None:
        required_keys = self.required_keys
    if item_names is not None:
        items = {k: v for (k, v) in items.items() if k in item_names}
        required_keys &amp;= set(item_names)
    return TypedDictType(items, required_keys, fallback, self.line, self.column)

</t>
<t tx="ekr.20230831011821.814">def create_anonymous_fallback(self) -&gt; Instance:
    anonymous = self.as_anonymous()
    return anonymous.fallback

</t>
<t tx="ekr.20230831011821.815">def names_are_wider_than(self, other: TypedDictType) -&gt; bool:
    return len(other.items.keys() - self.items.keys()) == 0

</t>
<t tx="ekr.20230831011821.816">def zip(self, right: TypedDictType) -&gt; Iterable[tuple[str, Type, Type]]:
    left = self
    for item_name, left_item_type in left.items.items():
        right_item_type = right.items.get(item_name)
        if right_item_type is not None:
            yield (item_name, left_item_type, right_item_type)

</t>
<t tx="ekr.20230831011821.817">def zipall(self, right: TypedDictType) -&gt; Iterable[tuple[str, Type | None, Type | None]]:
    left = self
    for item_name, left_item_type in left.items.items():
        right_item_type = right.items.get(item_name)
        yield (item_name, left_item_type, right_item_type)
    for item_name, right_item_type in right.items.items():
        if item_name in left.items:
            continue
        yield (item_name, None, right_item_type)


</t>
<t tx="ekr.20230831011821.818">class RawExpressionType(ProperType):
    """A synthetic type representing some arbitrary expression that does not cleanly
    translate into a type.

    This synthetic type is only used at the beginning stages of semantic analysis
    and should be completely removing during the process for mapping UnboundTypes to
    actual types: we either turn it into a LiteralType or an AnyType.

    For example, suppose `Foo[1]` is initially represented as the following:

        UnboundType(
            name='Foo',
            args=[
                RawExpressionType(value=1, base_type_name='builtins.int'),
            ],
        )

    As we perform semantic analysis, this type will transform into one of two
    possible forms.

    If 'Foo' was an alias for 'Literal' all along, this type is transformed into:

        LiteralType(value=1, fallback=int_instance_here)

    Alternatively, if 'Foo' is an unrelated class, we report an error and instead
    produce something like this:

        Instance(type=typeinfo_for_foo, args=[AnyType(TypeOfAny.from_error))

    If the "note" field is not None, the provided note will be reported alongside the
    error at this point.

    Note: if "literal_value" is None, that means this object is representing some
    expression that cannot possibly be a parameter of Literal[...]. For example,
    "Foo[3j]" would be represented as:

        UnboundType(
            name='Foo',
            args=[
                RawExpressionType(value=None, base_type_name='builtins.complex'),
            ],
        )
    """

    @others
</t>
<t tx="ekr.20230831011821.819">__slots__ = ("literal_value", "base_type_name", "note")

def __init__(
    self,
    literal_value: LiteralValue | None,
    base_type_name: str,
    line: int = -1,
    column: int = -1,
    note: str | None = None,
) -&gt; None:
    super().__init__(line, column)
    self.literal_value = literal_value
    self.base_type_name = base_type_name
    self.note = note

</t>
<t tx="ekr.20230831011821.82">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_if_stmt(o)

</t>
<t tx="ekr.20230831011821.820">def simple_name(self) -&gt; str:
    return self.base_type_name.replace("builtins.", "")

</t>
<t tx="ekr.20230831011821.821">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    ret: T = visitor.visit_raw_expression_type(self)
    return ret

</t>
<t tx="ekr.20230831011821.822">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"

</t>
<t tx="ekr.20230831011821.823">def __hash__(self) -&gt; int:
    return hash((self.literal_value, self.base_type_name))

</t>
<t tx="ekr.20230831011821.824">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, RawExpressionType):
        return (
            self.base_type_name == other.base_type_name
            and self.literal_value == other.literal_value
        )
    else:
        return NotImplemented


</t>
<t tx="ekr.20230831011821.825">class LiteralType(ProperType):
    """The type of a Literal instance. Literal[Value]

    A Literal always consists of:

    1. A native Python object corresponding to the contained inner value
    2. A fallback for this Literal. The fallback also corresponds to the
       parent type this Literal subtypes.

    For example, 'Literal[42]' is represented as
    'LiteralType(value=42, fallback=instance_of_int)'

    As another example, `Literal[Color.RED]` (where Color is an enum) is
    represented as `LiteralType(value="RED", fallback=instance_of_color)'.
    """

    @others
</t>
<t tx="ekr.20230831011821.826">__slots__ = ("value", "fallback", "_hash")

def __init__(
    self, value: LiteralValue, fallback: Instance, line: int = -1, column: int = -1
) -&gt; None:
    super().__init__(line, column)
    self.value = value
    self.fallback = fallback
    self._hash = -1  # Cached hash value

</t>
<t tx="ekr.20230831011821.827">def can_be_false_default(self) -&gt; bool:
    return not self.value

</t>
<t tx="ekr.20230831011821.828">def can_be_true_default(self) -&gt; bool:
    return bool(self.value)

</t>
<t tx="ekr.20230831011821.829">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_literal_type(self)

</t>
<t tx="ekr.20230831011821.83">def visit_break_stmt(self, o: BreakStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_break_stmt(o)

</t>
<t tx="ekr.20230831011821.830">def __hash__(self) -&gt; int:
    if self._hash == -1:
        self._hash = hash((self.value, self.fallback))
    return self._hash

</t>
<t tx="ekr.20230831011821.831">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, LiteralType):
        return self.fallback == other.fallback and self.value == other.value
    else:
        return NotImplemented

</t>
<t tx="ekr.20230831011821.832">def is_enum_literal(self) -&gt; bool:
    return self.fallback.type.is_enum

</t>
<t tx="ekr.20230831011821.833">def value_repr(self) -&gt; str:
    """Returns the string representation of the underlying type.

    This function is almost equivalent to running `repr(self.value)`,
    except it includes some additional logic to correctly handle cases
    where the value is a string, byte string, a unicode string, or an enum.
    """
    raw = repr(self.value)
    fallback_name = self.fallback.type.fullname

    # If this is backed by an enum,
    if self.is_enum_literal():
        return f"{fallback_name}.{self.value}"

    if fallback_name == "builtins.bytes":
        # Note: 'builtins.bytes' only appears in Python 3, so we want to
        # explicitly prefix with a "b"
        return "b" + raw
    else:
        # 'builtins.str' could mean either depending on context, but either way
        # we don't prefix: it's the "native" string. And of course, if value is
        # some other type, we just return that string repr directly.
        return raw

</t>
<t tx="ekr.20230831011821.834">def serialize(self) -&gt; JsonDict | str:
    return {
        ".class": "LiteralType",
        "value": self.value,
        "fallback": self.fallback.serialize(),
    }

</t>
<t tx="ekr.20230831011821.835">@classmethod
def deserialize(cls, data: JsonDict) -&gt; LiteralType:
    assert data[".class"] == "LiteralType"
    return LiteralType(value=data["value"], fallback=Instance.deserialize(data["fallback"]))

</t>
<t tx="ekr.20230831011821.836">def is_singleton_type(self) -&gt; bool:
    return self.is_enum_literal() or isinstance(self.value, bool)


</t>
<t tx="ekr.20230831011821.837">class UnionType(ProperType):
    """The union type Union[T1, ..., Tn] (at least one type argument)."""

    @others
</t>
<t tx="ekr.20230831011821.838">__slots__ = ("items", "is_evaluated", "uses_pep604_syntax")

def __init__(
    self,
    items: Sequence[Type],
    line: int = -1,
    column: int = -1,
    is_evaluated: bool = True,
    uses_pep604_syntax: bool = False,
) -&gt; None:
    super().__init__(line, column)
    # We must keep this false to avoid crashes during semantic analysis.
    # TODO: maybe switch this to True during type-checking pass?
    self.items = flatten_nested_unions(items, handle_type_alias_type=False)
    # is_evaluated should be set to false for type comments and string literals
    self.is_evaluated = is_evaluated
    # uses_pep604_syntax is True if Union uses OR syntax (X | Y)
    self.uses_pep604_syntax = uses_pep604_syntax

</t>
<t tx="ekr.20230831011821.839">def can_be_true_default(self) -&gt; bool:
    return any(item.can_be_true for item in self.items)

</t>
<t tx="ekr.20230831011821.84">def visit_continue_stmt(self, o: ContinueStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_continue_stmt(o)

</t>
<t tx="ekr.20230831011821.840">def can_be_false_default(self) -&gt; bool:
    return any(item.can_be_false for item in self.items)

</t>
<t tx="ekr.20230831011821.841">def __hash__(self) -&gt; int:
    return hash(frozenset(self.items))

</t>
<t tx="ekr.20230831011821.842">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, UnionType):
        return NotImplemented
    return frozenset(self.items) == frozenset(other.items)

</t>
<t tx="ekr.20230831011821.843">@overload
@staticmethod
def make_union(items: Sequence[ProperType], line: int = -1, column: int = -1) -&gt; ProperType:
    ...

</t>
<t tx="ekr.20230831011821.844">@overload
@staticmethod
def make_union(items: Sequence[Type], line: int = -1, column: int = -1) -&gt; Type:
    ...

</t>
<t tx="ekr.20230831011821.845">@staticmethod
def make_union(items: Sequence[Type], line: int = -1, column: int = -1) -&gt; Type:
    if len(items) &gt; 1:
        return UnionType(items, line, column)
    elif len(items) == 1:
        return items[0]
    else:
        return UninhabitedType()

</t>
<t tx="ekr.20230831011821.846">def length(self) -&gt; int:
    return len(self.items)

</t>
<t tx="ekr.20230831011821.847">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_union_type(self)

</t>
<t tx="ekr.20230831011821.848">def relevant_items(self) -&gt; list[Type]:
    """Removes NoneTypes from Unions when strict Optional checking is off."""
    if state.strict_optional:
        return self.items
    else:
        return [i for i in self.items if not isinstance(get_proper_type(i), NoneType)]

</t>
<t tx="ekr.20230831011821.849">def serialize(self) -&gt; JsonDict:
    return {".class": "UnionType", "items": [t.serialize() for t in self.items]}

</t>
<t tx="ekr.20230831011821.85">def visit_pass_stmt(self, o: PassStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_pass_stmt(o)

</t>
<t tx="ekr.20230831011821.850">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UnionType:
    assert data[".class"] == "UnionType"
    return UnionType([deserialize_type(t) for t in data["items"]])


</t>
<t tx="ekr.20230831011821.851">class PartialType(ProperType):
    """Type such as List[?] where type arguments are unknown, or partial None type.

    These are used for inferring types in multiphase initialization such as this:

      x = []       # x gets a partial type List[?], as item type is unknown
      x.append(1)  # partial type gets replaced with normal type List[int]

    Or with None:

      x = None  # x gets a partial type None
      if c:
          x = 1  # Infer actual type int for x
    """

    @others
</t>
<t tx="ekr.20230831011821.852">__slots__ = ("type", "var", "value_type")

# None for the 'None' partial type; otherwise a generic class
type: mypy.nodes.TypeInfo | None
var: mypy.nodes.Var
# For partial defaultdict[K, V], the type V (K is unknown). If V is generic,
# the type argument is Any and will be replaced later.
value_type: Instance | None

def __init__(
    self,
    type: mypy.nodes.TypeInfo | None,
    var: mypy.nodes.Var,
    value_type: Instance | None = None,
) -&gt; None:
    super().__init__()
    self.type = type
    self.var = var
    self.value_type = value_type

</t>
<t tx="ekr.20230831011821.853">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_partial_type(self)


</t>
<t tx="ekr.20230831011821.854">class EllipsisType(ProperType):
    """The type ... (ellipsis).

    This is not a real type but a syntactic AST construct, used in Callable[..., T], for example.

    A semantically analyzed type will never have ellipsis types.
    """

    @others
</t>
<t tx="ekr.20230831011821.855">__slots__ = ()

def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    ret: T = visitor.visit_ellipsis_type(self)
    return ret

</t>
<t tx="ekr.20230831011821.856">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20230831011821.857">class TypeType(ProperType):
    """For types like Type[User].

    This annotates variables that are class objects, constrained by
    the type argument.  See PEP 484 for more details.

    We may encounter expressions whose values are specific classes;
    those are represented as callables (possibly overloaded)
    corresponding to the class's constructor's signature and returning
    an instance of that class.  The difference with Type[C] is that
    those callables always represent the exact class given as the
    return type; Type[C] represents any class that's a subclass of C,
    and C may also be a type variable or a union (or Any).

    Many questions around subtype relationships between Type[C1] and
    def(...) -&gt; C2 are answered by looking at the subtype
    relationships between C1 and C2, since Type[] is considered
    covariant.

    There's an unsolved problem with constructor signatures (also
    unsolved in PEP 484): calling a variable whose type is Type[C]
    assumes the constructor signature for C, even though a subclass of
    C might completely change the constructor signature.  For now we
    just assume that users of Type[C] are careful not to do that (in
    the future we might detect when they are violating that
    assumption).
    """

    @others
</t>
<t tx="ekr.20230831011821.858">__slots__ = ("item",)

# This can't be everything, but it can be a class reference,
# a generic class instance, a union, Any, a type variable...
item: ProperType

def __init__(
    self,
    item: Bogus[Instance | AnyType | TypeVarType | TupleType | NoneType | CallableType],
    *,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    """To ensure Type[Union[A, B]] is always represented as Union[Type[A], Type[B]], item of
    type UnionType must be handled through make_normalized static method.
    """
    super().__init__(line, column)
    self.item = item

</t>
<t tx="ekr.20230831011821.859">@staticmethod
def make_normalized(item: Type, *, line: int = -1, column: int = -1) -&gt; ProperType:
    item = get_proper_type(item)
    if isinstance(item, UnionType):
        return UnionType.make_union(
            [TypeType.make_normalized(union_item) for union_item in item.items],
            line=line,
            column=column,
        )
    return TypeType(item, line=line, column=column)  # type: ignore[arg-type]

</t>
<t tx="ekr.20230831011821.86">def visit_raise_stmt(self, o: RaiseStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_raise_stmt(o)

</t>
<t tx="ekr.20230831011821.860">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_type(self)

</t>
<t tx="ekr.20230831011821.861">def __hash__(self) -&gt; int:
    return hash(self.item)

</t>
<t tx="ekr.20230831011821.862">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeType):
        return NotImplemented
    return self.item == other.item

</t>
<t tx="ekr.20230831011821.863">def serialize(self) -&gt; JsonDict:
    return {".class": "TypeType", "item": self.item.serialize()}

</t>
<t tx="ekr.20230831011821.864">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Type:
    assert data[".class"] == "TypeType"
    return TypeType.make_normalized(deserialize_type(data["item"]))


</t>
<t tx="ekr.20230831011821.865">class PlaceholderType(ProperType):
    """Temporary, yet-unknown type during semantic analysis.

    This is needed when there's a reference to a type before the real symbol
    table entry of the target type is available (specifically, we use a
    temporary PlaceholderNode symbol node). Consider this example:

      class str(Sequence[str]): ...

    We use a PlaceholderType for the 'str' in 'Sequence[str]' since we can't create
    a TypeInfo for 'str' until all base classes have been resolved. We'll soon
    perform another analysis iteration which replaces the base class with a complete
    type without any placeholders. After semantic analysis, no placeholder types must
    exist.
    """

    @others
</t>
<t tx="ekr.20230831011821.866">__slots__ = ("fullname", "args")

def __init__(self, fullname: str | None, args: list[Type], line: int) -&gt; None:
    super().__init__(line)
    self.fullname = fullname  # Must be a valid full name of an actual node (or None).
    self.args = args

</t>
<t tx="ekr.20230831011821.867">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    ret: T = visitor.visit_placeholder_type(self)
    return ret

</t>
<t tx="ekr.20230831011821.868">def __hash__(self) -&gt; int:
    return hash((self.fullname, tuple(self.args)))

</t>
<t tx="ekr.20230831011821.869">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, PlaceholderType):
        return NotImplemented
    return self.fullname == other.fullname and self.args == other.args

</t>
<t tx="ekr.20230831011821.87">def visit_try_stmt(self, o: TryStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_try_stmt(o)

</t>
<t tx="ekr.20230831011821.870">def serialize(self) -&gt; str:
    # We should never get here since all placeholders should be replaced
    # during semantic analysis.
    assert False, f"Internal error: unresolved placeholder type {self.fullname}"


</t>
<t tx="ekr.20230831011821.871">@overload
def get_proper_type(typ: None) -&gt; None:
    ...


</t>
<t tx="ekr.20230831011821.872">@overload
def get_proper_type(typ: Type) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20230831011821.873">def get_proper_type(typ: Type | None) -&gt; ProperType | None:
    """Get the expansion of a type alias type.

    If the type is already a proper type, this is a no-op. Use this function
    wherever a decision is made on a call like e.g. 'if isinstance(typ, UnionType): ...',
    because 'typ' in this case may be an alias to union. Note: if after making the decision
    on the isinstance() call you pass on the original type (and not one of its components)
    it is recommended to *always* pass on the unexpanded alias.
    """
    if typ is None:
        return None
    if isinstance(typ, TypeGuardedType):  # type: ignore[misc]
        typ = typ.type_guard
    while isinstance(typ, TypeAliasType):
        typ = typ._expand_once()
    # TODO: store the name of original type alias on this type, so we can show it in errors.
    return cast(ProperType, typ)


</t>
<t tx="ekr.20230831011821.874">@overload
def get_proper_types(types: list[Type] | tuple[Type, ...]) -&gt; list[ProperType]:  # type: ignore[misc]
    ...


</t>
<t tx="ekr.20230831011821.875">@overload
def get_proper_types(
    types: list[Type | None] | tuple[Type | None, ...]
) -&gt; list[ProperType | None]:
    ...


</t>
<t tx="ekr.20230831011821.876">def get_proper_types(
    types: list[Type] | list[Type | None] | tuple[Type | None, ...]
) -&gt; list[ProperType] | list[ProperType | None]:
    if isinstance(types, list):
        typelist = types
        # Optimize for the common case so that we don't need to allocate anything
        if not any(
            isinstance(t, (TypeAliasType, TypeGuardedType)) for t in typelist  # type: ignore[misc]
        ):
            return cast("list[ProperType]", typelist)
        return [get_proper_type(t) for t in typelist]
    else:
        return [get_proper_type(t) for t in types]


</t>
<t tx="ekr.20230831011821.877"># We split off the type visitor base classes to another module
# to make it easier to gradually get modules working with mypyc.
# Import them here, after the types are defined.
# This is intended as a re-export also.
from mypy.type_visitor import (  # noqa: F811
    ALL_STRATEGY as ALL_STRATEGY,
    ANY_STRATEGY as ANY_STRATEGY,
    BoolTypeQuery as BoolTypeQuery,
    SyntheticTypeVisitor as SyntheticTypeVisitor,
    TypeQuery as TypeQuery,
    TypeTranslator as TypeTranslator,
    TypeVisitor as TypeVisitor,
)
from mypy.typetraverser import TypeTraverserVisitor


class TypeStrVisitor(SyntheticTypeVisitor[str]):
    """Visitor for pretty-printing types into strings.

    This is mostly for debugging/testing.

    Do not preserve original formatting.

    Notes:
     - Represent unbound types as Foo? or Foo?[...].
     - Represent the NoneType type as None.
    """

    @others
</t>
<t tx="ekr.20230831011821.878">def __init__(self, id_mapper: IdMapper | None = None, *, options: Options) -&gt; None:
    self.id_mapper = id_mapper
    self.any_as_dots = False
    self.options = options

</t>
<t tx="ekr.20230831011821.879">def visit_unbound_type(self, t: UnboundType) -&gt; str:
    s = t.name + "?"
    if t.args:
        s += f"[{self.list_str(t.args)}]"
    return s

</t>
<t tx="ekr.20230831011821.88">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_with_stmt(o)

</t>
<t tx="ekr.20230831011821.880">def visit_type_list(self, t: TypeList) -&gt; str:
    return f"&lt;TypeList {self.list_str(t.items)}&gt;"

</t>
<t tx="ekr.20230831011821.881">def visit_callable_argument(self, t: CallableArgument) -&gt; str:
    typ = t.typ.accept(self)
    if t.name is None:
        return f"{t.constructor}({typ})"
    else:
        return f"{t.constructor}({typ}, {t.name})"

</t>
<t tx="ekr.20230831011821.882">def visit_any(self, t: AnyType) -&gt; str:
    if self.any_as_dots and t.type_of_any == TypeOfAny.special_form:
        return "..."
    return "Any"

</t>
<t tx="ekr.20230831011821.883">def visit_none_type(self, t: NoneType) -&gt; str:
    return "None"

</t>
<t tx="ekr.20230831011821.884">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; str:
    return "&lt;nothing&gt;"

</t>
<t tx="ekr.20230831011821.885">def visit_erased_type(self, t: ErasedType) -&gt; str:
    return "&lt;Erased&gt;"

</t>
<t tx="ekr.20230831011821.886">def visit_deleted_type(self, t: DeletedType) -&gt; str:
    if t.source is None:
        return "&lt;Deleted&gt;"
    else:
        return f"&lt;Deleted '{t.source}'&gt;"

</t>
<t tx="ekr.20230831011821.887">def visit_instance(self, t: Instance) -&gt; str:
    if t.last_known_value and not t.args:
        # Instances with a literal fallback should never be generic. If they are,
        # something went wrong so we fall back to showing the full Instance repr.
        s = f"{t.last_known_value.accept(self)}?"
    else:
        s = t.type.fullname or t.type.name or "&lt;???&gt;"

    if t.args:
        if t.type.fullname == "builtins.tuple":
            assert len(t.args) == 1
            s += f"[{self.list_str(t.args)}, ...]"
        else:
            s += f"[{self.list_str(t.args)}]"
    if self.id_mapper:
        s += f"&lt;{self.id_mapper.id(t.type)}&gt;"
    return s

</t>
<t tx="ekr.20230831011821.888">def visit_type_var(self, t: TypeVarType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f"`{t.id}"
    else:
        # Named type variable type.
        s = f"{t.name}`{t.id}"
    if self.id_mapper and t.upper_bound:
        s += f"(upper_bound={t.upper_bound.accept(self)})"
    if t.has_default():
        s += f" = {t.default.accept(self)}"
    return s

</t>
<t tx="ekr.20230831011821.889">def visit_param_spec(self, t: ParamSpecType) -&gt; str:
    # prefixes are displayed as Concatenate
    s = ""
    if t.prefix.arg_types:
        s += f"[{self.list_str(t.prefix.arg_types)}, **"
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s += f"`{t.id}"
    else:
        # Named type variable type.
        s += f"{t.name_with_suffix()}`{t.id}"
    if t.prefix.arg_types:
        s += "]"
    if t.has_default():
        s += f" = {t.default.accept(self)}"
    return s

</t>
<t tx="ekr.20230831011821.89">def visit_match_stmt(self, o: MatchStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_match_stmt(o)

</t>
<t tx="ekr.20230831011821.890">def visit_parameters(self, t: Parameters) -&gt; str:
    # This is copied from visit_callable -- is there a way to decrease duplication?
    if t.is_ellipsis_args:
        return "..."

    s = ""
    bare_asterisk = False
    for i in range(len(t.arg_types)):
        if s != "":
            s += ", "
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += "*, "
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += "*"
        if t.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = t.arg_names[i]
        if name:
            s += f"{name}: "
        r = t.arg_types[i].accept(self)

        s += r

        if t.arg_kinds[i].is_optional():
            s += " ="

    return f"[{s}]"

</t>
<t tx="ekr.20230831011821.891">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f"`{t.id}"
    else:
        # Named type variable type.
        s = f"{t.name}`{t.id}"
    if t.has_default():
        s += f" = {t.default.accept(self)}"
    return s

</t>
<t tx="ekr.20230831011821.892">def visit_callable_type(self, t: CallableType) -&gt; str:
    param_spec = t.param_spec()
    if param_spec is not None:
        num_skip = 2
    else:
        num_skip = 0

    s = ""
    bare_asterisk = False
    for i in range(len(t.arg_types) - num_skip):
        if s != "":
            s += ", "
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += "*, "
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += "*"
        if t.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = t.arg_names[i]
        if name:
            s += name + ": "
        type_str = t.arg_types[i].accept(self)
        if t.arg_kinds[i] == ARG_STAR2 and t.unpack_kwargs:
            type_str = f"Unpack[{type_str}]"
        s += type_str
        if t.arg_kinds[i].is_optional():
            s += " ="

    if param_spec is not None:
        n = param_spec.name
        if s:
            s += ", "
        s += f"*{n}.args, **{n}.kwargs"
        if param_spec.has_default():
            s += f" = {param_spec.default.accept(self)}"

    s = f"({s})"

    if not isinstance(get_proper_type(t.ret_type), NoneType):
        if t.type_guard is not None:
            s += f" -&gt; TypeGuard[{t.type_guard.accept(self)}]"
        else:
            s += f" -&gt; {t.ret_type.accept(self)}"

    if t.variables:
        vs = []
        for var in t.variables:
            if isinstance(var, TypeVarType):
                # We reimplement TypeVarType.__repr__ here in order to support id_mapper.
                if var.values:
                    vals = f"({', '.join(val.accept(self) for val in var.values)})"
                    vs.append(f"{var.name} in {vals}")
                elif not is_named_instance(var.upper_bound, "builtins.object"):
                    vs.append(
                        f"{var.name} &lt;: {var.upper_bound.accept(self)}{f' = {var.default.accept(self)}' if var.has_default() else ''}"
                    )
                else:
                    vs.append(
                        f"{var.name}{f' = {var.default.accept(self)}' if var.has_default()  else ''}"
                    )
            else:
                # For other TypeVarLikeTypes, use the name and default
                vs.append(
                    f"{var.name}{f' = {var.default.accept(self)}' if var.has_default() else ''}"
                )
        s = f"[{', '.join(vs)}] {s}"

    return f"def {s}"

</t>
<t tx="ekr.20230831011821.893">def visit_overloaded(self, t: Overloaded) -&gt; str:
    a = []
    for i in t.items:
        a.append(i.accept(self))
    return f"Overload({', '.join(a)})"

</t>
<t tx="ekr.20230831011821.894">def visit_tuple_type(self, t: TupleType) -&gt; str:
    s = self.list_str(t.items) or "()"
    tuple_name = "tuple" if self.options.use_lowercase_names() else "Tuple"
    if t.partial_fallback and t.partial_fallback.type:
        fallback_name = t.partial_fallback.type.fullname
        if fallback_name != "builtins.tuple":
            return f"{tuple_name}[{s}, fallback={t.partial_fallback.accept(self)}]"
    return f"{tuple_name}[{s}]"

</t>
<t tx="ekr.20230831011821.895">def visit_typeddict_type(self, t: TypedDictType) -&gt; str:
    def item_str(name: str, typ: str) -&gt; str:
        if name in t.required_keys:
            return f"{name!r}: {typ}"
        else:
            return f"{name!r}?: {typ}"

    s = (
        "{"
        + ", ".join(item_str(name, typ.accept(self)) for name, typ in t.items.items())
        + "}"
    )
    prefix = ""
    if t.fallback and t.fallback.type:
        if t.fallback.type.fullname not in TPDICT_FB_NAMES:
            prefix = repr(t.fallback.type.fullname) + ", "
    return f"TypedDict({prefix}{s})"

</t>
<t tx="ekr.20230831011821.896">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; str:
    return repr(t.literal_value)

</t>
<t tx="ekr.20230831011821.897">def visit_literal_type(self, t: LiteralType) -&gt; str:
    return f"Literal[{t.value_repr()}]"

</t>
<t tx="ekr.20230831011821.898">def visit_union_type(self, t: UnionType) -&gt; str:
    s = self.list_str(t.items)
    return f"Union[{s}]"

</t>
<t tx="ekr.20230831011821.899">def visit_partial_type(self, t: PartialType) -&gt; str:
    if t.type is None:
        return "&lt;partial None&gt;"
    else:
        return "&lt;partial {}[{}]&gt;".format(t.type.name, ", ".join(["?"] * len(t.type.type_vars)))

</t>
<t tx="ekr.20230831011821.9">def visit_class_def(self, o: ClassDef) -&gt; None:
    for d in o.decorators:
        d.accept(self)
    for base in o.base_type_exprs:
        base.accept(self)
    if o.metaclass:
        o.metaclass.accept(self)
    for v in o.keywords.values():
        v.accept(self)
    o.defs.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20230831011821.90"># Expressions (default no-op implementation)

def visit_int_expr(self, o: IntExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_int_expr(o)

</t>
<t tx="ekr.20230831011821.900">def visit_ellipsis_type(self, t: EllipsisType) -&gt; str:
    return "..."

</t>
<t tx="ekr.20230831011821.901">def visit_type_type(self, t: TypeType) -&gt; str:
    return f"Type[{t.item.accept(self)}]"

</t>
<t tx="ekr.20230831011821.902">def visit_placeholder_type(self, t: PlaceholderType) -&gt; str:
    return f"&lt;placeholder {t.fullname}&gt;"

</t>
<t tx="ekr.20230831011821.903">def visit_type_alias_type(self, t: TypeAliasType) -&gt; str:
    if t.alias is not None:
        unrolled, recursed = t._partial_expansion()
        self.any_as_dots = recursed
        type_str = unrolled.accept(self)
        self.any_as_dots = False
        return type_str
    return "&lt;alias (unfixed)&gt;"

</t>
<t tx="ekr.20230831011821.904">def visit_unpack_type(self, t: UnpackType) -&gt; str:
    return f"Unpack[{t.type.accept(self)}]"

</t>
<t tx="ekr.20230831011821.905">def list_str(self, a: Iterable[Type]) -&gt; str:
    """Convert items of an array to strings (pretty-print types)
    and join the results with commas.
    """
    res = []
    for t in a:
        res.append(t.accept(self))
    return ", ".join(res)


</t>
<t tx="ekr.20230831011821.906">class TrivialSyntheticTypeTranslator(TypeTranslator, SyntheticTypeVisitor[Type]):
    """A base class for type translators that need to be run during semantic analysis."""

    @others
</t>
<t tx="ekr.20230831011821.907">def visit_placeholder_type(self, t: PlaceholderType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.908">def visit_callable_argument(self, t: CallableArgument) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.909">def visit_ellipsis_type(self, t: EllipsisType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.91">def visit_str_expr(self, o: StrExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_str_expr(o)

</t>
<t tx="ekr.20230831011821.910">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; Type:
    return t

</t>
<t tx="ekr.20230831011821.911">def visit_type_list(self, t: TypeList) -&gt; Type:
    return t


</t>
<t tx="ekr.20230831011821.912">class UnrollAliasVisitor(TrivialSyntheticTypeTranslator):
    @others
</t>
<t tx="ekr.20230831011821.913">def __init__(self, initial_aliases: set[TypeAliasType]) -&gt; None:
    self.recursed = False
    self.initial_aliases = initial_aliases

</t>
<t tx="ekr.20230831011821.914">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    if t in self.initial_aliases:
        self.recursed = True
        return AnyType(TypeOfAny.special_form)
    # Create a new visitor on encountering a new type alias, so that an alias like
    #     A = Tuple[B, B]
    #     B = int
    # will not be detected as recursive on the second encounter of B.
    subvisitor = UnrollAliasVisitor(self.initial_aliases | {t})
    result = get_proper_type(t).accept(subvisitor)
    if subvisitor.recursed:
        self.recursed = True
    return result


</t>
<t tx="ekr.20230831011821.915">def is_named_instance(t: Type, fullnames: str | tuple[str, ...]) -&gt; TypeGuard[Instance]:
    if not isinstance(fullnames, tuple):
        fullnames = (fullnames,)

    t = get_proper_type(t)
    return isinstance(t, Instance) and t.type.fullname in fullnames


</t>
<t tx="ekr.20230831011821.916">class LocationSetter(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20230831011821.917"># TODO: Should we update locations of other Type subclasses?
def __init__(self, line: int, column: int) -&gt; None:
    self.line = line
    self.column = column

</t>
<t tx="ekr.20230831011821.918">def visit_instance(self, typ: Instance) -&gt; None:
    typ.line = self.line
    typ.column = self.column
    super().visit_instance(typ)


</t>
<t tx="ekr.20230831011821.919">class HasTypeVars(BoolTypeQuery):
    @others
</t>
<t tx="ekr.20230831011821.92">def visit_bytes_expr(self, o: BytesExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_bytes_expr(o)

</t>
<t tx="ekr.20230831011821.920">def __init__(self) -&gt; None:
    super().__init__(ANY_STRATEGY)
    self.skip_alias_target = True

</t>
<t tx="ekr.20230831011821.921">def visit_type_var(self, t: TypeVarType) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011821.922">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; bool:
    return True

</t>
<t tx="ekr.20230831011821.923">def visit_param_spec(self, t: ParamSpecType) -&gt; bool:
    return True


</t>
<t tx="ekr.20230831011821.924">def has_type_vars(typ: Type) -&gt; bool:
    """Check if a type contains any type variables (recursively)."""
    return typ.accept(HasTypeVars())


</t>
<t tx="ekr.20230831011821.925">class HasRecursiveType(BoolTypeQuery):
    @others
</t>
<t tx="ekr.20230831011821.926">def __init__(self) -&gt; None:
    super().__init__(ANY_STRATEGY)

</t>
<t tx="ekr.20230831011821.927">def visit_type_alias_type(self, t: TypeAliasType) -&gt; bool:
    return t.is_recursive or self.query_types(t.args)


</t>
<t tx="ekr.20230831011821.928"># Use singleton since this is hot (note: call reset() before using)
_has_recursive_type: Final = HasRecursiveType()


def has_recursive_types(typ: Type) -&gt; bool:
    """Check if a type contains any recursive aliases (recursively)."""
    _has_recursive_type.reset()
    return typ.accept(_has_recursive_type)


</t>
<t tx="ekr.20230831011821.929">def split_with_prefix_and_suffix(
    types: tuple[Type, ...], prefix: int, suffix: int
) -&gt; tuple[tuple[Type, ...], tuple[Type, ...], tuple[Type, ...]]:
    if len(types) &lt;= prefix + suffix:
        types = extend_args_for_prefix_and_suffix(types, prefix, suffix)
    if suffix:
        return types[:prefix], types[prefix:-suffix], types[-suffix:]
    else:
        return types[:prefix], types[prefix:], ()


</t>
<t tx="ekr.20230831011821.93">def visit_float_expr(self, o: FloatExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_float_expr(o)

</t>
<t tx="ekr.20230831011821.930">def extend_args_for_prefix_and_suffix(
    types: tuple[Type, ...], prefix: int, suffix: int
) -&gt; tuple[Type, ...]:
    """Extend list of types by eating out from variadic tuple to satisfy prefix and suffix."""
    idx = None
    item = None
    for i, t in enumerate(types):
        if isinstance(t, UnpackType):
            p_type = get_proper_type(t.type)
            if isinstance(p_type, Instance) and p_type.type.fullname == "builtins.tuple":
                item = p_type.args[0]
                idx = i
                break

    if idx is None:
        return types
    assert item is not None
    if idx &lt; prefix:
        start = (item,) * (prefix - idx)
    else:
        start = ()
    if len(types) - idx - 1 &lt; suffix:
        end = (item,) * (suffix - len(types) + idx + 1)
    else:
        end = ()
    return types[:idx] + start + (types[idx],) + end + types[idx + 1 :]


</t>
<t tx="ekr.20230831011821.931">def flatten_nested_unions(
    types: Sequence[Type], handle_type_alias_type: bool = True
) -&gt; list[Type]:
    """Flatten nested unions in a type list."""
    if not isinstance(types, list):
        typelist = list(types)
    else:
        typelist = cast("list[Type]", types)

    # Fast path: most of the time there is nothing to flatten
    if not any(isinstance(t, (TypeAliasType, UnionType)) for t in typelist):  # type: ignore[misc]
        return typelist

    flat_items: list[Type] = []
    for t in typelist:
        tp = get_proper_type(t) if handle_type_alias_type else t
        if isinstance(tp, ProperType) and isinstance(tp, UnionType):
            flat_items.extend(
                flatten_nested_unions(tp.items, handle_type_alias_type=handle_type_alias_type)
            )
        else:
            # Must preserve original aliases when possible.
            flat_items.append(t)
    return flat_items


</t>
<t tx="ekr.20230831011821.932">def find_unpack_in_list(items: Sequence[Type]) -&gt; int | None:
    unpack_index: int | None = None
    for i, item in enumerate(items):
        if isinstance(item, UnpackType):
            # We cannot fail here, so we must check this in an earlier
            # semanal phase.
            # Funky code here avoids mypyc narrowing the type of unpack_index.
            old_index = unpack_index
            assert old_index is None
            # Don't return so that we can also sanity check there is only one.
            unpack_index = i
    return unpack_index


</t>
<t tx="ekr.20230831011821.933">def flatten_nested_tuples(types: Sequence[Type]) -&gt; list[Type]:
    """Recursively flatten TupleTypes nested with Unpack.

    For example this will transform
        Tuple[A, Unpack[Tuple[B, Unpack[Tuple[C, D]]]]]
    into
        Tuple[A, B, C, D]
    """
    res = []
    for typ in types:
        if not isinstance(typ, UnpackType):
            res.append(typ)
            continue
        p_type = get_proper_type(typ.type)
        if not isinstance(p_type, TupleType):
            res.append(typ)
            continue
        res.extend(flatten_nested_tuples(p_type.items))
    return res


</t>
<t tx="ekr.20230831011821.934">def is_literal_type(typ: ProperType, fallback_fullname: str, value: LiteralValue) -&gt; bool:
    """Check if this type is a LiteralType with the given fallback type and value."""
    if isinstance(typ, Instance) and typ.last_known_value:
        typ = typ.last_known_value
    return (
        isinstance(typ, LiteralType)
        and typ.fallback.type.fullname == fallback_fullname
        and typ.value == value
    )


</t>
<t tx="ekr.20230831011821.935">names: Final = globals().copy()
names.pop("NOT_READY", None)
deserialize_map: Final = {
    key: obj.deserialize
    for key, obj in names.items()
    if isinstance(obj, type) and issubclass(obj, Type) and obj is not Type
}


def callable_with_ellipsis(any_type: AnyType, ret_type: Type, fallback: Instance) -&gt; CallableType:
    """Construct type Callable[..., ret_type]."""
    return CallableType(
        [any_type, any_type],
        [ARG_STAR, ARG_STAR2],
        [None, None],
        ret_type=ret_type,
        fallback=fallback,
        is_ellipsis_args=True,
    )


</t>
<t tx="ekr.20230831011821.936">def remove_dups(types: list[T]) -&gt; list[T]:
    if len(types) &lt;= 1:
        return types
    # Get unique elements in order of appearance
    all_types: set[T] = set()
    new_types: list[T] = []
    for t in types:
        if t not in all_types:
            new_types.append(t)
            all_types.add(t)
    return new_types


</t>
<t tx="ekr.20230831011821.937"># This cyclic import is unfortunate, but to avoid it we would need to move away all uses
# of get_proper_type() from types.py. Majority of them have been removed, but few remaining
# are quite tricky to get rid of, but ultimately we want to do it at some point.
from mypy.expandtype import ExpandTypeVisitor


class InstantiateAliasVisitor(ExpandTypeVisitor):
    @others
</t>
<t tx="ekr.20230831011821.938">def visit_union_type(self, t: UnionType) -&gt; Type:
    # Unlike regular expand_type(), we don't do any simplification for unions,
    # not even removing strict duplicates. There are three reasons for this:
    #   * get_proper_type() is a very hot function, even slightest slow down will
    #     cause a perf regression
    #   * We want to preserve this historical behaviour, to avoid possible
    #     regressions
    #   * Simplifying unions may (indirectly) call get_proper_type(), causing
    #     infinite recursion.
    return TypeTranslator.visit_union_type(self, t)
</t>
<t tx="ekr.20230831011821.939">@path mypy
"""
This module is for (more basic) type operations that should not depend on is_subtype(),
meet_types(), join_types() etc. We don't want to keep them in mypy/types.py for two reasons:
* Reduce the size of that module.
* Reduce use of get_proper_type() in types.py to avoid cyclic imports
  expand_type &lt;-&gt; types, if we move get_proper_type() to the former.
"""

&lt;&lt; types_utils.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.94">def visit_complex_expr(self, o: ComplexExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_complex_expr(o)

</t>
<t tx="ekr.20230831011821.941">from __future__ import annotations

from typing import Callable, Iterable, cast

from mypy.nodes import ARG_STAR, ARG_STAR2, FuncItem, TypeAlias
from mypy.types import (
    AnyType,
    CallableType,
    Instance,
    NoneType,
    Overloaded,
    ParamSpecType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypeType,
    TypeVarType,
    UnionType,
    UnpackType,
    flatten_nested_unions,
    get_proper_type,
    get_proper_types,
)


</t>
<t tx="ekr.20230831011821.942">def flatten_types(types: Iterable[Type]) -&gt; Iterable[Type]:
    for t in types:
        tp = get_proper_type(t)
        if isinstance(tp, UnionType):
            yield from flatten_types(tp.items)
        else:
            yield t


</t>
<t tx="ekr.20230831011821.943">def strip_type(typ: Type) -&gt; Type:
    """Make a copy of type without 'debugging info' (function name)."""
    orig_typ = typ
    typ = get_proper_type(typ)
    if isinstance(typ, CallableType):
        return typ.copy_modified(name=None)
    elif isinstance(typ, Overloaded):
        return Overloaded([cast(CallableType, strip_type(item)) for item in typ.items])
    else:
        return orig_typ


</t>
<t tx="ekr.20230831011821.944">def is_invalid_recursive_alias(seen_nodes: set[TypeAlias], target: Type) -&gt; bool:
    """Flag aliases like A = Union[int, A], T = tuple[int, *T] (and similar mutual aliases).

    Such aliases don't make much sense, and cause problems in later phases.
    """
    if isinstance(target, TypeAliasType):
        if target.alias in seen_nodes:
            return True
        assert target.alias, f"Unfixed type alias {target.type_ref}"
        return is_invalid_recursive_alias(seen_nodes | {target.alias}, get_proper_type(target))
    assert isinstance(target, ProperType)
    if not isinstance(target, (UnionType, TupleType)):
        return False
    if isinstance(target, UnionType):
        return any(is_invalid_recursive_alias(seen_nodes, item) for item in target.items)
    for item in target.items:
        if isinstance(item, UnpackType):
            if is_invalid_recursive_alias(seen_nodes, item.type):
                return True
    return False


</t>
<t tx="ekr.20230831011821.945">def is_bad_type_type_item(item: Type) -&gt; bool:
    """Prohibit types like Type[Type[...]].

    Such types are explicitly prohibited by PEP 484. Also, they cause problems
    with recursive types like T = Type[T], because internal representation of
    TypeType item is normalized (i.e. always a proper type).
    """
    item = get_proper_type(item)
    if isinstance(item, TypeType):
        return True
    if isinstance(item, UnionType):
        return any(
            isinstance(get_proper_type(i), TypeType) for i in flatten_nested_unions(item.items)
        )
    return False


</t>
<t tx="ekr.20230831011821.946">def is_union_with_any(tp: Type) -&gt; bool:
    """Is this a union with Any or a plain Any type?"""
    tp = get_proper_type(tp)
    if isinstance(tp, AnyType):
        return True
    if not isinstance(tp, UnionType):
        return False
    return any(is_union_with_any(t) for t in get_proper_types(tp.items))


</t>
<t tx="ekr.20230831011821.947">def is_generic_instance(tp: Type) -&gt; bool:
    tp = get_proper_type(tp)
    return isinstance(tp, Instance) and bool(tp.args)


</t>
<t tx="ekr.20230831011821.948">def is_overlapping_none(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, NoneType) or (
        isinstance(t, UnionType) and any(isinstance(get_proper_type(e), NoneType) for e in t.items)
    )


</t>
<t tx="ekr.20230831011821.949">def remove_optional(typ: Type) -&gt; Type:
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        return UnionType.make_union(
            [t for t in typ.items if not isinstance(get_proper_type(t), NoneType)]
        )
    else:
        return typ


</t>
<t tx="ekr.20230831011821.95">def visit_ellipsis(self, o: EllipsisExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_ellipsis(o)

</t>
<t tx="ekr.20230831011821.950">def is_self_type_like(typ: Type, *, is_classmethod: bool) -&gt; bool:
    """Does this look like a self-type annotation?"""
    typ = get_proper_type(typ)
    if not is_classmethod:
        return isinstance(typ, TypeVarType)
    if not isinstance(typ, TypeType):
        return False
    return isinstance(typ.item, TypeVarType)


</t>
<t tx="ekr.20230831011821.951">def store_argument_type(
    defn: FuncItem, i: int, typ: CallableType, named_type: Callable[[str, list[Type]], Instance]
) -&gt; None:
    arg_type = typ.arg_types[i]
    if typ.arg_kinds[i] == ARG_STAR:
        if isinstance(arg_type, ParamSpecType):
            pass
        elif isinstance(arg_type, UnpackType):
            unpacked_type = get_proper_type(arg_type.type)
            if isinstance(unpacked_type, TupleType):
                # Instead of using Tuple[Unpack[Tuple[...]]], just use
                # Tuple[...]
                arg_type = unpacked_type
            elif (
                isinstance(unpacked_type, Instance)
                and unpacked_type.type.fullname == "builtins.tuple"
            ):
                arg_type = unpacked_type
            else:
                arg_type = TupleType(
                    [arg_type],
                    fallback=named_type("builtins.tuple", [named_type("builtins.object", [])]),
                )
        else:
            # builtins.tuple[T] is typing.Tuple[T, ...]
            arg_type = named_type("builtins.tuple", [arg_type])
    elif typ.arg_kinds[i] == ARG_STAR2:
        if not isinstance(arg_type, ParamSpecType) and not typ.unpack_kwargs:
            arg_type = named_type("builtins.dict", [named_type("builtins.str", []), arg_type])
    defn.arguments[i].variable.type = arg_type
</t>
<t tx="ekr.20230831011821.952">@path mypy
"""
A shared state for all TypeInfos that holds global cache and dependency information,
and potentially other mutable TypeInfo state. This module contains mutable global state.
"""

&lt;&lt; typestate.py: declarations &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.954">from __future__ import annotations

from typing import Dict, Final, Set, Tuple
from typing_extensions import TypeAlias as _TypeAlias

from mypy.nodes import TypeInfo
from mypy.server.trigger import make_trigger
from mypy.types import Instance, Type, TypeVarId, get_proper_type

MAX_NEGATIVE_CACHE_TYPES: Final = 1000
MAX_NEGATIVE_CACHE_ENTRIES: Final = 10000

# Represents that the 'left' instance is a subtype of the 'right' instance
SubtypeRelationship: _TypeAlias = Tuple[Instance, Instance]

# A tuple encoding the specific conditions under which we performed the subtype check.
# (e.g. did we want a proper subtype? A regular subtype while ignoring variance?)
SubtypeKind: _TypeAlias = Tuple[bool, ...]

# A cache that keeps track of whether the given TypeInfo is a part of a particular
# subtype relationship
SubtypeCache: _TypeAlias = Dict[TypeInfo, Dict[SubtypeKind, Set[SubtypeRelationship]]]


</t>
<t tx="ekr.20230831011821.955">class TypeState:
    """This class provides subtype caching to improve performance of subtype checks.
    It also holds protocol fine grained dependencies.

    Note: to avoid leaking global state, 'reset_all_subtype_caches()' should be called
    after a build has finished and after a daemon shutdown. This subtype cache only exists for
    performance reasons, resetting subtype caches for a class has no semantic effect.
    The protocol dependencies however are only stored here, and shouldn't be deleted unless
    not needed any more (e.g. during daemon shutdown).
    """

    @others
</t>
<t tx="ekr.20230831011821.956"># '_subtype_caches' keeps track of (subtype, supertype) pairs where supertypes are
# instances of the given TypeInfo. The cache also keeps track of whether the check
# was done in strict optional mode and of the specific *kind* of subtyping relationship,
# which we represent as an arbitrary hashable tuple.
# We need the caches, since subtype checks for structural types are very slow.
_subtype_caches: Final[SubtypeCache]

# Same as above but for negative subtyping results.
_negative_subtype_caches: Final[SubtypeCache]

# This contains protocol dependencies generated after running a full build,
# or after an update. These dependencies are special because:
#   * They are a global property of the program; i.e. some dependencies for imported
#     classes can be generated in the importing modules.
#   * Because of the above, they are serialized separately, after a full run,
#     or a full update.
# `proto_deps` can be None if after deserialization it turns out that they are
# inconsistent with the other cache files (or an error occurred during deserialization).
# A blocking error will be generated in this case, since we can't proceed safely.
# For the description of kinds of protocol dependencies and corresponding examples,
# see _snapshot_protocol_deps.
proto_deps: dict[str, set[str]] | None

# Protocols (full names) a given class attempted to implement.
# Used to calculate fine grained protocol dependencies and optimize protocol
# subtype cache invalidation in fine grained mode. For example, if we pass a value
# of type a.A to a function expecting something compatible with protocol p.P,
# we'd have 'a.A' -&gt; {'p.P', ...} in the map. This map is flushed after every incremental
# update.
_attempted_protocols: Final[dict[str, set[str]]]
# We also snapshot protocol members of the above protocols. For example, if we pass
# a value of type a.A to a function expecting something compatible with Iterable, we'd have
# 'a.A' -&gt; {'__iter__', ...} in the map. This map is also flushed after every incremental
# update. This map is needed to only generate dependencies like &lt;a.A.__iter__&gt; -&gt; &lt;a.A&gt;
# instead of a wildcard to avoid unnecessarily invalidating classes.
_checked_against_members: Final[dict[str, set[str]]]
# TypeInfos that appeared as a left type (subtype) in a subtype check since latest
# dependency snapshot update. This is an optimisation for fine grained mode; during a full
# run we only take a dependency snapshot at the very end, so this set will contain all
# subtype-checked TypeInfos. After a fine grained update however, we can gather only new
# dependencies generated from (typically) few TypeInfos that were subtype-checked
# (i.e. appeared as r.h.s. in an assignment or an argument in a function call in
# a re-checked target) during the update.
_rechecked_types: Final[set[TypeInfo]]

# The two attributes below are assumption stacks for subtyping relationships between
# recursive type aliases. Normally, one would pass type assumptions as an additional
# arguments to is_subtype(), but this would mean updating dozens of related functions
# threading this through all callsites (see also comment for TypeInfo.assuming).
_assuming: Final[list[tuple[Type, Type]]]
_assuming_proper: Final[list[tuple[Type, Type]]]
# Ditto for inference of generic constraints against recursive type aliases.
inferring: Final[list[tuple[Type, Type]]]
# Whether to use joins or unions when solving constraints, see checkexpr.py for details.
infer_unions: bool
# Whether to use new type inference algorithm that can infer polymorphic types.
# This is temporary and will be removed soon when new algorithm is more polished.
infer_polymorphic: bool

# N.B: We do all of the accesses to these properties through
# TypeState, instead of making these classmethods and accessing
# via the cls parameter, since mypyc can optimize accesses to
# Final attributes of a directly referenced type.

def __init__(self) -&gt; None:
    self._subtype_caches = {}
    self._negative_subtype_caches = {}
    self.proto_deps = {}
    self._attempted_protocols = {}
    self._checked_against_members = {}
    self._rechecked_types = set()
    self._assuming = []
    self._assuming_proper = []
    self.inferring = []
    self.infer_unions = False
    self.infer_polymorphic = False

</t>
<t tx="ekr.20230831011821.957">def is_assumed_subtype(self, left: Type, right: Type) -&gt; bool:
    for l, r in reversed(self._assuming):
        if get_proper_type(l) == get_proper_type(left) and get_proper_type(
            r
        ) == get_proper_type(right):
            return True
    return False

</t>
<t tx="ekr.20230831011821.958">def is_assumed_proper_subtype(self, left: Type, right: Type) -&gt; bool:
    for l, r in reversed(self._assuming_proper):
        if get_proper_type(l) == get_proper_type(left) and get_proper_type(
            r
        ) == get_proper_type(right):
            return True
    return False

</t>
<t tx="ekr.20230831011821.959">def get_assumptions(self, is_proper: bool) -&gt; list[tuple[Type, Type]]:
    if is_proper:
        return self._assuming_proper
    return self._assuming

</t>
<t tx="ekr.20230831011821.96">def visit_star_expr(self, o: StarExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_star_expr(o)

</t>
<t tx="ekr.20230831011821.960">def reset_all_subtype_caches(self) -&gt; None:
    """Completely reset all known subtype caches."""
    self._subtype_caches.clear()
    self._negative_subtype_caches.clear()

</t>
<t tx="ekr.20230831011821.961">def reset_subtype_caches_for(self, info: TypeInfo) -&gt; None:
    """Reset subtype caches (if any) for a given supertype TypeInfo."""
    if info in self._subtype_caches:
        self._subtype_caches[info].clear()
    if info in self._negative_subtype_caches:
        self._negative_subtype_caches[info].clear()

</t>
<t tx="ekr.20230831011821.962">def reset_all_subtype_caches_for(self, info: TypeInfo) -&gt; None:
    """Reset subtype caches (if any) for a given supertype TypeInfo and its MRO."""
    for item in info.mro:
        self.reset_subtype_caches_for(item)

</t>
<t tx="ekr.20230831011821.963">def is_cached_subtype_check(self, kind: SubtypeKind, left: Instance, right: Instance) -&gt; bool:
    if left.last_known_value is not None or right.last_known_value is not None:
        # If there is a literal last known value, give up. There
        # will be an unbounded number of potential types to cache,
        # making caching less effective.
        return False
    info = right.type
    cache = self._subtype_caches.get(info)
    if cache is None:
        return False
    subcache = cache.get(kind)
    if subcache is None:
        return False
    return (left, right) in subcache

</t>
<t tx="ekr.20230831011821.964">def is_cached_negative_subtype_check(
    self, kind: SubtypeKind, left: Instance, right: Instance
) -&gt; bool:
    if left.last_known_value is not None or right.last_known_value is not None:
        # If there is a literal last known value, give up. There
        # will be an unbounded number of potential types to cache,
        # making caching less effective.
        return False
    info = right.type
    cache = self._negative_subtype_caches.get(info)
    if cache is None:
        return False
    subcache = cache.get(kind)
    if subcache is None:
        return False
    return (left, right) in subcache

</t>
<t tx="ekr.20230831011821.965">def record_subtype_cache_entry(
    self, kind: SubtypeKind, left: Instance, right: Instance
) -&gt; None:
    if left.last_known_value is not None or right.last_known_value is not None:
        # These are unlikely to match, due to the large space of
        # possible values.  Avoid uselessly increasing cache sizes.
        return
    cache = self._subtype_caches.setdefault(right.type, dict())
    cache.setdefault(kind, set()).add((left, right))

</t>
<t tx="ekr.20230831011821.966">def record_negative_subtype_cache_entry(
    self, kind: SubtypeKind, left: Instance, right: Instance
) -&gt; None:
    if left.last_known_value is not None or right.last_known_value is not None:
        # These are unlikely to match, due to the large space of
        # possible values.  Avoid uselessly increasing cache sizes.
        return
    if len(self._negative_subtype_caches) &gt; MAX_NEGATIVE_CACHE_TYPES:
        self._negative_subtype_caches.clear()
    cache = self._negative_subtype_caches.setdefault(right.type, dict())
    subcache = cache.setdefault(kind, set())
    if len(subcache) &gt; MAX_NEGATIVE_CACHE_ENTRIES:
        subcache.clear()
    cache.setdefault(kind, set()).add((left, right))

</t>
<t tx="ekr.20230831011821.967">def reset_protocol_deps(self) -&gt; None:
    """Reset dependencies after a full run or before a daemon shutdown."""
    self.proto_deps = {}
    self._attempted_protocols.clear()
    self._checked_against_members.clear()
    self._rechecked_types.clear()

</t>
<t tx="ekr.20230831011821.968">def record_protocol_subtype_check(self, left_type: TypeInfo, right_type: TypeInfo) -&gt; None:
    assert right_type.is_protocol
    self._rechecked_types.add(left_type)
    self._attempted_protocols.setdefault(left_type.fullname, set()).add(right_type.fullname)
    self._checked_against_members.setdefault(left_type.fullname, set()).update(
        right_type.protocol_members
    )

</t>
<t tx="ekr.20230831011821.969">def _snapshot_protocol_deps(self) -&gt; dict[str, set[str]]:
    """Collect protocol attribute dependencies found so far from registered subtype checks.

    There are three kinds of protocol dependencies. For example, after a subtype check:

        x: Proto = C()

    the following dependencies will be generated:
        1. ..., &lt;SuperProto[wildcard]&gt;, &lt;Proto[wildcard]&gt; -&gt; &lt;Proto&gt;
        2. ..., &lt;B.attr&gt;, &lt;C.attr&gt; -&gt; &lt;C&gt; [for every attr in Proto members]
        3. &lt;C&gt; -&gt; Proto  # this one to invalidate the subtype cache

    The first kind is generated immediately per-module in deps.py (see also an example there
    for motivation why it is needed). While two other kinds are generated here after all
    modules are type checked and we have recorded all the subtype checks. To understand these
    two kinds, consider a simple example:

        class A:
            def __iter__(self) -&gt; Iterator[int]:
                ...

        it: Iterable[int] = A()

    We add &lt;a.A.__iter__&gt; -&gt; &lt;a.A&gt; to invalidate the assignment (module target in this case),
    whenever the signature of a.A.__iter__ changes. We also add &lt;a.A&gt; -&gt; typing.Iterable,
    to invalidate the subtype caches of the latter. (Note that the same logic applies to
    proper subtype checks, and calculating meets and joins, if this involves calling
    'subtypes.is_protocol_implementation').
    """
    deps: dict[str, set[str]] = {}
    for info in self._rechecked_types:
        for attr in self._checked_against_members[info.fullname]:
            # The need for full MRO here is subtle, during an update, base classes of
            # a concrete class may not be reprocessed, so not all &lt;B.x&gt; -&gt; &lt;C.x&gt; deps
            # are added.
            for base_info in info.mro[:-1]:
                trigger = make_trigger(f"{base_info.fullname}.{attr}")
                if "typing" in trigger or "builtins" in trigger:
                    # TODO: avoid everything from typeshed
                    continue
                deps.setdefault(trigger, set()).add(make_trigger(info.fullname))
        for proto in self._attempted_protocols[info.fullname]:
            trigger = make_trigger(info.fullname)
            if "typing" in trigger or "builtins" in trigger:
                continue
            # If any class that was checked against a protocol changes,
            # we need to reset the subtype cache for the protocol.
            #
            # Note: strictly speaking, the protocol doesn't need to be
            # re-checked, we only need to reset the cache, and its uses
            # elsewhere are still valid (unless invalidated by other deps).
            deps.setdefault(trigger, set()).add(proto)
    return deps

</t>
<t tx="ekr.20230831011821.97">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_name_expr(o)

</t>
<t tx="ekr.20230831011821.970">def update_protocol_deps(self, second_map: dict[str, set[str]] | None = None) -&gt; None:
    """Update global protocol dependency map.

    We update the global map incrementally, using a snapshot only from recently
    type checked types. If second_map is given, update it as well. This is currently used
    by FineGrainedBuildManager that maintains normal (non-protocol) dependencies.
    """
    assert self.proto_deps is not None, "This should not be called after failed cache load"
    new_deps = self._snapshot_protocol_deps()
    for trigger, targets in new_deps.items():
        self.proto_deps.setdefault(trigger, set()).update(targets)
    if second_map is not None:
        for trigger, targets in new_deps.items():
            second_map.setdefault(trigger, set()).update(targets)
    self._rechecked_types.clear()
    self._attempted_protocols.clear()
    self._checked_against_members.clear()

</t>
<t tx="ekr.20230831011821.971">def add_all_protocol_deps(self, deps: dict[str, set[str]]) -&gt; None:
    """Add all known protocol dependencies to deps.

    This is used by tests and debug output, and also when collecting
    all collected or loaded dependencies as part of build.
    """
    self.update_protocol_deps()  # just in case
    if self.proto_deps is not None:
        for trigger, targets in self.proto_deps.items():
            deps.setdefault(trigger, set()).update(targets)


</t>
<t tx="ekr.20230831011821.972">type_state: Final = TypeState()


def reset_global_state() -&gt; None:
    """Reset most existing global state.

    Currently most of it is in this module. Few exceptions are strict optional status
    and functools.lru_cache.
    """
    type_state.reset_all_subtype_caches()
    type_state.reset_protocol_deps()
    TypeVarId.next_raw_id = 1
</t>
<t tx="ekr.20230831011821.973">@path mypy
&lt;&lt; typetraverser.py: preamble &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20230831011821.974">from __future__ import annotations

from typing import Iterable

from mypy_extensions import trait

from mypy.types import (
    AnyType,
    CallableArgument,
    CallableType,
    DeletedType,
    EllipsisType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    PlaceholderType,
    RawExpressionType,
    SyntheticTypeVisitor,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
)


@trait
</t>
<t tx="ekr.20230831011821.975">class TypeTraverserVisitor(SyntheticTypeVisitor[None]):
    """Visitor that traverses all components of a type"""

    @others
</t>
<t tx="ekr.20230831011821.976"># Atomic types

def visit_any(self, t: AnyType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.977">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.978">def visit_none_type(self, t: NoneType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.979">def visit_erased_type(self, t: ErasedType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.98">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_member_expr(o)

</t>
<t tx="ekr.20230831011821.980">def visit_deleted_type(self, t: DeletedType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.981">def visit_type_var(self, t: TypeVarType) -&gt; None:
    # Note that type variable values and upper bound aren't treated as
    # components, since they are components of the type variable
    # definition. We want to traverse everything just once.
    pass

</t>
<t tx="ekr.20230831011821.982">def visit_param_spec(self, t: ParamSpecType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.983">def visit_parameters(self, t: Parameters) -&gt; None:
    self.traverse_types(t.arg_types)

</t>
<t tx="ekr.20230831011821.984">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.985">def visit_literal_type(self, t: LiteralType) -&gt; None:
    t.fallback.accept(self)

</t>
<t tx="ekr.20230831011821.986"># Composite types

def visit_instance(self, t: Instance) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20230831011821.987">def visit_callable_type(self, t: CallableType) -&gt; None:
    # FIX generics
    self.traverse_types(t.arg_types)
    t.ret_type.accept(self)
    t.fallback.accept(self)

</t>
<t tx="ekr.20230831011821.988">def visit_tuple_type(self, t: TupleType) -&gt; None:
    self.traverse_types(t.items)
    t.partial_fallback.accept(self)

</t>
<t tx="ekr.20230831011821.989">def visit_typeddict_type(self, t: TypedDictType) -&gt; None:
    self.traverse_types(t.items.values())
    t.fallback.accept(self)

</t>
<t tx="ekr.20230831011821.99">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_yield_from_expr(o)

</t>
<t tx="ekr.20230831011821.990">def visit_union_type(self, t: UnionType) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20230831011821.991">def visit_overloaded(self, t: Overloaded) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20230831011821.992">def visit_type_type(self, t: TypeType) -&gt; None:
    t.item.accept(self)

</t>
<t tx="ekr.20230831011821.993"># Special types (not real types)

def visit_callable_argument(self, t: CallableArgument) -&gt; None:
    t.typ.accept(self)

</t>
<t tx="ekr.20230831011821.994">def visit_unbound_type(self, t: UnboundType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20230831011821.995">def visit_type_list(self, t: TypeList) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20230831011821.996">def visit_ellipsis_type(self, t: EllipsisType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.997">def visit_placeholder_type(self, t: PlaceholderType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20230831011821.998">def visit_partial_type(self, t: PartialType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831011821.999">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; None:
    pass

</t>
<t tx="ekr.20230831070114.1"></t>
<t tx="ekr.20230831070204.1"></t>
<t tx="ekr.20230831070315.1"></t>
<t tx="ekr.20230831070358.1"></t>
<t tx="ekr.20230831071046.1"></t>
<t tx="ekr.20230831071117.1"></t>
<t tx="ekr.20230831071452.1"></t>
<t tx="ekr.20230831071740.1"></t>
<t tx="ekr.20230831071850.1"></t>
<t tx="ekr.20230901114501.1">promote-section-definition</t>
<t tx="ekr.20230902050227.1"></t>
<t tx="ekr.20230902050406.1"></t>
<t tx="ekr.20230902050513.1">"""The semantic analyzer.

Bind names to definitions and do various other simple consistency
checks.  Populate symbol tables.  The semantic analyzer also detects
special forms which reuse generic syntax such as NamedTuple and
cast().  Multiple analysis iterations may be needed to analyze forward
references and import cycles. Each iteration "fills in" additional
bindings and references until everything has been bound.

For example, consider this program:

  x = 1
  y = x

Here semantic analysis would detect that the assignment 'x = 1'
defines a new variable, the type of which is to be inferred (in a
later pass; type inference or type checking is not part of semantic
analysis).  Also, it would bind both references to 'x' to the same
module-level variable (Var) node.  The second assignment would also
be analyzed, and the type of 'y' marked as being inferred.

Semantic analysis of types is implemented in typeanal.py.

See semanal_main.py for the top-level logic.

Some important properties:

* After semantic analysis is complete, no PlaceholderNode and
  PlaceholderType instances should remain. During semantic analysis,
  if we encounter one of these, the current target should be deferred.

* A TypeInfo is only created once we know certain basic information about
  a type, such as the MRO, existence of a Tuple base class (e.g., for named
  tuples), and whether we have a TypedDict. We use a temporary
  PlaceholderNode node in the symbol table if some such information is
  missing.

* For assignments, we only add a non-placeholder symbol table entry once
  we know the sort of thing being defined (variable, NamedTuple, type alias,
  etc.).

* Every part of the analysis step must support multiple iterations over
  the same AST nodes, and each iteration must be able to fill in arbitrary
  things that were missing or incomplete in previous iterations.

* Changes performed by the analysis need to be reversible, since mypy
  daemon strips and reuses existing ASTs (to improve performance and/or
  reduce memory use).
"""
</t>
<t tx="ekr.20230902062700.1">"""Top-level logic for the semantic analyzer.

The semantic analyzer binds names, resolves imports, detects various
special constructs that don't have dedicated AST nodes after parse
(such as 'cast' which looks like a call), populates symbol tables, and
performs various simple consistency checks.

Semantic analysis of each SCC (strongly connected component; import
cycle) is performed in one unit. Each module is analyzed as multiple
separate *targets*; the module top level is one target and each function
is a target. Nested functions are not separate targets, however. This is
mostly identical to targets used by mypy daemon (but classes aren't
targets in semantic analysis).

We first analyze each module top level in an SCC. If we encounter some
names that we can't bind because the target of the name may not have
been processed yet, we *defer* the current target for further
processing. Deferred targets will be analyzed additional times until
everything can be bound, or we reach a maximum number of iterations.

We keep track of a set of incomplete namespaces, i.e. namespaces that we
haven't finished populating yet. References to these namespaces cause a
deferral if they can't be satisfied. Initially every module in the SCC
will be incomplete.
"""

</t>
<t tx="ekr.20230902062730.1"># Found 2 marked nodes</t>
<t tx="ekr.20230902063100.1">"""This module makes it possible to use mypy as part of a Python application.

Since mypy still changes, the API was kept utterly simple and non-intrusive.
It just mimics command line activation without starting a new interpreter.
So the normal docs about the mypy command line apply.
Changes in the command line version of mypy will be immediately usable.

Just import this module and then call the 'run' function with a parameter of
type List[str], containing what normally would have been the command line
arguments to mypy.

Function 'run' returns a Tuple[str, str, int], namely
(&lt;normal_report&gt;, &lt;error_report&gt;, &lt;exit_status&gt;),
in which &lt;normal_report&gt; is what mypy normally writes to sys.stdout,
&lt;error_report&gt; is what mypy normally writes to sys.stderr and exit_status is
the exit status mypy normally returns to the operating system.

Any pretty formatting is left to the caller.

The 'run_dmypy' function is similar, but instead mimics invocation of
dmypy. Note that run_dmypy is not thread-safe and modifies sys.stdout
and sys.stderr during its invocation.

Note that these APIs don't support incremental generation of error
messages.

Trivial example of code using this module:

import sys
from mypy import api

result = api.run(sys.argv[1:])

if result[0]:
    print('\nType checking report:\n')
    print(result[0])  # stdout

if result[1]:
    print('\nError report:\n')
    print(result[1])  # stderr

print('\nExit status:', result[2])

"""

</t>
<t tx="ekr.20230902063201.1">"""Interface for accessing the file system with automatic caching.

The idea is to cache the results of any file system state reads during
a single transaction. This has two main benefits:

* This avoids redundant syscalls, as we won't perform the same OS
  operations multiple times.

* This makes it easier to reason about concurrent FS updates, as different
  operations targeting the same paths can't report different state during
  a transaction.

Note that this only deals with reading state, not writing.

Properties maintained by the API:

* The contents of the file are always from the same or later time compared
  to the reported mtime of the file, even if mtime is queried after reading
  a file.

* Repeating an operation produces the same result as the first one during
  a transaction.

* Call flush() to start a new transaction (flush the caches).

The API is a bit limited. It's easy to add new cached operations, however.
You should perform all file system reads through the API to actually take
advantage of the benefits.
"""

</t>
<t tx="ekr.20230902063413.1">"""
Format expression type checker.

This file is conceptually part of ExpressionChecker and TypeChecker. Main functionality
is located in StringFormatterChecker.check_str_format_call() for '{}'.format(), and in
StringFormatterChecker.check_str_interpolation() for printf-style % interpolation.

Note that although at runtime format strings are parsed using custom parsers,
here we use a regexp-based approach. This way we 99% match runtime behaviour while keeping
implementation simple.
"""

</t>
<t tx="ekr.20230902063457.1"></t>
<t tx="ekr.20230907081811.1">"""Stand-alone test file for issue #12352"""

def ekr_f_annotated(ekr_a: str) -&gt; None:
    pass
    
# Passes with legacy mypy.
def ekr_f_annotated_initialized(ekr_a: str="abc") -&gt; None:
    pass

# Fails with legacy mypy.
# Change this case!
def ekr_f_not_annotated(ekr_a="abc", i=1, f=0.1, aBool=True, ekr_b=b's' ) -&gt; None:
    pass
    
# Later
def ekr_f_not_annotated2(b: int, ekr_a="abc") -&gt; None:
    pass
    
# a: str="abc"
# b="xyz"
# c: str
</t>
<t tx="ekr.20230907081931.1">"""Tests for studying mypy."""

def test_add(i: int, j: int) -&gt; int:
    return i + j
    
test_add(1, 2)
test_add('a', 'b')  # Should create error.</t>
</tnodes>
</leo_file>
