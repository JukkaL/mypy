<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20220601131028.1"><vh>Startup</vh>
<v t="ekr.20220525082746.1"><vh> Recursive import script</vh></v>
<v t="ekr.20220601131038.1"><vh>@settings</vh>
<v t="ekr.20220601131042.1"><vh>@data history-list</vh></v>
<v t="ekr.20220606071921.1"><vh>@buttons</vh>
<v t="ekr.20220606071936.1"><vh>@button backup</vh></v>
</v>
</v>
</v>
<v t="ekr.20220526075331.1"><vh>Files</vh>
<v t="ekr.20220531174005.1"><vh>@@clean check-default-annotations.test</vh></v>
<v t="ekr.20221004064034.3"><vh>@clean conftest.py</vh>
<v t="ekr.20221004064034.4"><vh>pytest_configure</vh></v>
<v t="ekr.20221004064034.5"><vh>pytest_addoption</vh></v>
</v>
<v t="ekr.20220603080610.1"><vh>@clean ekr_test.py</vh></v>
<v t="ekr.20221004064034.6"><vh>@clean ekr_test.py</vh>
<v t="ekr.20221004064034.7"><vh>ekr_f_annotated</vh></v>
<v t="ekr.20221004064034.8"><vh>ekr_f_annotated_initialized</vh></v>
<v t="ekr.20221004064034.9"><vh>ekr_f_not_annotated</vh></v>
<v t="ekr.20221004064034.10"><vh>ekr_f_not_annotated2</vh></v>
</v>
<v t="ekr.20221004064034.11"><vh>@clean runtests.py</vh>
<v t="ekr.20221004064034.12"><vh>run_cmd</vh></v>
<v t="ekr.20221004064034.13"><vh>start_background_cmd</vh></v>
<v t="ekr.20221004064034.14"><vh>wait_background_cmd</vh></v>
<v t="ekr.20221004064034.15"><vh>main</vh></v>
</v>
<v t="ekr.20221025081332.1"><vh>@clean sitecustomize.py</vh></v>
<v t="ekr.20221025075836.1"><vh>@clean tm.cmd</vh></v>
<v t="ekr.20221004064034.25"><vh>@path misc</vh>
<v t="ekr.20221004064034.26"><vh>@clean analyze_cache.py</vh>
<v t="ekr.20221004064034.27"><vh>class CacheData</vh>
<v t="ekr.20221004064034.28"><vh>CacheData.__init__</vh></v>
<v t="ekr.20221004064034.29"><vh>CacheData.total_size</vh></v>
</v>
<v t="ekr.20221004064034.30"><vh>extract_classes</vh>
<v t="ekr.20221004064034.31"><vh>extract</vh></v>
</v>
<v t="ekr.20221004064034.32"><vh>load_json</vh></v>
<v t="ekr.20221004064034.33"><vh>get_files</vh></v>
<v t="ekr.20221004064034.34"><vh>pluck</vh></v>
<v t="ekr.20221004064034.35"><vh>report_counter</vh></v>
<v t="ekr.20221004064034.36"><vh>report_most_common</vh></v>
<v t="ekr.20221004064034.37"><vh>compress</vh>
<v t="ekr.20221004064034.38"><vh>helper</vh></v>
</v>
<v t="ekr.20221004064034.39"><vh>decompress</vh>
<v t="ekr.20221004064034.40"><vh>helper</vh></v>
</v>
<v t="ekr.20221004064034.41"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.42"><vh>@clean apply-cache-diff.py</vh>
<v t="ekr.20221004064034.43"><vh>make_cache</vh></v>
<v t="ekr.20221004064034.44"><vh>apply_diff</vh></v>
<v t="ekr.20221004064034.45"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.46"><vh>@clean async_matrix.py</vh>
<v t="ekr.20221004064034.47"><vh>plain_generator</vh></v>
<v t="ekr.20221004064034.48"><vh>plain_coroutine</vh></v>
<v t="ekr.20221004064034.49"><vh>decorated_generator</vh></v>
<v t="ekr.20221004064034.50"><vh>decorated_coroutine</vh></v>
<v t="ekr.20221004064034.51"><vh>class It</vh>
<v t="ekr.20221004064034.52"><vh>It.__iter__</vh></v>
<v t="ekr.20221004064034.53"><vh>It.__next__</vh></v>
</v>
<v t="ekr.20221004064034.54"><vh>other_iterator</vh></v>
<v t="ekr.20221004064034.55"><vh>class Aw</vh></v>
<v t="ekr.20221004064034.56"><vh>other_coroutine</vh></v>
<v t="ekr.20221004064034.57"><vh>The various contexts in which `await` or `yield from` might occur.</vh></v>
<v t="ekr.20221004064034.58"><vh>plain_host_generator</vh></v>
<v t="ekr.20221004064034.59"><vh>plain_host_coroutine</vh></v>
<v t="ekr.20221004064034.60"><vh>decorated_host_generator</vh></v>
<v t="ekr.20221004064034.61"><vh>decorated_host_coroutine</vh></v>
<v t="ekr.20221004064034.62"><vh>Main driver.</vh></v>
<v t="ekr.20221004064034.63"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.64"><vh>@clean build_wheel.py</vh></v>
<v t="ekr.20221004064034.65"><vh>@clean cherry-pick-typeshed.py</vh>
<v t="ekr.20221004064034.66"><vh>parse_commit_title</vh></v>
<v t="ekr.20221004064034.67"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.68"><vh>@clean convert-cache.py</vh>
<v t="ekr.20221004064034.69"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.70"><vh>@clean diff-cache.py</vh>
<v t="ekr.20221004064034.71"><vh>make_cache</vh></v>
<v t="ekr.20221004064034.72"><vh>merge_deps</vh></v>
<v t="ekr.20221004064034.73"><vh>load</vh></v>
<v t="ekr.20221004064034.74"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.75"><vh>@clean dump-ast.py</vh>
<v t="ekr.20221004064034.76"><vh>dump</vh></v>
<v t="ekr.20221004064034.77"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.78"><vh>@clean find_type.py</vh>
<v t="ekr.20221004064034.79"><vh>update_line</vh></v>
<v t="ekr.20221004064034.80"><vh>run_mypy</vh></v>
<v t="ekr.20221004064034.81"><vh>get_revealed_type</vh></v>
<v t="ekr.20221004064034.82"><vh>process_output</vh></v>
<v t="ekr.20221004064034.83"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.84"><vh>@clean fix_annotate.py</vh>
<v t="ekr.20221004064034.85"><vh>class FixAnnotate</vh>
<v t="ekr.20221004064034.86"><vh>FixAnnotate.transform</vh></v>
<v t="ekr.20221004064034.87"><vh>FixAnnotate.make_annotation</vh></v>
<v t="ekr.20221004064034.88"><vh>FixAnnotate.The parse tree has a different shape when there is a single</vh></v>
<v t="ekr.20221004064034.89"><vh>FixAnnotate.get_decorators</vh></v>
<v t="ekr.20221004064034.90"><vh>FixAnnotate.is_method</vh></v>
<v t="ekr.20221004064034.91"><vh>FixAnnotate.RETURN_EXPR = "return_stmt&lt; 'return' any &gt;"</vh></v>
<v t="ekr.20221004064034.92"><vh>FixAnnotate.has_return_exprs</vh></v>
</v>
</v>
<v t="ekr.20221004064034.93"><vh>@clean incremental_checker.py</vh>
<v t="ekr.20221004064034.94"><vh>print_offset</vh></v>
<v t="ekr.20221004064034.95"><vh>delete_folder</vh></v>
<v t="ekr.20221004064034.96"><vh>execute</vh></v>
<v t="ekr.20221004064034.97"><vh>ensure_environment_is_ready</vh></v>
<v t="ekr.20221004064034.98"><vh>initialize_repo</vh></v>
<v t="ekr.20221004064034.99"><vh>get_commits</vh></v>
<v t="ekr.20221004064034.100"><vh>get_commits_starting_at</vh></v>
<v t="ekr.20221004064034.101"><vh>get_nth_commit</vh></v>
<v t="ekr.20221004064034.102"><vh>run_mypy</vh></v>
<v t="ekr.20221004064034.103"><vh>filter_daemon_stats</vh></v>
<v t="ekr.20221004064034.104"><vh>start_daemon</vh></v>
<v t="ekr.20221004064034.105"><vh>stop_daemon</vh></v>
<v t="ekr.20221004064034.106"><vh>load_cache</vh></v>
<v t="ekr.20221004064034.107"><vh>save_cache</vh></v>
<v t="ekr.20221004064034.108"><vh>set_expected</vh></v>
<v t="ekr.20221004064034.109"><vh>test_incremental</vh></v>
<v t="ekr.20221004064034.110"><vh>combine_stats</vh></v>
<v t="ekr.20221004064034.111"><vh>cleanup</vh></v>
<v t="ekr.20221004064034.112"><vh>test_repo</vh></v>
<v t="ekr.20221004064034.113"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.114"><vh>@clean perf_checker.py</vh>
<v t="ekr.20221004064034.115"><vh>class Command</vh></v>
<v t="ekr.20221004064034.116"><vh>print_offset</vh></v>
<v t="ekr.20221004064034.117"><vh>delete_folder</vh></v>
<v t="ekr.20221004064034.118"><vh>execute</vh></v>
<v t="ekr.20221004064034.119"><vh>trial</vh></v>
<v t="ekr.20221004064034.120"><vh>report</vh></v>
<v t="ekr.20221004064034.121"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.122"><vh>@clean proper_plugin.py</vh>
<v t="ekr.20221004064034.123"><vh>class ProperTypePlugin</vh>
<v t="ekr.20221004064034.124"><vh>ProperTypePlugin.get_function_hook</vh></v>
</v>
<v t="ekr.20221004064034.125"><vh>isinstance_proper_hook</vh></v>
<v t="ekr.20221004064034.126"><vh>is_special_target</vh></v>
<v t="ekr.20221004064034.127"><vh>is_improper_type</vh></v>
<v t="ekr.20221004064034.128"><vh>is_dangerous_target</vh></v>
<v t="ekr.20221004064034.129"><vh>proper_type_hook</vh></v>
<v t="ekr.20221004064034.130"><vh>proper_types_hook</vh></v>
<v t="ekr.20221004064034.131"><vh>get_proper_type_instance</vh></v>
<v t="ekr.20221004064034.132"><vh>plugin</vh></v>
</v>
<v t="ekr.20221004064034.133"><vh>@clean sync-typeshed.py</vh>
<v t="ekr.20221004064034.134"><vh>check_state</vh></v>
<v t="ekr.20221004064034.135"><vh>update_typeshed</vh></v>
<v t="ekr.20221004064034.136"><vh>git_head_commit</vh></v>
<v t="ekr.20221004064034.137"><vh>main</vh></v>
</v>
<v t="ekr.20221004064034.138"><vh>@clean upload-pypi.py</vh>
<v t="ekr.20221004064034.139"><vh>is_whl_or_tar</vh></v>
<v t="ekr.20221004064034.140"><vh>get_release_for_tag</vh></v>
<v t="ekr.20221004064034.141"><vh>download_asset</vh></v>
<v t="ekr.20221004064034.142"><vh>download_all_release_assets</vh></v>
<v t="ekr.20221004064034.143"><vh>check_sdist</vh></v>
<v t="ekr.20221004064034.144"><vh>spot_check_dist</vh></v>
<v t="ekr.20221004064034.145"><vh>tmp_twine</vh></v>
<v t="ekr.20221004064034.146"><vh>upload_dist</vh></v>
<v t="ekr.20221004064034.147"><vh>upload_to_pypi</vh></v>
<v t="ekr.20221004064034.148"><vh>main</vh></v>
</v>
</v>
<v t="ekr.20221004064034.149"><vh>@path mypy</vh>
<v t="ekr.20221005081733.1"><vh>--- ast &amp; nodes</vh>
<v t="ekr.20221004064034.158"><vh>@clean argmap.py</vh>
<v t="ekr.20221004064034.159"><vh>map_actuals_to_formals</vh></v>
<v t="ekr.20221004064034.160"><vh>map_formals_to_actuals</vh></v>
<v t="ekr.20221004064034.161"><vh>class ArgTypeExpander</vh>
<v t="ekr.20221004064034.162"><vh>ArgTypeExpander.__init__</vh></v>
<v t="ekr.20221004064034.163"><vh>ArgTypeExpander.expand_actual_type</vh></v>
</v>
</v>
<v t="ekr.20221004064034.1168"><vh>@clean fastparse.py</vh>
<v t="ekr.20221004064034.1169"><vh>parse</vh></v>
<v t="ekr.20221004064034.1170"><vh>parse_type_ignore_tag</vh></v>
<v t="ekr.20221004064034.1171"><vh>parse_type_comment</vh></v>
<v t="ekr.20221004064034.1172"><vh>parse_type_string</vh></v>
<v t="ekr.20221004064034.1173"><vh>is_no_type_check_decorator</vh></v>
<v t="ekr.20221004064034.1174"><vh>class ASTConverter</vh>
<v t="ekr.20221004064034.1175"><vh>ASTConverter.__init__</vh></v>
<v t="ekr.20221004064034.1176"><vh>ASTConverter.note</vh></v>
<v t="ekr.20221004064034.1177"><vh>ASTConverter.fail</vh></v>
<v t="ekr.20221004064034.1178"><vh>ASTConverter.fail_merge_overload</vh></v>
<v t="ekr.20221004064034.1179"><vh>ASTConverter.visit</vh></v>
<v t="ekr.20221004064034.1180"><vh>ASTConverter.set_line</vh></v>
<v t="ekr.20221004064034.1181"><vh>ASTConverter.translate_opt_expr_list</vh></v>
<v t="ekr.20221004064034.1182"><vh>ASTConverter.translate_expr_list</vh></v>
<v t="ekr.20221004064034.1183"><vh>ASTConverter.get_lineno</vh></v>
<v t="ekr.20221004064034.1184"><vh>ASTConverter.translate_stmt_list</vh></v>
<v t="ekr.20221004064034.1185"><vh>ASTConverter.translate_type_comment</vh></v>
<v t="ekr.20221004064034.1186"><vh>ASTConverter.op_map: Final[dict[type[AST], str]] = {</vh></v>
<v t="ekr.20221004064034.1187"><vh>ASTConverter.from_operator</vh></v>
<v t="ekr.20221004064034.1188"><vh>ASTConverter.comp_op_map: Final[dict[type[AST], str]] = {</vh></v>
<v t="ekr.20221004064034.1189"><vh>ASTConverter.from_comp_operator</vh></v>
<v t="ekr.20221004064034.1190"><vh>ASTConverter.as_block</vh></v>
<v t="ekr.20221004064034.1191"><vh>ASTConverter.as_required_block</vh></v>
<v t="ekr.20221004064034.1192"><vh>ASTConverter.fix_function_overloads</vh></v>
<v t="ekr.20221004064034.1193"><vh>ASTConverter._check_ifstmt_for_overloads</vh></v>
<v t="ekr.20221004064034.1194"><vh>ASTConverter._get_executable_if_block_with_overloads</vh></v>
<v t="ekr.20221004064034.1195"><vh>ASTConverter._strip_contents_from_if_stmt</vh></v>
<v t="ekr.20221004064034.1196"><vh>ASTConverter._is_stripped_if_stmt</vh></v>
<v t="ekr.20221004064034.1197"><vh>ASTConverter.in_method_scope</vh></v>
<v t="ekr.20221004064034.1198"><vh>ASTConverter.translate_module_id</vh></v>
<v t="ekr.20221004064034.1199"><vh>ASTConverter.visit_Module</vh></v>
<v t="ekr.20221004064034.1200"><vh>ASTConverter.visit_FunctionDef</vh></v>
<v t="ekr.20221004064034.1201"><vh>ASTConverter.visit_AsyncFunctionDef</vh></v>
<v t="ekr.20221004064034.1202"><vh>ASTConverter.do_func_def</vh></v>
<v t="ekr.20221004064034.1203"><vh>ASTConverter.set_type_optional</vh></v>
<v t="ekr.20221004064034.1204"><vh>ASTConverter.transform_args</vh></v>
<v t="ekr.20221004064034.1205"><vh>ASTConverter.make_argument</vh></v>
<v t="ekr.20221004064034.1206"><vh>ASTConverter.fail_arg</vh></v>
<v t="ekr.20221004064034.1207"><vh>ASTConverter.visit_ClassDef</vh></v>
<v t="ekr.20221004064034.1208"><vh>ASTConverter.visit_Return</vh></v>
<v t="ekr.20221004064034.1209"><vh>ASTConverter.visit_Delete</vh></v>
<v t="ekr.20221004064034.1210"><vh>ASTConverter.visit_Assign</vh></v>
<v t="ekr.20221004064034.1211"><vh>ASTConverter.visit_AnnAssign</vh></v>
<v t="ekr.20221004064034.1212"><vh>ASTConverter.visit_AugAssign</vh></v>
<v t="ekr.20221004064034.1213"><vh>ASTConverter.visit_For</vh></v>
<v t="ekr.20221004064034.1214"><vh>ASTConverter.visit_AsyncFor</vh></v>
<v t="ekr.20221004064034.1215"><vh>ASTConverter.visit_While</vh></v>
<v t="ekr.20221004064034.1216"><vh>ASTConverter.visit_If</vh></v>
<v t="ekr.20221004064034.1217"><vh>ASTConverter.visit_With</vh></v>
<v t="ekr.20221004064034.1218"><vh>ASTConverter.visit_AsyncWith</vh></v>
<v t="ekr.20221004064034.1219"><vh>ASTConverter.visit_Raise</vh></v>
<v t="ekr.20221004064034.1220"><vh>ASTConverter.visit_Try</vh></v>
<v t="ekr.20221004064034.1221"><vh>ASTConverter.visit_Assert</vh></v>
<v t="ekr.20221004064034.1222"><vh>ASTConverter.visit_Import</vh></v>
<v t="ekr.20221004064034.1223"><vh>ASTConverter.visit_ImportFrom</vh></v>
<v t="ekr.20221004064034.1224"><vh>ASTConverter.visit_Global</vh></v>
<v t="ekr.20221004064034.1225"><vh>ASTConverter.visit_Nonlocal</vh></v>
<v t="ekr.20221004064034.1226"><vh>ASTConverter.visit_Expr</vh></v>
<v t="ekr.20221004064034.1227"><vh>ASTConverter.visit_Pass</vh></v>
<v t="ekr.20221004064034.1228"><vh>ASTConverter.visit_Break</vh></v>
<v t="ekr.20221004064034.1229"><vh>ASTConverter.visit_Continue</vh></v>
<v t="ekr.20221004064034.1230"><vh>ASTConverter.--- expr ---</vh></v>
<v t="ekr.20221004064034.1231"><vh>ASTConverter.visit_NamedExpr</vh></v>
<v t="ekr.20221004064034.1232"><vh>ASTConverter.visit_BoolOp</vh></v>
<v t="ekr.20221004064034.1233"><vh>ASTConverter.group</vh></v>
<v t="ekr.20221004064034.1234"><vh>ASTConverter.visit_BinOp</vh></v>
<v t="ekr.20221004064034.1235"><vh>ASTConverter.visit_UnaryOp</vh></v>
<v t="ekr.20221004064034.1236"><vh>ASTConverter.visit_Lambda</vh></v>
<v t="ekr.20221004064034.1237"><vh>ASTConverter.visit_IfExp</vh></v>
<v t="ekr.20221004064034.1238"><vh>ASTConverter.visit_Dict</vh></v>
<v t="ekr.20221004064034.1239"><vh>ASTConverter.visit_Set</vh></v>
<v t="ekr.20221004064034.1240"><vh>ASTConverter.visit_ListComp</vh></v>
<v t="ekr.20221004064034.1241"><vh>ASTConverter.visit_SetComp</vh></v>
<v t="ekr.20221004064034.1242"><vh>ASTConverter.visit_DictComp</vh></v>
<v t="ekr.20221004064034.1243"><vh>ASTConverter.visit_GeneratorExp</vh></v>
<v t="ekr.20221004064034.1244"><vh>ASTConverter.visit_Await</vh></v>
<v t="ekr.20221004064034.1245"><vh>ASTConverter.visit_Yield</vh></v>
<v t="ekr.20221004064034.1246"><vh>ASTConverter.visit_YieldFrom</vh></v>
<v t="ekr.20221004064034.1247"><vh>ASTConverter.visit_Compare</vh></v>
<v t="ekr.20221004064034.1248"><vh>ASTConverter.visit_Call</vh></v>
<v t="ekr.20221004064034.1249"><vh>ASTConverter.visit_Constant</vh></v>
<v t="ekr.20221004064034.1250"><vh>ASTConverter.visit_Num</vh></v>
<v t="ekr.20221004064034.1251"><vh>ASTConverter.visit_Str</vh></v>
<v t="ekr.20221004064034.1252"><vh>ASTConverter.visit_JoinedStr</vh></v>
<v t="ekr.20221004064034.1253"><vh>ASTConverter.visit_FormattedValue</vh></v>
<v t="ekr.20221004064034.1254"><vh>ASTConverter.visit_Bytes</vh></v>
<v t="ekr.20221004064034.1255"><vh>ASTConverter.visit_NameConstant</vh></v>
<v t="ekr.20221004064034.1256"><vh>ASTConverter.visit_Ellipsis</vh></v>
<v t="ekr.20221004064034.1257"><vh>ASTConverter.visit_Attribute</vh></v>
<v t="ekr.20221004064034.1258"><vh>ASTConverter.visit_Subscript</vh></v>
<v t="ekr.20221004064034.1259"><vh>ASTConverter.visit_Starred</vh></v>
<v t="ekr.20221004064034.1260"><vh>ASTConverter.visit_Name</vh></v>
<v t="ekr.20221004064034.1261"><vh>ASTConverter.visit_List</vh></v>
<v t="ekr.20221004064034.1262"><vh>ASTConverter.visit_Tuple</vh></v>
<v t="ekr.20221004064034.1263"><vh>ASTConverter.--- slice ---</vh></v>
<v t="ekr.20221004064034.1264"><vh>ASTConverter.visit_Slice</vh></v>
<v t="ekr.20221004064034.1265"><vh>ASTConverter.visit_ExtSlice</vh></v>
<v t="ekr.20221004064034.1266"><vh>ASTConverter.visit_Index</vh></v>
<v t="ekr.20221004064034.1267"><vh>ASTConverter.visit_Match</vh></v>
<v t="ekr.20221004064034.1268"><vh>ASTConverter.visit_MatchValue</vh></v>
<v t="ekr.20221004064034.1269"><vh>ASTConverter.visit_MatchSingleton</vh></v>
<v t="ekr.20221004064034.1270"><vh>ASTConverter.visit_MatchSequence</vh></v>
<v t="ekr.20221004064034.1271"><vh>ASTConverter.visit_MatchStar</vh></v>
<v t="ekr.20221004064034.1272"><vh>ASTConverter.visit_MatchMapping</vh></v>
<v t="ekr.20221004064034.1273"><vh>ASTConverter.visit_MatchClass</vh></v>
<v t="ekr.20221004064034.1274"><vh>ASTConverter.visit_MatchAs</vh></v>
<v t="ekr.20221004064034.1275"><vh>ASTConverter.visit_MatchOr</vh></v>
</v>
<v t="ekr.20221004064034.1276"><vh>class TypeConverter</vh>
<v t="ekr.20221004064034.1277"><vh>TypeConverter.__init__</vh></v>
<v t="ekr.20221004064034.1278"><vh>TypeConverter.convert_column</vh></v>
<v t="ekr.20221004064034.1279"><vh>TypeConverter.invalid_type</vh></v>
<v t="ekr.20221004064034.1280"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20221004064034.1281"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20221004064034.1282"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20221004064034.1283"><vh>TypeConverter.parent</vh></v>
<v t="ekr.20221004064034.1284"><vh>TypeConverter.fail</vh></v>
<v t="ekr.20221004064034.1285"><vh>TypeConverter.note</vh></v>
<v t="ekr.20221004064034.1286"><vh>TypeConverter.translate_expr_list</vh></v>
<v t="ekr.20221004064034.1287"><vh>TypeConverter.visit_Call</vh></v>
<v t="ekr.20221004064034.1288"><vh>TypeConverter.translate_argument_list</vh></v>
<v t="ekr.20221004064034.1289"><vh>TypeConverter._extract_argument_name</vh></v>
<v t="ekr.20221004064034.1290"><vh>TypeConverter.visit_Name</vh></v>
<v t="ekr.20221004064034.1291"><vh>TypeConverter.visit_BinOp</vh></v>
<v t="ekr.20221004064034.1292"><vh>TypeConverter.visit_NameConstant</vh></v>
<v t="ekr.20221004064034.1293"><vh>TypeConverter.visit_Constant</vh></v>
<v t="ekr.20221004064034.1294"><vh>TypeConverter.visit_UnaryOp</vh></v>
<v t="ekr.20221004064034.1295"><vh>TypeConverter.numeric_type</vh></v>
<v t="ekr.20221004064034.1296"><vh>TypeConverter.These next three methods are only used if we are on python &lt;</vh></v>
<v t="ekr.20221004064034.1297"><vh>TypeConverter.visit_Num</vh></v>
<v t="ekr.20221004064034.1298"><vh>TypeConverter.visit_Str</vh></v>
<v t="ekr.20221004064034.1299"><vh>TypeConverter.visit_Bytes</vh></v>
<v t="ekr.20221004064034.1300"><vh>TypeConverter.visit_Index</vh></v>
<v t="ekr.20221004064034.1301"><vh>TypeConverter.visit_Slice</vh></v>
<v t="ekr.20221004064034.1302"><vh>TypeConverter.visit_Subscript</vh></v>
<v t="ekr.20221004064034.1303"><vh>TypeConverter.visit_Tuple</vh></v>
<v t="ekr.20221004064034.1304"><vh>TypeConverter.visit_Attribute</vh></v>
<v t="ekr.20221004064034.1305"><vh>TypeConverter.visit_Ellipsis</vh></v>
<v t="ekr.20221004064034.1306"><vh>TypeConverter.visit_List</vh></v>
</v>
<v t="ekr.20221004064034.1307"><vh>stringify_name</vh></v>
</v>
<v t="ekr.20221004064034.1412"><vh>@clean indirection.py</vh>
<v t="ekr.20221004064034.1413"><vh>extract_module_names</vh></v>
<v t="ekr.20221004064034.1414"><vh>class TypeIndirectionVisitor</vh>
<v t="ekr.20221004064034.1415"><vh>TypeIndirectionVisitor.__init__</vh></v>
<v t="ekr.20221004064034.1416"><vh>TypeIndirectionVisitor.find_modules</vh></v>
<v t="ekr.20221004064034.1417"><vh>TypeIndirectionVisitor._visit</vh></v>
<v t="ekr.20221004064034.1418"><vh>TypeIndirectionVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064034.1419"><vh>TypeIndirectionVisitor.visit_any</vh></v>
<v t="ekr.20221004064034.1420"><vh>TypeIndirectionVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064034.1421"><vh>TypeIndirectionVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064034.1422"><vh>TypeIndirectionVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064034.1423"><vh>TypeIndirectionVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064034.1424"><vh>TypeIndirectionVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064034.1425"><vh>TypeIndirectionVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064034.1426"><vh>TypeIndirectionVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064034.1427"><vh>TypeIndirectionVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064034.1428"><vh>TypeIndirectionVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064034.1429"><vh>TypeIndirectionVisitor.visit_instance</vh></v>
<v t="ekr.20221004064034.1430"><vh>TypeIndirectionVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064034.1431"><vh>TypeIndirectionVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064034.1432"><vh>TypeIndirectionVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064034.1433"><vh>TypeIndirectionVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064034.1434"><vh>TypeIndirectionVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064034.1435"><vh>TypeIndirectionVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064034.1436"><vh>TypeIndirectionVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064034.1437"><vh>TypeIndirectionVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064034.1438"><vh>TypeIndirectionVisitor.visit_type_alias_type</vh></v>
</v>
</v>
<v t="ekr.20221004064034.1443"><vh>@clean inspections.py</vh>
<v t="ekr.20221004064034.1444"><vh>node_starts_after</vh></v>
<v t="ekr.20221004064034.1445"><vh>node_ends_before</vh></v>
<v t="ekr.20221004064034.1446"><vh>expr_span</vh></v>
<v t="ekr.20221004064034.1447"><vh>get_instance_fallback</vh></v>
<v t="ekr.20221004064034.1448"><vh>find_node</vh></v>
<v t="ekr.20221004064034.1449"><vh>find_module_by_fullname</vh></v>
<v t="ekr.20221004064034.1450"><vh>class SearchVisitor</vh>
<v t="ekr.20221004064034.1451"><vh>SearchVisitor.__init__</vh></v>
<v t="ekr.20221004064034.1452"><vh>SearchVisitor.visit</vh></v>
</v>
<v t="ekr.20221004064034.1453"><vh>find_by_location</vh></v>
<v t="ekr.20221004064034.1454"><vh>class SearchAllVisitor</vh>
<v t="ekr.20221004064034.1455"><vh>SearchAllVisitor.__init__</vh></v>
<v t="ekr.20221004064034.1456"><vh>SearchAllVisitor.visit</vh></v>
</v>
<v t="ekr.20221004064034.1457"><vh>find_all_by_location</vh></v>
<v t="ekr.20221004064034.1458"><vh>class InspectionEngine</vh>
<v t="ekr.20221004064034.1459"><vh>InspectionEngine.__init__</vh></v>
<v t="ekr.20221004064034.1460"><vh>InspectionEngine.parse_location</vh></v>
<v t="ekr.20221004064034.1461"><vh>InspectionEngine.reload_module</vh></v>
<v t="ekr.20221004064034.1462"><vh>InspectionEngine.expr_type</vh></v>
<v t="ekr.20221004064034.1463"><vh>InspectionEngine.object_type</vh></v>
<v t="ekr.20221004064034.1464"><vh>InspectionEngine.collect_attrs</vh></v>
<v t="ekr.20221004064034.1465"><vh>InspectionEngine._fill_from_dict</vh></v>
<v t="ekr.20221004064034.1466"><vh>InspectionEngine.expr_attrs</vh></v>
<v t="ekr.20221004064034.1467"><vh>InspectionEngine.format_node</vh></v>
<v t="ekr.20221004064034.1468"><vh>InspectionEngine.collect_nodes</vh></v>
<v t="ekr.20221004064034.1469"><vh>InspectionEngine.modules_for_nodes</vh></v>
<v t="ekr.20221004064034.1470"><vh>InspectionEngine.expression_def</vh></v>
<v t="ekr.20221004064034.1471"><vh>InspectionEngine.missing_type</vh></v>
<v t="ekr.20221004064034.1472"><vh>InspectionEngine.missing_node</vh></v>
<v t="ekr.20221004064034.1473"><vh>InspectionEngine.add_prefixes</vh></v>
<v t="ekr.20221004064034.1474"><vh>InspectionEngine.run_inspection_by_exact_location</vh></v>
<v t="ekr.20221004064034.1475"><vh>InspectionEngine.run_inspection_by_position</vh></v>
<v t="ekr.20221004064034.1476"><vh>InspectionEngine.find_module</vh></v>
<v t="ekr.20221004064034.1477"><vh>InspectionEngine.run_inspection</vh></v>
<v t="ekr.20221004064034.1478"><vh>InspectionEngine.get_type</vh></v>
<v t="ekr.20221004064034.1479"><vh>InspectionEngine.get_attrs</vh></v>
<v t="ekr.20221004064034.1480"><vh>InspectionEngine.get_definition</vh></v>
</v>
</v>
<v t="ekr.20221004064035.111"><vh>@clean lookup.py</vh>
<v t="ekr.20221004064035.112"><vh>lookup_fully_qualified</vh></v>
</v>
<v t="ekr.20221004064035.407"><vh>@clean mixedtraverser.py</vh>
<v t="ekr.20221004064035.408"><vh>class MixedTraverserVisitor</vh>
<v t="ekr.20221004064035.409"><vh>MixedTraverserVisitor.visit_var</vh></v>
<v t="ekr.20221004064035.410"><vh>MixedTraverserVisitor.visit_func</vh></v>
<v t="ekr.20221004064035.411"><vh>MixedTraverserVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064035.412"><vh>MixedTraverserVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20221004064035.413"><vh>MixedTraverserVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20221004064035.414"><vh>MixedTraverserVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20221004064035.415"><vh>MixedTraverserVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064035.416"><vh>MixedTraverserVisitor.visit__promote_expr</vh></v>
<v t="ekr.20221004064035.417"><vh>MixedTraverserVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20221004064035.418"><vh>MixedTraverserVisitor.Statements</vh></v>
<v t="ekr.20221004064035.419"><vh>MixedTraverserVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.420"><vh>MixedTraverserVisitor.visit_for_stmt</vh></v>
<v t="ekr.20221004064035.421"><vh>MixedTraverserVisitor.visit_with_stmt</vh></v>
<v t="ekr.20221004064035.422"><vh>MixedTraverserVisitor.Expressions</vh></v>
<v t="ekr.20221004064035.423"><vh>MixedTraverserVisitor.visit_cast_expr</vh></v>
<v t="ekr.20221004064035.424"><vh>MixedTraverserVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064035.425"><vh>MixedTraverserVisitor.visit_type_application</vh></v>
<v t="ekr.20221004064035.426"><vh>MixedTraverserVisitor.Helpers</vh></v>
<v t="ekr.20221004064035.427"><vh>MixedTraverserVisitor.visit_optional_type</vh></v>
</v>
</v>
<v t="ekr.20221004064035.485"><vh>@clean nodes.py</vh>
<v t="ekr.20221004064035.486"><vh>class Context</vh>
<v t="ekr.20221004064035.487"><vh>Context.__init__</vh></v>
<v t="ekr.20221004064035.488"><vh>Context.set_line</vh></v>
<v t="ekr.20221004064035.489"><vh>Context.get_line</vh></v>
<v t="ekr.20221004064035.490"><vh>Context.get_column</vh></v>
</v>
<v t="ekr.20221004064035.491"><vh>if TYPE_CHECKING:</vh></v>
<v t="ekr.20221004064035.492"><vh>get_nongen_builtins</vh></v>
<v t="ekr.20221004064035.493"><vh>RUNTIME_PROTOCOL_DECOS: Final = (</vh></v>
<v t="ekr.20221004064035.494"><vh>class Node</vh>
<v t="ekr.20221004064035.495"><vh>Node.__str__</vh></v>
<v t="ekr.20221004064035.496"><vh>Node.accept</vh></v>
</v>
<v t="ekr.20221004064035.497"><vh>class Statement</vh>
<v t="ekr.20221004064035.498"><vh>Statement.accept</vh></v>
</v>
<v t="ekr.20221004064035.499"><vh>class Expression</vh>
<v t="ekr.20221004064035.500"><vh>Expression.accept</vh></v>
</v>
<v t="ekr.20221004064035.501"><vh>class FakeExpression</vh></v>
<v t="ekr.20221004064035.502"><vh>TODO:</vh></v>
<v t="ekr.20221004064035.503"><vh>class SymbolNode</vh>
<v t="ekr.20221004064035.504"><vh>SymbolNode.name</vh></v>
<v t="ekr.20221004064035.505"><vh>SymbolNode.fullname</vh></v>
<v t="ekr.20221004064035.506"><vh>SymbolNode.serialize</vh></v>
<v t="ekr.20221004064035.507"><vh>SymbolNode.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.508"><vh>Items: fullname, related symbol table node, surrounding type (if any)</vh></v>
<v t="ekr.20221004064035.509"><vh>class MypyFile</vh>
<v t="ekr.20221004064035.510"><vh>MypyFile.__init__</vh></v>
<v t="ekr.20221004064035.511"><vh>MypyFile.local_definitions</vh></v>
<v t="ekr.20221004064035.512"><vh>MypyFile.name</vh></v>
<v t="ekr.20221004064035.513"><vh>MypyFile.fullname</vh></v>
<v t="ekr.20221004064035.514"><vh>MypyFile.accept</vh></v>
<v t="ekr.20221004064035.515"><vh>MypyFile.is_package_init_file</vh></v>
<v t="ekr.20221004064035.516"><vh>MypyFile.is_future_flag_set</vh></v>
<v t="ekr.20221004064035.517"><vh>MypyFile.serialize</vh></v>
<v t="ekr.20221004064035.518"><vh>MypyFile.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.519"><vh>class ImportBase</vh>
<v t="ekr.20221004064035.520"><vh>ImportBase.__init__</vh></v>
</v>
<v t="ekr.20221004064035.521"><vh>class Import</vh>
<v t="ekr.20221004064035.522"><vh>Import.__init__</vh></v>
<v t="ekr.20221004064035.523"><vh>Import.accept</vh></v>
</v>
<v t="ekr.20221004064035.524"><vh>class ImportFrom</vh>
<v t="ekr.20221004064035.525"><vh>ImportFrom.__init__</vh></v>
<v t="ekr.20221004064035.526"><vh>ImportFrom.accept</vh></v>
</v>
<v t="ekr.20221004064035.527"><vh>class ImportAll</vh>
<v t="ekr.20221004064035.528"><vh>ImportAll.__init__</vh></v>
<v t="ekr.20221004064035.529"><vh>ImportAll.accept</vh></v>
</v>
<v t="ekr.20221004064035.530"><vh>class ImportedName</vh>
<v t="ekr.20221004064035.531"><vh>ImportedName.__init__</vh></v>
<v t="ekr.20221004064035.532"><vh>ImportedName.name</vh></v>
<v t="ekr.20221004064035.533"><vh>ImportedName.fullname</vh></v>
<v t="ekr.20221004064035.534"><vh>ImportedName.serialize</vh></v>
<v t="ekr.20221004064035.535"><vh>ImportedName.deserialize</vh></v>
<v t="ekr.20221004064035.536"><vh>ImportedName.__str__</vh></v>
</v>
<v t="ekr.20221004064035.537"><vh>FUNCBASE_FLAGS: Final = ["is_property", "is_class", "is_static", "is_final"]</vh></v>
<v t="ekr.20221004064035.538"><vh>class FuncBase</vh>
<v t="ekr.20221004064035.539"><vh>FuncBase.__init__</vh></v>
<v t="ekr.20221004064035.540"><vh>FuncBase.name</vh></v>
<v t="ekr.20221004064035.541"><vh>FuncBase.fullname</vh></v>
</v>
<v t="ekr.20221004064035.542"><vh>OverloadPart: _TypeAlias = Union["FuncDef", "Decorator"]</vh></v>
<v t="ekr.20221004064035.543"><vh>class OverloadedFuncDef</vh>
<v t="ekr.20221004064035.544"><vh>OverloadedFuncDef.__init__</vh></v>
<v t="ekr.20221004064035.545"><vh>OverloadedFuncDef.name</vh></v>
<v t="ekr.20221004064035.546"><vh>OverloadedFuncDef.accept</vh></v>
<v t="ekr.20221004064035.547"><vh>OverloadedFuncDef.serialize</vh></v>
<v t="ekr.20221004064035.548"><vh>OverloadedFuncDef.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.549"><vh>class Argument</vh>
<v t="ekr.20221004064035.550"><vh>Argument.__init__</vh></v>
<v t="ekr.20221004064035.551"><vh>Argument.set_line</vh></v>
</v>
<v t="ekr.20221004064035.552"><vh>FUNCITEM_FLAGS: Final = FUNCBASE_FLAGS + [</vh></v>
<v t="ekr.20221004064035.553"><vh>class FuncItem</vh>
<v t="ekr.20221004064035.554"><vh>FuncItem.__init__</vh></v>
<v t="ekr.20221004064035.555"><vh>FuncItem.max_fixed_argc</vh></v>
<v t="ekr.20221004064035.556"><vh>FuncItem.set_line</vh></v>
<v t="ekr.20221004064035.557"><vh>FuncItem.is_dynamic</vh></v>
</v>
<v t="ekr.20221004064035.558"><vh>FUNCDEF_FLAGS: Final = FUNCITEM_FLAGS + [</vh></v>
<v t="ekr.20221004064035.559"><vh>class FuncDef</vh>
<v t="ekr.20221004064035.560"><vh>FuncDef.__init__</vh></v>
<v t="ekr.20221004064035.561"><vh>FuncDef.name</vh></v>
<v t="ekr.20221004064035.562"><vh>FuncDef.accept</vh></v>
<v t="ekr.20221004064035.563"><vh>FuncDef.serialize</vh></v>
<v t="ekr.20221004064035.564"><vh>FuncDef.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.565"><vh>All types that are both SymbolNodes and FuncBases. See the FuncBase</vh></v>
<v t="ekr.20221004064035.566"><vh>class Decorator</vh>
<v t="ekr.20221004064035.567"><vh>Decorator.__init__</vh></v>
<v t="ekr.20221004064035.568"><vh>Decorator.name</vh></v>
<v t="ekr.20221004064035.569"><vh>Decorator.fullname</vh></v>
<v t="ekr.20221004064035.570"><vh>Decorator.is_final</vh></v>
<v t="ekr.20221004064035.571"><vh>Decorator.info</vh></v>
<v t="ekr.20221004064035.572"><vh>Decorator.type</vh></v>
<v t="ekr.20221004064035.573"><vh>Decorator.accept</vh></v>
<v t="ekr.20221004064035.574"><vh>Decorator.serialize</vh></v>
<v t="ekr.20221004064035.575"><vh>Decorator.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.576"><vh>VAR_FLAGS: Final = [</vh></v>
<v t="ekr.20221004064035.577"><vh>class Var</vh>
<v t="ekr.20221004064035.578"><vh>Var.__init__</vh></v>
<v t="ekr.20221004064035.579"><vh>Var.name</vh></v>
<v t="ekr.20221004064035.580"><vh>Var.fullname</vh></v>
<v t="ekr.20221004064035.581"><vh>Var.accept</vh></v>
<v t="ekr.20221004064035.582"><vh>Var.serialize</vh></v>
<v t="ekr.20221004064035.583"><vh>Var.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.584"><vh>class ClassDef</vh>
<v t="ekr.20221004064035.585"><vh>ClassDef.__init__</vh></v>
<v t="ekr.20221004064035.586"><vh>ClassDef.accept</vh></v>
<v t="ekr.20221004064035.587"><vh>ClassDef.is_generic</vh></v>
<v t="ekr.20221004064035.588"><vh>ClassDef.serialize</vh></v>
<v t="ekr.20221004064035.589"><vh>ClassDef.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.590"><vh>class GlobalDecl</vh>
<v t="ekr.20221004064035.591"><vh>GlobalDecl.__init__</vh></v>
<v t="ekr.20221004064035.592"><vh>GlobalDecl.accept</vh></v>
</v>
<v t="ekr.20221004064035.593"><vh>class NonlocalDecl</vh>
<v t="ekr.20221004064035.594"><vh>NonlocalDecl.__init__</vh></v>
<v t="ekr.20221004064035.595"><vh>NonlocalDecl.accept</vh></v>
</v>
<v t="ekr.20221004064035.596"><vh>class Block</vh>
<v t="ekr.20221004064035.597"><vh>Block.__init__</vh></v>
<v t="ekr.20221004064035.598"><vh>Block.accept</vh></v>
</v>
<v t="ekr.20221004064035.599"><vh>Statements</vh></v>
<v t="ekr.20221004064035.600"><vh>class ExpressionStmt</vh>
<v t="ekr.20221004064035.601"><vh>ExpressionStmt.__init__</vh></v>
<v t="ekr.20221004064035.602"><vh>ExpressionStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.603"><vh>class AssignmentStmt</vh>
<v t="ekr.20221004064035.604"><vh>AssignmentStmt.__init__</vh></v>
<v t="ekr.20221004064035.605"><vh>AssignmentStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.606"><vh>class OperatorAssignmentStmt</vh>
<v t="ekr.20221004064035.607"><vh>OperatorAssignmentStmt.__init__</vh></v>
<v t="ekr.20221004064035.608"><vh>OperatorAssignmentStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.609"><vh>class WhileStmt</vh>
<v t="ekr.20221004064035.610"><vh>WhileStmt.__init__</vh></v>
<v t="ekr.20221004064035.611"><vh>WhileStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.612"><vh>class ForStmt</vh>
<v t="ekr.20221004064035.613"><vh>ForStmt.__init__</vh></v>
<v t="ekr.20221004064035.614"><vh>ForStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.615"><vh>class ReturnStmt</vh>
<v t="ekr.20221004064035.616"><vh>ReturnStmt.__init__</vh></v>
<v t="ekr.20221004064035.617"><vh>ReturnStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.618"><vh>class AssertStmt</vh>
<v t="ekr.20221004064035.619"><vh>AssertStmt.__init__</vh></v>
<v t="ekr.20221004064035.620"><vh>AssertStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.621"><vh>class DelStmt</vh>
<v t="ekr.20221004064035.622"><vh>DelStmt.__init__</vh></v>
<v t="ekr.20221004064035.623"><vh>DelStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.624"><vh>class BreakStmt</vh></v>
<v t="ekr.20221004064035.625"><vh>class ContinueStmt</vh></v>
<v t="ekr.20221004064035.626"><vh>class PassStmt</vh></v>
<v t="ekr.20221004064035.627"><vh>class IfStmt</vh>
<v t="ekr.20221004064035.628"><vh>IfStmt.__init__</vh></v>
<v t="ekr.20221004064035.629"><vh>IfStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.630"><vh>class RaiseStmt</vh>
<v t="ekr.20221004064035.631"><vh>RaiseStmt.__init__</vh></v>
<v t="ekr.20221004064035.632"><vh>RaiseStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.633"><vh>class TryStmt</vh>
<v t="ekr.20221004064035.634"><vh>TryStmt.__init__</vh></v>
<v t="ekr.20221004064035.635"><vh>TryStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.636"><vh>class WithStmt</vh>
<v t="ekr.20221004064035.637"><vh>WithStmt.__init__</vh></v>
<v t="ekr.20221004064035.638"><vh>WithStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.639"><vh>class MatchStmt</vh>
<v t="ekr.20221004064035.640"><vh>MatchStmt.__init__</vh></v>
<v t="ekr.20221004064035.641"><vh>MatchStmt.accept</vh></v>
</v>
<v t="ekr.20221004064035.642"><vh>Expressions</vh></v>
<v t="ekr.20221004064035.643"><vh>class IntExpr</vh>
<v t="ekr.20221004064035.644"><vh>IntExpr.__init__</vh></v>
<v t="ekr.20221004064035.645"><vh>IntExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.646"><vh>How mypy uses StrExpr and BytesExpr:</vh></v>
<v t="ekr.20221004064035.647"><vh>class StrExpr</vh>
<v t="ekr.20221004064035.648"><vh>StrExpr.__init__</vh></v>
<v t="ekr.20221004064035.649"><vh>StrExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.650"><vh>class BytesExpr</vh>
<v t="ekr.20221004064035.651"><vh>BytesExpr.__init__</vh></v>
<v t="ekr.20221004064035.652"><vh>BytesExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.653"><vh>class FloatExpr</vh>
<v t="ekr.20221004064035.654"><vh>FloatExpr.__init__</vh></v>
<v t="ekr.20221004064035.655"><vh>FloatExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.656"><vh>class ComplexExpr</vh>
<v t="ekr.20221004064035.657"><vh>ComplexExpr.__init__</vh></v>
<v t="ekr.20221004064035.658"><vh>ComplexExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.659"><vh>class EllipsisExpr</vh></v>
<v t="ekr.20221004064035.660"><vh>class StarExpr</vh>
<v t="ekr.20221004064035.661"><vh>StarExpr.__init__</vh></v>
<v t="ekr.20221004064035.662"><vh>StarExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.663"><vh>class RefExpr</vh>
<v t="ekr.20221004064035.664"><vh>RefExpr.__init__</vh></v>
</v>
<v t="ekr.20221004064035.665"><vh>class NameExpr</vh>
<v t="ekr.20221004064035.666"><vh>NameExpr.__init__</vh></v>
<v t="ekr.20221004064035.667"><vh>NameExpr.accept</vh></v>
<v t="ekr.20221004064035.668"><vh>NameExpr.serialize</vh></v>
</v>
<v t="ekr.20221004064035.669"><vh>class MemberExpr</vh>
<v t="ekr.20221004064035.670"><vh>MemberExpr.__init__</vh></v>
<v t="ekr.20221004064035.671"><vh>MemberExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.672"><vh>class ArgKind</vh>
<v t="ekr.20221004064035.673"><vh>ArgKind.is_positional</vh></v>
<v t="ekr.20221004064035.674"><vh>ArgKind.is_named</vh></v>
<v t="ekr.20221004064035.675"><vh>ArgKind.is_required</vh></v>
<v t="ekr.20221004064035.676"><vh>ArgKind.is_optional</vh></v>
<v t="ekr.20221004064035.677"><vh>ArgKind.is_star</vh></v>
</v>
<v t="ekr.20221004064035.678"><vh>ARG_POS: Final = ArgKind.ARG_POS</vh></v>
<v t="ekr.20221004064035.679"><vh>class CallExpr</vh>
<v t="ekr.20221004064035.680"><vh>CallExpr.__init__</vh></v>
<v t="ekr.20221004064035.681"><vh>CallExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.682"><vh>class YieldFromExpr</vh>
<v t="ekr.20221004064035.683"><vh>YieldFromExpr.__init__</vh></v>
<v t="ekr.20221004064035.684"><vh>YieldFromExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.685"><vh>class YieldExpr</vh>
<v t="ekr.20221004064035.686"><vh>YieldExpr.__init__</vh></v>
<v t="ekr.20221004064035.687"><vh>YieldExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.688"><vh>class IndexExpr</vh>
<v t="ekr.20221004064035.689"><vh>IndexExpr.__init__</vh></v>
<v t="ekr.20221004064035.690"><vh>IndexExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.691"><vh>class UnaryExpr</vh>
<v t="ekr.20221004064035.692"><vh>UnaryExpr.__init__</vh></v>
<v t="ekr.20221004064035.693"><vh>UnaryExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.694"><vh>class AssignmentExpr</vh>
<v t="ekr.20221004064035.695"><vh>AssignmentExpr.__init__</vh></v>
<v t="ekr.20221004064035.696"><vh>AssignmentExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.697"><vh>class OpExpr</vh>
<v t="ekr.20221004064035.698"><vh>OpExpr.__init__</vh></v>
<v t="ekr.20221004064035.699"><vh>OpExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.700"><vh>class ComparisonExpr</vh>
<v t="ekr.20221004064035.701"><vh>ComparisonExpr.__init__</vh></v>
<v t="ekr.20221004064035.702"><vh>ComparisonExpr.pairwise</vh></v>
<v t="ekr.20221004064035.703"><vh>ComparisonExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.704"><vh>class SliceExpr</vh>
<v t="ekr.20221004064035.705"><vh>SliceExpr.__init__</vh></v>
<v t="ekr.20221004064035.706"><vh>SliceExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.707"><vh>class CastExpr</vh>
<v t="ekr.20221004064035.708"><vh>CastExpr.__init__</vh></v>
<v t="ekr.20221004064035.709"><vh>CastExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.710"><vh>class AssertTypeExpr</vh>
<v t="ekr.20221004064035.711"><vh>AssertTypeExpr.__init__</vh></v>
<v t="ekr.20221004064035.712"><vh>AssertTypeExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.713"><vh>class RevealExpr</vh>
<v t="ekr.20221004064035.714"><vh>RevealExpr.__init__</vh></v>
<v t="ekr.20221004064035.715"><vh>RevealExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.716"><vh>class SuperExpr</vh>
<v t="ekr.20221004064035.717"><vh>SuperExpr.__init__</vh></v>
<v t="ekr.20221004064035.718"><vh>SuperExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.719"><vh>class LambdaExpr</vh>
<v t="ekr.20221004064035.720"><vh>LambdaExpr.name</vh></v>
<v t="ekr.20221004064035.721"><vh>LambdaExpr.expr</vh></v>
<v t="ekr.20221004064035.722"><vh>LambdaExpr.accept</vh></v>
<v t="ekr.20221004064035.723"><vh>LambdaExpr.is_dynamic</vh></v>
</v>
<v t="ekr.20221004064035.724"><vh>class ListExpr</vh>
<v t="ekr.20221004064035.725"><vh>ListExpr.__init__</vh></v>
<v t="ekr.20221004064035.726"><vh>ListExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.727"><vh>class DictExpr</vh>
<v t="ekr.20221004064035.728"><vh>DictExpr.__init__</vh></v>
<v t="ekr.20221004064035.729"><vh>DictExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.730"><vh>class TupleExpr</vh>
<v t="ekr.20221004064035.731"><vh>TupleExpr.__init__</vh></v>
<v t="ekr.20221004064035.732"><vh>TupleExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.733"><vh>class SetExpr</vh>
<v t="ekr.20221004064035.734"><vh>SetExpr.__init__</vh></v>
<v t="ekr.20221004064035.735"><vh>SetExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.736"><vh>class GeneratorExpr</vh>
<v t="ekr.20221004064035.737"><vh>GeneratorExpr.__init__</vh></v>
<v t="ekr.20221004064035.738"><vh>GeneratorExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.739"><vh>class ListComprehension</vh>
<v t="ekr.20221004064035.740"><vh>ListComprehension.__init__</vh></v>
<v t="ekr.20221004064035.741"><vh>ListComprehension.accept</vh></v>
</v>
<v t="ekr.20221004064035.742"><vh>class SetComprehension</vh>
<v t="ekr.20221004064035.743"><vh>SetComprehension.__init__</vh></v>
<v t="ekr.20221004064035.744"><vh>SetComprehension.accept</vh></v>
</v>
<v t="ekr.20221004064035.745"><vh>class DictionaryComprehension</vh>
<v t="ekr.20221004064035.746"><vh>DictionaryComprehension.__init__</vh></v>
<v t="ekr.20221004064035.747"><vh>DictionaryComprehension.accept</vh></v>
</v>
<v t="ekr.20221004064035.748"><vh>class ConditionalExpr</vh>
<v t="ekr.20221004064035.749"><vh>ConditionalExpr.__init__</vh></v>
<v t="ekr.20221004064035.750"><vh>ConditionalExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.751"><vh>class TypeApplication</vh>
<v t="ekr.20221004064035.752"><vh>TypeApplication.__init__</vh></v>
<v t="ekr.20221004064035.753"><vh>TypeApplication.accept</vh></v>
</v>
<v t="ekr.20221004064035.754"><vh>Variance of a type variable. For example, T in the definition of</vh></v>
<v t="ekr.20221004064035.755"><vh>class TypeVarLikeExpr</vh>
<v t="ekr.20221004064035.756"><vh>TypeVarLikeExpr.__init__</vh></v>
<v t="ekr.20221004064035.757"><vh>TypeVarLikeExpr.name</vh></v>
<v t="ekr.20221004064035.758"><vh>TypeVarLikeExpr.fullname</vh></v>
</v>
<v t="ekr.20221004064035.759"><vh>class TypeVarExpr</vh>
<v t="ekr.20221004064035.760"><vh>TypeVarExpr.__init__</vh></v>
<v t="ekr.20221004064035.761"><vh>TypeVarExpr.accept</vh></v>
<v t="ekr.20221004064035.762"><vh>TypeVarExpr.serialize</vh></v>
<v t="ekr.20221004064035.763"><vh>TypeVarExpr.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.764"><vh>class ParamSpecExpr</vh>
<v t="ekr.20221004064035.765"><vh>ParamSpecExpr.accept</vh></v>
<v t="ekr.20221004064035.766"><vh>ParamSpecExpr.serialize</vh></v>
<v t="ekr.20221004064035.767"><vh>ParamSpecExpr.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.768"><vh>class TypeVarTupleExpr</vh>
<v t="ekr.20221004064035.769"><vh>TypeVarTupleExpr.accept</vh></v>
<v t="ekr.20221004064035.770"><vh>TypeVarTupleExpr.serialize</vh></v>
<v t="ekr.20221004064035.771"><vh>TypeVarTupleExpr.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.772"><vh>class TypeAliasExpr</vh>
<v t="ekr.20221004064035.773"><vh>TypeAliasExpr.__init__</vh></v>
<v t="ekr.20221004064035.774"><vh>TypeAliasExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.775"><vh>class NamedTupleExpr</vh>
<v t="ekr.20221004064035.776"><vh>NamedTupleExpr.__init__</vh></v>
<v t="ekr.20221004064035.777"><vh>NamedTupleExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.778"><vh>class TypedDictExpr</vh>
<v t="ekr.20221004064035.779"><vh>TypedDictExpr.__init__</vh></v>
<v t="ekr.20221004064035.780"><vh>TypedDictExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.781"><vh>class EnumCallExpr</vh>
<v t="ekr.20221004064035.782"><vh>EnumCallExpr.__init__</vh></v>
<v t="ekr.20221004064035.783"><vh>EnumCallExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.784"><vh>class PromoteExpr</vh>
<v t="ekr.20221004064035.785"><vh>PromoteExpr.__init__</vh></v>
<v t="ekr.20221004064035.786"><vh>PromoteExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.787"><vh>class NewTypeExpr</vh>
<v t="ekr.20221004064035.788"><vh>NewTypeExpr.__init__</vh></v>
<v t="ekr.20221004064035.789"><vh>NewTypeExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.790"><vh>class AwaitExpr</vh>
<v t="ekr.20221004064035.791"><vh>AwaitExpr.__init__</vh></v>
<v t="ekr.20221004064035.792"><vh>AwaitExpr.accept</vh></v>
</v>
<v t="ekr.20221004064035.793"><vh>Constants</vh></v>
<v t="ekr.20221004064035.794"><vh>class TempNode</vh>
<v t="ekr.20221004064035.795"><vh>TempNode.__init__</vh></v>
<v t="ekr.20221004064035.796"><vh>TempNode.__repr__</vh></v>
<v t="ekr.20221004064035.797"><vh>TempNode.accept</vh></v>
</v>
<v t="ekr.20221004064035.798"><vh>class TypeInfo</vh>
<v t="ekr.20221004064035.799"><vh>TypeInfo.__init__</vh></v>
<v t="ekr.20221004064035.800"><vh>TypeInfo.add_type_vars</vh></v>
<v t="ekr.20221004064035.801"><vh>TypeInfo.name</vh></v>
<v t="ekr.20221004064035.802"><vh>TypeInfo.fullname</vh></v>
<v t="ekr.20221004064035.803"><vh>TypeInfo.is_generic</vh></v>
<v t="ekr.20221004064035.804"><vh>TypeInfo.get</vh></v>
<v t="ekr.20221004064035.805"><vh>TypeInfo.get_containing_type_info</vh></v>
<v t="ekr.20221004064035.806"><vh>TypeInfo.protocol_members</vh></v>
<v t="ekr.20221004064035.807"><vh>TypeInfo.__getitem__</vh></v>
<v t="ekr.20221004064035.808"><vh>TypeInfo.__repr__</vh></v>
<v t="ekr.20221004064035.809"><vh>TypeInfo.__bool__</vh></v>
<v t="ekr.20221004064035.810"><vh>TypeInfo.has_readable_member</vh></v>
<v t="ekr.20221004064035.811"><vh>TypeInfo.get_method</vh></v>
<v t="ekr.20221004064035.812"><vh>TypeInfo.calculate_metaclass_type</vh></v>
<v t="ekr.20221004064035.813"><vh>TypeInfo.is_metaclass</vh></v>
<v t="ekr.20221004064035.814"><vh>TypeInfo.has_base</vh></v>
<v t="ekr.20221004064035.815"><vh>TypeInfo.direct_base_classes</vh></v>
<v t="ekr.20221004064035.816"><vh>TypeInfo.update_tuple_type</vh></v>
<v t="ekr.20221004064035.817"><vh>TypeInfo.update_typeddict_type</vh></v>
<v t="ekr.20221004064035.818"><vh>TypeInfo.__str__</vh></v>
<v t="ekr.20221004064035.819"><vh>TypeInfo.dump</vh></v>
<v t="ekr.20221004064035.820"><vh>TypeInfo.serialize</vh></v>
<v t="ekr.20221004064035.821"><vh>TypeInfo.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.822"><vh>class FakeInfo</vh>
<v t="ekr.20221004064035.823"><vh>FakeInfo.__init__</vh></v>
<v t="ekr.20221004064035.824"><vh>FakeInfo.__getattribute__</vh></v>
</v>
<v t="ekr.20221004064035.825"><vh>VAR_NO_INFO: Final[TypeInfo] = FakeInfo("Var is lacking info")</vh></v>
<v t="ekr.20221004064035.826"><vh>class TypeAlias</vh>
<v t="ekr.20221004064035.827"><vh>TypeAlias.__init__</vh></v>
<v t="ekr.20221004064035.828"><vh>TypeAlias.from_tuple_type</vh></v>
<v t="ekr.20221004064035.829"><vh>TypeAlias.from_typeddict_type</vh></v>
<v t="ekr.20221004064035.830"><vh>TypeAlias.name</vh></v>
<v t="ekr.20221004064035.831"><vh>TypeAlias.fullname</vh></v>
<v t="ekr.20221004064035.832"><vh>TypeAlias.serialize</vh></v>
<v t="ekr.20221004064035.833"><vh>TypeAlias.accept</vh></v>
<v t="ekr.20221004064035.834"><vh>TypeAlias.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.835"><vh>class PlaceholderNode</vh>
<v t="ekr.20221004064035.836"><vh>PlaceholderNode.__init__</vh></v>
<v t="ekr.20221004064035.837"><vh>PlaceholderNode.name</vh></v>
<v t="ekr.20221004064035.838"><vh>PlaceholderNode.fullname</vh></v>
<v t="ekr.20221004064035.839"><vh>PlaceholderNode.serialize</vh></v>
<v t="ekr.20221004064035.840"><vh>PlaceholderNode.accept</vh></v>
</v>
<v t="ekr.20221004064035.841"><vh>class SymbolTableNode</vh>
<v t="ekr.20221004064035.842"><vh>SymbolTableNode.__init__</vh></v>
<v t="ekr.20221004064035.843"><vh>SymbolTableNode.fullname</vh></v>
<v t="ekr.20221004064035.844"><vh>SymbolTableNode.type</vh></v>
<v t="ekr.20221004064035.845"><vh>SymbolTableNode.copy</vh></v>
<v t="ekr.20221004064035.846"><vh>SymbolTableNode.__str__</vh></v>
<v t="ekr.20221004064035.847"><vh>SymbolTableNode.serialize</vh></v>
<v t="ekr.20221004064035.848"><vh>SymbolTableNode.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.849"><vh>class SymbolTable</vh>
<v t="ekr.20221004064035.850"><vh>SymbolTable.__str__</vh></v>
<v t="ekr.20221004064035.851"><vh>SymbolTable.copy</vh></v>
<v t="ekr.20221004064035.852"><vh>SymbolTable.serialize</vh></v>
<v t="ekr.20221004064035.853"><vh>SymbolTable.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.854"><vh>get_flags</vh></v>
<v t="ekr.20221004064035.855"><vh>set_flags</vh></v>
<v t="ekr.20221004064035.856"><vh>get_member_expr_fullname</vh></v>
<v t="ekr.20221004064035.857"><vh>deserialize_map: Final = {</vh></v>
<v t="ekr.20221004064035.858"><vh>check_arg_kinds</vh></v>
<v t="ekr.20221004064035.859"><vh>check_arg_names</vh></v>
<v t="ekr.20221004064035.860"><vh>is_class_var</vh></v>
<v t="ekr.20221004064035.861"><vh>is_final_node</vh></v>
<v t="ekr.20221004064035.862"><vh>local_definitions</vh></v>
</v>
<v t="ekr.20221004064035.877"><vh>@clean parse.py</vh>
<v t="ekr.20221004064035.878"><vh>parse</vh></v>
</v>
<v t="ekr.20221004064035.879"><vh>@clean partially_defined.py</vh>
<v t="ekr.20221004064035.880"><vh>class BranchState</vh>
<v t="ekr.20221004064035.881"><vh>BranchState.__init__</vh></v>
</v>
<v t="ekr.20221004064035.882"><vh>class BranchStatement</vh>
<v t="ekr.20221004064035.883"><vh>BranchStatement.__init__</vh></v>
<v t="ekr.20221004064035.884"><vh>BranchStatement.next_branch</vh></v>
<v t="ekr.20221004064035.885"><vh>BranchStatement.record_definition</vh></v>
<v t="ekr.20221004064035.886"><vh>BranchStatement.record_nested_branch</vh></v>
<v t="ekr.20221004064035.887"><vh>BranchStatement.skip_branch</vh></v>
<v t="ekr.20221004064035.888"><vh>BranchStatement.is_possibly_undefined</vh></v>
<v t="ekr.20221004064035.889"><vh>BranchStatement.done</vh></v>
</v>
<v t="ekr.20221004064035.890"><vh>class DefinedVariableTracker</vh>
<v t="ekr.20221004064035.891"><vh>DefinedVariableTracker.__init__</vh></v>
<v t="ekr.20221004064035.892"><vh>DefinedVariableTracker._scope</vh></v>
<v t="ekr.20221004064035.893"><vh>DefinedVariableTracker.enter_scope</vh></v>
<v t="ekr.20221004064035.894"><vh>DefinedVariableTracker.exit_scope</vh></v>
<v t="ekr.20221004064035.895"><vh>DefinedVariableTracker.start_branch_statement</vh></v>
<v t="ekr.20221004064035.896"><vh>DefinedVariableTracker.next_branch</vh></v>
<v t="ekr.20221004064035.897"><vh>DefinedVariableTracker.end_branch_statement</vh></v>
<v t="ekr.20221004064035.898"><vh>DefinedVariableTracker.skip_branch</vh></v>
<v t="ekr.20221004064035.899"><vh>DefinedVariableTracker.record_declaration</vh></v>
<v t="ekr.20221004064035.900"><vh>DefinedVariableTracker.is_possibly_undefined</vh></v>
</v>
<v t="ekr.20221004064035.901"><vh>class PartiallyDefinedVariableVisitor</vh>
<v t="ekr.20221004064035.902"><vh>PartiallyDefinedVariableVisitor.__init__</vh></v>
<v t="ekr.20221004064035.903"><vh>PartiallyDefinedVariableVisitor.process_lvalue</vh></v>
<v t="ekr.20221004064035.904"><vh>PartiallyDefinedVariableVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.905"><vh>PartiallyDefinedVariableVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.906"><vh>PartiallyDefinedVariableVisitor.visit_if_stmt</vh></v>
<v t="ekr.20221004064035.907"><vh>PartiallyDefinedVariableVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064035.908"><vh>PartiallyDefinedVariableVisitor.visit_func</vh></v>
<v t="ekr.20221004064035.909"><vh>PartiallyDefinedVariableVisitor.visit_generator_expr</vh></v>
<v t="ekr.20221004064035.910"><vh>PartiallyDefinedVariableVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064035.911"><vh>PartiallyDefinedVariableVisitor.visit_for_stmt</vh></v>
<v t="ekr.20221004064035.912"><vh>PartiallyDefinedVariableVisitor.visit_return_stmt</vh></v>
<v t="ekr.20221004064035.913"><vh>PartiallyDefinedVariableVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20221004064035.914"><vh>PartiallyDefinedVariableVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20221004064035.915"><vh>PartiallyDefinedVariableVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20221004064035.916"><vh>PartiallyDefinedVariableVisitor.visit_break_stmt</vh></v>
<v t="ekr.20221004064035.917"><vh>PartiallyDefinedVariableVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.918"><vh>PartiallyDefinedVariableVisitor.visit_while_stmt</vh></v>
<v t="ekr.20221004064035.919"><vh>PartiallyDefinedVariableVisitor.visit_name_expr</vh></v>
<v t="ekr.20221004064035.920"><vh>PartiallyDefinedVariableVisitor.visit_with_stmt</vh></v>
</v>
</v>
<v t="ekr.20221004064035.921"><vh>@clean patterns.py</vh>
<v t="ekr.20221004064035.922"><vh>class Pattern</vh>
<v t="ekr.20221004064035.923"><vh>Pattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.924"><vh>class AsPattern</vh>
<v t="ekr.20221004064035.925"><vh>AsPattern.__init__</vh></v>
<v t="ekr.20221004064035.926"><vh>AsPattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.927"><vh>class OrPattern</vh>
<v t="ekr.20221004064035.928"><vh>OrPattern.__init__</vh></v>
<v t="ekr.20221004064035.929"><vh>OrPattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.930"><vh>class ValuePattern</vh>
<v t="ekr.20221004064035.931"><vh>ValuePattern.__init__</vh></v>
<v t="ekr.20221004064035.932"><vh>ValuePattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.933"><vh>class SingletonPattern</vh>
<v t="ekr.20221004064035.934"><vh>SingletonPattern.__init__</vh></v>
<v t="ekr.20221004064035.935"><vh>SingletonPattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.936"><vh>class SequencePattern</vh>
<v t="ekr.20221004064035.937"><vh>SequencePattern.__init__</vh></v>
<v t="ekr.20221004064035.938"><vh>SequencePattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.939"><vh>class StarredPattern</vh>
<v t="ekr.20221004064035.940"><vh>StarredPattern.__init__</vh></v>
<v t="ekr.20221004064035.941"><vh>StarredPattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.942"><vh>class MappingPattern</vh>
<v t="ekr.20221004064035.943"><vh>MappingPattern.__init__</vh></v>
<v t="ekr.20221004064035.944"><vh>MappingPattern.accept</vh></v>
</v>
<v t="ekr.20221004064035.945"><vh>class ClassPattern</vh>
<v t="ekr.20221004064035.946"><vh>ClassPattern.__init__</vh></v>
<v t="ekr.20221004064035.947"><vh>ClassPattern.accept</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1639"><vh>@clean sharedparse.py</vh>
<v t="ekr.20221004064035.1640"><vh>special_function_elide_names</vh></v>
<v t="ekr.20221004064035.1641"><vh>argument_elide_name</vh></v>
</v>
<v t="ekr.20221004064035.2185"><vh>@clean traverser.py</vh>
<v t="ekr.20221004064035.2186"><vh>class TraverserVisitor</vh>
<v t="ekr.20221004064035.2187"><vh>TraverserVisitor.__init__</vh></v>
<v t="ekr.20221004064035.2188"><vh>TraverserVisitor.Visit methods</vh></v>
<v t="ekr.20221004064035.2189"><vh>TraverserVisitor.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.2190"><vh>TraverserVisitor.visit_block</vh></v>
<v t="ekr.20221004064035.2191"><vh>TraverserVisitor.visit_func</vh></v>
<v t="ekr.20221004064035.2192"><vh>TraverserVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064035.2193"><vh>TraverserVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064035.2194"><vh>TraverserVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064035.2195"><vh>TraverserVisitor.visit_decorator</vh></v>
<v t="ekr.20221004064035.2196"><vh>TraverserVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.2197"><vh>TraverserVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.2198"><vh>TraverserVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064035.2199"><vh>TraverserVisitor.visit_while_stmt</vh></v>
<v t="ekr.20221004064035.2200"><vh>TraverserVisitor.visit_for_stmt</vh></v>
<v t="ekr.20221004064035.2201"><vh>TraverserVisitor.visit_return_stmt</vh></v>
<v t="ekr.20221004064035.2202"><vh>TraverserVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20221004064035.2203"><vh>TraverserVisitor.visit_del_stmt</vh></v>
<v t="ekr.20221004064035.2204"><vh>TraverserVisitor.visit_if_stmt</vh></v>
<v t="ekr.20221004064035.2205"><vh>TraverserVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20221004064035.2206"><vh>TraverserVisitor.visit_try_stmt</vh></v>
<v t="ekr.20221004064035.2207"><vh>TraverserVisitor.visit_with_stmt</vh></v>
<v t="ekr.20221004064035.2208"><vh>TraverserVisitor.visit_match_stmt</vh></v>
<v t="ekr.20221004064035.2209"><vh>TraverserVisitor.visit_member_expr</vh></v>
<v t="ekr.20221004064035.2210"><vh>TraverserVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064035.2211"><vh>TraverserVisitor.visit_yield_expr</vh></v>
<v t="ekr.20221004064035.2212"><vh>TraverserVisitor.visit_call_expr</vh></v>
<v t="ekr.20221004064035.2213"><vh>TraverserVisitor.visit_op_expr</vh></v>
<v t="ekr.20221004064035.2214"><vh>TraverserVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20221004064035.2215"><vh>TraverserVisitor.visit_slice_expr</vh></v>
<v t="ekr.20221004064035.2216"><vh>TraverserVisitor.visit_cast_expr</vh></v>
<v t="ekr.20221004064035.2217"><vh>TraverserVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064035.2218"><vh>TraverserVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20221004064035.2219"><vh>TraverserVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.2220"><vh>TraverserVisitor.visit_unary_expr</vh></v>
<v t="ekr.20221004064035.2221"><vh>TraverserVisitor.visit_list_expr</vh></v>
<v t="ekr.20221004064035.2222"><vh>TraverserVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20221004064035.2223"><vh>TraverserVisitor.visit_dict_expr</vh></v>
<v t="ekr.20221004064035.2224"><vh>TraverserVisitor.visit_set_expr</vh></v>
<v t="ekr.20221004064035.2225"><vh>TraverserVisitor.visit_index_expr</vh></v>
<v t="ekr.20221004064035.2226"><vh>TraverserVisitor.visit_generator_expr</vh></v>
<v t="ekr.20221004064035.2227"><vh>TraverserVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064035.2228"><vh>TraverserVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20221004064035.2229"><vh>TraverserVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20221004064035.2230"><vh>TraverserVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20221004064035.2231"><vh>TraverserVisitor.visit_type_application</vh></v>
<v t="ekr.20221004064035.2232"><vh>TraverserVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20221004064035.2233"><vh>TraverserVisitor.visit_star_expr</vh></v>
<v t="ekr.20221004064035.2234"><vh>TraverserVisitor.visit_await_expr</vh></v>
<v t="ekr.20221004064035.2235"><vh>TraverserVisitor.visit_super_expr</vh></v>
<v t="ekr.20221004064035.2236"><vh>TraverserVisitor.visit_as_pattern</vh></v>
<v t="ekr.20221004064035.2237"><vh>TraverserVisitor.visit_or_pattern</vh></v>
<v t="ekr.20221004064035.2238"><vh>TraverserVisitor.visit_value_pattern</vh></v>
<v t="ekr.20221004064035.2239"><vh>TraverserVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20221004064035.2240"><vh>TraverserVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20221004064035.2241"><vh>TraverserVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20221004064035.2242"><vh>TraverserVisitor.visit_class_pattern</vh></v>
<v t="ekr.20221004064035.2243"><vh>TraverserVisitor.visit_import</vh></v>
<v t="ekr.20221004064035.2244"><vh>TraverserVisitor.visit_import_from</vh></v>
</v>
<v t="ekr.20221004064035.2245"><vh>class ExtendedTraverserVisitor</vh>
<v t="ekr.20221004064035.2247"><vh>visit</vh></v>
<v t="ekr.20221004064035.2248"><vh>visit_mypy_file</vh></v>
<v t="ekr.20221004064035.2249"><vh>Module structure</vh>
<v t="ekr.20221004064035.2250"><vh>visit_import</vh></v>
<v t="ekr.20221004064035.2251"><vh>visit_import_from</vh></v>
<v t="ekr.20221004064035.2252"><vh>visit_import_all</vh></v>
</v>
<v t="ekr.20221004064035.2253"><vh>Definitions</vh>
<v t="ekr.20221004064035.2254"><vh>visit_func_def</vh></v>
<v t="ekr.20221004064035.2255"><vh>visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064035.2256"><vh>visit_class_def</vh></v>
<v t="ekr.20221004064035.2257"><vh>visit_global_decl</vh></v>
<v t="ekr.20221004064035.2258"><vh>visit_nonlocal_decl</vh></v>
<v t="ekr.20221004064035.2259"><vh>visit_decorator</vh></v>
<v t="ekr.20221004064035.2260"><vh>visit_type_alias</vh></v>
</v>
<v t="ekr.20221004064035.2261"><vh>Statements</vh>
<v t="ekr.20221004064035.2262"><vh>visit_block</vh></v>
<v t="ekr.20221004064035.2263"><vh>visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.2264"><vh>visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.2265"><vh>visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064035.2266"><vh>visit_while_stmt</vh></v>
<v t="ekr.20221004064035.2267"><vh>visit_for_stmt</vh></v>
<v t="ekr.20221004064035.2268"><vh>visit_return_stmt</vh></v>
<v t="ekr.20221004064035.2269"><vh>visit_assert_stmt</vh></v>
<v t="ekr.20221004064035.2270"><vh>visit_del_stmt</vh></v>
<v t="ekr.20221004064035.2271"><vh>visit_if_stmt</vh></v>
<v t="ekr.20221004064035.2272"><vh>visit_break_stmt</vh></v>
<v t="ekr.20221004064035.2273"><vh>visit_continue_stmt</vh></v>
<v t="ekr.20221004064035.2274"><vh>visit_pass_stmt</vh></v>
<v t="ekr.20221004064035.2275"><vh>visit_raise_stmt</vh></v>
<v t="ekr.20221004064035.2276"><vh>visit_try_stmt</vh></v>
<v t="ekr.20221004064035.2277"><vh>visit_with_stmt</vh></v>
<v t="ekr.20221004064035.2278"><vh>visit_match_stmt</vh></v>
</v>
<v t="ekr.20221004064035.2279"><vh>Expressions (default no-op implementation)</vh>
<v t="ekr.20221004064035.2280"><vh>visit_int_expr</vh></v>
<v t="ekr.20221004064035.2281"><vh>visit_str_expr</vh></v>
<v t="ekr.20221004064035.2282"><vh>visit_bytes_expr</vh></v>
<v t="ekr.20221004064035.2283"><vh>visit_float_expr</vh></v>
<v t="ekr.20221004064035.2284"><vh>visit_complex_expr</vh></v>
<v t="ekr.20221004064035.2285"><vh>visit_ellipsis</vh></v>
<v t="ekr.20221004064035.2286"><vh>visit_star_expr</vh></v>
<v t="ekr.20221004064035.2287"><vh>visit_name_expr</vh></v>
<v t="ekr.20221004064035.2288"><vh>visit_member_expr</vh></v>
<v t="ekr.20221004064035.2289"><vh>visit_yield_from_expr</vh></v>
<v t="ekr.20221004064035.2290"><vh>visit_yield_expr</vh></v>
<v t="ekr.20221004064035.2291"><vh>visit_call_expr</vh></v>
<v t="ekr.20221004064035.2292"><vh>visit_op_expr</vh></v>
<v t="ekr.20221004064035.2293"><vh>visit_comparison_expr</vh></v>
<v t="ekr.20221004064035.2294"><vh>visit_cast_expr</vh></v>
<v t="ekr.20221004064035.2295"><vh>visit_assert_type_expr</vh></v>
<v t="ekr.20221004064035.2296"><vh>visit_reveal_expr</vh></v>
<v t="ekr.20221004064035.2297"><vh>visit_super_expr</vh></v>
<v t="ekr.20221004064035.2298"><vh>visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.2299"><vh>visit_unary_expr</vh></v>
<v t="ekr.20221004064035.2300"><vh>visit_list_expr</vh></v>
<v t="ekr.20221004064035.2301"><vh>visit_dict_expr</vh></v>
<v t="ekr.20221004064035.2302"><vh>visit_tuple_expr</vh></v>
<v t="ekr.20221004064035.2303"><vh>visit_set_expr</vh></v>
<v t="ekr.20221004064035.2304"><vh>visit_index_expr</vh></v>
<v t="ekr.20221004064035.2305"><vh>visit_type_application</vh></v>
<v t="ekr.20221004064035.2306"><vh>visit_lambda_expr</vh></v>
<v t="ekr.20221004064035.2307"><vh>visit_list_comprehension</vh></v>
<v t="ekr.20221004064035.2308"><vh>visit_set_comprehension</vh></v>
<v t="ekr.20221004064035.2309"><vh>visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064035.2310"><vh>visit_generator_expr</vh></v>
<v t="ekr.20221004064035.2311"><vh>visit_slice_expr</vh></v>
<v t="ekr.20221004064035.2312"><vh>visit_conditional_expr</vh></v>
<v t="ekr.20221004064035.2313"><vh>visit_type_var_expr</vh></v>
<v t="ekr.20221004064035.2314"><vh>visit_paramspec_expr</vh></v>
<v t="ekr.20221004064035.2315"><vh>visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064035.2316"><vh>visit_type_alias_expr</vh></v>
<v t="ekr.20221004064035.2317"><vh>visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064035.2318"><vh>visit_enum_call_expr</vh></v>
<v t="ekr.20221004064035.2319"><vh>visit_typeddict_expr</vh></v>
<v t="ekr.20221004064035.2320"><vh>visit_newtype_expr</vh></v>
<v t="ekr.20221004064035.2321"><vh>visit_await_expr</vh></v>
</v>
<v t="ekr.20221004064035.2322"><vh>Patterns</vh>
<v t="ekr.20221004064035.2323"><vh>visit_as_pattern</vh></v>
<v t="ekr.20221004064035.2324"><vh>visit_or_pattern</vh></v>
<v t="ekr.20221004064035.2325"><vh>visit_value_pattern</vh></v>
<v t="ekr.20221004064035.2326"><vh>visit_singleton_pattern</vh></v>
<v t="ekr.20221004064035.2327"><vh>visit_sequence_pattern</vh></v>
<v t="ekr.20221004064035.2328"><vh>visit_starred_pattern</vh></v>
<v t="ekr.20221004064035.2329"><vh>visit_mapping_pattern</vh></v>
<v t="ekr.20221004064035.2330"><vh>visit_class_pattern</vh></v>
</v>
</v>
<v t="ekr.20221004064035.2331"><vh>class ReturnSeeker</vh>
<v t="ekr.20221004064035.2332"><vh>ReturnSeeker.__init__</vh></v>
<v t="ekr.20221004064035.2333"><vh>ReturnSeeker.visit_return_stmt</vh></v>
</v>
<v t="ekr.20221004064035.2334"><vh>has_return_statement</vh></v>
<v t="ekr.20221004064035.2335"><vh>class FuncCollectorBase</vh>
<v t="ekr.20221004064035.2336"><vh>FuncCollectorBase.__init__</vh></v>
<v t="ekr.20221004064035.2337"><vh>FuncCollectorBase.visit_func_def</vh></v>
</v>
<v t="ekr.20221004064035.2338"><vh>class YieldSeeker</vh></v>
<v t="ekr.20221004064035.2339"><vh>has_yield_expression</vh></v>
<v t="ekr.20221004064035.2340"><vh>class AwaitSeeker</vh></v>
<v t="ekr.20221004064035.2341"><vh>has_await_expression</vh></v>
<v t="ekr.20221004064035.2342"><vh>class ReturnCollector</vh></v>
<v t="ekr.20221004064035.2343"><vh>all_return_statements</vh></v>
<v t="ekr.20221004064035.2344"><vh>class YieldCollector</vh>
<v t="ekr.20221004064035.2345"><vh>YieldCollector.__init__</vh></v>
<v t="ekr.20221004064035.2346"><vh>YieldCollector.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.2347"><vh>YieldCollector.visit_yield_expr</vh></v>
</v>
<v t="ekr.20221004064035.2348"><vh>all_yield_expressions</vh></v>
</v>
<v t="ekr.20221004064035.2349"><vh>@clean treetransform.py</vh>
<v t="ekr.20221004064035.2350"><vh>class TransformVisitor</vh>
<v t="ekr.20221004064035.2351"><vh>TransformVisitor.__init__</vh></v>
<v t="ekr.20221004064035.2352"><vh>TransformVisitor.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.2353"><vh>TransformVisitor.visit_import</vh></v>
<v t="ekr.20221004064035.2354"><vh>TransformVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064035.2355"><vh>TransformVisitor.visit_import_all</vh></v>
<v t="ekr.20221004064035.2356"><vh>TransformVisitor.copy_argument</vh></v>
<v t="ekr.20221004064035.2357"><vh>TransformVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064035.2358"><vh>TransformVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20221004064035.2359"><vh>TransformVisitor.copy_function_attributes</vh></v>
<v t="ekr.20221004064035.2360"><vh>TransformVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064035.2361"><vh>TransformVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064035.2362"><vh>TransformVisitor.visit_global_decl</vh></v>
<v t="ekr.20221004064035.2363"><vh>TransformVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20221004064035.2364"><vh>TransformVisitor.visit_block</vh></v>
<v t="ekr.20221004064035.2365"><vh>TransformVisitor.visit_decorator</vh></v>
<v t="ekr.20221004064035.2366"><vh>TransformVisitor.visit_var</vh></v>
<v t="ekr.20221004064035.2367"><vh>TransformVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.2368"><vh>TransformVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.2369"><vh>TransformVisitor.duplicate_assignment</vh></v>
<v t="ekr.20221004064035.2370"><vh>TransformVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064035.2371"><vh>TransformVisitor.visit_while_stmt</vh></v>
<v t="ekr.20221004064035.2372"><vh>TransformVisitor.visit_for_stmt</vh></v>
<v t="ekr.20221004064035.2373"><vh>TransformVisitor.visit_return_stmt</vh></v>
<v t="ekr.20221004064035.2374"><vh>TransformVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20221004064035.2375"><vh>TransformVisitor.visit_del_stmt</vh></v>
<v t="ekr.20221004064035.2376"><vh>TransformVisitor.visit_if_stmt</vh></v>
<v t="ekr.20221004064035.2377"><vh>TransformVisitor.visit_break_stmt</vh></v>
<v t="ekr.20221004064035.2378"><vh>TransformVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20221004064035.2379"><vh>TransformVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20221004064035.2380"><vh>TransformVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20221004064035.2381"><vh>TransformVisitor.visit_try_stmt</vh></v>
<v t="ekr.20221004064035.2382"><vh>TransformVisitor.visit_with_stmt</vh></v>
<v t="ekr.20221004064035.2383"><vh>TransformVisitor.visit_as_pattern</vh></v>
<v t="ekr.20221004064035.2384"><vh>TransformVisitor.visit_or_pattern</vh></v>
<v t="ekr.20221004064035.2385"><vh>TransformVisitor.visit_value_pattern</vh></v>
<v t="ekr.20221004064035.2386"><vh>TransformVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20221004064035.2387"><vh>TransformVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20221004064035.2388"><vh>TransformVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20221004064035.2389"><vh>TransformVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20221004064035.2390"><vh>TransformVisitor.visit_class_pattern</vh></v>
<v t="ekr.20221004064035.2391"><vh>TransformVisitor.visit_match_stmt</vh></v>
<v t="ekr.20221004064035.2392"><vh>TransformVisitor.visit_star_expr</vh></v>
<v t="ekr.20221004064035.2393"><vh>TransformVisitor.visit_int_expr</vh></v>
<v t="ekr.20221004064035.2394"><vh>TransformVisitor.visit_str_expr</vh></v>
<v t="ekr.20221004064035.2395"><vh>TransformVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20221004064035.2396"><vh>TransformVisitor.visit_float_expr</vh></v>
<v t="ekr.20221004064035.2397"><vh>TransformVisitor.visit_complex_expr</vh></v>
<v t="ekr.20221004064035.2398"><vh>TransformVisitor.visit_ellipsis</vh></v>
<v t="ekr.20221004064035.2399"><vh>TransformVisitor.visit_name_expr</vh></v>
<v t="ekr.20221004064035.2400"><vh>TransformVisitor.duplicate_name</vh></v>
<v t="ekr.20221004064035.2401"><vh>TransformVisitor.visit_member_expr</vh></v>
<v t="ekr.20221004064035.2402"><vh>TransformVisitor.copy_ref</vh></v>
<v t="ekr.20221004064035.2403"><vh>TransformVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064035.2404"><vh>TransformVisitor.visit_yield_expr</vh></v>
<v t="ekr.20221004064035.2405"><vh>TransformVisitor.visit_await_expr</vh></v>
<v t="ekr.20221004064035.2406"><vh>TransformVisitor.visit_call_expr</vh></v>
<v t="ekr.20221004064035.2407"><vh>TransformVisitor.visit_op_expr</vh></v>
<v t="ekr.20221004064035.2408"><vh>TransformVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20221004064035.2409"><vh>TransformVisitor.visit_cast_expr</vh></v>
<v t="ekr.20221004064035.2410"><vh>TransformVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064035.2411"><vh>TransformVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20221004064035.2412"><vh>TransformVisitor.visit_super_expr</vh></v>
<v t="ekr.20221004064035.2413"><vh>TransformVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.2414"><vh>TransformVisitor.visit_unary_expr</vh></v>
<v t="ekr.20221004064035.2415"><vh>TransformVisitor.visit_list_expr</vh></v>
<v t="ekr.20221004064035.2416"><vh>TransformVisitor.visit_dict_expr</vh></v>
<v t="ekr.20221004064035.2417"><vh>TransformVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20221004064035.2418"><vh>TransformVisitor.visit_set_expr</vh></v>
<v t="ekr.20221004064035.2419"><vh>TransformVisitor.visit_index_expr</vh></v>
<v t="ekr.20221004064035.2420"><vh>TransformVisitor.visit_type_application</vh></v>
<v t="ekr.20221004064035.2421"><vh>TransformVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20221004064035.2422"><vh>TransformVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20221004064035.2423"><vh>TransformVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064035.2424"><vh>TransformVisitor.visit_generator_expr</vh></v>
<v t="ekr.20221004064035.2425"><vh>TransformVisitor.duplicate_generator</vh></v>
<v t="ekr.20221004064035.2426"><vh>TransformVisitor.visit_slice_expr</vh></v>
<v t="ekr.20221004064035.2427"><vh>TransformVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20221004064035.2428"><vh>TransformVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20221004064035.2429"><vh>TransformVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20221004064035.2430"><vh>TransformVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064035.2431"><vh>TransformVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20221004064035.2432"><vh>TransformVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20221004064035.2433"><vh>TransformVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064035.2434"><vh>TransformVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20221004064035.2435"><vh>TransformVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20221004064035.2436"><vh>TransformVisitor.visit__promote_expr</vh></v>
<v t="ekr.20221004064035.2437"><vh>TransformVisitor.visit_temp_node</vh></v>
<v t="ekr.20221004064035.2438"><vh>TransformVisitor.node</vh></v>
<v t="ekr.20221004064035.2439"><vh>TransformVisitor.mypyfile</vh></v>
<v t="ekr.20221004064035.2440"><vh>TransformVisitor.expr</vh></v>
<v t="ekr.20221004064035.2441"><vh>TransformVisitor.stmt</vh></v>
<v t="ekr.20221004064035.2442"><vh>TransformVisitor.pattern</vh></v>
<v t="ekr.20221004064035.2443"><vh>TransformVisitor.Helpers</vh></v>
<v t="ekr.20221004064035.2444"><vh>TransformVisitor.optional_expr</vh></v>
<v t="ekr.20221004064035.2445"><vh>TransformVisitor.block</vh></v>
<v t="ekr.20221004064035.2446"><vh>TransformVisitor.optional_block</vh></v>
<v t="ekr.20221004064035.2447"><vh>TransformVisitor.statements</vh></v>
<v t="ekr.20221004064035.2448"><vh>TransformVisitor.expressions</vh></v>
<v t="ekr.20221004064035.2449"><vh>TransformVisitor.optional_expressions</vh></v>
<v t="ekr.20221004064035.2450"><vh>TransformVisitor.blocks</vh></v>
<v t="ekr.20221004064035.2451"><vh>TransformVisitor.names</vh></v>
<v t="ekr.20221004064035.2452"><vh>TransformVisitor.optional_names</vh></v>
<v t="ekr.20221004064035.2453"><vh>TransformVisitor.type</vh></v>
<v t="ekr.20221004064035.2454"><vh>TransformVisitor.optional_type</vh></v>
<v t="ekr.20221004064035.2455"><vh>TransformVisitor.types</vh></v>
</v>
<v t="ekr.20221004064035.2456"><vh>class FuncMapInitializer</vh>
<v t="ekr.20221004064035.2457"><vh>FuncMapInitializer.__init__</vh></v>
<v t="ekr.20221004064035.2458"><vh>FuncMapInitializer.visit_func_def</vh></v>
</v>
</v>
<v t="ekr.20221004064036.64"><vh>@clean visitor.py</vh>
<v t="ekr.20221004064036.65"><vh>class ExpressionVisitor</vh>
<v t="ekr.20221004064036.66"><vh>ExpressionVisitor.visit_int_expr</vh></v>
<v t="ekr.20221004064036.67"><vh>ExpressionVisitor.visit_str_expr</vh></v>
<v t="ekr.20221004064036.68"><vh>ExpressionVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20221004064036.69"><vh>ExpressionVisitor.visit_float_expr</vh></v>
<v t="ekr.20221004064036.70"><vh>ExpressionVisitor.visit_complex_expr</vh></v>
<v t="ekr.20221004064036.71"><vh>ExpressionVisitor.visit_ellipsis</vh></v>
<v t="ekr.20221004064036.72"><vh>ExpressionVisitor.visit_star_expr</vh></v>
<v t="ekr.20221004064036.73"><vh>ExpressionVisitor.visit_name_expr</vh></v>
<v t="ekr.20221004064036.74"><vh>ExpressionVisitor.visit_member_expr</vh></v>
<v t="ekr.20221004064036.75"><vh>ExpressionVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064036.76"><vh>ExpressionVisitor.visit_yield_expr</vh></v>
<v t="ekr.20221004064036.77"><vh>ExpressionVisitor.visit_call_expr</vh></v>
<v t="ekr.20221004064036.78"><vh>ExpressionVisitor.visit_op_expr</vh></v>
<v t="ekr.20221004064036.79"><vh>ExpressionVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20221004064036.80"><vh>ExpressionVisitor.visit_cast_expr</vh></v>
<v t="ekr.20221004064036.81"><vh>ExpressionVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064036.82"><vh>ExpressionVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20221004064036.83"><vh>ExpressionVisitor.visit_super_expr</vh></v>
<v t="ekr.20221004064036.84"><vh>ExpressionVisitor.visit_unary_expr</vh></v>
<v t="ekr.20221004064036.85"><vh>ExpressionVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20221004064036.86"><vh>ExpressionVisitor.visit_list_expr</vh></v>
<v t="ekr.20221004064036.87"><vh>ExpressionVisitor.visit_dict_expr</vh></v>
<v t="ekr.20221004064036.88"><vh>ExpressionVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20221004064036.89"><vh>ExpressionVisitor.visit_set_expr</vh></v>
<v t="ekr.20221004064036.90"><vh>ExpressionVisitor.visit_index_expr</vh></v>
<v t="ekr.20221004064036.91"><vh>ExpressionVisitor.visit_type_application</vh></v>
<v t="ekr.20221004064036.92"><vh>ExpressionVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20221004064036.93"><vh>ExpressionVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20221004064036.94"><vh>ExpressionVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20221004064036.95"><vh>ExpressionVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064036.96"><vh>ExpressionVisitor.visit_generator_expr</vh></v>
<v t="ekr.20221004064036.97"><vh>ExpressionVisitor.visit_slice_expr</vh></v>
<v t="ekr.20221004064036.98"><vh>ExpressionVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20221004064036.99"><vh>ExpressionVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20221004064036.100"><vh>ExpressionVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20221004064036.101"><vh>ExpressionVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064036.102"><vh>ExpressionVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20221004064036.103"><vh>ExpressionVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064036.104"><vh>ExpressionVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20221004064036.105"><vh>ExpressionVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20221004064036.106"><vh>ExpressionVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20221004064036.107"><vh>ExpressionVisitor.visit__promote_expr</vh></v>
<v t="ekr.20221004064036.108"><vh>ExpressionVisitor.visit_await_expr</vh></v>
<v t="ekr.20221004064036.109"><vh>ExpressionVisitor.visit_temp_node</vh></v>
</v>
<v t="ekr.20221004064036.110"><vh>class StatementVisitor</vh>
<v t="ekr.20221004064036.111"><vh>StatementVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064036.112"><vh>StatementVisitor.visit_for_stmt</vh></v>
<v t="ekr.20221004064036.113"><vh>StatementVisitor.visit_with_stmt</vh></v>
<v t="ekr.20221004064036.114"><vh>StatementVisitor.visit_del_stmt</vh></v>
<v t="ekr.20221004064036.115"><vh>StatementVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064036.116"><vh>StatementVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064036.117"><vh>StatementVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064036.118"><vh>StatementVisitor.visit_global_decl</vh></v>
<v t="ekr.20221004064036.119"><vh>StatementVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20221004064036.120"><vh>StatementVisitor.visit_decorator</vh></v>
<v t="ekr.20221004064036.121"><vh>StatementVisitor.Module structure</vh></v>
<v t="ekr.20221004064036.122"><vh>StatementVisitor.visit_import</vh></v>
<v t="ekr.20221004064036.123"><vh>StatementVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064036.124"><vh>StatementVisitor.visit_import_all</vh></v>
<v t="ekr.20221004064036.125"><vh>StatementVisitor.Statements</vh></v>
<v t="ekr.20221004064036.126"><vh>StatementVisitor.visit_block</vh></v>
<v t="ekr.20221004064036.127"><vh>StatementVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20221004064036.128"><vh>StatementVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064036.129"><vh>StatementVisitor.visit_while_stmt</vh></v>
<v t="ekr.20221004064036.130"><vh>StatementVisitor.visit_return_stmt</vh></v>
<v t="ekr.20221004064036.131"><vh>StatementVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20221004064036.132"><vh>StatementVisitor.visit_if_stmt</vh></v>
<v t="ekr.20221004064036.133"><vh>StatementVisitor.visit_break_stmt</vh></v>
<v t="ekr.20221004064036.134"><vh>StatementVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20221004064036.135"><vh>StatementVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20221004064036.136"><vh>StatementVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20221004064036.137"><vh>StatementVisitor.visit_try_stmt</vh></v>
<v t="ekr.20221004064036.138"><vh>StatementVisitor.visit_match_stmt</vh></v>
</v>
<v t="ekr.20221004064036.139"><vh>class PatternVisitor</vh>
<v t="ekr.20221004064036.140"><vh>PatternVisitor.visit_as_pattern</vh></v>
<v t="ekr.20221004064036.141"><vh>PatternVisitor.visit_or_pattern</vh></v>
<v t="ekr.20221004064036.142"><vh>PatternVisitor.visit_value_pattern</vh></v>
<v t="ekr.20221004064036.143"><vh>PatternVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20221004064036.144"><vh>PatternVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20221004064036.145"><vh>PatternVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20221004064036.146"><vh>PatternVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20221004064036.147"><vh>PatternVisitor.visit_class_pattern</vh></v>
</v>
<v t="ekr.20221004064036.148"><vh>class NodeVisitor</vh>
<v t="ekr.20221004064036.149"><vh>NodeVisitor.visit_mypy_file</vh></v>
<v t="ekr.20221004064036.150"><vh>NodeVisitor.visit_var</vh></v>
<v t="ekr.20221004064036.151"><vh>NodeVisitor.Module structure</vh></v>
<v t="ekr.20221004064036.152"><vh>NodeVisitor.visit_import</vh></v>
<v t="ekr.20221004064036.153"><vh>NodeVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064036.154"><vh>NodeVisitor.visit_import_all</vh></v>
<v t="ekr.20221004064036.155"><vh>NodeVisitor.Definitions</vh></v>
<v t="ekr.20221004064036.156"><vh>NodeVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064036.157"><vh>NodeVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064036.158"><vh>NodeVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064036.159"><vh>NodeVisitor.visit_global_decl</vh></v>
<v t="ekr.20221004064036.160"><vh>NodeVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20221004064036.161"><vh>NodeVisitor.visit_decorator</vh></v>
<v t="ekr.20221004064036.162"><vh>NodeVisitor.visit_type_alias</vh></v>
<v t="ekr.20221004064036.163"><vh>NodeVisitor.visit_placeholder_node</vh></v>
<v t="ekr.20221004064036.164"><vh>NodeVisitor.Statements</vh></v>
<v t="ekr.20221004064036.165"><vh>NodeVisitor.visit_block</vh></v>
<v t="ekr.20221004064036.166"><vh>NodeVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20221004064036.167"><vh>NodeVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064036.168"><vh>NodeVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064036.169"><vh>NodeVisitor.visit_while_stmt</vh></v>
<v t="ekr.20221004064036.170"><vh>NodeVisitor.visit_for_stmt</vh></v>
<v t="ekr.20221004064036.171"><vh>NodeVisitor.visit_return_stmt</vh></v>
<v t="ekr.20221004064036.172"><vh>NodeVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20221004064036.173"><vh>NodeVisitor.visit_del_stmt</vh></v>
<v t="ekr.20221004064036.174"><vh>NodeVisitor.visit_if_stmt</vh></v>
<v t="ekr.20221004064036.175"><vh>NodeVisitor.visit_break_stmt</vh></v>
<v t="ekr.20221004064036.176"><vh>NodeVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20221004064036.177"><vh>NodeVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20221004064036.178"><vh>NodeVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20221004064036.179"><vh>NodeVisitor.visit_try_stmt</vh></v>
<v t="ekr.20221004064036.180"><vh>NodeVisitor.visit_with_stmt</vh></v>
<v t="ekr.20221004064036.181"><vh>NodeVisitor.visit_match_stmt</vh></v>
<v t="ekr.20221004064036.182"><vh>NodeVisitor.Expressions (default no-op implementation)</vh></v>
<v t="ekr.20221004064036.183"><vh>NodeVisitor.visit_int_expr</vh></v>
<v t="ekr.20221004064036.184"><vh>NodeVisitor.visit_str_expr</vh></v>
<v t="ekr.20221004064036.185"><vh>NodeVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20221004064036.186"><vh>NodeVisitor.visit_float_expr</vh></v>
<v t="ekr.20221004064036.187"><vh>NodeVisitor.visit_complex_expr</vh></v>
<v t="ekr.20221004064036.188"><vh>NodeVisitor.visit_ellipsis</vh></v>
<v t="ekr.20221004064036.189"><vh>NodeVisitor.visit_star_expr</vh></v>
<v t="ekr.20221004064036.190"><vh>NodeVisitor.visit_name_expr</vh></v>
<v t="ekr.20221004064036.191"><vh>NodeVisitor.visit_member_expr</vh></v>
<v t="ekr.20221004064036.192"><vh>NodeVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064036.193"><vh>NodeVisitor.visit_yield_expr</vh></v>
<v t="ekr.20221004064036.194"><vh>NodeVisitor.visit_call_expr</vh></v>
<v t="ekr.20221004064036.195"><vh>NodeVisitor.visit_op_expr</vh></v>
<v t="ekr.20221004064036.196"><vh>NodeVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20221004064036.197"><vh>NodeVisitor.visit_cast_expr</vh></v>
<v t="ekr.20221004064036.198"><vh>NodeVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064036.199"><vh>NodeVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20221004064036.200"><vh>NodeVisitor.visit_super_expr</vh></v>
<v t="ekr.20221004064036.201"><vh>NodeVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20221004064036.202"><vh>NodeVisitor.visit_unary_expr</vh></v>
<v t="ekr.20221004064036.203"><vh>NodeVisitor.visit_list_expr</vh></v>
<v t="ekr.20221004064036.204"><vh>NodeVisitor.visit_dict_expr</vh></v>
<v t="ekr.20221004064036.205"><vh>NodeVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20221004064036.206"><vh>NodeVisitor.visit_set_expr</vh></v>
<v t="ekr.20221004064036.207"><vh>NodeVisitor.visit_index_expr</vh></v>
<v t="ekr.20221004064036.208"><vh>NodeVisitor.visit_type_application</vh></v>
<v t="ekr.20221004064036.209"><vh>NodeVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20221004064036.210"><vh>NodeVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20221004064036.211"><vh>NodeVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20221004064036.212"><vh>NodeVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064036.213"><vh>NodeVisitor.visit_generator_expr</vh></v>
<v t="ekr.20221004064036.214"><vh>NodeVisitor.visit_slice_expr</vh></v>
<v t="ekr.20221004064036.215"><vh>NodeVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20221004064036.216"><vh>NodeVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20221004064036.217"><vh>NodeVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20221004064036.218"><vh>NodeVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064036.219"><vh>NodeVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20221004064036.220"><vh>NodeVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064036.221"><vh>NodeVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20221004064036.222"><vh>NodeVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20221004064036.223"><vh>NodeVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20221004064036.224"><vh>NodeVisitor.visit__promote_expr</vh></v>
<v t="ekr.20221004064036.225"><vh>NodeVisitor.visit_await_expr</vh></v>
<v t="ekr.20221004064036.226"><vh>NodeVisitor.visit_temp_node</vh></v>
<v t="ekr.20221004064036.227"><vh>NodeVisitor.Patterns</vh></v>
<v t="ekr.20221004064036.228"><vh>NodeVisitor.visit_as_pattern</vh></v>
<v t="ekr.20221004064036.229"><vh>NodeVisitor.visit_or_pattern</vh></v>
<v t="ekr.20221004064036.230"><vh>NodeVisitor.visit_value_pattern</vh></v>
<v t="ekr.20221004064036.231"><vh>NodeVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20221004064036.232"><vh>NodeVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20221004064036.233"><vh>NodeVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20221004064036.234"><vh>NodeVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20221004064036.235"><vh>NodeVisitor.visit_class_pattern</vh></v>
</v>
</v>
</v>
<v t="ekr.20221005081130.1"><vh>--- checkers</vh>
<v t="ekr.20221004064034.324"><vh>@clean checker.py</vh>
<v t="ekr.20221004064034.325"><vh>class DeferredNode</vh></v>
<v t="ekr.20221004064034.326"><vh>class FineGrainedDeferredNode</vh></v>
<v t="ekr.20221004064034.327"><vh>Data structure returned by find_isinstance_check representing</vh></v>
<v t="ekr.20221004064034.328"><vh>class TypeRange</vh></v>
<v t="ekr.20221004064034.329"><vh>class PartialTypeScope</vh></v>
<v t="ekr.20221004064034.330"><vh>class TypeChecker</vh>
<v t="ekr.20221004064034.331"><vh>TypeChecker.__init__</vh></v>
<v t="ekr.20221004064034.332"><vh>TypeChecker.type_context</vh></v>
<v t="ekr.20221004064034.333"><vh>TypeChecker.reset</vh></v>
<v t="ekr.20221004064034.334"><vh>TypeChecker.check_first_pass</vh></v>
<v t="ekr.20221004064034.335"><vh>TypeChecker.check_second_pass</vh></v>
<v t="ekr.20221004064034.336"><vh>TypeChecker.check_partial</vh></v>
<v t="ekr.20221004064034.337"><vh>TypeChecker.check_top_level</vh></v>
<v t="ekr.20221004064034.338"><vh>TypeChecker.defer_node</vh></v>
<v t="ekr.20221004064034.339"><vh>TypeChecker.handle_cannot_determine_type</vh></v>
<v t="ekr.20221004064034.340"><vh>TypeChecker.accept</vh></v>
<v t="ekr.20221004064034.341"><vh>TypeChecker.accept_loop</vh></v>
<v t="ekr.20221004064034.342"><vh>TypeChecker.Definitions</vh></v>
<v t="ekr.20221004064034.343"><vh>TypeChecker.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064034.344"><vh>TypeChecker._visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064034.345"><vh>TypeChecker.check_overlapping_overloads</vh></v>
<v t="ekr.20221004064034.346"><vh>TypeChecker.Here's the scoop about generators and coroutines.</vh></v>
<v t="ekr.20221004064034.347"><vh>TypeChecker.is_generator_return_type</vh></v>
<v t="ekr.20221004064034.348"><vh>TypeChecker.is_async_generator_return_type</vh></v>
<v t="ekr.20221004064034.349"><vh>TypeChecker.get_generator_yield_type</vh></v>
<v t="ekr.20221004064034.350"><vh>TypeChecker.get_generator_receive_type</vh></v>
<v t="ekr.20221004064034.351"><vh>TypeChecker.get_coroutine_return_type</vh></v>
<v t="ekr.20221004064034.352"><vh>TypeChecker.get_generator_return_type</vh></v>
<v t="ekr.20221004064034.353"><vh>TypeChecker.visit_func_def</vh></v>
<v t="ekr.20221004064034.354"><vh>TypeChecker._visit_func_def</vh></v>
<v t="ekr.20221004064034.355"><vh>TypeChecker.check_func_item</vh></v>
<v t="ekr.20221004064034.356"><vh>TypeChecker.enter_attribute_inference_context</vh></v>
<v t="ekr.20221004064034.357"><vh>TypeChecker.check_func_def</vh></v>
<v t="ekr.20221004064034.358"><vh>TypeChecker.check_unbound_return_typevar</vh></v>
<v t="ekr.20221004064034.359"><vh>TypeChecker.check_default_args</vh></v>
<v t="ekr.20221004064034.360"><vh>TypeChecker.is_forward_op_method</vh></v>
<v t="ekr.20221004064034.361"><vh>TypeChecker.is_reverse_op_method</vh></v>
<v t="ekr.20221004064034.362"><vh>TypeChecker.check_for_missing_annotations</vh></v>
<v t="ekr.20221004064034.363"><vh>TypeChecker.check___new___signature</vh></v>
<v t="ekr.20221004064034.364"><vh>TypeChecker.check_reverse_op_method</vh></v>
<v t="ekr.20221004064034.365"><vh>TypeChecker.check_overlapping_op_methods</vh></v>
<v t="ekr.20221004064034.366"><vh>TypeChecker.is_unsafe_overlapping_op</vh></v>
<v t="ekr.20221004064034.367"><vh>TypeChecker.check_inplace_operator_method</vh></v>
<v t="ekr.20221004064034.368"><vh>TypeChecker.check_getattr_method</vh></v>
<v t="ekr.20221004064034.369"><vh>TypeChecker.check_setattr_method</vh></v>
<v t="ekr.20221004064034.370"><vh>TypeChecker.check_slots_definition</vh></v>
<v t="ekr.20221004064034.371"><vh>TypeChecker.check_match_args</vh></v>
<v t="ekr.20221004064034.372"><vh>TypeChecker.expand_typevars</vh></v>
<v t="ekr.20221004064034.373"><vh>TypeChecker.check_method_override</vh></v>
<v t="ekr.20221004064034.374"><vh>TypeChecker.check_method_or_accessor_override_for_base</vh></v>
<v t="ekr.20221004064034.375"><vh>TypeChecker.check_method_override_for_base_with_name</vh></v>
<v t="ekr.20221004064034.376"><vh>TypeChecker.bind_and_map_method</vh></v>
<v t="ekr.20221004064034.377"><vh>TypeChecker.get_op_other_domain</vh></v>
<v t="ekr.20221004064034.378"><vh>TypeChecker.check_override</vh></v>
<v t="ekr.20221004064034.379"><vh>TypeChecker.check__exit__return_type</vh></v>
<v t="ekr.20221004064034.380"><vh>TypeChecker.visit_class_def</vh></v>
<v t="ekr.20221004064034.381"><vh>TypeChecker.check_final_deletable</vh></v>
<v t="ekr.20221004064034.382"><vh>TypeChecker.check_init_subclass</vh></v>
<v t="ekr.20221004064034.383"><vh>TypeChecker.check_enum</vh></v>
<v t="ekr.20221004064034.384"><vh>TypeChecker.check_final_enum</vh></v>
<v t="ekr.20221004064034.385"><vh>TypeChecker.is_final_enum_value</vh></v>
<v t="ekr.20221004064034.386"><vh>TypeChecker.check_enum_bases</vh></v>
<v t="ekr.20221004064034.387"><vh>TypeChecker.check_enum_new</vh></v>
<v t="ekr.20221004064034.388"><vh>TypeChecker.check_protocol_variance</vh></v>
<v t="ekr.20221004064034.389"><vh>TypeChecker.check_multiple_inheritance</vh></v>
<v t="ekr.20221004064034.390"><vh>TypeChecker.determine_type_of_member</vh></v>
<v t="ekr.20221004064034.391"><vh>TypeChecker.check_compatibility</vh></v>
<v t="ekr.20221004064034.392"><vh>TypeChecker.check_metaclass_compatibility</vh></v>
<v t="ekr.20221004064034.393"><vh>TypeChecker.visit_import_from</vh></v>
<v t="ekr.20221004064034.394"><vh>TypeChecker.visit_import_all</vh></v>
<v t="ekr.20221004064034.395"><vh>TypeChecker.visit_import</vh></v>
<v t="ekr.20221004064034.396"><vh>TypeChecker.check_import</vh></v>
<v t="ekr.20221004064034.397"><vh>TypeChecker.Statements</vh></v>
<v t="ekr.20221004064034.398"><vh>TypeChecker.visit_block</vh></v>
<v t="ekr.20221004064034.399"><vh>TypeChecker.should_report_unreachable_issues</vh></v>
<v t="ekr.20221004064034.400"><vh>TypeChecker.is_raising_or_empty</vh></v>
<v t="ekr.20221004064034.401"><vh>TypeChecker.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064034.402"><vh>TypeChecker.check_type_alias_rvalue</vh>
<v t="ekr.20221004064034.403"><vh>TypeChecker.accept_items</vh></v>
</v>
<v t="ekr.20221004064034.404"><vh>TypeChecker.check_assignment</vh></v>
<v t="ekr.20221004064034.405"><vh>TypeChecker.(type, operator) tuples for augmented assignments supported with partial types</vh></v>
<v t="ekr.20221004064034.406"><vh>TypeChecker.get_variable_type_context</vh></v>
<v t="ekr.20221004064034.407"><vh>TypeChecker.try_infer_partial_generic_type_from_assignment</vh></v>
<v t="ekr.20221004064034.408"><vh>TypeChecker.check_compatibility_all_supers</vh></v>
<v t="ekr.20221004064034.409"><vh>TypeChecker.check_compatibility_super</vh></v>
<v t="ekr.20221004064034.410"><vh>TypeChecker.lvalue_type_from_base</vh></v>
<v t="ekr.20221004064034.411"><vh>TypeChecker.check_compatibility_classvar_super</vh></v>
<v t="ekr.20221004064034.412"><vh>TypeChecker.check_compatibility_final_super</vh></v>
<v t="ekr.20221004064034.413"><vh>TypeChecker.check_if_final_var_override_writable</vh></v>
<v t="ekr.20221004064034.414"><vh>TypeChecker.get_final_context</vh></v>
<v t="ekr.20221004064034.415"><vh>TypeChecker.enter_final_context</vh></v>
<v t="ekr.20221004064034.416"><vh>TypeChecker.check_final</vh></v>
<v t="ekr.20221004064034.417"><vh>TypeChecker.check_assignment_to_slots</vh></v>
<v t="ekr.20221004064034.418"><vh>TypeChecker.is_assignable_slot</vh></v>
<v t="ekr.20221004064034.419"><vh>TypeChecker.check_assignment_to_multiple_lvalues</vh></v>
<v t="ekr.20221004064034.420"><vh>TypeChecker.check_rvalue_count_in_assignment</vh></v>
<v t="ekr.20221004064034.421"><vh>TypeChecker.check_multi_assignment</vh></v>
<v t="ekr.20221004064034.422"><vh>TypeChecker.check_multi_assignment_from_union</vh></v>
<v t="ekr.20221004064034.423"><vh>TypeChecker.flatten_lvalues</vh></v>
<v t="ekr.20221004064034.424"><vh>TypeChecker.check_multi_assignment_from_tuple</vh></v>
<v t="ekr.20221004064034.425"><vh>TypeChecker.lvalue_type_for_inference</vh></v>
<v t="ekr.20221004064034.426"><vh>TypeChecker.split_around_star</vh></v>
<v t="ekr.20221004064034.427"><vh>TypeChecker.type_is_iterable</vh></v>
<v t="ekr.20221004064034.428"><vh>TypeChecker.check_multi_assignment_from_iterable</vh></v>
<v t="ekr.20221004064034.429"><vh>TypeChecker.check_lvalue</vh></v>
<v t="ekr.20221004064034.430"><vh>TypeChecker.is_definition</vh></v>
<v t="ekr.20221004064034.431"><vh>TypeChecker.infer_variable_type</vh></v>
<v t="ekr.20221004064034.432"><vh>TypeChecker.infer_partial_type</vh></v>
<v t="ekr.20221004064034.433"><vh>TypeChecker.is_valid_defaultdict_partial_value_type</vh></v>
<v t="ekr.20221004064034.434"><vh>TypeChecker.set_inferred_type</vh></v>
<v t="ekr.20221004064034.435"><vh>TypeChecker.set_inference_error_fallback_type</vh></v>
<v t="ekr.20221004064034.436"><vh>TypeChecker.inference_error_fallback_type</vh></v>
<v t="ekr.20221004064034.437"><vh>TypeChecker.check_simple_assignment</vh></v>
<v t="ekr.20221004064034.438"><vh>TypeChecker.check_member_assignment</vh></v>
<v t="ekr.20221004064034.439"><vh>TypeChecker.check_indexed_assignment</vh></v>
<v t="ekr.20221004064034.440"><vh>TypeChecker.try_infer_partial_type_from_indexed_assignment</vh></v>
<v t="ekr.20221004064034.441"><vh>TypeChecker.type_requires_usage</vh></v>
<v t="ekr.20221004064034.442"><vh>TypeChecker.visit_expression_stmt</vh></v>
<v t="ekr.20221004064034.443"><vh>TypeChecker.visit_return_stmt</vh></v>
<v t="ekr.20221004064034.444"><vh>TypeChecker.check_return_stmt</vh></v>
<v t="ekr.20221004064034.445"><vh>TypeChecker.visit_if_stmt</vh></v>
<v t="ekr.20221004064034.446"><vh>TypeChecker.visit_while_stmt</vh></v>
<v t="ekr.20221004064034.447"><vh>TypeChecker.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064034.448"><vh>TypeChecker.visit_assert_stmt</vh></v>
<v t="ekr.20221004064034.449"><vh>TypeChecker.visit_raise_stmt</vh></v>
<v t="ekr.20221004064034.450"><vh>TypeChecker.type_check_raise</vh></v>
<v t="ekr.20221004064034.451"><vh>TypeChecker.visit_try_stmt</vh></v>
<v t="ekr.20221004064034.452"><vh>TypeChecker.visit_try_without_finally</vh></v>
<v t="ekr.20221004064034.453"><vh>TypeChecker.check_except_handler_test</vh></v>
<v t="ekr.20221004064034.454"><vh>TypeChecker.get_types_from_except_handler</vh></v>
<v t="ekr.20221004064034.455"><vh>TypeChecker.visit_for_stmt</vh></v>
<v t="ekr.20221004064034.456"><vh>TypeChecker.analyze_async_iterable_item_type</vh></v>
<v t="ekr.20221004064034.457"><vh>TypeChecker.analyze_iterable_item_type</vh></v>
<v t="ekr.20221004064034.458"><vh>TypeChecker.analyze_range_native_int_type</vh></v>
<v t="ekr.20221004064034.459"><vh>TypeChecker.analyze_container_item_type</vh></v>
<v t="ekr.20221004064034.460"><vh>TypeChecker.analyze_index_variables</vh></v>
<v t="ekr.20221004064034.461"><vh>TypeChecker.visit_del_stmt</vh></v>
<v t="ekr.20221004064034.462"><vh>TypeChecker.visit_decorator</vh></v>
<v t="ekr.20221004064034.463"><vh>TypeChecker.check_for_untyped_decorator</vh></v>
<v t="ekr.20221004064034.464"><vh>TypeChecker.check_incompatible_property_override</vh></v>
<v t="ekr.20221004064034.465"><vh>TypeChecker.visit_with_stmt</vh></v>
<v t="ekr.20221004064034.466"><vh>TypeChecker.check_untyped_after_decorator</vh></v>
<v t="ekr.20221004064034.467"><vh>TypeChecker.check_async_with_item</vh></v>
<v t="ekr.20221004064034.468"><vh>TypeChecker.check_with_item</vh></v>
<v t="ekr.20221004064034.469"><vh>TypeChecker.visit_break_stmt</vh></v>
<v t="ekr.20221004064034.470"><vh>TypeChecker.visit_continue_stmt</vh></v>
<v t="ekr.20221004064034.471"><vh>TypeChecker.visit_match_stmt</vh></v>
<v t="ekr.20221004064034.472"><vh>TypeChecker.infer_variable_types_from_type_maps</vh></v>
<v t="ekr.20221004064034.473"><vh>TypeChecker.remove_capture_conflicts</vh></v>
<v t="ekr.20221004064034.474"><vh>TypeChecker.make_fake_typeinfo</vh></v>
<v t="ekr.20221004064034.475"><vh>TypeChecker.intersect_instances</vh></v>
<v t="ekr.20221004064034.476"><vh>TypeChecker.intersect_instance_callable</vh></v>
<v t="ekr.20221004064034.477"><vh>TypeChecker.make_fake_callable</vh></v>
<v t="ekr.20221004064034.478"><vh>TypeChecker.partition_by_callable</vh></v>
<v t="ekr.20221004064034.479"><vh>TypeChecker.conditional_callable_type_map</vh></v>
<v t="ekr.20221004064034.480"><vh>TypeChecker._is_truthy_type</vh></v>
<v t="ekr.20221004064034.481"><vh>TypeChecker._check_for_truthy_type</vh></v>
<v t="ekr.20221004064034.482"><vh>TypeChecker.find_type_equals_check</vh></v>
<v t="ekr.20221004064034.483"><vh>TypeChecker.find_isinstance_check</vh></v>
<v t="ekr.20221004064034.484"><vh>TypeChecker.find_isinstance_check_helper</vh></v>
<v t="ekr.20221004064034.485"><vh>TypeChecker.propagate_up_typemap_info</vh></v>
<v t="ekr.20221004064034.486"><vh>TypeChecker.refine_parent_types</vh></v>
<v t="ekr.20221004064034.487"><vh>TypeChecker.refine_identity_comparison_expression</vh></v>
<v t="ekr.20221004064034.488"><vh>TypeChecker.refine_away_none_in_comparison</vh></v>
<v t="ekr.20221004064034.489"><vh>TypeChecker.check_subtype</vh></v>
<v t="ekr.20221004064034.490"><vh>TypeChecker.check_subtype</vh></v>
<v t="ekr.20221004064034.491"><vh>TypeChecker.check_subtype</vh></v>
<v t="ekr.20221004064034.492"><vh>TypeChecker.get_precise_awaitable_type</vh></v>
<v t="ekr.20221004064034.493"><vh>TypeChecker.checking_await_set</vh></v>
<v t="ekr.20221004064034.494"><vh>TypeChecker.check_possible_missing_await</vh></v>
<v t="ekr.20221004064034.495"><vh>TypeChecker.contains_none</vh></v>
<v t="ekr.20221004064034.496"><vh>TypeChecker.named_type</vh></v>
<v t="ekr.20221004064034.497"><vh>TypeChecker.named_generic_type</vh></v>
<v t="ekr.20221004064034.498"><vh>TypeChecker.lookup_typeinfo</vh></v>
<v t="ekr.20221004064034.499"><vh>TypeChecker.type_type</vh></v>
<v t="ekr.20221004064034.500"><vh>TypeChecker.str_type</vh></v>
<v t="ekr.20221004064034.501"><vh>TypeChecker.store_type</vh></v>
<v t="ekr.20221004064034.502"><vh>TypeChecker.has_type</vh></v>
<v t="ekr.20221004064034.503"><vh>TypeChecker.lookup_type_or_none</vh></v>
<v t="ekr.20221004064034.504"><vh>TypeChecker.lookup_type</vh></v>
<v t="ekr.20221004064034.505"><vh>TypeChecker.store_types</vh></v>
<v t="ekr.20221004064034.506"><vh>TypeChecker.local_type_map</vh></v>
<v t="ekr.20221004064034.507"><vh>TypeChecker.in_checked_function</vh></v>
<v t="ekr.20221004064034.508"><vh>TypeChecker.lookup</vh></v>
<v t="ekr.20221004064034.509"><vh>TypeChecker.lookup_qualified</vh></v>
<v t="ekr.20221004064034.510"><vh>TypeChecker.enter_partial_types</vh></v>
<v t="ekr.20221004064034.511"><vh>TypeChecker.handle_partial_var_type</vh></v>
<v t="ekr.20221004064034.512"><vh>TypeChecker.fixup_partial_type</vh></v>
<v t="ekr.20221004064034.513"><vh>TypeChecker.is_defined_in_base_class</vh></v>
<v t="ekr.20221004064034.514"><vh>TypeChecker.find_partial_types</vh></v>
<v t="ekr.20221004064034.515"><vh>TypeChecker.find_partial_types_in_all_scopes</vh></v>
<v t="ekr.20221004064034.516"><vh>TypeChecker.temp_node</vh></v>
<v t="ekr.20221004064034.517"><vh>TypeChecker.fail</vh></v>
<v t="ekr.20221004064034.518"><vh>TypeChecker.note</vh></v>
<v t="ekr.20221004064034.519"><vh>TypeChecker.iterable_item_type</vh></v>
<v t="ekr.20221004064034.520"><vh>TypeChecker.function_type</vh></v>
<v t="ekr.20221004064034.521"><vh>TypeChecker.push_type_map</vh></v>
<v t="ekr.20221004064034.522"><vh>TypeChecker.infer_issubclass_maps</vh></v>
<v t="ekr.20221004064034.523"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20221004064034.524"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20221004064034.525"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20221004064034.526"><vh>TypeChecker.is_writable_attribute</vh></v>
<v t="ekr.20221004064034.527"><vh>TypeChecker.get_isinstance_type</vh></v>
<v t="ekr.20221004064034.528"><vh>TypeChecker.is_literal_enum</vh></v>
<v t="ekr.20221004064034.529"><vh>TypeChecker.add_any_attribute_to_type</vh></v>
<v t="ekr.20221004064034.530"><vh>TypeChecker.hasattr_type_maps</vh></v>
<v t="ekr.20221004064034.531"><vh>TypeChecker.partition_union_by_attr</vh></v>
<v t="ekr.20221004064034.532"><vh>TypeChecker.has_valid_attribute</vh></v>
</v>
<v t="ekr.20221004064034.533"><vh>class CollectArgTypeVarTypes</vh></v>
<v t="ekr.20221004064034.537"><vh>conditional_types</vh></v>
<v t="ekr.20221004064034.538"><vh>conditional_types</vh></v>
<v t="ekr.20221004064034.539"><vh>conditional_types</vh></v>
<v t="ekr.20221004064034.540"><vh>conditional_types_to_typemaps</vh></v>
<v t="ekr.20221004064034.541"><vh>gen_unique_name</vh></v>
<v t="ekr.20221004064034.542"><vh>is_true_literal</vh></v>
<v t="ekr.20221004064034.543"><vh>is_false_literal</vh></v>
<v t="ekr.20221004064034.544"><vh>is_literal_none</vh></v>
<v t="ekr.20221004064034.545"><vh>is_literal_not_implemented</vh></v>
<v t="ekr.20221004064034.546"><vh>builtin_item_type</vh></v>
<v t="ekr.20221004064034.547"><vh>and_conditional_maps</vh></v>
<v t="ekr.20221004064034.548"><vh>or_conditional_maps</vh></v>
<v t="ekr.20221004064034.549"><vh>reduce_conditional_maps</vh></v>
<v t="ekr.20221004064034.550"><vh>convert_to_typetype</vh></v>
<v t="ekr.20221004064034.551"><vh>flatten</vh></v>
<v t="ekr.20221004064034.552"><vh>flatten_types</vh></v>
<v t="ekr.20221004064034.553"><vh>expand_func</vh></v>
<v t="ekr.20221004064034.554"><vh>class TypeTransformVisitor</vh></v>
<v t="ekr.20221004064034.555"><vh>are_argument_counts_overlapping</vh></v>
<v t="ekr.20221004064034.556"><vh>is_unsafe_overlapping_overload_signatures</vh></v>
<v t="ekr.20221004064034.557"><vh>detach_callable</vh></v>
<v t="ekr.20221004064034.558"><vh>overload_can_never_match</vh></v>
<v t="ekr.20221004064034.559"><vh>is_more_general_arg_prefix</vh></v>
<v t="ekr.20221004064034.560"><vh>is_same_arg_prefix</vh></v>
<v t="ekr.20221004064034.561"><vh>infer_operator_assignment_method</vh></v>
<v t="ekr.20221004064034.562"><vh>is_valid_inferred_type</vh></v>
<v t="ekr.20221004064034.563"><vh>class NothingSeeker</vh>
<v t="ekr.20221004064034.564"><vh>NothingSeeker.__init__</vh></v>
<v t="ekr.20221004064034.565"><vh>NothingSeeker.visit_uninhabited_type</vh></v>
</v>
<v t="ekr.20221004064034.566"><vh>class SetNothingToAny</vh>
<v t="ekr.20221004064034.567"><vh>SetNothingToAny.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064034.568"><vh>SetNothingToAny.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064034.569"><vh>is_node_static</vh></v>
<v t="ekr.20221004064034.570"><vh>class CheckerScope</vh>
<v t="ekr.20221004064034.571"><vh>CheckerScope.__init__</vh></v>
<v t="ekr.20221004064034.572"><vh>CheckerScope.top_function</vh></v>
<v t="ekr.20221004064034.573"><vh>CheckerScope.top_non_lambda_function</vh></v>
<v t="ekr.20221004064034.574"><vh>CheckerScope.active_class</vh></v>
<v t="ekr.20221004064034.575"><vh>CheckerScope.enclosing_class</vh></v>
<v t="ekr.20221004064034.576"><vh>CheckerScope.active_self_type</vh></v>
<v t="ekr.20221004064034.577"><vh>CheckerScope.push_function</vh></v>
<v t="ekr.20221004064034.578"><vh>CheckerScope.push_class</vh></v>
</v>
<v t="ekr.20221004064034.579"><vh>TKey = TypeVar("TKey")</vh></v>
<v t="ekr.20221004064034.580"><vh>class DisjointDict</vh>
<v t="ekr.20221004064034.581"><vh>DisjointDict.__init__</vh></v>
<v t="ekr.20221004064034.582"><vh>DisjointDict.add_mapping</vh></v>
<v t="ekr.20221004064034.583"><vh>DisjointDict.items</vh></v>
<v t="ekr.20221004064034.584"><vh>DisjointDict._lookup_or_make_root_id</vh></v>
<v t="ekr.20221004064034.585"><vh>DisjointDict._lookup_root_id</vh></v>
</v>
<v t="ekr.20221004064034.586"><vh>group_comparison_operands</vh></v>
<v t="ekr.20221004064034.587"><vh>is_typed_callable</vh></v>
<v t="ekr.20221004064034.588"><vh>is_untyped_decorator</vh></v>
<v t="ekr.20221004064034.589"><vh>is_static</vh></v>
<v t="ekr.20221004064034.590"><vh>is_property</vh></v>
<v t="ekr.20221004064034.591"><vh>get_property_type</vh></v>
<v t="ekr.20221004064034.592"><vh>is_subtype_no_promote</vh></v>
<v t="ekr.20221004064034.593"><vh>is_overlapping_types_no_promote_no_uninhabited</vh></v>
<v t="ekr.20221004064034.594"><vh>is_private</vh></v>
<v t="ekr.20221004064034.595"><vh>is_string_literal</vh></v>
<v t="ekr.20221004064034.596"><vh>has_bool_item</vh></v>
<v t="ekr.20221004064034.597"><vh>collapse_walrus</vh></v>
</v>
<v t="ekr.20221004064034.598"><vh>@clean checkexpr.py</vh>
<v t="ekr.20221004064034.599"><vh>class TooManyUnions</vh></v>
<v t="ekr.20221004064034.600"><vh>allow_fast_container_literal</vh></v>
<v t="ekr.20221004064034.601"><vh>extract_refexpr_names</vh></v>
<v t="ekr.20221004064034.602"><vh>class Finished</vh></v>
<v t="ekr.20221004064034.603"><vh>class ExpressionChecker</vh>
<v t="ekr.20221004064034.604"><vh>ExpressionChecker.__init__</vh></v>
<v t="ekr.20221004064034.605"><vh>ExpressionChecker.reset</vh></v>
<v t="ekr.20221004064034.606"><vh>ExpressionChecker.visit_name_expr</vh></v>
<v t="ekr.20221004064034.607"><vh>ExpressionChecker.analyze_ref_expr</vh></v>
<v t="ekr.20221004064034.608"><vh>ExpressionChecker.analyze_var_ref</vh></v>
<v t="ekr.20221004064034.609"><vh>ExpressionChecker.module_type</vh></v>
<v t="ekr.20221004064034.610"><vh>ExpressionChecker.visit_call_expr</vh></v>
<v t="ekr.20221004064034.611"><vh>ExpressionChecker.refers_to_typeddict</vh></v>
<v t="ekr.20221004064034.612"><vh>ExpressionChecker.visit_call_expr_inner</vh></v>
<v t="ekr.20221004064034.613"><vh>ExpressionChecker.check_str_format_call</vh></v>
<v t="ekr.20221004064034.614"><vh>ExpressionChecker.method_fullname</vh></v>
<v t="ekr.20221004064034.615"><vh>ExpressionChecker.always_returns_none</vh></v>
<v t="ekr.20221004064034.616"><vh>ExpressionChecker.defn_returns_none</vh></v>
<v t="ekr.20221004064034.617"><vh>ExpressionChecker.check_runtime_protocol_test</vh></v>
<v t="ekr.20221004064034.618"><vh>ExpressionChecker.check_protocol_issubclass</vh></v>
<v t="ekr.20221004064034.619"><vh>ExpressionChecker.check_typeddict_call</vh></v>
<v t="ekr.20221004064034.620"><vh>ExpressionChecker.validate_typeddict_kwargs</vh></v>
<v t="ekr.20221004064034.621"><vh>ExpressionChecker.match_typeddict_call_with_dict</vh></v>
<v t="ekr.20221004064034.622"><vh>ExpressionChecker.check_typeddict_call_with_dict</vh></v>
<v t="ekr.20221004064034.623"><vh>ExpressionChecker.typeddict_callable</vh></v>
<v t="ekr.20221004064034.624"><vh>ExpressionChecker.typeddict_callable_from_context</vh></v>
<v t="ekr.20221004064034.625"><vh>ExpressionChecker.check_typeddict_call_with_kwargs</vh></v>
<v t="ekr.20221004064034.626"><vh>ExpressionChecker.get_partial_self_var</vh></v>
<v t="ekr.20221004064034.627"><vh>ExpressionChecker.Types and methods that can be used to infer partial types.</vh></v>
<v t="ekr.20221004064034.628"><vh>ExpressionChecker.try_infer_partial_type</vh></v>
<v t="ekr.20221004064034.629"><vh>ExpressionChecker.get_partial_var</vh></v>
<v t="ekr.20221004064034.630"><vh>ExpressionChecker.try_infer_partial_value_type_from_call</vh></v>
<v t="ekr.20221004064034.631"><vh>ExpressionChecker.apply_function_plugin</vh></v>
<v t="ekr.20221004064034.632"><vh>ExpressionChecker.apply_signature_hook</vh></v>
<v t="ekr.20221004064034.633"><vh>ExpressionChecker.apply_function_signature_hook</vh></v>
<v t="ekr.20221004064034.634"><vh>ExpressionChecker.apply_method_signature_hook</vh></v>
<v t="ekr.20221004064034.635"><vh>ExpressionChecker.transform_callee_type</vh></v>
<v t="ekr.20221004064034.636"><vh>ExpressionChecker.check_call_expr_with_callee_type</vh></v>
<v t="ekr.20221004064034.637"><vh>ExpressionChecker.check_union_call_expr</vh></v>
<v t="ekr.20221004064034.638"><vh>ExpressionChecker.check_call</vh></v>
<v t="ekr.20221004064034.639"><vh>ExpressionChecker.check_callable_call</vh></v>
<v t="ekr.20221004064034.640"><vh>ExpressionChecker.can_return_none</vh></v>
<v t="ekr.20221004064034.641"><vh>ExpressionChecker.analyze_type_type_callee</vh></v>
<v t="ekr.20221004064034.642"><vh>ExpressionChecker.infer_arg_types_in_empty_context</vh></v>
<v t="ekr.20221004064034.643"><vh>ExpressionChecker.allow_unions</vh></v>
<v t="ekr.20221004064034.644"><vh>ExpressionChecker.infer_arg_types_in_context</vh></v>
<v t="ekr.20221004064034.645"><vh>ExpressionChecker.infer_function_type_arguments_using_context</vh></v>
<v t="ekr.20221004064034.646"><vh>ExpressionChecker.infer_function_type_arguments</vh></v>
<v t="ekr.20221004064034.647"><vh>ExpressionChecker.infer_function_type_arguments_pass2</vh></v>
<v t="ekr.20221004064034.648"><vh>ExpressionChecker.argument_infer_context</vh></v>
<v t="ekr.20221004064034.649"><vh>ExpressionChecker.get_arg_infer_passes</vh></v>
<v t="ekr.20221004064034.650"><vh>ExpressionChecker.apply_inferred_arguments</vh></v>
<v t="ekr.20221004064034.651"><vh>ExpressionChecker.check_argument_count</vh></v>
<v t="ekr.20221004064034.652"><vh>ExpressionChecker.check_for_extra_actual_arguments</vh></v>
<v t="ekr.20221004064034.653"><vh>ExpressionChecker.missing_classvar_callable_note</vh></v>
<v t="ekr.20221004064034.654"><vh>ExpressionChecker.check_argument_types</vh></v>
<v t="ekr.20221004064034.655"><vh>ExpressionChecker.check_arg</vh></v>
<v t="ekr.20221004064034.656"><vh>ExpressionChecker.check_overload_call</vh></v>
<v t="ekr.20221004064034.657"><vh>ExpressionChecker.plausible_overload_call_targets</vh></v>
<v t="ekr.20221004064034.658"><vh>ExpressionChecker.infer_overload_return_type</vh></v>
<v t="ekr.20221004064034.659"><vh>ExpressionChecker.overload_erased_call_targets</vh></v>
<v t="ekr.20221004064034.660"><vh>ExpressionChecker.union_overload_result</vh></v>
<v t="ekr.20221004064034.661"><vh>ExpressionChecker.real_union</vh></v>
<v t="ekr.20221004064034.662"><vh>ExpressionChecker.type_overrides_set</vh></v>
<v t="ekr.20221004064034.663"><vh>ExpressionChecker.combine_function_signatures</vh></v>
<v t="ekr.20221004064034.664"><vh>ExpressionChecker.erased_signature_similarity</vh></v>
<v t="ekr.20221004064034.665"><vh>ExpressionChecker.apply_generic_arguments</vh></v>
<v t="ekr.20221004064034.666"><vh>ExpressionChecker.check_any_type_call</vh></v>
<v t="ekr.20221004064034.667"><vh>ExpressionChecker.check_union_call</vh></v>
<v t="ekr.20221004064034.668"><vh>ExpressionChecker.visit_member_expr</vh></v>
<v t="ekr.20221004064034.669"><vh>ExpressionChecker.analyze_ordinary_member_access</vh></v>
<v t="ekr.20221004064034.670"><vh>ExpressionChecker.analyze_external_member_access</vh></v>
<v t="ekr.20221004064034.671"><vh>ExpressionChecker.is_literal_context</vh></v>
<v t="ekr.20221004064034.672"><vh>ExpressionChecker.infer_literal_expr_type</vh></v>
<v t="ekr.20221004064034.673"><vh>ExpressionChecker.concat_tuples</vh></v>
<v t="ekr.20221004064034.674"><vh>ExpressionChecker.visit_int_expr</vh></v>
<v t="ekr.20221004064034.675"><vh>ExpressionChecker.visit_str_expr</vh></v>
<v t="ekr.20221004064034.676"><vh>ExpressionChecker.visit_bytes_expr</vh></v>
<v t="ekr.20221004064034.677"><vh>ExpressionChecker.visit_float_expr</vh></v>
<v t="ekr.20221004064034.678"><vh>ExpressionChecker.visit_complex_expr</vh></v>
<v t="ekr.20221004064034.679"><vh>ExpressionChecker.visit_ellipsis</vh></v>
<v t="ekr.20221004064034.680"><vh>ExpressionChecker.visit_op_expr</vh></v>
<v t="ekr.20221004064034.681"><vh>ExpressionChecker.visit_comparison_expr</vh></v>
<v t="ekr.20221004064034.682"><vh>ExpressionChecker.find_partial_type_ref_fast_path</vh></v>
<v t="ekr.20221004064034.683"><vh>ExpressionChecker.dangerous_comparison</vh></v>
<v t="ekr.20221004064034.684"><vh>ExpressionChecker.check_method_call_by_name</vh></v>
<v t="ekr.20221004064034.685"><vh>ExpressionChecker.check_union_method_call_by_name</vh></v>
<v t="ekr.20221004064034.686"><vh>ExpressionChecker.check_method_call</vh></v>
<v t="ekr.20221004064034.687"><vh>ExpressionChecker.check_op_reversible</vh></v>
<v t="ekr.20221004064034.688"><vh>ExpressionChecker.check_op</vh></v>
<v t="ekr.20221004064034.689"><vh>ExpressionChecker.check_boolean_op</vh></v>
<v t="ekr.20221004064034.690"><vh>ExpressionChecker.check_list_multiply</vh></v>
<v t="ekr.20221004064034.691"><vh>ExpressionChecker.visit_assignment_expr</vh></v>
<v t="ekr.20221004064034.692"><vh>ExpressionChecker.visit_unary_expr</vh></v>
<v t="ekr.20221004064034.693"><vh>ExpressionChecker.visit_index_expr</vh></v>
<v t="ekr.20221004064034.694"><vh>ExpressionChecker.visit_index_expr_helper</vh></v>
<v t="ekr.20221004064034.695"><vh>ExpressionChecker.visit_index_with_type</vh></v>
<v t="ekr.20221004064034.696"><vh>ExpressionChecker.visit_tuple_slice_helper</vh></v>
<v t="ekr.20221004064034.697"><vh>ExpressionChecker.try_getting_int_literals</vh></v>
<v t="ekr.20221004064034.698"><vh>ExpressionChecker.nonliteral_tuple_index_helper</vh></v>
<v t="ekr.20221004064034.699"><vh>ExpressionChecker.visit_typeddict_index_expr</vh></v>
<v t="ekr.20221004064034.700"><vh>ExpressionChecker.visit_enum_index_expr</vh></v>
<v t="ekr.20221004064034.701"><vh>ExpressionChecker.visit_cast_expr</vh></v>
<v t="ekr.20221004064034.702"><vh>ExpressionChecker.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064034.703"><vh>ExpressionChecker.visit_reveal_expr</vh></v>
<v t="ekr.20221004064034.704"><vh>ExpressionChecker.visit_type_application</vh></v>
<v t="ekr.20221004064034.705"><vh>ExpressionChecker.visit_type_alias_expr</vh></v>
<v t="ekr.20221004064034.706"><vh>ExpressionChecker.alias_type_in_runtime_context</vh></v>
<v t="ekr.20221004064034.707"><vh>ExpressionChecker.apply_type_arguments_to_callable</vh></v>
<v t="ekr.20221004064034.708"><vh>ExpressionChecker.visit_list_expr</vh></v>
<v t="ekr.20221004064034.709"><vh>ExpressionChecker.visit_set_expr</vh></v>
<v t="ekr.20221004064034.710"><vh>ExpressionChecker.fast_container_type</vh></v>
<v t="ekr.20221004064034.711"><vh>ExpressionChecker.check_lst_expr</vh></v>
<v t="ekr.20221004064034.712"><vh>ExpressionChecker.visit_tuple_expr</vh></v>
<v t="ekr.20221004064034.713"><vh>ExpressionChecker.fast_dict_type</vh></v>
<v t="ekr.20221004064034.714"><vh>ExpressionChecker.visit_dict_expr</vh></v>
<v t="ekr.20221004064034.715"><vh>ExpressionChecker.find_typeddict_context</vh></v>
<v t="ekr.20221004064034.716"><vh>ExpressionChecker.visit_lambda_expr</vh></v>
<v t="ekr.20221004064034.717"><vh>ExpressionChecker.infer_lambda_type_using_context</vh></v>
<v t="ekr.20221004064034.718"><vh>ExpressionChecker.visit_super_expr</vh></v>
<v t="ekr.20221004064034.719"><vh>ExpressionChecker._super_arg_types</vh></v>
<v t="ekr.20221004064034.720"><vh>ExpressionChecker.visit_slice_expr</vh></v>
<v t="ekr.20221004064034.721"><vh>ExpressionChecker.visit_list_comprehension</vh></v>
<v t="ekr.20221004064034.722"><vh>ExpressionChecker.visit_set_comprehension</vh></v>
<v t="ekr.20221004064034.723"><vh>ExpressionChecker.visit_generator_expr</vh></v>
<v t="ekr.20221004064034.724"><vh>ExpressionChecker.check_generator_or_comprehension</vh></v>
<v t="ekr.20221004064034.725"><vh>ExpressionChecker.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064034.726"><vh>ExpressionChecker.check_for_comp</vh></v>
<v t="ekr.20221004064034.727"><vh>ExpressionChecker.visit_conditional_expr</vh></v>
<v t="ekr.20221004064034.728"><vh>ExpressionChecker.analyze_cond_branch</vh></v>
<v t="ekr.20221004064034.729"><vh>ExpressionChecker.Helpers</vh></v>
<v t="ekr.20221004064034.730"><vh>ExpressionChecker.accept</vh></v>
<v t="ekr.20221004064034.731"><vh>ExpressionChecker.named_type</vh></v>
<v t="ekr.20221004064034.732"><vh>ExpressionChecker.is_valid_var_arg</vh></v>
<v t="ekr.20221004064034.733"><vh>ExpressionChecker.is_valid_keyword_var_arg</vh></v>
<v t="ekr.20221004064034.734"><vh>ExpressionChecker.has_member</vh></v>
<v t="ekr.20221004064034.735"><vh>ExpressionChecker.not_ready_callback</vh></v>
<v t="ekr.20221004064034.736"><vh>ExpressionChecker.visit_yield_expr</vh></v>
<v t="ekr.20221004064034.737"><vh>ExpressionChecker.visit_await_expr</vh></v>
<v t="ekr.20221004064034.738"><vh>ExpressionChecker.check_awaitable_expr</vh></v>
<v t="ekr.20221004064034.739"><vh>ExpressionChecker.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064034.740"><vh>ExpressionChecker.visit_temp_node</vh></v>
<v t="ekr.20221004064034.741"><vh>ExpressionChecker.visit_type_var_expr</vh></v>
<v t="ekr.20221004064034.742"><vh>ExpressionChecker.visit_paramspec_expr</vh></v>
<v t="ekr.20221004064034.743"><vh>ExpressionChecker.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064034.744"><vh>ExpressionChecker.visit_newtype_expr</vh></v>
<v t="ekr.20221004064034.745"><vh>ExpressionChecker.visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064034.746"><vh>ExpressionChecker.visit_enum_call_expr</vh></v>
<v t="ekr.20221004064034.747"><vh>ExpressionChecker.visit_typeddict_expr</vh></v>
<v t="ekr.20221004064034.748"><vh>ExpressionChecker.visit__promote_expr</vh></v>
<v t="ekr.20221004064034.749"><vh>ExpressionChecker.visit_star_expr</vh></v>
<v t="ekr.20221004064034.750"><vh>ExpressionChecker.object_type</vh></v>
<v t="ekr.20221004064034.751"><vh>ExpressionChecker.bool_type</vh></v>
<v t="ekr.20221004064034.752"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
<v t="ekr.20221004064034.753"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
<v t="ekr.20221004064034.754"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
</v>
<v t="ekr.20221004064034.755"><vh>has_any_type</vh></v>
<v t="ekr.20221004064034.756"><vh>class HasAnyType</vh>
<v t="ekr.20221004064034.757"><vh>HasAnyType.__init__</vh></v>
<v t="ekr.20221004064034.758"><vh>HasAnyType.visit_any</vh></v>
<v t="ekr.20221004064034.759"><vh>HasAnyType.visit_callable_type</vh></v>
</v>
<v t="ekr.20221004064034.760"><vh>has_coroutine_decorator</vh></v>
<v t="ekr.20221004064034.761"><vh>is_async_def</vh></v>
<v t="ekr.20221004064034.762"><vh>is_non_empty_tuple</vh></v>
<v t="ekr.20221004064034.763"><vh>is_duplicate_mapping</vh></v>
<v t="ekr.20221004064034.764"><vh>replace_callable_return_type</vh></v>
<v t="ekr.20221004064034.765"><vh>class ArgInferSecondPassQuery</vh>
<v t="ekr.20221004064034.766"><vh>ArgInferSecondPassQuery.__init__</vh></v>
<v t="ekr.20221004064034.767"><vh>ArgInferSecondPassQuery.visit_callable_type</vh></v>
</v>
<v t="ekr.20221004064034.768"><vh>class HasTypeVarQuery</vh>
<v t="ekr.20221004064034.769"><vh>HasTypeVarQuery.__init__</vh></v>
<v t="ekr.20221004064034.770"><vh>HasTypeVarQuery.visit_type_var</vh></v>
</v>
<v t="ekr.20221004064034.771"><vh>has_erased_component</vh></v>
<v t="ekr.20221004064034.772"><vh>class HasErasedComponentsQuery</vh>
<v t="ekr.20221004064034.773"><vh>HasErasedComponentsQuery.__init__</vh></v>
<v t="ekr.20221004064034.774"><vh>HasErasedComponentsQuery.visit_erased_type</vh></v>
</v>
<v t="ekr.20221004064034.775"><vh>has_uninhabited_component</vh></v>
<v t="ekr.20221004064034.776"><vh>class HasUninhabitedComponentsQuery</vh>
<v t="ekr.20221004064034.777"><vh>HasUninhabitedComponentsQuery.__init__</vh></v>
<v t="ekr.20221004064034.778"><vh>HasUninhabitedComponentsQuery.visit_uninhabited_type</vh></v>
</v>
<v t="ekr.20221004064034.779"><vh>arg_approximate_similarity</vh>
<v t="ekr.20221004064034.780"><vh>is_typetype_like</vh></v>
</v>
<v t="ekr.20221004064034.781"><vh>any_causes_overload_ambiguity</vh></v>
<v t="ekr.20221004064034.782"><vh>all_same_types</vh></v>
<v t="ekr.20221004064034.783"><vh>merge_typevars_in_callables_by_name</vh></v>
<v t="ekr.20221004064034.784"><vh>try_getting_literal</vh></v>
<v t="ekr.20221004064034.785"><vh>is_expr_literal_type</vh></v>
<v t="ekr.20221004064034.786"><vh>has_bytes_component</vh></v>
<v t="ekr.20221004064034.787"><vh>type_info_from_type</vh></v>
<v t="ekr.20221004064034.788"><vh>is_operator_method</vh></v>
<v t="ekr.20221004064034.789"><vh>get_partial_instance_type</vh></v>
</v>
<v t="ekr.20221004064034.790"><vh>@clean checkmember.py</vh>
<v t="ekr.20221004064034.791"><vh>class MemberContext</vh>
<v t="ekr.20221004064034.792"><vh>MemberContext.__init__</vh></v>
<v t="ekr.20221004064034.793"><vh>MemberContext.named_type</vh></v>
<v t="ekr.20221004064034.794"><vh>MemberContext.not_ready_callback</vh></v>
<v t="ekr.20221004064034.795"><vh>MemberContext.copy_modified</vh></v>
</v>
<v t="ekr.20221004064034.796"><vh>analyze_member_access</vh></v>
<v t="ekr.20221004064034.797"><vh>_analyze_member_access</vh></v>
<v t="ekr.20221004064034.798"><vh>may_be_awaitable_attribute</vh></v>
<v t="ekr.20221004064034.799"><vh>report_missing_attribute</vh></v>
<v t="ekr.20221004064034.800"><vh>The several functions that follow implement analyze_member_access for various</vh></v>
<v t="ekr.20221004064034.801"><vh>analyze_instance_member_access</vh></v>
<v t="ekr.20221004064034.802"><vh>validate_super_call</vh></v>
<v t="ekr.20221004064034.803"><vh>analyze_type_callable_member_access</vh></v>
<v t="ekr.20221004064034.804"><vh>analyze_type_type_member_access</vh></v>
<v t="ekr.20221004064034.805"><vh>analyze_union_member_access</vh></v>
<v t="ekr.20221004064034.806"><vh>analyze_none_member_access</vh></v>
<v t="ekr.20221004064034.807"><vh>analyze_member_var_access</vh></v>
<v t="ekr.20221004064034.808"><vh>check_final_member</vh></v>
<v t="ekr.20221004064034.809"><vh>analyze_descriptor_access</vh></v>
<v t="ekr.20221004064034.810"><vh>is_instance_var</vh></v>
<v t="ekr.20221004064034.811"><vh>analyze_var</vh></v>
<v t="ekr.20221004064034.812"><vh>freeze_type_vars</vh></v>
<v t="ekr.20221004064034.813"><vh>lookup_member_var_or_accessor</vh></v>
<v t="ekr.20221004064034.814"><vh>check_self_arg</vh></v>
<v t="ekr.20221004064034.815"><vh>analyze_class_attribute_access</vh></v>
<v t="ekr.20221004064034.816"><vh>apply_class_attr_hook</vh></v>
<v t="ekr.20221004064034.817"><vh>analyze_enum_class_attribute_access</vh></v>
<v t="ekr.20221004064034.818"><vh>analyze_typeddict_access</vh></v>
<v t="ekr.20221004064034.819"><vh>add_class_tvars</vh></v>
<v t="ekr.20221004064034.820"><vh>type_object_type</vh></v>
<v t="ekr.20221004064034.821"><vh>analyze_decorator_or_funcbase_access</vh></v>
<v t="ekr.20221004064034.822"><vh>is_valid_constructor</vh></v>
</v>
<v t="ekr.20221004064034.823"><vh>@clean checkpattern.py</vh>
<v t="ekr.20221004064034.824"><vh>class PatternType</vh></v>
<v t="ekr.20221004064034.825"><vh>class PatternChecker</vh>
<v t="ekr.20221004064034.826"><vh>PatternChecker.__init__</vh></v>
<v t="ekr.20221004064034.827"><vh>PatternChecker.accept</vh></v>
<v t="ekr.20221004064034.828"><vh>PatternChecker.visit_as_pattern</vh></v>
<v t="ekr.20221004064034.829"><vh>PatternChecker.visit_or_pattern</vh></v>
<v t="ekr.20221004064034.830"><vh>PatternChecker.visit_value_pattern</vh></v>
<v t="ekr.20221004064034.831"><vh>PatternChecker.visit_singleton_pattern</vh></v>
<v t="ekr.20221004064034.832"><vh>PatternChecker.visit_sequence_pattern</vh></v>
<v t="ekr.20221004064034.833"><vh>PatternChecker.get_sequence_type</vh></v>
<v t="ekr.20221004064034.834"><vh>PatternChecker.contract_starred_pattern_types</vh></v>
<v t="ekr.20221004064034.835"><vh>PatternChecker.expand_starred_pattern_types</vh></v>
<v t="ekr.20221004064034.836"><vh>PatternChecker.visit_starred_pattern</vh></v>
<v t="ekr.20221004064034.837"><vh>PatternChecker.visit_mapping_pattern</vh></v>
<v t="ekr.20221004064034.838"><vh>PatternChecker.get_mapping_item_type</vh></v>
<v t="ekr.20221004064034.839"><vh>PatternChecker.get_simple_mapping_item_type</vh></v>
<v t="ekr.20221004064034.840"><vh>PatternChecker.visit_class_pattern</vh></v>
<v t="ekr.20221004064034.841"><vh>PatternChecker.should_self_match</vh></v>
<v t="ekr.20221004064034.842"><vh>PatternChecker.can_match_sequence</vh></v>
<v t="ekr.20221004064034.843"><vh>PatternChecker.generate_types_from_names</vh></v>
<v t="ekr.20221004064034.844"><vh>PatternChecker.update_type_map</vh></v>
<v t="ekr.20221004064034.845"><vh>PatternChecker.construct_sequence_child</vh></v>
<v t="ekr.20221004064034.846"><vh>PatternChecker.early_non_match</vh></v>
</v>
<v t="ekr.20221004064034.847"><vh>get_match_arg_names</vh></v>
<v t="ekr.20221004064034.848"><vh>get_var</vh></v>
<v t="ekr.20221004064034.849"><vh>get_type_range</vh></v>
<v t="ekr.20221004064034.850"><vh>is_uninhabited</vh></v>
</v>
<v t="ekr.20221004064034.851"><vh>@clean checkstrformat.py</vh>
<v t="ekr.20221004064034.852"><vh>compile_format_re</vh></v>
<v t="ekr.20221004064034.853"><vh>compile_new_format_re</vh></v>
<v t="ekr.20221004064034.854"><vh>FORMAT_RE: Final = compile_format_re()</vh></v>
<v t="ekr.20221004064034.855"><vh>class ConversionSpecifier</vh>
<v t="ekr.20221004064034.856"><vh>ConversionSpecifier.__init__</vh></v>
<v t="ekr.20221004064034.857"><vh>ConversionSpecifier.has_key</vh></v>
<v t="ekr.20221004064034.858"><vh>ConversionSpecifier.has_star</vh></v>
</v>
<v t="ekr.20221004064034.859"><vh>parse_conversion_specifiers</vh></v>
<v t="ekr.20221004064034.860"><vh>parse_format_value</vh></v>
<v t="ekr.20221004064034.861"><vh>find_non_escaped_targets</vh></v>
<v t="ekr.20221004064034.862"><vh>class StringFormatterChecker</vh>
<v t="ekr.20221004064034.863"><vh>StringFormatterChecker.__init__</vh></v>
<v t="ekr.20221004064034.864"><vh>StringFormatterChecker.check_str_format_call</vh></v>
<v t="ekr.20221004064034.865"><vh>StringFormatterChecker.check_specs_in_format_call</vh></v>
<v t="ekr.20221004064034.866"><vh>StringFormatterChecker.perform_special_format_checks</vh></v>
<v t="ekr.20221004064034.867"><vh>StringFormatterChecker.find_replacements_in_call</vh></v>
<v t="ekr.20221004064034.868"><vh>StringFormatterChecker.get_expr_by_position</vh></v>
<v t="ekr.20221004064034.869"><vh>StringFormatterChecker.get_expr_by_name</vh></v>
<v t="ekr.20221004064034.870"><vh>StringFormatterChecker.auto_generate_keys</vh></v>
<v t="ekr.20221004064034.871"><vh>StringFormatterChecker.apply_field_accessors</vh></v>
<v t="ekr.20221004064034.872"><vh>StringFormatterChecker.validate_and_transform_accessors</vh></v>
<v t="ekr.20221004064034.873"><vh>StringFormatterChecker.check_str_interpolation</vh></v>
<v t="ekr.20221004064034.874"><vh>StringFormatterChecker.analyze_conversion_specifiers</vh></v>
<v t="ekr.20221004064034.875"><vh>StringFormatterChecker.check_simple_str_interpolation</vh></v>
<v t="ekr.20221004064034.876"><vh>StringFormatterChecker.check_mapping_str_interpolation</vh></v>
<v t="ekr.20221004064034.877"><vh>StringFormatterChecker.build_dict_type</vh></v>
<v t="ekr.20221004064034.878"><vh>StringFormatterChecker.build_replacement_checkers</vh></v>
<v t="ekr.20221004064034.879"><vh>StringFormatterChecker.replacement_checkers</vh></v>
<v t="ekr.20221004064034.880"><vh>StringFormatterChecker.checkers_for_star</vh></v>
<v t="ekr.20221004064034.881"><vh>StringFormatterChecker.check_placeholder_type</vh></v>
<v t="ekr.20221004064034.882"><vh>StringFormatterChecker.checkers_for_regular_type</vh></v>
<v t="ekr.20221004064034.883"><vh>StringFormatterChecker.check_s_special_cases</vh></v>
<v t="ekr.20221004064034.884"><vh>StringFormatterChecker.checkers_for_c_type</vh></v>
<v t="ekr.20221004064034.885"><vh>StringFormatterChecker.conversion_type</vh></v>
<v t="ekr.20221004064034.886"><vh>StringFormatterChecker.Helpers</vh></v>
<v t="ekr.20221004064034.887"><vh>StringFormatterChecker.named_type</vh></v>
<v t="ekr.20221004064034.888"><vh>StringFormatterChecker.accept</vh></v>
</v>
<v t="ekr.20221004064034.889"><vh>has_type_component</vh></v>
</v>
</v>
<v t="ekr.20221005082302.1"><vh>--- config</vh>
<v t="ekr.20221004064034.890"><vh>@clean config_parser.py</vh>
<v t="ekr.20221004064034.891"><vh>parse_version</vh></v>
<v t="ekr.20221004064034.892"><vh>try_split</vh></v>
<v t="ekr.20221004064034.893"><vh>validate_codes</vh></v>
<v t="ekr.20221004064034.894"><vh>expand_path</vh></v>
<v t="ekr.20221004064034.895"><vh>str_or_array_as_list</vh></v>
<v t="ekr.20221004064034.896"><vh>split_and_match_files_list</vh></v>
<v t="ekr.20221004064034.897"><vh>split_and_match_files</vh></v>
<v t="ekr.20221004064034.898"><vh>check_follow_imports</vh></v>
<v t="ekr.20221004064034.899"><vh>For most options, the type of the default value set in options.py is</vh></v>
<v t="ekr.20221004064034.900"><vh>parse_config_file</vh></v>
<v t="ekr.20221004064034.901"><vh>get_prefix</vh></v>
<v t="ekr.20221004064034.902"><vh>is_toml</vh></v>
<v t="ekr.20221004064034.903"><vh>destructure_overrides</vh></v>
<v t="ekr.20221004064034.904"><vh>parse_section</vh></v>
<v t="ekr.20221004064034.905"><vh>convert_to_boolean</vh></v>
<v t="ekr.20221004064034.906"><vh>split_directive</vh></v>
<v t="ekr.20221004064034.907"><vh>mypy_comments_to_config_map</vh></v>
<v t="ekr.20221004064034.908"><vh>parse_mypy_comments</vh></v>
<v t="ekr.20221004064034.909"><vh>get_config_module_names</vh></v>
<v t="ekr.20221004064034.910"><vh>class ConfigTOMLValueError</vh></v>
</v>
<v t="ekr.20221004064035.864"><vh>@clean options.py</vh>
<v t="ekr.20221004064035.865"><vh>class BuildType</vh></v>
<v t="ekr.20221004064035.866"><vh>PER_MODULE_OPTIONS: Final = {</vh></v>
<v t="ekr.20221004064035.867"><vh>class Options</vh>
<v t="ekr.20221004064035.868"><vh>Options.__init__</vh></v>
<v t="ekr.20221004064035.869"><vh>Options.new_semantic_analyzer</vh></v>
<v t="ekr.20221004064035.870"><vh>Options.snapshot</vh></v>
<v t="ekr.20221004064035.871"><vh>Options.__repr__</vh></v>
<v t="ekr.20221004064035.872"><vh>Options.apply_changes</vh></v>
<v t="ekr.20221004064035.873"><vh>Options.build_per_module_cache</vh></v>
<v t="ekr.20221004064035.874"><vh>Options.clone_for_module</vh></v>
<v t="ekr.20221004064035.875"><vh>Options.compile_glob</vh></v>
<v t="ekr.20221004064035.876"><vh>Options.select_options_affecting_cache</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1650"><vh>@clean state.py</vh>
<v t="ekr.20221004064035.1651"><vh>class StrictOptionalState</vh>
<v t="ekr.20221004064035.1652"><vh>StrictOptionalState.__init__</vh></v>
<v t="ekr.20221004064035.1653"><vh>StrictOptionalState.strict_optional_set</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1644"><vh>@clean split_namespace.py</vh>
<v t="ekr.20221004064035.1645"><vh>class SplitNamespace</vh>
<v t="ekr.20221004064035.1646"><vh>SplitNamespace.__init__</vh></v>
<v t="ekr.20221004064035.1647"><vh>SplitNamespace._get</vh></v>
<v t="ekr.20221004064035.1648"><vh>SplitNamespace.__setattr__</vh></v>
<v t="ekr.20221004064035.1649"><vh>SplitNamespace.__getattr__</vh></v>
</v>
</v>
</v>
<v t="ekr.20221005080846.1"><vh>--- dmypy</vh>
<v t="ekr.20221004064034.992"><vh>@clean dmypy_os.py</vh>
<v t="ekr.20221004064034.993"><vh>alive</vh></v>
<v t="ekr.20221004064034.994"><vh>kill</vh></v>
</v>
<v t="ekr.20221004064034.995"><vh>@clean dmypy_server.py</vh>
<v t="ekr.20221004064034.996"><vh>process_start_options</vh></v>
<v t="ekr.20221004064034.997"><vh>ignore_suppressed_imports</vh></v>
<v t="ekr.20221004064034.998"><vh>ModulePathPair: _TypeAlias = Tuple[str, str]</vh></v>
<v t="ekr.20221004064034.999"><vh>class Server</vh>
<v t="ekr.20221004064034.1000"><vh>Server.__init__</vh></v>
<v t="ekr.20221004064034.1001"><vh>Server._response_metadata</vh></v>
<v t="ekr.20221004064034.1002"><vh>Server.serve</vh></v>
<v t="ekr.20221004064034.1003"><vh>Server.run_command</vh></v>
<v t="ekr.20221004064034.1004"><vh>Server.Command functions (run in the server via RPC).</vh></v>
<v t="ekr.20221004064034.1005"><vh>Server.cmd_status</vh></v>
<v t="ekr.20221004064034.1006"><vh>Server.cmd_stop</vh></v>
<v t="ekr.20221004064034.1007"><vh>Server.cmd_run</vh></v>
<v t="ekr.20221004064034.1008"><vh>Server.cmd_check</vh></v>
<v t="ekr.20221004064034.1009"><vh>Server.cmd_recheck</vh></v>
<v t="ekr.20221004064034.1010"><vh>Server.check</vh></v>
<v t="ekr.20221004064034.1011"><vh>Server.flush_caches</vh></v>
<v t="ekr.20221004064034.1012"><vh>Server.update_stats</vh></v>
<v t="ekr.20221004064034.1013"><vh>Server.following_imports</vh></v>
<v t="ekr.20221004064034.1014"><vh>Server.initialize_fine_grained</vh></v>
<v t="ekr.20221004064034.1015"><vh>Server.fine_grained_increment</vh></v>
<v t="ekr.20221004064034.1016"><vh>Server.fine_grained_increment_follow_imports</vh></v>
<v t="ekr.20221004064034.1017"><vh>Server.find_reachable_changed_modules</vh></v>
<v t="ekr.20221004064034.1018"><vh>Server.direct_imports</vh></v>
<v t="ekr.20221004064034.1019"><vh>Server.find_added_suppressed</vh></v>
<v t="ekr.20221004064034.1020"><vh>Server.increment_output</vh></v>
<v t="ekr.20221004064034.1021"><vh>Server.pretty_messages</vh></v>
<v t="ekr.20221004064034.1022"><vh>Server.update_sources</vh></v>
<v t="ekr.20221004064034.1023"><vh>Server.update_changed</vh></v>
<v t="ekr.20221004064034.1024"><vh>Server.find_changed</vh></v>
<v t="ekr.20221004064034.1025"><vh>Server._find_changed</vh></v>
<v t="ekr.20221004064034.1026"><vh>Server.cmd_inspect</vh></v>
<v t="ekr.20221004064034.1027"><vh>Server.cmd_suggest</vh></v>
<v t="ekr.20221004064034.1028"><vh>Server.cmd_hang</vh></v>
</v>
<v t="ekr.20221004064034.1029"><vh>Misc utilities.</vh></v>
<v t="ekr.20221004064034.1030"><vh>get_meminfo</vh></v>
<v t="ekr.20221004064034.1031"><vh>find_all_sources_in_build</vh></v>
<v t="ekr.20221004064034.1032"><vh>fix_module_deps</vh></v>
<v t="ekr.20221004064034.1033"><vh>filter_out_missing_top_level_packages</vh></v>
</v>
<v t="ekr.20221004064034.1034"><vh>@clean dmypy_util.py</vh>
<v t="ekr.20221004064034.1035"><vh>receive</vh></v>
</v>
</v>
<v t="ekr.20221005081317.1"><vh>--- errors, logs messages &amp; stats</vh>
<v t="ekr.20221004064034.1074"><vh>@clean errorcodes.py</vh>
<v t="ekr.20221004064034.1075"><vh>class ErrorCode</vh>
<v t="ekr.20221004064034.1076"><vh>ErrorCode.__init__</vh></v>
<v t="ekr.20221004064034.1077"><vh>ErrorCode.__str__</vh></v>
</v>
</v>
<v t="ekr.20221004064034.1078"><vh>@clean errors.py</vh>
<v t="ekr.20221004064034.1079"><vh>class ErrorInfo</vh>
<v t="ekr.20221004064034.1080"><vh>ErrorInfo.__init__</vh></v>
</v>
<v t="ekr.20221004064034.1081"><vh>Type used internally to represent errors:</vh></v>
<v t="ekr.20221004064034.1082"><vh>class ErrorWatcher</vh>
<v t="ekr.20221004064034.1083"><vh>ErrorWatcher.__init__</vh></v>
<v t="ekr.20221004064034.1084"><vh>ErrorWatcher.__enter__</vh></v>
<v t="ekr.20221004064034.1085"><vh>ErrorWatcher.__exit__</vh></v>
<v t="ekr.20221004064034.1086"><vh>ErrorWatcher.on_error</vh></v>
<v t="ekr.20221004064034.1087"><vh>ErrorWatcher.has_new_errors</vh></v>
<v t="ekr.20221004064034.1088"><vh>ErrorWatcher.filtered_errors</vh></v>
</v>
<v t="ekr.20221004064034.1089"><vh>class Errors</vh>
<v t="ekr.20221004064034.1090"><vh>Errors.__init__</vh></v>
<v t="ekr.20221004064034.1091"><vh>Errors.initialize</vh></v>
<v t="ekr.20221004064034.1092"><vh>Errors.reset</vh></v>
<v t="ekr.20221004064034.1093"><vh>Errors.set_ignore_prefix</vh></v>
<v t="ekr.20221004064034.1094"><vh>Errors.simplify_path</vh></v>
<v t="ekr.20221004064034.1095"><vh>Errors.set_file</vh></v>
<v t="ekr.20221004064034.1096"><vh>Errors.set_file_ignored_lines</vh></v>
<v t="ekr.20221004064034.1097"><vh>Errors.current_target</vh></v>
<v t="ekr.20221004064034.1098"><vh>Errors.current_module</vh></v>
<v t="ekr.20221004064034.1099"><vh>Errors.import_context</vh></v>
<v t="ekr.20221004064034.1100"><vh>Errors.set_import_context</vh></v>
<v t="ekr.20221004064034.1101"><vh>Errors.report</vh></v>
<v t="ekr.20221004064034.1102"><vh>Errors._add_error_info</vh></v>
<v t="ekr.20221004064034.1103"><vh>Errors._filter_error</vh></v>
<v t="ekr.20221004064034.1104"><vh>Errors.add_error_info</vh></v>
<v t="ekr.20221004064034.1105"><vh>Errors.has_many_errors</vh></v>
<v t="ekr.20221004064034.1106"><vh>Errors.report_hidden_errors</vh></v>
<v t="ekr.20221004064034.1107"><vh>Errors.is_ignored_error</vh></v>
<v t="ekr.20221004064034.1108"><vh>Errors.is_error_code_enabled</vh></v>
<v t="ekr.20221004064034.1109"><vh>Errors.clear_errors_in_targets</vh></v>
<v t="ekr.20221004064034.1110"><vh>Errors.generate_unused_ignore_errors</vh></v>
<v t="ekr.20221004064034.1111"><vh>Errors.generate_ignore_without_code_errors</vh></v>
<v t="ekr.20221004064034.1112"><vh>Errors.num_messages</vh></v>
<v t="ekr.20221004064034.1113"><vh>Errors.is_errors</vh></v>
<v t="ekr.20221004064034.1114"><vh>Errors.is_blockers</vh></v>
<v t="ekr.20221004064034.1115"><vh>Errors.blocker_module</vh></v>
<v t="ekr.20221004064034.1116"><vh>Errors.is_errors_for_file</vh></v>
<v t="ekr.20221004064034.1117"><vh>Errors.raise_error</vh></v>
<v t="ekr.20221004064034.1118"><vh>Errors.format_messages</vh></v>
<v t="ekr.20221004064034.1119"><vh>Errors.file_messages</vh></v>
<v t="ekr.20221004064034.1120"><vh>Errors.new_messages</vh></v>
<v t="ekr.20221004064034.1121"><vh>Errors.targets</vh></v>
<v t="ekr.20221004064034.1122"><vh>Errors.render_messages</vh></v>
<v t="ekr.20221004064034.1123"><vh>Errors.sort_messages</vh></v>
<v t="ekr.20221004064034.1124"><vh>Errors.remove_duplicates</vh></v>
</v>
<v t="ekr.20221004064034.1125"><vh>class CompileError</vh>
<v t="ekr.20221004064034.1126"><vh>CompileError.__init__</vh></v>
</v>
<v t="ekr.20221004064034.1127"><vh>remove_path_prefix</vh></v>
<v t="ekr.20221004064034.1128"><vh>report_internal_error</vh></v>
</v>
<v t="ekr.20221004064034.1401"><vh>@clean gclogger.py</vh>
<v t="ekr.20221004064034.1402"><vh>class GcLogger</vh>
<v t="ekr.20221004064034.1403"><vh>GcLogger.__enter__</vh></v>
<v t="ekr.20221004064034.1404"><vh>GcLogger.gc_callback</vh></v>
<v t="ekr.20221004064034.1405"><vh>GcLogger.__exit__</vh></v>
<v t="ekr.20221004064034.1406"><vh>GcLogger.get_stats</vh></v>
</v>
</v>
<v t="ekr.20221004064035.202"><vh>@clean memprofile.py</vh>
<v t="ekr.20221004064035.203"><vh>collect_memory_stats</vh></v>
<v t="ekr.20221004064035.204"><vh>print_memory_profile</vh></v>
<v t="ekr.20221004064035.205"><vh>find_recursive_objects</vh>
<v t="ekr.20221004064035.206"><vh>visit</vh></v>
</v>
</v>
<v t="ekr.20221004064035.375"><vh>@clean message_registry.py</vh>
<v t="ekr.20221004064035.376"><vh>class ErrorMessage</vh>
<v t="ekr.20221004064035.377"><vh>ErrorMessage.format</vh></v>
<v t="ekr.20221004064035.378"><vh>ErrorMessage.with_additional_msg</vh></v>
</v>
</v>
<v t="ekr.20221004064035.207"><vh>@clean messages.py</vh>
<v t="ekr.20221004064035.208"><vh>class MessageBuilder</vh>
<v t="ekr.20221004064035.209"><vh>MessageBuilder.__init__</vh></v>
<v t="ekr.20221004064035.210"><vh>MessageBuilder.Helpers</vh></v>
<v t="ekr.20221004064035.211"><vh>MessageBuilder.filter_errors</vh></v>
<v t="ekr.20221004064035.212"><vh>MessageBuilder.add_errors</vh></v>
<v t="ekr.20221004064035.213"><vh>MessageBuilder.disable_type_names</vh></v>
<v t="ekr.20221004064035.214"><vh>MessageBuilder.are_type_names_disabled</vh></v>
<v t="ekr.20221004064035.215"><vh>MessageBuilder.report</vh></v>
<v t="ekr.20221004064035.216"><vh>MessageBuilder.fail</vh></v>
<v t="ekr.20221004064035.217"><vh>MessageBuilder.note</vh></v>
<v t="ekr.20221004064035.218"><vh>MessageBuilder.note_multiline</vh></v>
<v t="ekr.20221004064035.219"><vh>MessageBuilder.Specific operations</vh></v>
<v t="ekr.20221004064035.220"><vh>MessageBuilder.has_no_attr</vh></v>
<v t="ekr.20221004064035.221"><vh>MessageBuilder.unsupported_operand_types</vh></v>
<v t="ekr.20221004064035.222"><vh>MessageBuilder.unsupported_left_operand</vh></v>
<v t="ekr.20221004064035.223"><vh>MessageBuilder.not_callable</vh></v>
<v t="ekr.20221004064035.224"><vh>MessageBuilder.untyped_function_call</vh></v>
<v t="ekr.20221004064035.225"><vh>MessageBuilder.incompatible_argument</vh></v>
<v t="ekr.20221004064035.226"><vh>MessageBuilder.incompatible_argument_note</vh></v>
<v t="ekr.20221004064035.227"><vh>MessageBuilder.maybe_note_concatenate_pos_args</vh></v>
<v t="ekr.20221004064035.228"><vh>MessageBuilder.invalid_index_type</vh></v>
<v t="ekr.20221004064035.229"><vh>MessageBuilder.too_few_arguments</vh></v>
<v t="ekr.20221004064035.230"><vh>MessageBuilder.missing_named_argument</vh></v>
<v t="ekr.20221004064035.231"><vh>MessageBuilder.too_many_arguments</vh></v>
<v t="ekr.20221004064035.232"><vh>MessageBuilder.too_many_arguments_from_typed_dict</vh></v>
<v t="ekr.20221004064035.233"><vh>MessageBuilder.too_many_positional_arguments</vh></v>
<v t="ekr.20221004064035.234"><vh>MessageBuilder.maybe_note_about_special_args</vh></v>
<v t="ekr.20221004064035.235"><vh>MessageBuilder.unexpected_keyword_argument</vh></v>
<v t="ekr.20221004064035.236"><vh>MessageBuilder.duplicate_argument_value</vh></v>
<v t="ekr.20221004064035.237"><vh>MessageBuilder.does_not_return_value</vh></v>
<v t="ekr.20221004064035.238"><vh>MessageBuilder.deleted_as_rvalue</vh></v>
<v t="ekr.20221004064035.239"><vh>MessageBuilder.deleted_as_lvalue</vh></v>
<v t="ekr.20221004064035.240"><vh>MessageBuilder.no_variant_matches_arguments</vh></v>
<v t="ekr.20221004064035.241"><vh>MessageBuilder.wrong_number_values_to_unpack</vh></v>
<v t="ekr.20221004064035.242"><vh>MessageBuilder.unpacking_strings_disallowed</vh></v>
<v t="ekr.20221004064035.243"><vh>MessageBuilder.type_not_iterable</vh></v>
<v t="ekr.20221004064035.244"><vh>MessageBuilder.possible_missing_await</vh></v>
<v t="ekr.20221004064035.245"><vh>MessageBuilder.incompatible_operator_assignment</vh></v>
<v t="ekr.20221004064035.246"><vh>MessageBuilder.overload_signature_incompatible_with_supertype</vh></v>
<v t="ekr.20221004064035.247"><vh>MessageBuilder.signature_incompatible_with_supertype</vh></v>
<v t="ekr.20221004064035.248"><vh>MessageBuilder.pretty_callable_or_overload</vh></v>
<v t="ekr.20221004064035.249"><vh>MessageBuilder.argument_incompatible_with_supertype</vh></v>
<v t="ekr.20221004064035.250"><vh>MessageBuilder.comparison_method_example_msg</vh></v>
<v t="ekr.20221004064035.251"><vh>MessageBuilder.return_type_incompatible_with_supertype</vh></v>
<v t="ekr.20221004064035.252"><vh>MessageBuilder.override_target</vh></v>
<v t="ekr.20221004064035.253"><vh>MessageBuilder.incompatible_type_application</vh></v>
<v t="ekr.20221004064035.254"><vh>MessageBuilder.could_not_infer_type_arguments</vh></v>
<v t="ekr.20221004064035.255"><vh>MessageBuilder.invalid_var_arg</vh></v>
<v t="ekr.20221004064035.256"><vh>MessageBuilder.invalid_keyword_var_arg</vh></v>
<v t="ekr.20221004064035.257"><vh>MessageBuilder.undefined_in_superclass</vh></v>
<v t="ekr.20221004064035.258"><vh>MessageBuilder.variable_may_be_undefined</vh></v>
<v t="ekr.20221004064035.259"><vh>MessageBuilder.first_argument_for_super_must_be_type</vh></v>
<v t="ekr.20221004064035.260"><vh>MessageBuilder.unsafe_super</vh></v>
<v t="ekr.20221004064035.261"><vh>MessageBuilder.too_few_string_formatting_arguments</vh></v>
<v t="ekr.20221004064035.262"><vh>MessageBuilder.too_many_string_formatting_arguments</vh></v>
<v t="ekr.20221004064035.263"><vh>MessageBuilder.unsupported_placeholder</vh></v>
<v t="ekr.20221004064035.264"><vh>MessageBuilder.string_interpolation_with_star_and_key</vh></v>
<v t="ekr.20221004064035.265"><vh>MessageBuilder.requires_int_or_single_byte</vh></v>
<v t="ekr.20221004064035.266"><vh>MessageBuilder.requires_int_or_char</vh></v>
<v t="ekr.20221004064035.267"><vh>MessageBuilder.key_not_in_mapping</vh></v>
<v t="ekr.20221004064035.268"><vh>MessageBuilder.string_interpolation_mixing_key_and_non_keys</vh></v>
<v t="ekr.20221004064035.269"><vh>MessageBuilder.cannot_determine_type</vh></v>
<v t="ekr.20221004064035.270"><vh>MessageBuilder.cannot_determine_type_in_base</vh></v>
<v t="ekr.20221004064035.271"><vh>MessageBuilder.no_formal_self</vh></v>
<v t="ekr.20221004064035.272"><vh>MessageBuilder.incompatible_self_argument</vh></v>
<v t="ekr.20221004064035.273"><vh>MessageBuilder.incompatible_conditional_function_def</vh></v>
<v t="ekr.20221004064035.274"><vh>MessageBuilder.cannot_instantiate_abstract_class</vh></v>
<v t="ekr.20221004064035.275"><vh>MessageBuilder.base_class_definitions_incompatible</vh></v>
<v t="ekr.20221004064035.276"><vh>MessageBuilder.cant_assign_to_method</vh></v>
<v t="ekr.20221004064035.277"><vh>MessageBuilder.cant_assign_to_classvar</vh></v>
<v t="ekr.20221004064035.278"><vh>MessageBuilder.final_cant_override_writable</vh></v>
<v t="ekr.20221004064035.279"><vh>MessageBuilder.cant_override_final</vh></v>
<v t="ekr.20221004064035.280"><vh>MessageBuilder.cant_assign_to_final</vh></v>
<v t="ekr.20221004064035.281"><vh>MessageBuilder.protocol_members_cant_be_final</vh></v>
<v t="ekr.20221004064035.282"><vh>MessageBuilder.final_without_value</vh></v>
<v t="ekr.20221004064035.283"><vh>MessageBuilder.read_only_property</vh></v>
<v t="ekr.20221004064035.284"><vh>MessageBuilder.incompatible_typevar_value</vh></v>
<v t="ekr.20221004064035.285"><vh>MessageBuilder.dangerous_comparison</vh></v>
<v t="ekr.20221004064035.286"><vh>MessageBuilder.overload_inconsistently_applies_decorator</vh></v>
<v t="ekr.20221004064035.287"><vh>MessageBuilder.overloaded_signatures_overlap</vh></v>
<v t="ekr.20221004064035.288"><vh>MessageBuilder.overloaded_signature_will_never_match</vh></v>
<v t="ekr.20221004064035.289"><vh>MessageBuilder.overloaded_signatures_typevar_specific</vh></v>
<v t="ekr.20221004064035.290"><vh>MessageBuilder.overloaded_signatures_arg_specific</vh></v>
<v t="ekr.20221004064035.291"><vh>MessageBuilder.overloaded_signatures_ret_specific</vh></v>
<v t="ekr.20221004064035.292"><vh>MessageBuilder.warn_both_operands_are_from_unions</vh></v>
<v t="ekr.20221004064035.293"><vh>MessageBuilder.warn_operand_was_from_union</vh></v>
<v t="ekr.20221004064035.294"><vh>MessageBuilder.operator_method_signatures_overlap</vh></v>
<v t="ekr.20221004064035.295"><vh>MessageBuilder.forward_operator_not_callable</vh></v>
<v t="ekr.20221004064035.296"><vh>MessageBuilder.signatures_incompatible</vh></v>
<v t="ekr.20221004064035.297"><vh>MessageBuilder.yield_from_invalid_operand_type</vh></v>
<v t="ekr.20221004064035.298"><vh>MessageBuilder.invalid_signature</vh></v>
<v t="ekr.20221004064035.299"><vh>MessageBuilder.invalid_signature_for_special_method</vh></v>
<v t="ekr.20221004064035.300"><vh>MessageBuilder.reveal_type</vh></v>
<v t="ekr.20221004064035.301"><vh>MessageBuilder.reveal_locals</vh></v>
<v t="ekr.20221004064035.302"><vh>MessageBuilder.unsupported_type_type</vh></v>
<v t="ekr.20221004064035.303"><vh>MessageBuilder.redundant_cast</vh></v>
<v t="ekr.20221004064035.304"><vh>MessageBuilder.assert_type_fail</vh></v>
<v t="ekr.20221004064035.305"><vh>MessageBuilder.unimported_type_becomes_any</vh></v>
<v t="ekr.20221004064035.306"><vh>MessageBuilder.need_annotation_for_var</vh></v>
<v t="ekr.20221004064035.307"><vh>MessageBuilder.explicit_any</vh></v>
<v t="ekr.20221004064035.308"><vh>MessageBuilder.unexpected_typeddict_keys</vh></v>
<v t="ekr.20221004064035.309"><vh>MessageBuilder.typeddict_key_must_be_string_literal</vh></v>
<v t="ekr.20221004064035.310"><vh>MessageBuilder.typeddict_key_not_found</vh></v>
<v t="ekr.20221004064035.311"><vh>MessageBuilder.typeddict_context_ambiguous</vh></v>
<v t="ekr.20221004064035.312"><vh>MessageBuilder.typeddict_key_cannot_be_deleted</vh></v>
<v t="ekr.20221004064035.313"><vh>MessageBuilder.typeddict_setdefault_arguments_inconsistent</vh></v>
<v t="ekr.20221004064035.314"><vh>MessageBuilder.type_arguments_not_allowed</vh></v>
<v t="ekr.20221004064035.315"><vh>MessageBuilder.disallowed_any_type</vh></v>
<v t="ekr.20221004064035.316"><vh>MessageBuilder.incorrectly_returning_any</vh></v>
<v t="ekr.20221004064035.317"><vh>MessageBuilder.incorrect__exit__return</vh></v>
<v t="ekr.20221004064035.318"><vh>MessageBuilder.untyped_decorated_function</vh></v>
<v t="ekr.20221004064035.319"><vh>MessageBuilder.typed_function_untyped_decorator</vh></v>
<v t="ekr.20221004064035.320"><vh>MessageBuilder.bad_proto_variance</vh></v>
<v t="ekr.20221004064035.321"><vh>MessageBuilder.concrete_only_assign</vh></v>
<v t="ekr.20221004064035.322"><vh>MessageBuilder.concrete_only_call</vh></v>
<v t="ekr.20221004064035.323"><vh>MessageBuilder.cannot_use_function_with_type</vh></v>
<v t="ekr.20221004064035.324"><vh>MessageBuilder.report_non_method_protocol</vh></v>
<v t="ekr.20221004064035.325"><vh>MessageBuilder.note_call</vh></v>
<v t="ekr.20221004064035.326"><vh>MessageBuilder.unreachable_statement</vh></v>
<v t="ekr.20221004064035.327"><vh>MessageBuilder.redundant_left_operand</vh></v>
<v t="ekr.20221004064035.328"><vh>MessageBuilder.unreachable_right_operand</vh></v>
<v t="ekr.20221004064035.329"><vh>MessageBuilder.redundant_condition_in_comprehension</vh></v>
<v t="ekr.20221004064035.330"><vh>MessageBuilder.redundant_condition_in_if</vh></v>
<v t="ekr.20221004064035.331"><vh>MessageBuilder.redundant_expr</vh></v>
<v t="ekr.20221004064035.332"><vh>MessageBuilder.impossible_intersection</vh></v>
<v t="ekr.20221004064035.333"><vh>MessageBuilder.report_protocol_problems</vh></v>
<v t="ekr.20221004064035.334"><vh>MessageBuilder.pretty_overload</vh></v>
<v t="ekr.20221004064035.335"><vh>MessageBuilder.print_more</vh></v>
<v t="ekr.20221004064035.336"><vh>MessageBuilder.try_report_long_tuple_assignment_error</vh></v>
<v t="ekr.20221004064035.337"><vh>MessageBuilder.format_long_tuple_type</vh></v>
<v t="ekr.20221004064035.338"><vh>MessageBuilder.generate_incompatible_tuple_error</vh></v>
<v t="ekr.20221004064035.339"><vh>MessageBuilder.add_fixture_note</vh></v>
</v>
<v t="ekr.20221004064035.340"><vh>quote_type_string</vh></v>
<v t="ekr.20221004064035.341"><vh>format_callable_args</vh></v>
<v t="ekr.20221004064035.342"><vh>format_type_inner</vh>
<v t="ekr.20221004064035.343"><vh>format</vh></v>
<v t="ekr.20221004064035.344"><vh>format_list</vh></v>
<v t="ekr.20221004064035.345"><vh>format_literal_value</vh></v>
</v>
<v t="ekr.20221004064035.346"><vh>collect_all_instances</vh></v>
<v t="ekr.20221004064035.347"><vh>class CollectAllInstancesQuery</vh>
<v t="ekr.20221004064035.348"><vh>CollectAllInstancesQuery.__init__</vh></v>
<v t="ekr.20221004064035.349"><vh>CollectAllInstancesQuery.visit_instance</vh></v>
<v t="ekr.20221004064035.350"><vh>CollectAllInstancesQuery.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064035.351"><vh>find_type_overlaps</vh></v>
<v t="ekr.20221004064035.352"><vh>format_type</vh></v>
<v t="ekr.20221004064035.353"><vh>format_type_bare</vh></v>
<v t="ekr.20221004064035.354"><vh>format_type_distinctly</vh></v>
<v t="ekr.20221004064035.355"><vh>pretty_class_or_static_decorator</vh></v>
<v t="ekr.20221004064035.356"><vh>pretty_callable</vh></v>
<v t="ekr.20221004064035.357"><vh>variance_string</vh></v>
<v t="ekr.20221004064035.358"><vh>get_missing_protocol_members</vh></v>
<v t="ekr.20221004064035.359"><vh>get_conflict_protocol_types</vh></v>
<v t="ekr.20221004064035.360"><vh>get_bad_protocol_flags</vh></v>
<v t="ekr.20221004064035.361"><vh>capitalize</vh></v>
<v t="ekr.20221004064035.362"><vh>extract_type</vh></v>
<v t="ekr.20221004064035.363"><vh>strip_quotes</vh></v>
<v t="ekr.20221004064035.364"><vh>format_string_list</vh></v>
<v t="ekr.20221004064035.365"><vh>format_item_name_list</vh></v>
<v t="ekr.20221004064035.366"><vh>callable_name</vh></v>
<v t="ekr.20221004064035.367"><vh>for_function</vh></v>
<v t="ekr.20221004064035.368"><vh>find_defining_module</vh></v>
<v t="ekr.20221004064035.369"><vh>For hard-coding suggested missing member alternatives.</vh></v>
<v t="ekr.20221004064035.370"><vh>best_matches</vh></v>
<v t="ekr.20221004064035.371"><vh>pretty_seq</vh></v>
<v t="ekr.20221004064035.372"><vh>append_invariance_notes</vh></v>
<v t="ekr.20221004064035.373"><vh>make_inferred_type_note</vh></v>
<v t="ekr.20221004064035.374"><vh>format_key_list</vh></v>
</v>
<v t="ekr.20221004064035.1108"><vh>@clean report.py</vh>
<v t="ekr.20221004064035.1109"><vh>class Reports</vh>
<v t="ekr.20221004064035.1110"><vh>Reports.__init__</vh></v>
<v t="ekr.20221004064035.1111"><vh>Reports.add_report</vh></v>
<v t="ekr.20221004064035.1112"><vh>Reports.file</vh></v>
<v t="ekr.20221004064035.1113"><vh>Reports.finish</vh></v>
</v>
<v t="ekr.20221004064035.1114"><vh>class AbstractReporter</vh>
<v t="ekr.20221004064035.1115"><vh>AbstractReporter.__init__</vh></v>
<v t="ekr.20221004064035.1116"><vh>AbstractReporter.on_file</vh></v>
<v t="ekr.20221004064035.1117"><vh>AbstractReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1118"><vh>register_reporter</vh></v>
<v t="ekr.20221004064035.1119"><vh>alias_reporter</vh></v>
<v t="ekr.20221004064035.1120"><vh>should_skip_path</vh></v>
<v t="ekr.20221004064035.1121"><vh>iterate_python_lines</vh></v>
<v t="ekr.20221004064035.1122"><vh>class FuncCounterVisitor</vh></v>
<v t="ekr.20221004064035.1123"><vh>class LineCountReporter</vh>
<v t="ekr.20221004064035.1124"><vh>LineCountReporter.__init__</vh></v>
<v t="ekr.20221004064035.1125"><vh>LineCountReporter.on_file</vh></v>
<v t="ekr.20221004064035.1126"><vh>LineCountReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1127"><vh>register_reporter("linecount", LineCountReporter)</vh></v>
<v t="ekr.20221004064035.1128"><vh>class AnyExpressionsReporter</vh>
<v t="ekr.20221004064035.1129"><vh>AnyExpressionsReporter.__init__</vh></v>
<v t="ekr.20221004064035.1130"><vh>AnyExpressionsReporter.on_file</vh></v>
<v t="ekr.20221004064035.1131"><vh>AnyExpressionsReporter.on_finish</vh></v>
<v t="ekr.20221004064035.1132"><vh>AnyExpressionsReporter._write_out_report</vh></v>
<v t="ekr.20221004064035.1133"><vh>AnyExpressionsReporter._report_any_exprs</vh></v>
<v t="ekr.20221004064035.1134"><vh>AnyExpressionsReporter._report_types_of_anys</vh></v>
</v>
<v t="ekr.20221004064035.1135"><vh>register_reporter("any-exprs", AnyExpressionsReporter)</vh></v>
<v t="ekr.20221004064035.1136"><vh>class LineCoverageVisitor</vh>
<v t="ekr.20221004064035.1137"><vh>LineCoverageVisitor.__init__</vh></v>
<v t="ekr.20221004064035.1138"><vh>LineCoverageVisitor.The Python AST has position information for the starts of</vh></v>
<v t="ekr.20221004064035.1139"><vh>LineCoverageVisitor.indentation_level</vh></v>
<v t="ekr.20221004064035.1140"><vh>LineCoverageVisitor.visit_func_def</vh></v>
</v>
<v t="ekr.20221004064035.1141"><vh>class LineCoverageReporter</vh>
<v t="ekr.20221004064035.1142"><vh>LineCoverageReporter.__init__</vh></v>
<v t="ekr.20221004064035.1143"><vh>LineCoverageReporter.on_file</vh></v>
<v t="ekr.20221004064035.1144"><vh>LineCoverageReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1145"><vh>register_reporter("linecoverage", LineCoverageReporter)</vh></v>
<v t="ekr.20221004064035.1146"><vh>class FileInfo</vh>
<v t="ekr.20221004064035.1147"><vh>FileInfo.__init__</vh></v>
<v t="ekr.20221004064035.1148"><vh>FileInfo.total</vh></v>
<v t="ekr.20221004064035.1149"><vh>FileInfo.attrib</vh></v>
</v>
<v t="ekr.20221004064035.1150"><vh>class MemoryXmlReporter</vh>
<v t="ekr.20221004064035.1151"><vh>MemoryXmlReporter.__init__</vh></v>
<v t="ekr.20221004064035.1152"><vh>MemoryXmlReporter.XML doesn't like control characters, but they are sometimes</vh></v>
<v t="ekr.20221004064035.1153"><vh>MemoryXmlReporter.on_file</vh></v>
<v t="ekr.20221004064035.1154"><vh>MemoryXmlReporter._get_any_info_for_line</vh></v>
<v t="ekr.20221004064035.1155"><vh>MemoryXmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1156"><vh>register_reporter("memory-xml", MemoryXmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20221004064035.1157"><vh>get_line_rate</vh></v>
<v t="ekr.20221004064035.1158"><vh>class CoberturaPackage</vh>
<v t="ekr.20221004064035.1159"><vh>CoberturaPackage.__init__</vh></v>
<v t="ekr.20221004064035.1160"><vh>CoberturaPackage.as_xml</vh></v>
<v t="ekr.20221004064035.1161"><vh>CoberturaPackage.add_packages</vh></v>
</v>
<v t="ekr.20221004064035.1162"><vh>class CoberturaXmlReporter</vh>
<v t="ekr.20221004064035.1163"><vh>CoberturaXmlReporter.__init__</vh></v>
<v t="ekr.20221004064035.1164"><vh>CoberturaXmlReporter.on_file</vh></v>
<v t="ekr.20221004064035.1165"><vh>CoberturaXmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1166"><vh>register_reporter("cobertura-xml", CoberturaXmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20221004064035.1167"><vh>class AbstractXmlReporter</vh>
<v t="ekr.20221004064035.1168"><vh>AbstractXmlReporter.__init__</vh></v>
</v>
<v t="ekr.20221004064035.1169"><vh>class XmlReporter</vh>
<v t="ekr.20221004064035.1170"><vh>XmlReporter.on_file</vh></v>
<v t="ekr.20221004064035.1171"><vh>XmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1172"><vh>register_reporter("xml", XmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20221004064035.1173"><vh>class XsltHtmlReporter</vh>
<v t="ekr.20221004064035.1174"><vh>XsltHtmlReporter.__init__</vh></v>
<v t="ekr.20221004064035.1175"><vh>XsltHtmlReporter.on_file</vh></v>
<v t="ekr.20221004064035.1176"><vh>XsltHtmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1177"><vh>register_reporter("xslt-html", XsltHtmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20221004064035.1178"><vh>class XsltTxtReporter</vh>
<v t="ekr.20221004064035.1179"><vh>XsltTxtReporter.__init__</vh></v>
<v t="ekr.20221004064035.1180"><vh>XsltTxtReporter.on_file</vh></v>
<v t="ekr.20221004064035.1181"><vh>XsltTxtReporter.on_finish</vh></v>
</v>
<v t="ekr.20221004064035.1182"><vh>register_reporter("xslt-txt", XsltTxtReporter, needs_lxml=True)</vh></v>
<v t="ekr.20221004064035.1183"><vh>class LinePrecisionReporter</vh>
<v t="ekr.20221004064035.1184"><vh>LinePrecisionReporter.__init__</vh></v>
<v t="ekr.20221004064035.1185"><vh>LinePrecisionReporter.on_file</vh></v>
<v t="ekr.20221004064035.1186"><vh>LinePrecisionReporter.on_finish</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1654"><vh>@clean stats.py</vh>
<v t="ekr.20221004064035.1655"><vh>class StatisticsVisitor</vh>
<v t="ekr.20221004064035.1656"><vh>StatisticsVisitor.__init__</vh></v>
<v t="ekr.20221004064035.1657"><vh>StatisticsVisitor.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.1658"><vh>StatisticsVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064035.1659"><vh>StatisticsVisitor.visit_import_all</vh></v>
<v t="ekr.20221004064035.1660"><vh>StatisticsVisitor.process_import</vh></v>
<v t="ekr.20221004064035.1661"><vh>StatisticsVisitor.visit_import</vh></v>
<v t="ekr.20221004064035.1662"><vh>StatisticsVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064035.1663"><vh>StatisticsVisitor.enter_scope</vh></v>
<v t="ekr.20221004064035.1664"><vh>StatisticsVisitor.is_checked_scope</vh></v>
<v t="ekr.20221004064035.1665"><vh>StatisticsVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064035.1666"><vh>StatisticsVisitor.visit_type_application</vh></v>
<v t="ekr.20221004064035.1667"><vh>StatisticsVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1668"><vh>StatisticsVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.1669"><vh>StatisticsVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20221004064035.1670"><vh>StatisticsVisitor.visit_break_stmt</vh></v>
<v t="ekr.20221004064035.1671"><vh>StatisticsVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20221004064035.1672"><vh>StatisticsVisitor.visit_name_expr</vh></v>
<v t="ekr.20221004064035.1673"><vh>StatisticsVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064035.1674"><vh>StatisticsVisitor.visit_call_expr</vh></v>
<v t="ekr.20221004064035.1675"><vh>StatisticsVisitor.record_call_target_precision</vh></v>
<v t="ekr.20221004064035.1676"><vh>StatisticsVisitor.record_callable_target_precision</vh></v>
<v t="ekr.20221004064035.1677"><vh>StatisticsVisitor.visit_member_expr</vh></v>
<v t="ekr.20221004064035.1678"><vh>StatisticsVisitor.visit_op_expr</vh></v>
<v t="ekr.20221004064035.1679"><vh>StatisticsVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20221004064035.1680"><vh>StatisticsVisitor.visit_index_expr</vh></v>
<v t="ekr.20221004064035.1681"><vh>StatisticsVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.1682"><vh>StatisticsVisitor.visit_unary_expr</vh></v>
<v t="ekr.20221004064035.1683"><vh>StatisticsVisitor.visit_str_expr</vh></v>
<v t="ekr.20221004064035.1684"><vh>StatisticsVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20221004064035.1685"><vh>StatisticsVisitor.visit_int_expr</vh></v>
<v t="ekr.20221004064035.1686"><vh>StatisticsVisitor.visit_float_expr</vh></v>
<v t="ekr.20221004064035.1687"><vh>StatisticsVisitor.visit_complex_expr</vh></v>
<v t="ekr.20221004064035.1688"><vh>StatisticsVisitor.visit_ellipsis</vh></v>
<v t="ekr.20221004064035.1689"><vh>StatisticsVisitor.Helpers</vh></v>
<v t="ekr.20221004064035.1690"><vh>StatisticsVisitor.process_node</vh></v>
<v t="ekr.20221004064035.1691"><vh>StatisticsVisitor.record_precise_if_checked_scope</vh></v>
<v t="ekr.20221004064035.1692"><vh>StatisticsVisitor.type</vh></v>
<v t="ekr.20221004064035.1693"><vh>StatisticsVisitor.log</vh></v>
<v t="ekr.20221004064035.1694"><vh>StatisticsVisitor.record_line</vh></v>
</v>
<v t="ekr.20221004064035.1695"><vh>dump_type_stats</vh></v>
<v t="ekr.20221004064035.1696"><vh>is_special_module</vh></v>
<v t="ekr.20221004064035.1697"><vh>is_imprecise</vh></v>
<v t="ekr.20221004064035.1698"><vh>class HasAnyQuery</vh></v>
<v t="ekr.20221004064035.1699"><vh>is_imprecise2</vh></v>
<v t="ekr.20221004064035.1700"><vh>class HasAnyQuery2</vh></v>
<v t="ekr.20221004064035.1701"><vh>is_generic</vh></v>
<v t="ekr.20221004064035.1702"><vh>is_complex</vh></v>
<v t="ekr.20221004064035.1703"><vh>ensure_dir_exists</vh></v>
<v t="ekr.20221004064035.1704"><vh>is_special_form_any</vh></v>
<v t="ekr.20221004064035.1705"><vh>get_original_any</vh></v>
</v>
<v t="ekr.20221004064035.1706"><vh>@clean strconv.py</vh>
<v t="ekr.20221004064035.1707"><vh>class StrConv</vh>
<v t="ekr.20221004064035.1708"><vh>StrConv.__init__</vh></v>
<v t="ekr.20221004064035.1709"><vh>StrConv.get_id</vh></v>
<v t="ekr.20221004064035.1710"><vh>StrConv.format_id</vh></v>
<v t="ekr.20221004064035.1711"><vh>StrConv.dump</vh></v>
<v t="ekr.20221004064035.1712"><vh>StrConv.func_helper</vh></v>
<v t="ekr.20221004064035.1713"><vh>StrConv.Top-level structures</vh></v>
<v t="ekr.20221004064035.1714"><vh>StrConv.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.1715"><vh>StrConv.visit_import</vh></v>
<v t="ekr.20221004064035.1716"><vh>StrConv.visit_import_from</vh></v>
<v t="ekr.20221004064035.1717"><vh>StrConv.visit_import_all</vh></v>
<v t="ekr.20221004064035.1718"><vh>StrConv.Definitions</vh></v>
<v t="ekr.20221004064035.1719"><vh>StrConv.visit_func_def</vh></v>
<v t="ekr.20221004064035.1720"><vh>StrConv.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064035.1721"><vh>StrConv.visit_class_def</vh></v>
<v t="ekr.20221004064035.1722"><vh>StrConv.visit_var</vh></v>
<v t="ekr.20221004064035.1723"><vh>StrConv.visit_global_decl</vh></v>
<v t="ekr.20221004064035.1724"><vh>StrConv.visit_nonlocal_decl</vh></v>
<v t="ekr.20221004064035.1725"><vh>StrConv.visit_decorator</vh></v>
<v t="ekr.20221004064035.1726"><vh>StrConv.Statements</vh></v>
<v t="ekr.20221004064035.1727"><vh>StrConv.visit_block</vh></v>
<v t="ekr.20221004064035.1728"><vh>StrConv.visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.1729"><vh>StrConv.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1730"><vh>StrConv.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1731"><vh>StrConv.visit_while_stmt</vh></v>
<v t="ekr.20221004064035.1732"><vh>StrConv.visit_for_stmt</vh></v>
<v t="ekr.20221004064035.1733"><vh>StrConv.visit_return_stmt</vh></v>
<v t="ekr.20221004064035.1734"><vh>StrConv.visit_if_stmt</vh></v>
<v t="ekr.20221004064035.1735"><vh>StrConv.visit_break_stmt</vh></v>
<v t="ekr.20221004064035.1736"><vh>StrConv.visit_continue_stmt</vh></v>
<v t="ekr.20221004064035.1737"><vh>StrConv.visit_pass_stmt</vh></v>
<v t="ekr.20221004064035.1738"><vh>StrConv.visit_raise_stmt</vh></v>
<v t="ekr.20221004064035.1739"><vh>StrConv.visit_assert_stmt</vh></v>
<v t="ekr.20221004064035.1740"><vh>StrConv.visit_await_expr</vh></v>
<v t="ekr.20221004064035.1741"><vh>StrConv.visit_del_stmt</vh></v>
<v t="ekr.20221004064035.1742"><vh>StrConv.visit_try_stmt</vh></v>
<v t="ekr.20221004064035.1743"><vh>StrConv.visit_with_stmt</vh></v>
<v t="ekr.20221004064035.1744"><vh>StrConv.visit_match_stmt</vh></v>
<v t="ekr.20221004064035.1745"><vh>StrConv.Expressions</vh></v>
<v t="ekr.20221004064035.1746"><vh>StrConv.visit_int_expr</vh></v>
<v t="ekr.20221004064035.1747"><vh>StrConv.visit_str_expr</vh></v>
<v t="ekr.20221004064035.1748"><vh>StrConv.visit_bytes_expr</vh></v>
<v t="ekr.20221004064035.1749"><vh>StrConv.str_repr</vh></v>
<v t="ekr.20221004064035.1750"><vh>StrConv.visit_float_expr</vh></v>
<v t="ekr.20221004064035.1751"><vh>StrConv.visit_complex_expr</vh></v>
<v t="ekr.20221004064035.1752"><vh>StrConv.visit_ellipsis</vh></v>
<v t="ekr.20221004064035.1753"><vh>StrConv.visit_star_expr</vh></v>
<v t="ekr.20221004064035.1754"><vh>StrConv.visit_name_expr</vh></v>
<v t="ekr.20221004064035.1755"><vh>StrConv.pretty_name</vh></v>
<v t="ekr.20221004064035.1756"><vh>StrConv.visit_member_expr</vh></v>
<v t="ekr.20221004064035.1757"><vh>StrConv.visit_yield_expr</vh></v>
<v t="ekr.20221004064035.1758"><vh>StrConv.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064035.1759"><vh>StrConv.visit_call_expr</vh></v>
<v t="ekr.20221004064035.1760"><vh>StrConv.visit_op_expr</vh></v>
<v t="ekr.20221004064035.1761"><vh>StrConv.visit_comparison_expr</vh></v>
<v t="ekr.20221004064035.1762"><vh>StrConv.visit_cast_expr</vh></v>
<v t="ekr.20221004064035.1763"><vh>StrConv.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064035.1764"><vh>StrConv.visit_reveal_expr</vh></v>
<v t="ekr.20221004064035.1765"><vh>StrConv.visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.1766"><vh>StrConv.visit_unary_expr</vh></v>
<v t="ekr.20221004064035.1767"><vh>StrConv.visit_list_expr</vh></v>
<v t="ekr.20221004064035.1768"><vh>StrConv.visit_dict_expr</vh></v>
<v t="ekr.20221004064035.1769"><vh>StrConv.visit_set_expr</vh></v>
<v t="ekr.20221004064035.1770"><vh>StrConv.visit_tuple_expr</vh></v>
<v t="ekr.20221004064035.1771"><vh>StrConv.visit_index_expr</vh></v>
<v t="ekr.20221004064035.1772"><vh>StrConv.visit_super_expr</vh></v>
<v t="ekr.20221004064035.1773"><vh>StrConv.visit_type_application</vh></v>
<v t="ekr.20221004064035.1774"><vh>StrConv.visit_type_var_expr</vh></v>
<v t="ekr.20221004064035.1775"><vh>StrConv.visit_paramspec_expr</vh></v>
<v t="ekr.20221004064035.1776"><vh>StrConv.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064035.1777"><vh>StrConv.visit_type_alias_expr</vh></v>
<v t="ekr.20221004064035.1778"><vh>StrConv.visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064035.1779"><vh>StrConv.visit_enum_call_expr</vh></v>
<v t="ekr.20221004064035.1780"><vh>StrConv.visit_typeddict_expr</vh></v>
<v t="ekr.20221004064035.1781"><vh>StrConv.visit__promote_expr</vh></v>
<v t="ekr.20221004064035.1782"><vh>StrConv.visit_newtype_expr</vh></v>
<v t="ekr.20221004064035.1783"><vh>StrConv.visit_lambda_expr</vh></v>
<v t="ekr.20221004064035.1784"><vh>StrConv.visit_generator_expr</vh></v>
<v t="ekr.20221004064035.1785"><vh>StrConv.visit_list_comprehension</vh></v>
<v t="ekr.20221004064035.1786"><vh>StrConv.visit_set_comprehension</vh></v>
<v t="ekr.20221004064035.1787"><vh>StrConv.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064035.1788"><vh>StrConv.visit_conditional_expr</vh></v>
<v t="ekr.20221004064035.1789"><vh>StrConv.visit_slice_expr</vh></v>
<v t="ekr.20221004064035.1790"><vh>StrConv.visit_temp_node</vh></v>
<v t="ekr.20221004064035.1791"><vh>StrConv.visit_as_pattern</vh></v>
<v t="ekr.20221004064035.1792"><vh>StrConv.visit_or_pattern</vh></v>
<v t="ekr.20221004064035.1793"><vh>StrConv.visit_value_pattern</vh></v>
<v t="ekr.20221004064035.1794"><vh>StrConv.visit_singleton_pattern</vh></v>
<v t="ekr.20221004064035.1795"><vh>StrConv.visit_sequence_pattern</vh></v>
<v t="ekr.20221004064035.1796"><vh>StrConv.visit_starred_pattern</vh></v>
<v t="ekr.20221004064035.1797"><vh>StrConv.visit_mapping_pattern</vh></v>
<v t="ekr.20221004064035.1798"><vh>StrConv.visit_class_pattern</vh></v>
</v>
<v t="ekr.20221004064035.1799"><vh>dump_tagged</vh></v>
<v t="ekr.20221004064035.1800"><vh>indent</vh></v>
</v>
</v>
<v t="ekr.20221005081400.1"><vh>--- file system &amp; serialization</vh>
<v t="ekr.20221004064034.1308"><vh>@clean find_sources.py</vh>
<v t="ekr.20221004064034.1309"><vh>class InvalidSourceList</vh></v>
<v t="ekr.20221004064034.1310"><vh>create_source_list</vh></v>
<v t="ekr.20221004064034.1311"><vh>keyfunc</vh></v>
<v t="ekr.20221004064034.1312"><vh>normalise_package_base</vh></v>
<v t="ekr.20221004064034.1313"><vh>get_explicit_package_bases</vh></v>
<v t="ekr.20221004064034.1314"><vh>class SourceFinder</vh>
<v t="ekr.20221004064034.1315"><vh>SourceFinder.__init__</vh></v>
<v t="ekr.20221004064034.1316"><vh>SourceFinder.is_explicit_package_base</vh></v>
<v t="ekr.20221004064034.1317"><vh>SourceFinder.find_sources_in_dir</vh></v>
<v t="ekr.20221004064034.1318"><vh>SourceFinder.crawl_up</vh></v>
<v t="ekr.20221004064034.1319"><vh>SourceFinder.crawl_up_dir</vh></v>
<v t="ekr.20221004064034.1320"><vh>SourceFinder._crawl_up_helper</vh></v>
<v t="ekr.20221004064034.1321"><vh>SourceFinder.get_init_file</vh></v>
</v>
<v t="ekr.20221004064034.1322"><vh>module_join</vh></v>
<v t="ekr.20221004064034.1323"><vh>strip_py</vh></v>
</v>
<v t="ekr.20221004064034.1324"><vh>@clean fixup.py</vh>
<v t="ekr.20221004064034.1325"><vh>fixup_module</vh></v>
<v t="ekr.20221004064034.1326"><vh>class NodeFixer</vh>
<v t="ekr.20221004064034.1327"><vh>NodeFixer.__init__</vh></v>
<v t="ekr.20221004064034.1328"><vh>NodeFixer.visit_type_info</vh></v>
<v t="ekr.20221004064034.1329"><vh>NodeFixer.visit_symbol_table</vh></v>
<v t="ekr.20221004064034.1330"><vh>NodeFixer.visit_func_def</vh></v>
<v t="ekr.20221004064034.1331"><vh>NodeFixer.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064034.1332"><vh>NodeFixer.visit_decorator</vh></v>
<v t="ekr.20221004064034.1333"><vh>NodeFixer.visit_class_def</vh></v>
<v t="ekr.20221004064034.1334"><vh>NodeFixer.visit_type_var_expr</vh></v>
<v t="ekr.20221004064034.1335"><vh>NodeFixer.visit_paramspec_expr</vh></v>
<v t="ekr.20221004064034.1336"><vh>NodeFixer.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064034.1337"><vh>NodeFixer.visit_var</vh></v>
<v t="ekr.20221004064034.1338"><vh>NodeFixer.visit_type_alias</vh></v>
</v>
<v t="ekr.20221004064034.1339"><vh>class TypeFixer</vh>
<v t="ekr.20221004064034.1340"><vh>TypeFixer.__init__</vh></v>
<v t="ekr.20221004064034.1341"><vh>TypeFixer.visit_instance</vh></v>
<v t="ekr.20221004064034.1342"><vh>TypeFixer.visit_type_alias_type</vh></v>
<v t="ekr.20221004064034.1343"><vh>TypeFixer.visit_any</vh></v>
<v t="ekr.20221004064034.1344"><vh>TypeFixer.visit_callable_type</vh></v>
<v t="ekr.20221004064034.1345"><vh>TypeFixer.visit_overloaded</vh></v>
<v t="ekr.20221004064034.1346"><vh>TypeFixer.visit_erased_type</vh></v>
<v t="ekr.20221004064034.1347"><vh>TypeFixer.visit_deleted_type</vh></v>
<v t="ekr.20221004064034.1348"><vh>TypeFixer.visit_none_type</vh></v>
<v t="ekr.20221004064034.1349"><vh>TypeFixer.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064034.1350"><vh>TypeFixer.visit_partial_type</vh></v>
<v t="ekr.20221004064034.1351"><vh>TypeFixer.visit_tuple_type</vh></v>
<v t="ekr.20221004064034.1352"><vh>TypeFixer.visit_typeddict_type</vh></v>
<v t="ekr.20221004064034.1353"><vh>TypeFixer.visit_literal_type</vh></v>
<v t="ekr.20221004064034.1354"><vh>TypeFixer.visit_type_var</vh></v>
<v t="ekr.20221004064034.1355"><vh>TypeFixer.visit_param_spec</vh></v>
<v t="ekr.20221004064034.1356"><vh>TypeFixer.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064034.1357"><vh>TypeFixer.visit_unpack_type</vh></v>
<v t="ekr.20221004064034.1358"><vh>TypeFixer.visit_parameters</vh></v>
<v t="ekr.20221004064034.1359"><vh>TypeFixer.visit_unbound_type</vh></v>
<v t="ekr.20221004064034.1360"><vh>TypeFixer.visit_union_type</vh></v>
<v t="ekr.20221004064034.1361"><vh>TypeFixer.visit_void</vh></v>
<v t="ekr.20221004064034.1362"><vh>TypeFixer.visit_type_type</vh></v>
</v>
<v t="ekr.20221004064034.1363"><vh>lookup_fully_qualified_typeinfo</vh></v>
<v t="ekr.20221004064034.1364"><vh>lookup_fully_qualified_alias</vh></v>
<v t="ekr.20221004064034.1365"><vh>_SUGGESTION: Final = "&lt;missing {}: *should* have gone away during fine-grained update&gt;"</vh></v>
<v t="ekr.20221004064034.1366"><vh>missing_info</vh></v>
<v t="ekr.20221004064034.1367"><vh>missing_alias</vh></v>
</v>
<v t="ekr.20221004064034.1371"><vh>@clean fscache.py</vh>
<v t="ekr.20221004064034.1372"><vh>class FileSystemCache</vh>
<v t="ekr.20221004064034.1373"><vh>FileSystemCache.__init__</vh></v>
<v t="ekr.20221004064034.1374"><vh>FileSystemCache.set_package_root</vh></v>
<v t="ekr.20221004064034.1375"><vh>FileSystemCache.flush</vh></v>
<v t="ekr.20221004064034.1376"><vh>FileSystemCache.stat</vh></v>
<v t="ekr.20221004064034.1377"><vh>FileSystemCache.init_under_package_root</vh></v>
<v t="ekr.20221004064034.1378"><vh>FileSystemCache._fake_init</vh></v>
<v t="ekr.20221004064034.1379"><vh>FileSystemCache.listdir</vh></v>
<v t="ekr.20221004064034.1380"><vh>FileSystemCache.isfile</vh></v>
<v t="ekr.20221004064034.1381"><vh>FileSystemCache.isfile_case</vh></v>
<v t="ekr.20221004064034.1382"><vh>FileSystemCache.exists_case</vh></v>
<v t="ekr.20221004064034.1383"><vh>FileSystemCache.isdir</vh></v>
<v t="ekr.20221004064034.1384"><vh>FileSystemCache.exists</vh></v>
<v t="ekr.20221004064034.1385"><vh>FileSystemCache.read</vh></v>
<v t="ekr.20221004064034.1386"><vh>FileSystemCache.hash_digest</vh></v>
<v t="ekr.20221004064034.1387"><vh>FileSystemCache.samefile</vh></v>
</v>
<v t="ekr.20221004064034.1388"><vh>copy_os_error</vh></v>
</v>
<v t="ekr.20221004064034.1389"><vh>@clean fswatcher.py</vh>
<v t="ekr.20221004064034.1390"><vh>class FileData</vh></v>
<v t="ekr.20221004064034.1391"><vh>class FileSystemWatcher</vh>
<v t="ekr.20221004064034.1392"><vh>FileSystemWatcher.__init__</vh></v>
<v t="ekr.20221004064034.1393"><vh>FileSystemWatcher.dump_file_data</vh></v>
<v t="ekr.20221004064034.1394"><vh>FileSystemWatcher.set_file_data</vh></v>
<v t="ekr.20221004064034.1395"><vh>FileSystemWatcher.add_watched_paths</vh></v>
<v t="ekr.20221004064034.1396"><vh>FileSystemWatcher.remove_watched_paths</vh></v>
<v t="ekr.20221004064034.1397"><vh>FileSystemWatcher._update</vh></v>
<v t="ekr.20221004064034.1398"><vh>FileSystemWatcher._find_changed</vh></v>
<v t="ekr.20221004064034.1399"><vh>FileSystemWatcher.find_changed</vh></v>
<v t="ekr.20221004064034.1400"><vh>FileSystemWatcher.update_changed</vh></v>
</v>
</v>
<v t="ekr.20221004064035.379"><vh>@clean metastore.py</vh>
<v t="ekr.20221004064035.380"><vh>class MetadataStore</vh>
<v t="ekr.20221004064035.381"><vh>MetadataStore.getmtime</vh></v>
<v t="ekr.20221004064035.382"><vh>MetadataStore.read</vh></v>
<v t="ekr.20221004064035.383"><vh>MetadataStore.write</vh></v>
<v t="ekr.20221004064035.384"><vh>MetadataStore.remove</vh></v>
<v t="ekr.20221004064035.385"><vh>MetadataStore.commit</vh></v>
<v t="ekr.20221004064035.386"><vh>MetadataStore.list_all</vh></v>
</v>
<v t="ekr.20221004064035.387"><vh>random_string</vh></v>
<v t="ekr.20221004064035.388"><vh>class FilesystemMetadataStore</vh>
<v t="ekr.20221004064035.389"><vh>FilesystemMetadataStore.__init__</vh></v>
<v t="ekr.20221004064035.390"><vh>FilesystemMetadataStore.getmtime</vh></v>
<v t="ekr.20221004064035.391"><vh>FilesystemMetadataStore.read</vh></v>
<v t="ekr.20221004064035.392"><vh>FilesystemMetadataStore.write</vh></v>
<v t="ekr.20221004064035.393"><vh>FilesystemMetadataStore.remove</vh></v>
<v t="ekr.20221004064035.394"><vh>FilesystemMetadataStore.commit</vh></v>
<v t="ekr.20221004064035.395"><vh>FilesystemMetadataStore.list_all</vh></v>
</v>
<v t="ekr.20221004064035.396"><vh>SCHEMA = """</vh></v>
<v t="ekr.20221004064035.397"><vh>connect_db</vh></v>
<v t="ekr.20221004064035.398"><vh>class SqliteMetadataStore</vh>
<v t="ekr.20221004064035.399"><vh>SqliteMetadataStore.__init__</vh></v>
<v t="ekr.20221004064035.400"><vh>SqliteMetadataStore._query</vh></v>
<v t="ekr.20221004064035.401"><vh>SqliteMetadataStore.getmtime</vh></v>
<v t="ekr.20221004064035.402"><vh>SqliteMetadataStore.read</vh></v>
<v t="ekr.20221004064035.403"><vh>SqliteMetadataStore.write</vh></v>
<v t="ekr.20221004064035.404"><vh>SqliteMetadataStore.remove</vh></v>
<v t="ekr.20221004064035.405"><vh>SqliteMetadataStore.commit</vh></v>
<v t="ekr.20221004064035.406"><vh>SqliteMetadataStore.list_all</vh></v>
</v>
</v>
<v t="ekr.20221004064035.428"><vh>@clean modulefinder.py</vh>
<v t="ekr.20221004064035.429"><vh>class SearchPaths</vh></v>
<v t="ekr.20221004064035.430"><vh>Package dirs are a two-tuple of path to search and whether to verify the module</vh></v>
<v t="ekr.20221004064035.431"><vh>class ModuleNotFoundReason</vh>
<v t="ekr.20221004064035.432"><vh>ModuleNotFoundReason.error_message_templates</vh></v>
</v>
<v t="ekr.20221004064035.433"><vh>If we found the module, returns the path to the module as a str.</vh></v>
<v t="ekr.20221004064035.434"><vh>class BuildSource</vh>
<v t="ekr.20221004064035.435"><vh>BuildSource.__init__</vh></v>
<v t="ekr.20221004064035.436"><vh>BuildSource.__repr__</vh></v>
</v>
<v t="ekr.20221004064035.437"><vh>class BuildSourceSet</vh>
<v t="ekr.20221004064035.438"><vh>BuildSourceSet.__init__</vh></v>
<v t="ekr.20221004064035.439"><vh>BuildSourceSet.is_source</vh></v>
</v>
<v t="ekr.20221004064035.440"><vh>class FindModuleCache</vh>
<v t="ekr.20221004064035.441"><vh>FindModuleCache.__init__</vh></v>
<v t="ekr.20221004064035.442"><vh>FindModuleCache.clear</vh></v>
<v t="ekr.20221004064035.443"><vh>FindModuleCache.find_module_via_source_set</vh></v>
<v t="ekr.20221004064035.444"><vh>FindModuleCache.find_lib_path_dirs</vh></v>
<v t="ekr.20221004064035.445"><vh>FindModuleCache.get_toplevel_possibilities</vh></v>
<v t="ekr.20221004064035.446"><vh>FindModuleCache.find_module</vh></v>
<v t="ekr.20221004064035.447"><vh>FindModuleCache._typeshed_has_version</vh></v>
<v t="ekr.20221004064035.448"><vh>FindModuleCache._find_module_non_stub_helper</vh></v>
<v t="ekr.20221004064035.449"><vh>FindModuleCache._update_ns_ancestors</vh></v>
<v t="ekr.20221004064035.450"><vh>FindModuleCache._can_find_module_in_parent_dir</vh></v>
<v t="ekr.20221004064035.451"><vh>FindModuleCache._find_module</vh></v>
<v t="ekr.20221004064035.452"><vh>FindModuleCache._is_compatible_stub_package</vh></v>
<v t="ekr.20221004064035.453"><vh>FindModuleCache.find_modules_recursive</vh></v>
</v>
<v t="ekr.20221004064035.454"><vh>matches_exclude</vh></v>
<v t="ekr.20221004064035.455"><vh>is_init_file</vh></v>
<v t="ekr.20221004064035.456"><vh>verify_module</vh></v>
<v t="ekr.20221004064035.457"><vh>highest_init_level</vh></v>
<v t="ekr.20221004064035.458"><vh>mypy_path</vh></v>
<v t="ekr.20221004064035.459"><vh>default_lib_path</vh></v>
<v t="ekr.20221004064035.460"><vh>get_search_dirs</vh></v>
<v t="ekr.20221004064035.461"><vh>compute_search_paths</vh></v>
<v t="ekr.20221004064035.462"><vh>load_stdlib_py_versions</vh></v>
<v t="ekr.20221004064035.463"><vh>parse_version</vh></v>
<v t="ekr.20221004064035.464"><vh>typeshed_py_version</vh></v>
</v>
<v t="ekr.20221004064035.465"><vh>@clean moduleinspect.py</vh>
<v t="ekr.20221004064035.466"><vh>class ModuleProperties</vh>
<v t="ekr.20221004064035.467"><vh>ModuleProperties.__init__</vh></v>
</v>
<v t="ekr.20221004064035.468"><vh>is_c_module</vh></v>
<v t="ekr.20221004064035.469"><vh>class InspectError</vh></v>
<v t="ekr.20221004064035.470"><vh>get_package_properties</vh></v>
<v t="ekr.20221004064035.471"><vh>worker</vh></v>
<v t="ekr.20221004064035.472"><vh>class ModuleInspect</vh>
<v t="ekr.20221004064035.473"><vh>ModuleInspect.__init__</vh></v>
<v t="ekr.20221004064035.474"><vh>ModuleInspect._start</vh></v>
<v t="ekr.20221004064035.475"><vh>ModuleInspect.close</vh></v>
<v t="ekr.20221004064035.476"><vh>ModuleInspect.get_package_properties</vh></v>
<v t="ekr.20221004064035.477"><vh>ModuleInspect._get_from_queue</vh></v>
<v t="ekr.20221004064035.478"><vh>ModuleInspect.__enter__</vh></v>
<v t="ekr.20221004064035.479"><vh>ModuleInspect.__exit__</vh></v>
</v>
</v>
</v>
<v t="ekr.20221005080738.1"><vh>--- semantic analysis</vh>
<v t="ekr.20221004064034.164"><vh>@clean binder.py</vh>
<v t="ekr.20221004064034.165"><vh>class Frame</vh>
<v t="ekr.20221004064034.166"><vh>Frame.__init__</vh></v>
</v>
<v t="ekr.20221004064034.167"><vh>Assigns = DefaultDict[Expression, List[Tuple[Type, Optional[Type]]]]</vh></v>
<v t="ekr.20221004064034.168"><vh>class ConditionalTypeBinder</vh>
<v t="ekr.20221004064034.169"><vh>ConditionalTypeBinder.__init__</vh></v>
<v t="ekr.20221004064034.170"><vh>ConditionalTypeBinder._get_id</vh></v>
<v t="ekr.20221004064034.171"><vh>ConditionalTypeBinder._add_dependencies</vh></v>
<v t="ekr.20221004064034.172"><vh>ConditionalTypeBinder.push_frame</vh></v>
<v t="ekr.20221004064034.173"><vh>ConditionalTypeBinder._put</vh></v>
<v t="ekr.20221004064034.174"><vh>ConditionalTypeBinder._get</vh></v>
<v t="ekr.20221004064034.175"><vh>ConditionalTypeBinder.put</vh></v>
<v t="ekr.20221004064034.176"><vh>ConditionalTypeBinder.unreachable</vh></v>
<v t="ekr.20221004064034.177"><vh>ConditionalTypeBinder.suppress_unreachable_warnings</vh></v>
<v t="ekr.20221004064034.178"><vh>ConditionalTypeBinder.get</vh></v>
<v t="ekr.20221004064034.179"><vh>ConditionalTypeBinder.is_unreachable</vh></v>
<v t="ekr.20221004064034.180"><vh>ConditionalTypeBinder.is_unreachable_warning_suppressed</vh></v>
<v t="ekr.20221004064034.181"><vh>ConditionalTypeBinder.cleanse</vh></v>
<v t="ekr.20221004064034.182"><vh>ConditionalTypeBinder._cleanse_key</vh></v>
<v t="ekr.20221004064034.183"><vh>ConditionalTypeBinder.update_from_options</vh></v>
<v t="ekr.20221004064034.184"><vh>ConditionalTypeBinder.pop_frame</vh></v>
<v t="ekr.20221004064034.185"><vh>ConditionalTypeBinder.accumulate_type_assignments</vh></v>
<v t="ekr.20221004064034.186"><vh>ConditionalTypeBinder.assign_type</vh></v>
<v t="ekr.20221004064034.187"><vh>ConditionalTypeBinder.invalidate_dependencies</vh></v>
<v t="ekr.20221004064034.188"><vh>ConditionalTypeBinder.most_recent_enclosing_type</vh></v>
<v t="ekr.20221004064034.189"><vh>ConditionalTypeBinder.allow_jump</vh></v>
<v t="ekr.20221004064034.190"><vh>ConditionalTypeBinder.handle_break</vh></v>
<v t="ekr.20221004064034.191"><vh>ConditionalTypeBinder.handle_continue</vh></v>
<v t="ekr.20221004064034.192"><vh>ConditionalTypeBinder.frame_context</vh></v>
<v t="ekr.20221004064034.193"><vh>ConditionalTypeBinder.top_frame_context</vh></v>
</v>
<v t="ekr.20221004064034.194"><vh>get_declaration</vh></v>
</v>
<v t="ekr.20221004064035.1031"><vh>@clean reachability.py</vh>
<v t="ekr.20221004064035.1032"><vh>infer_reachability_of_if_statement</vh></v>
<v t="ekr.20221004064035.1033"><vh>infer_reachability_of_match_statement</vh></v>
<v t="ekr.20221004064035.1034"><vh>assert_will_always_fail</vh></v>
<v t="ekr.20221004064035.1035"><vh>infer_condition_value</vh></v>
<v t="ekr.20221004064035.1036"><vh>infer_pattern_value</vh></v>
<v t="ekr.20221004064035.1037"><vh>consider_sys_version_info</vh></v>
<v t="ekr.20221004064035.1038"><vh>consider_sys_platform</vh></v>
<v t="ekr.20221004064035.1039"><vh>Targ = TypeVar("Targ", int, str, Tuple[int, ...])</vh></v>
<v t="ekr.20221004064035.1040"><vh>fixed_comparison</vh></v>
<v t="ekr.20221004064035.1041"><vh>contains_int_or_tuple_of_ints</vh></v>
<v t="ekr.20221004064035.1042"><vh>contains_sys_version_info</vh></v>
<v t="ekr.20221004064035.1043"><vh>is_sys_attr</vh></v>
<v t="ekr.20221004064035.1044"><vh>mark_block_unreachable</vh></v>
<v t="ekr.20221004064035.1045"><vh>class MarkImportsUnreachableVisitor</vh>
<v t="ekr.20221004064035.1046"><vh>MarkImportsUnreachableVisitor.visit_import</vh></v>
<v t="ekr.20221004064035.1047"><vh>MarkImportsUnreachableVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064035.1048"><vh>MarkImportsUnreachableVisitor.visit_import_all</vh></v>
</v>
<v t="ekr.20221004064035.1049"><vh>mark_block_mypy_only</vh></v>
<v t="ekr.20221004064035.1050"><vh>class MarkImportsMypyOnlyVisitor</vh>
<v t="ekr.20221004064035.1051"><vh>MarkImportsMypyOnlyVisitor.visit_import</vh></v>
<v t="ekr.20221004064035.1052"><vh>MarkImportsMypyOnlyVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064035.1053"><vh>MarkImportsMypyOnlyVisitor.visit_import_all</vh></v>
<v t="ekr.20221004064035.1054"><vh>MarkImportsMypyOnlyVisitor.visit_func_def</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1055"><vh>@clean renaming.py</vh>
<v t="ekr.20221004064035.1056"><vh>class VariableRenameVisitor</vh>
<v t="ekr.20221004064035.1057"><vh>VariableRenameVisitor.__init__</vh></v>
<v t="ekr.20221004064035.1058"><vh>VariableRenameVisitor.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.1059"><vh>VariableRenameVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064035.1060"><vh>VariableRenameVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064035.1061"><vh>VariableRenameVisitor.visit_block</vh></v>
<v t="ekr.20221004064035.1062"><vh>VariableRenameVisitor.visit_while_stmt</vh></v>
<v t="ekr.20221004064035.1063"><vh>VariableRenameVisitor.visit_for_stmt</vh></v>
<v t="ekr.20221004064035.1064"><vh>VariableRenameVisitor.visit_break_stmt</vh></v>
<v t="ekr.20221004064035.1065"><vh>VariableRenameVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20221004064035.1066"><vh>VariableRenameVisitor.visit_try_stmt</vh></v>
<v t="ekr.20221004064035.1067"><vh>VariableRenameVisitor.visit_with_stmt</vh></v>
<v t="ekr.20221004064035.1068"><vh>VariableRenameVisitor.visit_import</vh></v>
<v t="ekr.20221004064035.1069"><vh>VariableRenameVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064035.1070"><vh>VariableRenameVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1071"><vh>VariableRenameVisitor.visit_match_stmt</vh></v>
<v t="ekr.20221004064035.1072"><vh>VariableRenameVisitor.visit_capture_pattern</vh></v>
<v t="ekr.20221004064035.1073"><vh>VariableRenameVisitor.analyze_lvalue</vh></v>
<v t="ekr.20221004064035.1074"><vh>VariableRenameVisitor.visit_name_expr</vh></v>
<v t="ekr.20221004064035.1075"><vh>VariableRenameVisitor.Helpers for renaming references</vh></v>
<v t="ekr.20221004064035.1076"><vh>VariableRenameVisitor.handle_arg</vh></v>
<v t="ekr.20221004064035.1077"><vh>VariableRenameVisitor.handle_def</vh></v>
<v t="ekr.20221004064035.1078"><vh>VariableRenameVisitor.handle_refine</vh></v>
<v t="ekr.20221004064035.1079"><vh>VariableRenameVisitor.handle_ref</vh></v>
<v t="ekr.20221004064035.1080"><vh>VariableRenameVisitor.flush_refs</vh></v>
<v t="ekr.20221004064035.1081"><vh>VariableRenameVisitor.Helpers for determining which assignments define new variables</vh></v>
<v t="ekr.20221004064035.1082"><vh>VariableRenameVisitor.clear</vh></v>
<v t="ekr.20221004064035.1083"><vh>VariableRenameVisitor.enter_block</vh></v>
<v t="ekr.20221004064035.1084"><vh>VariableRenameVisitor.enter_try</vh></v>
<v t="ekr.20221004064035.1085"><vh>VariableRenameVisitor.enter_loop</vh></v>
<v t="ekr.20221004064035.1086"><vh>VariableRenameVisitor.current_block</vh></v>
<v t="ekr.20221004064035.1087"><vh>VariableRenameVisitor.enter_scope</vh></v>
<v t="ekr.20221004064035.1088"><vh>VariableRenameVisitor.is_nested</vh></v>
<v t="ekr.20221004064035.1089"><vh>VariableRenameVisitor.reject_redefinition_of_vars_in_scope</vh></v>
<v t="ekr.20221004064035.1090"><vh>VariableRenameVisitor.reject_redefinition_of_vars_in_loop</vh></v>
<v t="ekr.20221004064035.1091"><vh>VariableRenameVisitor.record_assignment</vh></v>
</v>
<v t="ekr.20221004064035.1092"><vh>class LimitedVariableRenameVisitor</vh>
<v t="ekr.20221004064035.1093"><vh>LimitedVariableRenameVisitor.__init__</vh></v>
<v t="ekr.20221004064035.1094"><vh>LimitedVariableRenameVisitor.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.1095"><vh>LimitedVariableRenameVisitor.visit_func_def</vh></v>
<v t="ekr.20221004064035.1096"><vh>LimitedVariableRenameVisitor.visit_class_def</vh></v>
<v t="ekr.20221004064035.1097"><vh>LimitedVariableRenameVisitor.visit_with_stmt</vh></v>
<v t="ekr.20221004064035.1098"><vh>LimitedVariableRenameVisitor.analyze_lvalue</vh></v>
<v t="ekr.20221004064035.1099"><vh>LimitedVariableRenameVisitor.visit_import</vh></v>
<v t="ekr.20221004064035.1100"><vh>LimitedVariableRenameVisitor.visit_import_from</vh></v>
<v t="ekr.20221004064035.1101"><vh>LimitedVariableRenameVisitor.visit_import_all</vh></v>
<v t="ekr.20221004064035.1102"><vh>LimitedVariableRenameVisitor.visit_name_expr</vh></v>
<v t="ekr.20221004064035.1103"><vh>LimitedVariableRenameVisitor.enter_scope</vh></v>
<v t="ekr.20221004064035.1104"><vh>LimitedVariableRenameVisitor.reject_redefinition_of_vars_in_scope</vh></v>
<v t="ekr.20221004064035.1105"><vh>LimitedVariableRenameVisitor.record_skipped</vh></v>
<v t="ekr.20221004064035.1106"><vh>LimitedVariableRenameVisitor.flush_refs</vh></v>
</v>
<v t="ekr.20221004064035.1107"><vh>rename_refs</vh></v>
</v>
<v t="ekr.20221004064035.1187"><vh>@clean scope.py</vh>
<v t="ekr.20221004064035.1188"><vh>class Scope</vh>
<v t="ekr.20221004064035.1189"><vh>Scope.__init__</vh></v>
<v t="ekr.20221004064035.1190"><vh>Scope.current_module_id</vh></v>
<v t="ekr.20221004064035.1191"><vh>Scope.current_target</vh></v>
<v t="ekr.20221004064035.1192"><vh>Scope.current_full_target</vh></v>
<v t="ekr.20221004064035.1193"><vh>Scope.current_type_name</vh></v>
<v t="ekr.20221004064035.1194"><vh>Scope.current_function_name</vh></v>
<v t="ekr.20221004064035.1195"><vh>Scope.module_scope</vh></v>
<v t="ekr.20221004064035.1196"><vh>Scope.function_scope</vh></v>
<v t="ekr.20221004064035.1197"><vh>Scope.enter_class</vh></v>
<v t="ekr.20221004064035.1198"><vh>Scope.leave_class</vh></v>
<v t="ekr.20221004064035.1199"><vh>Scope.class_scope</vh></v>
<v t="ekr.20221004064035.1200"><vh>Scope.save</vh></v>
<v t="ekr.20221004064035.1201"><vh>Scope.saved_scope</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1202"><vh>@clean semanal.py</vh>
<v t="ekr.20221004064035.1203"><vh>class SemanticAnalyzer</vh>
<v t="ekr.20221004064035.1204"><vh>SemanticAnalyzer.__init__</vh></v>
<v t="ekr.20221004064035.1205"><vh>SemanticAnalyzer.is_stub_file</vh></v>
<v t="ekr.20221004064035.1206"><vh>SemanticAnalyzer.is_typeshed_stub_file</vh></v>
<v t="ekr.20221004064035.1207"><vh>SemanticAnalyzer.final_iteration</vh></v>
<v t="ekr.20221004064035.1208"><vh>SemanticAnalyzer.Preparing module (performed before semantic analysis)</vh></v>
<v t="ekr.20221004064035.1209"><vh>SemanticAnalyzer.prepare_file</vh></v>
<v t="ekr.20221004064035.1210"><vh>SemanticAnalyzer.prepare_typing_namespace</vh></v>
<v t="ekr.20221004064035.1211"><vh>SemanticAnalyzer.prepare_builtins_namespace</vh></v>
<v t="ekr.20221004064035.1212"><vh>SemanticAnalyzer.Analyzing a target</vh></v>
<v t="ekr.20221004064035.1213"><vh>SemanticAnalyzer.refresh_partial</vh></v>
<v t="ekr.20221004064035.1214"><vh>SemanticAnalyzer.refresh_top_level</vh></v>
<v t="ekr.20221004064035.1215"><vh>SemanticAnalyzer.add_implicit_module_attrs</vh></v>
<v t="ekr.20221004064035.1216"><vh>SemanticAnalyzer.add_builtin_aliases</vh></v>
<v t="ekr.20221004064035.1217"><vh>SemanticAnalyzer.add_typing_extension_aliases</vh></v>
<v t="ekr.20221004064035.1218"><vh>SemanticAnalyzer.create_alias</vh></v>
<v t="ekr.20221004064035.1219"><vh>SemanticAnalyzer.adjust_public_exports</vh></v>
<v t="ekr.20221004064035.1220"><vh>SemanticAnalyzer.file_context</vh></v>
<v t="ekr.20221004064035.1221"><vh>SemanticAnalyzer.Functions</vh></v>
<v t="ekr.20221004064035.1222"><vh>SemanticAnalyzer.visit_func_def</vh></v>
<v t="ekr.20221004064035.1223"><vh>SemanticAnalyzer.analyze_func_def</vh></v>
<v t="ekr.20221004064035.1224"><vh>SemanticAnalyzer.remove_unpack_kwargs</vh></v>
<v t="ekr.20221004064035.1225"><vh>SemanticAnalyzer.prepare_method_signature</vh></v>
<v t="ekr.20221004064035.1226"><vh>SemanticAnalyzer.set_original_def</vh></v>
<v t="ekr.20221004064035.1227"><vh>SemanticAnalyzer.update_function_type_variables</vh></v>
<v t="ekr.20221004064035.1228"><vh>SemanticAnalyzer.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064035.1229"><vh>SemanticAnalyzer.analyze_overloaded_func_def</vh></v>
<v t="ekr.20221004064035.1230"><vh>SemanticAnalyzer.process_overload_impl</vh></v>
<v t="ekr.20221004064035.1231"><vh>SemanticAnalyzer.analyze_overload_sigs_and_impl</vh></v>
<v t="ekr.20221004064035.1232"><vh>SemanticAnalyzer.handle_missing_overload_decorators</vh></v>
<v t="ekr.20221004064035.1233"><vh>SemanticAnalyzer.handle_missing_overload_implementation</vh></v>
<v t="ekr.20221004064035.1234"><vh>SemanticAnalyzer.process_final_in_overload</vh></v>
<v t="ekr.20221004064035.1235"><vh>SemanticAnalyzer.process_static_or_class_method_in_overload</vh></v>
<v t="ekr.20221004064035.1236"><vh>SemanticAnalyzer.analyze_property_with_multi_part_definition</vh></v>
<v t="ekr.20221004064035.1237"><vh>SemanticAnalyzer.add_function_to_symbol_table</vh></v>
<v t="ekr.20221004064035.1238"><vh>SemanticAnalyzer.analyze_arg_initializers</vh></v>
<v t="ekr.20221004064035.1239"><vh>SemanticAnalyzer.analyze_function_body</vh></v>
<v t="ekr.20221004064035.1240"><vh>SemanticAnalyzer.check_classvar_in_signature</vh></v>
<v t="ekr.20221004064035.1241"><vh>SemanticAnalyzer.check_function_signature</vh></v>
<v t="ekr.20221004064035.1242"><vh>SemanticAnalyzer.visit_decorator</vh></v>
<v t="ekr.20221004064035.1243"><vh>SemanticAnalyzer.check_decorated_function_is_method</vh></v>
<v t="ekr.20221004064035.1244"><vh>SemanticAnalyzer.Classes</vh></v>
<v t="ekr.20221004064035.1245"><vh>SemanticAnalyzer.visit_class_def</vh></v>
<v t="ekr.20221004064035.1246"><vh>SemanticAnalyzer.analyze_class</vh></v>
<v t="ekr.20221004064035.1247"><vh>SemanticAnalyzer.setup_type_vars</vh></v>
<v t="ekr.20221004064035.1248"><vh>SemanticAnalyzer.setup_alias_type_vars</vh></v>
<v t="ekr.20221004064035.1249"><vh>SemanticAnalyzer.is_core_builtin_class</vh></v>
<v t="ekr.20221004064035.1250"><vh>SemanticAnalyzer.analyze_class_body_common</vh></v>
<v t="ekr.20221004064035.1251"><vh>SemanticAnalyzer.analyze_typeddict_classdef</vh></v>
<v t="ekr.20221004064035.1252"><vh>SemanticAnalyzer.analyze_namedtuple_classdef</vh></v>
<v t="ekr.20221004064035.1253"><vh>SemanticAnalyzer.apply_class_plugin_hooks</vh></v>
<v t="ekr.20221004064035.1254"><vh>SemanticAnalyzer.get_fullname_for_hook</vh></v>
<v t="ekr.20221004064035.1255"><vh>SemanticAnalyzer.analyze_class_keywords</vh></v>
<v t="ekr.20221004064035.1256"><vh>SemanticAnalyzer.enter_class</vh></v>
<v t="ekr.20221004064035.1257"><vh>SemanticAnalyzer.leave_class</vh></v>
<v t="ekr.20221004064035.1258"><vh>SemanticAnalyzer.analyze_class_decorator</vh></v>
<v t="ekr.20221004064035.1259"><vh>SemanticAnalyzer.clean_up_bases_and_infer_type_variables</vh></v>
<v t="ekr.20221004064035.1260"><vh>SemanticAnalyzer.analyze_class_typevar_declaration</vh></v>
<v t="ekr.20221004064035.1261"><vh>SemanticAnalyzer.analyze_unbound_tvar</vh></v>
<v t="ekr.20221004064035.1262"><vh>SemanticAnalyzer.get_all_bases_tvars</vh></v>
<v t="ekr.20221004064035.1263"><vh>SemanticAnalyzer.get_and_bind_all_tvars</vh></v>
<v t="ekr.20221004064035.1264"><vh>SemanticAnalyzer.prepare_class_def</vh></v>
<v t="ekr.20221004064035.1265"><vh>SemanticAnalyzer.make_empty_type_info</vh></v>
<v t="ekr.20221004064035.1266"><vh>SemanticAnalyzer.get_name_repr_of_expr</vh></v>
<v t="ekr.20221004064035.1267"><vh>SemanticAnalyzer.analyze_base_classes</vh></v>
<v t="ekr.20221004064035.1268"><vh>SemanticAnalyzer.configure_base_classes</vh></v>
<v t="ekr.20221004064035.1269"><vh>SemanticAnalyzer.configure_tuple_base_class</vh></v>
<v t="ekr.20221004064035.1270"><vh>SemanticAnalyzer.set_dummy_mro</vh></v>
<v t="ekr.20221004064035.1271"><vh>SemanticAnalyzer.calculate_class_mro</vh></v>
<v t="ekr.20221004064035.1272"><vh>SemanticAnalyzer.infer_metaclass_and_bases_from_compat_helpers</vh></v>
<v t="ekr.20221004064035.1273"><vh>SemanticAnalyzer.verify_base_classes</vh></v>
<v t="ekr.20221004064035.1274"><vh>SemanticAnalyzer.is_base_class</vh></v>
<v t="ekr.20221004064035.1275"><vh>SemanticAnalyzer.get_declared_metaclass</vh></v>
<v t="ekr.20221004064035.1276"><vh>SemanticAnalyzer.recalculate_metaclass</vh></v>
<v t="ekr.20221004064035.1277"><vh>SemanticAnalyzer.Imports</vh></v>
<v t="ekr.20221004064035.1278"><vh>SemanticAnalyzer.visit_import</vh></v>
<v t="ekr.20221004064035.1279"><vh>SemanticAnalyzer.visit_import_from</vh></v>
<v t="ekr.20221004064035.1280"><vh>SemanticAnalyzer.process_imported_symbol</vh></v>
<v t="ekr.20221004064035.1281"><vh>SemanticAnalyzer.report_missing_module_attribute</vh></v>
<v t="ekr.20221004064035.1282"><vh>SemanticAnalyzer.process_import_over_existing_name</vh></v>
<v t="ekr.20221004064035.1283"><vh>SemanticAnalyzer.correct_relative_import</vh></v>
<v t="ekr.20221004064035.1284"><vh>SemanticAnalyzer.visit_import_all</vh></v>
<v t="ekr.20221004064035.1285"><vh>SemanticAnalyzer.Assignment</vh></v>
<v t="ekr.20221004064035.1286"><vh>SemanticAnalyzer.visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.1287"><vh>SemanticAnalyzer.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1288"><vh>SemanticAnalyzer.analyze_identity_global_assignment</vh></v>
<v t="ekr.20221004064035.1289"><vh>SemanticAnalyzer.should_wait_rhs</vh></v>
<v t="ekr.20221004064035.1290"><vh>SemanticAnalyzer.can_be_type_alias</vh></v>
<v t="ekr.20221004064035.1291"><vh>SemanticAnalyzer.can_possibly_be_index_alias</vh></v>
<v t="ekr.20221004064035.1292"><vh>SemanticAnalyzer.basic_type_applications_set</vh></v>
<v t="ekr.20221004064035.1293"><vh>SemanticAnalyzer.is_type_ref</vh></v>
<v t="ekr.20221004064035.1294"><vh>SemanticAnalyzer.is_none_alias</vh></v>
<v t="ekr.20221004064035.1295"><vh>SemanticAnalyzer.record_special_form_lvalue</vh></v>
<v t="ekr.20221004064035.1296"><vh>SemanticAnalyzer.analyze_enum_assign</vh></v>
<v t="ekr.20221004064035.1297"><vh>SemanticAnalyzer.analyze_namedtuple_assign</vh></v>
<v t="ekr.20221004064035.1298"><vh>SemanticAnalyzer.analyze_typeddict_assign</vh></v>
<v t="ekr.20221004064035.1299"><vh>SemanticAnalyzer.analyze_lvalues</vh></v>
<v t="ekr.20221004064035.1300"><vh>SemanticAnalyzer.apply_dynamic_class_hook</vh></v>
<v t="ekr.20221004064035.1301"><vh>SemanticAnalyzer.unwrap_final</vh></v>
<v t="ekr.20221004064035.1302"><vh>SemanticAnalyzer.check_final_implicit_def</vh></v>
<v t="ekr.20221004064035.1303"><vh>SemanticAnalyzer.store_final_status</vh></v>
<v t="ekr.20221004064035.1304"><vh>SemanticAnalyzer.flatten_lvalues</vh></v>
<v t="ekr.20221004064035.1305"><vh>SemanticAnalyzer.unbox_literal</vh></v>
<v t="ekr.20221004064035.1306"><vh>SemanticAnalyzer.process_type_annotation</vh></v>
<v t="ekr.20221004064035.1307"><vh>SemanticAnalyzer.is_annotated_protocol_member</vh></v>
<v t="ekr.20221004064035.1308"><vh>SemanticAnalyzer.analyze_simple_literal_type</vh></v>
<v t="ekr.20221004064035.1309"><vh>SemanticAnalyzer.analyze_alias</vh></v>
<v t="ekr.20221004064035.1310"><vh>SemanticAnalyzer.is_pep_613</vh></v>
<v t="ekr.20221004064035.1311"><vh>SemanticAnalyzer.check_and_set_up_type_alias</vh></v>
<v t="ekr.20221004064035.1312"><vh>SemanticAnalyzer.disable_invalid_recursive_aliases</vh></v>
<v t="ekr.20221004064035.1313"><vh>SemanticAnalyzer.analyze_lvalue</vh></v>
<v t="ekr.20221004064035.1314"><vh>SemanticAnalyzer.analyze_name_lvalue</vh></v>
<v t="ekr.20221004064035.1315"><vh>SemanticAnalyzer.is_final_redefinition</vh></v>
<v t="ekr.20221004064035.1316"><vh>SemanticAnalyzer.is_alias_for_final_name</vh></v>
<v t="ekr.20221004064035.1317"><vh>SemanticAnalyzer.make_name_lvalue_var</vh></v>
<v t="ekr.20221004064035.1318"><vh>SemanticAnalyzer.make_name_lvalue_point_to_existing_def</vh></v>
<v t="ekr.20221004064035.1319"><vh>SemanticAnalyzer.analyze_tuple_or_list_lvalue</vh></v>
<v t="ekr.20221004064035.1320"><vh>SemanticAnalyzer.analyze_member_lvalue</vh></v>
<v t="ekr.20221004064035.1321"><vh>SemanticAnalyzer.is_self_member_ref</vh></v>
<v t="ekr.20221004064035.1322"><vh>SemanticAnalyzer.check_lvalue_validity</vh></v>
<v t="ekr.20221004064035.1323"><vh>SemanticAnalyzer.store_declared_types</vh></v>
<v t="ekr.20221004064035.1324"><vh>SemanticAnalyzer.process_typevar_declaration</vh></v>
<v t="ekr.20221004064035.1325"><vh>SemanticAnalyzer.check_typevarlike_name</vh></v>
<v t="ekr.20221004064035.1326"><vh>SemanticAnalyzer.get_typevarlike_declaration</vh></v>
<v t="ekr.20221004064035.1327"><vh>SemanticAnalyzer.process_typevar_parameters</vh></v>
<v t="ekr.20221004064035.1328"><vh>SemanticAnalyzer.extract_typevarlike_name</vh></v>
<v t="ekr.20221004064035.1329"><vh>SemanticAnalyzer.process_paramspec_declaration</vh></v>
<v t="ekr.20221004064035.1330"><vh>SemanticAnalyzer.process_typevartuple_declaration</vh></v>
<v t="ekr.20221004064035.1331"><vh>SemanticAnalyzer.basic_new_typeinfo</vh></v>
<v t="ekr.20221004064035.1332"><vh>SemanticAnalyzer.analyze_value_types</vh></v>
<v t="ekr.20221004064035.1333"><vh>SemanticAnalyzer.check_classvar</vh></v>
<v t="ekr.20221004064035.1334"><vh>SemanticAnalyzer.is_classvar</vh></v>
<v t="ekr.20221004064035.1335"><vh>SemanticAnalyzer.is_final_type</vh></v>
<v t="ekr.20221004064035.1336"><vh>SemanticAnalyzer.fail_invalid_classvar</vh></v>
<v t="ekr.20221004064035.1337"><vh>SemanticAnalyzer.process_module_assignment</vh></v>
<v t="ekr.20221004064035.1338"><vh>SemanticAnalyzer.process__all__</vh></v>
<v t="ekr.20221004064035.1339"><vh>SemanticAnalyzer.process__deletable__</vh></v>
<v t="ekr.20221004064035.1340"><vh>SemanticAnalyzer.process__slots__</vh></v>
<v t="ekr.20221004064035.1341"><vh>SemanticAnalyzer.Misc statements</vh></v>
<v t="ekr.20221004064035.1342"><vh>SemanticAnalyzer.visit_block</vh></v>
<v t="ekr.20221004064035.1343"><vh>SemanticAnalyzer.visit_block_maybe</vh></v>
<v t="ekr.20221004064035.1344"><vh>SemanticAnalyzer.visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.1345"><vh>SemanticAnalyzer.visit_return_stmt</vh></v>
<v t="ekr.20221004064035.1346"><vh>SemanticAnalyzer.visit_raise_stmt</vh></v>
<v t="ekr.20221004064035.1347"><vh>SemanticAnalyzer.visit_assert_stmt</vh></v>
<v t="ekr.20221004064035.1348"><vh>SemanticAnalyzer.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1349"><vh>SemanticAnalyzer.visit_while_stmt</vh></v>
<v t="ekr.20221004064035.1350"><vh>SemanticAnalyzer.visit_for_stmt</vh></v>
<v t="ekr.20221004064035.1351"><vh>SemanticAnalyzer.visit_break_stmt</vh></v>
<v t="ekr.20221004064035.1352"><vh>SemanticAnalyzer.visit_continue_stmt</vh></v>
<v t="ekr.20221004064035.1353"><vh>SemanticAnalyzer.visit_if_stmt</vh></v>
<v t="ekr.20221004064035.1354"><vh>SemanticAnalyzer.visit_try_stmt</vh></v>
<v t="ekr.20221004064035.1355"><vh>SemanticAnalyzer.analyze_try_stmt</vh></v>
<v t="ekr.20221004064035.1356"><vh>SemanticAnalyzer.visit_with_stmt</vh></v>
<v t="ekr.20221004064035.1357"><vh>SemanticAnalyzer.visit_del_stmt</vh></v>
<v t="ekr.20221004064035.1358"><vh>SemanticAnalyzer.is_valid_del_target</vh></v>
<v t="ekr.20221004064035.1359"><vh>SemanticAnalyzer.visit_global_decl</vh></v>
<v t="ekr.20221004064035.1360"><vh>SemanticAnalyzer.visit_nonlocal_decl</vh></v>
<v t="ekr.20221004064035.1361"><vh>SemanticAnalyzer.visit_match_stmt</vh></v>
<v t="ekr.20221004064035.1362"><vh>SemanticAnalyzer.Expressions</vh></v>
<v t="ekr.20221004064035.1363"><vh>SemanticAnalyzer.visit_name_expr</vh></v>
<v t="ekr.20221004064035.1364"><vh>SemanticAnalyzer.bind_name_expr</vh></v>
<v t="ekr.20221004064035.1365"><vh>SemanticAnalyzer.visit_super_expr</vh></v>
<v t="ekr.20221004064035.1366"><vh>SemanticAnalyzer.visit_tuple_expr</vh></v>
<v t="ekr.20221004064035.1367"><vh>SemanticAnalyzer.visit_list_expr</vh></v>
<v t="ekr.20221004064035.1368"><vh>SemanticAnalyzer.visit_set_expr</vh></v>
<v t="ekr.20221004064035.1369"><vh>SemanticAnalyzer.visit_dict_expr</vh></v>
<v t="ekr.20221004064035.1370"><vh>SemanticAnalyzer.visit_star_expr</vh></v>
<v t="ekr.20221004064035.1371"><vh>SemanticAnalyzer.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064035.1372"><vh>SemanticAnalyzer.visit_call_expr</vh></v>
<v t="ekr.20221004064035.1373"><vh>SemanticAnalyzer.translate_dict_call</vh></v>
<v t="ekr.20221004064035.1374"><vh>SemanticAnalyzer.check_fixed_args</vh></v>
<v t="ekr.20221004064035.1375"><vh>SemanticAnalyzer.visit_member_expr</vh></v>
<v t="ekr.20221004064035.1376"><vh>SemanticAnalyzer.visit_op_expr</vh></v>
<v t="ekr.20221004064035.1377"><vh>SemanticAnalyzer.visit_comparison_expr</vh></v>
<v t="ekr.20221004064035.1378"><vh>SemanticAnalyzer.visit_unary_expr</vh></v>
<v t="ekr.20221004064035.1379"><vh>SemanticAnalyzer.visit_index_expr</vh></v>
<v t="ekr.20221004064035.1380"><vh>SemanticAnalyzer.analyze_type_application</vh></v>
<v t="ekr.20221004064035.1381"><vh>SemanticAnalyzer.analyze_type_application_args</vh></v>
<v t="ekr.20221004064035.1382"><vh>SemanticAnalyzer.visit_slice_expr</vh></v>
<v t="ekr.20221004064035.1383"><vh>SemanticAnalyzer.visit_cast_expr</vh></v>
<v t="ekr.20221004064035.1384"><vh>SemanticAnalyzer.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064035.1385"><vh>SemanticAnalyzer.visit_reveal_expr</vh></v>
<v t="ekr.20221004064035.1386"><vh>SemanticAnalyzer.visit_type_application</vh></v>
<v t="ekr.20221004064035.1387"><vh>SemanticAnalyzer.visit_list_comprehension</vh></v>
<v t="ekr.20221004064035.1388"><vh>SemanticAnalyzer.visit_set_comprehension</vh></v>
<v t="ekr.20221004064035.1389"><vh>SemanticAnalyzer.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064035.1390"><vh>SemanticAnalyzer.visit_generator_expr</vh></v>
<v t="ekr.20221004064035.1391"><vh>SemanticAnalyzer.analyze_comp_for</vh></v>
<v t="ekr.20221004064035.1392"><vh>SemanticAnalyzer.analyze_comp_for_2</vh></v>
<v t="ekr.20221004064035.1393"><vh>SemanticAnalyzer.visit_lambda_expr</vh></v>
<v t="ekr.20221004064035.1394"><vh>SemanticAnalyzer.visit_conditional_expr</vh></v>
<v t="ekr.20221004064035.1395"><vh>SemanticAnalyzer.visit__promote_expr</vh></v>
<v t="ekr.20221004064035.1396"><vh>SemanticAnalyzer.visit_yield_expr</vh></v>
<v t="ekr.20221004064035.1397"><vh>SemanticAnalyzer.visit_await_expr</vh></v>
<v t="ekr.20221004064035.1398"><vh>SemanticAnalyzer.Patterns</vh></v>
<v t="ekr.20221004064035.1399"><vh>SemanticAnalyzer.visit_as_pattern</vh></v>
<v t="ekr.20221004064035.1400"><vh>SemanticAnalyzer.visit_or_pattern</vh></v>
<v t="ekr.20221004064035.1401"><vh>SemanticAnalyzer.visit_value_pattern</vh></v>
<v t="ekr.20221004064035.1402"><vh>SemanticAnalyzer.visit_sequence_pattern</vh></v>
<v t="ekr.20221004064035.1403"><vh>SemanticAnalyzer.visit_starred_pattern</vh></v>
<v t="ekr.20221004064035.1404"><vh>SemanticAnalyzer.visit_mapping_pattern</vh></v>
<v t="ekr.20221004064035.1405"><vh>SemanticAnalyzer.visit_class_pattern</vh></v>
<v t="ekr.20221004064035.1406"><vh>SemanticAnalyzer.Lookup functions</vh></v>
<v t="ekr.20221004064035.1407"><vh>SemanticAnalyzer.lookup</vh></v>
<v t="ekr.20221004064035.1408"><vh>SemanticAnalyzer.is_active_symbol_in_class_body</vh></v>
<v t="ekr.20221004064035.1409"><vh>SemanticAnalyzer.is_textually_before_statement</vh></v>
<v t="ekr.20221004064035.1410"><vh>SemanticAnalyzer.is_overloaded_item</vh></v>
<v t="ekr.20221004064035.1411"><vh>SemanticAnalyzer.is_defined_in_current_module</vh></v>
<v t="ekr.20221004064035.1412"><vh>SemanticAnalyzer.lookup_qualified</vh></v>
<v t="ekr.20221004064035.1413"><vh>SemanticAnalyzer.lookup_type_node</vh></v>
<v t="ekr.20221004064035.1414"><vh>SemanticAnalyzer.get_module_symbol</vh></v>
<v t="ekr.20221004064035.1415"><vh>SemanticAnalyzer.is_missing_module</vh></v>
<v t="ekr.20221004064035.1416"><vh>SemanticAnalyzer.implicit_symbol</vh></v>
<v t="ekr.20221004064035.1417"><vh>SemanticAnalyzer.create_getattr_var</vh></v>
<v t="ekr.20221004064035.1418"><vh>SemanticAnalyzer.lookup_fully_qualified</vh></v>
<v t="ekr.20221004064035.1419"><vh>SemanticAnalyzer.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20221004064035.1420"><vh>SemanticAnalyzer.object_type</vh></v>
<v t="ekr.20221004064035.1421"><vh>SemanticAnalyzer.str_type</vh></v>
<v t="ekr.20221004064035.1422"><vh>SemanticAnalyzer.named_type</vh></v>
<v t="ekr.20221004064035.1423"><vh>SemanticAnalyzer.named_type_or_none</vh></v>
<v t="ekr.20221004064035.1424"><vh>SemanticAnalyzer.builtin_type</vh></v>
<v t="ekr.20221004064035.1425"><vh>SemanticAnalyzer.lookup_current_scope</vh></v>
<v t="ekr.20221004064035.1426"><vh>SemanticAnalyzer.Adding symbols</vh></v>
<v t="ekr.20221004064035.1427"><vh>SemanticAnalyzer.add_symbol</vh></v>
<v t="ekr.20221004064035.1428"><vh>SemanticAnalyzer.add_symbol_skip_local</vh></v>
<v t="ekr.20221004064035.1429"><vh>SemanticAnalyzer.add_symbol_table_node</vh></v>
<v t="ekr.20221004064035.1430"><vh>SemanticAnalyzer.add_redefinition</vh></v>
<v t="ekr.20221004064035.1431"><vh>SemanticAnalyzer.add_local</vh></v>
<v t="ekr.20221004064035.1432"><vh>SemanticAnalyzer.add_module_symbol</vh></v>
<v t="ekr.20221004064035.1433"><vh>SemanticAnalyzer._get_node_for_class_scoped_import</vh></v>
<v t="ekr.20221004064035.1434"><vh>SemanticAnalyzer.add_imported_symbol</vh></v>
<v t="ekr.20221004064035.1435"><vh>SemanticAnalyzer.add_unknown_imported_symbol</vh></v>
<v t="ekr.20221004064035.1436"><vh>SemanticAnalyzer.Other helpers</vh></v>
<v t="ekr.20221004064035.1437"><vh>SemanticAnalyzer.tvar_scope_frame</vh></v>
<v t="ekr.20221004064035.1438"><vh>SemanticAnalyzer.defer</vh></v>
<v t="ekr.20221004064035.1439"><vh>SemanticAnalyzer.track_incomplete_refs</vh></v>
<v t="ekr.20221004064035.1440"><vh>SemanticAnalyzer.found_incomplete_ref</vh></v>
<v t="ekr.20221004064035.1441"><vh>SemanticAnalyzer.record_incomplete_ref</vh></v>
<v t="ekr.20221004064035.1442"><vh>SemanticAnalyzer.mark_incomplete</vh></v>
<v t="ekr.20221004064035.1443"><vh>SemanticAnalyzer.is_incomplete_namespace</vh></v>
<v t="ekr.20221004064035.1444"><vh>SemanticAnalyzer.process_placeholder</vh></v>
<v t="ekr.20221004064035.1445"><vh>SemanticAnalyzer.cannot_resolve_name</vh></v>
<v t="ekr.20221004064035.1446"><vh>SemanticAnalyzer.qualified_name</vh></v>
<v t="ekr.20221004064035.1447"><vh>SemanticAnalyzer.enter</vh></v>
<v t="ekr.20221004064035.1448"><vh>SemanticAnalyzer.is_func_scope</vh></v>
<v t="ekr.20221004064035.1449"><vh>SemanticAnalyzer.is_nested_within_func_scope</vh></v>
<v t="ekr.20221004064035.1450"><vh>SemanticAnalyzer.is_class_scope</vh></v>
<v t="ekr.20221004064035.1451"><vh>SemanticAnalyzer.is_module_scope</vh></v>
<v t="ekr.20221004064035.1452"><vh>SemanticAnalyzer.current_symbol_kind</vh></v>
<v t="ekr.20221004064035.1453"><vh>SemanticAnalyzer.current_symbol_table</vh></v>
<v t="ekr.20221004064035.1454"><vh>SemanticAnalyzer.is_global_or_nonlocal</vh></v>
<v t="ekr.20221004064035.1455"><vh>SemanticAnalyzer.add_exports</vh></v>
<v t="ekr.20221004064035.1456"><vh>SemanticAnalyzer.name_not_defined</vh></v>
<v t="ekr.20221004064035.1457"><vh>SemanticAnalyzer.already_defined</vh></v>
<v t="ekr.20221004064035.1458"><vh>SemanticAnalyzer.name_already_defined</vh></v>
<v t="ekr.20221004064035.1459"><vh>SemanticAnalyzer.attribute_already_defined</vh></v>
<v t="ekr.20221004064035.1460"><vh>SemanticAnalyzer.is_local_name</vh></v>
<v t="ekr.20221004064035.1461"><vh>SemanticAnalyzer.in_checked_function</vh></v>
<v t="ekr.20221004064035.1462"><vh>SemanticAnalyzer.fail</vh></v>
<v t="ekr.20221004064035.1463"><vh>SemanticAnalyzer.note</vh></v>
<v t="ekr.20221004064035.1464"><vh>SemanticAnalyzer.accept</vh></v>
<v t="ekr.20221004064035.1465"><vh>SemanticAnalyzer.expr_to_analyzed_type</vh></v>
<v t="ekr.20221004064035.1466"><vh>SemanticAnalyzer.analyze_type_expr</vh></v>
<v t="ekr.20221004064035.1467"><vh>SemanticAnalyzer.type_analyzer</vh></v>
<v t="ekr.20221004064035.1468"><vh>SemanticAnalyzer.expr_to_unanalyzed_type</vh></v>
<v t="ekr.20221004064035.1469"><vh>SemanticAnalyzer.anal_type</vh></v>
<v t="ekr.20221004064035.1470"><vh>SemanticAnalyzer.class_type</vh></v>
<v t="ekr.20221004064035.1471"><vh>SemanticAnalyzer.schedule_patch</vh></v>
<v t="ekr.20221004064035.1472"><vh>SemanticAnalyzer.report_hang</vh></v>
<v t="ekr.20221004064035.1473"><vh>SemanticAnalyzer.add_plugin_dependency</vh></v>
<v t="ekr.20221004064035.1474"><vh>SemanticAnalyzer.add_type_alias_deps</vh></v>
<v t="ekr.20221004064035.1475"><vh>SemanticAnalyzer.is_mangled_global</vh></v>
<v t="ekr.20221004064035.1476"><vh>SemanticAnalyzer.is_initial_mangled_global</vh></v>
<v t="ekr.20221004064035.1477"><vh>SemanticAnalyzer.parse_bool</vh></v>
<v t="ekr.20221004064035.1478"><vh>SemanticAnalyzer.set_future_import_flags</vh></v>
<v t="ekr.20221004064035.1479"><vh>SemanticAnalyzer.is_future_flag_set</vh></v>
</v>
<v t="ekr.20221004064035.1480"><vh>replace_implicit_first_type</vh></v>
<v t="ekr.20221004064035.1481"><vh>refers_to_fullname</vh></v>
<v t="ekr.20221004064035.1482"><vh>refers_to_class_or_function</vh></v>
<v t="ekr.20221004064035.1483"><vh>find_duplicate</vh></v>
<v t="ekr.20221004064035.1484"><vh>remove_imported_names_from_symtable</vh></v>
<v t="ekr.20221004064035.1485"><vh>make_any_non_explicit</vh></v>
<v t="ekr.20221004064035.1486"><vh>class MakeAnyNonExplicit</vh>
<v t="ekr.20221004064035.1487"><vh>MakeAnyNonExplicit.visit_any</vh></v>
<v t="ekr.20221004064035.1488"><vh>MakeAnyNonExplicit.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064035.1489"><vh>apply_semantic_analyzer_patches</vh></v>
<v t="ekr.20221004064035.1490"><vh>names_modified_by_assignment</vh></v>
<v t="ekr.20221004064035.1491"><vh>names_modified_in_lvalue</vh></v>
<v t="ekr.20221004064035.1492"><vh>is_same_var_from_getattr</vh></v>
<v t="ekr.20221004064035.1493"><vh>dummy_context</vh></v>
<v t="ekr.20221004064035.1494"><vh>is_valid_replacement</vh></v>
<v t="ekr.20221004064035.1495"><vh>is_same_symbol</vh></v>
<v t="ekr.20221004064035.1496"><vh>is_trivial_body</vh></v>
</v>
<v t="ekr.20221004064035.1497"><vh>@clean semanal_classprop.py</vh>
<v t="ekr.20221004064035.1498"><vh>calculate_class_abstract_status</vh></v>
<v t="ekr.20221004064035.1499"><vh>check_protocol_status</vh></v>
<v t="ekr.20221004064035.1500"><vh>calculate_class_vars</vh></v>
<v t="ekr.20221004064035.1501"><vh>add_type_promotion</vh></v>
</v>
<v t="ekr.20221004064035.1502"><vh>@clean semanal_enum.py</vh>
<v t="ekr.20221004064035.1503"><vh>class EnumCallAnalyzer</vh>
<v t="ekr.20221004064035.1504"><vh>EnumCallAnalyzer.__init__</vh></v>
<v t="ekr.20221004064035.1505"><vh>EnumCallAnalyzer.process_enum_call</vh></v>
<v t="ekr.20221004064035.1506"><vh>EnumCallAnalyzer.check_enum_call</vh></v>
<v t="ekr.20221004064035.1507"><vh>EnumCallAnalyzer.build_enum_call_typeinfo</vh></v>
<v t="ekr.20221004064035.1508"><vh>EnumCallAnalyzer.parse_enum_call_args</vh></v>
<v t="ekr.20221004064035.1509"><vh>EnumCallAnalyzer.fail_enum_call_arg</vh></v>
<v t="ekr.20221004064035.1510"><vh>EnumCallAnalyzer.Helpers</vh></v>
<v t="ekr.20221004064035.1511"><vh>EnumCallAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1512"><vh>@clean semanal_infer.py</vh>
<v t="ekr.20221004064035.1513"><vh>infer_decorator_signature_if_simple</vh></v>
<v t="ekr.20221004064035.1514"><vh>is_identity_signature</vh></v>
<v t="ekr.20221004064035.1515"><vh>calculate_return_type</vh></v>
<v t="ekr.20221004064035.1516"><vh>find_fixed_callable_return</vh></v>
</v>
<v t="ekr.20221004064035.1517"><vh>@clean semanal_main.py</vh>
<v t="ekr.20221004064035.1518"><vh>semantic_analysis_for_scc</vh></v>
<v t="ekr.20221004064035.1519"><vh>cleanup_builtin_scc</vh></v>
<v t="ekr.20221004064035.1520"><vh>semantic_analysis_for_targets</vh></v>
<v t="ekr.20221004064035.1521"><vh>restore_saved_attrs</vh></v>
<v t="ekr.20221004064035.1522"><vh>process_top_levels</vh></v>
<v t="ekr.20221004064035.1523"><vh>process_functions</vh></v>
<v t="ekr.20221004064035.1524"><vh>process_top_level_function</vh></v>
<v t="ekr.20221004064035.1525"><vh>TargetInfo: _TypeAlias = Tuple[</vh></v>
<v t="ekr.20221004064035.1526"><vh>get_all_leaf_targets</vh></v>
<v t="ekr.20221004064035.1527"><vh>semantic_analyze_target</vh></v>
<v t="ekr.20221004064035.1528"><vh>check_type_arguments</vh></v>
<v t="ekr.20221004064035.1529"><vh>check_type_arguments_in_targets</vh></v>
<v t="ekr.20221004064035.1530"><vh>apply_class_plugin_hooks</vh></v>
<v t="ekr.20221004064035.1531"><vh>apply_hooks_to_class</vh></v>
<v t="ekr.20221004064035.1532"><vh>calculate_class_properties</vh></v>
<v t="ekr.20221004064035.1533"><vh>check_blockers</vh></v>
</v>
<v t="ekr.20221004064035.1534"><vh>@clean semanal_namedtuple.py</vh>
<v t="ekr.20221004064035.1535"><vh>class NamedTupleAnalyzer</vh>
<v t="ekr.20221004064035.1536"><vh>NamedTupleAnalyzer.__init__</vh></v>
<v t="ekr.20221004064035.1537"><vh>NamedTupleAnalyzer.analyze_namedtuple_classdef</vh></v>
<v t="ekr.20221004064035.1538"><vh>NamedTupleAnalyzer.check_namedtuple_classdef</vh></v>
<v t="ekr.20221004064035.1539"><vh>NamedTupleAnalyzer.check_namedtuple</vh></v>
<v t="ekr.20221004064035.1540"><vh>NamedTupleAnalyzer.store_namedtuple_info</vh></v>
<v t="ekr.20221004064035.1541"><vh>NamedTupleAnalyzer.parse_namedtuple_args</vh></v>
<v t="ekr.20221004064035.1542"><vh>NamedTupleAnalyzer.parse_namedtuple_fields_with_types</vh></v>
<v t="ekr.20221004064035.1543"><vh>NamedTupleAnalyzer.build_namedtuple_typeinfo</vh></v>
<v t="ekr.20221004064035.1544"><vh>NamedTupleAnalyzer.save_namedtuple_body</vh></v>
<v t="ekr.20221004064035.1545"><vh>NamedTupleAnalyzer.Helpers</vh></v>
<v t="ekr.20221004064035.1546"><vh>NamedTupleAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1547"><vh>@clean semanal_newtype.py</vh>
<v t="ekr.20221004064035.1548"><vh>class NewTypeAnalyzer</vh>
<v t="ekr.20221004064035.1549"><vh>NewTypeAnalyzer.__init__</vh></v>
<v t="ekr.20221004064035.1550"><vh>NewTypeAnalyzer.process_newtype_declaration</vh></v>
<v t="ekr.20221004064035.1551"><vh>NewTypeAnalyzer.analyze_newtype_declaration</vh></v>
<v t="ekr.20221004064035.1552"><vh>NewTypeAnalyzer.check_newtype_args</vh></v>
<v t="ekr.20221004064035.1553"><vh>NewTypeAnalyzer.build_newtype_typeinfo</vh></v>
<v t="ekr.20221004064035.1554"><vh>NewTypeAnalyzer.Helpers</vh></v>
<v t="ekr.20221004064035.1555"><vh>NewTypeAnalyzer.make_argument</vh></v>
<v t="ekr.20221004064035.1556"><vh>NewTypeAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1557"><vh>@clean semanal_pass1.py</vh>
<v t="ekr.20221004064035.1558"><vh>class SemanticAnalyzerPreAnalysis</vh>
<v t="ekr.20221004064035.1559"><vh>SemanticAnalyzerPreAnalysis.visit_file</vh></v>
<v t="ekr.20221004064035.1560"><vh>SemanticAnalyzerPreAnalysis.visit_func_def</vh></v>
<v t="ekr.20221004064035.1561"><vh>SemanticAnalyzerPreAnalysis.visit_class_def</vh></v>
<v t="ekr.20221004064035.1562"><vh>SemanticAnalyzerPreAnalysis.visit_import_from</vh></v>
<v t="ekr.20221004064035.1563"><vh>SemanticAnalyzerPreAnalysis.visit_import_all</vh></v>
<v t="ekr.20221004064035.1564"><vh>SemanticAnalyzerPreAnalysis.visit_import</vh></v>
<v t="ekr.20221004064035.1565"><vh>SemanticAnalyzerPreAnalysis.visit_if_stmt</vh></v>
<v t="ekr.20221004064035.1566"><vh>SemanticAnalyzerPreAnalysis.visit_block</vh></v>
<v t="ekr.20221004064035.1567"><vh>SemanticAnalyzerPreAnalysis.visit_match_stmt</vh></v>
<v t="ekr.20221004064035.1568"><vh>SemanticAnalyzerPreAnalysis.The remaining methods are an optimization: don't visit nested expressions</vh></v>
<v t="ekr.20221004064035.1569"><vh>SemanticAnalyzerPreAnalysis.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1570"><vh>SemanticAnalyzerPreAnalysis.visit_expression_stmt</vh></v>
<v t="ekr.20221004064035.1571"><vh>SemanticAnalyzerPreAnalysis.visit_return_stmt</vh></v>
<v t="ekr.20221004064035.1572"><vh>SemanticAnalyzerPreAnalysis.visit_for_stmt</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1573"><vh>@clean semanal_shared.py</vh>
<v t="ekr.20221004064035.1574"><vh>class SemanticAnalyzerCoreInterface</vh>
<v t="ekr.20221004064035.1575"><vh>SemanticAnalyzerCoreInterface.lookup_qualified</vh></v>
<v t="ekr.20221004064035.1576"><vh>SemanticAnalyzerCoreInterface.lookup_fully_qualified</vh></v>
<v t="ekr.20221004064035.1577"><vh>SemanticAnalyzerCoreInterface.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20221004064035.1578"><vh>SemanticAnalyzerCoreInterface.fail</vh></v>
<v t="ekr.20221004064035.1579"><vh>SemanticAnalyzerCoreInterface.note</vh></v>
<v t="ekr.20221004064035.1580"><vh>SemanticAnalyzerCoreInterface.record_incomplete_ref</vh></v>
<v t="ekr.20221004064035.1581"><vh>SemanticAnalyzerCoreInterface.defer</vh></v>
<v t="ekr.20221004064035.1582"><vh>SemanticAnalyzerCoreInterface.is_incomplete_namespace</vh></v>
<v t="ekr.20221004064035.1583"><vh>SemanticAnalyzerCoreInterface.final_iteration</vh></v>
<v t="ekr.20221004064035.1584"><vh>SemanticAnalyzerCoreInterface.is_future_flag_set</vh></v>
<v t="ekr.20221004064035.1585"><vh>SemanticAnalyzerCoreInterface.is_stub_file</vh></v>
<v t="ekr.20221004064035.1586"><vh>SemanticAnalyzerCoreInterface.is_func_scope</vh></v>
</v>
<v t="ekr.20221004064035.1587"><vh>class SemanticAnalyzerInterface</vh>
<v t="ekr.20221004064035.1588"><vh>SemanticAnalyzerInterface.lookup</vh></v>
<v t="ekr.20221004064035.1589"><vh>SemanticAnalyzerInterface.named_type</vh></v>
<v t="ekr.20221004064035.1590"><vh>SemanticAnalyzerInterface.named_type_or_none</vh></v>
<v t="ekr.20221004064035.1591"><vh>SemanticAnalyzerInterface.accept</vh></v>
<v t="ekr.20221004064035.1592"><vh>SemanticAnalyzerInterface.anal_type</vh></v>
<v t="ekr.20221004064035.1593"><vh>SemanticAnalyzerInterface.get_and_bind_all_tvars</vh></v>
<v t="ekr.20221004064035.1594"><vh>SemanticAnalyzerInterface.basic_new_typeinfo</vh></v>
<v t="ekr.20221004064035.1595"><vh>SemanticAnalyzerInterface.schedule_patch</vh></v>
<v t="ekr.20221004064035.1596"><vh>SemanticAnalyzerInterface.add_symbol_table_node</vh></v>
<v t="ekr.20221004064035.1597"><vh>SemanticAnalyzerInterface.current_symbol_table</vh></v>
<v t="ekr.20221004064035.1598"><vh>SemanticAnalyzerInterface.add_symbol</vh></v>
<v t="ekr.20221004064035.1599"><vh>SemanticAnalyzerInterface.add_symbol_skip_local</vh></v>
<v t="ekr.20221004064035.1600"><vh>SemanticAnalyzerInterface.parse_bool</vh></v>
<v t="ekr.20221004064035.1601"><vh>SemanticAnalyzerInterface.qualified_name</vh></v>
<v t="ekr.20221004064035.1602"><vh>SemanticAnalyzerInterface.is_typeshed_stub_file</vh></v>
</v>
<v t="ekr.20221004064035.1603"><vh>set_callable_name</vh></v>
<v t="ekr.20221004064035.1604"><vh>calculate_tuple_fallback</vh></v>
<v t="ekr.20221004064035.1605"><vh>class _NamedTypeCallback</vh></v>
<v t="ekr.20221004064035.1606"><vh>paramspec_args</vh></v>
<v t="ekr.20221004064035.1607"><vh>paramspec_kwargs</vh></v>
<v t="ekr.20221004064035.1608"><vh>class HasPlaceholders</vh></v>
<v t="ekr.20221004064035.1609"><vh>has_placeholder</vh></v>
</v>
<v t="ekr.20221004064035.1610"><vh>@clean semanal_typeargs.py</vh>
<v t="ekr.20221004064035.1611"><vh>class TypeArgumentAnalyzer</vh>
<v t="ekr.20221004064035.1612"><vh>TypeArgumentAnalyzer.__init__</vh></v>
<v t="ekr.20221004064035.1613"><vh>TypeArgumentAnalyzer.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.1614"><vh>TypeArgumentAnalyzer.visit_func</vh></v>
<v t="ekr.20221004064035.1615"><vh>TypeArgumentAnalyzer.visit_class_def</vh></v>
<v t="ekr.20221004064035.1616"><vh>TypeArgumentAnalyzer.visit_block</vh></v>
<v t="ekr.20221004064035.1617"><vh>TypeArgumentAnalyzer.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.1618"><vh>TypeArgumentAnalyzer.visit_instance</vh></v>
<v t="ekr.20221004064035.1619"><vh>TypeArgumentAnalyzer.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.1620"><vh>TypeArgumentAnalyzer.check_type_var_values</vh></v>
<v t="ekr.20221004064035.1621"><vh>TypeArgumentAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1622"><vh>@clean semanal_typeddict.py</vh>
<v t="ekr.20221004064035.1623"><vh>class TypedDictAnalyzer</vh>
<v t="ekr.20221004064035.1624"><vh>TypedDictAnalyzer.__init__</vh></v>
<v t="ekr.20221004064035.1625"><vh>TypedDictAnalyzer.analyze_typeddict_classdef</vh></v>
<v t="ekr.20221004064035.1626"><vh>TypedDictAnalyzer.add_keys_and_types_from_base</vh></v>
<v t="ekr.20221004064035.1627"><vh>TypedDictAnalyzer.analyze_base_args</vh></v>
<v t="ekr.20221004064035.1628"><vh>TypedDictAnalyzer.map_items_to_base</vh></v>
<v t="ekr.20221004064035.1629"><vh>TypedDictAnalyzer.analyze_typeddict_classdef_fields</vh></v>
<v t="ekr.20221004064035.1630"><vh>TypedDictAnalyzer.check_typeddict</vh></v>
<v t="ekr.20221004064035.1631"><vh>TypedDictAnalyzer.parse_typeddict_args</vh></v>
<v t="ekr.20221004064035.1632"><vh>TypedDictAnalyzer.parse_typeddict_fields_with_types</vh></v>
<v t="ekr.20221004064035.1633"><vh>TypedDictAnalyzer.fail_typeddict_arg</vh></v>
<v t="ekr.20221004064035.1634"><vh>TypedDictAnalyzer.build_typeddict_typeinfo</vh></v>
<v t="ekr.20221004064035.1635"><vh>TypedDictAnalyzer.Helpers</vh></v>
<v t="ekr.20221004064035.1636"><vh>TypedDictAnalyzer.is_typeddict</vh></v>
<v t="ekr.20221004064035.1637"><vh>TypedDictAnalyzer.fail</vh></v>
<v t="ekr.20221004064035.1638"><vh>TypedDictAnalyzer.note</vh></v>
</v>
</v>
</v>
<v t="ekr.20221005080808.1"><vh>--- stubgen</vh>
<v t="ekr.20221004064035.1801"><vh>@clean stubdoc.py</vh>
<v t="ekr.20221004064035.1802"><vh>is_valid_type</vh></v>
<v t="ekr.20221004064035.1803"><vh>class ArgSig</vh>
<v t="ekr.20221004064035.1804"><vh>ArgSig.__init__</vh></v>
<v t="ekr.20221004064035.1805"><vh>ArgSig.__repr__</vh></v>
<v t="ekr.20221004064035.1806"><vh>ArgSig.__eq__</vh></v>
</v>
<v t="ekr.20221004064035.1807"><vh>class FunctionSig</vh></v>
<v t="ekr.20221004064035.1808"><vh>States of the docstring parser.</vh></v>
<v t="ekr.20221004064035.1809"><vh>class DocStringParser</vh>
<v t="ekr.20221004064035.1810"><vh>DocStringParser.__init__</vh></v>
<v t="ekr.20221004064035.1811"><vh>DocStringParser.add_token</vh></v>
<v t="ekr.20221004064035.1812"><vh>DocStringParser.reset</vh></v>
<v t="ekr.20221004064035.1813"><vh>DocStringParser.get_signatures</vh></v>
</v>
<v t="ekr.20221004064035.1814"><vh>infer_sig_from_docstring</vh>
<v t="ekr.20221004064035.1815"><vh>is_unique_args</vh></v>
</v>
<v t="ekr.20221004064035.1816"><vh>infer_arg_sig_from_anon_docstring</vh></v>
<v t="ekr.20221004064035.1817"><vh>infer_ret_type_sig_from_docstring</vh></v>
<v t="ekr.20221004064035.1818"><vh>infer_ret_type_sig_from_anon_docstring</vh></v>
<v t="ekr.20221004064035.1819"><vh>parse_signature</vh></v>
<v t="ekr.20221004064035.1820"><vh>build_signature</vh></v>
<v t="ekr.20221004064035.1821"><vh>parse_all_signatures</vh></v>
<v t="ekr.20221004064035.1822"><vh>find_unique_signatures</vh></v>
<v t="ekr.20221004064035.1823"><vh>infer_prop_type_from_docstring</vh></v>
</v>
<v t="ekr.20221004064035.1824"><vh>@clean stubgen.py</vh>
<v t="ekr.20221004064035.1825"><vh>class Options</vh>
<v t="ekr.20221004064035.1826"><vh>Options.__init__</vh></v>
</v>
<v t="ekr.20221004064035.1827"><vh>class StubSource</vh>
<v t="ekr.20221004064035.1828"><vh>StubSource.__init__</vh></v>
<v t="ekr.20221004064035.1829"><vh>StubSource.module</vh></v>
<v t="ekr.20221004064035.1830"><vh>StubSource.path</vh></v>
</v>
<v t="ekr.20221004064035.1831"><vh>What was generated previously in the stub file. We keep track of these to generate</vh></v>
<v t="ekr.20221004064035.1832"><vh>class AnnotationPrinter</vh>
<v t="ekr.20221004064035.1833"><vh>AnnotationPrinter.__init__</vh></v>
<v t="ekr.20221004064035.1834"><vh>AnnotationPrinter.visit_any</vh></v>
<v t="ekr.20221004064035.1835"><vh>AnnotationPrinter.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.1836"><vh>AnnotationPrinter.visit_none_type</vh></v>
<v t="ekr.20221004064035.1837"><vh>AnnotationPrinter.visit_type_list</vh></v>
<v t="ekr.20221004064035.1838"><vh>AnnotationPrinter.args_str</vh></v>
</v>
<v t="ekr.20221004064035.1839"><vh>class AliasPrinter</vh>
<v t="ekr.20221004064035.1840"><vh>AliasPrinter.__init__</vh></v>
<v t="ekr.20221004064035.1841"><vh>AliasPrinter.visit_call_expr</vh></v>
<v t="ekr.20221004064035.1842"><vh>AliasPrinter.visit_name_expr</vh></v>
<v t="ekr.20221004064035.1843"><vh>AliasPrinter.visit_member_expr</vh></v>
<v t="ekr.20221004064035.1844"><vh>AliasPrinter.visit_str_expr</vh></v>
<v t="ekr.20221004064035.1845"><vh>AliasPrinter.visit_index_expr</vh></v>
<v t="ekr.20221004064035.1846"><vh>AliasPrinter.visit_tuple_expr</vh></v>
<v t="ekr.20221004064035.1847"><vh>AliasPrinter.visit_list_expr</vh></v>
<v t="ekr.20221004064035.1848"><vh>AliasPrinter.visit_ellipsis</vh></v>
</v>
<v t="ekr.20221004064035.1849"><vh>class ImportTracker</vh>
<v t="ekr.20221004064035.1850"><vh>ImportTracker.__init__</vh></v>
<v t="ekr.20221004064035.1851"><vh>ImportTracker.add_import_from</vh></v>
<v t="ekr.20221004064035.1852"><vh>ImportTracker.add_import</vh></v>
<v t="ekr.20221004064035.1853"><vh>ImportTracker.require_name</vh></v>
<v t="ekr.20221004064035.1854"><vh>ImportTracker.reexport</vh></v>
<v t="ekr.20221004064035.1855"><vh>ImportTracker.import_lines</vh></v>
</v>
<v t="ekr.20221004064035.1856"><vh>find_defined_names</vh></v>
<v t="ekr.20221004064035.1857"><vh>class DefinitionFinder</vh>
<v t="ekr.20221004064035.1858"><vh>DefinitionFinder.__init__</vh></v>
<v t="ekr.20221004064035.1859"><vh>DefinitionFinder.visit_class_def</vh></v>
<v t="ekr.20221004064035.1860"><vh>DefinitionFinder.visit_func_def</vh></v>
</v>
<v t="ekr.20221004064035.1861"><vh>find_referenced_names</vh></v>
<v t="ekr.20221004064035.1862"><vh>class ReferenceFinder</vh>
<v t="ekr.20221004064035.1863"><vh>ReferenceFinder.__init__</vh></v>
<v t="ekr.20221004064035.1864"><vh>ReferenceFinder.visit_block</vh></v>
<v t="ekr.20221004064035.1865"><vh>ReferenceFinder.visit_name_expr</vh></v>
<v t="ekr.20221004064035.1866"><vh>ReferenceFinder.visit_instance</vh></v>
<v t="ekr.20221004064035.1867"><vh>ReferenceFinder.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.1868"><vh>ReferenceFinder.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.1869"><vh>ReferenceFinder.visit_callable_type</vh></v>
<v t="ekr.20221004064035.1870"><vh>ReferenceFinder.add_ref</vh></v>
</v>
<v t="ekr.20221004064035.1871"><vh>class StubGenerator</vh>
<v t="ekr.20221004064035.1872"><vh>StubGenerator.__init__</vh></v>
<v t="ekr.20221004064035.1873"><vh>StubGenerator.visit_mypy_file</vh></v>
<v t="ekr.20221004064035.1874"><vh>StubGenerator.visit_overloaded_func_def</vh></v>
<v t="ekr.20221004064035.1875"><vh>StubGenerator.visit_func_def</vh></v>
<v t="ekr.20221004064035.1876"><vh>StubGenerator.is_none_expr</vh></v>
<v t="ekr.20221004064035.1877"><vh>StubGenerator.visit_decorator</vh></v>
<v t="ekr.20221004064035.1878"><vh>StubGenerator.process_decorator</vh></v>
<v t="ekr.20221004064035.1879"><vh>StubGenerator.process_name_expr_decorator</vh></v>
<v t="ekr.20221004064035.1880"><vh>StubGenerator.refers_to_fullname</vh></v>
<v t="ekr.20221004064035.1881"><vh>StubGenerator.process_member_expr_decorator</vh></v>
<v t="ekr.20221004064035.1882"><vh>StubGenerator.visit_class_def</vh></v>
<v t="ekr.20221004064035.1883"><vh>StubGenerator.get_base_types</vh></v>
<v t="ekr.20221004064035.1884"><vh>StubGenerator.visit_block</vh></v>
<v t="ekr.20221004064035.1885"><vh>StubGenerator.visit_assignment_stmt</vh></v>
<v t="ekr.20221004064035.1886"><vh>StubGenerator.is_namedtuple</vh></v>
<v t="ekr.20221004064035.1887"><vh>StubGenerator.process_namedtuple</vh></v>
<v t="ekr.20221004064035.1888"><vh>StubGenerator.is_alias_expression</vh></v>
<v t="ekr.20221004064035.1889"><vh>StubGenerator.process_typealias</vh></v>
<v t="ekr.20221004064035.1890"><vh>StubGenerator.visit_if_stmt</vh></v>
<v t="ekr.20221004064035.1891"><vh>StubGenerator.visit_import_all</vh></v>
<v t="ekr.20221004064035.1892"><vh>StubGenerator.visit_import_from</vh></v>
<v t="ekr.20221004064035.1893"><vh>StubGenerator.visit_import</vh></v>
<v t="ekr.20221004064035.1894"><vh>StubGenerator.get_init</vh></v>
<v t="ekr.20221004064035.1895"><vh>StubGenerator.add</vh></v>
<v t="ekr.20221004064035.1896"><vh>StubGenerator.add_decorator</vh></v>
<v t="ekr.20221004064035.1897"><vh>StubGenerator.clear_decorators</vh></v>
<v t="ekr.20221004064035.1898"><vh>StubGenerator.typing_name</vh></v>
<v t="ekr.20221004064035.1899"><vh>StubGenerator.add_typing_import</vh></v>
<v t="ekr.20221004064035.1900"><vh>StubGenerator.add_abc_import</vh></v>
<v t="ekr.20221004064035.1901"><vh>StubGenerator.add_import_line</vh></v>
<v t="ekr.20221004064035.1902"><vh>StubGenerator.add_coroutine_decorator</vh></v>
<v t="ekr.20221004064035.1903"><vh>StubGenerator.output</vh></v>
<v t="ekr.20221004064035.1904"><vh>StubGenerator.is_not_in_all</vh></v>
<v t="ekr.20221004064035.1905"><vh>StubGenerator.is_private_name</vh></v>
<v t="ekr.20221004064035.1906"><vh>StubGenerator.is_private_member</vh></v>
<v t="ekr.20221004064035.1907"><vh>StubGenerator.get_str_type_of_node</vh></v>
<v t="ekr.20221004064035.1908"><vh>StubGenerator.print_annotation</vh></v>
<v t="ekr.20221004064035.1909"><vh>StubGenerator.is_top_level</vh></v>
<v t="ekr.20221004064035.1910"><vh>StubGenerator.record_name</vh></v>
<v t="ekr.20221004064035.1911"><vh>StubGenerator.is_recorded_name</vh></v>
</v>
<v t="ekr.20221004064035.1912"><vh>find_method_names</vh></v>
<v t="ekr.20221004064035.1913"><vh>class SelfTraverser</vh>
<v t="ekr.20221004064035.1914"><vh>SelfTraverser.__init__</vh></v>
<v t="ekr.20221004064035.1915"><vh>SelfTraverser.visit_assignment_stmt</vh></v>
</v>
<v t="ekr.20221004064035.1916"><vh>find_self_initializers</vh></v>
<v t="ekr.20221004064035.1917"><vh>get_qualified_name</vh></v>
<v t="ekr.20221004064035.1918"><vh>remove_blacklisted_modules</vh></v>
<v t="ekr.20221004064035.1919"><vh>is_blacklisted_path</vh></v>
<v t="ekr.20221004064035.1920"><vh>normalize_path_separators</vh></v>
<v t="ekr.20221004064035.1921"><vh>collect_build_targets</vh></v>
<v t="ekr.20221004064035.1922"><vh>find_module_paths_using_imports</vh></v>
<v t="ekr.20221004064035.1923"><vh>is_non_library_module</vh></v>
<v t="ekr.20221004064035.1924"><vh>translate_module_name</vh></v>
<v t="ekr.20221004064035.1925"><vh>find_module_paths_using_search</vh></v>
<v t="ekr.20221004064035.1926"><vh>mypy_options</vh></v>
<v t="ekr.20221004064035.1927"><vh>parse_source_file</vh></v>
<v t="ekr.20221004064035.1928"><vh>generate_asts_for_modules</vh></v>
<v t="ekr.20221004064035.1929"><vh>generate_stub_from_ast</vh></v>
<v t="ekr.20221004064035.1930"><vh>get_sig_generators</vh></v>
<v t="ekr.20221004064035.1931"><vh>collect_docs_signatures</vh></v>
<v t="ekr.20221004064035.1932"><vh>generate_stubs</vh></v>
<v t="ekr.20221004064035.1933"><vh>HEADER = """%(prog)s [-h] [more options, see -h]</vh></v>
<v t="ekr.20221004064035.1934"><vh>parse_options</vh></v>
<v t="ekr.20221004064035.1935"><vh>main</vh></v>
</v>
<v t="ekr.20221004064035.1936"><vh>@clean stubgenc.py</vh>
<v t="ekr.20221004064035.1937"><vh>class SignatureGenerator</vh>
<v t="ekr.20221004064035.1938"><vh>SignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20221004064035.1939"><vh>SignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20221004064035.1940"><vh>class ExternalSignatureGenerator</vh>
<v t="ekr.20221004064035.1941"><vh>ExternalSignatureGenerator.__init__</vh></v>
<v t="ekr.20221004064035.1942"><vh>ExternalSignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20221004064035.1943"><vh>ExternalSignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20221004064035.1944"><vh>class DocstringSignatureGenerator</vh>
<v t="ekr.20221004064035.1945"><vh>DocstringSignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20221004064035.1946"><vh>DocstringSignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20221004064035.1947"><vh>class FallbackSignatureGenerator</vh>
<v t="ekr.20221004064035.1948"><vh>FallbackSignatureGenerator.get_function_sig</vh></v>
<v t="ekr.20221004064035.1949"><vh>FallbackSignatureGenerator.get_method_sig</vh></v>
</v>
<v t="ekr.20221004064035.1950"><vh>generate_stub_for_c_module</vh></v>
<v t="ekr.20221004064035.1951"><vh>add_typing_import</vh></v>
<v t="ekr.20221004064035.1952"><vh>is_c_function</vh></v>
<v t="ekr.20221004064035.1953"><vh>is_c_method</vh></v>
<v t="ekr.20221004064035.1954"><vh>is_c_classmethod</vh></v>
<v t="ekr.20221004064035.1955"><vh>is_c_property</vh></v>
<v t="ekr.20221004064035.1956"><vh>is_c_property_readonly</vh></v>
<v t="ekr.20221004064035.1957"><vh>is_c_type</vh></v>
<v t="ekr.20221004064035.1958"><vh>is_pybind11_overloaded_function_docstring</vh></v>
<v t="ekr.20221004064035.1959"><vh>generate_c_function_stub</vh></v>
<v t="ekr.20221004064035.1960"><vh>strip_or_import</vh></v>
<v t="ekr.20221004064035.1961"><vh>is_static_property</vh></v>
<v t="ekr.20221004064035.1962"><vh>generate_c_property_stub</vh>
<v t="ekr.20221004064035.1963"><vh>infer_prop_type</vh></v>
</v>
<v t="ekr.20221004064035.1964"><vh>generate_c_type_stub</vh></v>
<v t="ekr.20221004064035.1965"><vh>get_type_fullname</vh></v>
<v t="ekr.20221004064035.1966"><vh>method_name_sort_key</vh></v>
<v t="ekr.20221004064035.1967"><vh>is_pybind_skipped_attribute</vh></v>
<v t="ekr.20221004064035.1968"><vh>is_skipped_attribute</vh></v>
<v t="ekr.20221004064035.1969"><vh>infer_method_sig</vh></v>
</v>
<v t="ekr.20221004064035.1970"><vh>@clean stubinfo.py</vh>
<v t="ekr.20221004064035.1971"><vh>is_legacy_bundled_package</vh></v>
<v t="ekr.20221004064035.1972"><vh>approved_stub_package_exists</vh></v>
<v t="ekr.20221004064035.1973"><vh>stub_package_name</vh></v>
</v>
<v t="ekr.20221004064035.1974"><vh>@clean stubtest.py</vh>
<v t="ekr.20221004064035.1975"><vh>class Missing</vh></v>
<v t="ekr.20221004064035.1976"><vh>MISSING: typing_extensions.Final = Missing()</vh></v>
<v t="ekr.20221004064035.1977"><vh>_style</vh></v>
<v t="ekr.20221004064035.1978"><vh>_truncate</vh></v>
<v t="ekr.20221004064035.1979"><vh>class StubtestFailure</vh></v>
<v t="ekr.20221004064035.1980"><vh>class Error</vh>
<v t="ekr.20221004064035.1981"><vh>Error.__init__</vh></v>
<v t="ekr.20221004064035.1982"><vh>Error.is_missing_stub</vh></v>
<v t="ekr.20221004064035.1983"><vh>Error.is_positional_only_related</vh></v>
<v t="ekr.20221004064035.1984"><vh>Error.get_description</vh></v>
</v>
<v t="ekr.20221004064035.1985"><vh>====================</vh></v>
<v t="ekr.20221004064035.1986"><vh>silent_import_module</vh></v>
<v t="ekr.20221004064035.1987"><vh>test_module</vh></v>
<v t="ekr.20221004064035.1988"><vh>verify</vh></v>
<v t="ekr.20221004064035.1989"><vh>_verify_exported_names</vh></v>
<v t="ekr.20221004064035.1990"><vh>verify_mypyfile</vh>
<v t="ekr.20221004064035.1991"><vh>_belongs_to_runtime</vh></v>
</v>
<v t="ekr.20221004064035.1992"><vh>_verify_final</vh></v>
<v t="ekr.20221004064035.1993"><vh>_verify_metaclass</vh></v>
<v t="ekr.20221004064035.1994"><vh>verify_typeinfo</vh></v>
<v t="ekr.20221004064035.1995"><vh>_verify_static_class_methods</vh></v>
<v t="ekr.20221004064035.1996"><vh>_verify_arg_name</vh>
<v t="ekr.20221004064035.1997"><vh>strip_prefix</vh></v>
<v t="ekr.20221004064035.1998"><vh>if strip_prefix(stub_arg.variable.name, "__") == runtime_arg.name:</vh></v>
<v t="ekr.20221004064035.1999"><vh>names_approx_match</vh></v>
</v>
<v t="ekr.20221004064035.2000"><vh>_verify_arg_default_value</vh></v>
<v t="ekr.20221004064035.2001"><vh>maybe_strip_cls</vh></v>
<v t="ekr.20221004064035.2002"><vh>class Signature</vh>
<v t="ekr.20221004064035.2003"><vh>Signature.__init__</vh></v>
<v t="ekr.20221004064035.2004"><vh>Signature.__str__</vh></v>
<v t="ekr.20221004064035.2005"><vh>Signature.from_funcitem</vh></v>
<v t="ekr.20221004064035.2006"><vh>Signature.from_inspect_signature</vh></v>
<v t="ekr.20221004064035.2007"><vh>Signature.from_overloadedfuncdef</vh></v>
</v>
<v t="ekr.20221004064035.2008"><vh>_verify_signature</vh></v>
<v t="ekr.20221004064035.2009"><vh>verify_funcitem</vh></v>
<v t="ekr.20221004064035.2010"><vh>verify_none</vh></v>
<v t="ekr.20221004064035.2011"><vh>verify_var</vh></v>
<v t="ekr.20221004064035.2012"><vh>verify_overloadedfuncdef</vh></v>
<v t="ekr.20221004064035.2013"><vh>verify_typevarexpr</vh></v>
<v t="ekr.20221004064035.2014"><vh>verify_paramspecexpr</vh></v>
<v t="ekr.20221004064035.2015"><vh>_verify_readonly_property</vh></v>
<v t="ekr.20221004064035.2016"><vh>_verify_abstract_status</vh></v>
<v t="ekr.20221004064035.2017"><vh>_resolve_funcitem_from_decorator</vh>
<v t="ekr.20221004064035.2018"><vh>apply_decorator_to_funcitem</vh></v>
</v>
<v t="ekr.20221004064035.2019"><vh>verify_decorator</vh></v>
<v t="ekr.20221004064035.2020"><vh>verify_typealias</vh></v>
<v t="ekr.20221004064035.2021"><vh>====================</vh></v>
<v t="ekr.20221004064035.2022"><vh>is_probably_private</vh></v>
<v t="ekr.20221004064035.2023"><vh>is_probably_a_function</vh></v>
<v t="ekr.20221004064035.2024"><vh>is_read_only_property</vh></v>
<v t="ekr.20221004064035.2025"><vh>safe_inspect_signature</vh></v>
<v t="ekr.20221004064035.2026"><vh>is_subtype_helper</vh></v>
<v t="ekr.20221004064035.2027"><vh>get_mypy_type_of_runtime_value</vh>
<v t="ekr.20221004064035.2028"><vh>anytype</vh></v>
</v>
<v t="ekr.20221004064035.2029"><vh>====================</vh></v>
<v t="ekr.20221004064035.2030"><vh>build_stubs</vh></v>
<v t="ekr.20221004064035.2031"><vh>get_stub</vh></v>
<v t="ekr.20221004064035.2032"><vh>get_typeshed_stdlib_modules</vh>
<v t="ekr.20221004064035.2033"><vh>exists_in_version</vh></v>
</v>
<v t="ekr.20221004064035.2034"><vh>get_allowlist_entries</vh>
<v t="ekr.20221004064035.2035"><vh>strip_comments</vh></v>
</v>
<v t="ekr.20221004064035.2036"><vh>class _Arguments</vh></v>
<v t="ekr.20221004064035.2037"><vh>test_stubs</vh></v>
<v t="ekr.20221004064035.2038"><vh>parse_options</vh></v>
<v t="ekr.20221004064035.2039"><vh>main</vh></v>
</v>
<v t="ekr.20221004064035.2040"><vh>@clean stubutil.py</vh>
<v t="ekr.20221004064035.2041"><vh>class CantImport</vh></v>
<v t="ekr.20221004064035.2042"><vh>walk_packages</vh></v>
<v t="ekr.20221004064035.2043"><vh>find_module_path_using_sys_path</vh></v>
<v t="ekr.20221004064035.2044"><vh>find_module_path_and_all_py3</vh></v>
<v t="ekr.20221004064035.2045"><vh>generate_guarded</vh></v>
<v t="ekr.20221004064035.2046"><vh>report_missing</vh></v>
<v t="ekr.20221004064035.2047"><vh>fail_missing</vh></v>
<v t="ekr.20221004064035.2048"><vh>remove_misplaced_type_comments</vh></v>
<v t="ekr.20221004064035.2049"><vh>remove_misplaced_type_comments</vh></v>
<v t="ekr.20221004064035.2050"><vh>remove_misplaced_type_comments</vh></v>
<v t="ekr.20221004064035.2051"><vh>common_dir_prefix</vh></v>
</v>
</v>
<v t="ekr.20221005081102.1"><vh>--- top-level</vh>
<v t="ekr.20221004064036.237"><vh>@clean __main__.py</vh>
<v t="ekr.20221004064036.238"><vh>console_entry</vh></v>
</v>
<v t="ekr.20221004064034.150"><vh>@clean api.py</vh>
<v t="ekr.20221004064034.151"><vh>_run</vh></v>
<v t="ekr.20221004064034.152"><vh>run</vh></v>
<v t="ekr.20221004064034.153"><vh>run_dmypy</vh>
<v t="ekr.20221004064034.154"><vh>f</vh></v>
</v>
</v>
<v t="ekr.20221004064034.196"><vh>@clean build.py</vh>
<v t="ekr.20221004064034.197"><vh>class BuildResult</vh>
<v t="ekr.20221004064034.198"><vh>BuildResult.__init__</vh></v>
</v>
<v t="ekr.20221004064034.199"><vh>build</vh>
<v t="ekr.20221004064034.200"><vh>default_flush_errors</vh></v>
</v>
<v t="ekr.20221004064034.201"><vh>_build</vh></v>
<v t="ekr.20221004064034.202"><vh>default_data_dir</vh></v>
<v t="ekr.20221004064034.203"><vh>normpath</vh></v>
<v t="ekr.20221004064034.204"><vh>class CacheMeta</vh></v>
<v t="ekr.20221004064034.205"><vh>NOTE: dependencies + suppressed == all reachable imports;</vh></v>
<v t="ekr.20221004064034.206"><vh>cache_meta_from_dict</vh></v>
<v t="ekr.20221004064034.207"><vh>Priorities used for imports.  (Here, top-level includes inside a class.)</vh></v>
<v t="ekr.20221004064034.208"><vh>import_priority</vh></v>
<v t="ekr.20221004064034.209"><vh>load_plugins_from_config</vh>
<v t="ekr.20221004064034.210"><vh>plugin_error</vh></v>
</v>
<v t="ekr.20221004064034.211"><vh>load_plugins</vh></v>
<v t="ekr.20221004064034.212"><vh>take_module_snapshot</vh></v>
<v t="ekr.20221004064034.213"><vh>find_config_file_line_number</vh></v>
<v t="ekr.20221004064034.214"><vh>class BuildManager</vh>
<v t="ekr.20221004064034.215"><vh>BuildManager.__init__</vh></v>
<v t="ekr.20221004064034.216"><vh>BuildManager.dump_stats</vh></v>
<v t="ekr.20221004064034.217"><vh>BuildManager.use_fine_grained_cache</vh></v>
<v t="ekr.20221004064034.218"><vh>BuildManager.maybe_swap_for_shadow_path</vh></v>
<v t="ekr.20221004064034.219"><vh>BuildManager.get_stat</vh></v>
<v t="ekr.20221004064034.220"><vh>BuildManager.getmtime</vh></v>
<v t="ekr.20221004064034.221"><vh>BuildManager.all_imported_modules_in_file</vh></v>
<v t="ekr.20221004064034.222"><vh>BuildManager.is_module</vh></v>
<v t="ekr.20221004064034.223"><vh>BuildManager.parse_file</vh></v>
<v t="ekr.20221004064034.224"><vh>BuildManager.load_fine_grained_deps</vh></v>
<v t="ekr.20221004064034.225"><vh>BuildManager.report_file</vh></v>
<v t="ekr.20221004064034.226"><vh>BuildManager.verbosity</vh></v>
<v t="ekr.20221004064034.227"><vh>BuildManager.log</vh></v>
<v t="ekr.20221004064034.228"><vh>BuildManager.log_fine_grained</vh></v>
<v t="ekr.20221004064034.229"><vh>BuildManager.trace</vh></v>
<v t="ekr.20221004064034.230"><vh>BuildManager.add_stats</vh></v>
<v t="ekr.20221004064034.231"><vh>BuildManager.stats_summary</vh></v>
</v>
<v t="ekr.20221004064034.232"><vh>deps_to_json</vh></v>
<v t="ekr.20221004064034.233"><vh>File for storing metadata about all the fine-grained dependency caches</vh></v>
<v t="ekr.20221004064034.234"><vh>write_deps_cache</vh></v>
<v t="ekr.20221004064034.235"><vh>invert_deps</vh></v>
<v t="ekr.20221004064034.236"><vh>generate_deps_for_cache</vh></v>
<v t="ekr.20221004064034.237"><vh>PLUGIN_SNAPSHOT_FILE: Final = "@plugins_snapshot.json"</vh></v>
<v t="ekr.20221004064034.238"><vh>write_plugins_snapshot</vh></v>
<v t="ekr.20221004064034.239"><vh>read_plugins_snapshot</vh></v>
<v t="ekr.20221004064034.240"><vh>read_quickstart_file</vh></v>
<v t="ekr.20221004064034.241"><vh>read_deps_cache</vh></v>
<v t="ekr.20221004064034.242"><vh>_load_json_file</vh></v>
<v t="ekr.20221004064034.243"><vh>_cache_dir_prefix</vh></v>
<v t="ekr.20221004064034.244"><vh>add_catch_all_gitignore</vh></v>
<v t="ekr.20221004064034.245"><vh>exclude_from_backups</vh></v>
<v t="ekr.20221004064034.246"><vh>create_metastore</vh></v>
<v t="ekr.20221004064034.247"><vh>get_cache_names</vh></v>
<v t="ekr.20221004064034.248"><vh>find_cache_meta</vh></v>
<v t="ekr.20221004064034.249"><vh>validate_meta</vh></v>
<v t="ekr.20221004064034.250"><vh>compute_hash</vh></v>
<v t="ekr.20221004064034.251"><vh>json_dumps</vh></v>
<v t="ekr.20221004064034.252"><vh>write_cache</vh></v>
<v t="ekr.20221004064034.253"><vh>delete_cache</vh></v>
<v t="ekr.20221004064034.254"><vh>"""Dependency manager.</vh></v>
<v t="ekr.20221004064034.255"><vh>class ModuleNotFound</vh></v>
<v t="ekr.20221004064034.256"><vh>class State</vh>
<v t="ekr.20221004064034.257"><vh>State.__init__</vh></v>
<v t="ekr.20221004064034.258"><vh>State.xmeta</vh></v>
<v t="ekr.20221004064034.259"><vh>State.add_ancestors</vh></v>
<v t="ekr.20221004064034.260"><vh>State.is_fresh</vh></v>
<v t="ekr.20221004064034.261"><vh>State.is_interface_fresh</vh></v>
<v t="ekr.20221004064034.262"><vh>State.mark_as_rechecked</vh></v>
<v t="ekr.20221004064034.263"><vh>State.mark_interface_stale</vh></v>
<v t="ekr.20221004064034.264"><vh>State.check_blockers</vh></v>
<v t="ekr.20221004064034.265"><vh>State.wrap_context</vh></v>
<v t="ekr.20221004064034.266"><vh>State.load_fine_grained_deps</vh></v>
<v t="ekr.20221004064034.267"><vh>State.load_tree</vh></v>
<v t="ekr.20221004064034.268"><vh>State.fix_cross_refs</vh></v>
<v t="ekr.20221004064034.269"><vh>State.Methods for processing modules from source code.</vh></v>
<v t="ekr.20221004064034.270"><vh>State.parse_file</vh></v>
<v t="ekr.20221004064034.271"><vh>State.parse_inline_configuration</vh></v>
<v t="ekr.20221004064034.272"><vh>State.semantic_analysis_pass1</vh></v>
<v t="ekr.20221004064034.273"><vh>State.add_dependency</vh></v>
<v t="ekr.20221004064034.274"><vh>State.suppress_dependency</vh></v>
<v t="ekr.20221004064034.275"><vh>State.compute_dependencies</vh></v>
<v t="ekr.20221004064034.276"><vh>State.type_check_first_pass</vh></v>
<v t="ekr.20221004064034.277"><vh>State.type_checker</vh></v>
<v t="ekr.20221004064034.278"><vh>State.type_map</vh></v>
<v t="ekr.20221004064034.279"><vh>State.type_check_second_pass</vh></v>
<v t="ekr.20221004064034.280"><vh>State.detect_partially_defined_vars</vh></v>
<v t="ekr.20221004064034.281"><vh>State.finish_passes</vh></v>
<v t="ekr.20221004064034.282"><vh>State.free_state</vh></v>
<v t="ekr.20221004064034.283"><vh>State._patch_indirect_dependencies</vh></v>
<v t="ekr.20221004064034.284"><vh>State.compute_fine_grained_deps</vh></v>
<v t="ekr.20221004064034.285"><vh>State.update_fine_grained_deps</vh></v>
<v t="ekr.20221004064034.286"><vh>State.valid_references</vh></v>
<v t="ekr.20221004064034.287"><vh>State.write_cache</vh></v>
<v t="ekr.20221004064034.288"><vh>State.verify_dependencies</vh></v>
<v t="ekr.20221004064034.289"><vh>State.dependency_priorities</vh></v>
<v t="ekr.20221004064034.290"><vh>State.dependency_lines</vh></v>
<v t="ekr.20221004064034.291"><vh>State.generate_unused_ignore_notes</vh></v>
<v t="ekr.20221004064034.292"><vh>State.generate_ignore_without_code_notes</vh></v>
</v>
<v t="ekr.20221004064034.293"><vh>Module import and diagnostic glue</vh></v>
<v t="ekr.20221004064034.294"><vh>find_module_and_diagnose</vh></v>
<v t="ekr.20221004064034.295"><vh>exist_added_packages</vh></v>
<v t="ekr.20221004064034.296"><vh>find_module_simple</vh></v>
<v t="ekr.20221004064034.297"><vh>find_module_with_reason</vh></v>
<v t="ekr.20221004064034.298"><vh>in_partial_package</vh></v>
<v t="ekr.20221004064034.299"><vh>module_not_found</vh></v>
<v t="ekr.20221004064034.300"><vh>skipping_module</vh></v>
<v t="ekr.20221004064034.301"><vh>skipping_ancestor</vh></v>
<v t="ekr.20221004064034.302"><vh>log_configuration</vh></v>
<v t="ekr.20221004064034.303"><vh>The driver</vh></v>
<v t="ekr.20221004064034.304"><vh>dispatch</vh></v>
<v t="ekr.20221004064034.305"><vh>class NodeInfo</vh>
<v t="ekr.20221004064034.306"><vh>NodeInfo.__init__</vh></v>
<v t="ekr.20221004064034.307"><vh>NodeInfo.dumps</vh></v>
</v>
<v t="ekr.20221004064034.308"><vh>dump_timing_stats</vh></v>
<v t="ekr.20221004064034.309"><vh>dump_graph</vh></v>
<v t="ekr.20221004064034.310"><vh>load_graph</vh></v>
<v t="ekr.20221004064034.311"><vh>process_graph</vh></v>
<v t="ekr.20221004064034.312"><vh>order_ascc</vh></v>
<v t="ekr.20221004064034.313"><vh>process_fresh_modules</vh></v>
<v t="ekr.20221004064034.314"><vh>process_stale_scc</vh></v>
<v t="ekr.20221004064034.315"><vh>sorted_components</vh></v>
<v t="ekr.20221004064034.316"><vh>deps_filtered</vh></v>
<v t="ekr.20221004064034.317"><vh>strongly_connected_components</vh>
<v t="ekr.20221004064034.318"><vh>dfs</vh></v>
</v>
<v t="ekr.20221004064034.319"><vh>T = TypeVar("T")</vh></v>
<v t="ekr.20221004064034.320"><vh>topsort</vh></v>
<v t="ekr.20221004064034.321"><vh>missing_stubs_file</vh></v>
<v t="ekr.20221004064034.322"><vh>record_missing_stub_packages</vh></v>
<v t="ekr.20221004064034.323"><vh>is_silent_import_module</vh></v>
</v>
<v t="ekr.20221004064034.991"><vh>@clean defaults.py</vh></v>
<v t="ekr.20221004064034.1368"><vh>@clean freetree.py</vh>
<v t="ekr.20221004064034.1369"><vh>class TreeFreer</vh></v>
<v t="ekr.20221004064034.1370"><vh>free_tree</vh></v>
</v>
<v t="ekr.20221004064035.113"><vh>@clean main.py</vh>
<v t="ekr.20221004064035.114"><vh>stat_proxy</vh></v>
<v t="ekr.20221004064035.115"><vh>main</vh></v>
<v t="ekr.20221004064035.116"><vh>run_build</vh>
<v t="ekr.20221004064035.117"><vh>flush_errors</vh></v>
</v>
<v t="ekr.20221004064035.118"><vh>show_messages</vh></v>
<v t="ekr.20221004064035.119"><vh>class AugmentedHelpFormatter</vh>
<v t="ekr.20221004064035.120"><vh>AugmentedHelpFormatter.__init__</vh></v>
<v t="ekr.20221004064035.121"><vh>AugmentedHelpFormatter._fill_text</vh></v>
</v>
<v t="ekr.20221004064035.122"><vh>Define pairs of flag prefixes with inverse meaning.</vh></v>
<v t="ekr.20221004064035.123"><vh>invert_flag_name</vh></v>
<v t="ekr.20221004064035.124"><vh>class PythonExecutableInferenceError</vh></v>
<v t="ekr.20221004064035.125"><vh>python_executable_prefix</vh></v>
<v t="ekr.20221004064035.126"><vh>_python_executable_from_version</vh></v>
<v t="ekr.20221004064035.127"><vh>infer_python_executable</vh></v>
<v t="ekr.20221004064035.128"><vh>HEADER: Final = """%(prog)s [-h] [-v] [-V] [more options; see below]</vh></v>
<v t="ekr.20221004064035.129"><vh>class CapturableArgumentParser</vh>
<v t="ekr.20221004064035.130"><vh>CapturableArgumentParser.__init__</vh></v>
<v t="ekr.20221004064035.131"><vh>CapturableArgumentParser.print_usage</vh></v>
<v t="ekr.20221004064035.132"><vh>CapturableArgumentParser.print_help</vh></v>
<v t="ekr.20221004064035.133"><vh>CapturableArgumentParser._print_message</vh></v>
<v t="ekr.20221004064035.134"><vh>CapturableArgumentParser.exit</vh></v>
<v t="ekr.20221004064035.135"><vh>CapturableArgumentParser.error</vh></v>
</v>
<v t="ekr.20221004064035.136"><vh>class CapturableVersionAction</vh>
<v t="ekr.20221004064035.137"><vh>CapturableVersionAction.__init__</vh></v>
<v t="ekr.20221004064035.138"><vh>CapturableVersionAction.__call__</vh></v>
</v>
<v t="ekr.20221004064035.139"><vh>process_options</vh>
<v t="ekr.20221004064035.140"><vh>add_invertible_flag</vh></v>
<v t="ekr.20221004064035.141"><vh>Unless otherwise specified, arguments will be parsed directly onto an</vh></v>
<v t="ekr.20221004064035.142"><vh>set_strict_flags</vh></v>
</v>
<v t="ekr.20221004064035.143"><vh>process_package_roots</vh></v>
<v t="ekr.20221004064035.144"><vh>process_cache_map</vh></v>
<v t="ekr.20221004064035.145"><vh>maybe_write_junit_xml</vh></v>
<v t="ekr.20221004064035.146"><vh>fail</vh></v>
<v t="ekr.20221004064035.147"><vh>read_types_packages_to_install</vh></v>
<v t="ekr.20221004064035.148"><vh>install_types</vh></v>
</v>
</v>
<v t="ekr.20221005080920.1"><vh>--- types</vh>
<v t="ekr.20221004064034.155"><vh>@clean applytype.py</vh>
<v t="ekr.20221004064034.156"><vh>get_target_type</vh></v>
<v t="ekr.20221004064034.157"><vh>apply_generic_arguments</vh></v>
</v>
<v t="ekr.20221004064034.195"><vh>@clean bogus_type.py</vh></v>
<v t="ekr.20221004064034.911"><vh>@clean constraints.py</vh>
<v t="ekr.20221004064034.912"><vh>class Constraint</vh>
<v t="ekr.20221004064034.913"><vh>Constraint.__init__</vh></v>
<v t="ekr.20221004064034.914"><vh>Constraint.__repr__</vh></v>
<v t="ekr.20221004064034.915"><vh>Constraint.__hash__</vh></v>
<v t="ekr.20221004064034.916"><vh>Constraint.__eq__</vh></v>
</v>
<v t="ekr.20221004064034.917"><vh>infer_constraints_for_callable</vh></v>
<v t="ekr.20221004064034.918"><vh>infer_constraints</vh></v>
<v t="ekr.20221004064034.919"><vh>_infer_constraints</vh></v>
<v t="ekr.20221004064034.920"><vh>infer_constraints_if_possible</vh></v>
<v t="ekr.20221004064034.921"><vh>select_trivial</vh></v>
<v t="ekr.20221004064034.922"><vh>merge_with_any</vh></v>
<v t="ekr.20221004064034.923"><vh>handle_recursive_union</vh></v>
<v t="ekr.20221004064034.924"><vh>any_constraints</vh></v>
<v t="ekr.20221004064034.925"><vh>filter_satisfiable</vh></v>
<v t="ekr.20221004064034.926"><vh>is_same_constraints</vh></v>
<v t="ekr.20221004064034.927"><vh>is_same_constraint</vh></v>
<v t="ekr.20221004064034.928"><vh>is_similar_constraints</vh></v>
<v t="ekr.20221004064034.929"><vh>_is_similar_constraints</vh></v>
<v t="ekr.20221004064034.930"><vh>simplify_away_incomplete_types</vh></v>
<v t="ekr.20221004064034.931"><vh>is_complete_type</vh></v>
<v t="ekr.20221004064034.932"><vh>class CompleteTypeVisitor</vh></v>
<v t="ekr.20221004064034.933"><vh>class ConstraintBuilderVisitor</vh>
<v t="ekr.20221004064034.934"><vh>ConstraintBuilderVisitor.__init__</vh></v>
<v t="ekr.20221004064034.935"><vh>ConstraintBuilderVisitor.Trivial leaf types</vh></v>
<v t="ekr.20221004064034.936"><vh>ConstraintBuilderVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064034.937"><vh>ConstraintBuilderVisitor.visit_any</vh></v>
<v t="ekr.20221004064034.938"><vh>ConstraintBuilderVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064034.939"><vh>ConstraintBuilderVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064034.940"><vh>ConstraintBuilderVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064034.941"><vh>ConstraintBuilderVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064034.942"><vh>ConstraintBuilderVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064034.943"><vh>ConstraintBuilderVisitor.Errors</vh></v>
<v t="ekr.20221004064034.944"><vh>ConstraintBuilderVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064034.945"><vh>ConstraintBuilderVisitor.Non-trivial leaf type</vh></v>
<v t="ekr.20221004064034.946"><vh>ConstraintBuilderVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064034.947"><vh>ConstraintBuilderVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064034.948"><vh>ConstraintBuilderVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064034.949"><vh>ConstraintBuilderVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064034.950"><vh>ConstraintBuilderVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064034.951"><vh>ConstraintBuilderVisitor.Non-leaf types</vh></v>
<v t="ekr.20221004064034.952"><vh>ConstraintBuilderVisitor.visit_instance</vh></v>
<v t="ekr.20221004064034.953"><vh>ConstraintBuilderVisitor.infer_constraints_from_protocol_members</vh></v>
<v t="ekr.20221004064034.954"><vh>ConstraintBuilderVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064034.955"><vh>ConstraintBuilderVisitor.infer_against_overloaded</vh></v>
<v t="ekr.20221004064034.956"><vh>ConstraintBuilderVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064034.957"><vh>ConstraintBuilderVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064034.958"><vh>ConstraintBuilderVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064034.959"><vh>ConstraintBuilderVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064034.960"><vh>ConstraintBuilderVisitor.infer_against_any</vh></v>
<v t="ekr.20221004064034.961"><vh>ConstraintBuilderVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064034.962"><vh>ConstraintBuilderVisitor.visit_type_type</vh></v>
</v>
<v t="ekr.20221004064034.963"><vh>neg_op</vh></v>
<v t="ekr.20221004064034.964"><vh>find_matching_overload_item</vh></v>
<v t="ekr.20221004064034.965"><vh>find_matching_overload_items</vh></v>
</v>
<v t="ekr.20221004064034.966"><vh>@clean copytype.py</vh>
<v t="ekr.20221004064034.967"><vh>copy_type</vh></v>
<v t="ekr.20221004064034.968"><vh>class TypeShallowCopier</vh>
<v t="ekr.20221004064034.969"><vh>TypeShallowCopier.visit_unbound_type</vh></v>
<v t="ekr.20221004064034.970"><vh>TypeShallowCopier.visit_any</vh></v>
<v t="ekr.20221004064034.971"><vh>TypeShallowCopier.visit_none_type</vh></v>
<v t="ekr.20221004064034.972"><vh>TypeShallowCopier.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064034.973"><vh>TypeShallowCopier.visit_erased_type</vh></v>
<v t="ekr.20221004064034.974"><vh>TypeShallowCopier.visit_deleted_type</vh></v>
<v t="ekr.20221004064034.975"><vh>TypeShallowCopier.visit_instance</vh></v>
<v t="ekr.20221004064034.976"><vh>TypeShallowCopier.visit_type_var</vh></v>
<v t="ekr.20221004064034.977"><vh>TypeShallowCopier.visit_param_spec</vh></v>
<v t="ekr.20221004064034.978"><vh>TypeShallowCopier.visit_parameters</vh></v>
<v t="ekr.20221004064034.979"><vh>TypeShallowCopier.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064034.980"><vh>TypeShallowCopier.visit_unpack_type</vh></v>
<v t="ekr.20221004064034.981"><vh>TypeShallowCopier.visit_partial_type</vh></v>
<v t="ekr.20221004064034.982"><vh>TypeShallowCopier.visit_callable_type</vh></v>
<v t="ekr.20221004064034.983"><vh>TypeShallowCopier.visit_tuple_type</vh></v>
<v t="ekr.20221004064034.984"><vh>TypeShallowCopier.visit_typeddict_type</vh></v>
<v t="ekr.20221004064034.985"><vh>TypeShallowCopier.visit_literal_type</vh></v>
<v t="ekr.20221004064034.986"><vh>TypeShallowCopier.visit_union_type</vh></v>
<v t="ekr.20221004064034.987"><vh>TypeShallowCopier.visit_overloaded</vh></v>
<v t="ekr.20221004064034.988"><vh>TypeShallowCopier.visit_type_type</vh></v>
<v t="ekr.20221004064034.989"><vh>TypeShallowCopier.visit_type_alias_type</vh></v>
<v t="ekr.20221004064034.990"><vh>TypeShallowCopier.copy_common</vh></v>
</v>
</v>
<v t="ekr.20221004064034.1036"><vh>@clean erasetype.py</vh>
<v t="ekr.20221004064034.1037"><vh>erase_type</vh></v>
<v t="ekr.20221004064034.1038"><vh>class EraseTypeVisitor</vh>
<v t="ekr.20221004064034.1039"><vh>EraseTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064034.1040"><vh>EraseTypeVisitor.visit_any</vh></v>
<v t="ekr.20221004064034.1041"><vh>EraseTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064034.1042"><vh>EraseTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064034.1043"><vh>EraseTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064034.1044"><vh>EraseTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064034.1045"><vh>EraseTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064034.1046"><vh>EraseTypeVisitor.visit_instance</vh></v>
<v t="ekr.20221004064034.1047"><vh>EraseTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064034.1048"><vh>EraseTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064034.1049"><vh>EraseTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064034.1050"><vh>EraseTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064034.1051"><vh>EraseTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064034.1052"><vh>EraseTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064034.1053"><vh>EraseTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064034.1054"><vh>EraseTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064034.1055"><vh>EraseTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064034.1056"><vh>EraseTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064034.1057"><vh>EraseTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064034.1058"><vh>EraseTypeVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064034.1059"><vh>EraseTypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064034.1060"><vh>erase_typevars</vh>
<v t="ekr.20221004064034.1061"><vh>erase_id</vh></v>
</v>
<v t="ekr.20221004064034.1062"><vh>replace_meta_vars</vh></v>
<v t="ekr.20221004064034.1063"><vh>class TypeVarEraser</vh>
<v t="ekr.20221004064034.1064"><vh>TypeVarEraser.__init__</vh></v>
<v t="ekr.20221004064034.1065"><vh>TypeVarEraser.visit_type_var</vh></v>
<v t="ekr.20221004064034.1066"><vh>TypeVarEraser.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064034.1067"><vh>TypeVarEraser.visit_param_spec</vh></v>
<v t="ekr.20221004064034.1068"><vh>TypeVarEraser.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064034.1069"><vh>remove_instance_last_known_values</vh></v>
<v t="ekr.20221004064034.1070"><vh>class LastKnownValueEraser</vh>
<v t="ekr.20221004064034.1071"><vh>LastKnownValueEraser.visit_instance</vh></v>
<v t="ekr.20221004064034.1072"><vh>LastKnownValueEraser.visit_type_alias_type</vh></v>
<v t="ekr.20221004064034.1073"><vh>LastKnownValueEraser.visit_union_type</vh></v>
</v>
</v>
<v t="ekr.20221004064034.1129"><vh>@clean expandtype.py</vh>
<v t="ekr.20221004064034.1130"><vh>expand_type</vh></v>
<v t="ekr.20221004064034.1131"><vh>expand_type</vh></v>
<v t="ekr.20221004064034.1132"><vh>expand_type</vh></v>
<v t="ekr.20221004064034.1133"><vh>expand_type_by_instance</vh></v>
<v t="ekr.20221004064034.1134"><vh>expand_type_by_instance</vh></v>
<v t="ekr.20221004064034.1135"><vh>expand_type_by_instance</vh></v>
<v t="ekr.20221004064034.1136"><vh>F = TypeVar("F", bound=FunctionLike)</vh></v>
<v t="ekr.20221004064034.1137"><vh>freshen_function_type_vars</vh></v>
<v t="ekr.20221004064034.1138"><vh>class ExpandTypeVisitor</vh>
<v t="ekr.20221004064034.1139"><vh>ExpandTypeVisitor.__init__</vh></v>
<v t="ekr.20221004064034.1140"><vh>ExpandTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064034.1141"><vh>ExpandTypeVisitor.visit_any</vh></v>
<v t="ekr.20221004064034.1142"><vh>ExpandTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064034.1143"><vh>ExpandTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064034.1144"><vh>ExpandTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064034.1145"><vh>ExpandTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064034.1146"><vh>ExpandTypeVisitor.visit_instance</vh></v>
<v t="ekr.20221004064034.1147"><vh>ExpandTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064034.1148"><vh>ExpandTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064034.1149"><vh>ExpandTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064034.1150"><vh>ExpandTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064034.1151"><vh>ExpandTypeVisitor.expand_unpack</vh></v>
<v t="ekr.20221004064034.1152"><vh>ExpandTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064034.1153"><vh>ExpandTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064034.1154"><vh>ExpandTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064034.1155"><vh>ExpandTypeVisitor.expand_types_with_unpack</vh></v>
<v t="ekr.20221004064034.1156"><vh>ExpandTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064034.1157"><vh>ExpandTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064034.1158"><vh>ExpandTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064034.1159"><vh>ExpandTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064034.1160"><vh>ExpandTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064034.1161"><vh>ExpandTypeVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064034.1162"><vh>ExpandTypeVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064034.1163"><vh>ExpandTypeVisitor.expand_types</vh></v>
</v>
</v>
<v t="ekr.20221004064034.1164"><vh>@clean exprtotype.py</vh>
<v t="ekr.20221004064034.1165"><vh>class TypeTranslationError</vh></v>
<v t="ekr.20221004064034.1166"><vh>_extract_argument_name</vh></v>
<v t="ekr.20221004064034.1167"><vh>expr_to_unanalyzed_type</vh></v>
</v>
<v t="ekr.20221004064034.1439"><vh>@clean infer.py</vh>
<v t="ekr.20221004064034.1440"><vh>class ArgumentInferContext</vh></v>
<v t="ekr.20221004064034.1441"><vh>infer_function_type_arguments</vh></v>
<v t="ekr.20221004064034.1442"><vh>infer_type_arguments</vh></v>
</v>
<v t="ekr.20221004064035.17"><vh>@clean join.py</vh>
<v t="ekr.20221004064035.18"><vh>class InstanceJoiner</vh>
<v t="ekr.20221004064035.19"><vh>InstanceJoiner.__init__</vh></v>
<v t="ekr.20221004064035.20"><vh>InstanceJoiner.join_instances</vh></v>
<v t="ekr.20221004064035.21"><vh>InstanceJoiner.join_instances_via_supertype</vh></v>
</v>
<v t="ekr.20221004064035.22"><vh>join_simple</vh></v>
<v t="ekr.20221004064035.23"><vh>trivial_join</vh></v>
<v t="ekr.20221004064035.24"><vh>join_types</vh></v>
<v t="ekr.20221004064035.25"><vh>class TypeJoinVisitor</vh>
<v t="ekr.20221004064035.26"><vh>TypeJoinVisitor.__init__</vh></v>
<v t="ekr.20221004064035.27"><vh>TypeJoinVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.28"><vh>TypeJoinVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064035.29"><vh>TypeJoinVisitor.visit_any</vh></v>
<v t="ekr.20221004064035.30"><vh>TypeJoinVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064035.31"><vh>TypeJoinVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.32"><vh>TypeJoinVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.33"><vh>TypeJoinVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064035.34"><vh>TypeJoinVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064035.35"><vh>TypeJoinVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064035.36"><vh>TypeJoinVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.37"><vh>TypeJoinVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.38"><vh>TypeJoinVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064035.39"><vh>TypeJoinVisitor.visit_instance</vh></v>
<v t="ekr.20221004064035.40"><vh>TypeJoinVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064035.41"><vh>TypeJoinVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064035.42"><vh>TypeJoinVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.43"><vh>TypeJoinVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.44"><vh>TypeJoinVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064035.45"><vh>TypeJoinVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064035.46"><vh>TypeJoinVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064035.47"><vh>TypeJoinVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.48"><vh>TypeJoinVisitor.join</vh></v>
<v t="ekr.20221004064035.49"><vh>TypeJoinVisitor.default</vh></v>
</v>
<v t="ekr.20221004064035.50"><vh>is_better</vh></v>
<v t="ekr.20221004064035.51"><vh>normalize_callables</vh></v>
<v t="ekr.20221004064035.52"><vh>is_similar_callables</vh></v>
<v t="ekr.20221004064035.53"><vh>join_similar_callables</vh></v>
<v t="ekr.20221004064035.54"><vh>combine_similar_callables</vh></v>
<v t="ekr.20221004064035.55"><vh>combine_arg_names</vh></v>
<v t="ekr.20221004064035.56"><vh>object_from_instance</vh></v>
<v t="ekr.20221004064035.57"><vh>object_or_any_from_type</vh></v>
<v t="ekr.20221004064035.58"><vh>join_type_list</vh></v>
<v t="ekr.20221004064035.59"><vh>unpack_callback_protocol</vh></v>
</v>
<v t="ekr.20221004064035.149"><vh>@clean maptype.py</vh>
<v t="ekr.20221004064035.150"><vh>map_instance_to_supertype</vh></v>
<v t="ekr.20221004064035.151"><vh>map_instance_to_supertypes</vh></v>
<v t="ekr.20221004064035.152"><vh>class_derivation_paths</vh></v>
<v t="ekr.20221004064035.153"><vh>map_instance_to_direct_supertypes</vh></v>
<v t="ekr.20221004064035.154"><vh>instance_to_type_environment</vh></v>
</v>
<v t="ekr.20221004064035.155"><vh>@clean meet.py</vh>
<v t="ekr.20221004064035.156"><vh>trivial_meet</vh></v>
<v t="ekr.20221004064035.157"><vh>meet_types</vh></v>
<v t="ekr.20221004064035.158"><vh>narrow_declared_type</vh></v>
<v t="ekr.20221004064035.159"><vh>get_possible_variants</vh></v>
<v t="ekr.20221004064035.160"><vh>is_enum_overlapping_union</vh></v>
<v t="ekr.20221004064035.161"><vh>is_literal_in_union</vh></v>
<v t="ekr.20221004064035.162"><vh>is_overlapping_types</vh>
<v t="ekr.20221004064035.163"><vh>_is_overlapping_types</vh></v>
<v t="ekr.20221004064035.164"><vh>We should never encounter this type.</vh></v>
<v t="ekr.20221004064035.165"><vh>is_none_typevarlike_overlap</vh></v>
<v t="ekr.20221004064035.166"><vh>if prohibit_none_typevar_overlap:</vh></v>
<v t="ekr.20221004064035.167"><vh>_type_object_overlap</vh></v>
</v>
<v t="ekr.20221004064035.168"><vh>is_overlapping_erased_types</vh></v>
<v t="ekr.20221004064035.169"><vh>are_typed_dicts_overlapping</vh></v>
<v t="ekr.20221004064035.170"><vh>are_tuples_overlapping</vh></v>
<v t="ekr.20221004064035.171"><vh>adjust_tuple</vh></v>
<v t="ekr.20221004064035.172"><vh>is_tuple</vh></v>
<v t="ekr.20221004064035.173"><vh>class TypeMeetVisitor</vh>
<v t="ekr.20221004064035.174"><vh>TypeMeetVisitor.__init__</vh></v>
<v t="ekr.20221004064035.175"><vh>TypeMeetVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.176"><vh>TypeMeetVisitor.visit_any</vh></v>
<v t="ekr.20221004064035.177"><vh>TypeMeetVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064035.178"><vh>TypeMeetVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064035.179"><vh>TypeMeetVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.180"><vh>TypeMeetVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.181"><vh>TypeMeetVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064035.182"><vh>TypeMeetVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064035.183"><vh>TypeMeetVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064035.184"><vh>TypeMeetVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.185"><vh>TypeMeetVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.186"><vh>TypeMeetVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064035.187"><vh>TypeMeetVisitor.visit_instance</vh></v>
<v t="ekr.20221004064035.188"><vh>TypeMeetVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064035.189"><vh>TypeMeetVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064035.190"><vh>TypeMeetVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.191"><vh>TypeMeetVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.192"><vh>TypeMeetVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064035.193"><vh>TypeMeetVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064035.194"><vh>TypeMeetVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064035.195"><vh>TypeMeetVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.196"><vh>TypeMeetVisitor.meet</vh></v>
<v t="ekr.20221004064035.197"><vh>TypeMeetVisitor.default</vh></v>
</v>
<v t="ekr.20221004064035.198"><vh>meet_similar_callables</vh></v>
<v t="ekr.20221004064035.199"><vh>meet_type_list</vh></v>
<v t="ekr.20221004064035.200"><vh>typed_dict_mapping_pair</vh></v>
<v t="ekr.20221004064035.201"><vh>typed_dict_mapping_overlap</vh></v>
</v>
<v t="ekr.20221004064035.480"><vh>@clean mro.py</vh>
<v t="ekr.20221004064035.481"><vh>calculate_mro</vh></v>
<v t="ekr.20221004064035.482"><vh>class MroError</vh></v>
<v t="ekr.20221004064035.483"><vh>linearize_hierarchy</vh></v>
<v t="ekr.20221004064035.484"><vh>merge</vh></v>
</v>
<v t="ekr.20221004064035.1642"><vh>@clean solve.py</vh>
<v t="ekr.20221004064035.1643"><vh>solve_constraints</vh></v>
</v>
<v t="ekr.20221004064035.2052"><vh>@clean subtypes.py</vh>
<v t="ekr.20221004064035.2053"><vh>class SubtypeContext</vh>
<v t="ekr.20221004064035.2054"><vh>SubtypeContext.__init__</vh></v>
<v t="ekr.20221004064035.2055"><vh>SubtypeContext.check_context</vh></v>
</v>
<v t="ekr.20221004064035.2056"><vh>is_subtype</vh></v>
<v t="ekr.20221004064035.2057"><vh>is_proper_subtype</vh></v>
<v t="ekr.20221004064035.2058"><vh>is_equivalent</vh></v>
<v t="ekr.20221004064035.2059"><vh>is_same_type</vh></v>
<v t="ekr.20221004064035.2060"><vh>_is_subtype</vh>
<v t="ekr.20221004064035.2061"><vh>check_item</vh></v>
</v>
<v t="ekr.20221004064035.2062"><vh>check_type_parameter</vh>
<v t="ekr.20221004064035.2063"><vh>check</vh></v>
</v>
<v t="ekr.20221004064035.2064"><vh>ignore_type_parameter</vh></v>
<v t="ekr.20221004064035.2065"><vh>class SubtypeVisitor</vh>
<v t="ekr.20221004064035.2066"><vh>SubtypeVisitor.__init__</vh></v>
<v t="ekr.20221004064035.2067"><vh>SubtypeVisitor.build_subtype_kind</vh></v>
<v t="ekr.20221004064035.2068"><vh>SubtypeVisitor._is_subtype</vh></v>
<v t="ekr.20221004064035.2069"><vh>SubtypeVisitor.visit_x(left) means: is left (which is an instance of X) a subtype of</vh></v>
<v t="ekr.20221004064035.2070"><vh>SubtypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.2071"><vh>SubtypeVisitor.visit_any</vh></v>
<v t="ekr.20221004064035.2072"><vh>SubtypeVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064035.2073"><vh>SubtypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.2074"><vh>SubtypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064035.2075"><vh>SubtypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.2076"><vh>SubtypeVisitor.visit_instance</vh></v>
<v t="ekr.20221004064035.2077"><vh>SubtypeVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064035.2078"><vh>SubtypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064035.2079"><vh>SubtypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.2080"><vh>SubtypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.2081"><vh>SubtypeVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064035.2082"><vh>SubtypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064035.2083"><vh>SubtypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.2084"><vh>SubtypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.2085"><vh>SubtypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064035.2086"><vh>SubtypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064035.2087"><vh>SubtypeVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064035.2088"><vh>SubtypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064035.2089"><vh>SubtypeVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064035.2090"><vh>SubtypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064035.2091"><vh>T = TypeVar("T", bound=Type)</vh></v>
<v t="ekr.20221004064035.2092"><vh>pop_on_exit</vh></v>
<v t="ekr.20221004064035.2093"><vh>is_protocol_implementation</vh></v>
<v t="ekr.20221004064035.2094"><vh>find_member</vh></v>
<v t="ekr.20221004064035.2095"><vh>get_member_flags</vh></v>
<v t="ekr.20221004064035.2096"><vh>find_node_type</vh></v>
<v t="ekr.20221004064035.2097"><vh>non_method_protocol_members</vh></v>
<v t="ekr.20221004064035.2098"><vh>is_callable_compatible</vh></v>
<v t="ekr.20221004064035.2099"><vh>are_parameters_compatible</vh>
<v t="ekr.20221004064035.2100"><vh>_incompatible</vh></v>
</v>
<v t="ekr.20221004064035.2101"><vh>are_args_compatible</vh>
<v t="ekr.20221004064035.2102"><vh>is_different</vh></v>
</v>
<v t="ekr.20221004064035.2103"><vh>flip_compat_check</vh></v>
<v t="ekr.20221004064035.2104"><vh>unify_generic_callable</vh>
<v t="ekr.20221004064035.2105"><vh>report</vh></v>
</v>
<v t="ekr.20221004064035.2106"><vh>try_restrict_literal_union</vh></v>
<v t="ekr.20221004064035.2107"><vh>restrict_subtype_away</vh></v>
<v t="ekr.20221004064035.2108"><vh>covers_at_runtime</vh></v>
<v t="ekr.20221004064035.2109"><vh>is_more_precise</vh></v>
</v>
<v t="ekr.20221004064035.2110"><vh>@clean suggestions.py</vh>
<v t="ekr.20221004064035.2111"><vh>class PyAnnotateSignature</vh></v>
<v t="ekr.20221004064035.2112"><vh>class Callsite</vh></v>
<v t="ekr.20221004064035.2113"><vh>class SuggestionPlugin</vh>
<v t="ekr.20221004064035.2114"><vh>SuggestionPlugin.__init__</vh></v>
<v t="ekr.20221004064035.2115"><vh>SuggestionPlugin.get_function_hook</vh></v>
<v t="ekr.20221004064035.2116"><vh>SuggestionPlugin.get_method_hook</vh></v>
<v t="ekr.20221004064035.2117"><vh>SuggestionPlugin.log</vh></v>
</v>
<v t="ekr.20221004064035.2118"><vh>class ReturnFinder</vh>
<v t="ekr.20221004064035.2119"><vh>ReturnFinder.__init__</vh></v>
<v t="ekr.20221004064035.2120"><vh>ReturnFinder.visit_return_stmt</vh></v>
<v t="ekr.20221004064035.2121"><vh>ReturnFinder.visit_func_def</vh></v>
</v>
<v t="ekr.20221004064035.2122"><vh>get_return_types</vh></v>
<v t="ekr.20221004064035.2123"><vh>class ArgUseFinder</vh>
<v t="ekr.20221004064035.2124"><vh>ArgUseFinder.__init__</vh></v>
<v t="ekr.20221004064035.2125"><vh>ArgUseFinder.visit_call_expr</vh></v>
</v>
<v t="ekr.20221004064035.2126"><vh>get_arg_uses</vh></v>
<v t="ekr.20221004064035.2127"><vh>class SuggestionFailure</vh></v>
<v t="ekr.20221004064035.2128"><vh>is_explicit_any</vh></v>
<v t="ekr.20221004064035.2129"><vh>is_implicit_any</vh></v>
<v t="ekr.20221004064035.2130"><vh>class SuggestionEngine</vh>
<v t="ekr.20221004064035.2131"><vh>SuggestionEngine.__init__</vh></v>
<v t="ekr.20221004064035.2132"><vh>SuggestionEngine.suggest</vh></v>
<v t="ekr.20221004064035.2133"><vh>SuggestionEngine.suggest_callsites</vh></v>
<v t="ekr.20221004064035.2134"><vh>SuggestionEngine.restore_after</vh></v>
<v t="ekr.20221004064035.2135"><vh>SuggestionEngine.with_export_types</vh></v>
<v t="ekr.20221004064035.2136"><vh>SuggestionEngine.get_trivial_type</vh></v>
<v t="ekr.20221004064035.2137"><vh>SuggestionEngine.get_starting_type</vh></v>
<v t="ekr.20221004064035.2138"><vh>SuggestionEngine.get_args</vh></v>
<v t="ekr.20221004064035.2139"><vh>SuggestionEngine.get_default_arg_types</vh></v>
<v t="ekr.20221004064035.2140"><vh>SuggestionEngine.get_guesses</vh></v>
<v t="ekr.20221004064035.2141"><vh>SuggestionEngine.get_callsites</vh></v>
<v t="ekr.20221004064035.2142"><vh>SuggestionEngine.filter_options</vh></v>
<v t="ekr.20221004064035.2143"><vh>SuggestionEngine.find_best</vh></v>
<v t="ekr.20221004064035.2144"><vh>SuggestionEngine.get_guesses_from_parent</vh></v>
<v t="ekr.20221004064035.2145"><vh>SuggestionEngine.get_suggestion</vh></v>
<v t="ekr.20221004064035.2146"><vh>SuggestionEngine.format_args</vh></v>
<v t="ekr.20221004064035.2147"><vh>SuggestionEngine.find_node</vh></v>
<v t="ekr.20221004064035.2148"><vh>SuggestionEngine.find_node_by_module_and_name</vh></v>
<v t="ekr.20221004064035.2149"><vh>SuggestionEngine.find_node_by_file_and_line</vh></v>
<v t="ekr.20221004064035.2150"><vh>SuggestionEngine.extract_from_decorator</vh></v>
<v t="ekr.20221004064035.2151"><vh>SuggestionEngine.try_type</vh></v>
<v t="ekr.20221004064035.2152"><vh>SuggestionEngine.reload</vh></v>
<v t="ekr.20221004064035.2153"><vh>SuggestionEngine.ensure_loaded</vh></v>
<v t="ekr.20221004064035.2154"><vh>SuggestionEngine.named_type</vh></v>
<v t="ekr.20221004064035.2155"><vh>SuggestionEngine.json_suggestion</vh></v>
<v t="ekr.20221004064035.2156"><vh>SuggestionEngine.pyannotate_signature</vh></v>
<v t="ekr.20221004064035.2157"><vh>SuggestionEngine.format_signature</vh></v>
<v t="ekr.20221004064035.2158"><vh>SuggestionEngine.format_type</vh></v>
<v t="ekr.20221004064035.2159"><vh>SuggestionEngine.score_type</vh></v>
<v t="ekr.20221004064035.2160"><vh>SuggestionEngine.score_callable</vh></v>
</v>
<v t="ekr.20221004064035.2161"><vh>any_score_type</vh></v>
<v t="ekr.20221004064035.2162"><vh>any_score_callable</vh></v>
<v t="ekr.20221004064035.2163"><vh>is_tricky_callable</vh></v>
<v t="ekr.20221004064035.2164"><vh>class TypeFormatter</vh>
<v t="ekr.20221004064035.2165"><vh>TypeFormatter.__init__</vh></v>
<v t="ekr.20221004064035.2166"><vh>TypeFormatter.visit_any</vh></v>
<v t="ekr.20221004064035.2167"><vh>TypeFormatter.visit_instance</vh></v>
<v t="ekr.20221004064035.2168"><vh>TypeFormatter.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.2169"><vh>TypeFormatter.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.2170"><vh>TypeFormatter.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.2171"><vh>TypeFormatter.visit_union_type</vh></v>
<v t="ekr.20221004064035.2172"><vh>TypeFormatter.visit_callable_type</vh></v>
</v>
<v t="ekr.20221004064035.2173"><vh>TType = TypeVar("TType", bound=Type)</vh></v>
<v t="ekr.20221004064035.2174"><vh>make_suggestion_anys</vh></v>
<v t="ekr.20221004064035.2175"><vh>class MakeSuggestionAny</vh>
<v t="ekr.20221004064035.2176"><vh>MakeSuggestionAny.visit_any</vh></v>
<v t="ekr.20221004064035.2177"><vh>MakeSuggestionAny.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064035.2178"><vh>generate_type_combinations</vh></v>
<v t="ekr.20221004064035.2179"><vh>count_errors</vh></v>
<v t="ekr.20221004064035.2180"><vh>refine_type</vh></v>
<v t="ekr.20221004064035.2181"><vh>refine_union</vh></v>
<v t="ekr.20221004064035.2182"><vh>refine_callable</vh></v>
<v t="ekr.20221004064035.2183"><vh>T = TypeVar("T")</vh></v>
<v t="ekr.20221004064035.2184"><vh>dedup</vh></v>
</v>
<v t="ekr.20221004064035.2459"><vh>@clean tvar_scope.py</vh>
<v t="ekr.20221004064035.2460"><vh>class TypeVarLikeScope</vh>
<v t="ekr.20221004064035.2461"><vh>TypeVarLikeScope.__init__</vh></v>
<v t="ekr.20221004064035.2462"><vh>TypeVarLikeScope.get_function_scope</vh></v>
<v t="ekr.20221004064035.2463"><vh>TypeVarLikeScope.allow_binding</vh></v>
<v t="ekr.20221004064035.2464"><vh>TypeVarLikeScope.method_frame</vh></v>
<v t="ekr.20221004064035.2465"><vh>TypeVarLikeScope.class_frame</vh></v>
<v t="ekr.20221004064035.2466"><vh>TypeVarLikeScope.new_unique_func_id</vh></v>
<v t="ekr.20221004064035.2467"><vh>TypeVarLikeScope.bind_new</vh></v>
<v t="ekr.20221004064035.2468"><vh>TypeVarLikeScope.bind_existing</vh></v>
<v t="ekr.20221004064035.2469"><vh>TypeVarLikeScope.get_binding</vh></v>
<v t="ekr.20221004064035.2470"><vh>TypeVarLikeScope.__str__</vh></v>
</v>
</v>
<v t="ekr.20221004064035.3048"><vh>@clean type_visitor.py</vh>
<v t="ekr.20221004064035.3049"><vh>class TypeVisitor</vh>
<v t="ekr.20221004064035.3050"><vh>TypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.3051"><vh>TypeVisitor.visit_any</vh></v>
<v t="ekr.20221004064035.3052"><vh>TypeVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064035.3053"><vh>TypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.3054"><vh>TypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064035.3055"><vh>TypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.3056"><vh>TypeVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064035.3057"><vh>TypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064035.3058"><vh>TypeVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064035.3059"><vh>TypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.3060"><vh>TypeVisitor.visit_instance</vh></v>
<v t="ekr.20221004064035.3061"><vh>TypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064035.3062"><vh>TypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064035.3063"><vh>TypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.3064"><vh>TypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.3065"><vh>TypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064035.3066"><vh>TypeVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064035.3067"><vh>TypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064035.3068"><vh>TypeVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064035.3069"><vh>TypeVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.3070"><vh>TypeVisitor.visit_unpack_type</vh></v>
</v>
<v t="ekr.20221004064035.3071"><vh>class SyntheticTypeVisitor</vh>
<v t="ekr.20221004064035.3072"><vh>SyntheticTypeVisitor.visit_star_type</vh></v>
<v t="ekr.20221004064035.3073"><vh>SyntheticTypeVisitor.visit_type_list</vh></v>
<v t="ekr.20221004064035.3074"><vh>SyntheticTypeVisitor.visit_callable_argument</vh></v>
<v t="ekr.20221004064035.3075"><vh>SyntheticTypeVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20221004064035.3076"><vh>SyntheticTypeVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20221004064035.3077"><vh>SyntheticTypeVisitor.visit_placeholder_type</vh></v>
</v>
<v t="ekr.20221004064035.3078"><vh>class TypeTranslator</vh>
<v t="ekr.20221004064035.3079"><vh>TypeTranslator.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.3080"><vh>TypeTranslator.visit_any</vh></v>
<v t="ekr.20221004064035.3081"><vh>TypeTranslator.visit_none_type</vh></v>
<v t="ekr.20221004064035.3082"><vh>TypeTranslator.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.3083"><vh>TypeTranslator.visit_erased_type</vh></v>
<v t="ekr.20221004064035.3084"><vh>TypeTranslator.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.3085"><vh>TypeTranslator.visit_instance</vh></v>
<v t="ekr.20221004064035.3086"><vh>TypeTranslator.visit_type_var</vh></v>
<v t="ekr.20221004064035.3087"><vh>TypeTranslator.visit_param_spec</vh></v>
<v t="ekr.20221004064035.3088"><vh>TypeTranslator.visit_parameters</vh></v>
<v t="ekr.20221004064035.3089"><vh>TypeTranslator.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.3090"><vh>TypeTranslator.visit_partial_type</vh></v>
<v t="ekr.20221004064035.3091"><vh>TypeTranslator.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.3092"><vh>TypeTranslator.visit_callable_type</vh></v>
<v t="ekr.20221004064035.3093"><vh>TypeTranslator.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.3094"><vh>TypeTranslator.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.3095"><vh>TypeTranslator.visit_literal_type</vh></v>
<v t="ekr.20221004064035.3096"><vh>TypeTranslator.visit_union_type</vh></v>
<v t="ekr.20221004064035.3097"><vh>TypeTranslator.translate_types</vh></v>
<v t="ekr.20221004064035.3098"><vh>TypeTranslator.translate_variables</vh></v>
<v t="ekr.20221004064035.3099"><vh>TypeTranslator.visit_overloaded</vh></v>
<v t="ekr.20221004064035.3100"><vh>TypeTranslator.visit_type_type</vh></v>
<v t="ekr.20221004064035.3101"><vh>TypeTranslator.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064035.3102"><vh>class TypeQuery</vh>
<v t="ekr.20221004064035.3103"><vh>TypeQuery.__init__</vh></v>
<v t="ekr.20221004064035.3104"><vh>TypeQuery.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.3105"><vh>TypeQuery.visit_type_list</vh></v>
<v t="ekr.20221004064035.3106"><vh>TypeQuery.visit_callable_argument</vh></v>
<v t="ekr.20221004064035.3107"><vh>TypeQuery.visit_any</vh></v>
<v t="ekr.20221004064035.3108"><vh>TypeQuery.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.3109"><vh>TypeQuery.visit_none_type</vh></v>
<v t="ekr.20221004064035.3110"><vh>TypeQuery.visit_erased_type</vh></v>
<v t="ekr.20221004064035.3111"><vh>TypeQuery.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.3112"><vh>TypeQuery.visit_type_var</vh></v>
<v t="ekr.20221004064035.3113"><vh>TypeQuery.visit_param_spec</vh></v>
<v t="ekr.20221004064035.3114"><vh>TypeQuery.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.3115"><vh>TypeQuery.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.3116"><vh>TypeQuery.visit_parameters</vh></v>
<v t="ekr.20221004064035.3117"><vh>TypeQuery.visit_partial_type</vh></v>
<v t="ekr.20221004064035.3118"><vh>TypeQuery.visit_instance</vh></v>
<v t="ekr.20221004064035.3119"><vh>TypeQuery.visit_callable_type</vh></v>
<v t="ekr.20221004064035.3120"><vh>TypeQuery.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.3121"><vh>TypeQuery.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.3122"><vh>TypeQuery.visit_raw_expression_type</vh></v>
<v t="ekr.20221004064035.3123"><vh>TypeQuery.visit_literal_type</vh></v>
<v t="ekr.20221004064036.1"><vh>TypeQuery.visit_star_type</vh></v>
<v t="ekr.20221004064036.2"><vh>TypeQuery.visit_union_type</vh></v>
<v t="ekr.20221004064036.3"><vh>TypeQuery.visit_overloaded</vh></v>
<v t="ekr.20221004064036.4"><vh>TypeQuery.visit_type_type</vh></v>
<v t="ekr.20221004064036.5"><vh>TypeQuery.visit_ellipsis_type</vh></v>
<v t="ekr.20221004064036.6"><vh>TypeQuery.visit_placeholder_type</vh></v>
<v t="ekr.20221004064036.7"><vh>TypeQuery.visit_type_alias_type</vh></v>
<v t="ekr.20221004064036.8"><vh>TypeQuery.query_types</vh></v>
</v>
</v>
<v t="ekr.20221004064035.2471"><vh>@clean typeanal.py</vh>
<v t="ekr.20221005053958.1"><vh>&lt;&lt; typeanal imports &gt;&gt;</vh></v>
<v t="ekr.20221005054114.1"><vh>&lt;&lt; typeanal data &gt;&gt;</vh></v>
<v t="ekr.20221004064035.2472"><vh>analyze_type_alias</vh></v>
<v t="ekr.20221004064035.2473"><vh>no_subscript_builtin_alias</vh></v>
<v t="ekr.20221004064035.2474"><vh>class TypeAnalyser</vh>
<v t="ekr.20221004064035.2475"><vh>TypeAnalyser.__init__</vh></v>
<v t="ekr.20221004064035.2476"><vh>TypeAnalyser.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.2477"><vh>TypeAnalyser.visit_unbound_type_nonoptional</vh></v>
<v t="ekr.20221004064035.2478"><vh>TypeAnalyser.cannot_resolve_type</vh></v>
<v t="ekr.20221004064035.2479"><vh>TypeAnalyser.apply_concatenate_operator</vh></v>
<v t="ekr.20221004064035.2480"><vh>TypeAnalyser.try_analyze_special_unbound_type</vh></v>
<v t="ekr.20221004064035.2481"><vh>TypeAnalyser.get_omitted_any</vh></v>
<v t="ekr.20221004064035.2482"><vh>TypeAnalyser.analyze_type_with_type_info</vh></v>
<v t="ekr.20221004064035.2483"><vh>TypeAnalyser.analyze_unbound_type_without_type_info</vh></v>
<v t="ekr.20221004064035.2484"><vh>TypeAnalyser.visit_any</vh></v>
<v t="ekr.20221004064035.2485"><vh>TypeAnalyser.visit_none_type</vh></v>
<v t="ekr.20221004064035.2486"><vh>TypeAnalyser.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.2487"><vh>TypeAnalyser.visit_erased_type</vh></v>
<v t="ekr.20221004064035.2488"><vh>TypeAnalyser.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.2489"><vh>TypeAnalyser.visit_type_list</vh></v>
<v t="ekr.20221004064035.2490"><vh>TypeAnalyser.visit_callable_argument</vh></v>
<v t="ekr.20221004064035.2491"><vh>TypeAnalyser.visit_instance</vh></v>
<v t="ekr.20221004064035.2492"><vh>TypeAnalyser.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.2493"><vh>TypeAnalyser.visit_type_var</vh></v>
<v t="ekr.20221004064035.2494"><vh>TypeAnalyser.visit_param_spec</vh></v>
<v t="ekr.20221004064035.2495"><vh>TypeAnalyser.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.2496"><vh>TypeAnalyser.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.2497"><vh>TypeAnalyser.visit_parameters</vh></v>
<v t="ekr.20221004064035.2498"><vh>TypeAnalyser.visit_callable_type</vh></v>
<v t="ekr.20221004064035.2499"><vh>TypeAnalyser.anal_type_guard</vh></v>
<v t="ekr.20221004064035.2500"><vh>TypeAnalyser.anal_type_guard_arg</vh></v>
<v t="ekr.20221004064035.2501"><vh>TypeAnalyser.anal_star_arg_type</vh></v>
<v t="ekr.20221004064035.2502"><vh>TypeAnalyser.visit_overloaded</vh></v>
<v t="ekr.20221004064035.2503"><vh>TypeAnalyser.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.2504"><vh>TypeAnalyser.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.2505"><vh>TypeAnalyser.visit_raw_expression_type</vh></v>
<v t="ekr.20221004064035.2506"><vh>TypeAnalyser.visit_literal_type</vh></v>
<v t="ekr.20221004064035.2507"><vh>TypeAnalyser.visit_star_type</vh></v>
<v t="ekr.20221004064035.2508"><vh>TypeAnalyser.visit_union_type</vh></v>
<v t="ekr.20221004064035.2509"><vh>TypeAnalyser.visit_partial_type</vh></v>
<v t="ekr.20221004064035.2510"><vh>TypeAnalyser.visit_ellipsis_type</vh></v>
<v t="ekr.20221004064035.2511"><vh>TypeAnalyser.visit_type_type</vh></v>
<v t="ekr.20221004064035.2512"><vh>TypeAnalyser.visit_placeholder_type</vh></v>
<v t="ekr.20221004064035.2513"><vh>TypeAnalyser.analyze_callable_args_for_paramspec</vh></v>
<v t="ekr.20221004064035.2514"><vh>TypeAnalyser.analyze_callable_args_for_concatenate</vh></v>
<v t="ekr.20221004064035.2515"><vh>TypeAnalyser.analyze_callable_type</vh></v>
<v t="ekr.20221004064035.2516"><vh>TypeAnalyser.analyze_callable_args</vh></v>
<v t="ekr.20221004064035.2517"><vh>TypeAnalyser.analyze_literal_type</vh></v>
<v t="ekr.20221004064035.2518"><vh>TypeAnalyser.analyze_literal_param</vh></v>
<v t="ekr.20221004064035.2519"><vh>TypeAnalyser.analyze_type</vh></v>
<v t="ekr.20221004064035.2520"><vh>TypeAnalyser.fail</vh></v>
<v t="ekr.20221004064035.2521"><vh>TypeAnalyser.note</vh></v>
<v t="ekr.20221004064035.2522"><vh>TypeAnalyser.tvar_scope_frame</vh></v>
<v t="ekr.20221004064035.2523"><vh>TypeAnalyser.infer_type_variables</vh></v>
<v t="ekr.20221004064035.2524"><vh>TypeAnalyser.bind_function_type_variables</vh></v>
<v t="ekr.20221004064035.2525"><vh>TypeAnalyser.is_defined_type_var</vh></v>
<v t="ekr.20221004064035.2526"><vh>TypeAnalyser.anal_array</vh></v>
<v t="ekr.20221004064035.2527"><vh>TypeAnalyser.anal_type</vh></v>
<v t="ekr.20221004064035.2528"><vh>TypeAnalyser.anal_var_def</vh></v>
<v t="ekr.20221004064035.2529"><vh>TypeAnalyser.anal_var_defs</vh></v>
<v t="ekr.20221004064035.2530"><vh>TypeAnalyser.named_type</vh></v>
<v t="ekr.20221004064035.2531"><vh>TypeAnalyser.tuple_type</vh></v>
<v t="ekr.20221004064035.2532"><vh>TypeAnalyser.set_allow_param_spec_literals</vh></v>
</v>
<v t="ekr.20221004064035.2533"><vh>TypeVarLikeList = List[Tuple[str, TypeVarLikeExpr]]</vh></v>
<v t="ekr.20221004064035.2534"><vh>class MsgCallback</vh>
<v t="ekr.20221004064035.2535"><vh>MsgCallback.__call__</vh></v>
</v>
<v t="ekr.20221004064035.2536"><vh>get_omitted_any</vh></v>
<v t="ekr.20221004064035.2537"><vh>fix_instance</vh></v>
<v t="ekr.20221004064035.2538"><vh>expand_type_alias</vh></v>
<v t="ekr.20221004064035.2539"><vh>set_any_tvars</vh></v>
<v t="ekr.20221004064035.2540"><vh>remove_dups</vh></v>
<v t="ekr.20221004064035.2541"><vh>flatten_tvars</vh></v>
<v t="ekr.20221004064035.2542"><vh>class TypeVarLikeQuery</vh>
<v t="ekr.20221004064035.2543"><vh>TypeVarLikeQuery.__init__</vh></v>
<v t="ekr.20221004064035.2544"><vh>TypeVarLikeQuery._seems_like_callable</vh></v>
<v t="ekr.20221004064035.2545"><vh>TypeVarLikeQuery.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.2546"><vh>TypeVarLikeQuery.visit_callable_type</vh></v>
</v>
<v t="ekr.20221004064035.2547"><vh>class DivergingAliasDetector</vh>
<v t="ekr.20221004064035.2548"><vh>DivergingAliasDetector.__init__</vh></v>
<v t="ekr.20221004064035.2549"><vh>DivergingAliasDetector.is_alias_tvar</vh></v>
<v t="ekr.20221004064035.2550"><vh>DivergingAliasDetector.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20221004064035.2551"><vh>detect_diverging_alias</vh></v>
<v t="ekr.20221004064035.2552"><vh>check_for_explicit_any</vh></v>
<v t="ekr.20221004064035.2553"><vh>has_explicit_any</vh></v>
<v t="ekr.20221004064035.2554"><vh>class HasExplicitAny</vh>
<v t="ekr.20221004064035.2555"><vh>HasExplicitAny.__init__</vh></v>
<v t="ekr.20221004064035.2556"><vh>HasExplicitAny.visit_any</vh></v>
<v t="ekr.20221004064035.2557"><vh>HasExplicitAny.visit_typeddict_type</vh></v>
</v>
<v t="ekr.20221004064035.2558"><vh>has_any_from_unimported_type</vh></v>
<v t="ekr.20221004064035.2559"><vh>class HasAnyFromUnimportedType</vh>
<v t="ekr.20221004064035.2560"><vh>HasAnyFromUnimportedType.__init__</vh></v>
<v t="ekr.20221004064035.2561"><vh>HasAnyFromUnimportedType.visit_any</vh></v>
<v t="ekr.20221004064035.2562"><vh>HasAnyFromUnimportedType.visit_typeddict_type</vh></v>
</v>
<v t="ekr.20221004064035.2563"><vh>collect_all_inner_types</vh></v>
<v t="ekr.20221004064035.2564"><vh>class CollectAllInnerTypesQuery</vh>
<v t="ekr.20221004064035.2565"><vh>CollectAllInnerTypesQuery.__init__</vh></v>
<v t="ekr.20221004064035.2566"><vh>CollectAllInnerTypesQuery.query_types</vh></v>
<v t="ekr.20221004064035.2567"><vh>CollectAllInnerTypesQuery.combine_lists_strategy</vh></v>
</v>
<v t="ekr.20221004064035.2568"><vh>make_optional_type</vh></v>
<v t="ekr.20221004064035.2569"><vh>fix_instance_types</vh></v>
<v t="ekr.20221004064035.2570"><vh>class InstanceFixer</vh>
<v t="ekr.20221004064035.2571"><vh>InstanceFixer.__init__</vh></v>
<v t="ekr.20221004064035.2572"><vh>InstanceFixer.visit_instance</vh></v>
</v>
</v>
<v t="ekr.20221004064035.2573"><vh>@clean typeops.py</vh>
<v t="ekr.20221004064035.2574"><vh>is_recursive_pair</vh></v>
<v t="ekr.20221004064035.2575"><vh>tuple_fallback</vh></v>
<v t="ekr.20221004064035.2576"><vh>get_self_type</vh></v>
<v t="ekr.20221004064035.2577"><vh>type_object_type_from_function</vh></v>
<v t="ekr.20221004064035.2578"><vh>class_callable</vh></v>
<v t="ekr.20221004064035.2579"><vh>map_type_from_supertype</vh></v>
<v t="ekr.20221004064035.2580"><vh>supported_self_type</vh></v>
<v t="ekr.20221004064035.2581"><vh>F = TypeVar("F", bound=FunctionLike)</vh></v>
<v t="ekr.20221004064035.2582"><vh>bind_self</vh></v>
<v t="ekr.20221004064035.2583"><vh>erase_to_bound</vh></v>
<v t="ekr.20221004064035.2584"><vh>callable_corresponding_argument</vh></v>
<v t="ekr.20221004064035.2585"><vh>simple_literal_value_key</vh></v>
<v t="ekr.20221004064035.2586"><vh>simple_literal_type</vh></v>
<v t="ekr.20221004064035.2587"><vh>is_simple_literal</vh></v>
<v t="ekr.20221004064035.2588"><vh>make_simplified_union</vh></v>
<v t="ekr.20221004064035.2589"><vh>_remove_redundant_union_items</vh></v>
<v t="ekr.20221004064035.2590"><vh>_get_type_special_method_bool_ret_type</vh></v>
<v t="ekr.20221004064035.2591"><vh>true_only</vh></v>
<v t="ekr.20221004064035.2592"><vh>false_only</vh></v>
<v t="ekr.20221004064035.2593"><vh>true_or_false</vh></v>
<v t="ekr.20221004064035.2594"><vh>erase_def_to_union_or_bound</vh></v>
<v t="ekr.20221004064035.2595"><vh>erase_to_union_or_bound</vh></v>
<v t="ekr.20221004064035.2596"><vh>function_type</vh></v>
<v t="ekr.20221004064035.2597"><vh>callable_type</vh></v>
<v t="ekr.20221004064035.2598"><vh>try_getting_str_literals</vh></v>
<v t="ekr.20221004064035.2599"><vh>try_getting_str_literals_from_type</vh></v>
<v t="ekr.20221004064035.2600"><vh>try_getting_int_literals_from_type</vh></v>
<v t="ekr.20221004064035.2601"><vh>T = TypeVar("T")</vh></v>
<v t="ekr.20221004064035.2602"><vh>try_getting_literals_from_type</vh></v>
<v t="ekr.20221004064035.2603"><vh>is_literal_type_like</vh></v>
<v t="ekr.20221004064035.2604"><vh>is_singleton_type</vh></v>
<v t="ekr.20221004064035.2605"><vh>try_expanding_sum_type_to_union</vh></v>
<v t="ekr.20221004064035.2606"><vh>try_contracting_literals_in_union</vh></v>
<v t="ekr.20221004064035.2607"><vh>coerce_to_literal</vh></v>
<v t="ekr.20221004064035.2608"><vh>get_type_vars</vh></v>
<v t="ekr.20221004064035.2609"><vh>class TypeVarExtractor</vh>
<v t="ekr.20221004064035.2610"><vh>TypeVarExtractor.__init__</vh></v>
<v t="ekr.20221004064035.2611"><vh>TypeVarExtractor._merge</vh></v>
<v t="ekr.20221004064035.2612"><vh>TypeVarExtractor.visit_type_var</vh></v>
</v>
<v t="ekr.20221004064035.2613"><vh>custom_special_method</vh></v>
<v t="ekr.20221004064035.2614"><vh>is_redundant_literal_instance</vh></v>
<v t="ekr.20221004064035.2615"><vh>separate_union_literals</vh></v>
<v t="ekr.20221004064035.2616"><vh>try_getting_instance_fallback</vh></v>
</v>
<v t="ekr.20221004064035.2617"><vh>@clean types.py</vh>
<v t="ekr.20221004064035.2618"><vh>class TypeOfAny</vh></v>
<v t="ekr.20221004064035.2619"><vh>deserialize_type</vh></v>
<v t="ekr.20221004064035.2620"><vh>class Type</vh>
<v t="ekr.20221004064035.2621"><vh>Type.__init__</vh></v>
<v t="ekr.20221004064035.2622"><vh>Type.can_be_true_default</vh></v>
<v t="ekr.20221004064035.2623"><vh>Type.can_be_false_default</vh></v>
<v t="ekr.20221004064035.2624"><vh>Type.accept</vh></v>
<v t="ekr.20221004064035.2625"><vh>Type.__repr__</vh></v>
<v t="ekr.20221004064035.2626"><vh>Type.serialize</vh></v>
<v t="ekr.20221004064035.2627"><vh>Type.deserialize</vh></v>
<v t="ekr.20221004064035.2628"><vh>Type.is_singleton_type</vh></v>
</v>
<v t="ekr.20221004064035.2629"><vh>class TypeAliasType</vh>
<v t="ekr.20221004064035.2630"><vh>TypeAliasType.__init__</vh></v>
<v t="ekr.20221004064035.2631"><vh>TypeAliasType._expand_once</vh></v>
<v t="ekr.20221004064035.2632"><vh>TypeAliasType._partial_expansion</vh></v>
<v t="ekr.20221004064035.2633"><vh>TypeAliasType.expand_all_if_possible</vh></v>
<v t="ekr.20221004064035.2634"><vh>TypeAliasType.is_recursive</vh></v>
<v t="ekr.20221004064035.2635"><vh>TypeAliasType.can_be_true_default</vh></v>
<v t="ekr.20221004064035.2636"><vh>TypeAliasType.can_be_false_default</vh></v>
<v t="ekr.20221004064035.2637"><vh>TypeAliasType.accept</vh></v>
<v t="ekr.20221004064035.2638"><vh>TypeAliasType.__hash__</vh></v>
<v t="ekr.20221004064035.2639"><vh>TypeAliasType.__eq__</vh></v>
<v t="ekr.20221004064035.2640"><vh>TypeAliasType.serialize</vh></v>
<v t="ekr.20221004064035.2641"><vh>TypeAliasType.deserialize</vh></v>
<v t="ekr.20221004064035.2642"><vh>TypeAliasType.copy_modified</vh></v>
</v>
<v t="ekr.20221004064035.2643"><vh>class TypeGuardedType</vh>
<v t="ekr.20221004064035.2644"><vh>TypeGuardedType.__init__</vh></v>
<v t="ekr.20221004064035.2645"><vh>TypeGuardedType.__repr__</vh></v>
</v>
<v t="ekr.20221004064035.2646"><vh>class RequiredType</vh>
<v t="ekr.20221004064035.2647"><vh>RequiredType.__init__</vh></v>
<v t="ekr.20221004064035.2648"><vh>RequiredType.__repr__</vh></v>
<v t="ekr.20221004064035.2649"><vh>RequiredType.accept</vh></v>
</v>
<v t="ekr.20221004064035.2650"><vh>class ProperType</vh></v>
<v t="ekr.20221004064035.2651"><vh>class TypeVarId</vh>
<v t="ekr.20221004064035.2652"><vh>TypeVarId.__init__</vh></v>
<v t="ekr.20221004064035.2653"><vh>TypeVarId.new</vh></v>
<v t="ekr.20221004064035.2654"><vh>TypeVarId.__repr__</vh></v>
<v t="ekr.20221004064035.2655"><vh>TypeVarId.__eq__</vh></v>
<v t="ekr.20221004064035.2656"><vh>TypeVarId.__ne__</vh></v>
<v t="ekr.20221004064035.2657"><vh>TypeVarId.__hash__</vh></v>
<v t="ekr.20221004064035.2658"><vh>TypeVarId.is_meta_var</vh></v>
</v>
<v t="ekr.20221004064035.2659"><vh>class TypeVarLikeType</vh>
<v t="ekr.20221004064035.2660"><vh>TypeVarLikeType.__init__</vh></v>
<v t="ekr.20221004064035.2661"><vh>TypeVarLikeType.serialize</vh></v>
<v t="ekr.20221004064035.2662"><vh>TypeVarLikeType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2663"><vh>class TypeVarType</vh>
<v t="ekr.20221004064035.2664"><vh>TypeVarType.__init__</vh></v>
<v t="ekr.20221004064035.2665"><vh>TypeVarType.new_unification_variable</vh></v>
<v t="ekr.20221004064035.2666"><vh>TypeVarType.copy_modified</vh></v>
<v t="ekr.20221004064035.2667"><vh>TypeVarType.accept</vh></v>
<v t="ekr.20221004064035.2668"><vh>TypeVarType.__hash__</vh></v>
<v t="ekr.20221004064035.2669"><vh>TypeVarType.__eq__</vh></v>
<v t="ekr.20221004064035.2670"><vh>TypeVarType.serialize</vh></v>
<v t="ekr.20221004064035.2671"><vh>TypeVarType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2672"><vh>class ParamSpecFlavor</vh></v>
<v t="ekr.20221004064035.2673"><vh>class ParamSpecType</vh>
<v t="ekr.20221004064035.2674"><vh>ParamSpecType.__init__</vh></v>
<v t="ekr.20221004064035.2675"><vh>ParamSpecType.new_unification_variable</vh></v>
<v t="ekr.20221004064035.2676"><vh>ParamSpecType.with_flavor</vh></v>
<v t="ekr.20221004064035.2677"><vh>ParamSpecType.copy_modified</vh></v>
<v t="ekr.20221004064035.2678"><vh>ParamSpecType.accept</vh></v>
<v t="ekr.20221004064035.2679"><vh>ParamSpecType.name_with_suffix</vh></v>
<v t="ekr.20221004064035.2680"><vh>ParamSpecType.__hash__</vh></v>
<v t="ekr.20221004064035.2681"><vh>ParamSpecType.__eq__</vh></v>
<v t="ekr.20221004064035.2682"><vh>ParamSpecType.serialize</vh></v>
<v t="ekr.20221004064035.2683"><vh>ParamSpecType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2684"><vh>class TypeVarTupleType</vh>
<v t="ekr.20221004064035.2685"><vh>TypeVarTupleType.serialize</vh></v>
<v t="ekr.20221004064035.2686"><vh>TypeVarTupleType.deserialize</vh></v>
<v t="ekr.20221004064035.2687"><vh>TypeVarTupleType.accept</vh></v>
<v t="ekr.20221004064035.2688"><vh>TypeVarTupleType.__hash__</vh></v>
<v t="ekr.20221004064035.2689"><vh>TypeVarTupleType.__eq__</vh></v>
<v t="ekr.20221004064035.2690"><vh>TypeVarTupleType.new_unification_variable</vh></v>
<v t="ekr.20221004064035.2691"><vh>TypeVarTupleType.copy_modified</vh></v>
</v>
<v t="ekr.20221004064035.2692"><vh>class UnboundType</vh>
<v t="ekr.20221004064035.2693"><vh>UnboundType.__init__</vh></v>
<v t="ekr.20221004064035.2694"><vh>UnboundType.copy_modified</vh></v>
<v t="ekr.20221004064035.2695"><vh>UnboundType.accept</vh></v>
<v t="ekr.20221004064035.2696"><vh>UnboundType.__hash__</vh></v>
<v t="ekr.20221004064035.2697"><vh>UnboundType.__eq__</vh></v>
<v t="ekr.20221004064035.2698"><vh>UnboundType.serialize</vh></v>
<v t="ekr.20221004064035.2699"><vh>UnboundType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2700"><vh>class CallableArgument</vh>
<v t="ekr.20221004064035.2701"><vh>CallableArgument.__init__</vh></v>
<v t="ekr.20221004064035.2702"><vh>CallableArgument.accept</vh></v>
<v t="ekr.20221004064035.2703"><vh>CallableArgument.serialize</vh></v>
</v>
<v t="ekr.20221004064035.2704"><vh>class TypeList</vh>
<v t="ekr.20221004064035.2705"><vh>TypeList.__init__</vh></v>
<v t="ekr.20221004064035.2706"><vh>TypeList.accept</vh></v>
<v t="ekr.20221004064035.2707"><vh>TypeList.serialize</vh></v>
<v t="ekr.20221004064035.2708"><vh>TypeList.__hash__</vh></v>
<v t="ekr.20221004064035.2709"><vh>TypeList.__eq__</vh></v>
</v>
<v t="ekr.20221004064035.2710"><vh>class UnpackType</vh>
<v t="ekr.20221004064035.2711"><vh>UnpackType.__init__</vh></v>
<v t="ekr.20221004064035.2712"><vh>UnpackType.accept</vh></v>
<v t="ekr.20221004064035.2713"><vh>UnpackType.serialize</vh></v>
<v t="ekr.20221004064035.2714"><vh>UnpackType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2715"><vh>class AnyType</vh>
<v t="ekr.20221004064035.2716"><vh>AnyType.__init__</vh></v>
<v t="ekr.20221004064035.2717"><vh>AnyType.is_from_error</vh></v>
<v t="ekr.20221004064035.2718"><vh>AnyType.accept</vh></v>
<v t="ekr.20221004064035.2719"><vh>AnyType.copy_modified</vh></v>
<v t="ekr.20221004064035.2720"><vh>AnyType.__hash__</vh></v>
<v t="ekr.20221004064035.2721"><vh>AnyType.__eq__</vh></v>
<v t="ekr.20221004064035.2722"><vh>AnyType.serialize</vh></v>
<v t="ekr.20221004064035.2723"><vh>AnyType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2724"><vh>class UninhabitedType</vh>
<v t="ekr.20221004064035.2725"><vh>UninhabitedType.__init__</vh></v>
<v t="ekr.20221004064035.2726"><vh>UninhabitedType.can_be_true_default</vh></v>
<v t="ekr.20221004064035.2727"><vh>UninhabitedType.can_be_false_default</vh></v>
<v t="ekr.20221004064035.2728"><vh>UninhabitedType.accept</vh></v>
<v t="ekr.20221004064035.2729"><vh>UninhabitedType.__hash__</vh></v>
<v t="ekr.20221004064035.2730"><vh>UninhabitedType.__eq__</vh></v>
<v t="ekr.20221004064035.2731"><vh>UninhabitedType.serialize</vh></v>
<v t="ekr.20221004064035.2732"><vh>UninhabitedType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2733"><vh>class NoneType</vh>
<v t="ekr.20221004064035.2734"><vh>NoneType.__init__</vh></v>
<v t="ekr.20221004064035.2735"><vh>NoneType.can_be_true_default</vh></v>
<v t="ekr.20221004064035.2736"><vh>NoneType.__hash__</vh></v>
<v t="ekr.20221004064035.2737"><vh>NoneType.__eq__</vh></v>
<v t="ekr.20221004064035.2738"><vh>NoneType.accept</vh></v>
<v t="ekr.20221004064035.2739"><vh>NoneType.serialize</vh></v>
<v t="ekr.20221004064035.2740"><vh>NoneType.deserialize</vh></v>
<v t="ekr.20221004064035.2741"><vh>NoneType.is_singleton_type</vh></v>
</v>
<v t="ekr.20221004064035.2742"><vh>NoneType used to be called NoneTyp so to avoid needlessly breaking</vh></v>
<v t="ekr.20221004064035.2743"><vh>class ErasedType</vh>
<v t="ekr.20221004064035.2744"><vh>ErasedType.accept</vh></v>
</v>
<v t="ekr.20221004064035.2745"><vh>class DeletedType</vh>
<v t="ekr.20221004064035.2746"><vh>DeletedType.__init__</vh></v>
<v t="ekr.20221004064035.2747"><vh>DeletedType.accept</vh></v>
<v t="ekr.20221004064035.2748"><vh>DeletedType.serialize</vh></v>
<v t="ekr.20221004064035.2749"><vh>DeletedType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2750"><vh>Fake TypeInfo to be used as a placeholder during Instance de-serialization.</vh></v>
<v t="ekr.20221004064035.2751"><vh>class ExtraAttrs</vh>
<v t="ekr.20221004064035.2752"><vh>ExtraAttrs.__init__</vh></v>
<v t="ekr.20221004064035.2753"><vh>ExtraAttrs.__hash__</vh></v>
<v t="ekr.20221004064035.2754"><vh>ExtraAttrs.__eq__</vh></v>
<v t="ekr.20221004064035.2755"><vh>ExtraAttrs.copy</vh></v>
<v t="ekr.20221004064035.2756"><vh>ExtraAttrs.__repr__</vh></v>
</v>
<v t="ekr.20221004064035.2757"><vh>class Instance</vh>
<v t="ekr.20221004064035.2758"><vh>Instance.__init__</vh></v>
<v t="ekr.20221004064035.2759"><vh>Instance.accept</vh></v>
<v t="ekr.20221004064035.2760"><vh>Instance.__hash__</vh></v>
<v t="ekr.20221004064035.2761"><vh>Instance.__eq__</vh></v>
<v t="ekr.20221004064035.2762"><vh>Instance.serialize</vh></v>
<v t="ekr.20221004064035.2763"><vh>Instance.deserialize</vh></v>
<v t="ekr.20221004064035.2764"><vh>Instance.copy_modified</vh></v>
<v t="ekr.20221004064035.2765"><vh>Instance.copy_with_extra_attr</vh></v>
<v t="ekr.20221004064035.2766"><vh>Instance.has_readable_member</vh></v>
<v t="ekr.20221004064035.2767"><vh>Instance.is_singleton_type</vh></v>
<v t="ekr.20221004064035.2768"><vh>Instance.get_enum_values</vh></v>
</v>
<v t="ekr.20221004064035.2769"><vh>class FunctionLike</vh>
<v t="ekr.20221004064035.2770"><vh>FunctionLike.__init__</vh></v>
<v t="ekr.20221004064035.2771"><vh>FunctionLike.is_type_obj</vh></v>
<v t="ekr.20221004064035.2772"><vh>FunctionLike.type_object</vh></v>
<v t="ekr.20221004064035.2773"><vh>FunctionLike.items</vh></v>
<v t="ekr.20221004064035.2774"><vh>FunctionLike.with_name</vh></v>
<v t="ekr.20221004064035.2775"><vh>FunctionLike.get_name</vh></v>
</v>
<v t="ekr.20221004064035.2776"><vh>class FormalArgument</vh></v>
<v t="ekr.20221004064035.2777"><vh>class Parameters</vh>
<v t="ekr.20221004064035.2778"><vh>Parameters.__init__</vh></v>
<v t="ekr.20221004064035.2779"><vh>Parameters.copy_modified</vh></v>
<v t="ekr.20221004064035.2780"><vh>Parameters.var_arg</vh></v>
<v t="ekr.20221004064035.2781"><vh>Parameters.kw_arg</vh></v>
<v t="ekr.20221004064035.2782"><vh>Parameters.formal_arguments</vh></v>
<v t="ekr.20221004064035.2783"><vh>Parameters.argument_by_name</vh></v>
<v t="ekr.20221004064035.2784"><vh>Parameters.argument_by_position</vh></v>
<v t="ekr.20221004064035.2785"><vh>Parameters.try_synthesizing_arg_from_kwarg</vh></v>
<v t="ekr.20221004064035.2786"><vh>Parameters.try_synthesizing_arg_from_vararg</vh></v>
<v t="ekr.20221004064035.2787"><vh>Parameters.accept</vh></v>
<v t="ekr.20221004064035.2788"><vh>Parameters.serialize</vh></v>
<v t="ekr.20221004064035.2789"><vh>Parameters.deserialize</vh></v>
<v t="ekr.20221004064035.2790"><vh>Parameters.__hash__</vh></v>
<v t="ekr.20221004064035.2791"><vh>Parameters.__eq__</vh></v>
</v>
<v t="ekr.20221004064035.2792"><vh>class CallableType</vh>
<v t="ekr.20221004064035.2793"><vh>CallableType.__init__</vh></v>
<v t="ekr.20221004064035.2794"><vh>CallableType.copy_modified</vh></v>
<v t="ekr.20221004064035.2795"><vh>CallableType.var_arg</vh></v>
<v t="ekr.20221004064035.2796"><vh>CallableType.kw_arg</vh></v>
<v t="ekr.20221004064035.2797"><vh>CallableType.is_var_arg</vh></v>
<v t="ekr.20221004064035.2798"><vh>CallableType.is_kw_arg</vh></v>
<v t="ekr.20221004064035.2799"><vh>CallableType.is_type_obj</vh></v>
<v t="ekr.20221004064035.2800"><vh>CallableType.type_object</vh></v>
<v t="ekr.20221004064035.2801"><vh>CallableType.accept</vh></v>
<v t="ekr.20221004064035.2802"><vh>CallableType.with_name</vh></v>
<v t="ekr.20221004064035.2803"><vh>CallableType.get_name</vh></v>
<v t="ekr.20221004064035.2804"><vh>CallableType.max_possible_positional_args</vh></v>
<v t="ekr.20221004064035.2805"><vh>CallableType.formal_arguments</vh></v>
<v t="ekr.20221004064035.2806"><vh>CallableType.argument_by_name</vh></v>
<v t="ekr.20221004064035.2807"><vh>CallableType.argument_by_position</vh></v>
<v t="ekr.20221004064035.2808"><vh>CallableType.try_synthesizing_arg_from_kwarg</vh></v>
<v t="ekr.20221004064035.2809"><vh>CallableType.try_synthesizing_arg_from_vararg</vh></v>
<v t="ekr.20221004064035.2810"><vh>CallableType.items</vh></v>
<v t="ekr.20221004064035.2811"><vh>CallableType.is_generic</vh></v>
<v t="ekr.20221004064035.2812"><vh>CallableType.type_var_ids</vh></v>
<v t="ekr.20221004064035.2813"><vh>CallableType.param_spec</vh></v>
<v t="ekr.20221004064035.2814"><vh>CallableType.expand_param_spec</vh></v>
<v t="ekr.20221004064035.2815"><vh>CallableType.with_unpacked_kwargs</vh></v>
<v t="ekr.20221004064035.2816"><vh>CallableType.__hash__</vh></v>
<v t="ekr.20221004064035.2817"><vh>CallableType.__eq__</vh></v>
<v t="ekr.20221004064035.2818"><vh>CallableType.serialize</vh></v>
<v t="ekr.20221004064035.2819"><vh>CallableType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2820"><vh>class Overloaded</vh>
<v t="ekr.20221004064035.2821"><vh>Overloaded.__init__</vh></v>
<v t="ekr.20221004064035.2822"><vh>Overloaded.items</vh></v>
<v t="ekr.20221004064035.2823"><vh>Overloaded.name</vh></v>
<v t="ekr.20221004064035.2824"><vh>Overloaded.is_type_obj</vh></v>
<v t="ekr.20221004064035.2825"><vh>Overloaded.type_object</vh></v>
<v t="ekr.20221004064035.2826"><vh>Overloaded.with_name</vh></v>
<v t="ekr.20221004064035.2827"><vh>Overloaded.get_name</vh></v>
<v t="ekr.20221004064035.2828"><vh>Overloaded.with_unpacked_kwargs</vh></v>
<v t="ekr.20221004064035.2829"><vh>Overloaded.accept</vh></v>
<v t="ekr.20221004064035.2830"><vh>Overloaded.__hash__</vh></v>
<v t="ekr.20221004064035.2831"><vh>Overloaded.__eq__</vh></v>
<v t="ekr.20221004064035.2832"><vh>Overloaded.serialize</vh></v>
<v t="ekr.20221004064035.2833"><vh>Overloaded.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2834"><vh>class TupleType</vh>
<v t="ekr.20221004064035.2835"><vh>TupleType.__init__</vh></v>
<v t="ekr.20221004064035.2836"><vh>TupleType.can_be_true_default</vh></v>
<v t="ekr.20221004064035.2837"><vh>TupleType.can_be_false_default</vh></v>
<v t="ekr.20221004064035.2838"><vh>TupleType.can_be_any_bool</vh></v>
<v t="ekr.20221004064035.2839"><vh>TupleType.length</vh></v>
<v t="ekr.20221004064035.2840"><vh>TupleType.accept</vh></v>
<v t="ekr.20221004064035.2841"><vh>TupleType.__hash__</vh></v>
<v t="ekr.20221004064035.2842"><vh>TupleType.__eq__</vh></v>
<v t="ekr.20221004064035.2843"><vh>TupleType.serialize</vh></v>
<v t="ekr.20221004064035.2844"><vh>TupleType.deserialize</vh></v>
<v t="ekr.20221004064035.2845"><vh>TupleType.copy_modified</vh></v>
<v t="ekr.20221004064035.2846"><vh>TupleType.slice</vh></v>
</v>
<v t="ekr.20221004064035.2847"><vh>class TypedDictType</vh>
<v t="ekr.20221004064035.2848"><vh>TypedDictType.__init__</vh></v>
<v t="ekr.20221004064035.2849"><vh>TypedDictType.accept</vh></v>
<v t="ekr.20221004064035.2850"><vh>TypedDictType.__hash__</vh></v>
<v t="ekr.20221004064035.2851"><vh>TypedDictType.__eq__</vh></v>
<v t="ekr.20221004064035.2852"><vh>TypedDictType.serialize</vh></v>
<v t="ekr.20221004064035.2853"><vh>TypedDictType.deserialize</vh></v>
<v t="ekr.20221004064035.2854"><vh>TypedDictType.is_anonymous</vh></v>
<v t="ekr.20221004064035.2855"><vh>TypedDictType.as_anonymous</vh></v>
<v t="ekr.20221004064035.2856"><vh>TypedDictType.copy_modified</vh></v>
<v t="ekr.20221004064035.2857"><vh>TypedDictType.create_anonymous_fallback</vh></v>
<v t="ekr.20221004064035.2858"><vh>TypedDictType.names_are_wider_than</vh></v>
<v t="ekr.20221004064035.2859"><vh>TypedDictType.zip</vh></v>
<v t="ekr.20221004064035.2860"><vh>TypedDictType.zipall</vh></v>
</v>
<v t="ekr.20221004064035.2861"><vh>class RawExpressionType</vh>
<v t="ekr.20221004064035.2862"><vh>RawExpressionType.__init__</vh></v>
<v t="ekr.20221004064035.2863"><vh>RawExpressionType.simple_name</vh></v>
<v t="ekr.20221004064035.2864"><vh>RawExpressionType.accept</vh></v>
<v t="ekr.20221004064035.2865"><vh>RawExpressionType.serialize</vh></v>
<v t="ekr.20221004064035.2866"><vh>RawExpressionType.__hash__</vh></v>
<v t="ekr.20221004064035.2867"><vh>RawExpressionType.__eq__</vh></v>
</v>
<v t="ekr.20221004064035.2868"><vh>class LiteralType</vh>
<v t="ekr.20221004064035.2869"><vh>LiteralType.__init__</vh></v>
<v t="ekr.20221004064035.2870"><vh>LiteralType.can_be_false_default</vh></v>
<v t="ekr.20221004064035.2871"><vh>LiteralType.can_be_true_default</vh></v>
<v t="ekr.20221004064035.2872"><vh>LiteralType.accept</vh></v>
<v t="ekr.20221004064035.2873"><vh>LiteralType.__hash__</vh></v>
<v t="ekr.20221004064035.2874"><vh>LiteralType.__eq__</vh></v>
<v t="ekr.20221004064035.2875"><vh>LiteralType.is_enum_literal</vh></v>
<v t="ekr.20221004064035.2876"><vh>LiteralType.value_repr</vh></v>
<v t="ekr.20221004064035.2877"><vh>LiteralType.serialize</vh></v>
<v t="ekr.20221004064035.2878"><vh>LiteralType.deserialize</vh></v>
<v t="ekr.20221004064035.2879"><vh>LiteralType.is_singleton_type</vh></v>
</v>
<v t="ekr.20221004064035.2880"><vh>class StarType</vh>
<v t="ekr.20221004064035.2881"><vh>StarType.__init__</vh></v>
<v t="ekr.20221004064035.2882"><vh>StarType.accept</vh></v>
<v t="ekr.20221004064035.2883"><vh>StarType.serialize</vh></v>
</v>
<v t="ekr.20221004064035.2884"><vh>class UnionType</vh>
<v t="ekr.20221004064035.2885"><vh>UnionType.__init__</vh></v>
<v t="ekr.20221004064035.2886"><vh>UnionType.__hash__</vh></v>
<v t="ekr.20221004064035.2887"><vh>UnionType.__eq__</vh></v>
<v t="ekr.20221004064035.2888"><vh>UnionType.make_union</vh></v>
<v t="ekr.20221004064035.2889"><vh>UnionType.make_union</vh></v>
<v t="ekr.20221004064035.2890"><vh>UnionType.make_union</vh></v>
<v t="ekr.20221004064035.2891"><vh>UnionType.length</vh></v>
<v t="ekr.20221004064035.2892"><vh>UnionType.accept</vh></v>
<v t="ekr.20221004064035.2893"><vh>UnionType.has_readable_member</vh></v>
<v t="ekr.20221004064035.2894"><vh>UnionType.relevant_items</vh></v>
<v t="ekr.20221004064035.2895"><vh>UnionType.serialize</vh></v>
<v t="ekr.20221004064035.2896"><vh>UnionType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2897"><vh>class PartialType</vh>
<v t="ekr.20221004064035.2898"><vh>PartialType.__init__</vh></v>
<v t="ekr.20221004064035.2899"><vh>PartialType.accept</vh></v>
</v>
<v t="ekr.20221004064035.2900"><vh>class EllipsisType</vh>
<v t="ekr.20221004064035.2901"><vh>EllipsisType.accept</vh></v>
<v t="ekr.20221004064035.2902"><vh>EllipsisType.serialize</vh></v>
</v>
<v t="ekr.20221004064035.2903"><vh>class TypeType</vh>
<v t="ekr.20221004064035.2904"><vh>TypeType.__init__</vh></v>
<v t="ekr.20221004064035.2905"><vh>TypeType.make_normalized</vh></v>
<v t="ekr.20221004064035.2906"><vh>TypeType.accept</vh></v>
<v t="ekr.20221004064035.2907"><vh>TypeType.__hash__</vh></v>
<v t="ekr.20221004064035.2908"><vh>TypeType.__eq__</vh></v>
<v t="ekr.20221004064035.2909"><vh>TypeType.serialize</vh></v>
<v t="ekr.20221004064035.2910"><vh>TypeType.deserialize</vh></v>
</v>
<v t="ekr.20221004064035.2911"><vh>class PlaceholderType</vh>
<v t="ekr.20221004064035.2912"><vh>PlaceholderType.__init__</vh></v>
<v t="ekr.20221004064035.2913"><vh>PlaceholderType.accept</vh></v>
<v t="ekr.20221004064035.2914"><vh>PlaceholderType.serialize</vh></v>
</v>
<v t="ekr.20221004064035.2915"><vh>get_proper_type</vh></v>
<v t="ekr.20221004064035.2916"><vh>get_proper_type</vh></v>
<v t="ekr.20221004064035.2917"><vh>get_proper_type</vh></v>
<v t="ekr.20221004064035.2918"><vh>get_proper_types</vh></v>
<v t="ekr.20221004064035.2919"><vh>get_proper_types</vh></v>
<v t="ekr.20221004064035.2920"><vh>get_proper_types</vh></v>
<v t="ekr.20221004064035.2921"><vh>We split off the type visitor base classes to another module</vh></v>
<v t="ekr.20221004064035.2922"><vh>class TypeStrVisitor</vh>
<v t="ekr.20221004064035.2923"><vh>TypeStrVisitor.__init__</vh></v>
<v t="ekr.20221004064035.2924"><vh>TypeStrVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.2925"><vh>TypeStrVisitor.visit_type_list</vh></v>
<v t="ekr.20221004064035.2926"><vh>TypeStrVisitor.visit_callable_argument</vh></v>
<v t="ekr.20221004064035.2927"><vh>TypeStrVisitor.visit_any</vh></v>
<v t="ekr.20221004064035.2928"><vh>TypeStrVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064035.2929"><vh>TypeStrVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.2930"><vh>TypeStrVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064035.2931"><vh>TypeStrVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.2932"><vh>TypeStrVisitor.visit_instance</vh></v>
<v t="ekr.20221004064035.2933"><vh>TypeStrVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064035.2934"><vh>TypeStrVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064035.2935"><vh>TypeStrVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064035.2936"><vh>TypeStrVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.2937"><vh>TypeStrVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064035.2938"><vh>TypeStrVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064035.2939"><vh>TypeStrVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.2940"><vh>TypeStrVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.2941"><vh>TypeStrVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20221004064035.2942"><vh>TypeStrVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064035.2943"><vh>TypeStrVisitor.visit_star_type</vh></v>
<v t="ekr.20221004064035.2944"><vh>TypeStrVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064035.2945"><vh>TypeStrVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064035.2946"><vh>TypeStrVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20221004064035.2947"><vh>TypeStrVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064035.2948"><vh>TypeStrVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20221004064035.2949"><vh>TypeStrVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.2950"><vh>TypeStrVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.2951"><vh>TypeStrVisitor.list_str</vh></v>
</v>
<v t="ekr.20221004064035.2952"><vh>class TrivialSyntheticTypeTranslator</vh>
<v t="ekr.20221004064035.2953"><vh>TrivialSyntheticTypeTranslator.visit_placeholder_type</vh></v>
<v t="ekr.20221004064035.2954"><vh>TrivialSyntheticTypeTranslator.visit_callable_argument</vh></v>
<v t="ekr.20221004064035.2955"><vh>TrivialSyntheticTypeTranslator.visit_ellipsis_type</vh></v>
<v t="ekr.20221004064035.2956"><vh>TrivialSyntheticTypeTranslator.visit_raw_expression_type</vh></v>
<v t="ekr.20221004064035.2957"><vh>TrivialSyntheticTypeTranslator.visit_star_type</vh></v>
<v t="ekr.20221004064035.2958"><vh>TrivialSyntheticTypeTranslator.visit_type_list</vh></v>
</v>
<v t="ekr.20221004064035.2959"><vh>class UnrollAliasVisitor</vh></v>
<v t="ekr.20221004064035.2962"><vh>strip_type</vh></v>
<v t="ekr.20221004064035.2963"><vh>is_named_instance</vh></v>
<v t="ekr.20221004064035.2964"><vh>class InstantiateAliasVisitor</vh>
<v t="ekr.20221004064035.2965"><vh>InstantiateAliasVisitor.__init__</vh></v>
<v t="ekr.20221004064035.2966"><vh>InstantiateAliasVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.2967"><vh>InstantiateAliasVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.2968"><vh>InstantiateAliasVisitor.visit_type_var</vh></v>
</v>
<v t="ekr.20221004064035.2969"><vh>class LocationSetter</vh>
<v t="ekr.20221004064035.2970"><vh>LocationSetter.__init__</vh></v>
<v t="ekr.20221004064035.2971"><vh>LocationSetter.visit_instance</vh></v>
</v>
<v t="ekr.20221004064035.2972"><vh>replace_alias_tvars</vh></v>
<v t="ekr.20221004064035.2973"><vh>class HasTypeVars</vh></v>
<v t="ekr.20221004064035.2974"><vh>has_type_vars</vh></v>
<v t="ekr.20221004064035.2975"><vh>class HasRecursiveType</vh></v>
<v t="ekr.20221004064035.2976"><vh>has_recursive_types</vh></v>
<v t="ekr.20221004064035.2977"><vh>flatten_nested_unions</vh></v>
<v t="ekr.20221004064035.2978"><vh>invalid_recursive_alias</vh></v>
<v t="ekr.20221004064035.2979"><vh>bad_type_type_item</vh></v>
<v t="ekr.20221004064035.2980"><vh>is_union_with_any</vh></v>
<v t="ekr.20221004064035.2981"><vh>is_generic_instance</vh></v>
<v t="ekr.20221004064035.2982"><vh>is_optional</vh></v>
<v t="ekr.20221004064035.2983"><vh>remove_optional</vh></v>
<v t="ekr.20221004064035.2984"><vh>is_literal_type</vh></v>
<v t="ekr.20221004064035.2985"><vh>is_self_type_like</vh></v>
<v t="ekr.20221004064035.2986"><vh>names: Final = globals().copy()</vh></v>
<v t="ekr.20221004064035.2987"><vh>callable_with_ellipsis</vh></v>
</v>
<v t="ekr.20221004064035.2988"><vh>@clean typestate.py</vh>
<v t="ekr.20221004064035.2989"><vh>class TypeState</vh>
<v t="ekr.20221004064035.2990"><vh>TypeState.is_assumed_subtype</vh></v>
<v t="ekr.20221004064035.2991"><vh>TypeState.is_assumed_proper_subtype</vh></v>
<v t="ekr.20221004064035.2992"><vh>TypeState.get_assumptions</vh></v>
<v t="ekr.20221004064035.2993"><vh>TypeState.reset_all_subtype_caches</vh></v>
<v t="ekr.20221004064035.2994"><vh>TypeState.reset_subtype_caches_for</vh></v>
<v t="ekr.20221004064035.2995"><vh>TypeState.reset_all_subtype_caches_for</vh></v>
<v t="ekr.20221004064035.2996"><vh>TypeState.is_cached_subtype_check</vh></v>
<v t="ekr.20221004064035.2997"><vh>TypeState.record_subtype_cache_entry</vh></v>
<v t="ekr.20221004064035.2998"><vh>TypeState.reset_protocol_deps</vh></v>
<v t="ekr.20221004064035.2999"><vh>TypeState.record_protocol_subtype_check</vh></v>
<v t="ekr.20221004064035.3000"><vh>TypeState._snapshot_protocol_deps</vh></v>
<v t="ekr.20221004064035.3001"><vh>TypeState.update_protocol_deps</vh></v>
<v t="ekr.20221004064035.3002"><vh>TypeState.add_all_protocol_deps</vh></v>
</v>
<v t="ekr.20221004064035.3003"><vh>reset_global_state</vh></v>
</v>
<v t="ekr.20221004064035.3004"><vh>@clean typetraverser.py</vh>
<v t="ekr.20221004064035.3005"><vh>class TypeTraverserVisitor</vh>
<v t="ekr.20221004064035.3006"><vh>TypeTraverserVisitor.visit_any</vh></v>
<v t="ekr.20221004064035.3007"><vh>TypeTraverserVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20221004064035.3008"><vh>TypeTraverserVisitor.visit_none_type</vh></v>
<v t="ekr.20221004064035.3009"><vh>TypeTraverserVisitor.visit_erased_type</vh></v>
<v t="ekr.20221004064035.3010"><vh>TypeTraverserVisitor.visit_deleted_type</vh></v>
<v t="ekr.20221004064035.3011"><vh>TypeTraverserVisitor.visit_type_var</vh></v>
<v t="ekr.20221004064035.3012"><vh>TypeTraverserVisitor.visit_param_spec</vh></v>
<v t="ekr.20221004064035.3013"><vh>TypeTraverserVisitor.visit_parameters</vh></v>
<v t="ekr.20221004064035.3014"><vh>TypeTraverserVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20221004064035.3015"><vh>TypeTraverserVisitor.visit_literal_type</vh></v>
<v t="ekr.20221004064035.3016"><vh>TypeTraverserVisitor.Composite types</vh></v>
<v t="ekr.20221004064035.3017"><vh>TypeTraverserVisitor.visit_instance</vh></v>
<v t="ekr.20221004064035.3018"><vh>TypeTraverserVisitor.visit_callable_type</vh></v>
<v t="ekr.20221004064035.3019"><vh>TypeTraverserVisitor.visit_tuple_type</vh></v>
<v t="ekr.20221004064035.3020"><vh>TypeTraverserVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20221004064035.3021"><vh>TypeTraverserVisitor.visit_union_type</vh></v>
<v t="ekr.20221004064035.3022"><vh>TypeTraverserVisitor.visit_overloaded</vh></v>
<v t="ekr.20221004064035.3023"><vh>TypeTraverserVisitor.visit_type_type</vh></v>
<v t="ekr.20221004064035.3024"><vh>TypeTraverserVisitor.Special types (not real types)</vh></v>
<v t="ekr.20221004064035.3025"><vh>TypeTraverserVisitor.visit_callable_argument</vh></v>
<v t="ekr.20221004064035.3026"><vh>TypeTraverserVisitor.visit_unbound_type</vh></v>
<v t="ekr.20221004064035.3027"><vh>TypeTraverserVisitor.visit_type_list</vh></v>
<v t="ekr.20221004064035.3028"><vh>TypeTraverserVisitor.visit_star_type</vh></v>
<v t="ekr.20221004064035.3029"><vh>TypeTraverserVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20221004064035.3030"><vh>TypeTraverserVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20221004064035.3031"><vh>TypeTraverserVisitor.visit_partial_type</vh></v>
<v t="ekr.20221004064035.3032"><vh>TypeTraverserVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20221004064035.3033"><vh>TypeTraverserVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20221004064035.3034"><vh>TypeTraverserVisitor.visit_unpack_type</vh></v>
<v t="ekr.20221004064035.3035"><vh>TypeTraverserVisitor.Helpers</vh></v>
<v t="ekr.20221004064035.3036"><vh>TypeTraverserVisitor.traverse_types</vh></v>
</v>
</v>
<v t="ekr.20221004064035.3037"><vh>@clean typevars.py</vh>
<v t="ekr.20221004064035.3038"><vh>fill_typevars</vh></v>
<v t="ekr.20221004064035.3039"><vh>fill_typevars_with_any</vh></v>
<v t="ekr.20221004064035.3040"><vh>has_no_typevars</vh></v>
</v>
<v t="ekr.20221004064035.3041"><vh>@clean typevartuples.py</vh>
<v t="ekr.20221004064035.3042"><vh>find_unpack_in_list</vh></v>
<v t="ekr.20221004064035.3043"><vh>T = TypeVar("T")</vh></v>
<v t="ekr.20221004064035.3044"><vh>split_with_prefix_and_suffix</vh></v>
<v t="ekr.20221004064035.3045"><vh>split_with_instance</vh></v>
<v t="ekr.20221004064035.3046"><vh>split_with_mapped_and_template</vh></v>
<v t="ekr.20221004064035.3047"><vh>extract_unpack</vh></v>
</v>
</v>
<v t="ekr.20221005081520.1"><vh>--- utils</vh>
<v t="ekr.20221004064034.1407"><vh>@clean git.py</vh>
<v t="ekr.20221004064034.1408"><vh>is_git_repo</vh></v>
<v t="ekr.20221004064034.1409"><vh>have_git</vh></v>
<v t="ekr.20221004064034.1410"><vh>git_revision</vh></v>
<v t="ekr.20221004064034.1411"><vh>is_dirty</vh></v>
</v>
<v t="ekr.20221004064034.1481"><vh>@clean ipc.py</vh>
<v t="ekr.20221004064035.1"><vh>class IPCException</vh></v>
<v t="ekr.20221004064035.2"><vh>class IPCBase</vh>
<v t="ekr.20221004064035.3"><vh>IPCBase.__init__</vh></v>
<v t="ekr.20221004064035.4"><vh>IPCBase.read</vh></v>
<v t="ekr.20221004064035.5"><vh>IPCBase.write</vh></v>
<v t="ekr.20221004064035.6"><vh>IPCBase.close</vh></v>
</v>
<v t="ekr.20221004064035.7"><vh>class IPCClient</vh>
<v t="ekr.20221004064035.8"><vh>IPCClient.__init__</vh></v>
<v t="ekr.20221004064035.9"><vh>IPCClient.__enter__</vh></v>
<v t="ekr.20221004064035.10"><vh>IPCClient.__exit__</vh></v>
</v>
<v t="ekr.20221004064035.11"><vh>class IPCServer</vh>
<v t="ekr.20221004064035.12"><vh>IPCServer.__init__</vh></v>
<v t="ekr.20221004064035.13"><vh>IPCServer.__enter__</vh></v>
<v t="ekr.20221004064035.14"><vh>IPCServer.__exit__</vh></v>
<v t="ekr.20221004064035.15"><vh>IPCServer.cleanup</vh></v>
<v t="ekr.20221004064035.16"><vh>IPCServer.connection_name</vh></v>
</v>
</v>
<v t="ekr.20221004064035.60"><vh>@clean literals.py</vh>
<v t="ekr.20221004064035.61"><vh>literal</vh></v>
<v t="ekr.20221004064035.62"><vh>Key: _TypeAlias = Tuple[Any, ...]</vh></v>
<v t="ekr.20221004064035.63"><vh>subkeys</vh></v>
<v t="ekr.20221004064035.64"><vh>literal_hash</vh></v>
<v t="ekr.20221004064035.65"><vh>class _Hasher</vh>
<v t="ekr.20221004064035.66"><vh>_Hasher.visit_int_expr</vh></v>
<v t="ekr.20221004064035.67"><vh>_Hasher.visit_str_expr</vh></v>
<v t="ekr.20221004064035.68"><vh>_Hasher.visit_bytes_expr</vh></v>
<v t="ekr.20221004064035.69"><vh>_Hasher.visit_float_expr</vh></v>
<v t="ekr.20221004064035.70"><vh>_Hasher.visit_complex_expr</vh></v>
<v t="ekr.20221004064035.71"><vh>_Hasher.visit_star_expr</vh></v>
<v t="ekr.20221004064035.72"><vh>_Hasher.visit_name_expr</vh></v>
<v t="ekr.20221004064035.73"><vh>_Hasher.visit_member_expr</vh></v>
<v t="ekr.20221004064035.74"><vh>_Hasher.visit_op_expr</vh></v>
<v t="ekr.20221004064035.75"><vh>_Hasher.visit_comparison_expr</vh></v>
<v t="ekr.20221004064035.76"><vh>_Hasher.visit_unary_expr</vh></v>
<v t="ekr.20221004064035.77"><vh>_Hasher.seq_expr</vh></v>
<v t="ekr.20221004064035.78"><vh>_Hasher.visit_list_expr</vh></v>
<v t="ekr.20221004064035.79"><vh>_Hasher.visit_dict_expr</vh></v>
<v t="ekr.20221004064035.80"><vh>_Hasher.visit_tuple_expr</vh></v>
<v t="ekr.20221004064035.81"><vh>_Hasher.visit_set_expr</vh></v>
<v t="ekr.20221004064035.82"><vh>_Hasher.visit_index_expr</vh></v>
<v t="ekr.20221004064035.83"><vh>_Hasher.visit_assignment_expr</vh></v>
<v t="ekr.20221004064035.84"><vh>_Hasher.visit_call_expr</vh></v>
<v t="ekr.20221004064035.85"><vh>_Hasher.visit_slice_expr</vh></v>
<v t="ekr.20221004064035.86"><vh>_Hasher.visit_cast_expr</vh></v>
<v t="ekr.20221004064035.87"><vh>_Hasher.visit_assert_type_expr</vh></v>
<v t="ekr.20221004064035.88"><vh>_Hasher.visit_conditional_expr</vh></v>
<v t="ekr.20221004064035.89"><vh>_Hasher.visit_ellipsis</vh></v>
<v t="ekr.20221004064035.90"><vh>_Hasher.visit_yield_from_expr</vh></v>
<v t="ekr.20221004064035.91"><vh>_Hasher.visit_yield_expr</vh></v>
<v t="ekr.20221004064035.92"><vh>_Hasher.visit_reveal_expr</vh></v>
<v t="ekr.20221004064035.93"><vh>_Hasher.visit_super_expr</vh></v>
<v t="ekr.20221004064035.94"><vh>_Hasher.visit_type_application</vh></v>
<v t="ekr.20221004064035.95"><vh>_Hasher.visit_lambda_expr</vh></v>
<v t="ekr.20221004064035.96"><vh>_Hasher.visit_list_comprehension</vh></v>
<v t="ekr.20221004064035.97"><vh>_Hasher.visit_set_comprehension</vh></v>
<v t="ekr.20221004064035.98"><vh>_Hasher.visit_dictionary_comprehension</vh></v>
<v t="ekr.20221004064035.99"><vh>_Hasher.visit_generator_expr</vh></v>
<v t="ekr.20221004064035.100"><vh>_Hasher.visit_type_var_expr</vh></v>
<v t="ekr.20221004064035.101"><vh>_Hasher.visit_paramspec_expr</vh></v>
<v t="ekr.20221004064035.102"><vh>_Hasher.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20221004064035.103"><vh>_Hasher.visit_type_alias_expr</vh></v>
<v t="ekr.20221004064035.104"><vh>_Hasher.visit_namedtuple_expr</vh></v>
<v t="ekr.20221004064035.105"><vh>_Hasher.visit_enum_call_expr</vh></v>
<v t="ekr.20221004064035.106"><vh>_Hasher.visit_typeddict_expr</vh></v>
<v t="ekr.20221004064035.107"><vh>_Hasher.visit_newtype_expr</vh></v>
<v t="ekr.20221004064035.108"><vh>_Hasher.visit__promote_expr</vh></v>
<v t="ekr.20221004064035.109"><vh>_Hasher.visit_await_expr</vh></v>
<v t="ekr.20221004064035.110"><vh>_Hasher.visit_temp_node</vh></v>
</v>
</v>
<v t="ekr.20221004064035.863"><vh>@clean operators.py</vh></v>
<v t="ekr.20221004064035.948"><vh>@clean plugin.py</vh>
<v t="ekr.20221004064035.949"><vh>class TypeAnalyzerPluginInterface</vh>
<v t="ekr.20221004064035.950"><vh>TypeAnalyzerPluginInterface.fail</vh></v>
<v t="ekr.20221004064035.951"><vh>TypeAnalyzerPluginInterface.named_type</vh></v>
<v t="ekr.20221004064035.952"><vh>TypeAnalyzerPluginInterface.analyze_type</vh></v>
<v t="ekr.20221004064035.953"><vh>TypeAnalyzerPluginInterface.analyze_callable_args</vh></v>
</v>
<v t="ekr.20221004064035.954"><vh>class AnalyzeTypeContext</vh></v>
<v t="ekr.20221004064035.955"><vh>class CommonPluginApi</vh>
<v t="ekr.20221004064035.956"><vh>CommonPluginApi.lookup_fully_qualified</vh></v>
</v>
<v t="ekr.20221004064035.957"><vh>class CheckerPluginInterface</vh>
<v t="ekr.20221004064035.958"><vh>CheckerPluginInterface.type_context</vh></v>
<v t="ekr.20221004064035.959"><vh>CheckerPluginInterface.fail</vh></v>
<v t="ekr.20221004064035.960"><vh>CheckerPluginInterface.named_generic_type</vh></v>
</v>
<v t="ekr.20221004064035.961"><vh>class SemanticAnalyzerPluginInterface</vh>
<v t="ekr.20221004064035.962"><vh>SemanticAnalyzerPluginInterface.named_type</vh></v>
<v t="ekr.20221004064035.963"><vh>SemanticAnalyzerPluginInterface.builtin_type</vh></v>
<v t="ekr.20221004064035.964"><vh>SemanticAnalyzerPluginInterface.named_type_or_none</vh></v>
<v t="ekr.20221004064035.965"><vh>SemanticAnalyzerPluginInterface.basic_new_typeinfo</vh></v>
<v t="ekr.20221004064035.966"><vh>SemanticAnalyzerPluginInterface.parse_bool</vh></v>
<v t="ekr.20221004064035.967"><vh>SemanticAnalyzerPluginInterface.fail</vh></v>
<v t="ekr.20221004064035.968"><vh>SemanticAnalyzerPluginInterface.anal_type</vh></v>
<v t="ekr.20221004064035.969"><vh>SemanticAnalyzerPluginInterface.class_type</vh></v>
<v t="ekr.20221004064035.970"><vh>SemanticAnalyzerPluginInterface.lookup_fully_qualified</vh></v>
<v t="ekr.20221004064035.971"><vh>SemanticAnalyzerPluginInterface.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20221004064035.972"><vh>SemanticAnalyzerPluginInterface.lookup_qualified</vh></v>
<v t="ekr.20221004064035.973"><vh>SemanticAnalyzerPluginInterface.add_plugin_dependency</vh></v>
<v t="ekr.20221004064035.974"><vh>SemanticAnalyzerPluginInterface.add_symbol_table_node</vh></v>
<v t="ekr.20221004064035.975"><vh>SemanticAnalyzerPluginInterface.qualified_name</vh></v>
<v t="ekr.20221004064035.976"><vh>SemanticAnalyzerPluginInterface.defer</vh></v>
<v t="ekr.20221004064035.977"><vh>SemanticAnalyzerPluginInterface.final_iteration</vh></v>
<v t="ekr.20221004064035.978"><vh>SemanticAnalyzerPluginInterface.is_stub_file</vh></v>
<v t="ekr.20221004064035.979"><vh>SemanticAnalyzerPluginInterface.analyze_simple_literal_type</vh></v>
</v>
<v t="ekr.20221004064035.980"><vh>class ReportConfigContext</vh></v>
<v t="ekr.20221004064035.981"><vh>class FunctionSigContext</vh></v>
<v t="ekr.20221004064035.982"><vh>class FunctionContext</vh></v>
<v t="ekr.20221004064035.983"><vh>class MethodSigContext</vh></v>
<v t="ekr.20221004064035.984"><vh>class MethodContext</vh></v>
<v t="ekr.20221004064035.985"><vh>class AttributeContext</vh></v>
<v t="ekr.20221004064035.986"><vh>class ClassDefContext</vh></v>
<v t="ekr.20221004064035.987"><vh>class DynamicClassDefContext</vh></v>
<v t="ekr.20221004064035.988"><vh>class Plugin</vh>
<v t="ekr.20221004064035.989"><vh>Plugin.__init__</vh></v>
<v t="ekr.20221004064035.990"><vh>Plugin.set_modules</vh></v>
<v t="ekr.20221004064035.991"><vh>Plugin.lookup_fully_qualified</vh></v>
<v t="ekr.20221004064035.992"><vh>Plugin.report_config_data</vh></v>
<v t="ekr.20221004064035.993"><vh>Plugin.get_additional_deps</vh></v>
<v t="ekr.20221004064035.994"><vh>Plugin.get_type_analyze_hook</vh></v>
<v t="ekr.20221004064035.995"><vh>Plugin.get_function_signature_hook</vh></v>
<v t="ekr.20221004064035.996"><vh>Plugin.get_function_hook</vh></v>
<v t="ekr.20221004064035.997"><vh>Plugin.get_method_signature_hook</vh></v>
<v t="ekr.20221004064035.998"><vh>Plugin.get_method_hook</vh></v>
<v t="ekr.20221004064035.999"><vh>Plugin.get_attribute_hook</vh></v>
<v t="ekr.20221004064035.1000"><vh>Plugin.get_class_attribute_hook</vh></v>
<v t="ekr.20221004064035.1001"><vh>Plugin.get_class_decorator_hook</vh></v>
<v t="ekr.20221004064035.1002"><vh>Plugin.get_class_decorator_hook_2</vh></v>
<v t="ekr.20221004064035.1003"><vh>Plugin.get_metaclass_hook</vh></v>
<v t="ekr.20221004064035.1004"><vh>Plugin.get_base_class_hook</vh></v>
<v t="ekr.20221004064035.1005"><vh>Plugin.get_customize_class_mro_hook</vh></v>
<v t="ekr.20221004064035.1006"><vh>Plugin.get_dynamic_class_hook</vh></v>
</v>
<v t="ekr.20221004064035.1007"><vh>T = TypeVar("T")</vh></v>
<v t="ekr.20221004064035.1008"><vh>class ChainedPlugin</vh>
<v t="ekr.20221004064035.1009"><vh>ChainedPlugin.__init__</vh></v>
<v t="ekr.20221004064035.1010"><vh>ChainedPlugin.set_modules</vh></v>
<v t="ekr.20221004064035.1011"><vh>ChainedPlugin.report_config_data</vh></v>
<v t="ekr.20221004064035.1012"><vh>ChainedPlugin.get_additional_deps</vh></v>
<v t="ekr.20221004064035.1013"><vh>ChainedPlugin.get_type_analyze_hook</vh></v>
<v t="ekr.20221004064035.1014"><vh>ChainedPlugin.get_function_signature_hook</vh></v>
<v t="ekr.20221004064035.1015"><vh>ChainedPlugin.get_function_hook</vh></v>
<v t="ekr.20221004064035.1016"><vh>ChainedPlugin.get_method_signature_hook</vh></v>
<v t="ekr.20221004064035.1017"><vh>ChainedPlugin.get_method_hook</vh></v>
<v t="ekr.20221004064035.1018"><vh>ChainedPlugin.get_attribute_hook</vh></v>
<v t="ekr.20221004064035.1019"><vh>ChainedPlugin.get_class_attribute_hook</vh></v>
<v t="ekr.20221004064035.1020"><vh>ChainedPlugin.get_class_decorator_hook</vh></v>
<v t="ekr.20221004064035.1021"><vh>ChainedPlugin.get_class_decorator_hook_2</vh></v>
<v t="ekr.20221004064035.1022"><vh>ChainedPlugin.get_metaclass_hook</vh></v>
<v t="ekr.20221004064035.1023"><vh>ChainedPlugin.get_base_class_hook</vh></v>
<v t="ekr.20221004064035.1024"><vh>ChainedPlugin.get_customize_class_mro_hook</vh></v>
<v t="ekr.20221004064035.1025"><vh>ChainedPlugin.get_dynamic_class_hook</vh></v>
<v t="ekr.20221004064035.1026"><vh>ChainedPlugin._find_hook</vh></v>
</v>
</v>
<v t="ekr.20221004064035.1027"><vh>@clean pyinfo.py</vh>
<v t="ekr.20221004064035.1028"><vh>getsitepackages</vh></v>
<v t="ekr.20221004064035.1029"><vh>getsyspath</vh></v>
<v t="ekr.20221004064035.1030"><vh>getsearchdirs</vh></v>
</v>
<v t="ekr.20221004064036.9"><vh>@clean util.py</vh>
<v t="ekr.20221004064036.10"><vh>is_dunder</vh></v>
<v t="ekr.20221004064036.11"><vh>is_sunder</vh></v>
<v t="ekr.20221004064036.12"><vh>split_module_names</vh></v>
<v t="ekr.20221004064036.13"><vh>module_prefix</vh></v>
<v t="ekr.20221004064036.14"><vh>split_target</vh></v>
<v t="ekr.20221004064036.15"><vh>short_type</vh></v>
<v t="ekr.20221004064036.16"><vh>find_python_encoding</vh></v>
<v t="ekr.20221004064036.17"><vh>bytes_to_human_readable_repr</vh></v>
<v t="ekr.20221004064036.18"><vh>class DecodeError</vh></v>
<v t="ekr.20221004064036.19"><vh>decode_python_encoding</vh></v>
<v t="ekr.20221004064036.20"><vh>read_py_file</vh></v>
<v t="ekr.20221004064036.21"><vh>trim_source_line</vh></v>
<v t="ekr.20221004064036.22"><vh>get_mypy_comments</vh></v>
<v t="ekr.20221004064036.23"><vh>PASS_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;</vh></v>
<v t="ekr.20221004064036.24"><vh>write_junit_xml</vh></v>
<v t="ekr.20221004064036.25"><vh>class IdMapper</vh>
<v t="ekr.20221004064036.26"><vh>IdMapper.__init__</vh></v>
<v t="ekr.20221004064036.27"><vh>IdMapper.id</vh></v>
</v>
<v t="ekr.20221004064036.28"><vh>get_prefix</vh></v>
<v t="ekr.20221004064036.29"><vh>get_top_two_prefixes</vh></v>
<v t="ekr.20221004064036.30"><vh>correct_relative_import</vh></v>
<v t="ekr.20221004064036.31"><vh>fields_cache: Final[dict[type[object], list[str]]] = {}</vh></v>
<v t="ekr.20221004064036.32"><vh>get_class_descriptors</vh></v>
<v t="ekr.20221004064036.33"><vh>replace_object_state</vh></v>
<v t="ekr.20221004064036.34"><vh>is_sub_path</vh></v>
<v t="ekr.20221004064036.35"><vh>hard_exit</vh></v>
<v t="ekr.20221004064036.36"><vh>unmangle</vh></v>
<v t="ekr.20221004064036.37"><vh>get_unique_redefinition_name</vh></v>
<v t="ekr.20221004064036.38"><vh>check_python_version</vh></v>
<v t="ekr.20221004064036.39"><vh>count_stats</vh></v>
<v t="ekr.20221004064036.40"><vh>split_words</vh></v>
<v t="ekr.20221004064036.41"><vh>get_terminal_width</vh></v>
<v t="ekr.20221004064036.42"><vh>soft_wrap</vh></v>
<v t="ekr.20221004064036.43"><vh>hash_digest</vh></v>
<v t="ekr.20221004064036.44"><vh>parse_gray_color</vh></v>
<v t="ekr.20221004064036.45"><vh>class FancyFormatter</vh>
<v t="ekr.20221004064036.46"><vh>FancyFormatter.__init__</vh></v>
<v t="ekr.20221004064036.47"><vh>FancyFormatter.initialize_vt100_colors</vh></v>
<v t="ekr.20221004064036.48"><vh>FancyFormatter.initialize_win_colors</vh></v>
<v t="ekr.20221004064036.49"><vh>FancyFormatter.initialize_unix_colors</vh></v>
<v t="ekr.20221004064036.50"><vh>FancyFormatter.style</vh></v>
<v t="ekr.20221004064036.51"><vh>FancyFormatter.fit_in_terminal</vh></v>
<v t="ekr.20221004064036.52"><vh>FancyFormatter.colorize</vh></v>
<v t="ekr.20221004064036.53"><vh>FancyFormatter.highlight_quote_groups</vh></v>
<v t="ekr.20221004064036.54"><vh>FancyFormatter.underline_link</vh></v>
<v t="ekr.20221004064036.55"><vh>FancyFormatter.format_success</vh></v>
<v t="ekr.20221004064036.56"><vh>FancyFormatter.format_error</vh></v>
</v>
<v t="ekr.20221004064036.57"><vh>is_typeshed_file</vh></v>
<v t="ekr.20221004064036.58"><vh>is_stub_package_file</vh></v>
<v t="ekr.20221004064036.59"><vh>unnamed_function</vh></v>
<v t="ekr.20221004064036.60"><vh>TODO: replace with uses of perf_counter_ns when support for py3.6 is dropped</vh></v>
<v t="ekr.20221004064036.61"><vh>time_spent_us</vh></v>
<v t="ekr.20221004064036.62"><vh>plural_s</vh></v>
</v>
<v t="ekr.20221004064036.63"><vh>@clean version.py</vh></v>
</v>
</v>
</v>
<v t="ekr.20220925073606.1"><vh>Recent code</vh>
<v t="ekr.20220320050857.1"><vh>=== #12352: auto annotations</vh>
<v t="ekr.20220620055103.1"><vh>--- COPIES</vh>
<v t="ekr.20220612050429.1"><vh>COPY: &lt;&lt; define callers &amp; callerName &gt;&gt;</vh></v>
<v t="ekr.20220620060013.1"><vh>COPY: All traces &amp; changes</vh>
<v t="ekr.20220620060013.2"><vh>COPY: ASTConverter.do_func_def (traced)</vh></v>
<v t="ekr.20220620060013.3"><vh>COPY: ASTConverter.make_argument (traced, ***)</vh></v>
<v t="ekr.20220620060013.4"><vh>COPY: ASTConverter.transform_args (traced)</vh></v>
<v t="ekr.20220620060013.5"><vh>COPY: expr_to_unanalyzed_type (exprtotype.py) (disabled trace)</vh></v>
<v t="ekr.20220620060013.6"><vh>COPY: FuncDef.__init__ (traced, diabled)</vh></v>
<v t="ekr.20220620060013.7"><vh>COPY: process_stale_scc (build.py) (traced)</vh></v>
<v t="ekr.20220620060013.8"><vh>COPY: SA.analyze_func_def (traced, disabled)</vh></v>
<v t="ekr.20220620060013.9"><vh>COPY: SA.process_type_annotation (trace, disabled)</vh>
<v t="ekr.20220620060013.10"><vh>&lt;&lt; define callers &amp; callerName &gt;&gt;</vh></v>
</v>
<v t="ekr.20220620060013.11"><vh>COPY: SA.visit_func_def (traced, disabled)</vh></v>
<v t="ekr.20220620060013.12"><vh>COPY: TA.visit_callable_type (trace, disabled)</vh></v>
<v t="ekr.20220620060013.13"><vh>COPY: TypeChecker.check_for_missing_annotations (trace, disabled)</vh></v>
<v t="ekr.20220620060013.14"><vh>COPY: TypeConverter.visit_Str (traced)</vh></v>
<v t="ekr.20220620060013.15"><vh>COPY: TypeStrVisitor.visit_callable_type (changed)</vh></v>
<v t="ekr.20220620060013.16"><vh>COPY: TypeStrVisitor.visit_parameters (changed)</vh></v>
</v>
</v>
<v t="ekr.20220613154216.1"><vh>--- recent</vh>
<v t="ekr.20220525082935.34"><vh>SA.analyze_arg_initializers (trace???)</vh></v>
<v t="ekr.20220525082935.21"><vh>SA.analyze_func_def</vh></v>
<v t="ekr.20220525082935.1231"><vh>TA.anal_type</vh></v>
<v t="ekr.20220525082935.1202"><vh>TA.visit_callable_type</vh></v>
<v t="ekr.20220525082933.377"><vh>TypeChecker.check_for_missing_annotations</vh></v>
</v>
<v t="ekr.20220601070439.1"><vh>@button test-all</vh></v>
<v t="ekr.20220531174305.1"><vh>@button test-one</vh></v>
<v t="ekr.20220619083632.1"><vh>--- Changed</vh>
<v t="ekr.20220525082934.71"><vh>ASTConverter.make_argument (changed, trace, disabled)</vh></v>
<v t="ekr.20220525082934.70"><vh>ASTConverter.transform_args (trace, disabled)</vh></v>
</v>
<v t="ekr.20220530121430.1"><vh>--- testing and traces</vh>
<v t="ekr.20220530121437.1"><vh>Adding tests</vh></v>
<v t="ekr.20220607072154.1"><vh>traces</vh></v>
<v t="ekr.20220531170425.1"><vh>Test log: 10 tests fail.</vh></v>
<v t="ekr.20220621111917.1"><vh>Test log (after change): 28 tests fail</vh>
<v t="ekr.20220621112034.1"><vh>details</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20221025092126.1"><vh>Recent files</vh>
<v t="ekr.20221004064036.791"><vh>@clean testcheck.py</vh>
<v t="ekr.20221004064036.792"><vh>class TypeCheckSuite</vh>
<v t="ekr.20221004064036.793"><vh>TypeCheckSuite.run_case</vh></v>
<v t="ekr.20221004064036.794"><vh>TypeCheckSuite.run_case_once</vh></v>
<v t="ekr.20221004064036.795"><vh>TypeCheckSuite.verify_cache</vh></v>
<v t="ekr.20221004064036.796"><vh>TypeCheckSuite.find_error_message_paths</vh></v>
<v t="ekr.20221004064036.797"><vh>TypeCheckSuite.find_module_files</vh></v>
<v t="ekr.20221004064036.798"><vh>TypeCheckSuite.find_missing_cache_files</vh></v>
<v t="ekr.20221004064036.799"><vh>TypeCheckSuite.parse_module</vh></v>
</v>
</v>
<v t="ekr.20221004064034.196"></v>
</v>
<v t="ekr.20221001054943.1"><vh>** To do</vh></v>
<v t="ekr.20221005074708.1"><vh>--- failure files</vh>
<v t="ekr.20221004064034.324"></v>
<v t="ekr.20221004064035.2471"></v>
<v t="ekr.20221004064035.2617"></v>
</v>
<v t="ekr.20221025071515.1"><vh>@clean mypy-notes.md</vh>
<v t="ekr.20221025071935.1"><vh>Ahas</vh></v>
<v t="ekr.20221025072341.1"><vh>Running unit tests</vh></v>
</v>
<v t="ekr.20220930225101.1"><vh>=== Study </vh>
<v t="ekr.20221003060449.1"><vh>--- Notes from discussions</vh>
<v t="ekr.20221003060901.1"><vh>Summary of issue</vh></v>
<v t="ekr.20221003060506.1"><vh>Understanding x.accept</vh></v>
<v t="ekr.20221003060650.1"><vh>Transportation</vh></v>
<v t="ekr.20221003061049.1"><vh>Why so many calls to analyze_func_def</vh></v>
</v>
<v t="ekr.20220614100654.1"><vh>--- docstrings &amp; comments</vh>
<v t="ekr.20220617075745.1"><vh>&lt;&lt; docstring: plugin.py &gt;&gt;</vh></v>
<v t="ekr.20220617085508.1"><vh>&lt;&lt; docstring: build &gt;&gt;</vh></v>
<v t="ekr.20220614100511.1"><vh>&lt;&lt; docstring: semanal.py &gt;&gt;</vh></v>
<v t="ekr.20220614100550.1"><vh>&lt;&lt; docstring: semanal_main.py &gt;&gt;</vh></v>
</v>
<v t="ekr.20220603081942.1"><vh>--- class hierarchies</vh>
<v t="ekr.20220605090347.1"><vh>--- Hierarchy for Instance</vh>
<v t="ekr.20220525082934.872"><vh>class Context</vh>
<v t="ekr.20220525082934.873"><vh>Context.__init__</vh></v>
<v t="ekr.20220525082934.874"><vh>Context.set_line</vh></v>
<v t="ekr.20220525082934.875"><vh>Context.get_line</vh></v>
<v t="ekr.20220525082934.876"><vh>Context.get_column</vh></v>
</v>
<v t="ekr.20220525082935.1319"><vh>class Type(Context) (has accept method)</vh>
<v t="ekr.20220525082935.1320"><vh>Type.__init__</vh></v>
<v t="ekr.20220525082935.1321"><vh>Type.can_be_true_default</vh></v>
<v t="ekr.20220525082935.1322"><vh>Type.can_be_false_default</vh></v>
<v t="ekr.20220525082935.1323"><vh>Type.accept</vh></v>
<v t="ekr.20220525082935.1324"><vh>Type.__repr__</vh></v>
<v t="ekr.20220525082935.1325"><vh>Type.serialize</vh></v>
<v t="ekr.20220525082935.1326"><vh>Type.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1348"><vh>class ProperType(Type)</vh></v>
<v t="ekr.20220525082935.1444"><vh>class Instance(ProperType)</vh>
<v t="ekr.20220525082935.1445"><vh>Instance.__init__</vh></v>
<v t="ekr.20220525082935.1446"><vh>Instance.accept</vh></v>
<v t="ekr.20220525082935.1447"><vh>Instance.__hash__</vh></v>
<v t="ekr.20220525082935.1448"><vh>Instance.__eq__</vh></v>
<v t="ekr.20220525082935.1449"><vh>Instance.serialize</vh></v>
<v t="ekr.20220525082935.1450"><vh>Instance.deserialize</vh></v>
<v t="ekr.20220525082935.1451"><vh>Instance.copy_modified</vh></v>
<v t="ekr.20220525082935.1452"><vh>Instance.has_readable_member</vh></v>
</v>
</v>
<v t="ekr.20220606085857.1"><vh>--- Hierarchy for CallableType</vh>
<v t="ekr.20220525082934.872"></v>
<v t="ekr.20220525082935.1319"></v>
<v t="ekr.20220525082935.1348"></v>
<v t="ekr.20220525082935.1453"><vh>class FunctionLike(ProperType)</vh>
<v t="ekr.20220525082935.1454"><vh>FunctionLike.__init__</vh></v>
<v t="ekr.20220525082935.1455"><vh>FunctionLike.is_type_obj</vh></v>
<v t="ekr.20220525082935.1456"><vh>FunctionLike.type_object</vh></v>
<v t="ekr.20220525082935.1457"><vh>FunctionLike.items</vh></v>
<v t="ekr.20220525082935.1458"><vh>FunctionLike.with_name</vh></v>
<v t="ekr.20220525082935.1459"><vh>FunctionLike.get_name</vh></v>
</v>
<v t="ekr.20220525082935.1476"><vh>class CallableType(FunctionLike)</vh>
<v t="ekr.20220525082935.1477"><vh>CallableType.__init__</vh></v>
<v t="ekr.20220525082935.1478"><vh>CallableType.copy_modified</vh></v>
<v t="ekr.20220525082935.1479"><vh>CallableType.var_arg</vh></v>
<v t="ekr.20220525082935.1480"><vh>CallableType.kw_arg</vh></v>
<v t="ekr.20220525082935.1481"><vh>CallableType.is_var_arg</vh></v>
<v t="ekr.20220525082935.1482"><vh>CallableType.is_kw_arg</vh></v>
<v t="ekr.20220525082935.1483"><vh>CallableType.is_type_obj</vh></v>
<v t="ekr.20220525082935.1484"><vh>CallableType.type_object</vh></v>
<v t="ekr.20220525082935.1485"><vh>CallableType.accept</vh></v>
<v t="ekr.20220525082935.1486"><vh>CallableType.with_name</vh></v>
<v t="ekr.20220525082935.1487"><vh>CallableType.get_name</vh></v>
<v t="ekr.20220525082935.1488"><vh>CallableType.max_possible_positional_args</vh></v>
<v t="ekr.20220525082935.1489"><vh>CallableType.formal_arguments</vh></v>
<v t="ekr.20220525082935.1490"><vh>CallableType.argument_by_name</vh></v>
<v t="ekr.20220525082935.1491"><vh>CallableType.argument_by_position</vh></v>
<v t="ekr.20220525082935.1492"><vh>CallableType.try_synthesizing_arg_from_kwarg</vh></v>
<v t="ekr.20220525082935.1493"><vh>CallableType.try_synthesizing_arg_from_vararg</vh></v>
<v t="ekr.20220525082935.1494"><vh>CallableType.items</vh></v>
<v t="ekr.20220525082935.1495"><vh>CallableType.is_generic</vh></v>
<v t="ekr.20220525082935.1496"><vh>CallableType.type_var_ids</vh></v>
<v t="ekr.20220525082935.1497"><vh>CallableType.param_spec</vh></v>
<v t="ekr.20220525082935.1498"><vh>CallableType.expand_param_spec</vh></v>
<v t="ekr.20220525082935.1499"><vh>CallableType.__hash__</vh></v>
<v t="ekr.20220525082935.1500"><vh>CallableType.__eq__</vh></v>
<v t="ekr.20220525082935.1501"><vh>CallableType.serialize</vh></v>
<v t="ekr.20220525082935.1502"><vh>CallableType.deserialize</vh></v>
</v>
</v>
<v t="ekr.20220606101206.1"><vh>--- Hierarchy for FuncDef</vh>
<v t="ekr.20220525082934.880"><vh>class Node (abstract, but defines __str__)</vh>
<v t="ekr.20220525082934.881"><vh>Node.__str__</vh></v>
<v t="ekr.20220525082934.882"><vh>Node.accept</vh></v>
</v>
<v t="ekr.20220525082934.883"><vh>class Statement (abstract)</vh>
<v t="ekr.20220525082934.884"><vh>Statement.accept</vh></v>
</v>
<v t="ekr.20220525082934.889"><vh>class SymbolNode (abstract)</vh>
<v t="ekr.20220525082934.890"><vh>SymbolNode.name</vh></v>
<v t="ekr.20220525082934.891"><vh>SymbolNode.fullname</vh></v>
<v t="ekr.20220525082934.892"><vh>SymbolNode.serialize</vh></v>
<v t="ekr.20220525082934.893"><vh>SymbolNode.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.939"><vh>class FuncItem</vh>
<v t="ekr.20220525082934.940"><vh>FuncItem.__init__</vh></v>
<v t="ekr.20220525082934.941"><vh>FuncItem.max_fixed_argc</vh></v>
<v t="ekr.20220525082934.942"><vh>FuncItem.set_line</vh></v>
<v t="ekr.20220525082934.943"><vh>FuncItem.is_dynamic</vh></v>
</v>
<v t="ekr.20220525082934.945"><vh>class FuncDef(FuncItem, SymbolNode, Statement) (nodes.py)</vh>
<v t="ekr.20220525082934.946"><vh>FuncDef.__init__</vh></v>
<v t="ekr.20220525082934.947"><vh>FuncDef.name</vh></v>
<v t="ekr.20220525082934.948"><vh>FuncDef.accept</vh></v>
<v t="ekr.20220525082934.949"><vh>FuncDef.serialize</vh></v>
<v t="ekr.20220525082934.950"><vh>FuncDef.deserialize</vh></v>
</v>
</v>
</v>
<v t="ekr.20220608151139.1"><vh>--- classes</vh>
<v t="ekr.20220525082934.1067"><vh>class ArgKind</vh>
<v t="ekr.20220525082934.1068"><vh>ArgKind.is_positional</vh></v>
<v t="ekr.20220525082934.1069"><vh>ArgKind.is_named</vh></v>
<v t="ekr.20220525082934.1070"><vh>ArgKind.is_required</vh></v>
<v t="ekr.20220525082934.1071"><vh>ArgKind.is_optional</vh></v>
<v t="ekr.20220525082934.1072"><vh>ArgKind.is_star</vh></v>
</v>
<v t="ekr.20220525082934.935"><vh>class Argument</vh>
<v t="ekr.20220525082934.936"><vh>Argument.__init__</vh></v>
<v t="ekr.20220525082934.937"><vh>Argument.set_line</vh></v>
</v>
<v t="ekr.20220525082934.40"><vh>class ASTConverter (fastparse.py)</vh>
<v t="ekr.20220525082934.41"><vh>ASTConverter.__init__</vh></v>
<v t="ekr.20220614101220.1"><vh>ASTConverter: Messages</vh>
<v t="ekr.20220525082934.42"><vh>ASTConverter.note</vh></v>
<v t="ekr.20220525082934.43"><vh>ASTConverter.fail</vh></v>
<v t="ekr.20220525082934.44"><vh>ASTConverter.fail_merge_overload</vh></v>
</v>
<v t="ekr.20220614101246.1"><vh>ASTConverter: Utils</vh>
<v t="ekr.20220525082934.45"><vh>ASTConverter.visit</vh></v>
<v t="ekr.20220525082934.46"><vh>ASTConverter.set_line</vh></v>
<v t="ekr.20220525082934.47"><vh>ASTConverter.translate_opt_expr_list</vh></v>
<v t="ekr.20220525082934.48"><vh>ASTConverter.translate_expr_list</vh></v>
<v t="ekr.20220525082934.49"><vh>ASTConverter.get_lineno</vh></v>
<v t="ekr.20220525082934.50"><vh>ASTConverter.translate_stmt_list</vh></v>
<v t="ekr.20220525082934.51"><vh>ASTConverter.translate_type_comment</vh></v>
<v t="ekr.20220525082934.52"><vh>ASTConverter.var: op_map</vh></v>
<v t="ekr.20220525082934.53"><vh>ASTConverter.from_operator</vh></v>
<v t="ekr.20220525082934.54"><vh>ASTConverter.var: comp_op_map</vh></v>
<v t="ekr.20220525082934.55"><vh>ASTConverter.from_comp_operator</vh></v>
<v t="ekr.20220525082934.56"><vh>ASTConverter.as_block</vh></v>
<v t="ekr.20220525082934.57"><vh>ASTConverter.as_required_block</vh></v>
<v t="ekr.20220525082934.58"><vh>ASTConverter.fix_function_overloads</vh></v>
<v t="ekr.20220525082934.59"><vh>ASTConverter._check_ifstmt_for_overloads</vh></v>
<v t="ekr.20220525082934.60"><vh>ASTConverter._get_executable_if_block_with_overloads</vh></v>
<v t="ekr.20220525082934.61"><vh>ASTConverter._strip_contents_from_if_stmt</vh></v>
<v t="ekr.20220525082934.62"><vh>ASTConverter._is_stripped_if_stmt</vh></v>
<v t="ekr.20220525082934.63"><vh>ASTConverter.in_method_scope</vh></v>
<v t="ekr.20220525082934.64"><vh>ASTConverter.translate_module_id</vh></v>
</v>
<v t="ekr.20220614101407.1"><vh>ASTConverter: Modules &amp; Functions</vh>
<v t="ekr.20220525082934.65"><vh>ASTConverter.visit_Module</vh></v>
<v t="ekr.20220525082934.66"><vh>ASTConverter.visit_FunctionDef</vh></v>
<v t="ekr.20220525082934.67"><vh>ASTConverter.visit_AsyncFunctionDef</vh></v>
<v t="ekr.20220525082934.68"><vh>ASTConverter.do_func_def</vh></v>
<v t="ekr.20220525082934.69"><vh>ASTConverter.set_type_optional</vh></v>
<v t="ekr.20220525082934.70"></v>
<v t="ekr.20220525082934.71"></v>
<v t="ekr.20220525082934.72"><vh>ASTConverter.fail_arg</vh></v>
</v>
<v t="ekr.20220614100953.1"><vh>ASTConverter: Statements</vh>
<v t="ekr.20220525082934.73"><vh>ASTConverter.visit_ClassDef</vh></v>
<v t="ekr.20220525082934.74"><vh>ASTConverter.visit_Return</vh></v>
<v t="ekr.20220525082934.75"><vh>ASTConverter.visit_Delete</vh></v>
<v t="ekr.20220525082934.76"><vh>ASTConverter.visit_Assign</vh></v>
<v t="ekr.20220525082934.77"><vh>ASTConverter.visit_AnnAssign</vh></v>
<v t="ekr.20220525082934.78"><vh>ASTConverter.visit_AugAssign</vh></v>
<v t="ekr.20220525082934.79"><vh>ASTConverter.visit_For</vh></v>
<v t="ekr.20220525082934.80"><vh>ASTConverter.visit_AsyncFor</vh></v>
<v t="ekr.20220525082934.81"><vh>ASTConverter.visit_While</vh></v>
<v t="ekr.20220525082934.82"><vh>ASTConverter.visit_If</vh></v>
<v t="ekr.20220525082934.83"><vh>ASTConverter.visit_With</vh></v>
<v t="ekr.20220525082934.84"><vh>ASTConverter.visit_AsyncWith</vh></v>
<v t="ekr.20220525082934.85"><vh>ASTConverter.visit_Raise</vh></v>
<v t="ekr.20220525082934.86"><vh>ASTConverter.visit_Try</vh></v>
<v t="ekr.20220525082934.87"><vh>ASTConverter.visit_Assert</vh></v>
<v t="ekr.20220525082934.88"><vh>ASTConverter.visit_Import</vh></v>
<v t="ekr.20220525082934.89"><vh>ASTConverter.visit_ImportFrom</vh></v>
<v t="ekr.20220525082934.90"><vh>ASTConverter.visit_Global</vh></v>
<v t="ekr.20220525082934.91"><vh>ASTConverter.visit_Nonlocal</vh></v>
<v t="ekr.20220525082934.92"><vh>ASTConverter.visit_Expr</vh></v>
<v t="ekr.20220525082934.93"><vh>ASTConverter.visit_Pass</vh></v>
<v t="ekr.20220525082934.94"><vh>ASTConverter.visit_Break</vh></v>
<v t="ekr.20220525082934.95"><vh>ASTConverter.visit_Continue</vh></v>
</v>
<v t="ekr.20220525082934.96"><vh>ASTConverter: Expressions</vh>
<v t="ekr.20220525082934.97"><vh>ASTConverter.visit_NamedExpr</vh></v>
<v t="ekr.20220525082934.98"><vh>ASTConverter.visit_BoolOp</vh></v>
<v t="ekr.20220525082934.99"><vh>ASTConverter.group</vh></v>
<v t="ekr.20220525082934.100"><vh>ASTConverter.visit_BinOp</vh></v>
<v t="ekr.20220525082934.101"><vh>ASTConverter.visit_UnaryOp</vh></v>
<v t="ekr.20220525082934.102"><vh>ASTConverter.visit_Lambda</vh></v>
<v t="ekr.20220525082934.103"><vh>ASTConverter.visit_IfExp</vh></v>
<v t="ekr.20220525082934.104"><vh>ASTConverter.visit_Dict</vh></v>
<v t="ekr.20220525082934.105"><vh>ASTConverter.visit_Set</vh></v>
<v t="ekr.20220525082934.106"><vh>ASTConverter.visit_ListComp</vh></v>
<v t="ekr.20220525082934.107"><vh>ASTConverter.visit_SetComp</vh></v>
<v t="ekr.20220525082934.108"><vh>ASTConverter.visit_DictComp</vh></v>
<v t="ekr.20220525082934.109"><vh>ASTConverter.visit_GeneratorExp</vh></v>
<v t="ekr.20220525082934.110"><vh>ASTConverter.visit_Await</vh></v>
<v t="ekr.20220525082934.111"><vh>ASTConverter.visit_Yield</vh></v>
<v t="ekr.20220525082934.112"><vh>ASTConverter.visit_YieldFrom</vh></v>
<v t="ekr.20220525082934.113"><vh>ASTConverter.visit_Compare</vh></v>
<v t="ekr.20220525082934.114"><vh>ASTConverter.visit_Call</vh></v>
</v>
<v t="ekr.20220614101046.1"><vh>ASTConverter: Constants</vh>
<v t="ekr.20220525082934.115"><vh>ASTConverter.visit_Constant</vh></v>
<v t="ekr.20220525082934.116"><vh>ASTConverter.visit_Num</vh></v>
<v t="ekr.20220525082934.117"><vh>ASTConverter.visit_Str (fastparse.py)</vh></v>
<v t="ekr.20220525082934.118"><vh>ASTConverter.visit_JoinedStr</vh></v>
<v t="ekr.20220525082934.119"><vh>ASTConverter.visit_FormattedValue</vh></v>
<v t="ekr.20220525082934.120"><vh>ASTConverter.visit_Bytes</vh></v>
<v t="ekr.20220525082934.121"><vh>ASTConverter.visit_NameConstant</vh></v>
<v t="ekr.20220525082934.122"><vh>ASTConverter.visit_Ellipsis</vh></v>
<v t="ekr.20220525082934.123"><vh>ASTConverter.visit_Attribute</vh></v>
<v t="ekr.20220525082934.124"><vh>ASTConverter.visit_Subscript</vh></v>
<v t="ekr.20220525082934.125"><vh>ASTConverter.visit_Starred</vh></v>
<v t="ekr.20220525082934.126"><vh>ASTConverter.visit_Name</vh></v>
<v t="ekr.20220525082934.127"><vh>ASTConverter.visit_List</vh></v>
<v t="ekr.20220525082934.128"><vh>ASTConverter.visit_Tuple</vh></v>
</v>
<v t="ekr.20220525082934.129"><vh>ASTConverter: Slices</vh>
<v t="ekr.20220525082934.130"><vh>ASTConverter.visit_Slice</vh></v>
<v t="ekr.20220525082934.131"><vh>ASTConverter.visit_ExtSlice</vh></v>
<v t="ekr.20220525082934.132"><vh>ASTConverter.visit_Index</vh></v>
</v>
<v t="ekr.20220614101128.1"><vh>AstConverter: Match</vh>
<v t="ekr.20220525082934.133"><vh>ASTConverter.visit_Match</vh></v>
<v t="ekr.20220525082934.134"><vh>ASTConverter.visit_MatchValue</vh></v>
<v t="ekr.20220525082934.135"><vh>ASTConverter.visit_MatchSingleton</vh></v>
<v t="ekr.20220525082934.136"><vh>ASTConverter.visit_MatchSequence</vh></v>
<v t="ekr.20220525082934.137"><vh>ASTConverter.visit_MatchStar</vh></v>
<v t="ekr.20220525082934.138"><vh>ASTConverter.visit_MatchMapping</vh></v>
<v t="ekr.20220525082934.139"><vh>ASTConverter.visit_MatchClass</vh></v>
<v t="ekr.20220525082934.140"><vh>ASTConverter.visit_MatchAs</vh></v>
<v t="ekr.20220525082934.141"><vh>ASTConverter.visit_MatchOr</vh></v>
</v>
</v>
<v t="ekr.20220525082933.232"><vh>class BuildManager</vh>
<v t="ekr.20220525082933.233"><vh>BuildManager.__init__</vh></v>
<v t="ekr.20220525082933.234"><vh>BuildManager.dump_stats</vh></v>
<v t="ekr.20220525082933.235"><vh>BuildManager.use_fine_grained_cache</vh></v>
<v t="ekr.20220525082933.236"><vh>BuildManager.maybe_swap_for_shadow_path</vh></v>
<v t="ekr.20220525082933.237"><vh>BuildManager.get_stat</vh></v>
<v t="ekr.20220525082933.238"><vh>BuildManager.getmtime</vh></v>
<v t="ekr.20220525082933.239"><vh>BuildManager.all_imported_modules_in_file</vh></v>
<v t="ekr.20220525082933.240"><vh>BuildManager.is_module</vh></v>
<v t="ekr.20220525082933.241"><vh>BuildManager.parse_file (calls parse, returns tree)</vh></v>
<v t="ekr.20220525082933.242"><vh>BuildManager.load_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.243"><vh>BuildManager.report_file</vh></v>
<v t="ekr.20220525082933.244"><vh>BuildManager.verbosity</vh></v>
<v t="ekr.20220525082933.245"><vh>BuildManager.log</vh></v>
<v t="ekr.20220525082933.246"><vh>BuildManager.log_fine_grained</vh></v>
<v t="ekr.20220525082933.247"><vh>BuildManager.trace</vh></v>
<v t="ekr.20220525082933.248"><vh>BuildManager.add_stats</vh></v>
<v t="ekr.20220525082933.249"><vh>BuildManager.stats_summary</vh></v>
</v>
<v t="ekr.20220525082935.1476"></v>
<v t="ekr.20220525082935.1460"><vh>class FormalArgument</vh></v>
<v t="ekr.20220525082934.945"></v>
<v t="ekr.20220525082935.1444"></v>
<v t="ekr.20220525082934.895"><vh>class MypyFile(SymbolNode) (the AST of one file)</vh>
<v t="ekr.20220525082934.896"><vh>MypyFile.__init__</vh></v>
<v t="ekr.20220525082934.897"><vh>MypyFile.local_definitions</vh></v>
<v t="ekr.20220525082934.898"><vh>MypyFile.name</vh></v>
<v t="ekr.20220525082934.899"><vh>MypyFile.fullname</vh></v>
<v t="ekr.20220525082934.900"><vh>MypyFile.accept</vh></v>
<v t="ekr.20220525082934.901"><vh>MypyFile.is_package_init_file</vh></v>
<v t="ekr.20220525082934.902"><vh>MypyFile.is_future_flag_set</vh></v>
<v t="ekr.20220525082934.903"><vh>MypyFile.serialize</vh></v>
<v t="ekr.20220525082934.904"><vh>MypyFile.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1"><vh>class SemanticAnalyzer</vh>
<v t="ekr.20220525082935.2"><vh>SA.__init__</vh></v>
<v t="ekr.20220619062539.1"><vh>SA: properties</vh>
<v t="ekr.20220525082935.3"><vh>SA.is_stub_file</vh></v>
<v t="ekr.20220525082935.4"><vh>SA.is_typeshed_stub_file</vh></v>
<v t="ekr.20220525082935.5"><vh>SA.final_iteration</vh></v>
</v>
<v t="ekr.20220525082935.6"><vh>SA: preparation</vh>
<v t="ekr.20220525082935.7"><vh>SA.prepare_file</vh></v>
<v t="ekr.20220525082935.8"><vh>SA.prepare_typing_namespace</vh></v>
<v t="ekr.20220525082935.9"><vh>SA.prepare_builtins_namespace</vh></v>
</v>
<v t="ekr.20220525082935.10"><vh>SA: analyze targets</vh>
<v t="ekr.20220525082935.11"><vh>SA.refresh_partial</vh></v>
<v t="ekr.20220525082935.12"><vh>SA.refresh_top_level</vh></v>
<v t="ekr.20220525082935.13"><vh>SA.add_implicit_module_attrs</vh></v>
<v t="ekr.20220525082935.14"><vh>SA.add_builtin_aliases</vh></v>
<v t="ekr.20220525082935.15"><vh>SA.add_typing_extension_aliases</vh></v>
<v t="ekr.20220525082935.16"><vh>SA.create_alias</vh></v>
<v t="ekr.20220525082935.17"><vh>SA.adjust_public_exports</vh></v>
<v t="ekr.20220525082935.18"><vh>SA.file_context</vh></v>
</v>
<v t="ekr.20220525082935.19"><vh>SA: analyze functions</vh>
<v t="ekr.20220525082935.20"><vh>SA.visit_func_def</vh></v>
<v t="ekr.20220525082935.21"></v>
<v t="ekr.20220525082935.22"><vh>SA.prepare_method_signature</vh></v>
<v t="ekr.20220525082935.23"><vh>SA.set_original_def</vh></v>
<v t="ekr.20220525082935.24"><vh>SA.update_function_type_variables (trace ???)</vh></v>
<v t="ekr.20220525082935.25"><vh>SA.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.26"><vh>SA.analyze_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.27"><vh>SA.analyze_overload_sigs_and_impl</vh></v>
<v t="ekr.20220525082935.28"><vh>SA.handle_missing_overload_decorators</vh></v>
<v t="ekr.20220525082935.29"><vh>SA.handle_missing_overload_implementation</vh></v>
<v t="ekr.20220525082935.30"><vh>SA.process_final_in_overload</vh></v>
<v t="ekr.20220525082935.31"><vh>SA.process_static_or_class_method_in_overload</vh></v>
<v t="ekr.20220525082935.32"><vh>SA.analyze_property_with_multi_part_definition</vh></v>
<v t="ekr.20220525082935.33"><vh>SA.add_function_to_symbol_table</vh></v>
<v t="ekr.20220525082935.34"></v>
<v t="ekr.20220525082935.35"><vh>SA.analyze_function_body</vh></v>
<v t="ekr.20220525082935.36"><vh>SA.check_classvar_in_signature</vh></v>
<v t="ekr.20220525082935.37"><vh>SA.check_function_signature (trace???)</vh></v>
<v t="ekr.20220525082935.38"><vh>SA.visit_decorator</vh></v>
<v t="ekr.20220525082935.39"><vh>SA.check_decorated_function_is_method</vh></v>
</v>
<v t="ekr.20220525082935.40"><vh>SA: analyze classes</vh>
<v t="ekr.20220525082935.41"><vh>SA.visit_class_def</vh></v>
<v t="ekr.20220525082935.42"><vh>SA.analyze_class</vh></v>
<v t="ekr.20220525082935.43"><vh>SA.is_core_builtin_class</vh></v>
<v t="ekr.20220525082935.44"><vh>SA.analyze_class_body_common</vh></v>
<v t="ekr.20220525082935.45"><vh>SA.analyze_namedtuple_classdef</vh></v>
<v t="ekr.20220525082935.46"><vh>SA.apply_class_plugin_hooks</vh></v>
<v t="ekr.20220525082935.47"><vh>SA.get_fullname_for_hook</vh></v>
<v t="ekr.20220525082935.48"><vh>SA.analyze_class_keywords</vh></v>
<v t="ekr.20220525082935.49"><vh>SA.enter_class</vh></v>
<v t="ekr.20220525082935.50"><vh>SA.leave_class</vh></v>
<v t="ekr.20220525082935.51"><vh>SA.analyze_class_decorator</vh></v>
<v t="ekr.20220525082935.52"><vh>SA.clean_up_bases_and_infer_type_variables</vh></v>
<v t="ekr.20220525082935.53"><vh>SA.analyze_class_typevar_declaration</vh></v>
<v t="ekr.20220525082935.54"><vh>SA.analyze_unbound_tvar</vh></v>
<v t="ekr.20220525082935.55"><vh>SA.get_all_bases_tvars</vh></v>
<v t="ekr.20220525082935.56"><vh>SA.prepare_class_def</vh></v>
<v t="ekr.20220525082935.57"><vh>SA.make_empty_type_info</vh></v>
<v t="ekr.20220525082935.58"><vh>SA.get_name_repr_of_expr</vh></v>
<v t="ekr.20220525082935.59"><vh>SA.analyze_base_classes</vh></v>
<v t="ekr.20220525082935.60"><vh>SA.configure_base_classes</vh></v>
<v t="ekr.20220525082935.61"><vh>SA.configure_tuple_base_class</vh></v>
<v t="ekr.20220525082935.62"><vh>SA.set_dummy_mro</vh></v>
<v t="ekr.20220525082935.63"><vh>SA.calculate_class_mro</vh></v>
<v t="ekr.20220525082935.64"><vh>SA.update_metaclass</vh></v>
<v t="ekr.20220525082935.65"><vh>SA.verify_base_classes</vh></v>
<v t="ekr.20220525082935.66"><vh>SA.is_base_class</vh></v>
<v t="ekr.20220525082935.67"><vh>SA.analyze_metaclass</vh></v>
</v>
<v t="ekr.20220525082935.68"><vh>SA: analyze imports</vh>
<v t="ekr.20220525082935.69"><vh>SA.visit_import</vh></v>
<v t="ekr.20220525082935.70"><vh>SA.visit_import_from</vh></v>
<v t="ekr.20220525082935.71"><vh>SA.process_imported_symbol</vh></v>
<v t="ekr.20220525082935.72"><vh>SA.report_missing_module_attribute</vh></v>
<v t="ekr.20220525082935.73"><vh>SA.process_import_over_existing_name</vh></v>
<v t="ekr.20220525082935.74"><vh>SA.correct_relative_import</vh></v>
<v t="ekr.20220525082935.75"><vh>SA.visit_import_all</vh></v>
</v>
<v t="ekr.20220525082935.76"><vh>SA: analyze assignments</vh>
<v t="ekr.20220525082935.77"><vh>SA.visit_assignment_expr</vh></v>
<v t="ekr.20220525082935.78"><vh>SA.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.79"><vh>SA.analyze_identity_global_assignment</vh></v>
<v t="ekr.20220525082935.80"><vh>SA.should_wait_rhs</vh></v>
<v t="ekr.20220525082935.81"><vh>SA.can_be_type_alias</vh></v>
<v t="ekr.20220525082935.82"><vh>SA.is_type_ref</vh></v>
<v t="ekr.20220525082935.83"><vh>SA.is_none_alias</vh></v>
<v t="ekr.20220525082935.84"><vh>SA.record_special_form_lvalue</vh></v>
<v t="ekr.20220525082935.85"><vh>SA.analyze_enum_assign</vh></v>
<v t="ekr.20220525082935.86"><vh>SA.analyze_namedtuple_assign</vh></v>
<v t="ekr.20220525082935.87"><vh>SA.analyze_typeddict_assign</vh></v>
<v t="ekr.20220525082935.88"><vh>SA.analyze_lvalues</vh></v>
<v t="ekr.20220525082935.89"><vh>SA.apply_dynamic_class_hook</vh></v>
<v t="ekr.20220525082935.90"><vh>SA.unwrap_final</vh></v>
<v t="ekr.20220525082935.91"><vh>SA.check_final_implicit_def</vh></v>
<v t="ekr.20220525082935.92"><vh>SA.store_final_status</vh></v>
<v t="ekr.20220525082935.93"><vh>SA.flatten_lvalues</vh></v>
<v t="ekr.20220525082935.94"><vh>SA.unbox_literal</vh></v>
<v t="ekr.20220525082935.95"><vh>SA.process_type_annotation</vh>
<v t="ekr.20220612061447.1"><vh>&lt;&lt; define callers &amp; callerName &gt;&gt;</vh></v>
</v>
<v t="ekr.20220525082935.96"><vh>SA.is_annotated_protocol_member</vh></v>
<v t="ekr.20220525082935.97"><vh>SA.analyze_simple_literal_type (****)</vh></v>
<v t="ekr.20220525082935.98"><vh>SA.analyze_alias</vh></v>
<v t="ekr.20220525082935.99"><vh>SA.check_and_set_up_type_alias</vh></v>
<v t="ekr.20220525082935.100"><vh>SA.analyze_lvalue</vh></v>
<v t="ekr.20220525082935.101"><vh>SA.analyze_name_lvalue</vh></v>
<v t="ekr.20220525082935.102"><vh>SA.is_final_redefinition</vh></v>
<v t="ekr.20220525082935.103"><vh>SA.is_alias_for_final_name</vh></v>
<v t="ekr.20220525082935.104"><vh>SA.make_name_lvalue_var</vh></v>
<v t="ekr.20220525082935.105"><vh>SA.make_name_lvalue_point_to_existing_def</vh></v>
<v t="ekr.20220525082935.106"><vh>SA.analyze_tuple_or_list_lvalue</vh></v>
<v t="ekr.20220525082935.107"><vh>SA.analyze_member_lvalue</vh></v>
<v t="ekr.20220525082935.108"><vh>SA.is_self_member_ref</vh></v>
<v t="ekr.20220525082935.109"><vh>SA.check_lvalue_validity</vh></v>
<v t="ekr.20220525082935.110"><vh>SA.store_declared_types</vh></v>
<v t="ekr.20220525082935.111"><vh>SA.process_typevar_declaration</vh></v>
<v t="ekr.20220525082935.112"><vh>SA.check_typevarlike_name</vh></v>
<v t="ekr.20220525082935.113"><vh>SA.get_typevarlike_declaration</vh></v>
<v t="ekr.20220525082935.114"><vh>SA.process_typevar_parameters</vh></v>
<v t="ekr.20220525082935.115"><vh>SA.extract_typevarlike_name</vh></v>
<v t="ekr.20220525082935.116"><vh>SA.process_paramspec_declaration</vh></v>
<v t="ekr.20220525082935.117"><vh>SA.process_typevartuple_declaration</vh></v>
<v t="ekr.20220525082935.118"><vh>SA.basic_new_typeinfo</vh></v>
<v t="ekr.20220525082935.119"><vh>SA.analyze_value_types</vh></v>
<v t="ekr.20220525082935.120"><vh>SA.check_classvar</vh></v>
<v t="ekr.20220525082935.121"><vh>SA.is_classvar</vh></v>
<v t="ekr.20220525082935.122"><vh>SA.is_final_type</vh></v>
<v t="ekr.20220525082935.123"><vh>SA.fail_invalid_classvar</vh></v>
<v t="ekr.20220525082935.124"><vh>SA.process_module_assignment</vh></v>
<v t="ekr.20220525082935.125"><vh>SA.process__all__</vh></v>
<v t="ekr.20220525082935.126"><vh>SA.process__deletable__</vh></v>
<v t="ekr.20220525082935.127"><vh>SA.process__slots__</vh></v>
</v>
<v t="ekr.20220525082935.128"><vh>SA: analyze misc statements</vh>
<v t="ekr.20220525082935.129"><vh>SA.visit_block</vh></v>
<v t="ekr.20220525082935.130"><vh>SA.visit_block_maybe</vh></v>
<v t="ekr.20220525082935.131"><vh>SA.visit_expression_stmt</vh></v>
<v t="ekr.20220525082935.132"><vh>SA.visit_return_stmt</vh></v>
<v t="ekr.20220525082935.133"><vh>SA.visit_raise_stmt</vh></v>
<v t="ekr.20220525082935.134"><vh>SA.visit_assert_stmt</vh></v>
<v t="ekr.20220525082935.135"><vh>SA.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082935.136"><vh>SA.visit_while_stmt</vh></v>
<v t="ekr.20220525082935.137"><vh>SA.visit_for_stmt</vh></v>
<v t="ekr.20220525082935.138"><vh>SA.visit_break_stmt</vh></v>
<v t="ekr.20220525082935.139"><vh>SA.visit_continue_stmt</vh></v>
<v t="ekr.20220525082935.140"><vh>SA.visit_if_stmt</vh></v>
<v t="ekr.20220525082935.141"><vh>SA.visit_try_stmt</vh></v>
<v t="ekr.20220525082935.142"><vh>SA.analyze_try_stmt</vh></v>
<v t="ekr.20220525082935.143"><vh>SA.visit_with_stmt</vh></v>
<v t="ekr.20220525082935.144"><vh>SA.visit_del_stmt</vh></v>
<v t="ekr.20220525082935.145"><vh>SA.is_valid_del_target</vh></v>
<v t="ekr.20220525082935.146"><vh>SA.visit_global_decl</vh></v>
<v t="ekr.20220525082935.147"><vh>SA.visit_nonlocal_decl</vh></v>
<v t="ekr.20220525082935.148"><vh>SA.visit_print_stmt</vh></v>
<v t="ekr.20220525082935.149"><vh>SA.visit_exec_stmt</vh></v>
<v t="ekr.20220525082935.150"><vh>SA.visit_match_stmt</vh></v>
</v>
<v t="ekr.20220525082935.151"><vh>SA: analyze expressions</vh>
<v t="ekr.20220525082935.152"><vh>SA.visit_name_expr</vh></v>
<v t="ekr.20220525082935.153"><vh>SA.bind_name_expr</vh></v>
<v t="ekr.20220525082935.154"><vh>SA.visit_super_expr</vh></v>
<v t="ekr.20220525082935.155"><vh>SA.visit_tuple_expr</vh></v>
<v t="ekr.20220525082935.156"><vh>SA.visit_list_expr</vh></v>
<v t="ekr.20220525082935.157"><vh>SA.visit_set_expr</vh></v>
<v t="ekr.20220525082935.158"><vh>SA.visit_dict_expr</vh></v>
<v t="ekr.20220525082935.159"><vh>SA.visit_star_expr</vh></v>
<v t="ekr.20220525082935.160"><vh>SA.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082935.161"><vh>SA.visit_call_expr</vh></v>
<v t="ekr.20220525082935.162"><vh>SA.translate_dict_call</vh></v>
<v t="ekr.20220525082935.163"><vh>SA.check_fixed_args</vh></v>
<v t="ekr.20220525082935.164"><vh>SA.visit_member_expr</vh></v>
<v t="ekr.20220525082935.165"><vh>SA.visit_op_expr</vh></v>
<v t="ekr.20220525082935.166"><vh>SA.visit_comparison_expr</vh></v>
<v t="ekr.20220525082935.167"><vh>SA.visit_unary_expr</vh></v>
<v t="ekr.20220525082935.168"><vh>SA.visit_index_expr</vh></v>
<v t="ekr.20220525082935.169"><vh>SA.analyze_type_application</vh></v>
<v t="ekr.20220525082935.170"><vh>SA.analyze_type_application_args</vh></v>
<v t="ekr.20220525082935.171"><vh>SA.visit_slice_expr</vh></v>
<v t="ekr.20220525082935.172"><vh>SA.visit_cast_expr</vh></v>
<v t="ekr.20220525082935.173"><vh>SA.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082935.174"><vh>SA.visit_reveal_expr</vh></v>
<v t="ekr.20220525082935.175"><vh>SA.visit_type_application</vh></v>
<v t="ekr.20220525082935.176"><vh>SA.visit_list_comprehension</vh></v>
<v t="ekr.20220525082935.177"><vh>SA.visit_set_comprehension</vh></v>
<v t="ekr.20220525082935.178"><vh>SA.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082935.179"><vh>SA.visit_generator_expr</vh></v>
<v t="ekr.20220525082935.180"><vh>SA.analyze_comp_for</vh></v>
<v t="ekr.20220525082935.181"><vh>SA.analyze_comp_for_2</vh></v>
<v t="ekr.20220525082935.182"><vh>SA.visit_lambda_expr</vh></v>
<v t="ekr.20220525082935.183"><vh>SA.visit_conditional_expr</vh></v>
<v t="ekr.20220525082935.184"><vh>SA.visit_backquote_expr</vh></v>
<v t="ekr.20220525082935.185"><vh>SA.visit__promote_expr</vh></v>
<v t="ekr.20220525082935.186"><vh>SA.visit_yield_expr</vh></v>
<v t="ekr.20220525082935.187"><vh>SA.visit_await_expr</vh></v>
</v>
<v t="ekr.20220525082935.188"><vh>SA: analyze patterns</vh>
<v t="ekr.20220525082935.189"><vh>SA.visit_as_pattern</vh></v>
<v t="ekr.20220525082935.190"><vh>SA.visit_or_pattern</vh></v>
<v t="ekr.20220525082935.191"><vh>SA.visit_value_pattern</vh></v>
<v t="ekr.20220525082935.192"><vh>SA.visit_sequence_pattern</vh></v>
<v t="ekr.20220525082935.193"><vh>SA.visit_starred_pattern</vh></v>
<v t="ekr.20220525082935.194"><vh>SA.visit_mapping_pattern</vh></v>
<v t="ekr.20220525082935.195"><vh>SA.visit_class_pattern</vh></v>
</v>
<v t="ekr.20220525082935.196"><vh>SA: lookup functions</vh>
<v t="ekr.20220525082935.197"><vh>SA.lookup</vh></v>
<v t="ekr.20220525082935.198"><vh>SA.is_active_symbol_in_class_body</vh></v>
<v t="ekr.20220525082935.199"><vh>SA.is_textually_before_statement</vh></v>
<v t="ekr.20220525082935.200"><vh>SA.is_overloaded_item</vh></v>
<v t="ekr.20220525082935.201"><vh>SA.is_defined_in_current_module</vh></v>
<v t="ekr.20220525082935.202"><vh>SA.lookup_qualified</vh></v>
<v t="ekr.20220525082935.203"><vh>SA.lookup_type_node</vh></v>
<v t="ekr.20220525082935.204"><vh>SA.get_module_symbol</vh></v>
<v t="ekr.20220525082935.205"><vh>SA.is_missing_module</vh></v>
<v t="ekr.20220525082935.206"><vh>SA.implicit_symbol</vh></v>
<v t="ekr.20220525082935.207"><vh>SA.create_getattr_var</vh></v>
<v t="ekr.20220525082935.208"><vh>SA.lookup_fully_qualified (added trace)</vh></v>
<v t="ekr.20220525082935.209"><vh>SA.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20220525082935.210"><vh>SA.object_type</vh></v>
<v t="ekr.20220525082935.211"><vh>SA.str_type</vh></v>
<v t="ekr.20220525082935.212"><vh>SA.named_type</vh></v>
<v t="ekr.20220525082935.213"><vh>SA.named_type_or_none</vh></v>
<v t="ekr.20220525082935.214"><vh>SA.builtin_type</vh></v>
<v t="ekr.20220525082935.215"><vh>SA.lookup_current_scope</vh></v>
</v>
<v t="ekr.20220525082935.216"><vh>SA: adding symbols</vh>
<v t="ekr.20220525082935.217"><vh>SA.add_symbol</vh></v>
<v t="ekr.20220525082935.218"><vh>SA.add_symbol_skip_local</vh></v>
<v t="ekr.20220525082935.219"><vh>SA.add_symbol_table_node</vh></v>
<v t="ekr.20220525082935.220"><vh>SA.add_redefinition</vh></v>
<v t="ekr.20220525082935.221"><vh>SA.add_local</vh></v>
<v t="ekr.20220525082935.222"><vh>SA.add_module_symbol</vh></v>
<v t="ekr.20220525082935.223"><vh>SA._get_node_for_class_scoped_import</vh></v>
<v t="ekr.20220525082935.224"><vh>SA.add_imported_symbol</vh></v>
<v t="ekr.20220525082935.225"><vh>SA.add_unknown_imported_symbol</vh></v>
</v>
<v t="ekr.20220525082935.226"><vh>SA: other helpers</vh>
<v t="ekr.20220525082935.227"><vh>SA.tvar_scope_frame</vh></v>
<v t="ekr.20220525082935.228"><vh>SA.defer</vh></v>
<v t="ekr.20220525082935.229"><vh>SA.track_incomplete_refs</vh></v>
<v t="ekr.20220525082935.230"><vh>SA.found_incomplete_ref</vh></v>
<v t="ekr.20220525082935.231"><vh>SA.record_incomplete_ref</vh></v>
<v t="ekr.20220525082935.232"><vh>SA.mark_incomplete</vh></v>
<v t="ekr.20220525082935.233"><vh>SA.is_incomplete_namespace</vh></v>
<v t="ekr.20220525082935.234"><vh>SA.process_placeholder</vh></v>
<v t="ekr.20220525082935.235"><vh>SA.cannot_resolve_name</vh></v>
<v t="ekr.20220525082935.236"><vh>SA.qualified_name</vh></v>
<v t="ekr.20220525082935.237"><vh>SA.enter</vh></v>
<v t="ekr.20220525082935.238"><vh>SA.is_func_scope</vh></v>
<v t="ekr.20220525082935.239"><vh>SA.is_nested_within_func_scope</vh></v>
<v t="ekr.20220525082935.240"><vh>SA.is_class_scope</vh></v>
<v t="ekr.20220525082935.241"><vh>SA.is_module_scope</vh></v>
<v t="ekr.20220525082935.242"><vh>SA.current_symbol_kind</vh></v>
<v t="ekr.20220525082935.243"><vh>SA.current_symbol_table</vh></v>
<v t="ekr.20220525082935.244"><vh>SA.is_global_or_nonlocal</vh></v>
<v t="ekr.20220525082935.245"><vh>SA.add_exports</vh></v>
<v t="ekr.20220525082935.246"><vh>SA.name_not_defined</vh></v>
<v t="ekr.20220525082935.247"><vh>SA.already_defined</vh></v>
<v t="ekr.20220525082935.248"><vh>SA.name_already_defined</vh></v>
<v t="ekr.20220525082935.249"><vh>SA.attribute_already_defined</vh></v>
<v t="ekr.20220525082935.250"><vh>SA.is_local_name</vh></v>
<v t="ekr.20220525082935.251"><vh>SA.in_checked_function</vh></v>
<v t="ekr.20220525082935.252"><vh>SA.fail</vh></v>
<v t="ekr.20220525082935.253"><vh>SA.note</vh></v>
<v t="ekr.20220525082935.254"><vh>SA.accept</vh></v>
<v t="ekr.20220525082935.255"><vh>SA.expr_to_analyzed_type</vh></v>
<v t="ekr.20220525082935.256"><vh>SA.analyze_type_expr</vh></v>
<v t="ekr.20220525082935.257"><vh>SA.type_analyzer</vh></v>
<v t="ekr.20220525082935.258"><vh>SA.expr_to_unanalyzed_type</vh></v>
<v t="ekr.20220525082935.259"><vh>SA.anal_type</vh></v>
<v t="ekr.20220525082935.260"><vh>SA.class_type</vh></v>
<v t="ekr.20220525082935.261"><vh>SA.schedule_patch</vh></v>
<v t="ekr.20220525082935.262"><vh>SA.report_hang</vh></v>
<v t="ekr.20220525082935.263"><vh>SA.add_plugin_dependency</vh></v>
<v t="ekr.20220525082935.264"><vh>SA.add_type_alias_deps</vh></v>
<v t="ekr.20220525082935.265"><vh>SA.is_mangled_global</vh></v>
<v t="ekr.20220525082935.266"><vh>SA.is_initial_mangled_global</vh></v>
<v t="ekr.20220525082935.267"><vh>SA.parse_bool</vh></v>
<v t="ekr.20220525082935.268"><vh>SA.set_future_import_flags</vh></v>
<v t="ekr.20220525082935.269"><vh>SA.is_future_flag_set</vh></v>
</v>
</v>
<v t="ekr.20220525082933.274"><vh>class State</vh>
<v t="ekr.20220525082933.275"><vh>State.__init__ (may call self.parse_file)</vh></v>
<v t="ekr.20220525082933.276"><vh>State.xmeta</vh></v>
<v t="ekr.20220525082933.277"><vh>State.add_ancestors</vh></v>
<v t="ekr.20220525082933.278"><vh>State.is_fresh</vh></v>
<v t="ekr.20220525082933.279"><vh>State.is_interface_fresh</vh></v>
<v t="ekr.20220525082933.280"><vh>State.mark_as_rechecked</vh></v>
<v t="ekr.20220525082933.281"><vh>State.mark_interface_stale</vh></v>
<v t="ekr.20220525082933.282"><vh>State.check_blockers</vh></v>
<v t="ekr.20220525082933.283"><vh>State.wrap_context</vh></v>
<v t="ekr.20220525082933.284"><vh>State.load_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.285"><vh>State.load_tree</vh></v>
<v t="ekr.20220525082933.286"><vh>State.fix_cross_refs</vh></v>
<v t="ekr.20220525082933.287"><vh>--- Methods for processing modules from source code</vh>
<v t="ekr.20220525082933.288"><vh>State.parse_file (BuildManager.modules[self.id] = self.tree)</vh></v>
<v t="ekr.20220525082933.289"><vh>State.parse_inline_configuration</vh></v>
<v t="ekr.20220525082933.290"><vh>State.semantic_analysis_pass1</vh></v>
<v t="ekr.20220525082933.291"><vh>State.add_dependency</vh></v>
<v t="ekr.20220525082933.292"><vh>State.suppress_dependency</vh></v>
<v t="ekr.20220525082933.293"><vh>State.compute_dependencies (special case for 'builtins')</vh></v>
<v t="ekr.20220525082933.294"><vh>State.type_check_first_pass</vh></v>
<v t="ekr.20220525082933.295"><vh>State.type_checker</vh></v>
<v t="ekr.20220525082933.296"><vh>State.type_map</vh></v>
<v t="ekr.20220525082933.297"><vh>State.type_check_second_pass</vh></v>
<v t="ekr.20220525082933.298"><vh>State.finish_passes</vh></v>
<v t="ekr.20220525082933.299"><vh>State.free_state</vh></v>
<v t="ekr.20220525082933.300"><vh>State._patch_indirect_dependencies</vh></v>
<v t="ekr.20220525082933.301"><vh>State.compute_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.302"><vh>State.update_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.303"><vh>State.valid_references</vh></v>
<v t="ekr.20220525082933.304"><vh>State.write_cache</vh></v>
<v t="ekr.20220525082933.305"><vh>State.verify_dependencies</vh></v>
<v t="ekr.20220525082933.306"><vh>State.dependency_priorities</vh></v>
<v t="ekr.20220525082933.307"><vh>State.dependency_lines</vh></v>
<v t="ekr.20220525082933.308"><vh>State.generate_unused_ignore_notes</vh></v>
<v t="ekr.20220525082933.309"><vh>State.generate_ignore_without_code_notes</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1039"><vh>class StrExpr</vh>
<v t="ekr.20220525082934.1040"><vh>StrExpr.__init__</vh></v>
<v t="ekr.20220525082934.1041"><vh>StrExpr.accept</vh></v>
</v>
<v t="ekr.20220525082935.1319"></v>
<v t="ekr.20220525082935.1178"><vh>class TypeAnalyser ***</vh>
<v t="ekr.20220525082935.1179"><vh>TA.__init__</vh></v>
<v t="ekr.20220525082935.1180"><vh>TA.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.1181"><vh>TA.visit_unbound_type_nonoptional</vh></v>
<v t="ekr.20220525082935.1182"><vh>TA.cannot_resolve_type</vh></v>
<v t="ekr.20220525082935.1183"><vh>TA.apply_concatenate_operator</vh></v>
<v t="ekr.20220525082935.1184"><vh>TA.try_analyze_special_unbound_type</vh></v>
<v t="ekr.20220525082935.1185"><vh>TA.get_omitted_any</vh></v>
<v t="ekr.20220525082935.1186"><vh>TA.analyze_type_with_type_info</vh></v>
<v t="ekr.20220525082935.1187"><vh>TA.analyze_unbound_type_without_type_info</vh></v>
<v t="ekr.20220525082935.1188"><vh>TA.visit_any</vh></v>
<v t="ekr.20220525082935.1189"><vh>TA.visit_none_type</vh></v>
<v t="ekr.20220525082935.1190"><vh>TA.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.1191"><vh>TA.visit_erased_type</vh></v>
<v t="ekr.20220525082935.1192"><vh>TA.visit_deleted_type</vh></v>
<v t="ekr.20220525082935.1193"><vh>TA.visit_type_list</vh></v>
<v t="ekr.20220525082935.1194"><vh>TA.visit_callable_argument</vh></v>
<v t="ekr.20220525082935.1195"><vh>TA.visit_instance</vh></v>
<v t="ekr.20220525082935.1196"><vh>TA.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.1197"><vh>TA.visit_type_var</vh></v>
<v t="ekr.20220525082935.1198"><vh>TA.visit_param_spec</vh></v>
<v t="ekr.20220525082935.1199"><vh>TA.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082935.1200"><vh>TA.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.1201"><vh>TA.visit_parameters</vh></v>
<v t="ekr.20220525082935.1202"></v>
<v t="ekr.20220525082935.1203"><vh>TA.anal_type_guard</vh></v>
<v t="ekr.20220525082935.1204"><vh>TA.anal_type_guard_arg</vh></v>
<v t="ekr.20220525082935.1205"><vh>TA.anal_star_arg_type</vh></v>
<v t="ekr.20220525082935.1206"><vh>TA.visit_overloaded</vh></v>
<v t="ekr.20220525082935.1207"><vh>TA.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.1208"><vh>TA.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.1209"><vh>TA.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082935.1210"><vh>TA.visit_literal_type</vh></v>
<v t="ekr.20220525082935.1211"><vh>TA.visit_star_type</vh></v>
<v t="ekr.20220525082935.1212"><vh>TA.visit_union_type</vh></v>
<v t="ekr.20220525082935.1213"><vh>TA.visit_partial_type</vh></v>
<v t="ekr.20220525082935.1214"><vh>TA.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082935.1215"><vh>TA.visit_type_type</vh></v>
<v t="ekr.20220525082935.1216"><vh>TA.visit_placeholder_type</vh></v>
<v t="ekr.20220525082935.1217"><vh>TA.analyze_callable_args_for_paramspec</vh></v>
<v t="ekr.20220525082935.1218"><vh>TA.analyze_callable_args_for_concatenate</vh></v>
<v t="ekr.20220525082935.1219"><vh>TA.analyze_callable_type</vh></v>
<v t="ekr.20220525082935.1220"><vh>TA.analyze_callable_args</vh></v>
<v t="ekr.20220525082935.1221"><vh>TA.analyze_literal_type</vh></v>
<v t="ekr.20220525082935.1222"><vh>TA.analyze_literal_param</vh></v>
<v t="ekr.20220525082935.1223"><vh>TA.analyze_type</vh></v>
<v t="ekr.20220525082935.1224"><vh>TA.fail</vh></v>
<v t="ekr.20220525082935.1225"><vh>TA.note</vh></v>
<v t="ekr.20220525082935.1226"><vh>TA.tvar_scope_frame</vh></v>
<v t="ekr.20220525082935.1227"><vh>TA.infer_type_variables</vh></v>
<v t="ekr.20220525082935.1228"><vh>TA.bind_function_type_variables</vh></v>
<v t="ekr.20220525082935.1229"><vh>TA.is_defined_type_var</vh></v>
<v t="ekr.20220525082935.1230"><vh>TA.anal_array</vh></v>
<v t="ekr.20220525082935.1231"></v>
<v t="ekr.20220525082935.1232"><vh>TA.anal_var_def</vh></v>
<v t="ekr.20220525082935.1233"><vh>TA.anal_var_defs</vh></v>
<v t="ekr.20220525082935.1234"><vh>TA.named_type_with_normalized_str</vh></v>
<v t="ekr.20220525082935.1235"><vh>TA.named_type</vh></v>
<v t="ekr.20220525082935.1236"><vh>TA.tuple_type</vh></v>
<v t="ekr.20220525082935.1237"><vh>TA.set_allow_param_spec_literals</vh></v>
</v>
<v t="ekr.20220525082935.1603"><vh>class TypeStrVisitor</vh>
<v t="ekr.20220525082935.1604"><vh>TypeStrVisitor.__init__</vh></v>
<v t="ekr.20220525082935.1605"><vh>TypeStrVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.1606"><vh>TypeStrVisitor.visit_type_list</vh></v>
<v t="ekr.20220525082935.1607"><vh>TypeStrVisitor.visit_callable_argument</vh></v>
<v t="ekr.20220525082935.1608"><vh>TypeStrVisitor.visit_any</vh></v>
<v t="ekr.20220525082935.1609"><vh>TypeStrVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082935.1610"><vh>TypeStrVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.1611"><vh>TypeStrVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082935.1612"><vh>TypeStrVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082935.1613"><vh>TypeStrVisitor.visit_instance</vh></v>
<v t="ekr.20220525082935.1614"><vh>TypeStrVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082935.1615"><vh>TypeStrVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082935.1616"><vh>TypeStrVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082935.1617"><vh>TypeStrVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082935.1618"><vh>TypeStrVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082935.1619"><vh>TypeStrVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082935.1620"><vh>TypeStrVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.1621"><vh>TypeStrVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.1622"><vh>TypeStrVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082935.1623"><vh>TypeStrVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082935.1624"><vh>TypeStrVisitor.visit_star_type</vh></v>
<v t="ekr.20220525082935.1625"><vh>TypeStrVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082935.1626"><vh>TypeStrVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082935.1627"><vh>TypeStrVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082935.1628"><vh>TypeStrVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082935.1629"><vh>TypeStrVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20220525082935.1630"><vh>TypeStrVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.1631"><vh>TypeStrVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.1632"><vh>TypeStrVisitor.list_str</vh></v>
</v>
<v t="ekr.20220525082936.34"><vh>class TypeTranslator</vh>
<v t="ekr.20220525082936.35"><vh>TT.visit_unbound_type</vh></v>
<v t="ekr.20220525082936.36"><vh>TT.visit_any</vh></v>
<v t="ekr.20220525082936.37"><vh>TT.visit_none_type</vh></v>
<v t="ekr.20220525082936.38"><vh>TT.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082936.39"><vh>TT.visit_erased_type</vh></v>
<v t="ekr.20220525082936.40"><vh>TT.visit_deleted_type</vh></v>
<v t="ekr.20220525082936.41"><vh>TT.visit_instance</vh></v>
<v t="ekr.20220525082936.42"><vh>TT.visit_type_var</vh></v>
<v t="ekr.20220525082936.43"><vh>TT.visit_param_spec</vh></v>
<v t="ekr.20220525082936.44"><vh>TT.visit_parameters</vh></v>
<v t="ekr.20220525082936.45"><vh>TT.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082936.46"><vh>TT.visit_partial_type</vh></v>
<v t="ekr.20220525082936.47"><vh>TT.visit_unpack_type</vh></v>
<v t="ekr.20220525082936.48"><vh>TT.visit_callable_type</vh></v>
<v t="ekr.20220525082936.49"><vh>TT.visit_tuple_type</vh></v>
<v t="ekr.20220525082936.50"><vh>TT.visit_typeddict_type</vh></v>
<v t="ekr.20220525082936.51"><vh>TT.visit_literal_type</vh></v>
<v t="ekr.20220525082936.52"><vh>TT.visit_union_type</vh></v>
<v t="ekr.20220525082936.53"><vh>TT.translate_types</vh></v>
<v t="ekr.20220525082936.54"><vh>TT.translate_variables</vh></v>
<v t="ekr.20220525082936.55"><vh>TT.visit_overloaded</vh></v>
<v t="ekr.20220525082936.56"><vh>TT.visit_type_type</vh></v>
<v t="ekr.20220525082936.57"><vh>TT.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082935.1388"><vh>class UnboundType</vh>
<v t="ekr.20220525082935.1389"><vh>UnboundType.__init__</vh></v>
<v t="ekr.20220525082935.1390"><vh>UnboundType.copy_modified</vh></v>
<v t="ekr.20220525082935.1391"><vh>UnboundType.accept</vh></v>
<v t="ekr.20220525082935.1392"><vh>UnboundType.__hash__</vh></v>
<v t="ekr.20220525082935.1393"><vh>UnboundType.__eq__</vh></v>
<v t="ekr.20220525082935.1394"><vh>UnboundType.serialize</vh></v>
<v t="ekr.20220525082935.1395"><vh>UnboundType.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.963"><vh>class Var</vh>
<v t="ekr.20220525082934.964"><vh>Var.__init__</vh></v>
<v t="ekr.20220525082934.965"><vh>Var.name</vh></v>
<v t="ekr.20220525082934.966"><vh>Var.fullname</vh></v>
<v t="ekr.20220525082934.967"><vh>Var.accept</vh></v>
<v t="ekr.20220525082934.968"><vh>Var.serialize</vh></v>
<v t="ekr.20220525082934.969"><vh>Var.deserialize</vh></v>
</v>
</v>
<v t="ekr.20220603103251.1"><vh>--- main calling sequence</vh>
<v t="ekr.20220525082934.504"><vh>main (main.py)</vh>
<v t="ekr.20220617084344.1"><vh>&lt;&lt; check options &gt;&gt;</vh></v>
<v t="ekr.20220617085103.1"><vh>&lt;&lt; shut down &gt;&gt;</vh></v>
</v>
<v t="ekr.20220525082934.505"><vh>run_build (build.py)</vh>
<v t="ekr.20220525082934.506"><vh>flush_errors</vh></v>
<v t="ekr.20220617085407.1"><vh>&lt;&lt; warn about unused configs &gt;&gt;</vh></v>
</v>
<v t="ekr.20220525082933.217"><vh>build (build.py) (parsing, SA and type checking)</vh>
<v t="ekr.20220617085508.1"></v>
<v t="ekr.20220525082933.218"><vh>function: default_flush_errors</vh></v>
</v>
<v t="ekr.20220525082933.219"><vh>_build (build.py) (instantiates BuildManager, Errors)</vh>
<v t="ekr.20220617092625.1"><vh>&lt;&lt; Instantiate Errors &gt;&gt;</vh></v>
<v t="ekr.20220617092600.1"><vh>&lt;&lt; Instantiate BuildManager &gt;&gt;</vh></v>
<v t="ekr.20220617092846.1"><vh>&lt;&lt; Handle stats and reports &gt;&gt;</vh></v>
</v>
<v t="ekr.20220525082933.321"><vh>dispatch (build.py) (calls process graph. Return graph)</vh>
<v t="ekr.20220617090951.1"><vh>&lt;&lt; Handle stats and dumps &gt;&gt;</vh></v>
<v t="ekr.20220617092314.1"><vh>&lt;&lt; update snapshot &gt;&gt;</vh></v>
<v t="ekr.20220617092355.1"><vh>&lt;&lt; dump dependencies &gt;&gt;</vh></v>
</v>
<v t="ekr.20220525082933.327"><vh>load_graph (build.py) (creates graph with many modules)</vh></v>
<v t="ekr.20220525082933.328"><vh>process_graph (build.py)</vh>
<v t="ekr.20220617091730.1"><vh>&lt;&lt; order scc &gt;&gt;</vh></v>
<v t="ekr.20220617091557.1"><vh>&lt;&lt; log the processing &gt;&gt;</vh></v>
<v t="ekr.20220617093043.1"><vh>&lt;&lt; stats &amp; log &gt;&gt;</vh></v>
</v>
<v t="ekr.20220525082933.331"><vh>process_stale_scc (build.py)</vh></v>
<v t="ekr.20220525082935.309"><vh>semantic_analysis_for_scc (main.py)</vh></v>
</v>
</v>
<v t="ekr.20221004064036.794"></v>
</vnodes>
<tnodes>
<t tx="ekr.20220320050857.1">@language rest

commands: cd-m, m-leo, m-push, m-test
https://github.com/python/mypy/issues/12352
PR: https://github.com/python/mypy/pull/12918

Questions:
- What is fine-grained incremental mode???
- What is a trait?

@language python</t>
<t tx="ekr.20220525082746.1">'''Recursively import all python files in a directory and clean the result.'''
@tabwidth -4 # For a better match.
g.cls()
dir_ = r'C:\Repos\ekr-mypy2'
# dir_ = r'C:\Users\Edward Ream\Python\python39\Lib\site-packages\pylint'
c.recursiveImport(
    add_context=True,  # Override setting only if True/False
    add_file_context=False,  # Override setting only if True/False
    dir_=dir_,
    kind = '@clean', # '@auto', '@clean', '@nosent','@file',
    add_path = True,
    recursive = True,
    safe_at_file = False,
    theTypes = ['.py'],
        # ['.ts', '.js', '.json'] # ['.html', '.js', '.json', '.py', '.rs', '.svg', '.ts', '.tsx']
    verbose = False,
)
if 1:
    last = c.lastTopLevel()
    # for p in last.self_and_subtree():
        # p.expand()
    # c.expandAllSubheads()
    c.redraw(last)
print('Done')</t>
<t tx="ekr.20220525082933.217">def build(
    sources: list[BuildSource],
    options: Options,
    alt_lib_path: str | None = None,
    flush_errors: Callable[[list[str], bool], None] | None = None,
    fscache: FileSystemCache | None = None,
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
    extra_plugins: Sequence[Plugin] | None = None,
) -&gt; BuildResult:
    &lt;&lt; docstring: build &gt;&gt;
    # If we were not given a flush_errors, we use one that will populate those
    # fields for callers that want the traditional API.
    messages = []

    @others
    flush_errors = flush_errors or default_flush_errors
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr
    extra_plugins = extra_plugins or []

    try:
        result = _build(
            sources, options, alt_lib_path, flush_errors, fscache, stdout, stderr, extra_plugins
        )
        result.errors = messages
        return result
    except CompileError as e:
        # CompileErrors raised from an errors object carry all of the
        # messages that have not been reported out by error streaming.
        # Patch it up to contain either none or all none of the messages,
        # depending on whether we are flushing errors.
        serious = not e.use_stdout
        flush_errors(e.messages, serious)
        e.messages = messages
        raise


</t>
<t tx="ekr.20220525082933.218">def default_flush_errors(new_messages: list[str], is_serious: bool) -&gt; None:
    messages.extend(new_messages)

</t>
<t tx="ekr.20220525082933.219">def _build(
    sources: list[BuildSource],
    options: Options,
    alt_lib_path: str | None,
    flush_errors: Callable[[list[str], bool], None],
    fscache: FileSystemCache | None,
    stdout: TextIO,
    stderr: TextIO,
    extra_plugins: Sequence[Plugin],
) -&gt; BuildResult:
    if platform.python_implementation() == "CPython":
        # This seems the most reasonable place to tune garbage collection.
        gc.set_threshold(150 * 1000)

    data_dir = default_data_dir()
    fscache = fscache or FileSystemCache()

    search_paths = compute_search_paths(sources, options, data_dir, alt_lib_path)

    reports = None
    if options.report_dirs:
        # Import lazily to avoid slowing down startup.
        from mypy.report import Reports

        reports = Reports(data_dir, options.report_dirs)

    source_set = BuildSourceSet(sources)
    cached_read = fscache.read
    &lt;&lt; Instantiate Errors &gt;&gt;
    plugin, snapshot = load_plugins(options, errors, stdout, extra_plugins)

    # Add catch-all .gitignore to cache dir if we created it
    cache_dir_existed = os.path.isdir(options.cache_dir)
    &lt;&lt; Instantiate BuildManager &gt;&gt;
    reset_global_state()
    try:
        graph = dispatch(sources, manager, stdout)
        if not options.fine_grained_incremental:
            TypeState.reset_all_subtype_caches()
        if options.timing_stats is not None:
            dump_timing_stats(options.timing_stats, graph)
        return BuildResult(manager, graph)
    finally:
        &lt;&lt; Handle stats and reports &gt;&gt;
</t>
<t tx="ekr.20220525082933.232">class BuildManager:
    """This class holds shared state for building a mypy program.

    It is used to coordinate parsing, import processing, semantic
    analysis and type checking.  The actual build steps are carried
    out by dispatch().

    Attributes:
      data_dir:        Mypy data directory (contains stubs)
      search_paths:    SearchPaths instance indicating where to look for modules
      modules:         Mapping of module ID to MypyFile (shared by the passes)
      semantic_analyzer:
                       Semantic analyzer, pass 2
      all_types:       Map {Expression: Type} from all modules (enabled by export_types)
      options:         Build options
      missing_modules: Set of modules that could not be imported encountered so far
      stale_modules:   Set of modules that needed to be rechecked (only used by tests)
      fg_deps_meta:    Metadata for fine-grained dependencies caches associated with modules
      fg_deps:         A fine-grained dependency map
      version_id:      The current mypy version (based on commit id when possible)
      plugin:          Active mypy plugin(s)
      plugins_snapshot:
                       Snapshot of currently active user plugins (versions and hashes)
      old_plugins_snapshot:
                       Plugins snapshot from previous incremental run (or None in
                       non-incremental mode and if cache was not found)
      errors:          Used for reporting all errors
      flush_errors:    A function for processing errors after each SCC
      cache_enabled:   Whether cache is being read. This is set based on options,
                       but is disabled if fine-grained cache loading fails
                       and after an initial fine-grained load. This doesn't
                       determine whether we write cache files or not.
      quickstart_state:
                       A cache of filename -&gt; mtime/size/hash info used to avoid
                       needing to hash source files when using a cache with mismatching mtimes
      stats:           Dict with various instrumentation numbers, it is used
                       not only for debugging, but also required for correctness,
                       in particular to check consistency of the fine-grained dependency cache.
      fscache:         A file system cacher
      ast_cache:       AST cache to speed up mypy daemon
    """

    @others
</t>
<t tx="ekr.20220525082933.233">def __init__(
    self,
    data_dir: str,
    search_paths: SearchPaths,
    ignore_prefix: str,
    source_set: BuildSourceSet,
    reports: Reports | None,
    options: Options,
    version_id: str,
    plugin: Plugin,
    plugins_snapshot: dict[str, str],
    errors: Errors,
    flush_errors: Callable[[list[str], bool], None],
    fscache: FileSystemCache,
    stdout: TextIO,
    stderr: TextIO,
) -&gt; None:
    self.stats: dict[str, Any] = {}  # Values are ints or floats
    self.stdout = stdout
    self.stderr = stderr
    self.start_time = time.time()
    self.data_dir = data_dir
    self.errors = errors
    self.errors.set_ignore_prefix(ignore_prefix)
    self.search_paths = search_paths
    self.source_set = source_set
    self.reports = reports
    self.options = options
    self.version_id = version_id
    self.modules: dict[str, MypyFile] = {}
    self.missing_modules: set[str] = set()
    self.fg_deps_meta: dict[str, FgDepMeta] = {}
    # fg_deps holds the dependencies of every module that has been
    # processed. We store this in BuildManager so that we can compute
    # dependencies as we go, which allows us to free ASTs and type information,
    # saving a ton of memory on net.
    self.fg_deps: dict[str, set[str]] = {}
    # Always convert the plugin to a ChainedPlugin so that it can be manipulated if needed
    if not isinstance(plugin, ChainedPlugin):
        plugin = ChainedPlugin(options, [plugin])
    self.plugin = plugin
    # Set of namespaces (module or class) that are being populated during semantic
    # analysis and may have missing definitions.
    self.incomplete_namespaces: set[str] = set()
    self.semantic_analyzer = SemanticAnalyzer(
        self.modules,
        self.missing_modules,
        self.incomplete_namespaces,
        self.errors,
        self.plugin,
    )
    self.all_types: dict[Expression, Type] = {}  # Enabled by export_types
    self.indirection_detector = TypeIndirectionVisitor()
    self.stale_modules: set[str] = set()
    self.rechecked_modules: set[str] = set()
    self.flush_errors = flush_errors
    has_reporters = reports is not None and reports.reporters
    self.cache_enabled = (
        options.incremental
        and (not options.fine_grained_incremental or options.use_fine_grained_cache)
        and not has_reporters
    )
    self.fscache = fscache
    self.find_module_cache = FindModuleCache(
        self.search_paths, self.fscache, self.options, source_set=self.source_set
    )
    for module in CORE_BUILTIN_MODULES:
        if options.use_builtins_fixtures:
            continue
        if module == "_importlib_modulespec":
            continue
        path = self.find_module_cache.find_module(module)
        if not isinstance(path, str):
            raise CompileError(
                [f"Failed to find builtin module {module}, perhaps typeshed is broken?"]
            )
        if is_typeshed_file(options.abs_custom_typeshed_dir, path) or is_stub_package_file(
            path
        ):
            continue

        raise CompileError(
            [
                f'mypy: "{os.path.relpath(path)}" shadows library module "{module}"',
                f'note: A user-defined top-level module with name "{module}" is not supported',
            ]
        )

    self.metastore = create_metastore(options)

    # a mapping from source files to their corresponding shadow files
    # for efficient lookup
    self.shadow_map: dict[str, str] = {}
    if self.options.shadow_file is not None:
        self.shadow_map = {
            source_file: shadow_file for (source_file, shadow_file) in self.options.shadow_file
        }
    # a mapping from each file being typechecked to its possible shadow file
    self.shadow_equivalence_map: dict[str, str | None] = {}
    self.plugin = plugin
    self.plugins_snapshot = plugins_snapshot
    self.old_plugins_snapshot = read_plugins_snapshot(self)
    self.quickstart_state = read_quickstart_file(options, self.stdout)
    # Fine grained targets (module top levels and top level functions) processed by
    # the semantic analyzer, used only for testing. Currently used only by the new
    # semantic analyzer.
    self.processed_targets: list[str] = []
    # Missing stub packages encountered.
    self.missing_stub_packages: set[str] = set()
    # Cache for mypy ASTs that have completed semantic analysis
    # pass 1. When multiple files are added to the build in a
    # single daemon increment, only one of the files gets added
    # per step and the others are discarded. This gets repeated
    # until all the files have been added. This means that a
    # new file can be processed O(n**2) times. This cache
    # avoids most of this redundant work.
    self.ast_cache: dict[str, tuple[MypyFile, list[ErrorInfo]]] = {}

</t>
<t tx="ekr.20220525082933.234">def dump_stats(self) -&gt; None:
    if self.options.dump_build_stats:
        print("Stats:")
        for key, value in sorted(self.stats_summary().items()):
            print(f"{key + ':':24}{value}")

</t>
<t tx="ekr.20220525082933.235">def use_fine_grained_cache(self) -&gt; bool:
    return self.cache_enabled and self.options.use_fine_grained_cache

</t>
<t tx="ekr.20220525082933.236">def maybe_swap_for_shadow_path(self, path: str) -&gt; str:
    if not self.shadow_map:
        return path

    path = normpath(path, self.options)

    previously_checked = path in self.shadow_equivalence_map
    if not previously_checked:
        for source, shadow in self.shadow_map.items():
            if self.fscache.samefile(path, source):
                self.shadow_equivalence_map[path] = shadow
                break
            else:
                self.shadow_equivalence_map[path] = None

    shadow_file = self.shadow_equivalence_map.get(path)
    return shadow_file if shadow_file else path

</t>
<t tx="ekr.20220525082933.237">def get_stat(self, path: str) -&gt; os.stat_result:
    return self.fscache.stat(self.maybe_swap_for_shadow_path(path))

</t>
<t tx="ekr.20220525082933.238">def getmtime(self, path: str) -&gt; int:
    """Return a file's mtime; but 0 in bazel mode.

    (Bazel's distributed cache doesn't like filesystem metadata to
    end up in output files.)
    """
    if self.options.bazel:
        return 0
    else:
        return int(self.metastore.getmtime(path))

</t>
<t tx="ekr.20220525082933.239">def all_imported_modules_in_file(self, file: MypyFile) -&gt; list[tuple[int, str, int]]:
    """Find all reachable import statements in a file.

    Return list of tuples (priority, module id, import line number)
    for all modules imported in file; lower numbers == higher priority.

    Can generate blocking errors on bogus relative imports.
    """

    def correct_rel_imp(imp: ImportFrom | ImportAll) -&gt; str:
        """Function to correct for relative imports."""
        file_id = file.fullname
        rel = imp.relative
        if rel == 0:
            return imp.id
        if os.path.basename(file.path).startswith("__init__."):
            rel -= 1
        if rel != 0:
            file_id = ".".join(file_id.split(".")[:-rel])
        new_id = file_id + "." + imp.id if imp.id else file_id

        if not new_id:
            self.errors.set_file(file.path, file.name, self.options)
            self.errors.report(
                imp.line, 0, "No parent module -- cannot perform relative import", blocker=True
            )

        return new_id

    res: list[tuple[int, str, int]] = []
    for imp in file.imports:
        if not imp.is_unreachable:
            if isinstance(imp, Import):
                pri = import_priority(imp, PRI_MED)
                ancestor_pri = import_priority(imp, PRI_LOW)
                for id, _ in imp.ids:
                    res.append((pri, id, imp.line))
                    ancestor_parts = id.split(".")[:-1]
                    ancestors = []
                    for part in ancestor_parts:
                        ancestors.append(part)
                        res.append((ancestor_pri, ".".join(ancestors), imp.line))
            elif isinstance(imp, ImportFrom):
                cur_id = correct_rel_imp(imp)
                all_are_submodules = True
                # Also add any imported names that are submodules.
                pri = import_priority(imp, PRI_MED)
                for name, __ in imp.names:
                    sub_id = cur_id + "." + name
                    if self.is_module(sub_id):
                        res.append((pri, sub_id, imp.line))
                    else:
                        all_are_submodules = False
                # Add cur_id as a dependency, even if all of the
                # imports are submodules. Processing import from will try
                # to look through cur_id, so we should depend on it.
                # As a workaround for for some bugs in cycle handling (#4498),
                # if all of the imports are submodules, do the import at a lower
                # priority.
                pri = import_priority(imp, PRI_HIGH if not all_are_submodules else PRI_LOW)
                res.append((pri, cur_id, imp.line))
            elif isinstance(imp, ImportAll):
                pri = import_priority(imp, PRI_HIGH)
                res.append((pri, correct_rel_imp(imp), imp.line))

    # Sort such that module (e.g. foo.bar.baz) comes before its ancestors (e.g. foo
    # and foo.bar) so that, if FindModuleCache finds the target module in a
    # package marked with py.typed underneath a namespace package installed in
    # site-packages, (gasp), that cache's knowledge of the ancestors
    # (aka FindModuleCache.ns_ancestors) can be primed when it is asked to find
    # the parent.
    res.sort(key=lambda x: -x[1].count("."))
    return res

</t>
<t tx="ekr.20220525082933.240">def is_module(self, id: str) -&gt; bool:
    """Is there a file in the file system corresponding to module id?"""
    return find_module_simple(id, self) is not None

</t>
<t tx="ekr.20220525082933.241">def parse_file(
    self, id: str, path: str, source: str, ignore_errors: bool, options: Options
) -&gt; MypyFile:
    """Parse the source of a file with the given name.

    Raise CompileError if there is a parse error.
    """
    t0 = time.time()
    tree = parse(source, path, id, self.errors, options=options)
    tree._fullname = id
    self.add_stats(
        files_parsed=1,
        modules_parsed=int(not tree.is_stub),
        stubs_parsed=int(tree.is_stub),
        parse_time=time.time() - t0,
    )

    if self.errors.is_blockers():
        self.log("Bailing due to parse errors")
        self.errors.raise_error()

    self.errors.set_file_ignored_lines(path, tree.ignored_lines, ignore_errors)
    return tree

</t>
<t tx="ekr.20220525082933.242">def load_fine_grained_deps(self, id: str) -&gt; dict[str, set[str]]:
    t0 = time.time()
    if id in self.fg_deps_meta:
        # TODO: Assert deps file wasn't changed.
        deps = json.loads(self.metastore.read(self.fg_deps_meta[id]["path"]))
    else:
        deps = {}
    val = {k: set(v) for k, v in deps.items()}
    self.add_stats(load_fg_deps_time=time.time() - t0)
    return val

</t>
<t tx="ekr.20220525082933.243">def report_file(
    self, file: MypyFile, type_map: dict[Expression, Type], options: Options
) -&gt; None:
    if self.reports is not None and self.source_set.is_source(file):
        self.reports.file(file, self.modules, type_map, options)

</t>
<t tx="ekr.20220525082933.244">def verbosity(self) -&gt; int:
    return self.options.verbosity

</t>
<t tx="ekr.20220525082933.245">def log(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 1:
        if message:
            print("LOG: ", *message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20220525082933.246">def log_fine_grained(self, *message: str) -&gt; None:
    import mypy.build

    if self.verbosity() &gt;= 1:
        self.log("fine-grained:", *message)
    elif mypy.build.DEBUG_FINE_GRAINED:
        # Output log in a simplified format that is quick to browse.
        if message:
            print(*message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20220525082933.247">def trace(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 2:
        print("TRACE:", *message, file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20220525082933.248">def add_stats(self, **kwds: Any) -&gt; None:
    for key, value in kwds.items():
        if key in self.stats:
            self.stats[key] += value
        else:
            self.stats[key] = value

</t>
<t tx="ekr.20220525082933.249">def stats_summary(self) -&gt; Mapping[str, object]:
    return self.stats


</t>
<t tx="ekr.20220525082933.274">class State:
    """The state for a module.

    The source is only used for the -c command line option; in that
    case path is None.  Otherwise source is None and path isn't.
    """

    manager: BuildManager
    order_counter: ClassVar[int] = 0
    order: int  # Order in which modules were encountered
    id: str  # Fully qualified module name
    path: str | None = None  # Path to module source
    abspath: str | None = None  # Absolute path to module source
    xpath: str  # Path or '&lt;string&gt;'
    source: str | None = None  # Module source code
    source_hash: str | None = None  # Hash calculated based on the source code
    meta_source_hash: str | None = None  # Hash of the source given in the meta, if any
    meta: CacheMeta | None = None
    data: str | None = None
    tree: MypyFile | None = None
    # We keep both a list and set of dependencies. A set because it makes it efficient to
    # prevent duplicates and the list because I am afraid of changing the order of
    # iteration over dependencies.
    # They should be managed with add_dependency and suppress_dependency.
    dependencies: list[str]  # Modules directly imported by the module
    dependencies_set: set[str]  # The same but as a set for deduplication purposes
    suppressed: list[str]  # Suppressed/missing dependencies
    suppressed_set: set[str]  # Suppressed/missing dependencies
    priorities: dict[str, int]

    # Map each dependency to the line number where it is first imported
    dep_line_map: dict[str, int]

    # Parent package, its parent, etc.
    ancestors: list[str] | None = None

    # List of (path, line number) tuples giving context for import
    import_context: list[tuple[str, int]]

    # The State from which this module was imported, if any
    caller_state: State | None = None

    # If caller_state is set, the line number in the caller where the import occurred
    caller_line = 0

    # If True, indicate that the public interface of this module is unchanged
    externally_same = True

    # Contains a hash of the public interface in incremental mode
    interface_hash: str = ""

    # Options, specialized for this file
    options: Options

    # Whether to ignore all errors
    ignore_all = False

    # Whether the module has an error or any of its dependencies have one.
    transitive_error = False

    # Errors reported before semantic analysis, to allow fine-grained
    # mode to keep reporting them.
    early_errors: list[ErrorInfo]

    # Type checker used for checking this file.  Use type_checker() for
    # access and to construct this on demand.
    _type_checker: TypeChecker | None = None

    fine_grained_deps_loaded = False

    # Cumulative time spent on this file, in microseconds (for profiling stats)
    time_spent_us: int = 0

    @others
</t>
<t tx="ekr.20220525082933.275">def __init__(
    self,
    id: str | None,
    path: str | None,
    source: str | None,
    manager: BuildManager,
    caller_state: State | None = None,
    caller_line: int = 0,
    ancestor_for: State | None = None,
    root_source: bool = False,
    # If `temporary` is True, this State is being created to just
    # quickly parse/load the tree, without an intention to further
    # process it. With this flag, any changes to external state as well
    # as error reporting should be avoided.
    temporary: bool = False,
) -&gt; None:
    if not temporary:
        assert id or path or source is not None, "Neither id, path nor source given"
    self.manager = manager
    State.order_counter += 1
    self.order = State.order_counter
    self.caller_state = caller_state
    self.caller_line = caller_line
    if caller_state:
        self.import_context = caller_state.import_context[:]
        self.import_context.append((caller_state.xpath, caller_line))
    else:
        self.import_context = []
    self.id = id or "__main__"
    self.options = manager.options.clone_for_module(self.id)
    self.early_errors = []
    self._type_checker = None
    if not path and source is None:
        assert id is not None
        try:
            path, follow_imports = find_module_and_diagnose(
                manager,
                id,
                self.options,
                caller_state,
                caller_line,
                ancestor_for,
                root_source,
                skip_diagnose=temporary,
            )
        except ModuleNotFound:
            if not temporary:
                manager.missing_modules.add(id)
            raise
        if follow_imports == "silent":
            self.ignore_all = True
    elif path and is_silent_import_module(manager, path):
        self.ignore_all = True
    self.path = path
    if path:
        self.abspath = os.path.abspath(path)
    self.xpath = path or "&lt;string&gt;"
    if path and source is None and self.manager.cache_enabled:
        self.meta = find_cache_meta(self.id, path, manager)
        # TODO: Get mtime if not cached.
        if self.meta is not None:
            self.interface_hash = self.meta.interface_hash
            self.meta_source_hash = self.meta.hash
    if path and source is None and self.manager.fscache.isdir(path):
        source = ""
    self.source = source
    self.add_ancestors()
    t0 = time.time()
    self.meta = validate_meta(self.meta, self.id, self.path, self.ignore_all, manager)
    self.manager.add_stats(validate_meta_time=time.time() - t0)
    if self.meta:
        # Make copies, since we may modify these and want to
        # compare them to the originals later.
        self.dependencies = list(self.meta.dependencies)
        self.dependencies_set = set(self.dependencies)
        self.suppressed = list(self.meta.suppressed)
        self.suppressed_set = set(self.suppressed)
        all_deps = self.dependencies + self.suppressed
        assert len(all_deps) == len(self.meta.dep_prios)
        self.priorities = {id: pri for id, pri in zip(all_deps, self.meta.dep_prios)}
        assert len(all_deps) == len(self.meta.dep_lines)
        self.dep_line_map = {id: line for id, line in zip(all_deps, self.meta.dep_lines)}
        if temporary:
            self.load_tree(temporary=True)
        if not manager.use_fine_grained_cache():
            # Special case: if there were a previously missing package imported here
            # and it is not present, then we need to re-calculate dependencies.
            # This is to support patterns like this:
            #     from missing_package import missing_module  # type: ignore
            # At first mypy doesn't know that `missing_module` is a module
            # (it may be a variable, a class, or a function), so it is not added to
            # suppressed dependencies. Therefore, when the package with module is added,
            # we need to re-calculate dependencies.
            # NOTE: see comment below for why we skip this in fine grained mode.
            if exist_added_packages(self.suppressed, manager, self.options):
                self.parse_file()  # This is safe because the cache is anyway stale.
                self.compute_dependencies()
    else:
        # When doing a fine-grained cache load, pretend we only
        # know about modules that have cache information and defer
        # handling new modules until the fine-grained update.
        if manager.use_fine_grained_cache():
            manager.log(f"Deferring module to fine-grained update {path} ({id})")
            raise ModuleNotFound

        # Parse the file (and then some) to get the dependencies.
        self.parse_file()
        self.compute_dependencies()

</t>
<t tx="ekr.20220525082933.276">@property
def xmeta(self) -&gt; CacheMeta:
    assert self.meta, "missing meta on allegedly fresh module"
    return self.meta

</t>
<t tx="ekr.20220525082933.277">def add_ancestors(self) -&gt; None:
    if self.path is not None:
        _, name = os.path.split(self.path)
        base, _ = os.path.splitext(name)
        if "." in base:
            # This is just a weird filename, don't add anything
            self.ancestors = []
            return
    # All parent packages are new ancestors.
    ancestors = []
    parent = self.id
    while "." in parent:
        parent, _ = parent.rsplit(".", 1)
        ancestors.append(parent)
    self.ancestors = ancestors

</t>
<t tx="ekr.20220525082933.278">def is_fresh(self) -&gt; bool:
    """Return whether the cache data for this file is fresh."""
    # NOTE: self.dependencies may differ from
    # self.meta.dependencies when a dependency is dropped due to
    # suppression by silent mode.  However when a suppressed
    # dependency is added back we find out later in the process.
    return (
        self.meta is not None
        and self.is_interface_fresh()
        and self.dependencies == self.meta.dependencies
    )

</t>
<t tx="ekr.20220525082933.279">def is_interface_fresh(self) -&gt; bool:
    return self.externally_same

</t>
<t tx="ekr.20220525082933.280">def mark_as_rechecked(self) -&gt; None:
    """Marks this module as having been fully re-analyzed by the type-checker."""
    self.manager.rechecked_modules.add(self.id)

</t>
<t tx="ekr.20220525082933.281">def mark_interface_stale(self, *, on_errors: bool = False) -&gt; None:
    """Marks this module as having a stale public interface, and discards the cache data."""
    self.externally_same = False
    if not on_errors:
        self.manager.stale_modules.add(self.id)

</t>
<t tx="ekr.20220525082933.282">def check_blockers(self) -&gt; None:
    """Raise CompileError if a blocking error is detected."""
    if self.manager.errors.is_blockers():
        self.manager.log("Bailing due to blocking errors")
        self.manager.errors.raise_error()

</t>
<t tx="ekr.20220525082933.283">@contextlib.contextmanager
def wrap_context(self, check_blockers: bool = True) -&gt; Iterator[None]:
    """Temporarily change the error import context to match this state.

    Also report an internal error if an unexpected exception was raised
    and raise an exception on a blocking error, unless
    check_blockers is False. Skipping blocking error reporting is used
    in the semantic analyzer so that we can report all blocking errors
    for a file (across multiple targets) to maintain backward
    compatibility.
    """
    save_import_context = self.manager.errors.import_context()
    self.manager.errors.set_import_context(self.import_context)
    try:
        yield
    except CompileError:
        raise
    except Exception as err:
        report_internal_error(
            err,
            self.path,
            0,
            self.manager.errors,
            self.options,
            self.manager.stdout,
            self.manager.stderr,
        )
    self.manager.errors.set_import_context(save_import_context)
    # TODO: Move this away once we've removed the old semantic analyzer?
    if check_blockers:
        self.check_blockers()

</t>
<t tx="ekr.20220525082933.284">def load_fine_grained_deps(self) -&gt; dict[str, set[str]]:
    return self.manager.load_fine_grained_deps(self.id)

</t>
<t tx="ekr.20220525082933.285">def load_tree(self, temporary: bool = False) -&gt; None:
    assert (
        self.meta is not None
    ), "Internal error: this method must be called only for cached modules"

    data = _load_json_file(
        self.meta.data_json, self.manager, "Load tree ", "Could not load tree: "
    )
    if data is None:
        return None

    t0 = time.time()
    # TODO: Assert data file wasn't changed.
    self.tree = MypyFile.deserialize(data)
    t1 = time.time()
    self.manager.add_stats(deserialize_time=t1 - t0)
    if not temporary:
        self.manager.modules[self.id] = self.tree
        self.manager.add_stats(fresh_trees=1)

</t>
<t tx="ekr.20220525082933.286">def fix_cross_refs(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We need to set allow_missing when doing a fine grained cache
    # load because we need to gracefully handle missing modules.
    fixup_module(self.tree, self.manager.modules, self.options.use_fine_grained_cache)

</t>
<t tx="ekr.20220525082933.287"># Methods for processing modules from source code.

</t>
<t tx="ekr.20220525082933.288">def parse_file(self) -&gt; None:
    """Parse file and run first pass of semantic analysis.

    Everything done here is local to the file. Don't depend on imported
    modules in any way. Also record module dependencies based on imports.
    """
    if self.tree is not None:
        # The file was already parsed (in __init__()).
        return

    manager = self.manager

    # Can we reuse a previously parsed AST? This avoids redundant work in daemon.
    cached = self.id in manager.ast_cache
    modules = manager.modules
    if not cached:
        manager.log(f"Parsing {self.xpath} ({self.id})")
    else:
        manager.log(f"Using cached AST for {self.xpath} ({self.id})")

    t0 = time_ref()

    with self.wrap_context():
        source = self.source
        self.source = None  # We won't need it again.
        if self.path and source is None:
            try:
                path = manager.maybe_swap_for_shadow_path(self.path)
                source = decode_python_encoding(manager.fscache.read(path))
                self.source_hash = manager.fscache.hash_digest(path)
            except OSError as ioerr:
                # ioerr.strerror differs for os.stat failures between Windows and
                # other systems, but os.strerror(ioerr.errno) does not, so we use that.
                # (We want the error messages to be platform-independent so that the
                # tests have predictable output.)
                raise CompileError(
                    [
                        "mypy: can't read file '{}': {}".format(
                            self.path, os.strerror(ioerr.errno)
                        )
                    ],
                    module_with_blocker=self.id,
                ) from ioerr
            except (UnicodeDecodeError, DecodeError) as decodeerr:
                if self.path.endswith(".pyd"):
                    err = f"mypy: stubgen does not support .pyd files: '{self.path}'"
                else:
                    err = f"mypy: can't decode file '{self.path}': {str(decodeerr)}"
                raise CompileError([err], module_with_blocker=self.id) from decodeerr
        elif self.path and self.manager.fscache.isdir(self.path):
            source = ""
            self.source_hash = ""
        else:
            assert source is not None
            self.source_hash = compute_hash(source)

        self.parse_inline_configuration(source)
        if not cached:
            self.tree = manager.parse_file(
                self.id,
                self.xpath,
                source,
                self.ignore_all or self.options.ignore_errors,
                self.options,
            )

        else:
            # Reuse a cached AST
            self.tree = manager.ast_cache[self.id][0]
            manager.errors.set_file_ignored_lines(
                self.xpath,
                self.tree.ignored_lines,
                self.ignore_all or self.options.ignore_errors,
            )

    self.time_spent_us += time_spent_us(t0)

    if not cached:
        # Make a copy of any errors produced during parse time so that
        # fine-grained mode can repeat them when the module is
        # reprocessed.
        self.early_errors = list(manager.errors.error_info_map.get(self.xpath, []))
    else:
        self.early_errors = manager.ast_cache[self.id][1]

    modules[self.id] = self.tree

    if not cached:
        self.semantic_analysis_pass1()

    self.check_blockers()

    manager.ast_cache[self.id] = (self.tree, self.early_errors)

</t>
<t tx="ekr.20220525082933.289">def parse_inline_configuration(self, source: str) -&gt; None:
    """Check for inline mypy: options directive and parse them."""
    flags = get_mypy_comments(source)
    if flags:
        changes, config_errors = parse_mypy_comments(flags, self.options)
        self.options = self.options.apply_changes(changes)
        self.manager.errors.set_file(self.xpath, self.id, self.options)
        for lineno, error in config_errors:
            self.manager.errors.report(lineno, 0, error)

</t>
<t tx="ekr.20220525082933.290">def semantic_analysis_pass1(self) -&gt; None:
    """Perform pass 1 of semantic analysis, which happens immediately after parsing.

    This pass can't assume that any other modules have been processed yet.
    """
    options = self.options
    assert self.tree is not None

    t0 = time_ref()

    # Do the first pass of semantic analysis: analyze the reachability
    # of blocks and import statements. We must do this before
    # processing imports, since this may mark some import statements as
    # unreachable.
    #
    # TODO: This should not be considered as a semantic analysis
    #     pass -- it's an independent pass.
    analyzer = SemanticAnalyzerPreAnalysis()
    with self.wrap_context():
        analyzer.visit_file(self.tree, self.xpath, self.id, options)
    # TODO: Do this while constructing the AST?
    self.tree.names = SymbolTable()
    if not self.tree.is_stub:
        # Always perform some low-key variable renaming
        self.tree.accept(LimitedVariableRenameVisitor())
        if options.allow_redefinition:
            # Perform more renaming across the AST to allow variable redefinitions
            self.tree.accept(VariableRenameVisitor())
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20220525082933.291">def add_dependency(self, dep: str) -&gt; None:
    if dep not in self.dependencies_set:
        self.dependencies.append(dep)
        self.dependencies_set.add(dep)
    if dep in self.suppressed_set:
        self.suppressed.remove(dep)
        self.suppressed_set.remove(dep)

</t>
<t tx="ekr.20220525082933.292">def suppress_dependency(self, dep: str) -&gt; None:
    if dep in self.dependencies_set:
        self.dependencies.remove(dep)
        self.dependencies_set.remove(dep)
    if dep not in self.suppressed_set:
        self.suppressed.append(dep)
        self.suppressed_set.add(dep)

</t>
<t tx="ekr.20220525082933.293">def compute_dependencies(self) -&gt; None:
    """Compute a module's dependencies after parsing it.

    This is used when we parse a file that we didn't have
    up-to-date cache information for. When we have an up-to-date
    cache, we just use the cached info.
    """
    manager = self.manager
    assert self.tree is not None

    # Compute (direct) dependencies.
    # Add all direct imports (this is why we needed the first pass).
    # Also keep track of each dependency's source line.
    # Missing dependencies will be moved from dependencies to
    # suppressed when they fail to be loaded in load_graph.

    self.dependencies = []
    self.dependencies_set = set()
    self.suppressed = []
    self.suppressed_set = set()
    self.priorities = {}  # id -&gt; priority
    self.dep_line_map = {}  # id -&gt; line
    dep_entries = manager.all_imported_modules_in_file(
        self.tree
    ) + self.manager.plugin.get_additional_deps(self.tree)
    for pri, id, line in dep_entries:
        self.priorities[id] = min(pri, self.priorities.get(id, PRI_ALL))
        if id == self.id:
            continue
        self.add_dependency(id)
        if id not in self.dep_line_map:
            self.dep_line_map[id] = line
    # Every module implicitly depends on builtins.
    if self.id != "builtins":
        self.add_dependency("builtins")

    self.check_blockers()  # Can fail due to bogus relative imports

</t>
<t tx="ekr.20220525082933.294">def type_check_first_pass(self) -&gt; None:
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        self.type_checker().check_first_pass()
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20220525082933.295">def type_checker(self) -&gt; TypeChecker:
    if not self._type_checker:
        assert self.tree is not None, "Internal error: must be called on parsed file only"
        manager = self.manager
        self._type_checker = TypeChecker(
            manager.errors,
            manager.modules,
            self.options,
            self.tree,
            self.xpath,
            manager.plugin,
        )
    return self._type_checker

</t>
<t tx="ekr.20220525082933.296">def type_map(self) -&gt; dict[Expression, Type]:
    # We can extract the master type map directly since at this
    # point no temporary type maps can be active.
    assert len(self.type_checker()._type_maps) == 1
    return self.type_checker()._type_maps[0]

</t>
<t tx="ekr.20220525082933.297">def type_check_second_pass(self) -&gt; bool:
    if self.options.semantic_analysis_only:
        return False
    t0 = time_ref()
    with self.wrap_context():
        result = self.type_checker().check_second_pass()
    self.time_spent_us += time_spent_us(t0)
    return result

def detect_partially_defined_vars(self, type_map: dict[Expression, Type]) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    manager = self.manager
    if manager.errors.is_error_code_enabled(codes.PARTIALLY_DEFINED):
        manager.errors.set_file(self.xpath, self.tree.fullname, options=manager.options)
        self.tree.accept(
            PartiallyDefinedVariableVisitor(
                MessageBuilder(manager.errors, manager.modules), type_map
            )
        )

</t>
<t tx="ekr.20220525082933.298">def finish_passes(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    manager = self.manager
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        # Some tests (and tools) want to look at the set of all types.
        options = manager.options
        if options.export_types:
            manager.all_types.update(self.type_map())

        # We should always patch indirect dependencies, even in full (non-incremental) builds,
        # because the cache still may be written, and it must be correct.
        self._patch_indirect_dependencies(self.type_checker().module_refs, self.type_map())

        if self.options.dump_inference_stats:
            dump_type_stats(
                self.tree,
                self.xpath,
                modules=self.manager.modules,
                inferred=True,
                typemap=self.type_map(),
            )
        manager.report_file(self.tree, self.type_map(), self.options)

        self.update_fine_grained_deps(self.manager.fg_deps)
        self.free_state()
        if not manager.options.fine_grained_incremental and not manager.options.preserve_asts:
            free_tree(self.tree)
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20220525082933.299">def free_state(self) -&gt; None:
    if self._type_checker:
        self._type_checker.reset()
        self._type_checker = None

</t>
<t tx="ekr.20220525082933.300">def _patch_indirect_dependencies(
    self, module_refs: set[str], type_map: dict[Expression, Type]
) -&gt; None:
    types = set(type_map.values())
    assert None not in types
    valid = self.valid_references()

    encountered = self.manager.indirection_detector.find_modules(types) | module_refs
    extra = encountered - valid

    for dep in sorted(extra):
        if dep not in self.manager.modules:
            continue
        if dep not in self.suppressed_set and dep not in self.manager.missing_modules:
            self.add_dependency(dep)
            self.priorities[dep] = PRI_INDIRECT
        elif dep not in self.suppressed_set and dep in self.manager.missing_modules:
            self.suppress_dependency(dep)

</t>
<t tx="ekr.20220525082933.301">def compute_fine_grained_deps(self) -&gt; dict[str, set[str]]:
    assert self.tree is not None
    if self.id in ("builtins", "typing", "types", "sys", "_typeshed"):
        # We don't track changes to core parts of typeshed -- the
        # assumption is that they are only changed as part of mypy
        # updates, which will invalidate everything anyway. These
        # will always be processed in the initial non-fine-grained
        # build. Other modules may be brought in as a result of an
        # fine-grained increment, and we may need these
        # dependencies then to handle cyclic imports.
        return {}
    from mypy.server.deps import get_dependencies  # Lazy import to speed up startup

    return get_dependencies(
        target=self.tree,
        type_map=self.type_map(),
        python_version=self.options.python_version,
</t>
<t tx="ekr.20220525082933.302">        options=self.manager.options,
    )

def update_fine_grained_deps(self, deps: dict[str, set[str]]) -&gt; None:
    options = self.manager.options
    if options.cache_fine_grained or options.fine_grained_incremental:
        from mypy.server.deps import merge_dependencies  # Lazy import to speed up startup

        merge_dependencies(self.compute_fine_grained_deps(), deps)
        TypeState.update_protocol_deps(deps)

</t>
<t tx="ekr.20220525082933.303">def valid_references(self) -&gt; set[str]:
    assert self.ancestors is not None
    valid_refs = set(self.dependencies + self.suppressed + self.ancestors)
    valid_refs.add(self.id)

    if "os" in valid_refs:
        valid_refs.add("os.path")

    return valid_refs

</t>
<t tx="ekr.20220525082933.304">def write_cache(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We don't support writing cache files in fine-grained incremental mode.
    if (
        not self.path
        or self.options.cache_dir == os.devnull
        or self.options.fine_grained_incremental
    ):
        return
    is_errors = self.transitive_error
    if is_errors:
        delete_cache(self.id, self.path, self.manager)
        self.meta = None
        self.mark_interface_stale(on_errors=True)
        return
    dep_prios = self.dependency_priorities()
    dep_lines = self.dependency_lines()
    assert self.source_hash is not None
    assert len(set(self.dependencies)) == len(
        self.dependencies
    ), f"Duplicates in dependencies list for {self.id} ({self.dependencies})"
    new_interface_hash, self.meta = write_cache(
        self.id,
        self.path,
        self.tree,
        list(self.dependencies),
        list(self.suppressed),
        dep_prios,
        dep_lines,
        self.interface_hash,
        self.source_hash,
        self.ignore_all,
        self.manager,
    )
    if new_interface_hash == self.interface_hash:
        self.manager.log(f"Cached module {self.id} has same interface")
    else:
        self.manager.log(f"Cached module {self.id} has changed interface")
        self.mark_interface_stale()
        self.interface_hash = new_interface_hash

</t>
<t tx="ekr.20220525082933.305">def verify_dependencies(self, suppressed_only: bool = False) -&gt; None:
    """Report errors for import targets in modules that don't exist.

    If suppressed_only is set, only check suppressed dependencies.
    """
    manager = self.manager
    assert self.ancestors is not None
    if suppressed_only:
        all_deps = self.suppressed
    else:
        # Strip out indirect dependencies. See comment in build.load_graph().
        dependencies = [
            dep for dep in self.dependencies if self.priorities.get(dep) != PRI_INDIRECT
        ]
        all_deps = dependencies + self.suppressed + self.ancestors
    for dep in all_deps:
        if dep in manager.modules:
            continue
        options = manager.options.clone_for_module(dep)
        if options.ignore_missing_imports:
            continue
        line = self.dep_line_map.get(dep, 1)
        try:
            if dep in self.ancestors:
                state: State | None = None
                ancestor: State | None = self
            else:
                state, ancestor = self, None
            # Called just for its side effects of producing diagnostics.
            find_module_and_diagnose(
                manager,
                dep,
                options,
                caller_state=state,
                caller_line=line,
                ancestor_for=ancestor,
            )
        except (ModuleNotFound, CompileError):
            # Swallow up any ModuleNotFounds or CompilerErrors while generating
            # a diagnostic. CompileErrors may get generated in
            # fine-grained mode when an __init__.py is deleted, if a module
            # that was in that package has targets reprocessed before
            # it is renamed.
            pass

</t>
<t tx="ekr.20220525082933.306">def dependency_priorities(self) -&gt; list[int]:
    return [self.priorities.get(dep, PRI_HIGH) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20220525082933.307">def dependency_lines(self) -&gt; list[int]:
    return [self.dep_line_map.get(dep, 1) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20220525082933.308">def generate_unused_ignore_notes(self) -&gt; None:
    if self.options.warn_unused_ignores:
        # If this file was initially loaded from the cache, it may have suppressed
        # dependencies due to imports with ignores on them. We need to generate
        # those errors to avoid spuriously flagging them as unused ignores.
        if self.meta:
            self.verify_dependencies(suppressed_only=True)
        self.manager.errors.generate_unused_ignore_errors(self.xpath)

</t>
<t tx="ekr.20220525082933.309">def generate_ignore_without_code_notes(self) -&gt; None:
    if self.manager.errors.is_error_code_enabled(codes.IGNORE_WITHOUT_CODE):
        self.manager.errors.generate_ignore_without_code_errors(
            self.xpath, self.options.warn_unused_ignores
        )


</t>
<t tx="ekr.20220525082933.321">def dispatch(sources: list[BuildSource], manager: BuildManager, stdout: TextIO) -&gt; Graph:
    log_configuration(manager, sources)

    t0 = time.time()
    graph = load_graph(sources, manager)

    &lt;&lt; Handle stats and dumps &gt;&gt;
    # If we are loading a fine-grained incremental mode cache, we
    # don't want to do a real incremental reprocess of the
    # graph---we'll handle it all later.
    if not manager.use_fine_grained_cache():
        process_graph(graph, manager)
        &lt;&lt; update snapshot &gt;&gt;
    &lt;&lt; dump dependencies &gt;&gt;
    return graph


</t>
<t tx="ekr.20220525082933.327">def load_graph(
    sources: list[BuildSource],
    manager: BuildManager,
    old_graph: Graph | None = None,
    new_modules: list[State] | None = None,
) -&gt; Graph:
    """Given some source files, load the full dependency graph.

    If an old_graph is passed in, it is used as the starting point and
    modified during graph loading.

    If a new_modules is passed in, any modules that are loaded are
    added to the list. This is an argument and not a return value
    so that the caller can access it even if load_graph fails.

    As this may need to parse files, this can raise CompileError in case
    there are syntax errors.
    """

    graph: Graph = old_graph if old_graph is not None else {}

    # The deque is used to implement breadth-first traversal.
    # TODO: Consider whether to go depth-first instead.  This may
    # affect the order in which we process files within import cycles.
    new = new_modules if new_modules is not None else []
    entry_points: set[str] = set()
    # Seed the graph with the initial root sources.
    for bs in sources:
        try:
            st = State(
                id=bs.module, path=bs.path, source=bs.text, manager=manager, root_source=True
            )
        except ModuleNotFound:
            continue
        if st.id in graph:
            manager.errors.set_file(st.xpath, st.id, manager.options)
            manager.errors.report(
                -1,
                -1,
                f'Duplicate module named "{st.id}" (also at "{graph[st.id].xpath}")',
                blocker=True,
            )
            manager.errors.report(
                -1,
                -1,
                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "  # noqa: E501
                "for more info",
                severity="note",
            )
            manager.errors.report(
                -1,
                -1,
                "Common resolutions include: a) using `--exclude` to avoid checking one of them, "
                "b) adding `__init__.py` somewhere, c) using `--explicit-package-bases` or "
                "adjusting MYPYPATH",
                severity="note",
            )

            manager.errors.raise_error()
        graph[st.id] = st
        new.append(st)
        entry_points.add(bs.module)

    # Note: Running this each time could be slow in the daemon. If it's a problem, we
    # can do more work to maintain this incrementally.
    seen_files = {st.abspath: st for st in graph.values() if st.path}

    # Collect dependencies.  We go breadth-first.
    # More nodes might get added to new as we go, but that's fine.
    for st in new:
        assert st.ancestors is not None
        # Strip out indirect dependencies.  These will be dealt with
        # when they show up as direct dependencies, and there's a
        # scenario where they hurt:
        # - Suppose A imports B and B imports C.
        # - Suppose on the next round:
        #   - C is deleted;
        #   - B is updated to remove the dependency on C;
        #   - A is unchanged.
        # - In this case A's cached *direct* dependencies are still valid
        #   (since direct dependencies reflect the imports found in the source)
        #   but A's cached *indirect* dependency on C is wrong.
        dependencies = [dep for dep in st.dependencies if st.priorities.get(dep) != PRI_INDIRECT]
        if not manager.use_fine_grained_cache():
            # TODO: Ideally we could skip here modules that appeared in st.suppressed
            # because they are not in build with `follow-imports=skip`.
            # This way we could avoid overhead of cloning options in `State.__init__()`
            # below to get the option value. This is quite minor performance loss however.
            added = [dep for dep in st.suppressed if find_module_simple(dep, manager)]
        else:
            # During initial loading we don't care about newly added modules,
            # they will be taken care of during fine grained update. See also
            # comment about this in `State.__init__()`.
            added = []
        for dep in st.ancestors + dependencies + st.suppressed:
            ignored = dep in st.suppressed_set and dep not in entry_points
            if ignored and dep not in added:
                manager.missing_modules.add(dep)
            elif dep not in graph:
                try:
                    if dep in st.ancestors:
                        # TODO: Why not 'if dep not in st.dependencies' ?
                        # Ancestors don't have import context.
                        newst = State(
                            id=dep, path=None, source=None, manager=manager, ancestor_for=st
                        )
                    else:
                        newst = State(
                            id=dep,
                            path=None,
                            source=None,
                            manager=manager,
                            caller_state=st,
                            caller_line=st.dep_line_map.get(dep, 1),
                        )
                except ModuleNotFound:
                    if dep in st.dependencies_set:
                        st.suppress_dependency(dep)
                else:
                    if newst.path:
                        newst_path = os.path.abspath(newst.path)

                        if newst_path in seen_files:
                            manager.errors.report(
                                -1,
                                0,
                                "Source file found twice under different module names: "
                                '"{}" and "{}"'.format(seen_files[newst_path].id, newst.id),
                                blocker=True,
                            )
                            manager.errors.report(
                                -1,
                                0,
                                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "  # noqa: E501
                                "for more info",
                                severity="note",
                            )
                            manager.errors.report(
                                -1,
                                0,
                                "Common resolutions include: a) adding `__init__.py` somewhere, "
                                "b) using `--explicit-package-bases` or adjusting MYPYPATH",
                                severity="note",
                            )
                            manager.errors.raise_error()

                        seen_files[newst_path] = newst

                    assert newst.id not in graph, newst.id
                    graph[newst.id] = newst
                    new.append(newst)
            if dep in graph and dep in st.suppressed_set:
                # Previously suppressed file is now visible
                st.add_dependency(dep)
    manager.plugin.set_modules(manager.modules)
    return graph


</t>
<t tx="ekr.20220525082933.328">def process_graph(graph: Graph, manager: BuildManager) -&gt; None:
    """Process everything in dependency order."""
    sccs = sorted_components(graph)
    manager.log("Found %d SCCs; largest has %d nodes" % (len(sccs), max(len(scc) for scc in sccs)))

    fresh_scc_queue: list[list[str]] = []

    # We're processing SCCs from leaves (those without further
    # dependencies) to roots (those from which everything else can be
    # reached).
    for ascc in sccs:
        &lt;&lt; order scc &gt;&gt;
        scc_str = " ".join(scc)
        if fresh:
            manager.trace(f"Queuing {fresh_msg} SCC ({scc_str})")
            fresh_scc_queue.append(scc)
        else:
            &lt;&lt; log the processing &gt;&gt;
            process_stale_scc(graph, scc, manager)

    &lt;&lt; stats &amp; log &gt;&gt;
</t>
<t tx="ekr.20220525082933.331">def process_stale_scc(graph: Graph, scc: list[str], manager: BuildManager) -&gt; None:
    """Process the modules in one SCC from source code.

    Exception: If quick_and_dirty is set, use the cache for fresh modules.
    """
    stale = scc
    for id in stale:
        # We may already have parsed the module, or not.
        # If the former, parse_file() is a no-op.
        graph[id].parse_file()
    if "typing" in scc:
        # For historical reasons we need to manually add typing aliases
        # for built-in generic collections, see docstring of
        # SemanticAnalyzerPass2.add_builtin_aliases for details.
        typing_mod = graph["typing"].tree
        assert typing_mod, "The typing module was not parsed"
    mypy.semanal_main.semantic_analysis_for_scc(graph, scc, manager.errors)

    # Track what modules aren't yet done so we can finish them as soon
    # as possible, saving memory.
    unfinished_modules = set(stale)
    for id in stale:
        graph[id].type_check_first_pass()
        if not graph[id].type_checker().deferred_nodes:
            unfinished_modules.discard(id)
            graph[id].detect_partially_defined_vars(graph[id].type_map())
            graph[id].finish_passes()

    while unfinished_modules:
        for id in stale:
            if id not in unfinished_modules:
                continue
            if not graph[id].type_check_second_pass():
                unfinished_modules.discard(id)
                graph[id].detect_partially_defined_vars(graph[id].type_map())
                graph[id].finish_passes()
    for id in stale:
        graph[id].generate_unused_ignore_notes()
        graph[id].generate_ignore_without_code_notes()
    if any(manager.errors.is_errors_for_file(graph[id].xpath) for id in stale):
        for id in stale:
            graph[id].transitive_error = True
    for id in stale:
        manager.flush_errors(manager.errors.file_messages(graph[id].xpath), False)
        graph[id].write_cache()
        graph[id].mark_as_rechecked()


</t>
<t tx="ekr.20220525082933.377">def check_for_missing_annotations(self, fdef: FuncItem) -&gt; None:
    # Check for functions with unspecified/not fully specified types.
    def is_unannotated_any(t: Type) -&gt; bool:
        if not isinstance(t, ProperType):
            return False
        return isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated

    has_explicit_annotation = isinstance(fdef.type, CallableType) and any(
        not is_unannotated_any(t) for t in fdef.type.arg_types + [fdef.type.ret_type]
    )

    show_untyped = not self.is_typeshed_stub or self.options.warn_incomplete_stub
    check_incomplete_defs = self.options.disallow_incomplete_defs and has_explicit_annotation
    if show_untyped and (self.options.disallow_untyped_defs or check_incomplete_defs):
        if fdef.type is None and self.options.disallow_untyped_defs:
            if not fdef.arguments or (
                len(fdef.arguments) == 1
                and (fdef.arg_names[0] == "self" or fdef.arg_names[0] == "cls")
            ):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
                if not has_return_statement(fdef) and not fdef.is_generator:
                    self.note(
                        'Use "-&gt; None" if function does not return a value',
                        fdef,
                        code=codes.NO_UNTYPED_DEF,
                    )
            else:
                self.fail(message_registry.FUNCTION_TYPE_EXPECTED, fdef)
        elif isinstance(fdef.type, CallableType):
            ret_type = get_proper_type(fdef.type.ret_type)
            if is_unannotated_any(ret_type):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_generator:
                if is_unannotated_any(
                    self.get_generator_return_type(ret_type, fdef.is_coroutine)
                ):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_coroutine and isinstance(ret_type, Instance):
                if is_unannotated_any(self.get_coroutine_return_type(ret_type)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            if any(is_unannotated_any(t) for t in fdef.type.arg_types):
                self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)

</t>
<t tx="ekr.20220525082934.100"># BinOp(expr left, operator op, expr right)
def visit_BinOp(self, n: ast3.BinOp) -&gt; OpExpr:
    op = self.from_operator(n.op)

    if op is None:
        raise RuntimeError("cannot translate BinOp " + str(type(n.op)))

    e = OpExpr(op, self.visit(n.left), self.visit(n.right))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.101"># UnaryOp(unaryop op, expr operand)
def visit_UnaryOp(self, n: ast3.UnaryOp) -&gt; UnaryExpr:
    op = None
    if isinstance(n.op, ast3.Invert):
        op = "~"
    elif isinstance(n.op, ast3.Not):
        op = "not"
    elif isinstance(n.op, ast3.UAdd):
        op = "+"
    elif isinstance(n.op, ast3.USub):
        op = "-"

    if op is None:
        raise RuntimeError("cannot translate UnaryOp " + str(type(n.op)))

    e = UnaryExpr(op, self.visit(n.operand))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.102"># Lambda(arguments args, expr body)
def visit_Lambda(self, n: ast3.Lambda) -&gt; LambdaExpr:
    body = ast3.Return(n.body)
    body.lineno = n.body.lineno
    body.col_offset = n.body.col_offset

    e = LambdaExpr(
        self.transform_args(n.args, n.lineno), self.as_required_block([body], n.lineno)
    )
    e.set_line(n.lineno, n.col_offset)  # Overrides set_line -- can't use self.set_line
    return e

</t>
<t tx="ekr.20220525082934.103"># IfExp(expr test, expr body, expr orelse)
def visit_IfExp(self, n: ast3.IfExp) -&gt; ConditionalExpr:
    e = ConditionalExpr(self.visit(n.test), self.visit(n.body), self.visit(n.orelse))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1039">class StrExpr(Expression):
    """String literal"""

    __slots__ = ("value",)

    value: str  # '' by default

    def __init__(self, value: str) -&gt; None:
    @others
</t>
<t tx="ekr.20220525082934.104"># Dict(expr* keys, expr* values)
def visit_Dict(self, n: ast3.Dict) -&gt; DictExpr:
    e = DictExpr(
        list(zip(self.translate_opt_expr_list(n.keys), self.translate_expr_list(n.values)))
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1040">    super().__init__()
    self.value = value

</t>
<t tx="ekr.20220525082934.1041">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_str_expr(self)


</t>
<t tx="ekr.20220525082934.105"># Set(expr* elts)
def visit_Set(self, n: ast3.Set) -&gt; SetExpr:
    e = SetExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.106"># ListComp(expr elt, comprehension* generators)
def visit_ListComp(self, n: ast3.ListComp) -&gt; ListComprehension:
    e = ListComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1067"># Kinds of arguments
@unique
class ArgKind(Enum):
    # Positional argument
    ARG_POS = 0
    # Positional, optional argument (functions only, not calls)
    ARG_OPT = 1
    # *arg argument
    ARG_STAR = 2
    # Keyword argument x=y in call, or keyword-only function arg
    ARG_NAMED = 3
    # **arg argument
    ARG_STAR2 = 4
    # In an argument list, keyword-only and also optional
    ARG_NAMED_OPT = 5

    @others
ARG_POS: Final = ArgKind.ARG_POS
ARG_OPT: Final = ArgKind.ARG_OPT
ARG_STAR: Final = ArgKind.ARG_STAR
ARG_NAMED: Final = ArgKind.ARG_NAMED
ARG_STAR2: Final = ArgKind.ARG_STAR2
ARG_NAMED_OPT: Final = ArgKind.ARG_NAMED_OPT


</t>
<t tx="ekr.20220525082934.1068">def is_positional(self, star: bool = False) -&gt; bool:
    return self == ARG_POS or self == ARG_OPT or (star and self == ARG_STAR)

</t>
<t tx="ekr.20220525082934.1069">def is_named(self, star: bool = False) -&gt; bool:
    return self == ARG_NAMED or self == ARG_NAMED_OPT or (star and self == ARG_STAR2)

</t>
<t tx="ekr.20220525082934.107"># SetComp(expr elt, comprehension* generators)
def visit_SetComp(self, n: ast3.SetComp) -&gt; SetComprehension:
    e = SetComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1070">def is_required(self) -&gt; bool:
    return self == ARG_POS or self == ARG_NAMED

</t>
<t tx="ekr.20220525082934.1071">def is_optional(self) -&gt; bool:
    return self == ARG_OPT or self == ARG_NAMED_OPT

</t>
<t tx="ekr.20220525082934.1072">def is_star(self) -&gt; bool:
    return self == ARG_STAR or self == ARG_STAR2


</t>
<t tx="ekr.20220525082934.108"># DictComp(expr key, expr value, comprehension* generators)
def visit_DictComp(self, n: ast3.DictComp) -&gt; DictionaryComprehension:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = DictionaryComprehension(
        self.visit(n.key), self.visit(n.value), targets, iters, ifs_list, is_async
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.109"># GeneratorExp(expr elt, comprehension* generators)
def visit_GeneratorExp(self, n: ast3.GeneratorExp) -&gt; GeneratorExpr:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = GeneratorExpr(self.visit(n.elt), targets, iters, ifs_list, is_async)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.110"># Await(expr value)
def visit_Await(self, n: ast3.Await) -&gt; AwaitExpr:
    v = self.visit(n.value)
    e = AwaitExpr(v)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.111"># Yield(expr? value)
def visit_Yield(self, n: ast3.Yield) -&gt; YieldExpr:
    e = YieldExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.112"># YieldFrom(expr value)
def visit_YieldFrom(self, n: ast3.YieldFrom) -&gt; YieldFromExpr:
    e = YieldFromExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.113"># Compare(expr left, cmpop* ops, expr* comparators)
def visit_Compare(self, n: ast3.Compare) -&gt; ComparisonExpr:
    operators = [self.from_comp_operator(o) for o in n.ops]
    operands = self.translate_expr_list([n.left] + n.comparators)
    e = ComparisonExpr(operators, operands)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.114"># Call(expr func, expr* args, keyword* keywords)
# keyword = (identifier? arg, expr value)
def visit_Call(self, n: Call) -&gt; CallExpr:
    args = n.args
    keywords = n.keywords
    keyword_names = [k.arg for k in keywords]
    arg_types = self.translate_expr_list(
        [a.value if isinstance(a, Starred) else a for a in args] + [k.value for k in keywords]
    )
    arg_kinds = [ARG_STAR if type(a) is Starred else ARG_POS for a in args] + [
        ARG_STAR2 if arg is None else ARG_NAMED for arg in keyword_names
    ]
    e = CallExpr(
        self.visit(n.func),
        arg_types,
        arg_kinds,
        cast("List[Optional[str]]", [None] * len(args)) + keyword_names,
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.115"># Constant(object value) -- a constant, in Python 3.8.
def visit_Constant(self, n: Constant) -&gt; Any:
    val = n.value
    e: Any = None
    if val is None:
        e = NameExpr("None")
    elif isinstance(val, str):
        e = StrExpr(n.s)
    elif isinstance(val, bytes):
        e = BytesExpr(bytes_to_human_readable_repr(n.s))
    elif isinstance(val, bool):  # Must check before int!
        e = NameExpr(str(val))
    elif isinstance(val, int):
        e = IntExpr(val)
    elif isinstance(val, float):
        e = FloatExpr(val)
    elif isinstance(val, complex):
        e = ComplexExpr(val)
    elif val is Ellipsis:
        e = EllipsisExpr()
    else:
        raise RuntimeError("Constant not implemented for " + str(type(val)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.116"># Num(object n) -- a number as a PyObject.
def visit_Num(self, n: ast3.Num) -&gt; IntExpr | FloatExpr | ComplexExpr:
    # The n field has the type complex, but complex isn't *really*
    # a parent of int and float, and this causes isinstance below
    # to think that the complex branch is always picked. Avoid
    # this by throwing away the type.
    val: object = n.n
    if isinstance(val, int):
        e: IntExpr | FloatExpr | ComplexExpr = IntExpr(val)
    elif isinstance(val, float):
        e = FloatExpr(val)
    elif isinstance(val, complex):
        e = ComplexExpr(val)
    else:
        raise RuntimeError("num not implemented for " + str(type(val)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.117"># Str(string s)
def visit_Str(self, n: Str) -&gt; StrExpr:
    e = StrExpr(n.s)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.118"># JoinedStr(expr* values)
def visit_JoinedStr(self, n: ast3.JoinedStr) -&gt; Expression:
    # Each of n.values is a str or FormattedValue; we just concatenate
    # them all using ''.join.
    empty_string = StrExpr("")
    empty_string.set_line(n.lineno, n.col_offset)
    strs_to_join = ListExpr(self.translate_expr_list(n.values))
    strs_to_join.set_line(empty_string)
    # Don't make unnecessary join call if there is only one str to join
    if len(strs_to_join.items) == 1:
        return self.set_line(strs_to_join.items[0], n)
    join_method = MemberExpr(empty_string, "join")
    join_method.set_line(empty_string)
    result_expression = CallExpr(join_method, [strs_to_join], [ARG_POS], [None])
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20220525082934.119"># FormattedValue(expr value)
def visit_FormattedValue(self, n: ast3.FormattedValue) -&gt; Expression:
    # A FormattedValue is a component of a JoinedStr, or it can exist
    # on its own. We translate them to individual '{}'.format(value)
    # calls. Format specifier and conversion information is passed along
    # to allow mypyc to support f-strings with format specifiers and conversions.
    val_exp = self.visit(n.value)
    val_exp.set_line(n.lineno, n.col_offset)
    conv_str = "" if n.conversion is None or n.conversion &lt; 0 else "!" + chr(n.conversion)
    format_string = StrExpr("{" + conv_str + ":{}}")
    format_spec_exp = self.visit(n.format_spec) if n.format_spec is not None else StrExpr("")
    format_string.set_line(n.lineno, n.col_offset)
    format_method = MemberExpr(format_string, "format")
    format_method.set_line(format_string)
    result_expression = CallExpr(
        format_method, [val_exp, format_spec_exp], [ARG_POS, ARG_POS], [None, None]
    )
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20220525082934.120"># Bytes(bytes s)
def visit_Bytes(self, n: ast3.Bytes) -&gt; BytesExpr | StrExpr:
    e = BytesExpr(bytes_to_human_readable_repr(n.s))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.121"># NameConstant(singleton value)
def visit_NameConstant(self, n: NameConstant) -&gt; NameExpr:
    e = NameExpr(str(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.122"># Ellipsis
def visit_Ellipsis(self, n: ast3_Ellipsis) -&gt; EllipsisExpr:
    e = EllipsisExpr()
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.123"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; MemberExpr | SuperExpr:
    value = n.value
    member_expr = MemberExpr(self.visit(value), n.attr)
    obj = member_expr.expr
    if (
        isinstance(obj, CallExpr)
        and isinstance(obj.callee, NameExpr)
        and obj.callee.name == "super"
    ):
        e: MemberExpr | SuperExpr = SuperExpr(member_expr.name, obj)
    else:
        e = member_expr
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.124"># Subscript(expr value, slice slice, expr_context ctx)
def visit_Subscript(self, n: ast3.Subscript) -&gt; IndexExpr:
    e = IndexExpr(self.visit(n.value), self.visit(n.slice))
    self.set_line(e, n)
    # alias to please mypyc
    is_py38_or_earlier = sys.version_info &lt; (3, 9)
    if isinstance(n.slice, ast3.Slice) or (
        is_py38_or_earlier and isinstance(n.slice, ast3.ExtSlice)
    ):
        # Before Python 3.9, Slice has no line/column in the raw ast. To avoid incompatibility
        # visit_Slice doesn't set_line, even in Python 3.9 on.
        # ExtSlice also has no line/column info. In Python 3.9 on, line/column is set for
        # e.index when visiting n.slice.
        e.index.line = e.line
        e.index.column = e.column
    return e

</t>
<t tx="ekr.20220525082934.125"># Starred(expr value, expr_context ctx)
def visit_Starred(self, n: Starred) -&gt; StarExpr:
    e = StarExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.126"># Name(identifier id, expr_context ctx)
def visit_Name(self, n: Name) -&gt; NameExpr:
    e = NameExpr(n.id)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.127"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast3.List) -&gt; ListExpr | TupleExpr:
    expr_list: list[Expression] = [self.visit(e) for e in n.elts]
    if isinstance(n.ctx, ast3.Store):
        # [x, y] = z and (x, y) = z means exactly the same thing
        e: ListExpr | TupleExpr = TupleExpr(expr_list)
    else:
        e = ListExpr(expr_list)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.128"># Tuple(expr* elts, expr_context ctx)
def visit_Tuple(self, n: ast3.Tuple) -&gt; TupleExpr:
    e = TupleExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.129"># --- slice ---

</t>
<t tx="ekr.20220525082934.130"># Slice(expr? lower, expr? upper, expr? step)
def visit_Slice(self, n: ast3.Slice) -&gt; SliceExpr:
    return SliceExpr(self.visit(n.lower), self.visit(n.upper), self.visit(n.step))

</t>
<t tx="ekr.20220525082934.131"># ExtSlice(slice* dims)
def visit_ExtSlice(self, n: ast3.ExtSlice) -&gt; TupleExpr:
    # cast for mypyc's benefit on Python 3.9
    return TupleExpr(self.translate_expr_list(cast(Any, n).dims))

</t>
<t tx="ekr.20220525082934.132"># Index(expr value)
def visit_Index(self, n: Index) -&gt; Node:
    # cast for mypyc's benefit on Python 3.9
    value = self.visit(cast(Any, n).value)
    assert isinstance(value, Node)
    return value

</t>
<t tx="ekr.20220525082934.133"># Match(expr subject, match_case* cases) # python 3.10 and later
def visit_Match(self, n: Match) -&gt; MatchStmt:
    node = MatchStmt(
        self.visit(n.subject),
        [self.visit(c.pattern) for c in n.cases],
        [self.visit(c.guard) for c in n.cases],
        [self.as_required_block(c.body, n.lineno) for c in n.cases],
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.134">def visit_MatchValue(self, n: MatchValue) -&gt; ValuePattern:
    node = ValuePattern(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.135">def visit_MatchSingleton(self, n: MatchSingleton) -&gt; SingletonPattern:
    node = SingletonPattern(n.value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.136">def visit_MatchSequence(self, n: MatchSequence) -&gt; SequencePattern:
    patterns = [self.visit(p) for p in n.patterns]
    stars = [p for p in patterns if isinstance(p, StarredPattern)]
    assert len(stars) &lt; 2

    node = SequencePattern(patterns)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.137">def visit_MatchStar(self, n: MatchStar) -&gt; StarredPattern:
    if n.name is None:
        node = StarredPattern(None)
    else:
        node = StarredPattern(NameExpr(n.name))

    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.138">def visit_MatchMapping(self, n: MatchMapping) -&gt; MappingPattern:
    keys = [self.visit(k) for k in n.keys]
    values = [self.visit(v) for v in n.patterns]

    if n.rest is None:
        rest = None
    else:
        rest = NameExpr(n.rest)

    node = MappingPattern(keys, values, rest)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.139">def visit_MatchClass(self, n: MatchClass) -&gt; ClassPattern:
    class_ref = self.visit(n.cls)
    assert isinstance(class_ref, RefExpr)
    positionals = [self.visit(p) for p in n.patterns]
    keyword_keys = n.kwd_attrs
    keyword_values = [self.visit(p) for p in n.kwd_patterns]

    node = ClassPattern(class_ref, positionals, keyword_keys, keyword_values)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.140"># MatchAs(expr pattern, identifier name)
def visit_MatchAs(self, n: MatchAs) -&gt; AsPattern:
    if n.name is None:
        name = None
    else:
        name = NameExpr(n.name)
        name = self.set_line(name, n)
    node = AsPattern(self.visit(n.pattern), name)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.141"># MatchOr(expr* pattern)
def visit_MatchOr(self, n: MatchOr) -&gt; OrPattern:
    node = OrPattern([self.visit(pattern) for pattern in n.patterns])
    return self.set_line(node, n)


</t>
<t tx="ekr.20220525082934.40">class ASTConverter:
    @others
</t>
<t tx="ekr.20220525082934.41">def __init__(self, options: Options, is_stub: bool, errors: Errors) -&gt; None:
    # 'C' for class, 'F' for function
    self.class_and_function_stack: list[Literal["C", "F"]] = []
    self.imports: list[ImportBase] = []

    self.options = options
    self.is_stub = is_stub
    self.errors = errors

    self.type_ignores: dict[int, list[str]] = {}

    # Cache of visit_X methods keyed by type of visited object
    self.visitor_cache: dict[type, Callable[[AST | None], Any]] = {}

</t>
<t tx="ekr.20220525082934.42">def note(self, msg: str, line: int, column: int) -&gt; None:
    self.errors.report(line, column, msg, severity="note", code=codes.SYNTAX)

</t>
<t tx="ekr.20220525082934.43">def fail(
    self,
    msg: str,
    line: int,
    column: int,
    blocker: bool = True,
    code: codes.ErrorCode = codes.SYNTAX,
) -&gt; None:
    if blocker or not self.options.ignore_errors:
        self.errors.report(line, column, msg, blocker=blocker, code=code)

</t>
<t tx="ekr.20220525082934.44">def fail_merge_overload(self, node: IfStmt) -&gt; None:
    self.fail(
        "Condition can't be inferred, unable to merge overloads",
        line=node.line,
        column=node.column,
        blocker=False,
        code=codes.MISC,
    )

</t>
<t tx="ekr.20220525082934.45">def visit(self, node: AST | None) -&gt; Any:
    if node is None:
        return None
    typeobj = type(node)
    visitor = self.visitor_cache.get(typeobj)
    if visitor is None:
        method = "visit_" + node.__class__.__name__
        visitor = getattr(self, method)
        self.visitor_cache[typeobj] = visitor
    return visitor(node)

</t>
<t tx="ekr.20220525082934.46">def set_line(self, node: N, n: AstNode) -&gt; N:
    node.line = n.lineno
    node.column = n.col_offset
    node.end_line = getattr(n, "end_lineno", None)
    node.end_column = getattr(n, "end_col_offset", None)

    return node

</t>
<t tx="ekr.20220525082934.47">def translate_opt_expr_list(self, l: Sequence[AST | None]) -&gt; list[Expression | None]:
    res: list[Expression | None] = []
    for e in l:
        exp = self.visit(e)
        res.append(exp)
    return res

</t>
<t tx="ekr.20220525082934.48">def translate_expr_list(self, l: Sequence[AST]) -&gt; list[Expression]:
    return cast(List[Expression], self.translate_opt_expr_list(l))

</t>
<t tx="ekr.20220525082934.49">def get_lineno(self, node: ast3.expr | ast3.stmt) -&gt; int:
    if (
        isinstance(node, (ast3.AsyncFunctionDef, ast3.ClassDef, ast3.FunctionDef))
        and node.decorator_list
    ):
        return node.decorator_list[0].lineno
    return node.lineno

</t>
<t tx="ekr.20220525082934.50">def translate_stmt_list(
    self, stmts: Sequence[ast3.stmt], ismodule: bool = False
) -&gt; list[Statement]:
    # A "# type: ignore" comment before the first statement of a module
    # ignores the whole module:
    if (
        ismodule
        and stmts
        and self.type_ignores
        and min(self.type_ignores) &lt; self.get_lineno(stmts[0])
    ):
        if self.type_ignores[min(self.type_ignores)]:
            self.fail(
                (
                    "type ignore with error code is not supported for modules; "
                    "use `# mypy: disable-error-code=...`"
                ),
                line=min(self.type_ignores),
                column=0,
                blocker=False,
            )
        self.errors.used_ignored_lines[self.errors.file][min(self.type_ignores)].append(
            codes.FILE.code
        )
        block = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        mark_block_unreachable(block)
        return [block]

    res: list[Statement] = []
    for stmt in stmts:
        node = self.visit(stmt)
        res.append(node)

    return res

</t>
<t tx="ekr.20220525082934.504">def main(
    *,
    args: list[str] | None = None,
    stdout: TextIO = sys.stdout,
    stderr: TextIO = sys.stderr,
    clean_exit: bool = False,
) -&gt; None:
    """Main entry point to the type checker.

    Args:
        args: Custom command-line arguments.  If not given, sys.argv[1:] will
            be used.
        clean_exit: Don't hard kill the process on exit. This allows catching
            SystemExit.
    """
    util.check_python_version("mypy")
    t0 = time.time()
    # To log stat() calls: os.stat = stat_proxy
    sys.setrecursionlimit(2**14)
    if args is None:
        args = sys.argv[1:]

    fscache = FileSystemCache()
    &lt;&lt; check options &gt;&gt;
    if options.install_types and not sources:
        install_types(formatter, options, non_interactive=options.non_interactive)
        return

    res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)

    &lt;&lt; shut down &gt;&gt;

    # HACK: keep res alive so that mypyc won't free it before the hard_exit
    list([res])


</t>
<t tx="ekr.20220525082934.505">def run_build(
    sources: list[BuildSource],
    options: Options,
    fscache: FileSystemCache,
    t0: float,
    stdout: TextIO,
    stderr: TextIO,
) -&gt; tuple[build.BuildResult | None, list[str], bool]:
    formatter = util.FancyFormatter(stdout, stderr, options.hide_error_codes)

    messages = []

    @others
    serious = False
    blockers = False
    res = None
    try:
        # Keep a dummy reference (res) for memory profiling afterwards, as otherwise
        # the result could be freed.
        res = build.build(sources, options, None, flush_errors, fscache, stdout, stderr)
    except CompileError as e:
        blockers = True
        if not e.use_stdout:
            serious = True
    &lt;&lt; warn about unused configs &gt;&gt;
    maybe_write_junit_xml(time.time() - t0, serious, messages, options)
    return res, messages, blockers


</t>
<t tx="ekr.20220525082934.506">def flush_errors(new_messages: list[str], serious: bool) -&gt; None:
    if options.pretty:
        new_messages = formatter.fit_in_terminal(new_messages)
    messages.extend(new_messages)
    if options.non_interactive:
        # Collect messages and possibly show them later.
        return
    f = stderr if serious else stdout
    show_messages(new_messages, f, formatter, options)

</t>
<t tx="ekr.20220525082934.51">def translate_type_comment(
    self, n: ast3.stmt | ast3.arg, type_comment: str | None
) -&gt; ProperType | None:
    if type_comment is None:
        return None
    else:
        lineno = n.lineno
        extra_ignore, typ = parse_type_comment(type_comment, lineno, n.col_offset, self.errors)
        if extra_ignore is not None:
            self.type_ignores[lineno] = extra_ignore
        return typ

</t>
<t tx="ekr.20220525082934.52">op_map: Final[dict[type[AST], str]] = {
    ast3.Add: "+",
    ast3.Sub: "-",
    ast3.Mult: "*",
    ast3.MatMult: "@",
    ast3.Div: "/",
    ast3.Mod: "%",
    ast3.Pow: "**",
    ast3.LShift: "&lt;&lt;",
    ast3.RShift: "&gt;&gt;",
    ast3.BitOr: "|",
    ast3.BitXor: "^",
    ast3.BitAnd: "&amp;",
    ast3.FloorDiv: "//",
}

</t>
<t tx="ekr.20220525082934.53">def from_operator(self, op: ast3.operator) -&gt; str:
    op_name = ASTConverter.op_map.get(type(op))
    if op_name is None:
        raise RuntimeError("Unknown operator " + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20220525082934.54">comp_op_map: Final[dict[type[AST], str]] = {
    ast3.Gt: "&gt;",
    ast3.Lt: "&lt;",
    ast3.Eq: "==",
    ast3.GtE: "&gt;=",
    ast3.LtE: "&lt;=",
    ast3.NotEq: "!=",
    ast3.Is: "is",
    ast3.IsNot: "is not",
    ast3.In: "in",
    ast3.NotIn: "not in",
}

</t>
<t tx="ekr.20220525082934.55">def from_comp_operator(self, op: ast3.cmpop) -&gt; str:
    op_name = ASTConverter.comp_op_map.get(type(op))
    if op_name is None:
        raise RuntimeError("Unknown comparison operator " + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20220525082934.56">def as_block(self, stmts: list[ast3.stmt], lineno: int) -&gt; Block | None:
    b = None
    if stmts:
        b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        b.set_line(lineno)
    return b

</t>
<t tx="ekr.20220525082934.57">def as_required_block(self, stmts: list[ast3.stmt], lineno: int) -&gt; Block:
    assert stmts  # must be non-empty
    b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
    # TODO: in most call sites line is wrong (includes first line of enclosing statement)
    # TODO: also we need to set the column, and the end position here.
    b.set_line(lineno)
    return b

</t>
<t tx="ekr.20220525082934.58">def fix_function_overloads(self, stmts: list[Statement]) -&gt; list[Statement]:
    ret: list[Statement] = []
    current_overload: list[OverloadPart] = []
    current_overload_name: str | None = None
    seen_unconditional_func_def = False
    last_if_stmt: IfStmt | None = None
    last_if_overload: Decorator | FuncDef | OverloadedFuncDef | None = None
    last_if_stmt_overload_name: str | None = None
    last_if_unknown_truth_value: IfStmt | None = None
    skipped_if_stmts: list[IfStmt] = []
    for stmt in stmts:
        if_overload_name: str | None = None
        if_block_with_overload: Block | None = None
        if_unknown_truth_value: IfStmt | None = None
        if isinstance(stmt, IfStmt) and seen_unconditional_func_def is False:
            # Check IfStmt block to determine if function overloads can be merged
            if_overload_name = self._check_ifstmt_for_overloads(stmt, current_overload_name)
            if if_overload_name is not None:
                (
                    if_block_with_overload,
                    if_unknown_truth_value,
                ) = self._get_executable_if_block_with_overloads(stmt)

        if (
            current_overload_name is not None
            and isinstance(stmt, (Decorator, FuncDef))
            and stmt.name == current_overload_name
        ):
            if last_if_stmt is not None:
                skipped_if_stmts.append(last_if_stmt)
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if last_if_unknown_truth_value:
                self.fail_merge_overload(last_if_unknown_truth_value)
                last_if_unknown_truth_value = None
            current_overload.append(stmt)
            if isinstance(stmt, FuncDef):
                seen_unconditional_func_def = True
        elif (
            current_overload_name is not None
            and isinstance(stmt, IfStmt)
            and if_overload_name == current_overload_name
        ):
            # IfStmt only contains stmts relevant to current_overload.
            # Check if stmts are reachable and add them to current_overload,
            # otherwise skip IfStmt to allow subsequent overload
            # or function definitions.
            skipped_if_stmts.append(stmt)
            if if_block_with_overload is None:
                if if_unknown_truth_value is not None:
                    self.fail_merge_overload(if_unknown_truth_value)
                continue
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if isinstance(if_block_with_overload.body[-1], OverloadedFuncDef):
                skipped_if_stmts.extend(cast(List[IfStmt], if_block_with_overload.body[:-1]))
                current_overload.extend(if_block_with_overload.body[-1].items)
            else:
                current_overload.append(
                    cast(Union[Decorator, FuncDef], if_block_with_overload.body[0])
                )
        else:
            if last_if_stmt is not None:
                ret.append(last_if_stmt)
                last_if_stmt_overload_name = current_overload_name
                last_if_stmt, last_if_overload = None, None
                last_if_unknown_truth_value = None

            if current_overload and current_overload_name == last_if_stmt_overload_name:
                # Remove last stmt (IfStmt) from ret if the overload names matched
                # Only happens if no executable block had been found in IfStmt
                skipped_if_stmts.append(cast(IfStmt, ret.pop()))
            if current_overload and skipped_if_stmts:
                # Add bare IfStmt (without overloads) to ret
                # Required for mypy to be able to still check conditions
                for if_stmt in skipped_if_stmts:
                    self._strip_contents_from_if_stmt(if_stmt)
                    ret.append(if_stmt)
                skipped_if_stmts = []
            if len(current_overload) == 1:
                ret.append(current_overload[0])
            elif len(current_overload) &gt; 1:
                ret.append(OverloadedFuncDef(current_overload))

            # If we have multiple decorated functions named "_" next to each, we want to treat
            # them as a series of regular FuncDefs instead of one OverloadedFuncDef because
            # most of mypy/mypyc assumes that all the functions in an OverloadedFuncDef are
            # related, but multiple underscore functions next to each other aren't necessarily
            # related
            seen_unconditional_func_def = False
            if isinstance(stmt, Decorator) and not unnamed_function(stmt.name):
                current_overload = [stmt]
                current_overload_name = stmt.name
            elif isinstance(stmt, IfStmt) and if_overload_name is not None:
                current_overload = []
                current_overload_name = if_overload_name
                last_if_stmt = stmt
                last_if_stmt_overload_name = None
                if if_block_with_overload is not None:
                    skipped_if_stmts.extend(
                        cast(List[IfStmt], if_block_with_overload.body[:-1])
                    )
                    last_if_overload = cast(
                        Union[Decorator, FuncDef, OverloadedFuncDef],
                        if_block_with_overload.body[-1],
                    )
                last_if_unknown_truth_value = if_unknown_truth_value
            else:
                current_overload = []
                current_overload_name = None
                ret.append(stmt)

    if current_overload and skipped_if_stmts:
        # Add bare IfStmt (without overloads) to ret
        # Required for mypy to be able to still check conditions
        for if_stmt in skipped_if_stmts:
            self._strip_contents_from_if_stmt(if_stmt)
            ret.append(if_stmt)
    if len(current_overload) == 1:
        ret.append(current_overload[0])
    elif len(current_overload) &gt; 1:
        ret.append(OverloadedFuncDef(current_overload))
    elif last_if_overload is not None:
        ret.append(last_if_overload)
    elif last_if_stmt is not None:
        ret.append(last_if_stmt)
    return ret

</t>
<t tx="ekr.20220525082934.59">def _check_ifstmt_for_overloads(
    self, stmt: IfStmt, current_overload_name: str | None = None
) -&gt; str | None:
    """Check if IfStmt contains only overloads with the same name.
    Return overload_name if found, None otherwise.
    """
    # Check that block only contains a single Decorator, FuncDef, or OverloadedFuncDef.
    # Multiple overloads have already been merged as OverloadedFuncDef.
    if not (
        len(stmt.body[0].body) == 1
        and (
            isinstance(stmt.body[0].body[0], (Decorator, OverloadedFuncDef))
            or current_overload_name is not None
            and isinstance(stmt.body[0].body[0], FuncDef)
        )
        or len(stmt.body[0].body) &gt; 1
        and isinstance(stmt.body[0].body[-1], OverloadedFuncDef)
        and all(self._is_stripped_if_stmt(if_stmt) for if_stmt in stmt.body[0].body[:-1])
    ):
        return None

    overload_name = cast(
        Union[Decorator, FuncDef, OverloadedFuncDef], stmt.body[0].body[-1]
    ).name
    if stmt.else_body is None:
        return overload_name

    if len(stmt.else_body.body) == 1:
        # For elif: else_body contains an IfStmt itself -&gt; do a recursive check.
        if (
            isinstance(stmt.else_body.body[0], (Decorator, FuncDef, OverloadedFuncDef))
            and stmt.else_body.body[0].name == overload_name
        ):
            return overload_name
        if (
            isinstance(stmt.else_body.body[0], IfStmt)
            and self._check_ifstmt_for_overloads(stmt.else_body.body[0], current_overload_name)
            == overload_name
        ):
            return overload_name

    return None

</t>
<t tx="ekr.20220525082934.60">def _get_executable_if_block_with_overloads(
    self, stmt: IfStmt
) -&gt; tuple[Block | None, IfStmt | None]:
    """Return block from IfStmt that will get executed.

    Return
        0 -&gt; A block if sure that alternative blocks are unreachable.
        1 -&gt; An IfStmt if the reachability of it can't be inferred,
             i.e. the truth value is unknown.
    """
    infer_reachability_of_if_statement(stmt, self.options)
    if stmt.else_body is None and stmt.body[0].is_unreachable is True:
        # always False condition with no else
        return None, None
    if (
        stmt.else_body is None
        or stmt.body[0].is_unreachable is False
        and stmt.else_body.is_unreachable is False
    ):
        # The truth value is unknown, thus not conclusive
        return None, stmt
    if stmt.else_body.is_unreachable is True:
        # else_body will be set unreachable if condition is always True
        return stmt.body[0], None
    if stmt.body[0].is_unreachable is True:
        # body will be set unreachable if condition is always False
        # else_body can contain an IfStmt itself (for elif) -&gt; do a recursive check
        if isinstance(stmt.else_body.body[0], IfStmt):
            return self._get_executable_if_block_with_overloads(stmt.else_body.body[0])
        return stmt.else_body, None
    return None, stmt

</t>
<t tx="ekr.20220525082934.61">def _strip_contents_from_if_stmt(self, stmt: IfStmt) -&gt; None:
    """Remove contents from IfStmt.

    Needed to still be able to check the conditions after the contents
    have been merged with the surrounding function overloads.
    """
    if len(stmt.body) == 1:
        stmt.body[0].body = []
    if stmt.else_body and len(stmt.else_body.body) == 1:
        if isinstance(stmt.else_body.body[0], IfStmt):
            self._strip_contents_from_if_stmt(stmt.else_body.body[0])
        else:
            stmt.else_body.body = []

</t>
<t tx="ekr.20220525082934.62">def _is_stripped_if_stmt(self, stmt: Statement) -&gt; bool:
    """Check stmt to make sure it is a stripped IfStmt.

    See also: _strip_contents_from_if_stmt
    """
    if not isinstance(stmt, IfStmt):
        return False

    if not (len(stmt.body) == 1 and len(stmt.body[0].body) == 0):
        # Body not empty
        return False

    if not stmt.else_body or len(stmt.else_body.body) == 0:
        # No or empty else_body
        return True

    # For elif, IfStmt are stored recursively in else_body
    return self._is_stripped_if_stmt(stmt.else_body.body[0])

</t>
<t tx="ekr.20220525082934.63">def in_method_scope(self) -&gt; bool:
    return self.class_and_function_stack[-2:] == ["C", "F"]

</t>
<t tx="ekr.20220525082934.64">def translate_module_id(self, id: str) -&gt; str:
    """Return the actual, internal module id for a source text id."""
    if id == self.options.custom_typing_module:
        return "typing"
    return id

</t>
<t tx="ekr.20220525082934.65">def visit_Module(self, mod: ast3.Module) -&gt; MypyFile:
    self.type_ignores = {}
    for ti in mod.type_ignores:
        parsed = parse_type_ignore_tag(ti.tag)  # type: ignore[attr-defined]
        if parsed is not None:
            self.type_ignores[ti.lineno] = parsed
        else:
            self.fail(INVALID_TYPE_IGNORE, ti.lineno, -1, blocker=False)
    body = self.fix_function_overloads(self.translate_stmt_list(mod.body, ismodule=True))
    return MypyFile(body, self.imports, False, self.type_ignores)

</t>
<t tx="ekr.20220525082934.66"># --- stmt ---
# FunctionDef(identifier name, arguments args,
#             stmt* body, expr* decorator_list, expr? returns, string? type_comment)
# arguments = (arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults,
#              arg? kwarg, expr* defaults)
def visit_FunctionDef(self, n: ast3.FunctionDef) -&gt; FuncDef | Decorator:
    return self.do_func_def(n)

</t>
<t tx="ekr.20220525082934.67"># AsyncFunctionDef(identifier name, arguments args,
#                  stmt* body, expr* decorator_list, expr? returns, string? type_comment)
def visit_AsyncFunctionDef(self, n: ast3.AsyncFunctionDef) -&gt; FuncDef | Decorator:
    return self.do_func_def(n, is_coroutine=True)

</t>
<t tx="ekr.20220525082934.68">def do_func_def(
    self, n: ast3.FunctionDef | ast3.AsyncFunctionDef, is_coroutine: bool = False
) -&gt; FuncDef | Decorator:
    """Helper shared between visit_FunctionDef and visit_AsyncFunctionDef."""
    self.class_and_function_stack.append("F")
    no_type_check = bool(
        n.decorator_list and any(is_no_type_check_decorator(d) for d in n.decorator_list)
    )

    lineno = n.lineno
    args = self.transform_args(n.args, lineno, no_type_check=no_type_check)
    if special_function_elide_names(n.name):
        for arg in args:
            arg.pos_only = True

    arg_kinds = [arg.kind for arg in args]
    arg_names = [None if arg.pos_only else arg.variable.name for arg in args]

    arg_types: list[Type | None] = []
    if no_type_check:
        arg_types = [None] * len(args)
        return_type = None
    elif n.type_comment is not None:
        try:
            func_type_ast = ast3_parse(n.type_comment, "&lt;func_type&gt;", "func_type")
            assert isinstance(func_type_ast, FunctionType)
            # for ellipsis arg
            if len(func_type_ast.argtypes) == 1 and isinstance(
                func_type_ast.argtypes[0], ast3_Ellipsis
            ):
                if n.returns:
                    # PEP 484 disallows both type annotations and type comments
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                arg_types = [
                    a.type_annotation
                    if a.type_annotation is not None
                    else AnyType(TypeOfAny.unannotated)
                    for a in args
                ]
            else:
                # PEP 484 disallows both type annotations and type comments
                if n.returns or any(a.type_annotation is not None for a in args):
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                translated_args: list[Type] = TypeConverter(
                    self.errors, line=lineno, override_column=n.col_offset
                ).translate_expr_list(func_type_ast.argtypes)
                # Use a cast to work around `list` invariance
                arg_types = cast(List[Optional[Type]], translated_args)
            return_type = TypeConverter(self.errors, line=lineno).visit(func_type_ast.returns)

            # add implicit self type
            if self.in_method_scope() and len(arg_types) &lt; len(args):
                arg_types.insert(0, AnyType(TypeOfAny.special_form))
        except SyntaxError:
            stripped_type = n.type_comment.split("#", 2)[0].strip()
            err_msg = f'{TYPE_COMMENT_SYNTAX_ERROR} "{stripped_type}"'
            self.fail(err_msg, lineno, n.col_offset)
            if n.type_comment and n.type_comment[0] not in ["(", "#"]:
                self.note(
                    "Suggestion: wrap argument types in parentheses", lineno, n.col_offset
                )
            arg_types = [AnyType(TypeOfAny.from_error)] * len(args)
            return_type = AnyType(TypeOfAny.from_error)
    else:
        arg_types = [a.type_annotation for a in args]
        return_type = TypeConverter(
            self.errors, line=n.returns.lineno if n.returns else lineno
        ).visit(n.returns)

    for arg, arg_type in zip(args, arg_types):
        self.set_type_optional(arg_type, arg.initializer)

    func_type = None
    if any(arg_types) or return_type:
        if len(arg_types) != 1 and any(isinstance(t, EllipsisType) for t in arg_types):
            self.fail(
                "Ellipses cannot accompany other argument types in function type signature",
                lineno,
                n.col_offset,
            )
        elif len(arg_types) &gt; len(arg_kinds):
            self.fail(
                "Type signature has too many arguments", lineno, n.col_offset, blocker=False
            )
        elif len(arg_types) &lt; len(arg_kinds):
            self.fail(
                "Type signature has too few arguments", lineno, n.col_offset, blocker=False
            )
        else:
            func_type = CallableType(
                [a if a is not None else AnyType(TypeOfAny.unannotated) for a in arg_types],
                arg_kinds,
                arg_names,
                return_type if return_type is not None else AnyType(TypeOfAny.unannotated),
                _dummy_fallback,
            )

    # End position is always the same.
    end_line = getattr(n, "end_lineno", None)
    end_column = getattr(n, "end_col_offset", None)

    func_def = FuncDef(n.name, args, self.as_required_block(n.body, lineno), func_type)
    if isinstance(func_def.type, CallableType):
        # semanal.py does some in-place modifications we want to avoid
        func_def.unanalyzed_type = func_def.type.copy_modified()
    if is_coroutine:
        func_def.is_coroutine = True
    if func_type is not None:
        func_type.definition = func_def
        func_type.line = lineno

    if n.decorator_list:
        if sys.version_info &lt; (3, 8):
            # Before 3.8, [typed_]ast the line number points to the first decorator.
            # In 3.8, it points to the 'def' line, where we want it.
            deco_line = lineno
            lineno += len(n.decorator_list)  # this is only approximately true
        else:
            # Set deco_line to the old pre-3.8 lineno, in order to keep
            # existing "# type: ignore" comments working:
            deco_line = n.decorator_list[0].lineno

        var = Var(func_def.name)
        var.is_ready = False
        var.set_line(lineno)

        func_def.is_decorated = True
        func_def.deco_line = deco_line
        func_def.set_line(lineno, n.col_offset, end_line, end_column)
        # Set the line again after we updated it (to make value same in Python 3.7/3.8)
        # Note that TODOs in as_required_block() apply here as well.
        func_def.body.set_line(lineno)

        deco = Decorator(func_def, self.translate_expr_list(n.decorator_list), var)
        first = n.decorator_list[0]
        deco.set_line(first.lineno, first.col_offset, end_line, end_column)
        retval: FuncDef | Decorator = deco
    else:
        # FuncDef overrides set_line -- can't use self.set_line
        func_def.set_line(lineno, n.col_offset, end_line, end_column)
        retval = func_def
    self.class_and_function_stack.pop()
    return retval

</t>
<t tx="ekr.20220525082934.69">def set_type_optional(self, type: Type | None, initializer: Expression | None) -&gt; None:
    if not self.options.implicit_optional:
        return
    # Indicate that type should be wrapped in an Optional if arg is initialized to None.
    optional = isinstance(initializer, NameExpr) and initializer.name == "None"
    if isinstance(type, UnboundType):
        type.optional = optional

</t>
<t tx="ekr.20220525082934.70">def transform_args(
    self, args: ast3.arguments, line: int, no_type_check: bool = False
) -&gt; list[Argument]:
    new_args = []
    names: list[ast3.arg] = []
    posonlyargs = getattr(args, "posonlyargs", cast(List[ast3.arg], []))
    args_args = posonlyargs + args.args
    args_defaults = args.defaults
    num_no_defaults = len(args_args) - len(args_defaults)
    # positional arguments without defaults
    for i, a in enumerate(args_args[:num_no_defaults]):
        pos_only = i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, None, ARG_POS, no_type_check, pos_only))
        names.append(a)

    # positional arguments with defaults
    for i, (a, d) in enumerate(zip(args_args[num_no_defaults:], args_defaults)):
        pos_only = num_no_defaults + i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, d, ARG_OPT, no_type_check, pos_only))
        names.append(a)

    # *arg
    if args.vararg is not None:
        new_args.append(self.make_argument(args.vararg, None, ARG_STAR, no_type_check))
        names.append(args.vararg)

    # keyword-only arguments with defaults
    for a, kd in zip(args.kwonlyargs, args.kw_defaults):
        new_args.append(
            self.make_argument(
                a, kd, ARG_NAMED if kd is None else ARG_NAMED_OPT, no_type_check
            )
        )
        names.append(a)

    # **kwarg
    if args.kwarg is not None:
        new_args.append(self.make_argument(args.kwarg, None, ARG_STAR2, no_type_check))
        names.append(args.kwarg)

    check_arg_names([arg.variable.name for arg in new_args], names, self.fail_arg)

    return new_args

</t>
<t tx="ekr.20220525082934.71">def make_argument(
    self,
    arg: ast3.arg,
    default: ast3.expr | None,
    kind: ArgKind,
    no_type_check: bool,
    pos_only: bool = False,
) -&gt; Argument:
    if no_type_check:
        arg_type = None
    else:
        annotation = arg.annotation
        type_comment = arg.type_comment
        if annotation is not None and type_comment is not None:
            self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, arg.lineno, arg.col_offset)
        arg_type = None
        if annotation is not None:
            arg_type = TypeConverter(self.errors, line=arg.lineno).visit(annotation)
        else:
            arg_type = self.translate_type_comment(arg, type_comment)
    if argument_elide_name(arg.arg):
        pos_only = True

    return Argument(Var(arg.arg), arg_type, self.visit(default), kind, pos_only)

</t>
<t tx="ekr.20220525082934.72">def fail_arg(self, msg: str, arg: ast3.arg) -&gt; None:
    self.fail(msg, arg.lineno, arg.col_offset)

</t>
<t tx="ekr.20220525082934.73"># ClassDef(identifier name,
#  expr* bases,
#  keyword* keywords,
#  stmt* body,
#  expr* decorator_list)
def visit_ClassDef(self, n: ast3.ClassDef) -&gt; ClassDef:
    self.class_and_function_stack.append("C")
    keywords = [(kw.arg, self.visit(kw.value)) for kw in n.keywords if kw.arg]

    cdef = ClassDef(
        n.name,
        self.as_required_block(n.body, n.lineno),
        None,
        self.translate_expr_list(n.bases),
        metaclass=dict(keywords).get("metaclass"),
        keywords=keywords,
    )
    cdef.decorators = self.translate_expr_list(n.decorator_list)
    # Set lines to match the old mypy 0.700 lines, in order to keep
    # existing "# type: ignore" comments working:
    if sys.version_info &lt; (3, 8):
        cdef.line = n.lineno + len(n.decorator_list)
        cdef.deco_line = n.lineno
    else:
        cdef.line = n.lineno
        cdef.deco_line = n.decorator_list[0].lineno if n.decorator_list else None
    cdef.column = n.col_offset
    cdef.end_line = getattr(n, "end_lineno", None)
    cdef.end_column = getattr(n, "end_col_offset", None)
    self.class_and_function_stack.pop()
    return cdef

</t>
<t tx="ekr.20220525082934.74"># Return(expr? value)
def visit_Return(self, n: ast3.Return) -&gt; ReturnStmt:
    node = ReturnStmt(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.75"># Delete(expr* targets)
def visit_Delete(self, n: ast3.Delete) -&gt; DelStmt:
    if len(n.targets) &gt; 1:
        tup = TupleExpr(self.translate_expr_list(n.targets))
        tup.set_line(n.lineno)
        node = DelStmt(tup)
    else:
        node = DelStmt(self.visit(n.targets[0]))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.76"># Assign(expr* targets, expr? value, string? type_comment, expr? annotation)
def visit_Assign(self, n: ast3.Assign) -&gt; AssignmentStmt:
    lvalues = self.translate_expr_list(n.targets)
    rvalue = self.visit(n.value)
    typ = self.translate_type_comment(n, n.type_comment)
    s = AssignmentStmt(lvalues, rvalue, type=typ, new_syntax=False)
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.77"># AnnAssign(expr target, expr annotation, expr? value, int simple)
def visit_AnnAssign(self, n: ast3.AnnAssign) -&gt; AssignmentStmt:
    line = n.lineno
    if n.value is None:  # always allow 'x: int'
        rvalue: Expression = TempNode(AnyType(TypeOfAny.special_form), no_rhs=True)
        rvalue.line = line
        rvalue.column = n.col_offset
    else:
        rvalue = self.visit(n.value)
    typ = TypeConverter(self.errors, line=line).visit(n.annotation)
    assert typ is not None
    typ.column = n.annotation.col_offset
    s = AssignmentStmt([self.visit(n.target)], rvalue, type=typ, new_syntax=True)
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.78"># AugAssign(expr target, operator op, expr value)
def visit_AugAssign(self, n: ast3.AugAssign) -&gt; OperatorAssignmentStmt:
    s = OperatorAssignmentStmt(
        self.from_operator(n.op), self.visit(n.target), self.visit(n.value)
    )
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.79"># For(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_For(self, n: ast3.For) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(
        self.visit(n.target),
        self.visit(n.iter),
        self.as_required_block(n.body, n.lineno),
        self.as_block(n.orelse, n.lineno),
        target_type,
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.80"># AsyncFor(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_AsyncFor(self, n: ast3.AsyncFor) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(
        self.visit(n.target),
        self.visit(n.iter),
        self.as_required_block(n.body, n.lineno),
        self.as_block(n.orelse, n.lineno),
        target_type,
    )
    node.is_async = True
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.81"># While(expr test, stmt* body, stmt* orelse)
def visit_While(self, n: ast3.While) -&gt; WhileStmt:
    node = WhileStmt(
        self.visit(n.test),
        self.as_required_block(n.body, n.lineno),
        self.as_block(n.orelse, n.lineno),
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.82"># If(expr test, stmt* body, stmt* orelse)
def visit_If(self, n: ast3.If) -&gt; IfStmt:
    lineno = n.lineno
    node = IfStmt(
        [self.visit(n.test)],
        [self.as_required_block(n.body, lineno)],
        self.as_block(n.orelse, lineno),
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.83"># With(withitem* items, stmt* body, string? type_comment)
def visit_With(self, n: ast3.With) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = WithStmt(
        [self.visit(i.context_expr) for i in n.items],
        [self.visit(i.optional_vars) for i in n.items],
        self.as_required_block(n.body, n.lineno),
        target_type,
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.84"># AsyncWith(withitem* items, stmt* body, string? type_comment)
def visit_AsyncWith(self, n: ast3.AsyncWith) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    s = WithStmt(
        [self.visit(i.context_expr) for i in n.items],
        [self.visit(i.optional_vars) for i in n.items],
        self.as_required_block(n.body, n.lineno),
        target_type,
    )
    s.is_async = True
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.85"># Raise(expr? exc, expr? cause)
def visit_Raise(self, n: ast3.Raise) -&gt; RaiseStmt:
    node = RaiseStmt(self.visit(n.exc), self.visit(n.cause))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.86"># Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)
def visit_Try(self, n: ast3.Try) -&gt; TryStmt:
    vs = [
        self.set_line(NameExpr(h.name), h) if h.name is not None else None for h in n.handlers
    ]
    types = [self.visit(h.type) for h in n.handlers]
    handlers = [self.as_required_block(h.body, h.lineno) for h in n.handlers]

    node = TryStmt(
        self.as_required_block(n.body, n.lineno),
        vs,
        types,
        handlers,
        self.as_block(n.orelse, n.lineno),
        self.as_block(n.finalbody, n.lineno),
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.87"># Assert(expr test, expr? msg)
def visit_Assert(self, n: ast3.Assert) -&gt; AssertStmt:
    node = AssertStmt(self.visit(n.test), self.visit(n.msg))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.872">class Context:
    """Base type for objects that are valid as error message locations."""

    __slots__ = ("line", "column", "end_line", "end_column")

    @others
</t>
<t tx="ekr.20220525082934.873">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    self.line = line
    self.column = column
    self.end_line: int | None = None
    self.end_column: int | None = None
</t>
<t tx="ekr.20220525082934.874">
def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    """If target is a node, pull line (and column) information
    into this node. If column is specified, this will override any column
    information coming from a node.
    """
    if isinstance(target, int):
        self.line = target
    else:
        self.line = target.line
        self.column = target.column
        self.end_line = target.end_line
        self.end_column = target.end_column

    if column is not None:
        self.column = column

    if end_line is not None:
        self.end_line = end_line

    if end_column is not None:
        self.end_column = end_column

</t>
<t tx="ekr.20220525082934.875">def get_line(self) -&gt; int:
    """Don't use. Use x.line."""
    return self.line

</t>
<t tx="ekr.20220525082934.876">def get_column(self) -&gt; int:
    """Don't use. Use x.column."""
    return self.column


</t>
<t tx="ekr.20220525082934.88"># Import(alias* names)
def visit_Import(self, n: ast3.Import) -&gt; Import:
    names: list[tuple[str, str | None]] = []
    for alias in n.names:
        name = self.translate_module_id(alias.name)
        asname = alias.asname
        if asname is None and name != alias.name:
            # if the module name has been translated (and it's not already
            # an explicit import-as), make it an implicit import-as the
            # original name
            asname = alias.name
        names.append((name, asname))
    i = Import(names)
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20220525082934.880">RUNTIME_PROTOCOL_DECOS: Final = (
    "typing.runtime_checkable",
    "typing_extensions.runtime",
    "typing_extensions.runtime_checkable",
)


class Node(Context):
    """Common base class for all non-type parse tree nodes."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.881">def __str__(self) -&gt; str:
    ans = self.accept(mypy.strconv.StrConv())
    if ans is None:
        return repr(self)
    return ans

</t>
<t tx="ekr.20220525082934.882">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")


</t>
<t tx="ekr.20220525082934.883">@trait
class Statement(Node):
    """A statement node."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.884">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")


</t>
<t tx="ekr.20220525082934.889">@trait
class SymbolNode(Node):
    """Nodes that can be stored in a symbol table."""

    __slots__ = ()

    @others

# Items: fullname, related symbol table node, surrounding type (if any)
Definition: _TypeAlias = Tuple[str, "SymbolTableNode", Optional["TypeInfo"]]


</t>
<t tx="ekr.20220525082934.89"># ImportFrom(identifier? module, alias* names, int? level)
def visit_ImportFrom(self, n: ast3.ImportFrom) -&gt; ImportBase:
    assert n.level is not None
    if len(n.names) == 1 and n.names[0].name == "*":
        mod = n.module if n.module is not None else ""
        i: ImportBase = ImportAll(mod, n.level)
    else:
        i = ImportFrom(
            self.translate_module_id(n.module) if n.module is not None else "",
            n.level,
            [(a.name, a.asname) for a in n.names],
        )
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20220525082934.890">@property
@abstractmethod
def name(self) -&gt; str:
    pass

</t>
<t tx="ekr.20220525082934.891"># fullname can often be None even though the type system
# disagrees. We mark this with Bogus to let mypyc know not to
# worry about it.
@property
@abstractmethod
def fullname(self) -&gt; Bogus[str]:
    pass

</t>
<t tx="ekr.20220525082934.892">@abstractmethod
def serialize(self) -&gt; JsonDict:
    pass

</t>
<t tx="ekr.20220525082934.893">@classmethod
def deserialize(cls, data: JsonDict) -&gt; SymbolNode:
    classname = data[".class"]
    method = deserialize_map.get(classname)
    if method is not None:
        return method(data)
    raise NotImplementedError(f"unexpected .class {classname}")

</t>
<t tx="ekr.20220525082934.895">class MypyFile(SymbolNode):
    """The abstract syntax tree of a single source file."""

    __slots__ = (
        "_fullname",
        "path",
        "defs",
        "alias_deps",
        "is_bom",
        "names",
        "imports",
        "ignored_lines",
        "is_stub",
        "is_cache_skeleton",
        "is_partial_stub_package",
        "plugin_deps",
        "future_import_flags",
    )

    # Fully qualified module name
    _fullname: Bogus[str]
    # Path to the file (empty string if not known)
    path: str
    # Top-level definitions and statements
    defs: list[Statement]
    # Type alias dependencies as mapping from target to set of alias full names
    alias_deps: defaultdict[str, set[str]]
    # Is there a UTF-8 BOM at the start?
    is_bom: bool
    names: SymbolTable
    # All import nodes within the file (also ones within functions etc.)
    imports: list[ImportBase]
    # Lines on which to ignore certain errors when checking.
    # If the value is empty, ignore all errors; otherwise, the list contains all
    # error codes to ignore.
    ignored_lines: dict[int, list[str]]
    # Is this file represented by a stub file (.pyi)?
    is_stub: bool
    # Is this loaded from the cache and thus missing the actual body of the file?
    is_cache_skeleton: bool
    # Does this represent an __init__.pyi stub with a module __getattr__
    # (i.e. a partial stub package), for such packages we suppress any missing
    # module errors in addition to missing attribute errors.
    is_partial_stub_package: bool
    # Plugin-created dependencies
    plugin_deps: dict[str, set[str]]
    # Future imports defined in this file. Populated during semantic analysis.
    future_import_flags: set[str]

    @others
</t>
<t tx="ekr.20220525082934.896">def __init__(
    self,
    defs: list[Statement],
    imports: list[ImportBase],
    is_bom: bool = False,
    ignored_lines: dict[int, list[str]] | None = None,
) -&gt; None:
    super().__init__()
    self.defs = defs
    self.line = 1  # Dummy line number
    self.column = 0  # Dummy column
    self.imports = imports
    self.is_bom = is_bom
    self.alias_deps = defaultdict(set)
    self.plugin_deps = {}
    if ignored_lines:
        self.ignored_lines = ignored_lines
    else:
        self.ignored_lines = {}

    self.path = ""
    self.is_stub = False
    self.is_cache_skeleton = False
    self.is_partial_stub_package = False
    self.future_import_flags = set()

</t>
<t tx="ekr.20220525082934.897">def local_definitions(self) -&gt; Iterator[Definition]:
    """Return all definitions within the module (including nested).

    This doesn't include imported definitions.
    """
    return local_definitions(self.names, self.fullname)

</t>
<t tx="ekr.20220525082934.898">@property
def name(self) -&gt; str:
    return "" if not self._fullname else self._fullname.split(".")[-1]

</t>
<t tx="ekr.20220525082934.899">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20220525082934.90"># Global(identifier* names)
def visit_Global(self, n: ast3.Global) -&gt; GlobalDecl:
    g = GlobalDecl(n.names)
    return self.set_line(g, n)

</t>
<t tx="ekr.20220525082934.900">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_mypy_file(self)

</t>
<t tx="ekr.20220525082934.901">def is_package_init_file(self) -&gt; bool:
    return len(self.path) != 0 and os.path.basename(self.path).startswith("__init__.")

</t>
<t tx="ekr.20220525082934.902">def is_future_flag_set(self, flag: str) -&gt; bool:
    return flag in self.future_import_flags

</t>
<t tx="ekr.20220525082934.903">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "MypyFile",
        "_fullname": self._fullname,
        "names": self.names.serialize(self._fullname),
        "is_stub": self.is_stub,
        "path": self.path,
        "is_partial_stub_package": self.is_partial_stub_package,
        "future_import_flags": list(self.future_import_flags),
    }

</t>
<t tx="ekr.20220525082934.904">@classmethod
def deserialize(cls, data: JsonDict) -&gt; MypyFile:
    assert data[".class"] == "MypyFile", data
    tree = MypyFile([], [])
    tree._fullname = data["_fullname"]
    tree.names = SymbolTable.deserialize(data["names"])
    tree.is_stub = data["is_stub"]
    tree.path = data["path"]
    tree.is_partial_stub_package = data["is_partial_stub_package"]
    tree.is_cache_skeleton = True
    tree.future_import_flags = set(data["future_import_flags"])
    return tree


</t>
<t tx="ekr.20220525082934.91"># Nonlocal(identifier* names)
def visit_Nonlocal(self, n: ast3.Nonlocal) -&gt; NonlocalDecl:
    d = NonlocalDecl(n.names)
    return self.set_line(d, n)

</t>
<t tx="ekr.20220525082934.92"># Expr(expr value)
def visit_Expr(self, n: ast3.Expr) -&gt; ExpressionStmt:
    value = self.visit(n.value)
    node = ExpressionStmt(value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.93"># Pass
def visit_Pass(self, n: ast3.Pass) -&gt; PassStmt:
    s = PassStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.935">class Argument(Node):
    """A single argument in a FuncItem."""

    __slots__ = ("variable", "type_annotation", "initializer", "kind", "pos_only")

    @others
</t>
<t tx="ekr.20220525082934.936">def __init__(
    self,
    variable: Var,
    type_annotation: mypy.types.Type | None,
    initializer: Expression | None,
    kind: ArgKind,
    pos_only: bool = False,
) -&gt; None:
    super().__init__()
    self.variable = variable
    self.type_annotation = type_annotation
    self.initializer = initializer
    self.kind = kind  # must be an ARG_* constant
    self.pos_only = pos_only

</t>
<t tx="ekr.20220525082934.937">def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    super().set_line(target, column, end_line, end_column)

    if self.initializer and self.initializer.line &lt; 0:
        self.initializer.set_line(self.line, self.column, self.end_line, self.end_column)

    self.variable.set_line(self.line, self.column, self.end_line, self.end_column)


</t>
<t tx="ekr.20220525082934.939">FUNCITEM_FLAGS: Final = FUNCBASE_FLAGS + [
    "is_overload",
    "is_generator",
    "is_coroutine",
    "is_async_generator",
    "is_awaitable_coroutine",
]


class FuncItem(FuncBase):
    """Base class for nodes usable as overloaded function items."""

    __slots__ = (
        "arguments",  # Note that can be unset if deserialized (type is a lie!)
        "arg_names",  # Names of arguments
        "arg_kinds",  # Kinds of arguments
        "min_args",  # Minimum number of arguments
        "max_pos",  # Maximum number of positional arguments, -1 if no explicit
        # limit (*args not included)
        "body",  # Body of the function
        "is_overload",  # Is this an overload variant of function with more than
        # one overload variant?
        "is_generator",  # Contains a yield statement?
        "is_coroutine",  # Defined using 'async def' syntax?
        "is_async_generator",  # Is an async def generator?
        "is_awaitable_coroutine",  # Decorated with '@{typing,asyncio}.coroutine'?
        "expanded",  # Variants of function with type variables with values expanded
    )

    __deletable__ = ("arguments", "max_pos", "min_args")
    @others
</t>
<t tx="ekr.20220525082934.94"># Break
def visit_Break(self, n: ast3.Break) -&gt; BreakStmt:
    s = BreakStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.940">
def __init__(
    self,
    arguments: list[Argument] | None = None,
    body: Block | None = None,
    typ: mypy.types.FunctionLike | None = None,
) -&gt; None:
    super().__init__()
    self.arguments = arguments or []
    self.arg_names = [None if arg.pos_only else arg.variable.name for arg in self.arguments]
    self.arg_kinds: list[ArgKind] = [arg.kind for arg in self.arguments]
    self.max_pos: int = self.arg_kinds.count(ARG_POS) + self.arg_kinds.count(ARG_OPT)
    self.body: Block = body or Block([])
    self.type = typ
    self.unanalyzed_type = typ
    self.is_overload: bool = False
    self.is_generator: bool = False
    self.is_coroutine: bool = False
    self.is_async_generator: bool = False
    self.is_awaitable_coroutine: bool = False
    self.expanded: list[FuncItem] = []

    self.min_args = 0
    for i in range(len(self.arguments)):
        if self.arguments[i] is None and i &lt; self.max_fixed_argc():
            self.min_args = i + 1

</t>
<t tx="ekr.20220525082934.941">def max_fixed_argc(self) -&gt; int:
    return self.max_pos

</t>
<t tx="ekr.20220525082934.942">def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    super().set_line(target, column, end_line, end_column)
    for arg in self.arguments:
        # TODO: set arguments line/column to their precise locations.
        arg.set_line(self.line, self.column, self.end_line, end_column)

</t>
<t tx="ekr.20220525082934.943">def is_dynamic(self) -&gt; bool:
    return self.type is None


</t>
<t tx="ekr.20220525082934.945">FUNCDEF_FLAGS: Final = FUNCITEM_FLAGS + [
    "is_decorated",
    "is_conditional",
    "is_trivial_body",
    "is_mypy_only",
]

# Abstract status of a function
NOT_ABSTRACT: Final = 0
# Explicitly abstract (with @abstractmethod or overload without implementation)
IS_ABSTRACT: Final = 1
# Implicitly abstract: used for functions with trivial bodies defined in Protocols
IMPLICITLY_ABSTRACT: Final = 2


class FuncDef(FuncItem, SymbolNode, Statement):
    """Function definition.

    This is a non-lambda function defined using 'def'.
    """

    __slots__ = (
        "_name",
        "is_decorated",
        "is_conditional",
        "abstract_status",
        "original_def",
        "deco_line",
        "is_trivial_body",
        "is_mypy_only",
    )

    @others
# All types that are both SymbolNodes and FuncBases. See the FuncBase
# docstring for the rationale.
SYMBOL_FUNCBASE_TYPES = (OverloadedFuncDef, FuncDef)


</t>
<t tx="ekr.20220525082934.946"># Note that all __init__ args must have default values
def __init__(
    self,
    name: str = "",  # Function name
    arguments: list[Argument] | None = None,
    body: Block | None = None,
    typ: mypy.types.FunctionLike | None = None,
) -&gt; None:
    super().__init__(arguments, body, typ)
    self._name = name
    self.is_decorated = False
    self.is_conditional = False  # Defined conditionally (within block)?
    self.abstract_status = NOT_ABSTRACT
    # Is this an abstract method with trivial body?
    # Such methods can't be called via super().
    self.is_trivial_body = False
    self.is_final = False
    # Original conditional definition
    self.original_def: None | FuncDef | Var | Decorator = None
    # Used for error reporting (to keep backward compatibility with pre-3.8)
    self.deco_line: int | None = None
    # Definitions that appear in if TYPE_CHECKING are marked with this flag.
    self.is_mypy_only = False

</t>
<t tx="ekr.20220525082934.947">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20220525082934.948">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_func_def(self)

</t>
<t tx="ekr.20220525082934.949">def serialize(self) -&gt; JsonDict:
    # We're deliberating omitting arguments and storing only arg_names and
    # arg_kinds for space-saving reasons (arguments is not used in later
    # stages of mypy).
    # TODO: After a FuncDef is deserialized, the only time we use `arg_names`
    # and `arg_kinds` is when `type` is None and we need to infer a type. Can
    # we store the inferred type ahead of time?
    return {
        ".class": "FuncDef",
        "name": self._name,
        "fullname": self._fullname,
        "arg_names": self.arg_names,
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "type": None if self.type is None else self.type.serialize(),
        "flags": get_flags(self, FUNCDEF_FLAGS),
        "abstract_status": self.abstract_status,
        # TODO: Do we need expanded, original_def?
    }

</t>
<t tx="ekr.20220525082934.95"># Continue
def visit_Continue(self, n: ast3.Continue) -&gt; ContinueStmt:
    s = ContinueStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.950">@classmethod
def deserialize(cls, data: JsonDict) -&gt; FuncDef:
    assert data[".class"] == "FuncDef"
    body = Block([])
    ret = FuncDef(
        data["name"],
        [],
        body,
        (
            None
            if data["type"] is None
            else cast(mypy.types.FunctionLike, mypy.types.deserialize_type(data["type"]))
        ),
    )
    ret._fullname = data["fullname"]
    set_flags(ret, data["flags"])
    # NOTE: ret.info is set in the fixup phase.
    ret.arg_names = data["arg_names"]
    ret.arg_kinds = [ArgKind(x) for x in data["arg_kinds"]]
    ret.abstract_status = data["abstract_status"]
    # Leave these uninitialized so that future uses will trigger an error
    del ret.arguments
    del ret.max_pos
    del ret.min_args
    return ret


</t>
<t tx="ekr.20220525082934.96"># --- expr ---

</t>
<t tx="ekr.20220525082934.963">VAR_FLAGS: Final = [
    "is_self",
    "is_initialized_in_class",
    "is_staticmethod",
    "is_classmethod",
    "is_property",
    "is_settable_property",
    "is_suppressed_import",
    "is_classvar",
    "is_abstract_var",
    "is_final",
    "final_unset_in_class",
    "final_set_in_init",
    "explicit_self_type",
    "is_ready",
    "is_inferred",
    "invalid_partial_type",
    "from_module_getattr",
    "has_explicit_value",
    "allow_incompatible_override",
]


class Var(SymbolNode):
    """A variable.

    It can refer to global/local variable or a data attribute.
    """

    __slots__ = (
        "_name",
        "_fullname",
        "info",
        "type",
        "final_value",
        "is_self",
        "is_ready",
        "is_inferred",
        "is_initialized_in_class",
        "is_staticmethod",
        "is_classmethod",
        "is_property",
        "is_settable_property",
        "is_classvar",
        "is_abstract_var",
        "is_final",
        "final_unset_in_class",
        "final_set_in_init",
        "is_suppressed_import",
        "explicit_self_type",
        "from_module_getattr",
        "has_explicit_value",
        "allow_incompatible_override",
        "invalid_partial_type",
    @others
</t>
<t tx="ekr.20220525082934.964">)

def __init__(self, name: str, type: mypy.types.Type | None = None) -&gt; None:
    super().__init__()
    self._name = name  # Name without module prefix
    # TODO: Should be Optional[str]
    self._fullname = cast("Bogus[str]", None)  # Name with module prefix
    # TODO: Should be Optional[TypeInfo]
    self.info = VAR_NO_INFO
    self.type: mypy.types.Type | None = type  # Declared or inferred type, or None
    # Is this the first argument to an ordinary method (usually "self")?
    self.is_self = False
    self.is_ready = True  # If inferred, is the inferred type available?
    self.is_inferred = self.type is None
    # Is this initialized explicitly to a non-None value in class body?
    self.is_initialized_in_class = False
    self.is_staticmethod = False
    self.is_classmethod = False
    self.is_property = False
    self.is_settable_property = False
    self.is_classvar = False
    self.is_abstract_var = False
    # Set to true when this variable refers to a module we were unable to
    # parse for some reason (eg a silenced module)
    self.is_suppressed_import = False
    # Was this "variable" (rather a constant) defined as Final[...]?
    self.is_final = False
    # If constant value is a simple literal,
    # store the literal value (unboxed) for the benefit of
    # tools like mypyc.
    self.final_value: int | float | bool | str | None = None
    # Where the value was set (only for class attributes)
    self.final_unset_in_class = False
    self.final_set_in_init = False
    # This is True for a variable that was declared on self with an explicit type:
    #     class C:
    #         def __init__(self) -&gt; None:
    #             self.x: int
    # This case is important because this defines a new Var, even if there is one
    # present in a superclass (without explicit type this doesn't create a new Var).
    # See SemanticAnalyzer.analyze_member_lvalue() for details.
    self.explicit_self_type = False
    # If True, this is an implicit Var created due to module-level __getattr__.
    self.from_module_getattr = False
    # Var can be created with an explicit value `a = 1` or without one `a: int`,
    # we need a way to tell which one is which.
    self.has_explicit_value = False
    # If True, subclasses can override this with an incompatible type.
    self.allow_incompatible_override = False
    # If True, this means we didn't manage to infer full type and fall back to
    # something like list[Any]. We may decide to not use such types as context.
    self.invalid_partial_type = False

</t>
<t tx="ekr.20220525082934.965">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20220525082934.966">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20220525082934.967">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_var(self)

</t>
<t tx="ekr.20220525082934.968">def serialize(self) -&gt; JsonDict:
    # TODO: Leave default values out?
    # NOTE: Sometimes self.is_ready is False here, but we don't care.
    data: JsonDict = {
        ".class": "Var",
        "name": self._name,
        "fullname": self._fullname,
        "type": None if self.type is None else self.type.serialize(),
        "flags": get_flags(self, VAR_FLAGS),
    }
    if self.final_value is not None:
        data["final_value"] = self.final_value
    return data

</t>
<t tx="ekr.20220525082934.969">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Var:
    assert data[".class"] == "Var"
    name = data["name"]
    type = None if data["type"] is None else mypy.types.deserialize_type(data["type"])
    v = Var(name, type)
    v.is_ready = False  # Override True default set in __init__
    v._fullname = data["fullname"]
    set_flags(v, data["flags"])
    v.final_value = data.get("final_value")
    return v


</t>
<t tx="ekr.20220525082934.97">def visit_NamedExpr(self, n: NamedExpr) -&gt; AssignmentExpr:
    s = AssignmentExpr(self.visit(n.target), self.visit(n.value))
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.98"># BoolOp(boolop op, expr* values)
def visit_BoolOp(self, n: ast3.BoolOp) -&gt; OpExpr:
    # mypy translates (1 and 2 and 3) as (1 and (2 and 3))
    assert len(n.values) &gt;= 2
    op_node = n.op
    if isinstance(op_node, ast3.And):
        op = "and"
    elif isinstance(op_node, ast3.Or):
        op = "or"
    else:
        raise RuntimeError("unknown BoolOp " + str(type(n)))

    # potentially inefficient!
    return self.group(op, self.translate_expr_list(n.values), n)

</t>
<t tx="ekr.20220525082934.99">def group(self, op: str, vals: list[Expression], n: ast3.expr) -&gt; OpExpr:
    if len(vals) == 2:
        e = OpExpr(op, vals[0], vals[1])
    else:
        e = OpExpr(op, vals[0], self.group(op, vals[1:], n))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082935.1">class SemanticAnalyzer(
    NodeVisitor[None], SemanticAnalyzerInterface, SemanticAnalyzerPluginInterface
):
    """Semantically analyze parsed mypy files.

    The analyzer binds names and does various consistency checks for an
    AST. Note that type checking is performed as a separate pass.
    """

    __deletable__ = ["patches", "options", "cur_mod_node"]

    # Module name space
    modules: dict[str, MypyFile]
    # Global name space for current module
    globals: SymbolTable
    # Names declared using "global" (separate set for each scope)
    global_decls: list[set[str]]
    # Names declared using "nonlocal" (separate set for each scope)
    nonlocal_decls: list[set[str]]
    # Local names of function scopes; None for non-function scopes.
    locals: list[SymbolTable | None]
    # Whether each scope is a comprehension scope.
    is_comprehension_stack: list[bool]
    # Nested block depths of scopes
    block_depth: list[int]
    # TypeInfo of directly enclosing class (or None)
    type: TypeInfo | None = None
    # Stack of outer classes (the second tuple item contains tvars).
    type_stack: list[TypeInfo | None]
    # Type variables bound by the current scope, be it class or function
    tvar_scope: TypeVarLikeScope
    # Per-module options
    options: Options

    # Stack of functions being analyzed
    function_stack: list[FuncItem]

    # Set to True if semantic analysis defines a name, or replaces a
    # placeholder definition. If some iteration makes no progress,
    # there can be at most one additional final iteration (see below).
    progress = False
    deferred = False  # Set to true if another analysis pass is needed
    incomplete = False  # Set to true if current module namespace is missing things
    # Is this the final iteration of semantic analysis (where we report
    # unbound names due to cyclic definitions and should not defer)?
    _final_iteration = False
    # These names couldn't be added to the symbol table due to incomplete deps.
    # Note that missing names are per module, _not_ per namespace. This means that e.g.
    # a missing name at global scope will block adding same name at a class scope.
    # This should not affect correctness and is purely a performance issue,
    # since it can cause unnecessary deferrals. These are represented as
    # PlaceholderNodes in the symbol table. We use this to ensure that the first
    # definition takes precedence even if it's incomplete.
    #
    # Note that a star import adds a special name '*' to the set, this blocks
    # adding _any_ names in the current file.
    missing_names: list[set[str]]
    # Callbacks that will be called after semantic analysis to tweak things.
    patches: list[tuple[int, Callable[[], None]]]
    loop_depth = 0  # Depth of breakable loops
    cur_mod_id = ""  # Current module id (or None) (phase 2)
    _is_stub_file = False  # Are we analyzing a stub file?
    _is_typeshed_stub_file = False  # Are we analyzing a typeshed stub file?
    imports: set[str]  # Imported modules (during phase 2 analysis)
    # Note: some imports (and therefore dependencies) might
    # not be found in phase 1, for example due to * imports.
    errors: Errors  # Keeps track of generated errors
    plugin: Plugin  # Mypy plugin for special casing of library features
    statement: Statement | None = None  # Statement/definition being analyzed

    # Mapping from 'async def' function definitions to their return type wrapped as a
    # 'Coroutine[Any, Any, T]'. Used to keep track of whether a function definition's
    # return type has already been wrapped, by checking if the function definition's
    # type is stored in this mapping and that it still matches.
    wrapped_coro_return_types: dict[FuncDef, Type] = {}

    @others
</t>
<t tx="ekr.20220525082935.10">#
# Analyzing a target
#

</t>
<t tx="ekr.20220525082935.100">def disable_invalid_recursive_aliases(
    self, s: AssignmentStmt, current_node: TypeAlias
) -&gt; None:
    """Prohibit and fix recursive type aliases that are invalid/unsupported."""
    messages = []
    if invalid_recursive_alias({current_node}, current_node.target):
        messages.append("Invalid recursive alias: a union item of itself")
    if detect_diverging_alias(
        current_node, current_node.target, self.lookup_qualified, self.tvar_scope
    ):
        messages.append("Invalid recursive alias: type variable nesting on right hand side")
    if messages:
        current_node.target = AnyType(TypeOfAny.from_error)
        s.invalid_recursive_alias = True
    for msg in messages:
        self.fail(msg, s.rvalue)

def analyze_lvalue(
    self,
    lval: Lvalue,
    nested: bool = False,
    explicit_type: bool = False,
    is_final: bool = False,
    escape_comprehensions: bool = False,
    has_explicit_value: bool = False,
) -&gt; None:
    """Analyze an lvalue or assignment target.

    Args:
        lval: The target lvalue
        nested: If true, the lvalue is within a tuple or list lvalue expression
        explicit_type: Assignment has type annotation
        escape_comprehensions: If we are inside a comprehension, set the variable
            in the enclosing scope instead. This implements
            https://www.python.org/dev/peps/pep-0572/#scope-of-the-target
    """
    if escape_comprehensions:
        assert isinstance(lval, NameExpr), "assignment expression target must be NameExpr"
    if isinstance(lval, NameExpr):
        self.analyze_name_lvalue(
            lval,
            explicit_type,
            is_final,
            escape_comprehensions,
            has_explicit_value=has_explicit_value,
        )
    elif isinstance(lval, MemberExpr):
        self.analyze_member_lvalue(lval, explicit_type, is_final)
        if explicit_type and not self.is_self_member_ref(lval):
            self.fail("Type cannot be declared in assignment to non-self attribute", lval)
    elif isinstance(lval, IndexExpr):
        if explicit_type:
            self.fail("Unexpected type declaration", lval)
        lval.accept(self)
    elif isinstance(lval, TupleExpr):
        self.analyze_tuple_or_list_lvalue(lval, explicit_type)
    elif isinstance(lval, StarExpr):
        if nested:
            self.analyze_lvalue(lval.expr, nested, explicit_type)
        else:
            self.fail("Starred assignment target must be in a list or tuple", lval)
    else:
        self.fail("Invalid assignment target", lval)

</t>
<t tx="ekr.20220525082935.101">def analyze_name_lvalue(
    self,
    lvalue: NameExpr,
    explicit_type: bool,
    is_final: bool,
    escape_comprehensions: bool,
    has_explicit_value: bool,
) -&gt; None:
    """Analyze an lvalue that targets a name expression.

    Arguments are similar to "analyze_lvalue".
    """
    if lvalue.node:
        # This has been bound already in a previous iteration.
        return

    name = lvalue.name
    if self.is_alias_for_final_name(name):
        if is_final:
            self.fail("Cannot redefine an existing name as final", lvalue)
        else:
            self.msg.cant_assign_to_final(name, self.type is not None, lvalue)

    kind = self.current_symbol_kind()
    names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
    existing = names.get(name)

    outer = self.is_global_or_nonlocal(name)
    if kind == MDEF and isinstance(self.type, TypeInfo) and self.type.is_enum:
        # Special case: we need to be sure that `Enum` keys are unique.
        if existing is not None and not isinstance(existing.node, PlaceholderNode):
            self.fail(
                'Attempted to reuse member name "{}" in Enum definition "{}"'.format(
                    name, self.type.name
                ),
                lvalue,
            )

    if (not existing or isinstance(existing.node, PlaceholderNode)) and not outer:
        # Define new variable.
        var = self.make_name_lvalue_var(lvalue, kind, not explicit_type, has_explicit_value)
        added = self.add_symbol(name, var, lvalue, escape_comprehensions=escape_comprehensions)
        # Only bind expression if we successfully added name to symbol table.
        if added:
            lvalue.is_new_def = True
            lvalue.is_inferred_def = True
            lvalue.kind = kind
            lvalue.node = var
            if kind == GDEF:
                lvalue.fullname = var._fullname
            else:
                lvalue.fullname = lvalue.name
            if self.is_func_scope():
                if unmangle(name) == "_":
                    # Special case for assignment to local named '_': always infer 'Any'.
                    typ = AnyType(TypeOfAny.special_form)
                    self.store_declared_types(lvalue, typ)
        if is_final and self.is_final_redefinition(kind, name):
            self.fail("Cannot redefine an existing name as final", lvalue)
    else:
        self.make_name_lvalue_point_to_existing_def(lvalue, explicit_type, is_final)

</t>
<t tx="ekr.20220525082935.102">def is_final_redefinition(self, kind: int, name: str) -&gt; bool:
    if kind == GDEF:
        return self.is_mangled_global(name) and not self.is_initial_mangled_global(name)
    elif kind == MDEF and self.type:
        return unmangle(name) + "'" in self.type.names
    return False

</t>
<t tx="ekr.20220525082935.103">def is_alias_for_final_name(self, name: str) -&gt; bool:
    if self.is_func_scope():
        if not name.endswith("'"):
            # Not a mangled name -- can't be an alias
            return False
        name = unmangle(name)
        assert self.locals[-1] is not None, "No locals at function scope"
        existing = self.locals[-1].get(name)
        return existing is not None and is_final_node(existing.node)
    elif self.type is not None:
        orig_name = unmangle(name) + "'"
        if name == orig_name:
            return False
        existing = self.type.names.get(orig_name)
        return existing is not None and is_final_node(existing.node)
    else:
        orig_name = unmangle(name) + "'"
        if name == orig_name:
            return False
        existing = self.globals.get(orig_name)
        return existing is not None and is_final_node(existing.node)

</t>
<t tx="ekr.20220525082935.104">def make_name_lvalue_var(
    self, lvalue: NameExpr, kind: int, inferred: bool, has_explicit_value: bool
) -&gt; Var:
    """Return a Var node for an lvalue that is a name expression."""
    name = lvalue.name
    v = Var(name)
    v.set_line(lvalue)
    v.is_inferred = inferred
    if kind == MDEF:
        assert self.type is not None
        v.info = self.type
        v.is_initialized_in_class = True
        v.allow_incompatible_override = name in ALLOW_INCOMPATIBLE_OVERRIDE
    if kind != LDEF:
        v._fullname = self.qualified_name(name)
    else:
        # fullanme should never stay None
        v._fullname = name
    v.is_ready = False  # Type not inferred yet
    v.has_explicit_value = has_explicit_value
    return v

</t>
<t tx="ekr.20220525082935.105">def make_name_lvalue_point_to_existing_def(
    self, lval: NameExpr, explicit_type: bool, is_final: bool
) -&gt; None:
    """Update an lvalue to point to existing definition in the same scope.

    Arguments are similar to "analyze_lvalue".

    Assume that an existing name exists.
    """
    if is_final:
        # Redefining an existing name with final is always an error.
        self.fail("Cannot redefine an existing name as final", lval)
    original_def = self.lookup(lval.name, lval, suppress_errors=True)
    if original_def is None and self.type and not self.is_func_scope():
        # Workaround to allow "x, x = ..." in class body.
        original_def = self.type.get(lval.name)
    if explicit_type:
        # Don't re-bind if there is a type annotation.
        self.name_already_defined(lval.name, lval, original_def)
    else:
        # Bind to an existing name.
        if original_def:
            self.bind_name_expr(lval, original_def)
        else:
            self.name_not_defined(lval.name, lval)
        self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20220525082935.106">def analyze_tuple_or_list_lvalue(self, lval: TupleExpr, explicit_type: bool = False) -&gt; None:
    """Analyze an lvalue or assignment target that is a list or tuple."""
    items = lval.items
    star_exprs = [item for item in items if isinstance(item, StarExpr)]

    if len(star_exprs) &gt; 1:
        self.fail("Two starred expressions in assignment", lval)
    else:
        if len(star_exprs) == 1:
            star_exprs[0].valid = True
        for i in items:
            self.analyze_lvalue(
                lval=i,
                nested=True,
                explicit_type=explicit_type,
                # Lists and tuples always have explicit values defined:
                # `a, b, c = value`
                has_explicit_value=True,
            )

</t>
<t tx="ekr.20220525082935.107">def analyze_member_lvalue(self, lval: MemberExpr, explicit_type: bool, is_final: bool) -&gt; None:
    """Analyze lvalue that is a member expression.

    Arguments:
        lval: The target lvalue
        explicit_type: Assignment has type annotation
        is_final: Is the target final
    """
    if lval.node:
        # This has been bound already in a previous iteration.
        return
    lval.accept(self)
    if self.is_self_member_ref(lval):
        assert self.type, "Self member outside a class"
        cur_node = self.type.names.get(lval.name)
        node = self.type.get(lval.name)
        if cur_node and is_final:
            # Overrides will be checked in type checker.
            self.fail("Cannot redefine an existing name as final", lval)
        # On first encounter with this definition, if this attribute was defined before
        # with an inferred type and it's marked with an explicit type now, give an error.
        if (
            not lval.node
            and cur_node
            and isinstance(cur_node.node, Var)
            and cur_node.node.is_inferred
            and explicit_type
        ):
            self.attribute_already_defined(lval.name, lval, cur_node)
        # If the attribute of self is not defined in superclasses, create a new Var, ...
        if (
            node is None
            or (isinstance(node.node, Var) and node.node.is_abstract_var)
            # ... also an explicit declaration on self also creates a new Var.
            # Note that `explicit_type` might has been erased for bare `Final`,
            # so we also check if `is_final` is passed.
            or (cur_node is None and (explicit_type or is_final))
        ):
            if self.type.is_protocol and node is None:
                self.fail("Protocol members cannot be defined via assignment to self", lval)
            else:
                # Implicit attribute definition in __init__.
                lval.is_new_def = True
                lval.is_inferred_def = True
                v = Var(lval.name)
                v.set_line(lval)
                v._fullname = self.qualified_name(lval.name)
                v.info = self.type
                v.is_ready = False
                v.explicit_self_type = explicit_type or is_final
                lval.def_var = v
                lval.node = v
                # TODO: should we also set lval.kind = MDEF?
                self.type.names[lval.name] = SymbolTableNode(MDEF, v, implicit=True)
    self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20220525082935.108">def is_self_member_ref(self, memberexpr: MemberExpr) -&gt; bool:
    """Does memberexpr to refer to an attribute of self?"""
    if not isinstance(memberexpr.expr, NameExpr):
        return False
    node = memberexpr.expr.node
    return isinstance(node, Var) and node.is_self

</t>
<t tx="ekr.20220525082935.109">def check_lvalue_validity(self, node: Expression | SymbolNode | None, ctx: Context) -&gt; None:
    if isinstance(node, TypeVarExpr):
        self.fail("Invalid assignment target", ctx)
    elif isinstance(node, TypeInfo):
        self.fail(message_registry.CANNOT_ASSIGN_TO_TYPE, ctx)

</t>
<t tx="ekr.20220525082935.11">def refresh_partial(
    self,
    node: MypyFile | FuncDef | OverloadedFuncDef,
    patches: list[tuple[int, Callable[[], None]]],
    final_iteration: bool,
    file_node: MypyFile,
    options: Options,
    active_type: TypeInfo | None = None,
) -&gt; None:
    """Refresh a stale target in fine-grained incremental mode."""
    self.patches = patches
    self.deferred = False
    self.incomplete = False
    self._final_iteration = final_iteration
    self.missing_names[-1] = set()

    with self.file_context(file_node, options, active_type):
        if isinstance(node, MypyFile):
            self.refresh_top_level(node)
        else:
            self.recurse_into_functions = True
            self.accept(node)
    del self.patches

</t>
<t tx="ekr.20220525082935.110">def store_declared_types(self, lvalue: Lvalue, typ: Type) -&gt; None:
    if isinstance(typ, StarType) and not isinstance(lvalue, StarExpr):
        self.fail("Star type only allowed for starred expressions", lvalue)
    if isinstance(lvalue, RefExpr):
        lvalue.is_inferred_def = False
        if isinstance(lvalue.node, Var):
            var = lvalue.node
            var.type = typ
            var.is_ready = True
        # If node is not a variable, we'll catch it elsewhere.
    elif isinstance(lvalue, TupleExpr):
        typ = get_proper_type(typ)
        if isinstance(typ, TupleType):
            if len(lvalue.items) != len(typ.items):
                self.fail("Incompatible number of tuple items", lvalue)
                return
            for item, itemtype in zip(lvalue.items, typ.items):
                self.store_declared_types(item, itemtype)
        else:
            self.fail("Tuple type expected for multiple variables", lvalue)
    elif isinstance(lvalue, StarExpr):
        # Historical behavior for the old parser
        if isinstance(typ, StarType):
            self.store_declared_types(lvalue.expr, typ.type)
        else:
            self.store_declared_types(lvalue.expr, typ)
    else:
        # This has been flagged elsewhere as an error, so just ignore here.
        pass

</t>
<t tx="ekr.20220525082935.111">def process_typevar_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Check if s declares a TypeVar; it yes, store it in symbol table.

    Return True if this looks like a type variable declaration (but maybe
    with errors), otherwise return False.
    """
    call = self.get_typevarlike_declaration(s, ("typing.TypeVar",))
    if not call:
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # Constraining types
    n_values = call.arg_kinds[1:].count(ARG_POS)
    values = self.analyze_value_types(call.args[1 : 1 + n_values])

    res = self.process_typevar_parameters(
        call.args[1 + n_values :],
        call.arg_names[1 + n_values :],
        call.arg_kinds[1 + n_values :],
        n_values,
        s,
    )
    if res is None:
        return False
    variance, upper_bound = res

    existing = self.current_symbol_table().get(name)
    if existing and not (
        isinstance(existing.node, PlaceholderNode)
        or
        # Also give error for another type variable with the same name.
        (isinstance(existing.node, TypeVarExpr) and existing.node is call.analyzed)
    ):
        self.fail(f'Cannot redefine "{name}" as a type variable', s)
        return False

    if self.options.disallow_any_unimported:
        for idx, constraint in enumerate(values, start=1):
            if has_any_from_unimported_type(constraint):
                prefix = f"Constraint {idx}"
                self.msg.unimported_type_becomes_any(prefix, constraint, s)

        if has_any_from_unimported_type(upper_bound):
            prefix = "Upper bound of type variable"
            self.msg.unimported_type_becomes_any(prefix, upper_bound, s)

    for t in values + [upper_bound]:
        check_for_explicit_any(
            t, self.options, self.is_typeshed_stub_file, self.msg, context=s
        )

    # mypyc suppresses making copies of a function to check each
    # possible type, so set the upper bound to Any to prevent that
    # from causing errors.
    if values and self.options.mypyc:
        upper_bound = AnyType(TypeOfAny.implementation_artifact)

    # Yes, it's a valid type variable definition! Add it to the symbol table.
    if not call.analyzed:
        type_var = TypeVarExpr(name, self.qualified_name(name), values, upper_bound, variance)
        type_var.line = call.line
        call.analyzed = type_var
    else:
        assert isinstance(call.analyzed, TypeVarExpr)
        call.analyzed.upper_bound = upper_bound
        call.analyzed.values = values
    if any(has_placeholder(v) for v in values) or has_placeholder(upper_bound):
        self.defer(force_progress=True)

    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20220525082935.112">def check_typevarlike_name(self, call: CallExpr, name: str, context: Context) -&gt; bool:
    """Checks that the name of a TypeVar or ParamSpec matches its variable."""
    name = unmangle(name)
    assert isinstance(call.callee, RefExpr)
    typevarlike_type = (
        call.callee.name if isinstance(call.callee, NameExpr) else call.callee.fullname
    )
    if len(call.args) &lt; 1:
        self.fail(f"Too few arguments for {typevarlike_type}()", context)
        return False
    if not isinstance(call.args[0], StrExpr) or not call.arg_kinds[0] == ARG_POS:
        self.fail(f"{typevarlike_type}() expects a string literal as first argument", context)
        return False
    elif call.args[0].value != name:
        msg = 'String argument 1 "{}" to {}(...) does not match variable name "{}"'
        self.fail(msg.format(call.args[0].value, typevarlike_type, name), context)
        return False
    return True

</t>
<t tx="ekr.20220525082935.113">def get_typevarlike_declaration(
    self, s: AssignmentStmt, typevarlike_types: tuple[str, ...]
) -&gt; CallExpr | None:
    """Returns the call expression if `s` is a declaration of `typevarlike_type`
    (TypeVar or ParamSpec), or None otherwise.
    """
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], NameExpr):
        return None
    if not isinstance(s.rvalue, CallExpr):
        return None
    call = s.rvalue
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None
    if callee.fullname not in typevarlike_types:
        return None
    return call

</t>
<t tx="ekr.20220525082935.114">def process_typevar_parameters(
    self,
    args: list[Expression],
    names: list[str | None],
    kinds: list[ArgKind],
    num_values: int,
    context: Context,
) -&gt; tuple[int, Type] | None:
    has_values = num_values &gt; 0
    covariant = False
    contravariant = False
    upper_bound: Type = self.object_type()
    for param_value, param_name, param_kind in zip(args, names, kinds):
        if not param_kind.is_named():
            self.fail(message_registry.TYPEVAR_UNEXPECTED_ARGUMENT, context)
            return None
        if param_name == "covariant":
            if isinstance(param_value, NameExpr) and param_value.name in ("True", "False"):
                covariant = param_value.name == "True"
            else:
                self.fail(message_registry.TYPEVAR_VARIANCE_DEF.format("covariant"), context)
                return None
        elif param_name == "contravariant":
            if isinstance(param_value, NameExpr) and param_value.name in ("True", "False"):
                contravariant = param_value.name == "True"
            else:
                self.fail(
                    message_registry.TYPEVAR_VARIANCE_DEF.format("contravariant"), context
                )
                return None
        elif param_name == "bound":
            if has_values:
                self.fail("TypeVar cannot have both values and an upper bound", context)
                return None
            try:
                # We want to use our custom error message below, so we suppress
                # the default error message for invalid types here.
                analyzed = self.expr_to_analyzed_type(
                    param_value, allow_placeholder=True, report_invalid_types=False
                )
                if analyzed is None:
                    # Type variables are special: we need to place them in the symbol table
                    # soon, even if upper bound is not ready yet. Otherwise avoiding
                    # a "deadlock" in this common pattern would be tricky:
                    #     T = TypeVar('T', bound=Custom[Any])
                    #     class Custom(Generic[T]):
                    #         ...
                    analyzed = PlaceholderType(None, [], context.line)
                upper_bound = get_proper_type(analyzed)
                if isinstance(upper_bound, AnyType) and upper_bound.is_from_error:
                    self.fail(message_registry.TYPEVAR_BOUND_MUST_BE_TYPE, param_value)
                    # Note: we do not return 'None' here -- we want to continue
                    # using the AnyType as the upper bound.
            except TypeTranslationError:
                self.fail(message_registry.TYPEVAR_BOUND_MUST_BE_TYPE, param_value)
                return None
        elif param_name == "values":
            # Probably using obsolete syntax with values=(...). Explain the current syntax.
            self.fail('TypeVar "values" argument not supported', context)
            self.fail(
                "Use TypeVar('T', t, ...) instead of TypeVar('T', values=(t, ...))", context
            )
            return None
        else:
            self.fail(
                f'{message_registry.TYPEVAR_UNEXPECTED_ARGUMENT}: "{param_name}"', context
            )
            return None

    if covariant and contravariant:
        self.fail("TypeVar cannot be both covariant and contravariant", context)
        return None
    elif num_values == 1:
        self.fail("TypeVar cannot have only a single constraint", context)
        return None
    elif covariant:
        variance = COVARIANT
    elif contravariant:
        variance = CONTRAVARIANT
    else:
        variance = INVARIANT
    return variance, upper_bound

</t>
<t tx="ekr.20220525082935.115">def extract_typevarlike_name(self, s: AssignmentStmt, call: CallExpr) -&gt; str | None:
    if not call:
        return None

    lvalue = s.lvalues[0]
    assert isinstance(lvalue, NameExpr)
    if s.type:
        self.fail("Cannot declare the type of a TypeVar or similar construct", s)
        return None

    if not self.check_typevarlike_name(call, lvalue.name, s):
        return None
    return lvalue.name

</t>
<t tx="ekr.20220525082935.116">def process_paramspec_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Checks if s declares a ParamSpec; if yes, store it in symbol table.

    Return True if this looks like a ParamSpec (maybe with errors), otherwise return False.

    In the future, ParamSpec may accept bounds and variance arguments, in which
    case more aggressive sharing of code with process_typevar_declaration should be pursued.
    """
    call = self.get_typevarlike_declaration(
        s, ("typing_extensions.ParamSpec", "typing.ParamSpec")
    )
    if not call:
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # ParamSpec is different from a regular TypeVar:
    # arguments are not semantically valid. But, allowed in runtime.
    # So, we need to warn users about possible invalid usage.
    if len(call.args) &gt; 1:
        self.fail("Only the first argument to ParamSpec has defined semantics", s)

    # PEP 612 reserves the right to define bound, covariant and contravariant arguments to
    # ParamSpec in a later PEP. If and when that happens, we should do something
    # on the lines of process_typevar_parameters

    if not call.analyzed:
        paramspec_var = ParamSpecExpr(
            name, self.qualified_name(name), self.object_type(), INVARIANT
        )
        paramspec_var.line = call.line
        call.analyzed = paramspec_var
    else:
        assert isinstance(call.analyzed, ParamSpecExpr)
    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20220525082935.117">def process_typevartuple_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Checks if s declares a TypeVarTuple; if yes, store it in symbol table.

    Return True if this looks like a TypeVarTuple (maybe with errors), otherwise return False.
    """
    call = self.get_typevarlike_declaration(
        s, ("typing_extensions.TypeVarTuple", "typing.TypeVarTuple")
    )
    if not call:
        return False

    if len(call.args) &gt; 1:
        self.fail("Only the first argument to TypeVarTuple has defined semantics", s)

    if not self.options.enable_incomplete_features:
        self.fail('"TypeVarTuple" is not supported by mypy yet', s)
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # PEP 646 does not specify the behavior of variance, constraints, or bounds.
    if not call.analyzed:
        typevartuple_var = TypeVarTupleExpr(
            name, self.qualified_name(name), self.object_type(), INVARIANT
        )
        typevartuple_var.line = call.line
        call.analyzed = typevartuple_var
    else:
        assert isinstance(call.analyzed, TypeVarTupleExpr)
    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20220525082935.1178">class TypeAnalyser(SyntheticTypeVisitor[Type], TypeAnalyzerPluginInterface):
    """Semantic analyzer for types.

    Converts unbound types into bound types. This is a no-op for already
    bound types.

    If an incomplete reference is encountered, this does a defer. The
    caller never needs to defer.
    """

    # Is this called from an untyped function definition?
    in_dynamic_func: bool = False
    # Is this called from global scope?
    global_scope: bool = True

    @others
</t>
<t tx="ekr.20220525082935.1179">def __init__(
    self,
    api: SemanticAnalyzerCoreInterface,
    tvar_scope: TypeVarLikeScope,
    plugin: Plugin,
    options: Options,
    is_typeshed_stub: bool,
    *,
    defining_alias: bool = False,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_placeholder: bool = False,
    allow_required: bool = False,
    allow_param_spec_literals: bool = False,
    report_invalid_types: bool = True,
) -&gt; None:
    self.api = api
    self.lookup_qualified = api.lookup_qualified
    self.lookup_fqn_func = api.lookup_fully_qualified
    self.fail_func = api.fail
    self.note_func = api.note
    self.tvar_scope = tvar_scope
    # Are we analysing a type alias definition rvalue?
    self.defining_alias = defining_alias
    self.allow_tuple_literal = allow_tuple_literal
    # Positive if we are analyzing arguments of another (outer) type
    self.nesting_level = 0
    # Should we allow new type syntax when targeting older Python versions
    # like 'list[int]' or 'X | Y' (allowed in stubs and with `__future__` import)?
    self.always_allow_new_syntax = self.api.is_stub_file or self.api.is_future_flag_set(
        "annotations"
    )
    # Should we accept unbound type variables (always OK in aliases)?
    self.allow_unbound_tvars = allow_unbound_tvars or defining_alias
    # If false, record incomplete ref if we generate PlaceholderType.
    self.allow_placeholder = allow_placeholder
    # Are we in a context where Required[] is allowed?
    self.allow_required = allow_required
    # Are we in a context where ParamSpec literals are allowed?
    self.allow_param_spec_literals = allow_param_spec_literals
    # Should we report an error whenever we encounter a RawExpressionType outside
    # of a Literal context: e.g. whenever we encounter an invalid type? Normally,
    # we want to report an error, but the caller may want to do more specialized
    # error handling.
    self.report_invalid_types = report_invalid_types
    self.plugin = plugin
    self.options = options
    self.is_typeshed_stub = is_typeshed_stub
    # Names of type aliases encountered while analysing a type will be collected here.
    self.aliases_used: set[str] = set()

</t>
<t tx="ekr.20220525082935.118">def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    if self.is_func_scope() and not self.type and "@" not in name:
        name += "@" + str(line)
    class_def = ClassDef(name, Block([]))
    if self.is_func_scope() and not self.type:
        # Full names of generated classes should always be prefixed with the module names
        # even if they are nested in a function, since these classes will be (de-)serialized.
        # (Note that the caller should append @line to the name to avoid collisions.)
        # TODO: clean this up, see #6422.
        class_def.fullname = self.cur_mod_id + "." + self.qualified_name(name)
    else:
        class_def.fullname = self.qualified_name(name)

    info = TypeInfo(SymbolTable(), class_def, self.cur_mod_id)
    class_def.info = info
    mro = basetype_or_fallback.type.mro
    if not mro:
        # Probably an error, we should not crash so generate something meaningful.
        mro = [basetype_or_fallback.type, self.object_type().type]
    info.mro = [info] + mro
    info.bases = [basetype_or_fallback]
    return info

</t>
<t tx="ekr.20220525082935.1180">def visit_unbound_type(self, t: UnboundType, defining_literal: bool = False) -&gt; Type:
    typ = self.visit_unbound_type_nonoptional(t, defining_literal)
    if t.optional:
        # We don't need to worry about double-wrapping Optionals or
        # wrapping Anys: Union simplification will take care of that.
        return make_optional_type(typ)
    return typ

</t>
<t tx="ekr.20220525082935.1181">def visit_unbound_type_nonoptional(self, t: UnboundType, defining_literal: bool) -&gt; Type:
    sym = self.lookup_qualified(t.name, t)
    if sym is not None:
        node = sym.node
        if isinstance(node, PlaceholderNode):
            if node.becomes_typeinfo:
                # Reference to placeholder type.
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                elif self.allow_placeholder:
                    self.api.defer()
                else:
                    self.api.record_incomplete_ref()
                return PlaceholderType(node.fullname, self.anal_array(t.args), t.line)
            else:
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                else:
                    # Reference to an unknown placeholder node.
                    self.api.record_incomplete_ref()
                    return AnyType(TypeOfAny.special_form)
        if node is None:
            self.fail(f"Internal error (node is None, kind={sym.kind})", t)
            return AnyType(TypeOfAny.special_form)
        fullname = node.fullname
        hook = self.plugin.get_type_analyze_hook(fullname)
        if hook is not None:
            return hook(AnalyzeTypeContext(t, t, self))
        if (
            fullname in get_nongen_builtins(self.options.python_version)
            and t.args
            and not self.always_allow_new_syntax
        ):
            self.fail(
                no_subscript_builtin_alias(fullname, propose_alt=not self.defining_alias), t
            )
        tvar_def = self.tvar_scope.get_binding(sym)
        if isinstance(sym.node, ParamSpecExpr):
            if tvar_def is None:
                self.fail(f'ParamSpec "{t.name}" is unbound', t, code=codes.VALID_TYPE)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, ParamSpecType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'ParamSpec "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return ParamSpecType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.flavor,
                tvar_def.upper_bound,
                line=t.line,
                column=t.column,
            )
        if isinstance(sym.node, TypeVarExpr) and tvar_def is not None and self.defining_alias:
            self.fail(
                f'Can\'t use bound type variable "{t.name}" to define generic alias',
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarExpr) and tvar_def is not None:
            assert isinstance(tvar_def, TypeVarType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'Type variable "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return TypeVarType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.values,
                tvar_def.upper_bound,
                tvar_def.variance,
                line=t.line,
                column=t.column,
            )
        if isinstance(sym.node, TypeVarTupleExpr) and (
            tvar_def is not None and self.defining_alias
        ):
            self.fail(
                f'Can\'t use bound type variable "{t.name}" to define generic alias',
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarTupleExpr):
            if tvar_def is None:
                self.fail(f'TypeVarTuple "{t.name}" is unbound', t, code=codes.VALID_TYPE)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, TypeVarTupleType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'Type variable "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return TypeVarTupleType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.upper_bound,
                line=t.line,
                column=t.column,
            )
        special = self.try_analyze_special_unbound_type(t, fullname)
        if special is not None:
            return special
        if isinstance(node, TypeAlias):
            self.aliases_used.add(fullname)
            an_args = self.anal_array(t.args)
            disallow_any = self.options.disallow_any_generics and not self.is_typeshed_stub
            res = expand_type_alias(
                node,
                an_args,
                self.fail,
                node.no_args,
                t,
                unexpanded_type=t,
                disallow_any=disallow_any,
            )
            # The only case where expand_type_alias() can return an incorrect instance is
            # when it is top-level instance, so no need to recurse.
            if (
                isinstance(res, Instance)  # type: ignore[misc]
                and len(res.args) != len(res.type.type_vars)
                and not self.defining_alias
            ):
                fix_instance(
                    res,
                    self.fail,
                    self.note,
                    disallow_any=disallow_any,
                    python_version=self.options.python_version,
                    use_generic_error=True,
                    unexpanded_type=t,
                )
            if node.eager:
                res = get_proper_type(res)
            return res
        elif isinstance(node, TypeInfo):
            return self.analyze_type_with_type_info(node, t.args, t)
        elif node.fullname in TYPE_ALIAS_NAMES:
            return AnyType(TypeOfAny.special_form)
        # Concatenate is an operator, no need for a proper type
        elif node.fullname in ("typing_extensions.Concatenate", "typing.Concatenate"):
            # We check the return type further up the stack for valid use locations
            return self.apply_concatenate_operator(t)
        else:
            return self.analyze_unbound_type_without_type_info(t, sym, defining_literal)
    else:  # sym is None
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082935.1182">def cannot_resolve_type(self, t: UnboundType) -&gt; None:
    # TODO: Move error message generation to messages.py. We'd first
    #       need access to MessageBuilder here. Also move the similar
    #       message generation logic in semanal.py.
    self.api.fail(f'Cannot resolve name "{t.name}" (possible cyclic definition)', t)
    if not self.options.disable_recursive_aliases and self.api.is_func_scope():
        self.note("Recursive types are not allowed at function scope", t)

</t>
<t tx="ekr.20220525082935.1183">def apply_concatenate_operator(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.api.fail("Concatenate needs type arguments", t, code=codes.VALID_TYPE)
        return AnyType(TypeOfAny.from_error)

    # last argument has to be ParamSpec
    ps = self.anal_type(t.args[-1], allow_param_spec=True)
    if not isinstance(ps, ParamSpecType):
        self.api.fail(
            "The last parameter to Concatenate needs to be a ParamSpec",
            t,
            code=codes.VALID_TYPE,
        )
        return AnyType(TypeOfAny.from_error)

    # TODO: this may not work well with aliases, if those worked.
    #   Those should be special-cased.
    elif ps.prefix.arg_types:
        self.api.fail("Nested Concatenates are invalid", t, code=codes.VALID_TYPE)

    args = self.anal_array(t.args[:-1])
    pre = ps.prefix

    # mypy can't infer this :(
    names: list[str | None] = [None] * len(args)

    pre = Parameters(
        args + pre.arg_types, [ARG_POS] * len(args) + pre.arg_kinds, names + pre.arg_names
    )
    return ps.copy_modified(prefix=pre)

</t>
<t tx="ekr.20220525082935.1184">def try_analyze_special_unbound_type(self, t: UnboundType, fullname: str) -&gt; Type | None:
    """Bind special type that is recognized through magic name such as 'typing.Any'.

    Return the bound type if successful, and return None if the type is a normal type.
    """
    if fullname == "builtins.None":
        return NoneType()
    elif fullname == "typing.Any" or fullname == "builtins.Any":
        return AnyType(TypeOfAny.explicit)
    elif fullname in FINAL_TYPE_NAMES:
        self.fail(
            "Final can be only used as an outermost qualifier in a variable annotation",
            t,
            code=codes.VALID_TYPE,
        )
        return AnyType(TypeOfAny.from_error)
    elif fullname == "typing.Tuple" or (
        fullname == "builtins.tuple"
        and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9))
    ):
        # Tuple is special because it is involved in builtin import cycle
        # and may be not ready when used.
        sym = self.api.lookup_fully_qualified_or_none("builtins.tuple")
        if not sym or isinstance(sym.node, PlaceholderNode):
            if self.api.is_incomplete_namespace("builtins"):
                self.api.record_incomplete_ref()
            else:
                self.fail('Name "tuple" is not defined', t)
            return AnyType(TypeOfAny.special_form)
        if len(t.args) == 0 and not t.empty_tuple_index:
            # Bare 'Tuple' is same as 'tuple'
            any_type = self.get_omitted_any(t)
            return self.named_type("builtins.tuple", [any_type], line=t.line, column=t.column)
        if len(t.args) == 2 and isinstance(t.args[1], EllipsisType):
            # Tuple[T, ...] (uniform, variable-length tuple)
            instance = self.named_type("builtins.tuple", [self.anal_type(t.args[0])])
            instance.line = t.line
            return instance
        return self.tuple_type(self.anal_array(t.args))
    elif fullname == "typing.Union":
        items = self.anal_array(t.args)
        return UnionType.make_union(items)
    elif fullname == "typing.Optional":
        if len(t.args) != 1:
            self.fail(
                "Optional[...] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        item = self.anal_type(t.args[0])
        return make_optional_type(item)
    elif fullname == "typing.Callable":
        return self.analyze_callable_type(t)
    elif fullname == "typing.Type" or (
        fullname == "builtins.type"
        and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9))
    ):
        if len(t.args) == 0:
            if fullname == "typing.Type":
                any_type = self.get_omitted_any(t)
                return TypeType(any_type, line=t.line, column=t.column)
            else:
                # To prevent assignment of 'builtins.type' inferred as 'builtins.object'
                # See https://github.com/python/mypy/issues/9476 for more information
                return None
        if len(t.args) != 1:
            type_str = "Type[...]" if fullname == "typing.Type" else "type[...]"
            self.fail(
                type_str + " must have exactly one type argument", t, code=codes.VALID_TYPE
            )
        item = self.anal_type(t.args[0])
        if bad_type_type_item(item):
            self.fail("Type[...] can't contain another Type[...]", t, code=codes.VALID_TYPE)
            item = AnyType(TypeOfAny.from_error)
        return TypeType.make_normalized(item, line=t.line, column=t.column)
    elif fullname == "typing.ClassVar":
        if self.nesting_level &gt; 0:
            self.fail(
                "Invalid type: ClassVar nested inside other type", t, code=codes.VALID_TYPE
            )
        if len(t.args) == 0:
            return AnyType(TypeOfAny.from_omitted_generics, line=t.line, column=t.column)
        if len(t.args) != 1:
            self.fail(
                "ClassVar[...] must have at most one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in NEVER_NAMES:
        return UninhabitedType(is_noreturn=True)
    elif fullname in LITERAL_TYPE_NAMES:
        return self.analyze_literal_type(t)
    elif fullname in ANNOTATED_TYPE_NAMES:
        if len(t.args) &lt; 2:
            self.fail(
                "Annotated[...] must have exactly one type argument"
                " and at least one annotation",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in ("typing_extensions.Required", "typing.Required"):
        if not self.allow_required:
            self.fail(
                "Required[] can be only used in a TypedDict definition",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail(
                "Required[] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=True)
    elif fullname in ("typing_extensions.NotRequired", "typing.NotRequired"):
        if not self.allow_required:
            self.fail(
                "NotRequired[] can be only used in a TypedDict definition",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail(
                "NotRequired[] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=False)
    elif self.anal_type_guard_arg(t, fullname) is not None:
        # In most contexts, TypeGuard[...] acts as an alias for bool (ignoring its args)
        return self.named_type("builtins.bool")
    elif fullname in ("typing.Unpack", "typing_extensions.Unpack"):
        # We don't want people to try to use this yet.
        if not self.options.enable_incomplete_features:
            self.fail('"Unpack" is not supported yet, use --enable-incomplete-features', t)
            return AnyType(TypeOfAny.from_error)
        return UnpackType(self.anal_type(t.args[0]), line=t.line, column=t.column)
    return None

</t>
<t tx="ekr.20220525082935.1185">def get_omitted_any(self, typ: Type, fullname: str | None = None) -&gt; AnyType:
    disallow_any = not self.is_typeshed_stub and self.options.disallow_any_generics
    return get_omitted_any(
        disallow_any, self.fail, self.note, typ, self.options.python_version, fullname
    )

</t>
<t tx="ekr.20220525082935.1186">def analyze_type_with_type_info(
    self, info: TypeInfo, args: Sequence[Type], ctx: Context
) -&gt; Type:
    """Bind unbound type when were able to find target TypeInfo.

    This handles simple cases like 'int', 'modname.UserClass[str]', etc.
    """

    if len(args) &gt; 0 and info.fullname == "builtins.tuple":
        fallback = Instance(info, [AnyType(TypeOfAny.special_form)], ctx.line)
        return TupleType(self.anal_array(args), fallback, ctx.line)

    # This is a heuristic: it will be checked later anyways but the error
    # message may be worse.
    with self.set_allow_param_spec_literals(info.has_param_spec_type):
        # Analyze arguments and (usually) construct Instance type. The
        # number of type arguments and their values are
        # checked only later, since we do not always know the
        # valid count at this point. Thus we may construct an
        # Instance with an invalid number of type arguments.
        instance = Instance(
            info, self.anal_array(args, allow_param_spec=True), ctx.line, ctx.column
        )

    # "aesthetic" paramspec literals
    # these do not support mypy_extensions VarArgs, etc. as they were already analyzed
    #   TODO: should these be re-analyzed to get rid of this inconsistency?
    # another inconsistency is with empty type args (Z[] is more possibly an error imo)
    if len(info.type_vars) == 1 and info.has_param_spec_type and len(instance.args) &gt; 0:
        first_arg = get_proper_type(instance.args[0])

        # TODO: can I use tuple syntax to isinstance multiple in 3.6?
        if not (
            len(instance.args) == 1
            and (
                isinstance(first_arg, Parameters)
                or isinstance(first_arg, ParamSpecType)
                or isinstance(first_arg, AnyType)
            )
        ):
            args = instance.args
            instance.args = (Parameters(args, [ARG_POS] * len(args), [None] * len(args)),)

    if info.has_type_var_tuple_type:
        # - 1 to allow for the empty type var tuple case.
        valid_arg_length = len(instance.args) &gt;= len(info.type_vars) - 1
    else:
        valid_arg_length = len(instance.args) == len(info.type_vars)

    # Check type argument count.
    if not valid_arg_length and not self.defining_alias:
        fix_instance(
            instance,
            self.fail,
            self.note,
            disallow_any=self.options.disallow_any_generics and not self.is_typeshed_stub,
            python_version=self.options.python_version,
        )

    tup = info.tuple_type
    if tup is not None:
        # The class has a Tuple[...] base class so it will be
        # represented as a tuple type.
        if info.special_alias:
            return TypeAliasType(info.special_alias, self.anal_array(args))
        return tup.copy_modified(items=self.anal_array(tup.items), fallback=instance)
    td = info.typeddict_type
    if td is not None:
        # The class has a TypedDict[...] base class so it will be
        # represented as a typeddict type.
        if info.special_alias:
            return TypeAliasType(info.special_alias, self.anal_array(args))
        # Create a named TypedDictType
        return td.copy_modified(
            item_types=self.anal_array(list(td.items.values())), fallback=instance
        )

    if info.fullname == "types.NoneType":
        self.fail(
            "NoneType should not be used as a type, please use None instead",
            ctx,
            code=codes.VALID_TYPE,
        )
        return NoneType(ctx.line, ctx.column)

    return instance

</t>
<t tx="ekr.20220525082935.1187">def analyze_unbound_type_without_type_info(
    self, t: UnboundType, sym: SymbolTableNode, defining_literal: bool
) -&gt; Type:
    """Figure out what an unbound type that doesn't refer to a TypeInfo node means.

    This is something unusual. We try our best to find out what it is.
    """
    name = sym.fullname
    if name is None:
        assert sym.node is not None
        name = sym.node.name
    # Option 1:
    # Something with an Any type -- make it an alias for Any in a type
    # context. This is slightly problematic as it allows using the type 'Any'
    # as a base class -- however, this will fail soon at runtime so the problem
    # is pretty minor.
    if isinstance(sym.node, Var):
        typ = get_proper_type(sym.node.type)
        if isinstance(typ, AnyType):
            return AnyType(
                TypeOfAny.from_unimported_type, missing_import_name=typ.missing_import_name
            )
    # Option 2:
    # Unbound type variable. Currently these may be still valid,
    # for example when defining a generic type alias.
    unbound_tvar = (
        isinstance(sym.node, (TypeVarExpr, TypeVarTupleExpr))
        and self.tvar_scope.get_binding(sym) is None
    )
    if self.allow_unbound_tvars and unbound_tvar:
        return t

    # Option 3:
    # Enum value. Note: we only want to return a LiteralType when
    # we're using this enum value specifically within context of
    # a "Literal[...]" type. So, if `defining_literal` is not set,
    # we bail out early with an error.
    #
    # If, in the distant future, we decide to permit things like
    # `def foo(x: Color.RED) -&gt; None: ...`, we can remove that
    # check entirely.
    if isinstance(sym.node, Var) and sym.node.info and sym.node.info.is_enum:
        value = sym.node.name
        base_enum_short_name = sym.node.info.name
        if not defining_literal:
            msg = message_registry.INVALID_TYPE_RAW_ENUM_VALUE.format(
                base_enum_short_name, value
            )
            self.fail(msg.value, t, code=msg.code)
            return AnyType(TypeOfAny.from_error)
        return LiteralType(
            value=value,
            fallback=Instance(sym.node.info, [], line=t.line, column=t.column),
            line=t.line,
            column=t.column,
        )

    # None of the above options worked. We parse the args (if there are any)
    # to make sure there are no remaining semanal-only types, then give up.
    t = t.copy_modified(args=self.anal_array(t.args))
    # TODO: Move this message building logic to messages.py.
    notes: list[str] = []
    if isinstance(sym.node, Var):
        notes.append(
            "See https://mypy.readthedocs.io/en/"
            "stable/common_issues.html#variables-vs-type-aliases"
        )
        message = 'Variable "{}" is not valid as a type'
    elif isinstance(sym.node, (SYMBOL_FUNCBASE_TYPES, Decorator)):
        message = 'Function "{}" is not valid as a type'
        if name == "builtins.any":
            notes.append('Perhaps you meant "typing.Any" instead of "any"?')
        elif name == "builtins.callable":
            notes.append('Perhaps you meant "typing.Callable" instead of "callable"?')
        else:
            notes.append('Perhaps you need "Callable[...]" or a callback protocol?')
    elif isinstance(sym.node, MypyFile):
        # TODO: suggest a protocol when supported.
        message = 'Module "{}" is not valid as a type'
    elif unbound_tvar:
        message = 'Type variable "{}" is unbound'
        short = name.split(".")[-1]
        notes.append(
            (
                '(Hint: Use "Generic[{}]" or "Protocol[{}]" base class'
                ' to bind "{}" inside a class)'
            ).format(short, short, short)
        )
        notes.append(
            '(Hint: Use "{}" in function signature to bind "{}"'
            " inside a function)".format(short, short)
        )
    else:
        message = 'Cannot interpret reference "{}" as a type'
    self.fail(message.format(name), t, code=codes.VALID_TYPE)
    for note in notes:
        self.note(note, t, code=codes.VALID_TYPE)

    # TODO: Would it be better to always return Any instead of UnboundType
    # in case of an error? On one hand, UnboundType has a name so error messages
    # are more detailed, on the other hand, some of them may be bogus,
    # see https://github.com/python/mypy/issues/4987.
    return t

</t>
<t tx="ekr.20220525082935.1188">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1189">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.119">def analyze_value_types(self, items: list[Expression]) -&gt; list[Type]:
    """Analyze types from values expressions in type variable definition."""
    result: list[Type] = []
    for node in items:
        try:
            analyzed = self.anal_type(
                self.expr_to_unanalyzed_type(node), allow_placeholder=True
            )
            if analyzed is None:
                # Type variables are special: we need to place them in the symbol table
                # soon, even if some value is not ready yet, see process_typevar_parameters()
                # for an example.
                analyzed = PlaceholderType(None, [], node.line)
            result.append(analyzed)
        except TypeTranslationError:
            self.fail("Type expected", node)
            result.append(AnyType(TypeOfAny.from_error))
    return result

</t>
<t tx="ekr.20220525082935.1190">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1191">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    # This type should exist only temporarily during type inference
    assert False, "Internal error: Unexpected erased type"

</t>
<t tx="ekr.20220525082935.1192">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1193">def visit_type_list(self, t: TypeList) -&gt; Type:
    # paramspec literal (Z[[int, str, Whatever]])
    if self.allow_param_spec_literals:
        params = self.analyze_callable_args(t)
        if params:
            ts, kinds, names = params
            # bind these types
            return Parameters(self.anal_array(ts), kinds, names)
        else:
            return AnyType(TypeOfAny.from_error)
    else:
        self.fail(
            'Bracketed expression "[...]" is not valid as a type', t, code=codes.VALID_TYPE
        )
        self.note('Did you mean "List[...]"?', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082935.1194">def visit_callable_argument(self, t: CallableArgument) -&gt; Type:
    self.fail("Invalid type", t, code=codes.VALID_TYPE)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082935.1195">def visit_instance(self, t: Instance) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1196">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # TODO: should we do something here?
    return t

</t>
<t tx="ekr.20220525082935.1197">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1198">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1199">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.12">def refresh_top_level(self, file_node: MypyFile) -&gt; None:
    """Reanalyze a stale module top-level in fine-grained incremental mode."""
    self.recurse_into_functions = False
    self.add_implicit_module_attrs(file_node)
    for d in file_node.defs:
        self.accept(d)
    if file_node.fullname == "typing":
        self.add_builtin_aliases(file_node)
    if file_node.fullname == "typing_extensions":
        self.add_typing_extension_aliases(file_node)
    self.adjust_public_exports()
    self.export_map[self.cur_mod_id] = self.all_exports
    self.all_exports = []

</t>
<t tx="ekr.20220525082935.120">def check_classvar(self, s: AssignmentStmt) -&gt; None:
    """Check if assignment defines a class variable."""
    lvalue = s.lvalues[0]
    if len(s.lvalues) != 1 or not isinstance(lvalue, RefExpr):
        return
    if not s.type or not self.is_classvar(s.type):
        return
    if self.is_class_scope() and isinstance(lvalue, NameExpr):
        node = lvalue.node
        if isinstance(node, Var):
            node.is_classvar = True
        analyzed = self.anal_type(s.type)
        assert self.type is not None
        if analyzed is not None and set(get_type_vars(analyzed)) &amp; set(
            self.type.defn.type_vars
        ):
            # This means that we have a type var defined inside of a ClassVar.
            # This is not allowed by PEP526.
            # See https://github.com/python/mypy/issues/11538

            self.fail(message_registry.CLASS_VAR_WITH_TYPEVARS, s)
    elif not isinstance(lvalue, MemberExpr) or self.is_self_member_ref(lvalue):
        # In case of member access, report error only when assigning to self
        # Other kinds of member assignments should be already reported
        self.fail_invalid_classvar(lvalue)

</t>
<t tx="ekr.20220525082935.1200">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.1201">def visit_parameters(self, t: Parameters) -&gt; Type:
    raise NotImplementedError("ParamSpec literals cannot have unbound TypeVars")

</t>
<t tx="ekr.20220525082935.1202">def visit_callable_type(self, t: CallableType, nested: bool = True) -&gt; Type:
    # Every Callable can bind its own type variables, if they're not in the outer scope
    with self.tvar_scope_frame():
        if self.defining_alias:
            variables = t.variables
        else:
            variables = self.bind_function_type_variables(t, t)
        special = self.anal_type_guard(t.ret_type)
        arg_kinds = t.arg_kinds
        if len(arg_kinds) &gt;= 2 and arg_kinds[-2] == ARG_STAR and arg_kinds[-1] == ARG_STAR2:
            arg_types = self.anal_array(t.arg_types[:-2], nested=nested) + [
                self.anal_star_arg_type(t.arg_types[-2], ARG_STAR, nested=nested),
                self.anal_star_arg_type(t.arg_types[-1], ARG_STAR2, nested=nested),
            ]
        else:
            arg_types = self.anal_array(t.arg_types, nested=nested)
        ret = t.copy_modified(
            arg_types=arg_types,
            ret_type=self.anal_type(t.ret_type, nested=nested),
            # If the fallback isn't filled in yet,
            # its type will be the falsey FakeInfo
            fallback=(t.fallback if t.fallback.type else self.named_type("builtins.function")),
            variables=self.anal_var_defs(variables),
            type_guard=special,
        )
    return ret

</t>
<t tx="ekr.20220525082935.1203">def anal_type_guard(self, t: Type) -&gt; Type | None:
    if isinstance(t, UnboundType):
        sym = self.lookup_qualified(t.name, t)
        if sym is not None and sym.node is not None:
            return self.anal_type_guard_arg(t, sym.node.fullname)
    # TODO: What if it's an Instance? Then use t.type.fullname?
    return None

</t>
<t tx="ekr.20220525082935.1204">def anal_type_guard_arg(self, t: UnboundType, fullname: str) -&gt; Type | None:
    if fullname in ("typing_extensions.TypeGuard", "typing.TypeGuard"):
        if len(t.args) != 1:
            self.fail(
                "TypeGuard must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    return None

</t>
<t tx="ekr.20220525082935.1205">def anal_star_arg_type(self, t: Type, kind: ArgKind, nested: bool) -&gt; Type:
    """Analyze signature argument type for *args and **kwargs argument."""
    if isinstance(t, UnboundType) and t.name and "." in t.name and not t.args:
        components = t.name.split(".")
        tvar_name = ".".join(components[:-1])
        sym = self.lookup_qualified(tvar_name, t)
        if sym is not None and isinstance(sym.node, ParamSpecExpr):
            tvar_def = self.tvar_scope.get_binding(sym)
            if isinstance(tvar_def, ParamSpecType):
                if kind == ARG_STAR:
                    make_paramspec = paramspec_args
                    if components[-1] != "args":
                        self.fail(
                            f'Use "{tvar_name}.args" for variadic "*" parameter',
                            t,
                            code=codes.VALID_TYPE,
                        )
                elif kind == ARG_STAR2:
                    make_paramspec = paramspec_kwargs
                    if components[-1] != "kwargs":
                        self.fail(
                            f'Use "{tvar_name}.kwargs" for variadic "**" parameter',
                            t,
                            code=codes.VALID_TYPE,
                        )
                else:
                    assert False, kind
                return make_paramspec(
                    tvar_def.name,
                    tvar_def.fullname,
                    tvar_def.id,
                    named_type_func=self.named_type,
                    line=t.line,
                    column=t.column,
                )
    return self.anal_type(t, nested=nested)

</t>
<t tx="ekr.20220525082935.1206">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    # Overloaded types are manually constructed in semanal.py by analyzing the
    # AST and combining together the Callable types this visitor converts.
    #
    # So if we're ever asked to reanalyze an Overloaded type, we know it's
    # fine to just return it as-is.
    return t

</t>
<t tx="ekr.20220525082935.1207">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    # Types such as (t1, t2, ...) only allowed in assignment statements. They'll
    # generate errors elsewhere, and Tuple[t1, t2, ...] must be used instead.
    if t.implicit and not self.allow_tuple_literal:
        self.fail("Syntax error in type annotation", t, code=codes.SYNTAX)
        if len(t.items) == 0:
            self.note(
                "Suggestion: Use Tuple[()] instead of () for an empty tuple, or "
                "None for a function without a return value",
                t,
                code=codes.SYNTAX,
            )
        elif len(t.items) == 1:
            self.note("Suggestion: Is there a spurious trailing comma?", t, code=codes.SYNTAX)
        else:
            self.note(
                "Suggestion: Use Tuple[T1, ..., Tn] instead of (T1, ..., Tn)",
                t,
                code=codes.SYNTAX,
            )
        return AnyType(TypeOfAny.from_error)
    star_count = sum(1 for item in t.items if isinstance(item, StarType))
    if star_count &gt; 1:
        self.fail("At most one star type allowed in a tuple", t)
        if t.implicit:
            return TupleType(
                [AnyType(TypeOfAny.from_error) for _ in t.items],
                self.named_type("builtins.tuple"),
                t.line,
            )
        else:
            return AnyType(TypeOfAny.from_error)
    any_type = AnyType(TypeOfAny.special_form)
    # If the fallback isn't filled in yet, its type will be the falsey FakeInfo
    fallback = (
        t.partial_fallback
        if t.partial_fallback.type
        else self.named_type("builtins.tuple", [any_type])
    )
    return TupleType(self.anal_array(t.items), fallback, t.line)

</t>
<t tx="ekr.20220525082935.1208">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = {
        item_name: self.anal_type(item_type) for (item_name, item_type) in t.items.items()
    }
    return TypedDictType(items, set(t.required_keys), t.fallback)

</t>
<t tx="ekr.20220525082935.1209">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; Type:
    # We should never see a bare Literal. We synthesize these raw literals
    # in the earlier stages of semantic analysis, but those
    # "fake literals" should always be wrapped in an UnboundType
    # corresponding to 'Literal'.
    #
    # Note: if at some point in the distant future, we decide to
    # make signatures like "foo(x: 20) -&gt; None" legal, we can change
    # this method so it generates and returns an actual LiteralType
    # instead.

    if self.report_invalid_types:
        if t.base_type_name in ("builtins.int", "builtins.bool"):
            # The only time it makes sense to use an int or bool is inside of
            # a literal type.
            msg = f"Invalid type: try using Literal[{repr(t.literal_value)}] instead?"
        elif t.base_type_name in ("builtins.float", "builtins.complex"):
            # We special-case warnings for floats and complex numbers.
            msg = f"Invalid type: {t.simple_name()} literals cannot be used as a type"
        else:
            # And in all other cases, we default to a generic error message.
            # Note: the reason why we use a generic error message for strings
            # but not ints or bools is because whenever we see an out-of-place
            # string, it's unclear if the user meant to construct a literal type
            # or just misspelled a regular type. So we avoid guessing.
            msg = "Invalid type comment or annotation"

        self.fail(msg, t, code=codes.VALID_TYPE)
        if t.note is not None:
            self.note(t.note, t, code=codes.VALID_TYPE)

    return AnyType(TypeOfAny.from_error, line=t.line, column=t.column)

</t>
<t tx="ekr.20220525082935.121">def is_classvar(self, typ: Type) -&gt; bool:
    if not isinstance(typ, UnboundType):
        return False
    sym = self.lookup_qualified(typ.name, typ)
    if not sym or not sym.node:
        return False
    return sym.node.fullname == "typing.ClassVar"

</t>
<t tx="ekr.20220525082935.1210">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1211">def visit_star_type(self, t: StarType) -&gt; Type:
    return StarType(self.anal_type(t.type), t.line)

</t>
<t tx="ekr.20220525082935.1212">def visit_union_type(self, t: UnionType) -&gt; Type:
    if (
        t.uses_pep604_syntax is True
        and t.is_evaluated is True
        and not self.always_allow_new_syntax
        and not self.options.python_version &gt;= (3, 10)
    ):
        self.fail("X | Y syntax for unions requires Python 3.10", t, code=codes.SYNTAX)
    return UnionType(self.anal_array(t.items), t.line)

</t>
<t tx="ekr.20220525082935.1213">def visit_partial_type(self, t: PartialType) -&gt; Type:
    assert False, "Internal error: Unexpected partial type"

</t>
<t tx="ekr.20220525082935.1214">def visit_ellipsis_type(self, t: EllipsisType) -&gt; Type:
    if self.allow_param_spec_literals:
        any_type = AnyType(TypeOfAny.explicit)
        return Parameters(
            [any_type, any_type], [ARG_STAR, ARG_STAR2], [None, None], is_ellipsis_args=True
        )
    else:
        self.fail('Unexpected "..."', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082935.1215">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(self.anal_type(t.item), line=t.line)

</t>
<t tx="ekr.20220525082935.1216">def visit_placeholder_type(self, t: PlaceholderType) -&gt; Type:
    n = None if t.fullname is None else self.api.lookup_fully_qualified(t.fullname)
    if not n or isinstance(n.node, PlaceholderNode):
        self.api.defer()  # Still incomplete
        return t
    else:
        # TODO: Handle non-TypeInfo
        assert isinstance(n.node, TypeInfo)
        return self.analyze_type_with_type_info(n.node, t.args, t)

</t>
<t tx="ekr.20220525082935.1217">def analyze_callable_args_for_paramspec(
    self, callable_args: Type, ret_type: Type, fallback: Instance
) -&gt; CallableType | None:
    """Construct a 'Callable[P, RET]', where P is ParamSpec, return None if we cannot."""
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    tvar_def = self.tvar_scope.get_binding(sym)
    if not isinstance(tvar_def, ParamSpecType):
        return None

    return CallableType(
        [
            paramspec_args(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
            paramspec_kwargs(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
        ],
        [nodes.ARG_STAR, nodes.ARG_STAR2],
        [None, None],
        ret_type=ret_type,
        fallback=fallback,
    )

</t>
<t tx="ekr.20220525082935.1218">def analyze_callable_args_for_concatenate(
    self, callable_args: Type, ret_type: Type, fallback: Instance
) -&gt; CallableType | None:
    """Construct a 'Callable[C, RET]', where C is Concatenate[..., P], returning None if we
    cannot.
    """
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    if sym.node is None:
        return None
    if sym.node.fullname not in ("typing_extensions.Concatenate", "typing.Concatenate"):
        return None

    tvar_def = self.anal_type(callable_args, allow_param_spec=True)
    if not isinstance(tvar_def, ParamSpecType):
        return None

    # ick, CallableType should take ParamSpecType
    prefix = tvar_def.prefix
    # we don't set the prefix here as generic arguments will get updated at some point
    # in the future. CallableType.param_spec() accounts for this.
    return CallableType(
        [
            *prefix.arg_types,
            paramspec_args(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
            paramspec_kwargs(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
        ],
        [*prefix.arg_kinds, nodes.ARG_STAR, nodes.ARG_STAR2],
        [*prefix.arg_names, None, None],
        ret_type=ret_type,
        fallback=fallback,
        from_concatenate=True,
    )

</t>
<t tx="ekr.20220525082935.1219">def analyze_callable_type(self, t: UnboundType) -&gt; Type:
    fallback = self.named_type("builtins.function")
    if len(t.args) == 0:
        # Callable (bare). Treat as Callable[..., Any].
        any_type = self.get_omitted_any(t)
        ret = callable_with_ellipsis(any_type, any_type, fallback)
    elif len(t.args) == 2:
        callable_args = t.args[0]
        ret_type = t.args[1]
        if isinstance(callable_args, TypeList):
            # Callable[[ARG, ...], RET] (ordinary callable type)
            analyzed_args = self.analyze_callable_args(callable_args)
            if analyzed_args is None:
                return AnyType(TypeOfAny.from_error)
            args, kinds, names = analyzed_args
            ret = CallableType(args, kinds, names, ret_type=ret_type, fallback=fallback)
        elif isinstance(callable_args, EllipsisType):
            # Callable[..., RET] (with literal ellipsis; accept arbitrary arguments)
            ret = callable_with_ellipsis(
                AnyType(TypeOfAny.explicit), ret_type=ret_type, fallback=fallback
            )
        else:
            # Callable[P, RET] (where P is ParamSpec)
            maybe_ret = self.analyze_callable_args_for_paramspec(
                callable_args, ret_type, fallback
            ) or self.analyze_callable_args_for_concatenate(callable_args, ret_type, fallback)
            if maybe_ret is None:
                # Callable[?, RET] (where ? is something invalid)
                self.fail(
                    "The first argument to Callable must be a "
                    'list of types, parameter specification, or "..."',
                    t,
                    code=codes.VALID_TYPE,
                )
                self.note(
                    "See https://mypy.readthedocs.io/en/stable/kinds_of_types.html#callable-types-and-lambdas",  # noqa: E501
                    t,
                )
                return AnyType(TypeOfAny.from_error)
            ret = maybe_ret
    else:
        if self.options.disallow_any_generics:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]"', t)
        else:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]" or "Callable"', t)
        return AnyType(TypeOfAny.from_error)
    assert isinstance(ret, CallableType)
    return ret.accept(self)

</t>
<t tx="ekr.20220525082935.122">def is_final_type(self, typ: Type | None) -&gt; bool:
    if not isinstance(typ, UnboundType):
        return False
    sym = self.lookup_qualified(typ.name, typ)
    if not sym or not sym.node:
        return False
    return sym.node.fullname in FINAL_TYPE_NAMES

</t>
<t tx="ekr.20220525082935.1220">def analyze_callable_args(
    self, arglist: TypeList
) -&gt; tuple[list[Type], list[ArgKind], list[str | None]] | None:
    args: list[Type] = []
    kinds: list[ArgKind] = []
    names: list[str | None] = []
    for arg in arglist.items:
        if isinstance(arg, CallableArgument):
            args.append(arg.typ)
            names.append(arg.name)
            if arg.constructor is None:
                return None
            found = self.lookup_qualified(arg.constructor, arg)
            if found is None:
                # Looking it up already put an error message in
                return None
            elif found.fullname not in ARG_KINDS_BY_CONSTRUCTOR:
                self.fail(f'Invalid argument constructor "{found.fullname}"', arg)
                return None
            else:
                assert found.fullname is not None
                kind = ARG_KINDS_BY_CONSTRUCTOR[found.fullname]
                kinds.append(kind)
                if arg.name is not None and kind.is_star():
                    self.fail(f"{arg.constructor} arguments should not have names", arg)
                    return None
        else:
            args.append(arg)
            kinds.append(ARG_POS)
            names.append(None)
    # Note that arglist below is only used for error context.
    check_arg_names(names, [arglist] * len(args), self.fail, "Callable")
    check_arg_kinds(kinds, [arglist] * len(args), self.fail)
    return args, kinds, names

</t>
<t tx="ekr.20220525082935.1221">def analyze_literal_type(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.fail("Literal[...] must have at least one parameter", t, code=codes.VALID_TYPE)
        return AnyType(TypeOfAny.from_error)

    output: list[Type] = []
    for i, arg in enumerate(t.args):
        analyzed_types = self.analyze_literal_param(i + 1, arg, t)
        if analyzed_types is None:
            return AnyType(TypeOfAny.from_error)
        else:
            output.extend(analyzed_types)
    return UnionType.make_union(output, line=t.line)

</t>
<t tx="ekr.20220525082935.1222">def analyze_literal_param(self, idx: int, arg: Type, ctx: Context) -&gt; list[Type] | None:
    # This UnboundType was originally defined as a string.
    if isinstance(arg, UnboundType) and arg.original_str_expr is not None:
        assert arg.original_str_fallback is not None
        return [
            LiteralType(
                value=arg.original_str_expr,
                fallback=self.named_type(arg.original_str_fallback),
                line=arg.line,
                column=arg.column,
            )
        ]

    # If arg is an UnboundType that was *not* originally defined as
    # a string, try expanding it in case it's a type alias or something.
    if isinstance(arg, UnboundType):
        self.nesting_level += 1
        try:
            arg = self.visit_unbound_type(arg, defining_literal=True)
        finally:
            self.nesting_level -= 1

    # Literal[...] cannot contain Any. Give up and add an error message
    # (if we haven't already).
    arg = get_proper_type(arg)
    if isinstance(arg, AnyType):
        # Note: We can encounter Literals containing 'Any' under three circumstances:
        #
        # 1. If the user attempts use an explicit Any as a parameter
        # 2. If the user is trying to use an enum value imported from a module with
        #    no type hints, giving it an implicit type of 'Any'
        # 3. If there's some other underlying problem with the parameter.
        #
        # We report an error in only the first two cases. In the third case, we assume
        # some other region of the code has already reported a more relevant error.
        #
        # TODO: Once we start adding support for enums, make sure we report a custom
        # error for case 2 as well.
        if arg.type_of_any not in (TypeOfAny.from_error, TypeOfAny.special_form):
            self.fail(
                f'Parameter {idx} of Literal[...] cannot be of type "Any"',
                ctx,
                code=codes.VALID_TYPE,
            )
        return None
    elif isinstance(arg, RawExpressionType):
        # A raw literal. Convert it directly into a literal if we can.
        if arg.literal_value is None:
            name = arg.simple_name()
            if name in ("float", "complex"):
                msg = f'Parameter {idx} of Literal[...] cannot be of type "{name}"'
            else:
                msg = "Invalid type: Literal[...] cannot contain arbitrary expressions"
            self.fail(msg, ctx, code=codes.VALID_TYPE)
            # Note: we deliberately ignore arg.note here: the extra info might normally be
            # helpful, but it generally won't make sense in the context of a Literal[...].
            return None

        # Remap bytes and unicode into the appropriate type for the correct Python version
        fallback = self.named_type(arg.base_type_name)
        assert isinstance(fallback, Instance)
        return [LiteralType(arg.literal_value, fallback, line=arg.line, column=arg.column)]
    elif isinstance(arg, (NoneType, LiteralType)):
        # Types that we can just add directly to the literal/potential union of literals.
        return [arg]
    elif isinstance(arg, Instance) and arg.last_known_value is not None:
        # Types generated from declarations like "var: Final = 4".
        return [arg.last_known_value]
    elif isinstance(arg, UnionType):
        out = []
        for union_arg in arg.items:
            union_result = self.analyze_literal_param(idx, union_arg, ctx)
            if union_result is None:
                return None
            out.extend(union_result)
        return out
    else:
        self.fail(f"Parameter {idx} of Literal[...] is invalid", ctx, code=codes.VALID_TYPE)
        return None

</t>
<t tx="ekr.20220525082935.1223">def analyze_type(self, t: Type) -&gt; Type:
    return t.accept(self)

</t>
<t tx="ekr.20220525082935.1224">def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.fail_func(msg, ctx, code=code)

</t>
<t tx="ekr.20220525082935.1225">def note(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.note_func(msg, ctx, code=code)

</t>
<t tx="ekr.20220525082935.1226">@contextmanager
def tvar_scope_frame(self) -&gt; Iterator[None]:
    old_scope = self.tvar_scope
    self.tvar_scope = self.tvar_scope.method_frame()
    yield
    self.tvar_scope = old_scope

</t>
<t tx="ekr.20220525082935.1227">def infer_type_variables(self, type: CallableType) -&gt; list[tuple[str, TypeVarLikeExpr]]:
    """Return list of unique type variables referred to in a callable."""
    names: list[str] = []
    tvars: list[TypeVarLikeExpr] = []
    for arg in type.arg_types:
        for name, tvar_expr in arg.accept(
            TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope)
        ):
            if name not in names:
                names.append(name)
                tvars.append(tvar_expr)
    # When finding type variables in the return type of a function, don't
    # look inside Callable types.  Type variables only appearing in
    # functions in the return type belong to those functions, not the
    # function we're currently analyzing.
    for name, tvar_expr in type.ret_type.accept(
        TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope, include_callables=False)
    ):
        if name not in names:
            names.append(name)
            tvars.append(tvar_expr)
    return list(zip(names, tvars))

</t>
<t tx="ekr.20220525082935.1228">def bind_function_type_variables(
    self, fun_type: CallableType, defn: Context
) -&gt; Sequence[TypeVarLikeType]:
    """Find the type variables of the function type and bind them in our tvar_scope"""
    if fun_type.variables:
        defs = []
        for var in fun_type.variables:
            var_node = self.lookup_qualified(var.name, defn)
            assert var_node, "Binding for function type variable not found within function"
            var_expr = var_node.node
            assert isinstance(var_expr, TypeVarLikeExpr)
            binding = self.tvar_scope.bind_new(var.name, var_expr)
            defs.append(binding)
        return defs
    typevars = self.infer_type_variables(fun_type)
    # Do not define a new type variable if already defined in scope.
    typevars = [
        (name, tvar) for name, tvar in typevars if not self.is_defined_type_var(name, defn)
    ]
    defs = []
    for name, tvar in typevars:
        if not self.tvar_scope.allow_binding(tvar.fullname):
            self.fail(
                f'Type variable "{name}" is bound by an outer class',
                defn,
                code=codes.VALID_TYPE,
            )
        binding = self.tvar_scope.bind_new(name, tvar)
        defs.append(binding)

    return defs

</t>
<t tx="ekr.20220525082935.1229">def is_defined_type_var(self, tvar: str, context: Context) -&gt; bool:
    tvar_node = self.lookup_qualified(tvar, context)
    if not tvar_node:
        return False
    return self.tvar_scope.get_binding(tvar_node) is not None

</t>
<t tx="ekr.20220525082935.123">def fail_invalid_classvar(self, context: Context) -&gt; None:
    self.fail(message_registry.CLASS_VAR_OUTSIDE_OF_CLASS, context)

</t>
<t tx="ekr.20220525082935.1230">def anal_array(
    self, a: Iterable[Type], nested: bool = True, *, allow_param_spec: bool = False
) -&gt; list[Type]:
    res: list[Type] = []
    for t in a:
        res.append(self.anal_type(t, nested, allow_param_spec=allow_param_spec))
    return res

</t>
<t tx="ekr.20220525082935.1231">def anal_type(self, t: Type, nested: bool = True, *, allow_param_spec: bool = False) -&gt; Type:
    if nested:
        self.nesting_level += 1
    old_allow_required = self.allow_required
    self.allow_required = False
    try:
        analyzed = t.accept(self)
    finally:
        if nested:
            self.nesting_level -= 1
        self.allow_required = old_allow_required
    if (
        not allow_param_spec
        and isinstance(analyzed, ParamSpecType)
        and analyzed.flavor == ParamSpecFlavor.BARE
    ):
        if analyzed.prefix.arg_types:
            self.fail("Invalid location for Concatenate", t, code=codes.VALID_TYPE)
            self.note("You can use Concatenate as the first argument to Callable", t)
        else:
            self.fail(
                f'Invalid location for ParamSpec "{analyzed.name}"', t, code=codes.VALID_TYPE
            )
            self.note(
                "You can use ParamSpec as the first argument to Callable, e.g., "
                "'Callable[{}, int]'".format(analyzed.name),
                t,
            )
    return analyzed

</t>
<t tx="ekr.20220525082935.1232">def anal_var_def(self, var_def: TypeVarLikeType) -&gt; TypeVarLikeType:
    if isinstance(var_def, TypeVarType):
        return TypeVarType(
            var_def.name,
            var_def.fullname,
            var_def.id.raw_id,
            self.anal_array(var_def.values),
            var_def.upper_bound.accept(self),
            var_def.variance,
            var_def.line,
        )
    else:
        return var_def

</t>
<t tx="ekr.20220525082935.1233">def anal_var_defs(self, var_defs: Sequence[TypeVarLikeType]) -&gt; list[TypeVarLikeType]:
    return [self.anal_var_def(vd) for vd in var_defs]

</t>
<t tx="ekr.20220525082935.1234">def named_type(
    self,
    fully_qualified_name: str,
    args: list[Type] | None = None,
    line: int = -1,
    column: int = -1,
) -&gt; Instance:
</t>
<t tx="ekr.20220525082935.1235">    node = self.lookup_fqn_func(fully_qualified_name)
    assert isinstance(node.node, TypeInfo)
    any_type = AnyType(TypeOfAny.special_form)
    return Instance(
        node.node, args or [any_type] * len(node.node.defn.type_vars), line=line, column=column
    )
</t>
<t tx="ekr.20220525082935.1236">
def tuple_type(self, items: list[Type]) -&gt; TupleType:
    any_type = AnyType(TypeOfAny.special_form)
    return TupleType(items, fallback=self.named_type("builtins.tuple", [any_type]))

</t>
<t tx="ekr.20220525082935.1237">@contextmanager
def set_allow_param_spec_literals(self, to: bool) -&gt; Iterator[None]:
    old = self.allow_param_spec_literals
    try:
        self.allow_param_spec_literals = to
        yield
    finally:
        self.allow_param_spec_literals = old


</t>
<t tx="ekr.20220525082935.124">def process_module_assignment(
    self, lvals: list[Lvalue], rval: Expression, ctx: AssignmentStmt
) -&gt; None:
    """Propagate module references across assignments.

    Recursively handles the simple form of iterable unpacking; doesn't
    handle advanced unpacking with *rest, dictionary unpacking, etc.

    In an expression like x = y = z, z is the rval and lvals will be [x,
    y].

    """
    if isinstance(rval, (TupleExpr, ListExpr)) and all(
        isinstance(v, TupleExpr) for v in lvals
    ):
        # rval and all lvals are either list or tuple, so we are dealing
        # with unpacking assignment like `x, y = a, b`. Mypy didn't
        # understand our all(isinstance(...)), so cast them as TupleExpr
        # so mypy knows it is safe to access their .items attribute.
        seq_lvals = cast(List[TupleExpr], lvals)
        # given an assignment like:
        #     (x, y) = (m, n) = (a, b)
        # we now have:
        #     seq_lvals = [(x, y), (m, n)]
        #     seq_rval = (a, b)
        # We now zip this into:
        #     elementwise_assignments = [(a, x, m), (b, y, n)]
        # where each elementwise assignment includes one element of rval and the
        # corresponding element of each lval. Basically we unpack
        #     (x, y) = (m, n) = (a, b)
        # into elementwise assignments
        #     x = m = a
        #     y = n = b
        # and then we recursively call this method for each of those assignments.
        # If the rval and all lvals are not all of the same length, zip will just ignore
        # extra elements, so no error will be raised here; mypy will later complain
        # about the length mismatch in type-checking.
        elementwise_assignments = zip(rval.items, *[v.items for v in seq_lvals])
        for rv, *lvs in elementwise_assignments:
            self.process_module_assignment(lvs, rv, ctx)
    elif isinstance(rval, RefExpr):
        rnode = self.lookup_type_node(rval)
        if rnode and isinstance(rnode.node, MypyFile):
            for lval in lvals:
                if not isinstance(lval, RefExpr):
                    continue
                # respect explicitly annotated type
                if isinstance(lval.node, Var) and lval.node.type is not None:
                    continue

                # We can handle these assignments to locals and to self
                if isinstance(lval, NameExpr):
                    lnode = self.current_symbol_table().get(lval.name)
                elif isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                    assert self.type is not None
                    lnode = self.type.names.get(lval.name)
                else:
                    continue

                if lnode:
                    if isinstance(lnode.node, MypyFile) and lnode.node is not rnode.node:
                        assert isinstance(lval, (NameExpr, MemberExpr))
                        self.fail(
                            'Cannot assign multiple modules to name "{}" '
                            'without explicit "types.ModuleType" annotation'.format(lval.name),
                            ctx,
                        )
                    # never create module alias except on initial var definition
                    elif lval.is_inferred_def:
                        assert rnode.node is not None
                        lnode.node = rnode.node

</t>
<t tx="ekr.20220525082935.125">def process__all__(self, s: AssignmentStmt) -&gt; None:
    """Export names if argument is a __all__ assignment."""
    if (
        len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and s.lvalues[0].name == "__all__"
        and s.lvalues[0].kind == GDEF
        and isinstance(s.rvalue, (ListExpr, TupleExpr))
    ):
        self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20220525082935.126">def process__deletable__(self, s: AssignmentStmt) -&gt; None:
    if not self.options.mypyc:
        return
    if (
        len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and s.lvalues[0].name == "__deletable__"
        and s.lvalues[0].kind == MDEF
    ):
        rvalue = s.rvalue
        if not isinstance(rvalue, (ListExpr, TupleExpr)):
            self.fail('"__deletable__" must be initialized with a list or tuple expression', s)
            return
        items = rvalue.items
        attrs = []
        for item in items:
            if not isinstance(item, StrExpr):
                self.fail('Invalid "__deletable__" item; string literal expected', item)
            else:
                attrs.append(item.value)
        assert self.type
        self.type.deletable_attributes = attrs

</t>
<t tx="ekr.20220525082935.127">def process__slots__(self, s: AssignmentStmt) -&gt; None:
    """
    Processing ``__slots__`` if defined in type.

    See: https://docs.python.org/3/reference/datamodel.html#slots
    """
    # Later we can support `__slots__` defined as `__slots__ = other = ('a', 'b')`
    if (
        isinstance(self.type, TypeInfo)
        and len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and s.lvalues[0].name == "__slots__"
        and s.lvalues[0].kind == MDEF
    ):

        # We understand `__slots__` defined as string, tuple, list, set, and dict:
        if not isinstance(s.rvalue, (StrExpr, ListExpr, TupleExpr, SetExpr, DictExpr)):
            # For example, `__slots__` can be defined as a variable,
            # we don't support it for now.
            return

        if any(p.slots is None for p in self.type.mro[1:-1]):
            # At least one type in mro (excluding `self` and `object`)
            # does not have concrete `__slots__` defined. Ignoring.
            return

        concrete_slots = True
        rvalue: list[Expression] = []
        if isinstance(s.rvalue, StrExpr):
            rvalue.append(s.rvalue)
        elif isinstance(s.rvalue, (ListExpr, TupleExpr, SetExpr)):
            rvalue.extend(s.rvalue.items)
        else:
            # We have a special treatment of `dict` with possible `{**kwargs}` usage.
            # In this case we consider all `__slots__` to be non-concrete.
            for key, _ in s.rvalue.items:
                if concrete_slots and key is not None:
                    rvalue.append(key)
                else:
                    concrete_slots = False

        slots = []
        for item in rvalue:
            # Special case for `'__dict__'` value:
            # when specified it will still allow any attribute assignment.
            if isinstance(item, StrExpr) and item.value != "__dict__":
                slots.append(item.value)
            else:
                concrete_slots = False
        if not concrete_slots:
            # Some slot items are dynamic, we don't want any false positives,
            # so, we just pretend that this type does not have any slots at all.
            return

        # We need to copy all slots from super types:
        for super_type in self.type.mro[1:-1]:
            assert super_type.slots is not None
            slots.extend(super_type.slots)
        self.type.slots = set(slots)

</t>
<t tx="ekr.20220525082935.128">#
# Misc statements
#

</t>
<t tx="ekr.20220525082935.129">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        return
    self.block_depth[-1] += 1
    for s in b.body:
        self.accept(s)
    self.block_depth[-1] -= 1

</t>
<t tx="ekr.20220525082935.13">def add_implicit_module_attrs(self, file_node: MypyFile) -&gt; None:
    """Manually add implicit definitions of module '__name__' etc."""
    for name, t in implicit_module_attrs.items():
        if name == "__doc__":
            typ: Type = UnboundType("__builtins__.str")
        elif name == "__path__":
            if not file_node.is_package_init_file():
                continue
            # Need to construct the type ourselves, to avoid issues with __builtins__.list
            # not being subscriptable or typing.List not getting bound
            sym = self.lookup_qualified("__builtins__.list", Context())
            if not sym:
                continue
            node = sym.node
            if not isinstance(node, TypeInfo):
                self.defer(node)
                return
            typ = Instance(node, [self.str_type()])
        elif name == "__annotations__":
            sym = self.lookup_qualified("__builtins__.dict", Context(), suppress_errors=True)
            if not sym:
                continue
            node = sym.node
            if not isinstance(node, TypeInfo):
                self.defer(node)
                return
            typ = Instance(node, [self.str_type(), AnyType(TypeOfAny.special_form)])
        else:
            assert t is not None, f"type should be specified for {name}"
            typ = UnboundType(t)

        existing = file_node.names.get(name)
        if existing is not None and not isinstance(existing.node, PlaceholderNode):
            # Already exists.
            continue

        an_type = self.anal_type(typ)
        if an_type:
            var = Var(name, an_type)
            var._fullname = self.qualified_name(name)
            var.is_ready = True
            self.add_symbol(name, var, dummy_context())
        else:
            self.add_symbol(
                name,
                PlaceholderNode(self.qualified_name(name), file_node, -1),
                dummy_context(),
            )

</t>
<t tx="ekr.20220525082935.130">def visit_block_maybe(self, b: Block | None) -&gt; None:
    if b:
        self.visit_block(b)

</t>
<t tx="ekr.20220525082935.131">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1319">class Type(mypy.nodes.Context):
    """Abstract base class for all types."""

    __slots__ = ("can_be_true", "can_be_false")
    # 'can_be_true' and 'can_be_false' mean whether the value of the
    # expression can be true or false in a boolean context. They are useful
    # when inferring the type of logic expressions like `x and y`.
    #
    # For example:
    #   * the literal `False` can't be true while `True` can.
    #   * a value with type `bool` can be true or false.
    #   * `None` can't be true
    #   * ...

    @others
</t>
<t tx="ekr.20220525082935.132">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    self.statement = s
    if not self.is_func_scope():
        self.fail('"return" outside function', s)
    if s.expr:
        s.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1320">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.can_be_true = self.can_be_true_default()
    self.can_be_false = self.can_be_false_default()

</t>
<t tx="ekr.20220525082935.1321">def can_be_true_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.1322">def can_be_false_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.1323">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")

</t>
<t tx="ekr.20220525082935.1324">def __repr__(self) -&gt; str:
    return self.accept(TypeStrVisitor())

</t>
<t tx="ekr.20220525082935.1325">def serialize(self) -&gt; JsonDict | str:
    raise NotImplementedError(f"Cannot serialize {self.__class__.__name__} instance")

</t>
<t tx="ekr.20220525082935.1326">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Type:
    raise NotImplementedError(f"Cannot deserialize {cls.__name__} instance")

def is_singleton_type(self) -&gt; bool:
    return False


</t>
<t tx="ekr.20220525082935.133">def visit_raise_stmt(self, s: RaiseStmt) -&gt; None:
    self.statement = s
    if s.expr:
        s.expr.accept(self)
    if s.from_expr:
        s.from_expr.accept(self)

</t>
<t tx="ekr.20220525082935.134">def visit_assert_stmt(self, s: AssertStmt) -&gt; None:
    self.statement = s
    if s.expr:
        s.expr.accept(self)
    if s.msg:
        s.msg.accept(self)

</t>
<t tx="ekr.20220525082935.1348">class ProperType(Type):
    """Not a type alias.

    Every type except TypeAliasType must inherit from this type.
    """

    __slots__ = ()


</t>
<t tx="ekr.20220525082935.135">def visit_operator_assignment_stmt(self, s: OperatorAssignmentStmt) -&gt; None:
    self.statement = s
    s.lvalue.accept(self)
    s.rvalue.accept(self)
    if (
        isinstance(s.lvalue, NameExpr)
        and s.lvalue.name == "__all__"
        and s.lvalue.kind == GDEF
        and isinstance(s.rvalue, (ListExpr, TupleExpr))
    ):
        self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20220525082935.136">def visit_while_stmt(self, s: WhileStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)
    self.loop_depth += 1
    s.body.accept(self)
    self.loop_depth -= 1
    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20220525082935.137">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    if s.is_async:
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

    self.statement = s
    s.expr.accept(self)

    # Bind index variables and check if they define new names.
    self.analyze_lvalue(s.index, explicit_type=s.index_type is not None)
    if s.index_type:
        if self.is_classvar(s.index_type):
            self.fail_invalid_classvar(s.index)
        allow_tuple_literal = isinstance(s.index, TupleExpr)
        analyzed = self.anal_type(s.index_type, allow_tuple_literal=allow_tuple_literal)
        if analyzed is not None:
            self.store_declared_types(s.index, analyzed)
            s.index_type = analyzed

    self.loop_depth += 1
    self.visit_block(s.body)
    self.loop_depth -= 1

    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20220525082935.138">def visit_break_stmt(self, s: BreakStmt) -&gt; None:
    self.statement = s
    if self.loop_depth == 0:
        self.fail('"break" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20220525082935.1388">class UnboundType(ProperType):
    """Instance type that has not been bound during semantic analysis."""

    __slots__ = (
        "name",
        "args",
    @others
</t>
<t tx="ekr.20220525082935.1389">    "optional",
    "empty_tuple_index",
    "original_str_expr",
    "original_str_fallback",
)

def __init__(
    self,
    name: str | None,
    args: Sequence[Type] | None = None,
    line: int = -1,
    column: int = -1,
    optional: bool = False,
    empty_tuple_index: bool = False,
    original_str_expr: str | None = None,
    original_str_fallback: str | None = None,
) -&gt; None:
    super().__init__(line, column)
    if not args:
        args = []
    assert name is not None
    self.name = name
    self.args = tuple(args)
    # Should this type be wrapped in an Optional?
    self.optional = optional
    # Special case for X[()]
    self.empty_tuple_index = empty_tuple_index
    # If this UnboundType was originally defined as a str or bytes, keep track of
    # the original contents of that string-like thing. This way, if this UnboundExpr
    # ever shows up inside of a LiteralType, we can determine whether that
    # Literal[...] is valid or not. E.g. Literal[foo] is most likely invalid
    # (unless 'foo' is an alias for another literal or something) and
    # Literal["foo"] most likely is.
    #
    # We keep track of the entire string instead of just using a boolean flag
    # so we can distinguish between things like Literal["foo"] vs
    # Literal["    foo   "].
    #
    # We also keep track of what the original base fallback type was supposed to be
    # so we don't have to try and recompute it later
    self.original_str_expr = original_str_expr
    self.original_str_fallback = original_str_fallback

</t>
<t tx="ekr.20220525082935.139">def visit_continue_stmt(self, s: ContinueStmt) -&gt; None:
    self.statement = s
    if self.loop_depth == 0:
        self.fail('"continue" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20220525082935.1390">def copy_modified(self, args: Bogus[Sequence[Type] | None] = _dummy) -&gt; UnboundType:
    if args is _dummy:
        args = self.args
    return UnboundType(
        name=self.name,
        args=args,
        line=self.line,
        column=self.column,
        optional=self.optional,
        empty_tuple_index=self.empty_tuple_index,
        original_str_expr=self.original_str_expr,
        original_str_fallback=self.original_str_fallback,
    )

</t>
<t tx="ekr.20220525082935.1391">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_unbound_type(self)

</t>
<t tx="ekr.20220525082935.1392">def __hash__(self) -&gt; int:
    return hash((self.name, self.optional, tuple(self.args), self.original_str_expr))

</t>
<t tx="ekr.20220525082935.1393">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, UnboundType):
        return NotImplemented
    return (
        self.name == other.name
        and self.optional == other.optional
        and self.args == other.args
        and self.original_str_expr == other.original_str_expr
        and self.original_str_fallback == other.original_str_fallback
    )

</t>
<t tx="ekr.20220525082935.1394">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "UnboundType",
        "name": self.name,
        "args": [a.serialize() for a in self.args],
        "expr": self.original_str_expr,
        "expr_fallback": self.original_str_fallback,
    }

</t>
<t tx="ekr.20220525082935.1395">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UnboundType:
    assert data[".class"] == "UnboundType"
    return UnboundType(
        data["name"],
        [deserialize_type(a) for a in data["args"]],
        original_str_expr=data["expr"],
        original_str_fallback=data["expr_fallback"],
    )


</t>
<t tx="ekr.20220525082935.14">def add_builtin_aliases(self, tree: MypyFile) -&gt; None:
    """Add builtin type aliases to typing module.

    For historical reasons, the aliases like `List = list` are not defined
    in typeshed stubs for typing module. Instead we need to manually add the
    corresponding nodes on the fly. We explicitly mark these aliases as normalized,
    so that a user can write `typing.List[int]`.
    """
    assert tree.fullname == "typing"
    for alias, target_name in type_aliases.items():
        if type_aliases_source_versions[alias] &gt; self.options.python_version:
            # This alias is not available on this Python version.
            continue
        name = alias.split(".")[-1]
        if name in tree.names and not isinstance(tree.names[name].node, PlaceholderNode):
            continue
        self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20220525082935.140">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    self.statement = s
    infer_reachability_of_if_statement(s, self.options)
    for i in range(len(s.expr)):
        s.expr[i].accept(self)
        self.visit_block(s.body[i])
    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20220525082935.141">def visit_try_stmt(self, s: TryStmt) -&gt; None:
    self.statement = s
    self.analyze_try_stmt(s, self)

</t>
<t tx="ekr.20220525082935.142">def analyze_try_stmt(self, s: TryStmt, visitor: NodeVisitor[None]) -&gt; None:
    s.body.accept(visitor)
    for type, var, handler in zip(s.types, s.vars, s.handlers):
        if type:
            type.accept(visitor)
        if var:
            self.analyze_lvalue(var)
        handler.accept(visitor)
    if s.else_body:
        s.else_body.accept(visitor)
    if s.finally_body:
        s.finally_body.accept(visitor)

</t>
<t tx="ekr.20220525082935.143">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    self.statement = s
    types: list[Type] = []

    if s.is_async:
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_WITH_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

    if s.unanalyzed_type:
        assert isinstance(s.unanalyzed_type, ProperType)
        actual_targets = [t for t in s.target if t is not None]
        if len(actual_targets) == 0:
            # We have a type for no targets
            self.fail('Invalid type comment: "with" statement has no targets', s)
        elif len(actual_targets) == 1:
            # We have one target and one type
            types = [s.unanalyzed_type]
        elif isinstance(s.unanalyzed_type, TupleType):
            # We have multiple targets and multiple types
            if len(actual_targets) == len(s.unanalyzed_type.items):
                types = s.unanalyzed_type.items.copy()
            else:
                # But it's the wrong number of items
                self.fail('Incompatible number of types for "with" targets', s)
        else:
            # We have multiple targets and one type
            self.fail('Multiple types expected for multiple "with" targets', s)

    new_types: list[Type] = []
    for e, n in zip(s.expr, s.target):
        e.accept(self)
        if n:
            self.analyze_lvalue(n, explicit_type=s.unanalyzed_type is not None)

            # Since we have a target, pop the next type from types
            if types:
                t = types.pop(0)
                if self.is_classvar(t):
                    self.fail_invalid_classvar(n)
                allow_tuple_literal = isinstance(n, TupleExpr)
                analyzed = self.anal_type(t, allow_tuple_literal=allow_tuple_literal)
                if analyzed is not None:
                    # TODO: Deal with this better
                    new_types.append(analyzed)
                    self.store_declared_types(n, analyzed)

    s.analyzed_types = new_types

    self.visit_block(s.body)

</t>
<t tx="ekr.20220525082935.144">def visit_del_stmt(self, s: DelStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)
    if not self.is_valid_del_target(s.expr):
        self.fail("Invalid delete target", s)

</t>
<t tx="ekr.20220525082935.1444">class Instance(ProperType):
    """An instance type of form C[T1, ..., Tn].

    The list of type variables may be empty.

    Several types have fallbacks to `Instance`, because in Python everything is an object
    and this concept is impossible to express without intersection types. We therefore use
    fallbacks for all "non-special" (like UninhabitedType, ErasedType etc) types.
    """

    __slots__ = ("type", "args", "invalid", "type_ref", "last_known_value", "_hash", "extra_attrs")

    @others
</t>
<t tx="ekr.20220525082935.1445">def __init__(
    self,
    typ: mypy.nodes.TypeInfo,
    args: Sequence[Type],
    line: int = -1,
    column: int = -1,
    *,
    last_known_value: LiteralType | None = None,
    extra_attrs: ExtraAttrs | None = None,
) -&gt; None:
    super().__init__(line, column)
    self.type = typ
    self.args = tuple(args)
    self.type_ref: str | None = None

    # True if recovered after incorrect number of type arguments error
    self.invalid = False

    # This field keeps track of the underlying Literal[...] value associated with
    # this instance, if one is known.
    #
    # This field is set whenever possible within expressions, but is erased upon
    # variable assignment (see erasetype.remove_instance_last_known_values) unless
    # the variable is declared to be final.
    #
    # For example, consider the following program:
    #
    #     a = 1
    #     b: Final[int] = 2
    #     c: Final = 3
    #     print(a + b + c + 4)
    #
    # The 'Instance' objects associated with the expressions '1', '2', '3', and '4' will
    # have last_known_values of type Literal[1], Literal[2], Literal[3], and Literal[4]
    # respectively. However, the Instance object assigned to 'a' and 'b' will have their
    # last_known_value erased: variable 'a' is mutable; variable 'b' was declared to be
    # specifically an int.
    #
    # Or more broadly, this field lets this Instance "remember" its original declaration
    # when applicable. We want this behavior because we want implicit Final declarations
    # to act pretty much identically with constants: we should be able to replace any
    # places where we use some Final variable with the original value and get the same
    # type-checking behavior. For example, we want this program:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    var: Final = 3
    #    expects_literal(var)
    #
    # ...to type-check in the exact same way as if we had written the program like this:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    expects_literal(3)
    #
    # In order to make this work (especially with literal types), we need var's type
    # (an Instance) to remember the "original" value.
    #
    # Preserving this value within expressions is useful for similar reasons.
    #
    # Currently most of mypy will ignore this field and will continue to treat this type like
    # a regular Instance. We end up using this field only when we are explicitly within a
    # Literal context.
    self.last_known_value = last_known_value

    # Cached hash value
    self._hash = -1

</t>
<t tx="ekr.20220525082935.1446">    # Additional attributes defined per instance of this type. For example modules
    # have different attributes per instance of types.ModuleType. This is intended
    # to be "short-lived", we don't serialize it, and even don't store as variable type.
    self.extra_attrs = extra_attrs

def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_instance(self)

</t>
<t tx="ekr.20220525082935.1447">def __hash__(self) -&gt; int:
    if self._hash == -1:
        self._hash = hash((self.type, self.args, self.last_known_value, self.extra_attrs))
    return self._hash

</t>
<t tx="ekr.20220525082935.1448">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Instance):
        return NotImplemented
    return (
        self.type == other.type
        and self.args == other.args
        and self.last_known_value == other.last_known_value
</t>
<t tx="ekr.20220525082935.1449">        and self.extra_attrs == other.extra_attrs
    )

def serialize(self) -&gt; JsonDict | str:
    assert self.type is not None
    type_ref = self.type.fullname
    if not self.args and not self.last_known_value:
        return type_ref
    data: JsonDict = {".class": "Instance"}
    data["type_ref"] = type_ref
    data["args"] = [arg.serialize() for arg in self.args]
    if self.last_known_value is not None:
        data["last_known_value"] = self.last_known_value.serialize()
    return data

</t>
<t tx="ekr.20220525082935.145">def is_valid_del_target(self, s: Expression) -&gt; bool:
    if isinstance(s, (IndexExpr, NameExpr, MemberExpr)):
        return True
    elif isinstance(s, (TupleExpr, ListExpr)):
        return all(self.is_valid_del_target(item) for item in s.items)
    else:
        return False

</t>
<t tx="ekr.20220525082935.1450">@classmethod
def deserialize(cls, data: JsonDict | str) -&gt; Instance:
    if isinstance(data, str):
        inst = Instance(NOT_READY, [])
        inst.type_ref = data
        return inst
    assert data[".class"] == "Instance"
    args: list[Type] = []
    if "args" in data:
        args_list = data["args"]
        assert isinstance(args_list, list)
        args = [deserialize_type(arg) for arg in args_list]
    inst = Instance(NOT_READY, args)
    inst.type_ref = data["type_ref"]  # Will be fixed up by fixup.py later.
    if "last_known_value" in data:
        inst.last_known_value = LiteralType.deserialize(data["last_known_value"])
    return inst

</t>
<t tx="ekr.20220525082935.1451">def copy_modified(
    self,
    *,
    args: Bogus[list[Type]] = _dummy,
    last_known_value: Bogus[LiteralType | None] = _dummy,
) -&gt; Instance:
    new = Instance(
        self.type,
        args if args is not _dummy else self.args,
        self.line,
        self.column,
        last_known_value=last_known_value
        if last_known_value is not _dummy
        else self.last_known_value,
    )
    # We intentionally don't copy the extra_attrs here, so they will be erased.
    new.can_be_true = self.can_be_true
    new.can_be_false = self.can_be_false
    return new

def copy_with_extra_attr(self, name: str, typ: Type) -&gt; Instance:
    if self.extra_attrs:
        existing_attrs = self.extra_attrs.copy()
    else:
        existing_attrs = ExtraAttrs({}, set(), None)
    existing_attrs.attrs[name] = typ
    new = self.copy_modified()
    new.extra_attrs = existing_attrs
    return new

</t>
<t tx="ekr.20220525082935.1452">def has_readable_member(self, name: str) -&gt; bool:
    return self.type.has_readable_member(name)

def is_singleton_type(self) -&gt; bool:
    # TODO:
    # Also make this return True if the type corresponds to NotImplemented?
    return (
        self.type.is_enum
        and len(self.get_enum_values()) == 1
        or self.type.fullname == "builtins.ellipsis"
    )

def get_enum_values(self) -&gt; list[str]:
    """Return the list of values for an Enum."""
    return [
        name for name, sym in self.type.names.items() if isinstance(sym.node, mypy.nodes.Var)
    ]


</t>
<t tx="ekr.20220525082935.1453">class FunctionLike(ProperType):
    """Abstract base class for function types."""

    __slots__ = ("fallback",)

    fallback: Instance

    @others
</t>
<t tx="ekr.20220525082935.1454">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.can_be_false = False

</t>
<t tx="ekr.20220525082935.1455">@abstractmethod
def is_type_obj(self) -&gt; bool:
    pass

</t>
<t tx="ekr.20220525082935.1456">@abstractmethod
def type_object(self) -&gt; mypy.nodes.TypeInfo:
    pass

</t>
<t tx="ekr.20220525082935.1457">@property
@abstractmethod
def items(self) -&gt; list[CallableType]:
    pass

</t>
<t tx="ekr.20220525082935.1458">@abstractmethod
def with_name(self, name: str) -&gt; FunctionLike:
    pass

</t>
<t tx="ekr.20220525082935.1459">@abstractmethod
def get_name(self) -&gt; str | None:
    pass


</t>
<t tx="ekr.20220525082935.146">def visit_global_decl(self, g: GlobalDecl) -&gt; None:
    self.statement = g
    for name in g.names:
        if name in self.nonlocal_decls[-1]:
            self.fail(f'Name "{name}" is nonlocal and global', g)
        self.global_decls[-1].add(name)

</t>
<t tx="ekr.20220525082935.1460">class FormalArgument(NamedTuple):
    name: str | None
    pos: int | None
    typ: Type
    required: bool


</t>
<t tx="ekr.20220525082935.147">def visit_nonlocal_decl(self, d: NonlocalDecl) -&gt; None:
    self.statement = d
    if self.is_module_scope():
        self.fail("nonlocal declaration not allowed at module level", d)
    else:
        for name in d.names:
            for table in reversed(self.locals[:-1]):
                if table is not None and name in table:
                    break
            else:
                self.fail(f'No binding for nonlocal "{name}" found', d)

            if self.locals[-1] is not None and name in self.locals[-1]:
                self.fail(
                    'Name "{}" is already defined in local '
                    "scope before nonlocal declaration".format(name),
                    d,
                )

            if name in self.global_decls[-1]:
                self.fail(f'Name "{name}" is nonlocal and global', d)
            self.nonlocal_decls[-1].add(name)
</t>
<t tx="ekr.20220525082935.1476">class CallableType(FunctionLike):
    """Type of a non-overloaded callable object (such as function)."""

    __slots__ = (
        "arg_types",  # Types of function arguments
        "arg_kinds",  # ARG_ constants
        "arg_names",  # Argument names; None if not a keyword argument
        "min_args",  # Minimum number of arguments; derived from arg_kinds
        "ret_type",  # Return value type
        "name",  # Name (may be None; for error messages and plugins)
        "definition",  # For error messages.  May be None.
        "variables",  # Type variables for a generic function
        "is_ellipsis_args",  # Is this Callable[..., t] (with literal '...')?
        "is_classmethod_class",  # Is this callable constructed for the benefit
        # of a classmethod's 'cls' argument?
        "implicit",  # Was this type implicitly generated instead of explicitly
        # specified by the user?
        "special_sig",  # Non-None for signatures that require special handling
        # (currently only value is 'dict' for a signature similar to
        # 'dict')
        "from_type_type",  # Was this callable generated by analyzing Type[...]
        # instantiation?
        "bound_args",  # Bound type args, mostly unused but may be useful for
        # tools that consume mypy ASTs
        "def_extras",  # Information about original definition we want to serialize.
        # This is used for more detailed error messages.
        "type_guard",  # T, if -&gt; TypeGuard[T] (ret_type is bool in this case).
        "from_concatenate",  # whether this callable is from a concatenate object
        # (this is used for error messages)
        "unpack_kwargs",  # Was an Unpack[...] with **kwargs used to define this callable?
    @others
</t>
<t tx="ekr.20220525082935.1477">)

def __init__(
    self,
    # maybe this should be refactored to take a Parameters object
    arg_types: Sequence[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None],
    ret_type: Type,
    fallback: Instance,
    name: str | None = None,
    definition: SymbolNode | None = None,
    variables: Sequence[TypeVarLikeType] | None = None,
    line: int = -1,
    column: int = -1,
    is_ellipsis_args: bool = False,
    implicit: bool = False,
    special_sig: str | None = None,
    from_type_type: bool = False,
    bound_args: Sequence[Type | None] = (),
    def_extras: dict[str, Any] | None = None,
    type_guard: Type | None = None,
    from_concatenate: bool = False,
    unpack_kwargs: bool = False,
) -&gt; None:
    super().__init__(line, column)
    assert len(arg_types) == len(arg_kinds) == len(arg_names)
    if variables is None:
        variables = []
    self.arg_types = list(arg_types)
    self.arg_kinds = arg_kinds
    self.arg_names = list(arg_names)
    self.min_args = arg_kinds.count(ARG_POS)
    self.ret_type = ret_type
    self.fallback = fallback
    assert not name or "&lt;bound method" not in name
    self.name = name
    self.definition = definition
    self.variables = variables
    self.is_ellipsis_args = is_ellipsis_args
    self.implicit = implicit
    self.special_sig = special_sig
    self.from_type_type = from_type_type
    self.from_concatenate = from_concatenate
    if not bound_args:
        bound_args = ()
    self.bound_args = bound_args
    if def_extras:
        self.def_extras = def_extras
    elif isinstance(definition, FuncDef):
        # This information would be lost if we don't have definition
        # after serialization, but it is useful in error messages.
        # TODO: decide how to add more info here (file, line, column)
        # without changing interface hash.
        first_arg: str | None = None
        if definition.arg_names and definition.info and not definition.is_static:
            if getattr(definition, "arguments", None):
                first_arg = definition.arguments[0].variable.name
            else:
                first_arg = definition.arg_names[0]
        self.def_extras = {"first_arg": first_arg}
    else:
        self.def_extras = {}
    self.type_guard = type_guard
    self.unpack_kwargs = unpack_kwargs
</t>
<t tx="ekr.20220525082935.1478">
def copy_modified(
    self: CT,
    arg_types: Bogus[Sequence[Type]] = _dummy,
    arg_kinds: Bogus[list[ArgKind]] = _dummy,
    arg_names: Bogus[list[str | None]] = _dummy,
    ret_type: Bogus[Type] = _dummy,
    fallback: Bogus[Instance] = _dummy,
    name: Bogus[str | None] = _dummy,
    definition: Bogus[SymbolNode] = _dummy,
    variables: Bogus[Sequence[TypeVarLikeType]] = _dummy,
    line: Bogus[int] = _dummy,
    column: Bogus[int] = _dummy,
    is_ellipsis_args: Bogus[bool] = _dummy,
    implicit: Bogus[bool] = _dummy,
    special_sig: Bogus[str | None] = _dummy,
    from_type_type: Bogus[bool] = _dummy,
    bound_args: Bogus[list[Type | None]] = _dummy,
    def_extras: Bogus[dict[str, Any]] = _dummy,
    type_guard: Bogus[Type | None] = _dummy,
    from_concatenate: Bogus[bool] = _dummy,
    unpack_kwargs: Bogus[bool] = _dummy,
) -&gt; CT:
    return type(self)(
        arg_types=arg_types if arg_types is not _dummy else self.arg_types,
        arg_kinds=arg_kinds if arg_kinds is not _dummy else self.arg_kinds,
        arg_names=arg_names if arg_names is not _dummy else self.arg_names,
        ret_type=ret_type if ret_type is not _dummy else self.ret_type,
        fallback=fallback if fallback is not _dummy else self.fallback,
        name=name if name is not _dummy else self.name,
        definition=definition if definition is not _dummy else self.definition,
        variables=variables if variables is not _dummy else self.variables,
        line=line if line is not _dummy else self.line,
        column=column if column is not _dummy else self.column,
        is_ellipsis_args=(
            is_ellipsis_args if is_ellipsis_args is not _dummy else self.is_ellipsis_args
        ),
        implicit=implicit if implicit is not _dummy else self.implicit,
        special_sig=special_sig if special_sig is not _dummy else self.special_sig,
        from_type_type=from_type_type if from_type_type is not _dummy else self.from_type_type,
        bound_args=bound_args if bound_args is not _dummy else self.bound_args,
        def_extras=def_extras if def_extras is not _dummy else dict(self.def_extras),
        type_guard=type_guard if type_guard is not _dummy else self.type_guard,
        from_concatenate=(
            from_concatenate if from_concatenate is not _dummy else self.from_concatenate
        ),
        unpack_kwargs=unpack_kwargs if unpack_kwargs is not _dummy else self.unpack_kwargs,
</t>
<t tx="ekr.20220525082935.1479">    )

def var_arg(self) -&gt; FormalArgument | None:
    """The formal argument for *args."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20220525082935.148"></t>
<t tx="ekr.20220525082935.1480">def kw_arg(self) -&gt; FormalArgument | None:
    """The formal argument for **kwargs."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR2:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20220525082935.1481">@property
def is_var_arg(self) -&gt; bool:
    """Does this callable have a *args argument?"""
    return ARG_STAR in self.arg_kinds

</t>
<t tx="ekr.20220525082935.1482">@property
def is_kw_arg(self) -&gt; bool:
    """Does this callable have a **kwargs argument?"""
    return ARG_STAR2 in self.arg_kinds

</t>
<t tx="ekr.20220525082935.1483">def is_type_obj(self) -&gt; bool:
    return self.fallback.type.is_metaclass() and not isinstance(
        get_proper_type(self.ret_type), UninhabitedType
    )

</t>
<t tx="ekr.20220525082935.1484">def type_object(self) -&gt; mypy.nodes.TypeInfo:
    assert self.is_type_obj()
    ret = get_proper_type(self.ret_type)
    if isinstance(ret, TypeVarType):
        ret = get_proper_type(ret.upper_bound)
    if isinstance(ret, TupleType):
        ret = ret.partial_fallback
    if isinstance(ret, TypedDictType):
        ret = ret.fallback
    assert isinstance(ret, Instance)
    return ret.type

</t>
<t tx="ekr.20220525082935.1485">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_callable_type(self)

</t>
<t tx="ekr.20220525082935.1486">def with_name(self, name: str) -&gt; CallableType:
    """Return a copy of this type with the specified name."""
    return self.copy_modified(ret_type=self.ret_type, name=name)

</t>
<t tx="ekr.20220525082935.1487">def get_name(self) -&gt; str | None:
    return self.name

</t>
<t tx="ekr.20220525082935.1488">def max_possible_positional_args(self) -&gt; int:
    """Returns maximum number of positional arguments this method could possibly accept.

    This takes into account *arg and **kwargs but excludes keyword-only args."""
    if self.is_var_arg or self.is_kw_arg:
        return sys.maxsize
    return sum(kind.is_positional() for kind in self.arg_kinds)

</t>
<t tx="ekr.20220525082935.1489">def formal_arguments(self, include_star_args: bool = False) -&gt; list[FormalArgument]:
    """Return a list of the formal arguments of this callable, ignoring *arg and **kwargs.

    To handle *args and **kwargs, use the 'callable.var_args' and 'callable.kw_args' fields,
    if they are not None.

    If you really want to include star args in the yielded output, set the
    'include_star_args' parameter to 'True'."""
    args = []
    done_with_positional = False
    for i in range(len(self.arg_types)):
        kind = self.arg_kinds[i]
        if kind.is_named() or kind.is_star():
            done_with_positional = True
        if not include_star_args and kind.is_star():
            continue

        required = kind.is_required()
        pos = None if done_with_positional else i
        arg = FormalArgument(self.arg_names[i], pos, self.arg_types[i], required)
        args.append(arg)
    return args

</t>
<t tx="ekr.20220525082935.149">
</t>
<t tx="ekr.20220525082935.1490">def argument_by_name(self, name: str | None) -&gt; FormalArgument | None:
    if name is None:
        return None
    seen_star = False
    for i, (arg_name, kind, typ) in enumerate(
        zip(self.arg_names, self.arg_kinds, self.arg_types)
    ):
        # No more positional arguments after these.
        if kind.is_named() or kind.is_star():
            seen_star = True
        if kind.is_star():
            continue
        if arg_name == name:
            position = None if seen_star else i
            return FormalArgument(name, position, typ, kind.is_required())
    return self.try_synthesizing_arg_from_kwarg(name)

</t>
<t tx="ekr.20220525082935.1491">def argument_by_position(self, position: int | None) -&gt; FormalArgument | None:
    if position is None:
        return None
    if position &gt;= len(self.arg_names):
        return self.try_synthesizing_arg_from_vararg(position)
    name, kind, typ = (
        self.arg_names[position],
        self.arg_kinds[position],
        self.arg_types[position],
    )
    if kind.is_positional():
        return FormalArgument(name, position, typ, kind == ARG_POS)
    else:
        return self.try_synthesizing_arg_from_vararg(position)

</t>
<t tx="ekr.20220525082935.1492">def try_synthesizing_arg_from_kwarg(self, name: str | None) -&gt; FormalArgument | None:
    kw_arg = self.kw_arg()
    if kw_arg is not None:
        return FormalArgument(name, None, kw_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20220525082935.1493">def try_synthesizing_arg_from_vararg(self, position: int | None) -&gt; FormalArgument | None:
    var_arg = self.var_arg()
    if var_arg is not None:
        return FormalArgument(None, position, var_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20220525082935.1494">@property
def items(self) -&gt; list[CallableType]:
    return [self]

</t>
<t tx="ekr.20220525082935.1495">def is_generic(self) -&gt; bool:
    return bool(self.variables)

</t>
<t tx="ekr.20220525082935.1496">def type_var_ids(self) -&gt; list[TypeVarId]:
    a: list[TypeVarId] = []
    for tv in self.variables:
        a.append(tv.id)
    return a

</t>
<t tx="ekr.20220525082935.1497">def param_spec(self) -&gt; ParamSpecType | None:
    """Return ParamSpec if callable can be called with one.

    A Callable accepting ParamSpec P args (*args, **kwargs) must have the
    two final parameters like this: *args: P.args, **kwargs: P.kwargs.
    """
    if len(self.arg_types) &lt; 2:
        return None
    if self.arg_kinds[-2] != ARG_STAR or self.arg_kinds[-1] != ARG_STAR2:
        return None
    arg_type = self.arg_types[-2]
    if not isinstance(arg_type, ParamSpecType):
        return None
    # sometimes paramspectypes are analyzed in from mysterious places,
    # e.g. def f(prefix..., *args: P.args, **kwargs: P.kwargs) -&gt; ...: ...
    prefix = arg_type.prefix
    if not prefix.arg_types:
        # TODO: confirm that all arg kinds are positional
        prefix = Parameters(self.arg_types[:-2], self.arg_kinds[:-2], self.arg_names[:-2])
    return ParamSpecType(
        arg_type.name,
        arg_type.fullname,
</t>
<t tx="ekr.20220525082935.1498">        arg_type.id,
        ParamSpecFlavor.BARE,
        arg_type.upper_bound,
        prefix=prefix,
    )

def expand_param_spec(
    self, c: CallableType | Parameters, no_prefix: bool = False
) -&gt; CallableType:
    variables = c.variables

    if no_prefix:
        return self.copy_modified(
            arg_types=c.arg_types,
            arg_kinds=c.arg_kinds,
            arg_names=c.arg_names,
            is_ellipsis_args=c.is_ellipsis_args,
            variables=[*variables, *self.variables],
        )
    else:
        return self.copy_modified(
            arg_types=self.arg_types[:-2] + c.arg_types,
            arg_kinds=self.arg_kinds[:-2] + c.arg_kinds,
            arg_names=self.arg_names[:-2] + c.arg_names,
            is_ellipsis_args=c.is_ellipsis_args,
            variables=[*variables, *self.variables],
        )

def with_unpacked_kwargs(self) -&gt; NormalizedCallableType:
    if not self.unpack_kwargs:
        return NormalizedCallableType(self.copy_modified())
    last_type = get_proper_type(self.arg_types[-1])
    assert isinstance(last_type, TypedDictType)
    extra_kinds = [
        ArgKind.ARG_NAMED if name in last_type.required_keys else ArgKind.ARG_NAMED_OPT
        for name in last_type.items
    ]
    new_arg_kinds = self.arg_kinds[:-1] + extra_kinds
    new_arg_names = self.arg_names[:-1] + list(last_type.items)
    new_arg_types = self.arg_types[:-1] + list(last_type.items.values())
    return NormalizedCallableType(
        self.copy_modified(
            arg_kinds=new_arg_kinds,
            arg_names=new_arg_names,
            arg_types=new_arg_types,
            unpack_kwargs=False,
        )
    )

</t>
<t tx="ekr.20220525082935.1499">def __hash__(self) -&gt; int:
    # self.is_type_obj() will fail if self.fallback.type is a FakeInfo
    if isinstance(self.fallback.type, FakeInfo):
        is_type_obj = 2
    else:
        is_type_obj = self.is_type_obj()
    return hash(
        (
            self.ret_type,
            is_type_obj,
            self.is_ellipsis_args,
            self.name,
            tuple(self.arg_types),
            tuple(self.arg_names),
            tuple(self.arg_kinds),
            self.fallback,
        )
    )

</t>
<t tx="ekr.20220525082935.15">def add_typing_extension_aliases(self, tree: MypyFile) -&gt; None:
    """Typing extensions module does contain some type aliases.

    We need to analyze them as such, because in typeshed
    they are just defined as `_Alias()` call.
    Which is not supported natively.
    """
    assert tree.fullname == "typing_extensions"

    for alias, target_name in typing_extensions_aliases.items():
        name = alias.split(".")[-1]
        if name in tree.names and isinstance(tree.names[name].node, TypeAlias):
            continue  # Do not reset TypeAliases on the second pass.

        # We need to remove any node that is there at the moment. It is invalid.
        tree.names.pop(name, None)

        # Now, create a new alias.
        self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20220525082935.150">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    self.statement = s
    infer_reachability_of_match_statement(s, self.options)
    s.subject.accept(self)
    for i in range(len(s.patterns)):
        s.patterns[i].accept(self)
        guard = s.guards[i]
        if guard is not None:
            guard.accept(self)
        self.visit_block(s.bodies[i])

</t>
<t tx="ekr.20220525082935.1500">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, CallableType):
        return (
            self.ret_type == other.ret_type
            and self.arg_types == other.arg_types
            and self.arg_names == other.arg_names
            and self.arg_kinds == other.arg_kinds
            and self.name == other.name
            and self.is_type_obj() == other.is_type_obj()
            and self.is_ellipsis_args == other.is_ellipsis_args
            and self.fallback == other.fallback
        )
    else:
        return NotImplemented

</t>
<t tx="ekr.20220525082935.1501">def serialize(self) -&gt; JsonDict:
    # TODO: As an optimization, leave out everything related to
    # generic functions for non-generic functions.
    return {
        ".class": "CallableType",
        "arg_types": [t.serialize() for t in self.arg_types],
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "arg_names": self.arg_names,
        "ret_type": self.ret_type.serialize(),
        "fallback": self.fallback.serialize(),
        "name": self.name,
        # We don't serialize the definition (only used for error messages).
        "variables": [v.serialize() for v in self.variables],
        "is_ellipsis_args": self.is_ellipsis_args,
        "implicit": self.implicit,
        "bound_args": [(None if t is None else t.serialize()) for t in self.bound_args],
        "def_extras": dict(self.def_extras),
        "type_guard": self.type_guard.serialize() if self.type_guard is not None else None,
        "from_concatenate": self.from_concatenate,
        "unpack_kwargs": self.unpack_kwargs,
    }

</t>
<t tx="ekr.20220525082935.1502">@classmethod
def deserialize(cls, data: JsonDict) -&gt; CallableType:
    assert data[".class"] == "CallableType"
    # TODO: Set definition to the containing SymbolNode?
    return CallableType(
        [deserialize_type(t) for t in data["arg_types"]],
        [ArgKind(x) for x in data["arg_kinds"]],
        data["arg_names"],
        deserialize_type(data["ret_type"]),
        Instance.deserialize(data["fallback"]),
        name=data["name"],
        variables=[cast(TypeVarLikeType, deserialize_type(v)) for v in data["variables"]],
        is_ellipsis_args=data["is_ellipsis_args"],
        implicit=data["implicit"],
        bound_args=[(None if t is None else deserialize_type(t)) for t in data["bound_args"]],
        def_extras=data["def_extras"],
        type_guard=(
            deserialize_type(data["type_guard"]) if data["type_guard"] is not None else None
        ),
        from_concatenate=data["from_concatenate"],
        unpack_kwargs=data["unpack_kwargs"],
    )


# This is a little safety net to prevent reckless special-casing of callables
# that can potentially break Unpack[...] with **kwargs.
# TODO: use this in more places in checkexpr.py etc?
NormalizedCallableType = NewType("NormalizedCallableType", CallableType)


</t>
<t tx="ekr.20220525082935.151">#
# Expressions
#

</t>
<t tx="ekr.20220525082935.152">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    n = self.lookup(expr.name, expr)
    if n:
        self.bind_name_expr(expr, n)

</t>
<t tx="ekr.20220525082935.153">def bind_name_expr(self, expr: NameExpr, sym: SymbolTableNode) -&gt; None:
    """Bind name expression to a symbol table node."""
    if isinstance(sym.node, TypeVarExpr) and self.tvar_scope.get_binding(sym):
        self.fail(
            '"{}" is a type variable and only valid in type ' "context".format(expr.name), expr
        )
    elif isinstance(sym.node, PlaceholderNode):
        self.process_placeholder(expr.name, "name", expr)
    else:
        expr.kind = sym.kind
        expr.node = sym.node
        expr.fullname = sym.fullname

</t>
<t tx="ekr.20220525082935.154">def visit_super_expr(self, expr: SuperExpr) -&gt; None:
    if not self.type and not expr.call.args:
        self.fail('"super" used outside class', expr)
        return
    expr.info = self.type
    for arg in expr.call.args:
        arg.accept(self)

</t>
<t tx="ekr.20220525082935.155">def visit_tuple_expr(self, expr: TupleExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20220525082935.156">def visit_list_expr(self, expr: ListExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20220525082935.157">def visit_set_expr(self, expr: SetExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20220525082935.158">def visit_dict_expr(self, expr: DictExpr) -&gt; None:
    for key, value in expr.items:
        if key is not None:
            key.accept(self)
        value.accept(self)

</t>
<t tx="ekr.20220525082935.159">def visit_star_expr(self, expr: StarExpr) -&gt; None:
    if not expr.valid:
        # XXX TODO Change this error message
        self.fail("Can use starred expression only as assignment target", expr)
    else:
        expr.expr.accept(self)

</t>
<t tx="ekr.20220525082935.16">def create_alias(self, tree: MypyFile, target_name: str, alias: str, name: str) -&gt; None:
    tag = self.track_incomplete_refs()
    n = self.lookup_fully_qualified_or_none(target_name)
    if n:
        if isinstance(n.node, PlaceholderNode):
            self.mark_incomplete(name, tree)
        else:
            # Found built-in class target. Create alias.
            target = self.named_type_or_none(target_name, [])
            assert target is not None
            # Transform List to List[Any], etc.
            fix_instance_types(target, self.fail, self.note, self.options.python_version)
            alias_node = TypeAlias(
                target,
                alias,
                line=-1,
                column=-1,  # there is no context
                no_args=True,
                normalized=True,
            )
            self.add_symbol(name, alias_node, tree)
    elif self.found_incomplete_ref(tag):
        # Built-in class target may not ready yet -- defer.
        self.mark_incomplete(name, tree)
    else:
        # Test fixtures may be missing some builtin classes, which is okay.
        # Kill the placeholder if there is one.
        if name in tree.names:
            assert isinstance(tree.names[name].node, PlaceholderNode)
            del tree.names[name]

</t>
<t tx="ekr.20220525082935.160">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"yield from" outside function', e, serious=True, blocker=True)
    elif self.is_comprehension_stack[-1]:
        self.fail(
            '"yield from" inside comprehension or generator expression',
            e,
            serious=True,
            blocker=True,
        )
    elif self.function_stack[-1].is_coroutine:
        self.fail('"yield from" in async function', e, serious=True, blocker=True)
    else:
        self.function_stack[-1].is_generator = True
    if e.expr:
        e.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1603">class TypeStrVisitor(SyntheticTypeVisitor[str]):
    """Visitor for pretty-printing types into strings.

    This is mostly for debugging/testing.

    Do not preserve original formatting.

    Notes:
     - Represent unbound types as Foo? or Foo?[...].
     - Represent the NoneType type as None.
    """

    @others
</t>
<t tx="ekr.20220525082935.1604">def __init__(self, id_mapper: IdMapper | None = None) -&gt; None:
    self.id_mapper = id_mapper
    self.any_as_dots = False

</t>
<t tx="ekr.20220525082935.1605">def visit_unbound_type(self, t: UnboundType) -&gt; str:
    s = t.name + "?"
    if t.args:
        s += f"[{self.list_str(t.args)}]"
    return s

</t>
<t tx="ekr.20220525082935.1606">def visit_type_list(self, t: TypeList) -&gt; str:
    return f"&lt;TypeList {self.list_str(t.items)}&gt;"

</t>
<t tx="ekr.20220525082935.1607">def visit_callable_argument(self, t: CallableArgument) -&gt; str:
    typ = t.typ.accept(self)
    if t.name is None:
        return f"{t.constructor}({typ})"
    else:
        return f"{t.constructor}({typ}, {t.name})"

</t>
<t tx="ekr.20220525082935.1608">def visit_any(self, t: AnyType) -&gt; str:
    if self.any_as_dots and t.type_of_any == TypeOfAny.special_form:
        return "..."
    return "Any"

</t>
<t tx="ekr.20220525082935.1609">def visit_none_type(self, t: NoneType) -&gt; str:
    return "None"

</t>
<t tx="ekr.20220525082935.161">def visit_call_expr(self, expr: CallExpr) -&gt; None:
    """Analyze a call expression.

    Some call expressions are recognized as special forms, including
    cast(...).
    """
    expr.callee.accept(self)
    if refers_to_fullname(expr.callee, "typing.cast"):
        # Special form cast(...).
        if not self.check_fixed_args(expr, 2, "cast"):
            return
        # Translate first argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[0])
        except TypeTranslationError:
            self.fail("Cast target is not a type", expr)
            return
        # Piggyback CastExpr object to the CallExpr object; it takes
        # precedence over the CallExpr semantics.
        expr.analyzed = CastExpr(expr.args[1], target)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, ASSERT_TYPE_NAMES):
        if not self.check_fixed_args(expr, 2, "assert_type"):
            return
        # Translate second argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[1])
        except TypeTranslationError:
            self.fail("assert_type() type is not a type", expr)
            return
        expr.analyzed = AssertTypeExpr(expr.args[0], target)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, REVEAL_TYPE_NAMES):
        if not self.check_fixed_args(expr, 1, "reveal_type"):
            return
        expr.analyzed = RevealExpr(kind=REVEAL_TYPE, expr=expr.args[0])
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, "builtins.reveal_locals"):
        # Store the local variable names into the RevealExpr for use in the
        # type checking pass
        local_nodes: list[Var] = []
        if self.is_module_scope():
            # try to determine just the variable declarations in module scope
            # self.globals.values() contains SymbolTableNode's
            # Each SymbolTableNode has an attribute node that is nodes.Var
            # look for variable nodes that marked as is_inferred
            # Each symboltable node has a Var node as .node
            local_nodes = [
                n.node
                for name, n in self.globals.items()
                if getattr(n.node, "is_inferred", False) and isinstance(n.node, Var)
            ]
        elif self.is_class_scope():
            # type = None  # type: Optional[TypeInfo]
            if self.type is not None:
                local_nodes = [
                    st.node for st in self.type.names.values() if isinstance(st.node, Var)
                ]
        elif self.is_func_scope():
            # locals = None  # type: List[Optional[SymbolTable]]
            if self.locals is not None:
                symbol_table = self.locals[-1]
                if symbol_table is not None:
                    local_nodes = [
                        st.node for st in symbol_table.values() if isinstance(st.node, Var)
                    ]
        expr.analyzed = RevealExpr(kind=REVEAL_LOCALS, local_nodes=local_nodes)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, "typing.Any"):
        # Special form Any(...) no longer supported.
        self.fail("Any(...) is no longer supported. Use cast(Any, ...) instead", expr)
    elif refers_to_fullname(expr.callee, "typing._promote"):
        # Special form _promote(...).
        if not self.check_fixed_args(expr, 1, "_promote"):
            return
        # Translate first argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[0])
        except TypeTranslationError:
            self.fail("Argument 1 to _promote is not a type", expr)
            return
        expr.analyzed = PromoteExpr(target)
        expr.analyzed.line = expr.line
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, "builtins.dict"):
        expr.analyzed = self.translate_dict_call(expr)
    elif refers_to_fullname(expr.callee, "builtins.divmod"):
        if not self.check_fixed_args(expr, 2, "divmod"):
            return
        expr.analyzed = OpExpr("divmod", expr.args[0], expr.args[1])
        expr.analyzed.line = expr.line
        expr.analyzed.accept(self)
    else:
        # Normal call expression.
        for a in expr.args:
            a.accept(self)

        if (
            isinstance(expr.callee, MemberExpr)
            and isinstance(expr.callee.expr, NameExpr)
            and expr.callee.expr.name == "__all__"
            and expr.callee.expr.kind == GDEF
            and expr.callee.name in ("append", "extend")
        ):
            if expr.callee.name == "append" and expr.args:
                self.add_exports(expr.args[0])
            elif (
                expr.callee.name == "extend"
                and expr.args
                and isinstance(expr.args[0], (ListExpr, TupleExpr))
            ):
                self.add_exports(expr.args[0].items)

</t>
<t tx="ekr.20220525082935.1610">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; str:
    return "&lt;nothing&gt;"

</t>
<t tx="ekr.20220525082935.1611">def visit_erased_type(self, t: ErasedType) -&gt; str:
    return "&lt;Erased&gt;"

</t>
<t tx="ekr.20220525082935.1612">def visit_deleted_type(self, t: DeletedType) -&gt; str:
    if t.source is None:
        return "&lt;Deleted&gt;"
    else:
        return f"&lt;Deleted '{t.source}'&gt;"

</t>
<t tx="ekr.20220525082935.1613">def visit_instance(self, t: Instance) -&gt; str:
    if t.last_known_value and not t.args:
        # Instances with a literal fallback should never be generic. If they are,
        # something went wrong so we fall back to showing the full Instance repr.
        s = f"{t.last_known_value}?"
    else:
        s = t.type.fullname or t.type.name or "&lt;???&gt;"

    if t.args:
        if t.type.fullname == "builtins.tuple":
            assert len(t.args) == 1
            s += f"[{self.list_str(t.args)}, ...]"
        else:
            s += f"[{self.list_str(t.args)}]"
    if self.id_mapper:
        s += f"&lt;{self.id_mapper.id(t.type)}&gt;"
    return s

</t>
<t tx="ekr.20220525082935.1614">def visit_type_var(self, t: TypeVarType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f"`{t.id}"
    else:
        # Named type variable type.
        s = f"{t.name}`{t.id}"
    if self.id_mapper and t.upper_bound:
        s += f"(upper_bound={t.upper_bound.accept(self)})"
    return s

</t>
<t tx="ekr.20220525082935.1615">def visit_param_spec(self, t: ParamSpecType) -&gt; str:
    # prefixes are displayed as Concatenate
    s = ""
    if t.prefix.arg_types:
        s += f"[{self.list_str(t.prefix.arg_types)}, **"
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s += f"`{t.id}"
    else:
        # Named type variable type.
        s += f"{t.name_with_suffix()}`{t.id}"
    if t.prefix.arg_types:
        s += "]"
    return s

</t>
<t tx="ekr.20220525082935.1616">def visit_parameters(self, t: Parameters) -&gt; str:
    # This is copied from visit_callable -- is there a way to decrease duplication?
    if t.is_ellipsis_args:
        return "..."

    s = ""
    bare_asterisk = False
    for i in range(len(t.arg_types)):
        if s != "":
            s += ", "
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += "*, "
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += "*"
        if t.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = t.arg_names[i]
        if name:
            s += f"{name}: "
        r = t.arg_types[i].accept(self)

        s += r

        if t.arg_kinds[i].is_optional():
            s += " ="

    return f"[{s}]"

</t>
<t tx="ekr.20220525082935.1617">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f"`{t.id}"
    else:
        # Named type variable type.
        s = f"{t.name}`{t.id}"
    return s

</t>
<t tx="ekr.20220525082935.1618">def visit_callable_type(self, t: CallableType) -&gt; str:
    param_spec = t.param_spec()
    if param_spec is not None:
        num_skip = 2
    else:
        num_skip = 0

    s = ""
    bare_asterisk = False
    for i in range(len(t.arg_types) - num_skip):
        if s != "":
            s += ", "
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += "*, "
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += "*"
        if t.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = t.arg_names[i]
        if name:
            s += name + ": "
        type_str = t.arg_types[i].accept(self)
        if t.arg_kinds[i] == ARG_STAR2 and t.unpack_kwargs:
            type_str = f"Unpack[{type_str}]"
        s += type_str
        if t.arg_kinds[i].is_optional():
            s += " ="

    if param_spec is not None:
        n = param_spec.name
        if s:
            s += ", "
        s += f"*{n}.args, **{n}.kwargs"

    s = f"({s})"

    if not isinstance(get_proper_type(t.ret_type), NoneType):
        if t.type_guard is not None:
            s += f" -&gt; TypeGuard[{t.type_guard.accept(self)}]"
        else:
            s += f" -&gt; {t.ret_type.accept(self)}"

    if t.variables:
        vs = []
        for var in t.variables:
            if isinstance(var, TypeVarType):
                # We reimplement TypeVarType.__repr__ here in order to support id_mapper.
                if var.values:
                    vals = f"({', '.join(val.accept(self) for val in var.values)})"
                    vs.append(f"{var.name} in {vals}")
                elif not is_named_instance(var.upper_bound, "builtins.object"):
                    vs.append(f"{var.name} &lt;: {var.upper_bound.accept(self)}")
                else:
                    vs.append(var.name)
            else:
                # For other TypeVarLikeTypes, just use the name
                vs.append(var.name)
        s = f"[{', '.join(vs)}] {s}"

    return f"def {s}"

</t>
<t tx="ekr.20220525082935.1619">def visit_overloaded(self, t: Overloaded) -&gt; str:
    a = []
    for i in t.items:
        a.append(i.accept(self))
    return f"Overload({', '.join(a)})"

</t>
<t tx="ekr.20220525082935.162">def translate_dict_call(self, call: CallExpr) -&gt; DictExpr | None:
    """Translate 'dict(x=y, ...)' to {'x': y, ...} and 'dict()' to {}.

    For other variants of dict(...), return None.
    """
    if not all(kind == ARG_NAMED for kind in call.arg_kinds):
        # Must still accept those args.
        for a in call.args:
            a.accept(self)
        return None
    expr = DictExpr(
        [
            (StrExpr(cast(str, key)), value)  # since they are all ARG_NAMED
            for key, value in zip(call.arg_names, call.args)
        ]
    )
    expr.set_line(call)
    expr.accept(self)
    return expr

</t>
<t tx="ekr.20220525082935.1620">def visit_tuple_type(self, t: TupleType) -&gt; str:
    s = self.list_str(t.items)
    if t.partial_fallback and t.partial_fallback.type:
        fallback_name = t.partial_fallback.type.fullname
        if fallback_name != "builtins.tuple":
            return f"Tuple[{s}, fallback={t.partial_fallback.accept(self)}]"
    return f"Tuple[{s}]"

</t>
<t tx="ekr.20220525082935.1621">def visit_typeddict_type(self, t: TypedDictType) -&gt; str:
    def item_str(name: str, typ: str) -&gt; str:
        if name in t.required_keys:
            return f"{name!r}: {typ}"
        else:
            return f"{name!r}?: {typ}"

    s = (
        "{"
        + ", ".join(item_str(name, typ.accept(self)) for name, typ in t.items.items())
        + "}"
    )
    prefix = ""
    if t.fallback and t.fallback.type:
        if t.fallback.type.fullname not in TPDICT_FB_NAMES:
            prefix = repr(t.fallback.type.fullname) + ", "
    return f"TypedDict({prefix}{s})"

</t>
<t tx="ekr.20220525082935.1622">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; str:
    return repr(t.literal_value)

</t>
<t tx="ekr.20220525082935.1623">def visit_literal_type(self, t: LiteralType) -&gt; str:
    return f"Literal[{t.value_repr()}]"

</t>
<t tx="ekr.20220525082935.1624">def visit_star_type(self, t: StarType) -&gt; str:
    s = t.type.accept(self)
    return f"*{s}"

</t>
<t tx="ekr.20220525082935.1625">def visit_union_type(self, t: UnionType) -&gt; str:
    s = self.list_str(t.items)
    return f"Union[{s}]"

</t>
<t tx="ekr.20220525082935.1626">def visit_partial_type(self, t: PartialType) -&gt; str:
    if t.type is None:
        return "&lt;partial None&gt;"
    else:
        return "&lt;partial {}[{}]&gt;".format(t.type.name, ", ".join(["?"] * len(t.type.type_vars)))

</t>
<t tx="ekr.20220525082935.1627">def visit_ellipsis_type(self, t: EllipsisType) -&gt; str:
    return "..."

</t>
<t tx="ekr.20220525082935.1628">def visit_type_type(self, t: TypeType) -&gt; str:
    return f"Type[{t.item.accept(self)}]"

</t>
<t tx="ekr.20220525082935.1629">def visit_placeholder_type(self, t: PlaceholderType) -&gt; str:
    return f"&lt;placeholder {t.fullname}&gt;"

</t>
<t tx="ekr.20220525082935.163">def check_fixed_args(self, expr: CallExpr, numargs: int, name: str) -&gt; bool:
    """Verify that expr has specified number of positional args.

    Return True if the arguments are valid.
    """
    s = "s"
    if numargs == 1:
        s = ""
    if len(expr.args) != numargs:
        self.fail('"%s" expects %d argument%s' % (name, numargs, s), expr)
        return False
    if expr.arg_kinds != [ARG_POS] * numargs:
        self.fail(f'"{name}" must be called with {numargs} positional argument{s}', expr)
        return False
    return True

</t>
<t tx="ekr.20220525082935.1630">def visit_type_alias_type(self, t: TypeAliasType) -&gt; str:
    if t.alias is not None:
        unrolled, recursed = t._partial_expansion()
        self.any_as_dots = recursed
        type_str = unrolled.accept(self)
        self.any_as_dots = False
        return type_str
    return "&lt;alias (unfixed)&gt;"

</t>
<t tx="ekr.20220525082935.1631">def visit_unpack_type(self, t: UnpackType) -&gt; str:
    return f"Unpack[{t.type.accept(self)}]"

</t>
<t tx="ekr.20220525082935.1632">def list_str(self, a: Iterable[Type]) -&gt; str:
    """Convert items of an array to strings (pretty-print types)
    and join the results with commas.
    """
    res = []
    for t in a:
        res.append(t.accept(self))
    return ", ".join(res)


</t>
<t tx="ekr.20220525082935.164">def visit_member_expr(self, expr: MemberExpr) -&gt; None:
    base = expr.expr
    base.accept(self)
    if isinstance(base, RefExpr) and isinstance(base.node, MypyFile):
        # Handle module attribute.
        sym = self.get_module_symbol(base.node, expr.name)
        if sym:
            if isinstance(sym.node, PlaceholderNode):
                self.process_placeholder(expr.name, "attribute", expr)
                return
            expr.kind = sym.kind
            expr.fullname = sym.fullname
            expr.node = sym.node
    elif isinstance(base, RefExpr):
        # This branch handles the case C.bar (or cls.bar or self.bar inside
        # a classmethod/method), where C is a class and bar is a type
        # definition or a module resulting from `import bar` (or a module
        # assignment) inside class C. We look up bar in the class' TypeInfo
        # namespace.  This is done only when bar is a module or a type;
        # other things (e.g. methods) are handled by other code in
        # checkmember.
        type_info = None
        if isinstance(base.node, TypeInfo):
            # C.bar where C is a class
            type_info = base.node
        elif isinstance(base.node, Var) and self.type and self.function_stack:
            # check for self.bar or cls.bar in method/classmethod
            func_def = self.function_stack[-1]
            if not func_def.is_static and isinstance(func_def.type, CallableType):
                formal_arg = func_def.type.argument_by_name(base.node.name)
                if formal_arg and formal_arg.pos == 0:
                    type_info = self.type
        elif isinstance(base.node, TypeAlias) and base.node.no_args:
            assert isinstance(base.node.target, ProperType)
            if isinstance(base.node.target, Instance):
                type_info = base.node.target.type

        if type_info:
            n = type_info.names.get(expr.name)
            if n is not None and isinstance(n.node, (MypyFile, TypeInfo, TypeAlias)):
                if not n:
                    return
                expr.kind = n.kind
                expr.fullname = n.fullname
                expr.node = n.node

</t>
<t tx="ekr.20220525082935.165">def visit_op_expr(self, expr: OpExpr) -&gt; None:
    expr.left.accept(self)

    if expr.op in ("and", "or"):
        inferred = infer_condition_value(expr.left, self.options)
        if (inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "and") or (
            inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "or"
        ):
            expr.right_unreachable = True
            return
        elif (inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "and") or (
            inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "or"
        ):
            expr.right_always = True

    expr.right.accept(self)

</t>
<t tx="ekr.20220525082935.166">def visit_comparison_expr(self, expr: ComparisonExpr) -&gt; None:
    for operand in expr.operands:
        operand.accept(self)

</t>
<t tx="ekr.20220525082935.167">def visit_unary_expr(self, expr: UnaryExpr) -&gt; None:
    expr.expr.accept(self)

</t>
<t tx="ekr.20220525082935.168">def visit_index_expr(self, expr: IndexExpr) -&gt; None:
    base = expr.base
    base.accept(self)
    if (
        isinstance(base, RefExpr)
        and isinstance(base.node, TypeInfo)
        and not base.node.is_generic()
    ):
        expr.index.accept(self)
    elif (
        isinstance(base, RefExpr) and isinstance(base.node, TypeAlias)
    ) or refers_to_class_or_function(base):
        # We need to do full processing on every iteration, since some type
        # arguments may contain placeholder types.
        self.analyze_type_application(expr)
    else:
        expr.index.accept(self)

</t>
<t tx="ekr.20220525082935.169">def analyze_type_application(self, expr: IndexExpr) -&gt; None:
    """Analyze special form -- type application (either direct or via type aliasing)."""
    types = self.analyze_type_application_args(expr)
    if types is None:
        return
    base = expr.base
    expr.analyzed = TypeApplication(base, types)
    expr.analyzed.line = expr.line
    expr.analyzed.column = expr.column
    # Types list, dict, set are not subscriptable, prohibit this if
    # subscripted either via type alias...
    if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
        alias = base.node
        target = get_proper_type(alias.target)
        if isinstance(target, Instance):
            name = target.type.fullname
            if (
                alias.no_args
                and name  # this avoids bogus errors for already reported aliases
                in get_nongen_builtins(self.options.python_version)
                and not self.is_stub_file
                and not alias.normalized
            ):
                self.fail(no_subscript_builtin_alias(name, propose_alt=False), expr)
    # ...or directly.
    else:
        n = self.lookup_type_node(base)
        if (
            n
            and n.fullname in get_nongen_builtins(self.options.python_version)
            and not self.is_stub_file
        ):
            self.fail(no_subscript_builtin_alias(n.fullname, propose_alt=False), expr)

</t>
<t tx="ekr.20220525082935.17">def adjust_public_exports(self) -&gt; None:
    """Adjust the module visibility of globals due to __all__."""
    if "__all__" in self.globals:
        for name, g in self.globals.items():
            # Being included in __all__ explicitly exports and makes public.
            if name in self.all_exports:
                g.module_public = True
                g.module_hidden = False
            # But when __all__ is defined, and a symbol is not included in it,
            # it cannot be public.
            else:
                g.module_public = False

</t>
<t tx="ekr.20220525082935.170">def analyze_type_application_args(self, expr: IndexExpr) -&gt; list[Type] | None:
    """Analyze type arguments (index) in a type application.

    Return None if anything was incomplete.
    """
    index = expr.index
    tag = self.track_incomplete_refs()
    self.analyze_type_expr(index)
    if self.found_incomplete_ref(tag):
        return None
    if self.basic_type_applications:
        # Postpone the rest until we have more information (for r.h.s. of an assignment)
        return None
    types: list[Type] = []
    if isinstance(index, TupleExpr):
        items = index.items
        is_tuple = isinstance(expr.base, RefExpr) and expr.base.fullname == "builtins.tuple"
        if is_tuple and len(items) == 2 and isinstance(items[-1], EllipsisExpr):
            items = items[:-1]
    else:
        items = [index]

    # whether param spec literals be allowed here
    # TODO: should this be computed once and passed in?
    #   or is there a better way to do this?
    base = expr.base
    if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
        alias = base.node
        target = get_proper_type(alias.target)
        if isinstance(target, Instance):
            has_param_spec = target.type.has_param_spec_type
            num_args = len(target.type.type_vars)
        else:
            has_param_spec = False
            num_args = -1
    elif isinstance(base, NameExpr) and isinstance(base.node, TypeInfo):
        has_param_spec = base.node.has_param_spec_type
        num_args = len(base.node.type_vars)
    else:
        has_param_spec = False
        num_args = -1

    for item in items:
        try:
            typearg = self.expr_to_unanalyzed_type(item)
        except TypeTranslationError:
            self.fail("Type expected within [...]", expr)
            return None
        # We always allow unbound type variables in IndexExpr, since we
        # may be analysing a type alias definition rvalue. The error will be
        # reported elsewhere if it is not the case.
        analyzed = self.anal_type(
            typearg,
            allow_unbound_tvars=True,
            allow_placeholder=True,
            allow_param_spec_literals=has_param_spec,
        )
        if analyzed is None:
            return None
        types.append(analyzed)

    if has_param_spec and num_args == 1 and len(types) &gt; 0:
        first_arg = get_proper_type(types[0])
        if not (
            len(types) == 1
            and (
                isinstance(first_arg, Parameters)
                or isinstance(first_arg, ParamSpecType)
                or isinstance(first_arg, AnyType)
            )
        ):
            types = [Parameters(types, [ARG_POS] * len(types), [None] * len(types))]

    return types

</t>
<t tx="ekr.20220525082935.171">def visit_slice_expr(self, expr: SliceExpr) -&gt; None:
    if expr.begin_index:
        expr.begin_index.accept(self)
    if expr.end_index:
        expr.end_index.accept(self)
    if expr.stride:
        expr.stride.accept(self)

</t>
<t tx="ekr.20220525082935.172">def visit_cast_expr(self, expr: CastExpr) -&gt; None:
    expr.expr.accept(self)
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20220525082935.173">def visit_assert_type_expr(self, expr: AssertTypeExpr) -&gt; None:
    expr.expr.accept(self)
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20220525082935.174">def visit_reveal_expr(self, expr: RevealExpr) -&gt; None:
    if expr.kind == REVEAL_TYPE:
        if expr.expr is not None:
            expr.expr.accept(self)
    else:
        # Reveal locals doesn't have an inner expression, there's no
        # need to traverse inside it
        pass

</t>
<t tx="ekr.20220525082935.175">def visit_type_application(self, expr: TypeApplication) -&gt; None:
    expr.expr.accept(self)
    for i in range(len(expr.types)):
        analyzed = self.anal_type(expr.types[i])
        if analyzed is not None:
            expr.types[i] = analyzed

</t>
<t tx="ekr.20220525082935.176">def visit_list_comprehension(self, expr: ListComprehension) -&gt; None:
    if any(expr.generator.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    expr.generator.accept(self)

</t>
<t tx="ekr.20220525082935.177">def visit_set_comprehension(self, expr: SetComprehension) -&gt; None:
    if any(expr.generator.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    expr.generator.accept(self)

</t>
<t tx="ekr.20220525082935.178">def visit_dictionary_comprehension(self, expr: DictionaryComprehension) -&gt; None:
    if any(expr.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    with self.enter(expr):
        self.analyze_comp_for(expr)
        expr.key.accept(self)
        expr.value.accept(self)
    self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20220525082935.179">def visit_generator_expr(self, expr: GeneratorExpr) -&gt; None:
    with self.enter(expr):
        self.analyze_comp_for(expr)
        expr.left_expr.accept(self)
    self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20220525082935.18">@contextmanager
def file_context(
    self, file_node: MypyFile, options: Options, active_type: TypeInfo | None = None
) -&gt; Iterator[None]:
    """Configure analyzer for analyzing targets within a file/class.

    Args:
        file_node: target file
        options: options specific to the file
        active_type: must be the surrounding class to analyze method targets
    """
    scope = self.scope
    self.options = options
    self.errors.set_file(file_node.path, file_node.fullname, scope=scope, options=options)
    self.cur_mod_node = file_node
    self.cur_mod_id = file_node.fullname
    with scope.module_scope(self.cur_mod_id):
        self._is_stub_file = file_node.path.lower().endswith(".pyi")
        self._is_typeshed_stub_file = is_typeshed_file(
            options.abs_custom_typeshed_dir, file_node.path
        )
        self.globals = file_node.names
        self.tvar_scope = TypeVarLikeScope()

        self.named_tuple_analyzer = NamedTupleAnalyzer(options, self)
        self.typed_dict_analyzer = TypedDictAnalyzer(options, self, self.msg)
        self.enum_call_analyzer = EnumCallAnalyzer(options, self)
        self.newtype_analyzer = NewTypeAnalyzer(options, self, self.msg)

        # Counter that keeps track of references to undefined things potentially caused by
        # incomplete namespaces.
        self.num_incomplete_refs = 0

        if active_type:
            self.incomplete_type_stack.append(False)
            scope.enter_class(active_type)
            self.enter_class(active_type.defn.info)
            for tvar in active_type.defn.type_vars:
                self.tvar_scope.bind_existing(tvar)

        yield

        if active_type:
            scope.leave_class()
            self.leave_class()
            self.type = None
            self.incomplete_type_stack.pop()
    del self.options

</t>
<t tx="ekr.20220525082935.180">def analyze_comp_for(self, expr: GeneratorExpr | DictionaryComprehension) -&gt; None:
    """Analyses the 'comp_for' part of comprehensions (part 1).

    That is the part after 'for' in (x for x in l if p). This analyzes
    variables and conditions which are analyzed in a local scope.
    """
    for i, (index, sequence, conditions) in enumerate(
        zip(expr.indices, expr.sequences, expr.condlists)
    ):
        if i &gt; 0:
            sequence.accept(self)
        # Bind index variables.
        self.analyze_lvalue(index)
        for cond in conditions:
            cond.accept(self)

</t>
<t tx="ekr.20220525082935.181">def analyze_comp_for_2(self, expr: GeneratorExpr | DictionaryComprehension) -&gt; None:
    """Analyses the 'comp_for' part of comprehensions (part 2).

    That is the part after 'for' in (x for x in l if p). This analyzes
    the 'l' part which is analyzed in the surrounding scope.
    """
    expr.sequences[0].accept(self)

</t>
<t tx="ekr.20220525082935.182">def visit_lambda_expr(self, expr: LambdaExpr) -&gt; None:
    self.analyze_arg_initializers(expr)
    self.analyze_function_body(expr)

</t>
<t tx="ekr.20220525082935.183">def visit_conditional_expr(self, expr: ConditionalExpr) -&gt; None:
    expr.if_expr.accept(self)
    expr.cond.accept(self)
    expr.else_expr.accept(self)

</t>
<t tx="ekr.20220525082935.184"></t>
<t tx="ekr.20220525082935.185">def visit__promote_expr(self, expr: PromoteExpr) -&gt; None:
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20220525082935.186">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"yield" outside function', e, serious=True, blocker=True)
    elif self.is_comprehension_stack[-1]:
        self.fail(
            '"yield" inside comprehension or generator expression',
            e,
            serious=True,
            blocker=True,
        )
    elif self.function_stack[-1].is_coroutine:
        if self.options.python_version &lt; (3, 6):
            self.fail('"yield" in async function', e, serious=True, blocker=True)
        else:
            self.function_stack[-1].is_generator = True
            self.function_stack[-1].is_async_generator = True
    else:
        self.function_stack[-1].is_generator = True
    if e.expr:
        e.expr.accept(self)

</t>
<t tx="ekr.20220525082935.187">def visit_await_expr(self, expr: AwaitExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"await" outside function', expr)
    elif not self.function_stack[-1].is_coroutine:
        self.fail('"await" outside coroutine ("async def")', expr)
    expr.expr.accept(self)

</t>
<t tx="ekr.20220525082935.188">#
# Patterns
#

</t>
<t tx="ekr.20220525082935.189">def visit_as_pattern(self, p: AsPattern) -&gt; None:
    if p.pattern is not None:
        p.pattern.accept(self)
    if p.name is not None:
        self.analyze_lvalue(p.name)

</t>
<t tx="ekr.20220525082935.19">#
# Functions
#

</t>
<t tx="ekr.20220525082935.190">def visit_or_pattern(self, p: OrPattern) -&gt; None:
    for pattern in p.patterns:
        pattern.accept(self)

</t>
<t tx="ekr.20220525082935.191">def visit_value_pattern(self, p: ValuePattern) -&gt; None:
    p.expr.accept(self)

</t>
<t tx="ekr.20220525082935.192">def visit_sequence_pattern(self, p: SequencePattern) -&gt; None:
    for pattern in p.patterns:
        pattern.accept(self)

</t>
<t tx="ekr.20220525082935.193">def visit_starred_pattern(self, p: StarredPattern) -&gt; None:
    if p.capture is not None:
        self.analyze_lvalue(p.capture)

</t>
<t tx="ekr.20220525082935.194">def visit_mapping_pattern(self, p: MappingPattern) -&gt; None:
    for key in p.keys:
        key.accept(self)
    for value in p.values:
        value.accept(self)
    if p.rest is not None:
        self.analyze_lvalue(p.rest)

</t>
<t tx="ekr.20220525082935.195">def visit_class_pattern(self, p: ClassPattern) -&gt; None:
    p.class_ref.accept(self)
    for pos in p.positionals:
        pos.accept(self)
    for v in p.keyword_values:
        v.accept(self)

</t>
<t tx="ekr.20220525082935.196">#
# Lookup functions
#

</t>
<t tx="ekr.20220525082935.197">def lookup(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    """Look up an unqualified (no dots) name in all active namespaces.

    Note that the result may contain a PlaceholderNode. The caller may
    want to defer in that case.

    Generate an error if the name is not defined unless suppress_errors
    is true or the current namespace is incomplete. In the latter case
    defer.
    """
    implicit_name = False
    # 1a. Name declared using 'global x' takes precedence
    if name in self.global_decls[-1]:
        if name in self.globals:
            return self.globals[name]
        if not suppress_errors:
            self.name_not_defined(name, ctx)
        return None
    # 1b. Name declared using 'nonlocal x' takes precedence
    if name in self.nonlocal_decls[-1]:
        for table in reversed(self.locals[:-1]):
            if table is not None and name in table:
                return table[name]
        if not suppress_errors:
            self.name_not_defined(name, ctx)
        return None
    # 2. Class attributes (if within class definition)
    if self.type and not self.is_func_scope() and name in self.type.names:
        node = self.type.names[name]
        if not node.implicit:
            if self.is_active_symbol_in_class_body(node.node):
                return node
        else:
            # Defined through self.x assignment
            implicit_name = True
            implicit_node = node
    # 3. Local (function) scopes
    for table in reversed(self.locals):
        if table is not None and name in table:
            return table[name]
    # 4. Current file global scope
    if name in self.globals:
        return self.globals[name]
    # 5. Builtins
    b = self.globals.get("__builtins__", None)
    if b:
        assert isinstance(b.node, MypyFile)
        table = b.node.names
        if name in table:
            if len(name) &gt; 1 and name[0] == "_" and name[1] != "_":
                if not suppress_errors:
                    self.name_not_defined(name, ctx)
                return None
            node = table[name]
            return node
    # Give up.
    if not implicit_name and not suppress_errors:
        self.name_not_defined(name, ctx)
    else:
        if implicit_name:
            return implicit_node
    return None

</t>
<t tx="ekr.20220525082935.198">def is_active_symbol_in_class_body(self, node: SymbolNode | None) -&gt; bool:
    """Can a symbol defined in class body accessed at current statement?

    Only allow access to class attributes textually after
    the definition, so that it's possible to fall back to the
    outer scope. Example:

        class X: ...

        class C:
            X = X  # Initializer refers to outer scope

    Nested classes are an exception, since we want to support
    arbitrary forward references in type annotations. Also, we
    allow forward references to type aliases to support recursive
    types.
    """
    # TODO: Forward reference to name imported in class body is not
    #       caught.
    if self.statement is None:
        # Assume it's fine -- don't have enough context to check
        return True
    return (
        node is None
        or self.is_textually_before_statement(node)
        or not self.is_defined_in_current_module(node.fullname)
        or isinstance(node, (TypeInfo, TypeAlias))
        or (isinstance(node, PlaceholderNode) and node.becomes_typeinfo)
    )

</t>
<t tx="ekr.20220525082935.199">def is_textually_before_statement(self, node: SymbolNode) -&gt; bool:
    """Check if a node is defined textually before the current statement

    Note that decorated functions' line number are the same as
    the top decorator.
    """
    assert self.statement
    line_diff = self.statement.line - node.line

    # The first branch handles reference an overloaded function variant inside itself,
    # this is a corner case where mypy technically deviates from runtime name resolution,
    # but it is fine because we want an overloaded function to be treated as a single unit.
    if self.is_overloaded_item(node, self.statement):
        return False
    elif isinstance(node, Decorator) and not node.is_overload:
        return line_diff &gt; len(node.original_decorators)
    else:
        return line_diff &gt; 0

</t>
<t tx="ekr.20220525082935.2">def __init__(
    self,
    modules: dict[str, MypyFile],
    missing_modules: set[str],
    incomplete_namespaces: set[str],
    errors: Errors,
    plugin: Plugin,
) -&gt; None:
    """Construct semantic analyzer.

    We reuse the same semantic analyzer instance across multiple modules.

    Args:
        modules: Global modules dictionary
        missing_modules: Modules that could not be imported encountered so far
        incomplete_namespaces: Namespaces that are being populated during semantic analysis
            (can contain modules and classes within the current SCC; mutated by the caller)
        errors: Report analysis errors using this instance
    """
    self.locals = [None]
    self.is_comprehension_stack = [False]
    # Saved namespaces from previous iteration. Every top-level function/method body is
    # analyzed in several iterations until all names are resolved. We need to save
    # the local namespaces for the top level function and all nested functions between
    # these iterations. See also semanal_main.process_top_level_function().
    self.saved_locals: dict[
        FuncItem | GeneratorExpr | DictionaryComprehension, SymbolTable
    ] = {}
    self.imports = set()
    self.type = None
    self.type_stack = []
    # Are the namespaces of classes being processed complete?
    self.incomplete_type_stack: list[bool] = []
    self.tvar_scope = TypeVarLikeScope()
    self.function_stack = []
    self.block_depth = [0]
    self.loop_depth = 0
    self.errors = errors
    self.modules = modules
    self.msg = MessageBuilder(errors, modules)
    self.missing_modules = missing_modules
    self.missing_names = [set()]
    # These namespaces are still in process of being populated. If we encounter a
    # missing name in these namespaces, we need to defer the current analysis target,
    # since it's possible that the name will be there once the namespace is complete.
    self.incomplete_namespaces = incomplete_namespaces
    self.all_exports: list[str] = []
    # Map from module id to list of explicitly exported names (i.e. names in __all__).
    self.export_map: dict[str, list[str]] = {}
    self.plugin = plugin
    # If True, process function definitions. If False, don't. This is used
    # for processing module top levels in fine-grained incremental mode.
    self.recurse_into_functions = True
    self.scope = Scope()

    # Trace line numbers for every file where deferral happened during analysis of
    # current SCC or top-level function.
    self.deferral_debug_context: list[tuple[str, int]] = []

    # This is needed to properly support recursive type aliases. The problem is that
    # Foo[Bar] could mean three things depending on context: a target for type alias,
    # a normal index expression (including enum index), or a type application.
    # The latter is particularly problematic as it can falsely create incomplete
    # refs while analysing rvalues of type aliases. To avoid this we first analyse
    # rvalues while temporarily setting this to True.
    self.basic_type_applications = False

</t>
<t tx="ekr.20220525082935.20">def visit_func_def(self, defn: FuncDef) -&gt; None:
    self.statement = defn

    # Visit default values because they may contain assignment expressions.
    for arg in defn.arguments:
        if arg.initializer:
            arg.initializer.accept(self)

    defn.is_conditional = self.block_depth[-1] &gt; 0

    # Set full names even for those definitions that aren't added
    # to a symbol table. For example, for overload items.
    defn._fullname = self.qualified_name(defn.name)

    # We don't add module top-level functions to symbol tables
    # when we analyze their bodies in the second phase on analysis,
    # since they were added in the first phase. Nested functions
    # get always added, since they aren't separate targets.
    if not self.recurse_into_functions or len(self.function_stack) &gt; 0:
        if not defn.is_decorated and not defn.is_overload:
            self.add_function_to_symbol_table(defn)

    if not self.recurse_into_functions:
        return

    with self.scope.function_scope(defn):
        self.analyze_func_def(defn)

</t>
<t tx="ekr.20220525082935.200">def is_overloaded_item(self, node: SymbolNode, statement: Statement) -&gt; bool:
    """Check whether the function belongs to the overloaded variants"""
    if isinstance(node, OverloadedFuncDef) and isinstance(statement, FuncDef):
        in_items = statement in {
            item.func if isinstance(item, Decorator) else item for item in node.items
        }
        in_impl = node.impl is not None and (
            (isinstance(node.impl, Decorator) and statement is node.impl.func)
            or statement is node.impl
        )
        return in_items or in_impl
    return False

</t>
<t tx="ekr.20220525082935.201">def is_defined_in_current_module(self, fullname: str | None) -&gt; bool:
    if fullname is None:
        return False
    return module_prefix(self.modules, fullname) == self.cur_mod_id

</t>
<t tx="ekr.20220525082935.202">def lookup_qualified(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    """Lookup a qualified name in all activate namespaces.

    Note that the result may contain a PlaceholderNode. The caller may
    want to defer in that case.

    Generate an error if the name is not defined unless suppress_errors
    is true or the current namespace is incomplete. In the latter case
    defer.
    """
    if "." not in name:
        # Simple case: look up a short name.
        return self.lookup(name, ctx, suppress_errors=suppress_errors)
    parts = name.split(".")
    namespace = self.cur_mod_id
    sym = self.lookup(parts[0], ctx, suppress_errors=suppress_errors)
    if sym:
        for i in range(1, len(parts)):
            node = sym.node
            part = parts[i]
            if isinstance(node, TypeInfo):
                nextsym = node.get(part)
            elif isinstance(node, MypyFile):
                nextsym = self.get_module_symbol(node, part)
                namespace = node.fullname
            elif isinstance(node, PlaceholderNode):
                return sym
            elif isinstance(node, TypeAlias) and node.no_args:
                assert isinstance(node.target, ProperType)
                if isinstance(node.target, Instance):
                    nextsym = node.target.type.get(part)
                else:
                    nextsym = None
            else:
                if isinstance(node, Var):
                    typ = get_proper_type(node.type)
                    if isinstance(typ, AnyType):
                        # Allow access through Var with Any type without error.
                        return self.implicit_symbol(sym, name, parts[i:], typ)
                # This might be something like valid `P.args` or invalid `P.__bound__` access.
                # Important note that `ParamSpecExpr` is also ignored in other places.
                # See https://github.com/python/mypy/pull/13468
                if isinstance(node, ParamSpecExpr) and part in ("args", "kwargs"):
                    return None
                # Lookup through invalid node, such as variable or function
                nextsym = None
            if not nextsym or nextsym.module_hidden:
                if not suppress_errors:
                    self.name_not_defined(name, ctx, namespace=namespace)
                return None
            sym = nextsym
    return sym

</t>
<t tx="ekr.20220525082935.203">def lookup_type_node(self, expr: Expression) -&gt; SymbolTableNode | None:
    try:
        t = self.expr_to_unanalyzed_type(expr)
    except TypeTranslationError:
        return None
    if isinstance(t, UnboundType):
        n = self.lookup_qualified(t.name, expr, suppress_errors=True)
        return n
    return None

</t>
<t tx="ekr.20220525082935.204">def get_module_symbol(self, node: MypyFile, name: str) -&gt; SymbolTableNode | None:
    """Look up a symbol from a module.

    Return None if no matching symbol could be bound.
    """
    module = node.fullname
    names = node.names
    sym = names.get(name)
    if not sym:
        fullname = module + "." + name
        if fullname in self.modules:
            sym = SymbolTableNode(GDEF, self.modules[fullname])
        elif self.is_incomplete_namespace(module):
            self.record_incomplete_ref()
        elif "__getattr__" in names and (
            node.is_stub or self.options.python_version &gt;= (3, 7)
        ):
            gvar = self.create_getattr_var(names["__getattr__"], name, fullname)
            if gvar:
                sym = SymbolTableNode(GDEF, gvar)
        elif self.is_missing_module(fullname):
            # We use the fullname of the original definition so that we can
            # detect whether two names refer to the same thing.
            var_type = AnyType(TypeOfAny.from_unimported_type)
            v = Var(name, type=var_type)
            v._fullname = fullname
            sym = SymbolTableNode(GDEF, v)
    elif sym.module_hidden:
        sym = None
    return sym

</t>
<t tx="ekr.20220525082935.205">def is_missing_module(self, module: str) -&gt; bool:
    return module in self.missing_modules

</t>
<t tx="ekr.20220525082935.206">def implicit_symbol(
    self, sym: SymbolTableNode, name: str, parts: list[str], source_type: AnyType
) -&gt; SymbolTableNode:
    """Create symbol for a qualified name reference through Any type."""
    if sym.node is None:
        basename = None
    else:
        basename = sym.node.fullname
    if basename is None:
        fullname = name
    else:
        fullname = basename + "." + ".".join(parts)
    var_type = AnyType(TypeOfAny.from_another_any, source_type)
    var = Var(parts[-1], var_type)
    var._fullname = fullname
    return SymbolTableNode(GDEF, var)

</t>
<t tx="ekr.20220525082935.207">def create_getattr_var(
    self, getattr_defn: SymbolTableNode, name: str, fullname: str
) -&gt; Var | None:
    """Create a dummy variable using module-level __getattr__ return type.

    If not possible, return None.

    Note that multiple Var nodes can be created for a single name. We
    can use the from_module_getattr and the fullname attributes to
    check if two dummy Var nodes refer to the same thing. Reusing Var
    nodes would require non-local mutable state, which we prefer to
    avoid.
    """
    if isinstance(getattr_defn.node, (FuncDef, Var)):
        node_type = get_proper_type(getattr_defn.node.type)
        if isinstance(node_type, CallableType):
            typ = node_type.ret_type
        else:
            typ = AnyType(TypeOfAny.from_error)
        v = Var(name, type=typ)
        v._fullname = fullname
        v.from_module_getattr = True
        return v
    return None

</t>
<t tx="ekr.20220525082935.208">def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode:
    ret = self.lookup_fully_qualified_or_none(fullname)
    assert ret is not None, fullname
    return ret

</t>
<t tx="ekr.20220525082935.209">def lookup_fully_qualified_or_none(self, fullname: str) -&gt; SymbolTableNode | None:
    """Lookup a fully qualified name that refers to a module-level definition.

    Don't assume that the name is defined. This happens in the global namespace --
    the local module namespace is ignored. This does not dereference indirect
    refs.

    Note that this can't be used for names nested in class namespaces.
    """
    # TODO: unify/clean-up/simplify lookup methods, see #4157.
    # TODO: support nested classes (but consider performance impact,
    #       we might keep the module level only lookup for thing like 'builtins.int').
    assert "." in fullname
    module, name = fullname.rsplit(".", maxsplit=1)
    if module not in self.modules:
        return None
    filenode = self.modules[module]
    result = filenode.names.get(name)
    if result is None and self.is_incomplete_namespace(module):
        # TODO: More explicit handling of incomplete refs?
        self.record_incomplete_ref()
    return result

</t>
<t tx="ekr.20220525082935.21">def analyze_func_def(self, defn: FuncDef) -&gt; None:
    self.function_stack.append(defn)

    if defn.type:
        assert isinstance(defn.type, CallableType)
        self.update_function_type_variables(defn.type, defn)
    self.function_stack.pop()

    if self.is_class_scope():
        # Method definition
        assert self.type is not None
        defn.info = self.type
        if defn.type is not None and defn.name in ("__init__", "__init_subclass__"):
            assert isinstance(defn.type, CallableType)
            if isinstance(get_proper_type(defn.type.ret_type), AnyType):
                defn.type = defn.type.copy_modified(ret_type=NoneType())
        self.prepare_method_signature(defn, self.type)

    # Analyze function signature
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        if defn.type:
            self.check_classvar_in_signature(defn.type)
            assert isinstance(defn.type, CallableType)
            # Signature must be analyzed in the surrounding scope so that
            # class-level imported names and type variables are in scope.
            analyzer = self.type_analyzer()
            tag = self.track_incomplete_refs()
            result = analyzer.visit_callable_type(defn.type, nested=False)
            # Don't store not ready types (including placeholders).
            if self.found_incomplete_ref(tag) or has_placeholder(result):
                self.defer(defn)
                return
            assert isinstance(result, ProperType)
            if isinstance(result, CallableType):
                result = self.remove_unpack_kwargs(defn, result)
            defn.type = result
            self.add_type_alias_deps(analyzer.aliases_used)
            self.check_function_signature(defn)
            if isinstance(defn, FuncDef):
                assert isinstance(defn.type, CallableType)
                defn.type = set_callable_name(defn.type, defn)

    self.analyze_arg_initializers(defn)
    self.analyze_function_body(defn)

    if self.is_class_scope():
        assert self.type is not None
        # Mark protocol methods with empty bodies as implicitly abstract.
        # This makes explicit protocol subclassing type-safe.
        if (
            self.type.is_protocol
            and not self.is_stub_file  # Bodies in stub files are always empty.
            and (not isinstance(self.scope.function, OverloadedFuncDef) or defn.is_property)
            and defn.abstract_status != IS_ABSTRACT
            and is_trivial_body(defn.body)
        ):
            defn.abstract_status = IMPLICITLY_ABSTRACT
        if (
            is_trivial_body(defn.body)
            and not self.is_stub_file
            and defn.abstract_status != NOT_ABSTRACT
        ):
            defn.is_trivial_body = True

    if (
        defn.is_coroutine
        and isinstance(defn.type, CallableType)
        and self.wrapped_coro_return_types.get(defn) != defn.type
    ):
        if defn.is_async_generator:
            # Async generator types are handled elsewhere
            pass
        else:
            # A coroutine defined as `async def foo(...) -&gt; T: ...`
            # has external return type `Coroutine[Any, Any, T]`.
            any_type = AnyType(TypeOfAny.special_form)
            ret_type = self.named_type_or_none(
                "typing.Coroutine", [any_type, any_type, defn.type.ret_type]
            )
            assert ret_type is not None, "Internal error: typing.Coroutine not found"
            defn.type = defn.type.copy_modified(ret_type=ret_type)
            self.wrapped_coro_return_types[defn] = defn.type

def remove_unpack_kwargs(self, defn: FuncDef, typ: CallableType) -&gt; CallableType:
    if not typ.arg_kinds or typ.arg_kinds[-1] is not ArgKind.ARG_STAR2:
        return typ
    last_type = get_proper_type(typ.arg_types[-1])
    if not isinstance(last_type, UnpackType):
        return typ
    last_type = get_proper_type(last_type.type)
    if not isinstance(last_type, TypedDictType):
        self.fail("Unpack item in ** argument must be a TypedDict", defn)
        new_arg_types = typ.arg_types[:-1] + [AnyType(TypeOfAny.from_error)]
        return typ.copy_modified(arg_types=new_arg_types)
    overlap = set(typ.arg_names) &amp; set(last_type.items)
    # It is OK for TypedDict to have a key named 'kwargs'.
    overlap.discard(typ.arg_names[-1])
    if overlap:
        overlapped = ", ".join([f'"{name}"' for name in overlap])
        self.fail(f"Overlap between argument names and ** TypedDict items: {overlapped}", defn)
        new_arg_types = typ.arg_types[:-1] + [AnyType(TypeOfAny.from_error)]
        return typ.copy_modified(arg_types=new_arg_types)
    # OK, everything looks right now, mark the callable type as using unpack.
    new_arg_types = typ.arg_types[:-1] + [last_type]
    return typ.copy_modified(arg_types=new_arg_types, unpack_kwargs=True)

</t>
<t tx="ekr.20220525082935.210">def object_type(self) -&gt; Instance:
    return self.named_type("builtins.object")

</t>
<t tx="ekr.20220525082935.211">def str_type(self) -&gt; Instance:
    return self.named_type("builtins.str")

</t>
<t tx="ekr.20220525082935.212">def named_type(self, fullname: str, args: list[Type] | None = None) -&gt; Instance:
    sym = self.lookup_fully_qualified(fullname)
    assert sym, "Internal error: attempted to construct unknown type"
    node = sym.node
    assert isinstance(node, TypeInfo)
    if args:
        # TODO: assert len(args) == len(node.defn.type_vars)
        return Instance(node, args)
    return Instance(node, [AnyType(TypeOfAny.special_form)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20220525082935.213">def named_type_or_none(self, fullname: str, args: list[Type] | None = None) -&gt; Instance | None:
    sym = self.lookup_fully_qualified_or_none(fullname)
    if not sym or isinstance(sym.node, PlaceholderNode):
        return None
    node = sym.node
    if isinstance(node, TypeAlias):
        assert isinstance(node.target, Instance)  # type: ignore[misc]
        node = node.target.type
    assert isinstance(node, TypeInfo), node
    if args is not None:
        # TODO: assert len(args) == len(node.defn.type_vars)
        return Instance(node, args)
    return Instance(node, [AnyType(TypeOfAny.unannotated)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20220525082935.214">def builtin_type(self, fully_qualified_name: str) -&gt; Instance:
    """Legacy function -- use named_type() instead."""
    return self.named_type(fully_qualified_name)

</t>
<t tx="ekr.20220525082935.215">def lookup_current_scope(self, name: str) -&gt; SymbolTableNode | None:
    if self.locals[-1] is not None:
        return self.locals[-1].get(name)
    elif self.type is not None:
        return self.type.names.get(name)
    else:
        return self.globals.get(name)

</t>
<t tx="ekr.20220525082935.216">#
# Adding symbols
#

</t>
<t tx="ekr.20220525082935.217">def add_symbol(
    self,
    name: str,
    node: SymbolNode,
    context: Context,
    module_public: bool = True,
    module_hidden: bool = False,
    can_defer: bool = True,
    escape_comprehensions: bool = False,
) -&gt; bool:
    """Add symbol to the currently active symbol table.

    Generally additions to symbol table should go through this method or
    one of the methods below so that kinds, redefinitions, conditional
    definitions, and skipped names are handled consistently.

    Return True if we actually added the symbol, or False if we refused to do so
    (because something is not ready).

    If can_defer is True, defer current target if adding a placeholder.
    """
    if self.is_func_scope():
        kind = LDEF
    elif self.type is not None:
        kind = MDEF
    else:
        kind = GDEF
    symbol = SymbolTableNode(
        kind, node, module_public=module_public, module_hidden=module_hidden
    )
    return self.add_symbol_table_node(name, symbol, context, can_defer, escape_comprehensions)

</t>
<t tx="ekr.20220525082935.218">def add_symbol_skip_local(self, name: str, node: SymbolNode) -&gt; None:
    """Same as above, but skipping the local namespace.

    This doesn't check for previous definition and is only used
    for serialization of method-level classes.

    Classes defined within methods can be exposed through an
    attribute type, but method-level symbol tables aren't serialized.
    This method can be used to add such classes to an enclosing,
    serialized symbol table.
    """
    # TODO: currently this is only used by named tuples and typed dicts.
    # Use this method also by normal classes, see issue #6422.
    if self.type is not None:
        names = self.type.names
        kind = MDEF
    else:
        names = self.globals
        kind = GDEF
    symbol = SymbolTableNode(kind, node)
    names[name] = symbol

</t>
<t tx="ekr.20220525082935.219">def add_symbol_table_node(
    self,
    name: str,
    symbol: SymbolTableNode,
    context: Context | None = None,
    can_defer: bool = True,
    escape_comprehensions: bool = False,
) -&gt; bool:
    """Add symbol table node to the currently active symbol table.

    Return True if we actually added the symbol, or False if we refused
    to do so (because something is not ready or it was a no-op).

    Generate an error if there is an invalid redefinition.

    If context is None, unconditionally add node, since we can't report
    an error. Note that this is used by plugins to forcibly replace nodes!

    TODO: Prevent plugins from replacing nodes, as it could cause problems?

    Args:
        name: short name of symbol
        symbol: Node to add
        can_defer: if True, defer current target if adding a placeholder
        context: error context (see above about None value)
    """
    names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
    existing = names.get(name)
    if isinstance(symbol.node, PlaceholderNode) and can_defer:
        if context is not None:
            self.process_placeholder(name, "name", context)
        else:
            # see note in docstring describing None contexts
            self.defer()
    if (
        existing is not None
        and context is not None
        and not is_valid_replacement(existing, symbol)
    ):
        # There is an existing node, so this may be a redefinition.
        # If the new node points to the same node as the old one,
        # or if both old and new nodes are placeholders, we don't
        # need to do anything.
        old = existing.node
        new = symbol.node
        if isinstance(new, PlaceholderNode):
            # We don't know whether this is okay. Let's wait until the next iteration.
            return False
        if not is_same_symbol(old, new):
            if isinstance(new, (FuncDef, Decorator, OverloadedFuncDef, TypeInfo)):
                self.add_redefinition(names, name, symbol)
            if not (isinstance(new, (FuncDef, Decorator)) and self.set_original_def(old, new)):
                self.name_already_defined(name, context, existing)
    elif name not in self.missing_names[-1] and "*" not in self.missing_names[-1]:
        names[name] = symbol
        self.progress = True
        return True
    return False

</t>
<t tx="ekr.20220525082935.22">def prepare_method_signature(self, func: FuncDef, info: TypeInfo) -&gt; None:
    """Check basic signature validity and tweak annotation of self/cls argument."""
    # Only non-static methods are special.
    functype = func.type
    if not func.is_static:
        if func.name in ["__init_subclass__", "__class_getitem__"]:
            func.is_class = True
        if not func.arguments:
            self.fail("Method must have at least one argument", func)
        elif isinstance(functype, CallableType):
            self_type = get_proper_type(functype.arg_types[0])
            if isinstance(self_type, AnyType):
                leading_type: Type = fill_typevars(info)
                if func.is_class or func.name == "__new__":
                    leading_type = self.class_type(leading_type)
                func.type = replace_implicit_first_type(functype, leading_type)

</t>
<t tx="ekr.20220525082935.220">def add_redefinition(self, names: SymbolTable, name: str, symbol: SymbolTableNode) -&gt; None:
    """Add a symbol table node that reflects a redefinition as a function or a class.

    Redefinitions need to be added to the symbol table so that they can be found
    through AST traversal, but they have dummy names of form 'name-redefinition[N]',
    where N ranges over 2, 3, ... (omitted for the first redefinition).

    Note: we always store redefinitions independently of whether they are valid or not
    (so they will be semantically analyzed), the caller should give an error for invalid
    redefinitions (such as e.g. variable redefined as a class).
    """
    i = 1
    # Don't serialize redefined nodes. They are likely to have
    # busted internal references which can cause problems with
    # serialization and they can't have any external references to
    # them.
    symbol.no_serialize = True
    while True:
        if i == 1:
            new_name = f"{name}-redefinition"
        else:
            new_name = f"{name}-redefinition{i}"
        existing = names.get(new_name)
        if existing is None:
            names[new_name] = symbol
            return
        elif existing.node is symbol.node:
            # Already there
            return
        i += 1

</t>
<t tx="ekr.20220525082935.221">def add_local(self, node: Var | FuncDef | OverloadedFuncDef, context: Context) -&gt; None:
    """Add local variable or function."""
    assert self.is_func_scope()
    name = node.name
    node._fullname = name
    self.add_symbol(name, node, context)

</t>
<t tx="ekr.20220525082935.222">def add_module_symbol(
    self, id: str, as_id: str, context: Context, module_public: bool, module_hidden: bool
) -&gt; None:
    """Add symbol that is a reference to a module object."""
    if id in self.modules:
        node = self.modules[id]
        self.add_symbol(
            as_id, node, context, module_public=module_public, module_hidden=module_hidden
        )
    else:
        self.add_unknown_imported_symbol(
            as_id,
            context,
            target_name=id,
            module_public=module_public,
            module_hidden=module_hidden,
        )

</t>
<t tx="ekr.20220525082935.223">def _get_node_for_class_scoped_import(
    self, name: str, symbol_node: SymbolNode | None, context: Context
) -&gt; SymbolNode | None:
    if symbol_node is None:
        return None
    # I promise this type checks; I'm just making mypyc issues go away.
    # mypyc is absolutely convinced that `symbol_node` narrows to a Var in the following,
    # when it can also be a FuncBase. Once fixed, `f` in the following can be removed.
    # See also https://github.com/mypyc/mypyc/issues/892
    f = cast(Any, lambda x: x)
    if isinstance(f(symbol_node), (Decorator, FuncBase, Var)):
        # For imports in class scope, we construct a new node to represent the symbol and
        # set its `info` attribute to `self.type`.
        existing = self.current_symbol_table().get(name)
        if (
            # The redefinition checks in `add_symbol_table_node` don't work for our
            # constructed Var / FuncBase, so check for possible redefinitions here.
            existing is not None
            and isinstance(f(existing.node), (Decorator, FuncBase, Var))
            and (
                isinstance(f(existing.type), f(AnyType))
                or f(existing.type) == f(symbol_node).type
            )
        ):
            return existing.node

        # Construct the new node
        if isinstance(f(symbol_node), (FuncBase, Decorator)):
            # In theory we could construct a new node here as well, but in practice
            # it doesn't work well, see #12197
            typ: Type | None = AnyType(TypeOfAny.from_error)
            self.fail("Unsupported class scoped import", context)
        else:
            typ = f(symbol_node).type
        symbol_node = Var(name, typ)
        symbol_node._fullname = self.qualified_name(name)
        assert self.type is not None  # guaranteed by is_class_scope
        symbol_node.info = self.type
        symbol_node.line = context.line
        symbol_node.column = context.column
    return symbol_node

</t>
<t tx="ekr.20220525082935.224">def add_imported_symbol(
    self,
    name: str,
    node: SymbolTableNode,
    context: Context,
    module_public: bool,
    module_hidden: bool,
) -&gt; None:
    """Add an alias to an existing symbol through import."""
    assert not module_hidden or not module_public

    symbol_node: SymbolNode | None = node.node

    if self.is_class_scope():
        symbol_node = self._get_node_for_class_scoped_import(name, symbol_node, context)

    symbol = SymbolTableNode(
        node.kind, symbol_node, module_public=module_public, module_hidden=module_hidden
    )
    self.add_symbol_table_node(name, symbol, context)

</t>
<t tx="ekr.20220525082935.225">def add_unknown_imported_symbol(
    self,
    name: str,
    context: Context,
    target_name: str | None,
    module_public: bool,
    module_hidden: bool,
) -&gt; None:
    """Add symbol that we don't know what it points to because resolving an import failed.

    This can happen if a module is missing, or it is present, but doesn't have
    the imported attribute. The `target_name` is the name of symbol in the namespace
    it is imported from. For example, for 'from mod import x as y' the target_name is
    'mod.x'. This is currently used only to track logical dependencies.
    """
    existing = self.current_symbol_table().get(name)
    if existing and isinstance(existing.node, Var) and existing.node.is_suppressed_import:
        # This missing import was already added -- nothing to do here.
        return
    var = Var(name)
    if self.options.logical_deps and target_name is not None:
        # This makes it possible to add logical fine-grained dependencies
        # from a missing module. We can't use this by default, since in a
        # few places we assume that the full name points to a real
        # definition, but this name may point to nothing.
        var._fullname = target_name
    elif self.type:
        var._fullname = self.type.fullname + "." + name
        var.info = self.type
    else:
        var._fullname = self.qualified_name(name)
    var.is_ready = True
    any_type = AnyType(TypeOfAny.from_unimported_type, missing_import_name=var._fullname)
    var.type = any_type
    var.is_suppressed_import = True
    self.add_symbol(
        name, var, context, module_public=module_public, module_hidden=module_hidden
    )

</t>
<t tx="ekr.20220525082935.226">#
# Other helpers
#

</t>
<t tx="ekr.20220525082935.227">@contextmanager
def tvar_scope_frame(self, frame: TypeVarLikeScope) -&gt; Iterator[None]:
    old_scope = self.tvar_scope
    self.tvar_scope = frame
    yield
    self.tvar_scope = old_scope

</t>
<t tx="ekr.20220525082935.228">def defer(self, debug_context: Context | None = None, force_progress: bool = False) -&gt; None:
    """Defer current analysis target to be analyzed again.

    This must be called if something in the current target is
    incomplete or has a placeholder node. However, this must *not*
    be called during the final analysis iteration! Instead, an error
    should be generated. Often 'process_placeholder' is a good
    way to either defer or generate an error.

    NOTE: Some methods, such as 'anal_type', 'mark_incomplete' and
          'record_incomplete_ref', call this implicitly, or when needed.
          They are usually preferable to a direct defer() call.
    """
    assert not self.final_iteration, "Must not defer during final iteration"
    if force_progress:
        # Usually, we report progress if we have replaced a placeholder node
        # with an actual valid node. However, sometimes we need to update an
        # existing node *in-place*. For example, this is used by type aliases
        # in context of forward references and/or recursive aliases, and in
        # similar situations (recursive named tuples etc).
        self.progress = True
    self.deferred = True
    # Store debug info for this deferral.
    line = (
        debug_context.line if debug_context else self.statement.line if self.statement else -1
    )
    self.deferral_debug_context.append((self.cur_mod_id, line))

</t>
<t tx="ekr.20220525082935.229">def track_incomplete_refs(self) -&gt; Tag:
    """Return tag that can be used for tracking references to incomplete names."""
    return self.num_incomplete_refs

</t>
<t tx="ekr.20220525082935.23">def set_original_def(self, previous: Node | None, new: FuncDef | Decorator) -&gt; bool:
    """If 'new' conditionally redefine 'previous', set 'previous' as original

    We reject straight redefinitions of functions, as they are usually
    a programming error. For example:

      def f(): ...
      def f(): ...  # Error: 'f' redefined
    """
    if isinstance(new, Decorator):
        new = new.func
    if (
        isinstance(previous, (FuncDef, Decorator))
        and unnamed_function(new.name)
        and unnamed_function(previous.name)
    ):
        return True
    if isinstance(previous, (FuncDef, Var, Decorator)) and new.is_conditional:
        new.original_def = previous
        return True
    else:
        return False

</t>
<t tx="ekr.20220525082935.230">def found_incomplete_ref(self, tag: Tag) -&gt; bool:
    """Have we encountered an incomplete reference since starting tracking?"""
    return self.num_incomplete_refs != tag

</t>
<t tx="ekr.20220525082935.231">def record_incomplete_ref(self) -&gt; None:
    """Record the encounter of an incomplete reference and defer current analysis target."""
    self.defer()
    self.num_incomplete_refs += 1

</t>
<t tx="ekr.20220525082935.232">def mark_incomplete(
    self,
    name: str,
    node: Node,
    becomes_typeinfo: bool = False,
    module_public: bool = True,
    module_hidden: bool = False,
) -&gt; None:
    """Mark a definition as incomplete (and defer current analysis target).

    Also potentially mark the current namespace as incomplete.

    Args:
        name: The name that we weren't able to define (or '*' if the name is unknown)
        node: The node that refers to the name (definition or lvalue)
        becomes_typeinfo: Pass this to PlaceholderNode (used by special forms like
            named tuples that will create TypeInfos).
    """
    self.defer(node)
    if name == "*":
        self.incomplete = True
    elif not self.is_global_or_nonlocal(name):
        fullname = self.qualified_name(name)
        assert self.statement
        placeholder = PlaceholderNode(
            fullname, node, self.statement.line, becomes_typeinfo=becomes_typeinfo
        )
        self.add_symbol(
            name,
            placeholder,
            module_public=module_public,
            module_hidden=module_hidden,
            context=dummy_context(),
        )
    self.missing_names[-1].add(name)

</t>
<t tx="ekr.20220525082935.233">def is_incomplete_namespace(self, fullname: str) -&gt; bool:
    """Is a module or class namespace potentially missing some definitions?

    If a name is missing from an incomplete namespace, we'll need to defer the
    current analysis target.
    """
    return fullname in self.incomplete_namespaces

</t>
<t tx="ekr.20220525082935.234">def process_placeholder(self, name: str, kind: str, ctx: Context) -&gt; None:
    """Process a reference targeting placeholder node.

    If this is not a final iteration, defer current node,
    otherwise report an error.

    The 'kind' argument indicates if this a name or attribute expression
    (used for better error message).
    """
    if self.final_iteration:
        self.cannot_resolve_name(name, kind, ctx)
    else:
        self.defer(ctx)

</t>
<t tx="ekr.20220525082935.235">def cannot_resolve_name(self, name: str, kind: str, ctx: Context) -&gt; None:
    self.fail(f'Cannot resolve {kind} "{name}" (possible cyclic definition)', ctx)
    if not self.options.disable_recursive_aliases and self.is_func_scope():
        self.note("Recursive types are not allowed at function scope", ctx)

</t>
<t tx="ekr.20220525082935.236">def qualified_name(self, name: str) -&gt; str:
    if self.type is not None:
        return self.type._fullname + "." + name
    elif self.is_func_scope():
        return name
    else:
        return self.cur_mod_id + "." + name

</t>
<t tx="ekr.20220525082935.237">@contextmanager
def enter(
    self, function: FuncItem | GeneratorExpr | DictionaryComprehension
) -&gt; Iterator[None]:
    """Enter a function, generator or comprehension scope."""
    names = self.saved_locals.setdefault(function, SymbolTable())
    self.locals.append(names)
    is_comprehension = isinstance(function, (GeneratorExpr, DictionaryComprehension))
    self.is_comprehension_stack.append(is_comprehension)
    self.global_decls.append(set())
    self.nonlocal_decls.append(set())
    # -1 since entering block will increment this to 0.
    self.block_depth.append(-1)
    self.missing_names.append(set())
    try:
        yield
    finally:
        self.locals.pop()
        self.is_comprehension_stack.pop()
        self.global_decls.pop()
        self.nonlocal_decls.pop()
        self.block_depth.pop()
        self.missing_names.pop()

</t>
<t tx="ekr.20220525082935.238">def is_func_scope(self) -&gt; bool:
    return self.locals[-1] is not None

</t>
<t tx="ekr.20220525082935.239">def is_nested_within_func_scope(self) -&gt; bool:
    """Are we underneath a function scope, even if we are in a nested class also?"""
    return any(l is not None for l in self.locals)

</t>
<t tx="ekr.20220525082935.24">def update_function_type_variables(self, fun_type: CallableType, defn: FuncItem) -&gt; None:
    """Make any type variables in the signature of defn explicit.

    Update the signature of defn to contain type variable definitions
    if defn is generic.
    """
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        a = self.type_analyzer()
        fun_type.variables = a.bind_function_type_variables(fun_type, defn)

</t>
<t tx="ekr.20220525082935.240">def is_class_scope(self) -&gt; bool:
    return self.type is not None and not self.is_func_scope()

</t>
<t tx="ekr.20220525082935.241">def is_module_scope(self) -&gt; bool:
    return not (self.is_class_scope() or self.is_func_scope())

</t>
<t tx="ekr.20220525082935.242">def current_symbol_kind(self) -&gt; int:
    if self.is_class_scope():
        kind = MDEF
    elif self.is_func_scope():
        kind = LDEF
    else:
        kind = GDEF
    return kind

</t>
<t tx="ekr.20220525082935.243">def current_symbol_table(self, escape_comprehensions: bool = False) -&gt; SymbolTable:
    if self.is_func_scope():
        assert self.locals[-1] is not None
        if escape_comprehensions:
            assert len(self.locals) == len(self.is_comprehension_stack)
            # Retrieve the symbol table from the enclosing non-comprehension scope.
            for i, is_comprehension in enumerate(reversed(self.is_comprehension_stack)):
                if not is_comprehension:
                    if i == len(self.locals) - 1:  # The last iteration.
                        # The caller of the comprehension is in the global space.
                        names = self.globals
                    else:
                        names_candidate = self.locals[-1 - i]
                        assert (
                            names_candidate is not None
                        ), "Escaping comprehension from invalid scope"
                        names = names_candidate
                    break
            else:
                assert False, "Should have at least one non-comprehension scope"
        else:
            names = self.locals[-1]
        assert names is not None
    elif self.type is not None:
        names = self.type.names
    else:
        names = self.globals
    return names

</t>
<t tx="ekr.20220525082935.244">def is_global_or_nonlocal(self, name: str) -&gt; bool:
    return self.is_func_scope() and (
        name in self.global_decls[-1] or name in self.nonlocal_decls[-1]
    )

</t>
<t tx="ekr.20220525082935.245">def add_exports(self, exp_or_exps: Iterable[Expression] | Expression) -&gt; None:
    exps = [exp_or_exps] if isinstance(exp_or_exps, Expression) else exp_or_exps
    for exp in exps:
        if isinstance(exp, StrExpr):
            self.all_exports.append(exp.value)

</t>
<t tx="ekr.20220525082935.246">def name_not_defined(self, name: str, ctx: Context, namespace: str | None = None) -&gt; None:
    incomplete = self.is_incomplete_namespace(namespace or self.cur_mod_id)
    if (
        namespace is None
        and self.type
        and not self.is_func_scope()
        and self.incomplete_type_stack[-1]
        and not self.final_iteration
    ):
        # We are processing a class body for the first time, so it is incomplete.
        incomplete = True
    if incomplete:
        # Target namespace is incomplete, so it's possible that the name will be defined
        # later on. Defer current target.
        self.record_incomplete_ref()
        return
    message = f'Name "{name}" is not defined'
    self.fail(message, ctx, code=codes.NAME_DEFINED)

    if f"builtins.{name}" in SUGGESTED_TEST_FIXTURES:
        # The user probably has a missing definition in a test fixture. Let's verify.
        fullname = f"builtins.{name}"
        if self.lookup_fully_qualified_or_none(fullname) is None:
            # Yes. Generate a helpful note.
            self.msg.add_fixture_note(fullname, ctx)

    modules_with_unimported_hints = {
        name.split(".", 1)[0] for name in TYPES_FOR_UNIMPORTED_HINTS
    }
    lowercased = {name.lower(): name for name in TYPES_FOR_UNIMPORTED_HINTS}
    for module in modules_with_unimported_hints:
        fullname = f"{module}.{name}".lower()
        if fullname not in lowercased:
            continue
        # User probably forgot to import these types.
        hint = (
            'Did you forget to import it from "{module}"?'
            ' (Suggestion: "from {module} import {name}")'
        ).format(module=module, name=lowercased[fullname].rsplit(".", 1)[-1])
        self.note(hint, ctx, code=codes.NAME_DEFINED)

</t>
<t tx="ekr.20220525082935.247">def already_defined(
    self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None, noun: str
) -&gt; None:
    if isinstance(original_ctx, SymbolTableNode):
        node: SymbolNode | None = original_ctx.node
    elif isinstance(original_ctx, SymbolNode):
        node = original_ctx
    else:
        node = None

    if isinstance(original_ctx, SymbolTableNode) and isinstance(original_ctx.node, MypyFile):
        # Since this is an import, original_ctx.node points to the module definition.
        # Therefore its line number is always 1, which is not useful for this
        # error message.
        extra_msg = " (by an import)"
    elif node and node.line != -1 and self.is_local_name(node.fullname):
        # TODO: Using previous symbol node may give wrong line. We should use
        #       the line number where the binding was established instead.
        extra_msg = f" on line {node.line}"
    else:
        extra_msg = " (possibly by an import)"
    self.fail(
        f'{noun} "{unmangle(name)}" already defined{extra_msg}', ctx, code=codes.NO_REDEF
    )
</t>
<t tx="ekr.20220525082935.248">
def name_already_defined(
    self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None = None
) -&gt; None:
    self.already_defined(name, ctx, original_ctx, noun="Name")

def attribute_already_defined(
</t>
<t tx="ekr.20220525082935.249">    self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None = None
) -&gt; None:
    self.already_defined(name, ctx, original_ctx, noun="Attribute")

</t>
<t tx="ekr.20220525082935.25">def visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    self.statement = defn
    self.add_function_to_symbol_table(defn)

    if not self.recurse_into_functions:
        return

    # NB: Since _visit_overloaded_func_def will call accept on the
    # underlying FuncDefs, the function might get entered twice.
    # This is fine, though, because only the outermost function is
    # used to compute targets.
    with self.scope.function_scope(defn):
        self.analyze_overloaded_func_def(defn)

</t>
<t tx="ekr.20220525082935.250">def is_local_name(self, name: str) -&gt; bool:
    """Does name look like reference to a definition in the current module?"""
    return self.is_defined_in_current_module(name) or "." not in name

</t>
<t tx="ekr.20220525082935.251">def in_checked_function(self) -&gt; bool:
    """Should we type-check the current function?

    - Yes if --check-untyped-defs is set.
    - Yes outside functions.
    - Yes in annotated functions.
    - No otherwise.
    """
    if self.options.check_untyped_defs or not self.function_stack:
        return True

    current_index = len(self.function_stack) - 1
    while current_index &gt;= 0:
        current_func = self.function_stack[current_index]
        if not isinstance(current_func, LambdaExpr):
            return not current_func.is_dynamic()

        # Special case, `lambda` inherits the "checked" state from its parent.
        # Because `lambda` itself cannot be annotated.
        # `lambdas` can be deeply nested, so we try to find at least one other parent.
        current_index -= 1

    # This means that we only have a stack of `lambda` functions,
    # no regular functions.
    return True

</t>
<t tx="ekr.20220525082935.252">def fail(
    self,
    msg: str,
    ctx: Context,
    serious: bool = False,
    *,
    code: ErrorCode | None = None,
    blocker: bool = False,
) -&gt; None:
    if not serious and not self.in_checked_function():
        return
    # In case it's a bug and we don't really have context
    assert ctx is not None, msg
    self.errors.report(ctx.get_line(), ctx.get_column(), msg, blocker=blocker, code=code)

</t>
<t tx="ekr.20220525082935.253">def note(self, msg: str, ctx: Context, code: ErrorCode | None = None) -&gt; None:
    if not self.in_checked_function():
        return
    self.errors.report(ctx.get_line(), ctx.get_column(), msg, severity="note", code=code)

</t>
<t tx="ekr.20220525082935.254">def accept(self, node: Node) -&gt; None:
    try:
        node.accept(self)
    except Exception as err:
        report_internal_error(err, self.errors.file, node.line, self.errors, self.options)

</t>
<t tx="ekr.20220525082935.255">def expr_to_analyzed_type(
    self, expr: Expression, report_invalid_types: bool = True, allow_placeholder: bool = False
) -&gt; Type | None:
    if isinstance(expr, CallExpr):
        # This is a legacy syntax intended mostly for Python 2, we keep it for
        # backwards compatibility, but new features like generic named tuples
        # and recursive named tuples will be not supported.
        expr.accept(self)
        internal_name, info, tvar_defs = self.named_tuple_analyzer.check_namedtuple(
            expr, None, self.is_func_scope()
        )
        if tvar_defs:
            self.fail("Generic named tuples are not supported for legacy class syntax", expr)
            self.note("Use either Python 3 class syntax, or the assignment syntax", expr)
        if internal_name is None:
            # Some form of namedtuple is the only valid type that looks like a call
            # expression. This isn't a valid type.
            raise TypeTranslationError()
        elif not info:
            self.defer(expr)
            return None
        assert info.tuple_type, "NamedTuple without tuple type"
        fallback = Instance(info, [])
        return TupleType(info.tuple_type.items, fallback=fallback)
    typ = self.expr_to_unanalyzed_type(expr)
    return self.anal_type(
        typ, report_invalid_types=report_invalid_types, allow_placeholder=allow_placeholder
    )

</t>
<t tx="ekr.20220525082935.256">def analyze_type_expr(self, expr: Expression) -&gt; None:
    # There are certain expressions that mypy does not need to semantically analyze,
    # since they analyzed solely as type. (For example, indexes in type alias definitions
    # and base classes in class defs). External consumers of the mypy AST may need
    # them semantically analyzed, however, if they need to treat it as an expression
    # and not a type. (Which is to say, mypyc needs to do this.) Do the analysis
    # in a fresh tvar scope in order to suppress any errors about using type variables.
    with self.tvar_scope_frame(TypeVarLikeScope()):
        expr.accept(self)

</t>
<t tx="ekr.20220525082935.257">def type_analyzer(
    self,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_placeholder: bool = False,
    allow_required: bool = False,
    allow_param_spec_literals: bool = False,
    report_invalid_types: bool = True,
) -&gt; TypeAnalyser:
    if tvar_scope is None:
        tvar_scope = self.tvar_scope
    tpan = TypeAnalyser(
        self,
        tvar_scope,
        self.plugin,
        self.options,
        self.is_typeshed_stub_file,
        allow_unbound_tvars=allow_unbound_tvars,
        allow_tuple_literal=allow_tuple_literal,
        report_invalid_types=report_invalid_types,
        allow_placeholder=allow_placeholder,
        allow_required=allow_required,
        allow_param_spec_literals=allow_param_spec_literals,
    )
    tpan.in_dynamic_func = bool(self.function_stack and self.function_stack[-1].is_dynamic())
    tpan.global_scope = not self.type and not self.function_stack
    return tpan

</t>
<t tx="ekr.20220525082935.258">def expr_to_unanalyzed_type(self, node: Expression) -&gt; ProperType:
    return expr_to_unanalyzed_type(node, self.options, self.is_stub_file)

</t>
<t tx="ekr.20220525082935.259">def anal_type(
    self,
    typ: Type,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_placeholder: bool = False,
    allow_required: bool = False,
    allow_param_spec_literals: bool = False,
    report_invalid_types: bool = True,
    third_pass: bool = False,
) -&gt; Type | None:
    """Semantically analyze a type.

    Args:
        typ: Type to analyze (if already analyzed, this is a no-op)
        allow_placeholder: If True, may return PlaceholderType if
            encountering an incomplete definition
        third_pass: Unused; only for compatibility with old semantic
            analyzer

    Return None only if some part of the type couldn't be bound *and* it
    referred to an incomplete namespace or definition. In this case also
    defer as needed. During a final iteration this won't return None;
    instead report an error if the type can't be analyzed and return
    AnyType.

    In case of other errors, report an error message and return AnyType.

    NOTE: The caller shouldn't defer even if this returns None or a
          placeholder type.
    """
    a = self.type_analyzer(
        tvar_scope=tvar_scope,
        allow_unbound_tvars=allow_unbound_tvars,
        allow_tuple_literal=allow_tuple_literal,
        allow_placeholder=allow_placeholder,
        allow_required=allow_required,
        allow_param_spec_literals=allow_param_spec_literals,
        report_invalid_types=report_invalid_types,
    )
    tag = self.track_incomplete_refs()
    typ = typ.accept(a)
    if self.found_incomplete_ref(tag):
        # Something could not be bound yet.
        return None
    self.add_type_alias_deps(a.aliases_used)
    return typ

</t>
<t tx="ekr.20220525082935.26">def analyze_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    # OverloadedFuncDef refers to any legitimate situation where you have
    # more than one declaration for the same function in a row.  This occurs
    # with a @property with a setter or a deleter, and for a classic
    # @overload.

    defn._fullname = self.qualified_name(defn.name)
    # TODO: avoid modifying items.
    defn.items = defn.unanalyzed_items.copy()

    first_item = defn.items[0]
    first_item.is_overload = True
    first_item.accept(self)

    if isinstance(first_item, Decorator) and first_item.func.is_property:
        # This is a property.
        first_item.func.is_overload = True
        self.analyze_property_with_multi_part_definition(defn)
        typ = function_type(first_item.func, self.named_type("builtins.function"))
        assert isinstance(typ, CallableType)
        types = [typ]
    else:
        # This is an a normal overload. Find the item signatures, the
        # implementation (if outside a stub), and any missing @overload
        # decorators.
        types, impl, non_overload_indexes = self.analyze_overload_sigs_and_impl(defn)
        defn.impl = impl
        if non_overload_indexes:
            self.handle_missing_overload_decorators(
                defn, non_overload_indexes, some_overload_decorators=len(types) &gt; 0
            )
        # If we found an implementation, remove it from the overload item list,
        # as it's special.
        if impl is not None:
            assert impl is defn.items[-1]
            defn.items = defn.items[:-1]
        elif not non_overload_indexes:
            self.handle_missing_overload_implementation(defn)

    if types:
        defn.type = Overloaded(types)
        defn.type.line = defn.line

    if not defn.items:
        # It was not a real overload after all, but function redefinition. We've
        # visited the redefinition(s) already.
        if not defn.impl:
            # For really broken overloads with no items and no implementation we need to keep
            # at least one item to hold basic information like function name.
            defn.impl = defn.unanalyzed_items[-1]
        return

    # We know this is an overload def. Infer properties and perform some checks.
    self.process_final_in_overload(defn)
    self.process_static_or_class_method_in_overload(defn)
    self.process_overload_impl(defn)

def process_overload_impl(self, defn: OverloadedFuncDef) -&gt; None:
    """Set flags for an overload implementation.

    Currently, this checks for a trivial body in protocols classes,
    where it makes the method implicitly abstract.
    """
    if defn.impl is None:
        return
    impl = defn.impl if isinstance(defn.impl, FuncDef) else defn.impl.func
    if is_trivial_body(impl.body) and self.is_class_scope() and not self.is_stub_file:
        assert self.type is not None
        if self.type.is_protocol:
            impl.abstract_status = IMPLICITLY_ABSTRACT
        if impl.abstract_status != NOT_ABSTRACT:
            impl.is_trivial_body = True

</t>
<t tx="ekr.20220525082935.260">def class_type(self, self_type: Type) -&gt; Type:
    return TypeType.make_normalized(self_type)

</t>
<t tx="ekr.20220525082935.261">def schedule_patch(self, priority: int, patch: Callable[[], None]) -&gt; None:
    self.patches.append((priority, patch))

</t>
<t tx="ekr.20220525082935.262">def report_hang(self) -&gt; None:
    print("Deferral trace:")
    for mod, line in self.deferral_debug_context:
        print(f"    {mod}:{line}")
    self.errors.report(
        -1,
        -1,
        "INTERNAL ERROR: maximum semantic analysis iteration count reached",
</t>
<t tx="ekr.20220525082935.263">        blocker=True,
    )

def add_plugin_dependency(self, trigger: str, target: str | None = None) -&gt; None:
    """Add dependency from trigger to a target.

    If the target is not given explicitly, use the current target.
    """
    if target is None:
        target = self.scope.current_target()
    self.cur_mod_node.plugin_deps.setdefault(trigger, set()).add(target)

</t>
<t tx="ekr.20220525082935.264">def add_type_alias_deps(self, aliases_used: Iterable[str], target: str | None = None) -&gt; None:
    """Add full names of type aliases on which the current node depends.

    This is used by fine-grained incremental mode to re-check the corresponding nodes.
    If `target` is None, then the target node used will be the current scope.
    """
    if not aliases_used:
        # A basic optimization to avoid adding targets with no dependencies to
        # the `alias_deps` dict.
        return
    if target is None:
        target = self.scope.current_target()
    self.cur_mod_node.alias_deps[target].update(aliases_used)

</t>
<t tx="ekr.20220525082935.265">def is_mangled_global(self, name: str) -&gt; bool:
    # A global is mangled if there exists at least one renamed variant.
    return unmangle(name) + "'" in self.globals

</t>
<t tx="ekr.20220525082935.266">def is_initial_mangled_global(self, name: str) -&gt; bool:
    # If there are renamed definitions for a global, the first one has exactly one prime.
    return name == unmangle(name) + "'"

</t>
<t tx="ekr.20220525082935.267">def parse_bool(self, expr: Expression) -&gt; bool | None:
    if isinstance(expr, NameExpr):
        if expr.fullname == "builtins.True":
            return True
        if expr.fullname == "builtins.False":
            return False
    return None

</t>
<t tx="ekr.20220525082935.268">def set_future_import_flags(self, module_name: str) -&gt; None:
    if module_name in FUTURE_IMPORTS:
        self.modules[self.cur_mod_id].future_import_flags.add(FUTURE_IMPORTS[module_name])

</t>
<t tx="ekr.20220525082935.269">def is_future_flag_set(self, flag: str) -&gt; bool:
    return self.modules[self.cur_mod_id].is_future_flag_set(flag)
</t>
<t tx="ekr.20220525082935.27">def analyze_overload_sigs_and_impl(
    self, defn: OverloadedFuncDef
) -&gt; tuple[list[CallableType], OverloadPart | None, list[int]]:
    """Find overload signatures, the implementation, and items with missing @overload.

    Assume that the first was already analyzed. As a side effect:
    analyzes remaining items and updates 'is_overload' flags.
    """
    types = []
    non_overload_indexes = []
    impl: OverloadPart | None = None
    for i, item in enumerate(defn.items):
        if i != 0:
            # Assume that the first item was already visited
            item.is_overload = True
            item.accept(self)
        # TODO: support decorated overloaded functions properly
        if isinstance(item, Decorator):
            callable = function_type(item.func, self.named_type("builtins.function"))
            assert isinstance(callable, CallableType)
            if not any(refers_to_fullname(dec, OVERLOAD_NAMES) for dec in item.decorators):
                if i == len(defn.items) - 1 and not self.is_stub_file:
                    # Last item outside a stub is impl
                    impl = item
                else:
                    # Oops it wasn't an overload after all. A clear error
                    # will vary based on where in the list it is, record
                    # that.
                    non_overload_indexes.append(i)
            else:
                item.func.is_overload = True
                types.append(callable)
                if item.var.is_property:
                    self.fail("An overload can not be a property", item)
        elif isinstance(item, FuncDef):
            if i == len(defn.items) - 1 and not self.is_stub_file:
                impl = item
            else:
                non_overload_indexes.append(i)
    return types, impl, non_overload_indexes

</t>
<t tx="ekr.20220525082935.28">def handle_missing_overload_decorators(
    self,
    defn: OverloadedFuncDef,
    non_overload_indexes: list[int],
    some_overload_decorators: bool,
) -&gt; None:
    """Generate errors for overload items without @overload.

    Side effect: remote non-overload items.
    """
    if some_overload_decorators:
        # Some of them were overloads, but not all.
        for idx in non_overload_indexes:
            if self.is_stub_file:
                self.fail(
                    "An implementation for an overloaded function "
                    "is not allowed in a stub file",
                    defn.items[idx],
                )
            else:
                self.fail(
                    "The implementation for an overloaded function must come last",
                    defn.items[idx],
                )
    else:
        for idx in non_overload_indexes[1:]:
            self.name_already_defined(defn.name, defn.items[idx], defn.items[0])
        if defn.impl:
            self.name_already_defined(defn.name, defn.impl, defn.items[0])
    # Remove the non-overloads
    for idx in reversed(non_overload_indexes):
        del defn.items[idx]

</t>
<t tx="ekr.20220525082935.29">def handle_missing_overload_implementation(self, defn: OverloadedFuncDef) -&gt; None:
    """Generate error about missing overload implementation (only if needed)."""
    if not self.is_stub_file:
        if self.type and self.type.is_protocol and not self.is_func_scope():
            # An overloaded protocol method doesn't need an implementation,
            # but if it doesn't have one, then it is considered abstract.
            for item in defn.items:
                if isinstance(item, Decorator):
                    item.func.abstract_status = IS_ABSTRACT
                else:
                    item.abstract_status = IS_ABSTRACT
        else:
            # TODO: also allow omitting an implementation for abstract methods in ABCs?
            self.fail(
                "An overloaded function outside a stub file must have an implementation",
                defn,
                code=codes.NO_OVERLOAD_IMPL,
            )

</t>
<t tx="ekr.20220525082935.3"># mypyc doesn't properly handle implementing an abstractproperty
# with a regular attribute so we make them properties
@property
def is_stub_file(self) -&gt; bool:
    return self._is_stub_file

</t>
<t tx="ekr.20220525082935.30">def process_final_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
    """Detect the @final status of an overloaded function (and perform checks)."""
    # If the implementation is marked as @final (or the first overload in
    # stubs), then the whole overloaded definition if @final.
    if any(item.is_final for item in defn.items):
        # We anyway mark it as final because it was probably the intention.
        defn.is_final = True
        # Only show the error once per overload
        bad_final = next(ov for ov in defn.items if ov.is_final)
        if not self.is_stub_file:
            self.fail("@final should be applied only to overload implementation", bad_final)
        elif any(item.is_final for item in defn.items[1:]):
            bad_final = next(ov for ov in defn.items[1:] if ov.is_final)
            self.fail(
                "In a stub file @final must be applied only to the first overload", bad_final
            )
    if defn.impl is not None and defn.impl.is_final:
        defn.is_final = True

</t>
<t tx="ekr.20220525082935.309">def semantic_analysis_for_scc(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    """Perform semantic analysis for all modules in a SCC (import cycle).

    Assume that reachability analysis has already been performed.

    The scc will be processed roughly in the order the modules are included
    in the list.
    """
    patches: Patches = []
    # Note that functions can't define new module-level attributes
    # using 'global x', since module top levels are fully processed
    # before functions. This limitation is unlikely to go away soon.
    process_top_levels(graph, scc, patches)
    process_functions(graph, scc, patches)
    # We use patch callbacks to fix up things when we expect relatively few
    # callbacks to be required.
    apply_semantic_analyzer_patches(patches)
    # Run class decorator hooks (they requite complete MROs and no placeholders).
    apply_class_plugin_hooks(graph, scc, errors)
    # This pass might need fallbacks calculated above and the results of hooks.
    check_type_arguments(graph, scc, errors)
    calculate_class_properties(graph, scc, errors)
    check_blockers(graph, scc)
    # Clean-up builtins, so that TypeVar etc. are not accessible without importing.
    if "builtins" in scc:
        cleanup_builtin_scc(graph["builtins"])


</t>
<t tx="ekr.20220525082935.31">def process_static_or_class_method_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
    class_status = []
    static_status = []
    for item in defn.items:
        if isinstance(item, Decorator):
            inner = item.func
        elif isinstance(item, FuncDef):
            inner = item
        else:
            assert False, f"The 'item' variable is an unexpected type: {type(item)}"
        class_status.append(inner.is_class)
        static_status.append(inner.is_static)

    if defn.impl is not None:
        if isinstance(defn.impl, Decorator):
            inner = defn.impl.func
        elif isinstance(defn.impl, FuncDef):
            inner = defn.impl
        else:
            assert False, f"Unexpected impl type: {type(defn.impl)}"
        class_status.append(inner.is_class)
        static_status.append(inner.is_static)

    if len(set(class_status)) != 1:
        self.msg.overload_inconsistently_applies_decorator("classmethod", defn)
    elif len(set(static_status)) != 1:
        self.msg.overload_inconsistently_applies_decorator("staticmethod", defn)
    else:
        defn.is_class = class_status[0]
        defn.is_static = static_status[0]

</t>
<t tx="ekr.20220525082935.32">def analyze_property_with_multi_part_definition(self, defn: OverloadedFuncDef) -&gt; None:
    """Analyze a property defined using multiple methods (e.g., using @x.setter).

    Assume that the first method (@property) has already been analyzed.
    """
    defn.is_property = True
    items = defn.items
    first_item = cast(Decorator, defn.items[0])
    deleted_items = []
    for i, item in enumerate(items[1:]):
        if isinstance(item, Decorator):
            if len(item.decorators) &gt;= 1:
                node = item.decorators[0]
                if isinstance(node, MemberExpr):
                    if node.name == "setter":
                        # The first item represents the entire property.
                        first_item.var.is_settable_property = True
                        # Get abstractness from the original definition.
                        item.func.abstract_status = first_item.func.abstract_status
                else:
                    self.fail(
                        f"Only supported top decorator is @{first_item.func.name}.setter", item
                    )
            item.func.accept(self)
        else:
            self.fail(f'Unexpected definition for property "{first_item.func.name}"', item)
            deleted_items.append(i + 1)
    for i in reversed(deleted_items):
        del items[i]

</t>
<t tx="ekr.20220525082935.33">def add_function_to_symbol_table(self, func: FuncDef | OverloadedFuncDef) -&gt; None:
    if self.is_class_scope():
        assert self.type is not None
        func.info = self.type
    func._fullname = self.qualified_name(func.name)
    self.add_symbol(func.name, func, func)

</t>
<t tx="ekr.20220525082935.34">def analyze_arg_initializers(self, defn: FuncItem) -&gt; None:
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        # Analyze default arguments
        for arg in defn.arguments:
            if arg.initializer:
                arg.initializer.accept(self)

</t>
<t tx="ekr.20220525082935.35">def analyze_function_body(self, defn: FuncItem) -&gt; None:
    is_method = self.is_class_scope()
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        # Bind the type variables again to visit the body.
        if defn.type:
            a = self.type_analyzer()
            a.bind_function_type_variables(cast(CallableType, defn.type), defn)
        self.function_stack.append(defn)
        with self.enter(defn):
            for arg in defn.arguments:
                self.add_local(arg.variable, defn)

            # The first argument of a non-static, non-class method is like 'self'
            # (though the name could be different), having the enclosing class's
            # instance type.
            if is_method and not defn.is_static and not defn.is_class and defn.arguments:
                defn.arguments[0].variable.is_self = True

            defn.body.accept(self)
        self.function_stack.pop()

</t>
<t tx="ekr.20220525082935.36">def check_classvar_in_signature(self, typ: ProperType) -&gt; None:
    t: ProperType
    if isinstance(typ, Overloaded):
        for t in typ.items:
            self.check_classvar_in_signature(t)
        return
    if not isinstance(typ, CallableType):
        return
    for t in get_proper_types(typ.arg_types) + [get_proper_type(typ.ret_type)]:
        if self.is_classvar(t):
            self.fail_invalid_classvar(t)
            # Show only one error per signature
            break

</t>
<t tx="ekr.20220525082935.37">def check_function_signature(self, fdef: FuncItem) -&gt; None:
    sig = fdef.type
    assert isinstance(sig, CallableType)
    if len(sig.arg_types) &lt; len(fdef.arguments):
        self.fail("Type signature has too few arguments", fdef)
        # Add dummy Any arguments to prevent crashes later.
        num_extra_anys = len(fdef.arguments) - len(sig.arg_types)
        extra_anys = [AnyType(TypeOfAny.from_error)] * num_extra_anys
        sig.arg_types.extend(extra_anys)
    elif len(sig.arg_types) &gt; len(fdef.arguments):
        self.fail("Type signature has too many arguments", fdef, blocker=True)

</t>
<t tx="ekr.20220525082935.38">def visit_decorator(self, dec: Decorator) -&gt; None:
    self.statement = dec
    # TODO: better don't modify them at all.
    dec.decorators = dec.original_decorators.copy()
    dec.func.is_conditional = self.block_depth[-1] &gt; 0
    if not dec.is_overload:
        self.add_symbol(dec.name, dec, dec)
    dec.func._fullname = self.qualified_name(dec.name)
    dec.var._fullname = self.qualified_name(dec.name)
    for d in dec.decorators:
        d.accept(self)
    removed: list[int] = []
    no_type_check = False
    could_be_decorated_property = False
    for i, d in enumerate(dec.decorators):
        # A bunch of decorators are special cased here.
        if refers_to_fullname(d, "abc.abstractmethod"):
            removed.append(i)
            dec.func.abstract_status = IS_ABSTRACT
            self.check_decorated_function_is_method("abstractmethod", dec)
        elif refers_to_fullname(d, ("asyncio.coroutines.coroutine", "types.coroutine")):
            removed.append(i)
            dec.func.is_awaitable_coroutine = True
        elif refers_to_fullname(d, "builtins.staticmethod"):
            removed.append(i)
            dec.func.is_static = True
            dec.var.is_staticmethod = True
            self.check_decorated_function_is_method("staticmethod", dec)
        elif refers_to_fullname(d, "builtins.classmethod"):
            removed.append(i)
            dec.func.is_class = True
            dec.var.is_classmethod = True
            self.check_decorated_function_is_method("classmethod", dec)
        elif refers_to_fullname(
            d, ("builtins.property", "abc.abstractproperty", "functools.cached_property")
        ):
            removed.append(i)
            dec.func.is_property = True
            dec.var.is_property = True
            if refers_to_fullname(d, "abc.abstractproperty"):
                dec.func.abstract_status = IS_ABSTRACT
            elif refers_to_fullname(d, "functools.cached_property"):
                dec.var.is_settable_property = True
            self.check_decorated_function_is_method("property", dec)
        elif refers_to_fullname(d, "typing.no_type_check"):
            dec.var.type = AnyType(TypeOfAny.special_form)
            no_type_check = True
        elif refers_to_fullname(d, FINAL_DECORATOR_NAMES):
            if self.is_class_scope():
                assert self.type is not None, "No type set at class scope"
                if self.type.is_protocol:
                    self.msg.protocol_members_cant_be_final(d)
                else:
                    dec.func.is_final = True
                    dec.var.is_final = True
                removed.append(i)
            else:
                self.fail("@final cannot be used with non-method functions", d)
        elif not dec.var.is_property:
            # We have seen a "non-trivial" decorator before seeing @property, if
            # we will see a @property later, give an error, as we don't support this.
            could_be_decorated_property = True
    for i in reversed(removed):
        del dec.decorators[i]
    if (not dec.is_overload or dec.var.is_property) and self.type:
        dec.var.info = self.type
        dec.var.is_initialized_in_class = True
    if not no_type_check and self.recurse_into_functions:
        dec.func.accept(self)
    if could_be_decorated_property and dec.decorators and dec.var.is_property:
        self.fail("Decorators on top of @property are not supported", dec)
    if (dec.func.is_static or dec.func.is_class) and dec.var.is_property:
        self.fail("Only instance methods can be decorated with @property", dec)
    if dec.func.abstract_status == IS_ABSTRACT and dec.func.is_final:
        self.fail(f"Method {dec.func.name} is both abstract and final", dec)

</t>
<t tx="ekr.20220525082935.39">def check_decorated_function_is_method(self, decorator: str, context: Context) -&gt; None:
    if not self.type or self.is_func_scope():
        self.fail(f'"{decorator}" used with a non-method', context)

</t>
<t tx="ekr.20220525082935.4">@property
def is_typeshed_stub_file(self) -&gt; bool:
    return self._is_typeshed_stub_file

</t>
<t tx="ekr.20220525082935.40">#
# Classes
#

</t>
<t tx="ekr.20220525082935.41">def visit_class_def(self, defn: ClassDef) -&gt; None:
    self.statement = defn
    self.incomplete_type_stack.append(not defn.info)
    namespace = self.qualified_name(defn.name)
    with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
        self.analyze_class(defn)
    self.incomplete_type_stack.pop()

</t>
<t tx="ekr.20220525082935.42">def analyze_class(self, defn: ClassDef) -&gt; None:
    fullname = self.qualified_name(defn.name)
    if not defn.info and not self.is_core_builtin_class(defn):
        # Add placeholder so that self-references in base classes can be
        # resolved.  We don't want this to cause a deferral, since if there
        # are no incomplete references, we'll replace this with a TypeInfo
        # before returning.
        placeholder = PlaceholderNode(fullname, defn, defn.line, becomes_typeinfo=True)
        self.add_symbol(defn.name, placeholder, defn, can_defer=False)

    tag = self.track_incomplete_refs()

    # Restore base classes after previous iteration (things like Generic[T] might be removed).
    defn.base_type_exprs.extend(defn.removed_base_type_exprs)
    defn.removed_base_type_exprs.clear()

    self.infer_metaclass_and_bases_from_compat_helpers(defn)

    bases = defn.base_type_exprs
    bases, tvar_defs, is_protocol = self.clean_up_bases_and_infer_type_variables(
        defn, bases, context=defn
    )

    for tvd in tvar_defs:
        if isinstance(tvd, TypeVarType) and any(
            has_placeholder(t) for t in [tvd.upper_bound] + tvd.values
        ):
            # Some type variable bounds or values are not ready, we need
            # to re-analyze this class.
            self.defer()

    self.analyze_class_keywords(defn)
    bases_result = self.analyze_base_classes(bases)
    if bases_result is None or self.found_incomplete_ref(tag):
        # Something was incomplete. Defer current target.
        self.mark_incomplete(defn.name, defn)
        return

    base_types, base_error = bases_result
    if any(isinstance(base, PlaceholderType) for base, _ in base_types):
        # We need to know the TypeInfo of each base to construct the MRO. Placeholder types
        # are okay in nested positions, since they can't affect the MRO.
        self.mark_incomplete(defn.name, defn)
        return

    declared_metaclass, should_defer = self.get_declared_metaclass(defn.name, defn.metaclass)
    if should_defer or self.found_incomplete_ref(tag):
        # Metaclass was not ready. Defer current target.
        self.mark_incomplete(defn.name, defn)
        return

    if self.analyze_typeddict_classdef(defn):
        if defn.info:
            self.setup_type_vars(defn, tvar_defs)
            self.setup_alias_type_vars(defn)
        return

    if self.analyze_namedtuple_classdef(defn, tvar_defs):
        return

    # Create TypeInfo for class now that base classes and the MRO can be calculated.
    self.prepare_class_def(defn)
    self.setup_type_vars(defn, tvar_defs)
    if base_error:
        defn.info.fallback_to_any = True

    with self.scope.class_scope(defn.info):
        self.configure_base_classes(defn, base_types)
        defn.info.is_protocol = is_protocol
        self.recalculate_metaclass(defn, declared_metaclass)
        defn.info.runtime_protocol = False
        for decorator in defn.decorators:
            self.analyze_class_decorator(defn, decorator)
        self.analyze_class_body_common(defn)

def setup_type_vars(self, defn: ClassDef, tvar_defs: list[TypeVarLikeType]) -&gt; None:
    defn.type_vars = tvar_defs
    defn.info.type_vars = []
    # we want to make sure any additional logic in add_type_vars gets run
    defn.info.add_type_vars()

def setup_alias_type_vars(self, defn: ClassDef) -&gt; None:
    assert defn.info.special_alias is not None
    defn.info.special_alias.alias_tvars = list(defn.info.type_vars)
    target = defn.info.special_alias.target
    assert isinstance(target, ProperType)
    if isinstance(target, TypedDictType):
        target.fallback.args = tuple(defn.type_vars)
    elif isinstance(target, TupleType):
        target.partial_fallback.args = tuple(defn.type_vars)
    else:
        assert False, f"Unexpected special alias type: {type(target)}"

</t>
<t tx="ekr.20220525082935.43">def is_core_builtin_class(self, defn: ClassDef) -&gt; bool:
    return self.cur_mod_id == "builtins" and defn.name in CORE_BUILTIN_CLASSES

</t>
<t tx="ekr.20220525082935.44">def analyze_class_body_common(self, defn: ClassDef) -&gt; None:
    """Parts of class body analysis that are common to all kinds of class defs."""
    self.enter_class(defn.info)
    defn.defs.accept(self)
    self.apply_class_plugin_hooks(defn)
    self.leave_class()

</t>
<t tx="ekr.20220525082935.45">def analyze_typeddict_classdef(self, defn: ClassDef) -&gt; bool:
    if (
        defn.info
        and defn.info.typeddict_type
        and not has_placeholder(defn.info.typeddict_type)
    ):
        # This is a valid TypedDict, and it is fully analyzed.
        return True
    is_typeddict, info = self.typed_dict_analyzer.analyze_typeddict_classdef(defn)
    if is_typeddict:
        for decorator in defn.decorators:
            decorator.accept(self)
            if isinstance(decorator, RefExpr):
                if decorator.fullname in FINAL_DECORATOR_NAMES and info is not None:
                    info.is_final = True
        if info is None:
            self.mark_incomplete(defn.name, defn)
        else:
            self.prepare_class_def(defn, info)
        return True
    return False

def analyze_namedtuple_classdef(
    self, defn: ClassDef, tvar_defs: list[TypeVarLikeType]
) -&gt; bool:
    """Check if this class can define a named tuple."""
    if (
        defn.info
        and defn.info.is_named_tuple
        and defn.info.tuple_type
        and not has_placeholder(defn.info.tuple_type)
    ):
        # Don't reprocess everything. We just need to process methods defined
        # in the named tuple class body.
        is_named_tuple = True
        info: TypeInfo | None = defn.info
    else:
        is_named_tuple, info = self.named_tuple_analyzer.analyze_namedtuple_classdef(
            defn, self.is_stub_file, self.is_func_scope()
        )
    if is_named_tuple:
        if info is None:
            self.mark_incomplete(defn.name, defn)
        else:
            self.prepare_class_def(defn, info, custom_names=True)
            self.setup_type_vars(defn, tvar_defs)
            self.setup_alias_type_vars(defn)
            with self.scope.class_scope(defn.info):
                with self.named_tuple_analyzer.save_namedtuple_body(info):
                    self.analyze_class_body_common(defn)
        return True
    return False

</t>
<t tx="ekr.20220525082935.46">def apply_class_plugin_hooks(self, defn: ClassDef) -&gt; None:
    """Apply a plugin hook that may infer a more precise definition for a class."""

    for decorator in defn.decorators:
        decorator_name = self.get_fullname_for_hook(decorator)
        if decorator_name:
            hook = self.plugin.get_class_decorator_hook(decorator_name)
            if hook:
                hook(ClassDefContext(defn, decorator, self))

    if defn.metaclass:
        metaclass_name = self.get_fullname_for_hook(defn.metaclass)
        if metaclass_name:
            hook = self.plugin.get_metaclass_hook(metaclass_name)
            if hook:
                hook(ClassDefContext(defn, defn.metaclass, self))

    for base_expr in defn.base_type_exprs:
        base_name = self.get_fullname_for_hook(base_expr)
        if base_name:
            hook = self.plugin.get_base_class_hook(base_name)
            if hook:
                hook(ClassDefContext(defn, base_expr, self))

</t>
<t tx="ekr.20220525082935.47">def get_fullname_for_hook(self, expr: Expression) -&gt; str | None:
    if isinstance(expr, CallExpr):
        return self.get_fullname_for_hook(expr.callee)
    elif isinstance(expr, IndexExpr):
        return self.get_fullname_for_hook(expr.base)
    elif isinstance(expr, RefExpr):
        if expr.fullname:
            return expr.fullname
        # If we don't have a fullname look it up. This happens because base classes are
        # analyzed in a different manner (see exprtotype.py) and therefore those AST
        # nodes will not have full names.
        sym = self.lookup_type_node(expr)
        if sym:
            return sym.fullname
    return None

</t>
<t tx="ekr.20220525082935.48">def analyze_class_keywords(self, defn: ClassDef) -&gt; None:
    for value in defn.keywords.values():
        value.accept(self)

</t>
<t tx="ekr.20220525082935.49">def enter_class(self, info: TypeInfo) -&gt; None:
    # Remember previous active class
    self.type_stack.append(self.type)
    self.locals.append(None)  # Add class scope
    self.is_comprehension_stack.append(False)
    self.block_depth.append(-1)  # The class body increments this to 0
    self.type = info
    self.missing_names.append(set())

</t>
<t tx="ekr.20220525082935.5">@property
def final_iteration(self) -&gt; bool:
    return self._final_iteration

</t>
<t tx="ekr.20220525082935.50">def leave_class(self) -&gt; None:
    """Restore analyzer state."""
    self.block_depth.pop()
    self.locals.pop()
    self.is_comprehension_stack.pop()
    self.type = self.type_stack.pop()
    self.missing_names.pop()

</t>
<t tx="ekr.20220525082935.51">def analyze_class_decorator(self, defn: ClassDef, decorator: Expression) -&gt; None:
    decorator.accept(self)
    if isinstance(decorator, RefExpr):
        if decorator.fullname in RUNTIME_PROTOCOL_DECOS:
            if defn.info.is_protocol:
                defn.info.runtime_protocol = True
            else:
                self.fail("@runtime_checkable can only be used with protocol classes", defn)
        elif decorator.fullname in FINAL_DECORATOR_NAMES:
            defn.info.is_final = True

</t>
<t tx="ekr.20220525082935.52">def clean_up_bases_and_infer_type_variables(
    self, defn: ClassDef, base_type_exprs: list[Expression], context: Context
) -&gt; tuple[list[Expression], list[TypeVarLikeType], bool]:
    """Remove extra base classes such as Generic and infer type vars.

    For example, consider this class:

      class Foo(Bar, Generic[T]): ...

    Now we will remove Generic[T] from bases of Foo and infer that the
    type variable 'T' is a type argument of Foo.

    Note that this is performed *before* semantic analysis.

    Returns (remaining base expressions, inferred type variables, is protocol).
    """
    removed: list[int] = []
    declared_tvars: TypeVarLikeList = []
    is_protocol = False
    for i, base_expr in enumerate(base_type_exprs):
        self.analyze_type_expr(base_expr)

        try:
            base = self.expr_to_unanalyzed_type(base_expr)
        except TypeTranslationError:
            # This error will be caught later.
            continue
        result = self.analyze_class_typevar_declaration(base)
        if result is not None:
            if declared_tvars:
                self.fail("Only single Generic[...] or Protocol[...] can be in bases", context)
            removed.append(i)
            tvars = result[0]
            is_protocol |= result[1]
            declared_tvars.extend(tvars)
        if isinstance(base, UnboundType):
            sym = self.lookup_qualified(base.name, base)
            if sym is not None and sym.node is not None:
                if sym.node.fullname in PROTOCOL_NAMES and i not in removed:
                    # also remove bare 'Protocol' bases
                    removed.append(i)
                    is_protocol = True

    all_tvars = self.get_all_bases_tvars(base_type_exprs, removed)
    if declared_tvars:
        if len(remove_dups(declared_tvars)) &lt; len(declared_tvars):
            self.fail("Duplicate type variables in Generic[...] or Protocol[...]", context)
        declared_tvars = remove_dups(declared_tvars)
        if not set(all_tvars).issubset(set(declared_tvars)):
            self.fail(
                "If Generic[...] or Protocol[...] is present"
                " it should list all type variables",
                context,
            )
            # In case of error, Generic tvars will go first
            declared_tvars = remove_dups(declared_tvars + all_tvars)
    else:
        declared_tvars = all_tvars
    for i in reversed(removed):
        # We need to actually remove the base class expressions like Generic[T],
        # mostly because otherwise they will create spurious dependencies in fine
        # grained incremental mode.
        defn.removed_base_type_exprs.append(defn.base_type_exprs[i])
        del base_type_exprs[i]
    tvar_defs: list[TypeVarLikeType] = []
    for name, tvar_expr in declared_tvars:
        tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
        tvar_defs.append(tvar_def)
    return base_type_exprs, tvar_defs, is_protocol

</t>
<t tx="ekr.20220525082935.53">def analyze_class_typevar_declaration(self, base: Type) -&gt; tuple[TypeVarLikeList, bool] | None:
    """Analyze type variables declared using Generic[...] or Protocol[...].

    Args:
        base: Non-analyzed base class

    Return None if the base class does not declare type variables. Otherwise,
    return the type variables.
    """
    if not isinstance(base, UnboundType):
        return None
    unbound = base
    sym = self.lookup_qualified(unbound.name, unbound)
    if sym is None or sym.node is None:
        return None
    if (
        sym.node.fullname == "typing.Generic"
        or sym.node.fullname in PROTOCOL_NAMES
        and base.args
    ):
        is_proto = sym.node.fullname != "typing.Generic"
        tvars: TypeVarLikeList = []
        have_type_var_tuple = False
        for arg in unbound.args:
            tag = self.track_incomplete_refs()
            tvar = self.analyze_unbound_tvar(arg)
            if tvar:
                if isinstance(tvar[1], TypeVarTupleExpr):
                    if have_type_var_tuple:
                        self.fail("Can only use one type var tuple in a class def", base)
                        continue
                    have_type_var_tuple = True
                tvars.append(tvar)
            elif not self.found_incomplete_ref(tag):
                self.fail("Free type variable expected in %s[...]" % sym.node.name, base)
        return tvars, is_proto
    return None

</t>
<t tx="ekr.20220525082935.54">def analyze_unbound_tvar(self, t: Type) -&gt; tuple[str, TypeVarLikeExpr] | None:
    if not isinstance(t, UnboundType):
        return None
    unbound = t
    sym = self.lookup_qualified(unbound.name, unbound)
    if sym and isinstance(sym.node, PlaceholderNode):
        self.record_incomplete_ref()
    if sym and isinstance(sym.node, ParamSpecExpr):
        if sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
            # It's bound by our type variable scope
            return None
        return unbound.name, sym.node
    if sym and sym.fullname == "typing_extensions.Unpack":
        inner_t = unbound.args[0]
        if not isinstance(inner_t, UnboundType):
            return None
        inner_unbound = inner_t
        inner_sym = self.lookup_qualified(inner_unbound.name, inner_unbound)
        if inner_sym and isinstance(inner_sym.node, PlaceholderNode):
            self.record_incomplete_ref()
        if inner_sym and isinstance(inner_sym.node, TypeVarTupleExpr):
            if inner_sym.fullname and not self.tvar_scope.allow_binding(inner_sym.fullname):
                # It's bound by our type variable scope
                return None
            return inner_unbound.name, inner_sym.node
    if sym is None or not isinstance(sym.node, TypeVarExpr):
        return None
    elif sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
        # It's bound by our type variable scope
        return None
    else:
        assert isinstance(sym.node, TypeVarExpr)
        return unbound.name, sym.node

</t>
<t tx="ekr.20220525082935.55">def get_all_bases_tvars(
    self, base_type_exprs: list[Expression], removed: list[int]
) -&gt; TypeVarLikeList:
    """Return all type variable references in bases."""
    tvars: TypeVarLikeList = []
    for i, base_expr in enumerate(base_type_exprs):
        if i not in removed:
            try:
                base = self.expr_to_unanalyzed_type(base_expr)
            except TypeTranslationError:
                # This error will be caught later.
                continue
            base_tvars = base.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
            tvars.extend(base_tvars)
    return remove_dups(tvars)

</t>
<t tx="ekr.20220525082935.56">def get_and_bind_all_tvars(self, type_exprs: list[Expression]) -&gt; list[TypeVarLikeType]:
    """Return all type variable references in item type expressions.

    This is a helper for generic TypedDicts and NamedTuples. Essentially it is
    a simplified version of the logic we use for ClassDef bases. We duplicate
    some amount of code, because it is hard to refactor common pieces.
    """
    tvars = []
    for base_expr in type_exprs:
        try:
            base = self.expr_to_unanalyzed_type(base_expr)
        except TypeTranslationError:
            # This error will be caught later.
            continue
        base_tvars = base.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
        tvars.extend(base_tvars)
    tvars = remove_dups(tvars)  # Variables are defined in order of textual appearance.
    tvar_defs = []
    for name, tvar_expr in tvars:
        tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
        tvar_defs.append(tvar_def)
    return tvar_defs

def prepare_class_def(
    self, defn: ClassDef, info: TypeInfo | None = None, custom_names: bool = False
) -&gt; None:
    """Prepare for the analysis of a class definition.

    Create an empty TypeInfo and store it in a symbol table, or if the 'info'
    argument is provided, store it instead (used for magic type definitions).
    """
    if not defn.info:
        defn.fullname = self.qualified_name(defn.name)
        # TODO: Nested classes
        info = info or self.make_empty_type_info(defn)
        defn.info = info
        info.defn = defn
        if not custom_names:
            # Some special classes (in particular NamedTuples) use custom fullname logic.
            # Don't override it here (also see comment below, this needs cleanup).
            if not self.is_func_scope():
                info._fullname = self.qualified_name(defn.name)
            else:
                info._fullname = info.name
    local_name = defn.name
    if "@" in local_name:
        local_name = local_name.split("@")[0]
    self.add_symbol(local_name, defn.info, defn)
    if self.is_nested_within_func_scope():
        # We need to preserve local classes, let's store them
        # in globals under mangled unique names
        #
        # TODO: Putting local classes into globals breaks assumptions in fine-grained
        #       incremental mode and we should avoid it. In general, this logic is too
        #       ad-hoc and needs to be removed/refactored.
        if "@" not in defn.info._fullname:
            global_name = defn.info.name + "@" + str(defn.line)
            defn.info._fullname = self.cur_mod_id + "." + global_name
        else:
            # Preserve name from previous fine-grained incremental run.
            global_name = defn.info.name
        defn.fullname = defn.info._fullname
        if defn.info.is_named_tuple:
            # Named tuple nested within a class is stored in the class symbol table.
            self.add_symbol_skip_local(global_name, defn.info)
        else:
            self.globals[global_name] = SymbolTableNode(GDEF, defn.info)

</t>
<t tx="ekr.20220525082935.57">def make_empty_type_info(self, defn: ClassDef) -&gt; TypeInfo:
    if (
        self.is_module_scope()
        and self.cur_mod_id == "builtins"
        and defn.name in CORE_BUILTIN_CLASSES
    ):
        # Special case core built-in classes. A TypeInfo was already
        # created for it before semantic analysis, but with a dummy
        # ClassDef. Patch the real ClassDef object.
        info = self.globals[defn.name].node
        assert isinstance(info, TypeInfo)
    else:
        info = TypeInfo(SymbolTable(), defn, self.cur_mod_id)
        info.set_line(defn)
    return info

</t>
<t tx="ekr.20220525082935.58">def get_name_repr_of_expr(self, expr: Expression) -&gt; str | None:
    """Try finding a short simplified textual representation of a base class expression."""
    if isinstance(expr, NameExpr):
        return expr.name
    if isinstance(expr, MemberExpr):
        return get_member_expr_fullname(expr)
    if isinstance(expr, IndexExpr):
        return self.get_name_repr_of_expr(expr.base)
    if isinstance(expr, CallExpr):
        return self.get_name_repr_of_expr(expr.callee)
    return None

</t>
<t tx="ekr.20220525082935.59">def analyze_base_classes(
    self, base_type_exprs: list[Expression]
) -&gt; tuple[list[tuple[ProperType, Expression]], bool] | None:
    """Analyze base class types.

    Return None if some definition was incomplete. Otherwise, return a tuple
    with these items:

     * List of (analyzed type, original expression) tuples
     * Boolean indicating whether one of the bases had a semantic analysis error
    """
    is_error = False
    bases = []
    for base_expr in base_type_exprs:
        if (
            isinstance(base_expr, RefExpr)
            and base_expr.fullname in TYPED_NAMEDTUPLE_NAMES + TPDICT_NAMES
        ):
            # Ignore magic bases for now.
            continue

        try:
            base = self.expr_to_analyzed_type(base_expr, allow_placeholder=True)
        except TypeTranslationError:
            name = self.get_name_repr_of_expr(base_expr)
            if isinstance(base_expr, CallExpr):
                msg = "Unsupported dynamic base class"
            else:
                msg = "Invalid base class"
            if name:
                msg += f' "{name}"'
            self.fail(msg, base_expr)
            is_error = True
            continue
        if base is None:
            return None
        base = get_proper_type(base)
        bases.append((base, base_expr))
    return bases, is_error

</t>
<t tx="ekr.20220525082935.6">#
# Preparing module (performed before semantic analysis)
#

</t>
<t tx="ekr.20220525082935.60">def configure_base_classes(
    self, defn: ClassDef, bases: list[tuple[ProperType, Expression]]
) -&gt; None:
    """Set up base classes.

    This computes several attributes on the corresponding TypeInfo defn.info
    related to the base classes: defn.info.bases, defn.info.mro, and
    miscellaneous others (at least tuple_type, fallback_to_any, and is_enum.)
    """
    base_types: list[Instance] = []
    info = defn.info

    for base, base_expr in bases:
        if isinstance(base, TupleType):
            actual_base = self.configure_tuple_base_class(defn, base)
            base_types.append(actual_base)
        elif isinstance(base, Instance):
            if base.type.is_newtype:
                self.fail('Cannot subclass "NewType"', defn)
            base_types.append(base)
        elif isinstance(base, AnyType):
            if self.options.disallow_subclassing_any:
                if isinstance(base_expr, (NameExpr, MemberExpr)):
                    msg = f'Class cannot subclass "{base_expr.name}" (has type "Any")'
                else:
                    msg = 'Class cannot subclass value of type "Any"'
                self.fail(msg, base_expr)
            info.fallback_to_any = True
        elif isinstance(base, TypedDictType):
            base_types.append(base.fallback)
        else:
            msg = "Invalid base class"
            name = self.get_name_repr_of_expr(base_expr)
            if name:
                msg += f' "{name}"'
            self.fail(msg, base_expr)
            info.fallback_to_any = True
        if self.options.disallow_any_unimported and has_any_from_unimported_type(base):
            if isinstance(base_expr, (NameExpr, MemberExpr)):
                prefix = f"Base type {base_expr.name}"
            else:
                prefix = "Base type"
            self.msg.unimported_type_becomes_any(prefix, base, base_expr)
        check_for_explicit_any(
            base, self.options, self.is_typeshed_stub_file, self.msg, context=base_expr
        )

    # Add 'object' as implicit base if there is no other base class.
    if not base_types and defn.fullname != "builtins.object":
        base_types.append(self.object_type())

    info.bases = base_types

    # Calculate the MRO.
    if not self.verify_base_classes(defn):
        self.set_dummy_mro(defn.info)
        return
    self.calculate_class_mro(defn, self.object_type)

</t>
<t tx="ekr.20220525082935.61">def configure_tuple_base_class(self, defn: ClassDef, base: TupleType) -&gt; Instance:
    info = defn.info

    # There may be an existing valid tuple type from previous semanal iterations.
    # Use equality to check if it is the case.
    if info.tuple_type and info.tuple_type != base and not has_placeholder(info.tuple_type):
        self.fail("Class has two incompatible bases derived from tuple", defn)
        defn.has_incompatible_baseclass = True
    if info.special_alias and has_placeholder(info.special_alias.target):
        self.defer(force_progress=True)
    info.update_tuple_type(base)
    self.setup_alias_type_vars(defn)

    if base.partial_fallback.type.fullname == "builtins.tuple" and not has_placeholder(base):
        # Fallback can only be safely calculated after semantic analysis, since base
        # classes may be incomplete. Postpone the calculation.
        self.schedule_patch(PRIORITY_FALLBACKS, lambda: calculate_tuple_fallback(base))

    return base.partial_fallback

</t>
<t tx="ekr.20220525082935.62">def set_dummy_mro(self, info: TypeInfo) -&gt; None:
    # Give it an MRO consisting of just the class itself and object.
    info.mro = [info, self.object_type().type]
    info.bad_mro = True

</t>
<t tx="ekr.20220525082935.63">def calculate_class_mro(
    self, defn: ClassDef, obj_type: Callable[[], Instance] | None = None
) -&gt; None:
    """Calculate method resolution order for a class.

    `obj_type` exists just to fill in empty base class list in case of an error.
    """
    try:
        calculate_mro(defn.info, obj_type)
    except MroError:
        self.fail(
            "Cannot determine consistent method resolution "
            'order (MRO) for "%s"' % defn.name,
            defn,
        )
        self.set_dummy_mro(defn.info)
    # Allow plugins to alter the MRO to handle the fact that `def mro()`
    # on metaclasses permits MRO rewriting.
    if defn.fullname:
        hook = self.plugin.get_customize_class_mro_hook(defn.fullname)
        if hook:
            hook(ClassDefContext(defn, FakeExpression(), self))

</t>
<t tx="ekr.20220525082935.64">def infer_metaclass_and_bases_from_compat_helpers(self, defn: ClassDef) -&gt; None:
    """Lookup for special metaclass declarations, and update defn fields accordingly.

    * six.with_metaclass(M, B1, B2, ...)
    * @six.add_metaclass(M)
    * future.utils.with_metaclass(M, B1, B2, ...)
    * past.utils.with_metaclass(M, B1, B2, ...)
    """

    # Look for six.with_metaclass(M, B1, B2, ...)
    with_meta_expr: Expression | None = None
    if len(defn.base_type_exprs) == 1:
        base_expr = defn.base_type_exprs[0]
        if isinstance(base_expr, CallExpr) and isinstance(base_expr.callee, RefExpr):
            base_expr.accept(self)
            if (
                base_expr.callee.fullname
                in {
                    "six.with_metaclass",
                    "future.utils.with_metaclass",
                    "past.utils.with_metaclass",
                }
                and len(base_expr.args) &gt;= 1
                and all(kind == ARG_POS for kind in base_expr.arg_kinds)
            ):
                with_meta_expr = base_expr.args[0]
                defn.base_type_exprs = base_expr.args[1:]

    # Look for @six.add_metaclass(M)
    add_meta_expr: Expression | None = None
    for dec_expr in defn.decorators:
        if isinstance(dec_expr, CallExpr) and isinstance(dec_expr.callee, RefExpr):
            dec_expr.callee.accept(self)
            if (
                dec_expr.callee.fullname == "six.add_metaclass"
                and len(dec_expr.args) == 1
                and dec_expr.arg_kinds[0] == ARG_POS
            ):
                add_meta_expr = dec_expr.args[0]
                break

    metas = {defn.metaclass, with_meta_expr, add_meta_expr} - {None}
    if len(metas) == 0:
        return
    if len(metas) &gt; 1:
        self.fail("Multiple metaclass definitions", defn)
        return
    defn.metaclass = metas.pop()

</t>
<t tx="ekr.20220525082935.65">def verify_base_classes(self, defn: ClassDef) -&gt; bool:
    info = defn.info
    cycle = False
    for base in info.bases:
        baseinfo = base.type
        if self.is_base_class(info, baseinfo):
            self.fail("Cycle in inheritance hierarchy", defn)
            cycle = True
    dup = find_duplicate(info.direct_base_classes())
    if dup:
        self.fail(f'Duplicate base class "{dup.name}"', defn, blocker=True)
        return False
    return not cycle

</t>
<t tx="ekr.20220525082935.66">def is_base_class(self, t: TypeInfo, s: TypeInfo) -&gt; bool:
    """Determine if t is a base class of s (but do not use mro)."""
    # Search the base class graph for t, starting from s.
    worklist = [s]
    visited = {s}
    while worklist:
        nxt = worklist.pop()
        if nxt == t:
            return True
        for base in nxt.bases:
            if base.type not in visited:
                worklist.append(base.type)
                visited.add(base.type)
    return False

</t>
<t tx="ekr.20220525082935.67">def get_declared_metaclass(
    self, name: str, metaclass_expr: Expression | None
) -&gt; tuple[Instance | None, bool]:
    """Returns either metaclass instance or boolean whether we should defer."""
    declared_metaclass = None
    if metaclass_expr:
        metaclass_name = None
        if isinstance(metaclass_expr, NameExpr):
            metaclass_name = metaclass_expr.name
        elif isinstance(metaclass_expr, MemberExpr):
            metaclass_name = get_member_expr_fullname(metaclass_expr)
        if metaclass_name is None:
            self.fail(f'Dynamic metaclass not supported for "{name}"', metaclass_expr)
            return None, False
        sym = self.lookup_qualified(metaclass_name, metaclass_expr)
        if sym is None:
            # Probably a name error - it is already handled elsewhere
            return None, False
        if isinstance(sym.node, Var) and isinstance(get_proper_type(sym.node.type), AnyType):
            # Create a fake TypeInfo that fallbacks to `Any`, basically allowing
            # all the attributes. Same thing as we do for `Any` base class.
            any_info = self.make_empty_type_info(ClassDef(sym.node.name, Block([])))
            any_info.fallback_to_any = True
            any_info._fullname = sym.node.fullname
            if self.options.disallow_subclassing_any:
                self.fail(
                    f'Class cannot use "{any_info.fullname}" as a metaclass (has type "Any")',
                    metaclass_expr,
                )
            return Instance(any_info, []), False
        if isinstance(sym.node, PlaceholderNode):
            return None, True  # defer later in the caller

        # Support type aliases, like `_Meta: TypeAlias = type`
        if (
            isinstance(sym.node, TypeAlias)
            and sym.node.no_args
            and isinstance(sym.node.target, ProperType)
            and isinstance(sym.node.target, Instance)
        ):
            metaclass_info: Node | None = sym.node.target.type
        else:
            metaclass_info = sym.node

        if not isinstance(metaclass_info, TypeInfo) or metaclass_info.tuple_type is not None:
            self.fail(f'Invalid metaclass "{metaclass_name}"', metaclass_expr)
            return None, False
        if not metaclass_info.is_metaclass():
            self.fail(
                'Metaclasses not inheriting from "type" are not supported', metaclass_expr
            )
            return None, False
        inst = fill_typevars(metaclass_info)
        assert isinstance(inst, Instance)
        declared_metaclass = inst
    return declared_metaclass, False

def recalculate_metaclass(self, defn: ClassDef, declared_metaclass: Instance | None) -&gt; None:
    defn.info.declared_metaclass = declared_metaclass
    defn.info.metaclass_type = defn.info.calculate_metaclass_type()
    if any(info.is_protocol for info in defn.info.mro):
        if (
            not defn.info.metaclass_type
            or defn.info.metaclass_type.type.fullname == "builtins.type"
        ):
            # All protocols and their subclasses have ABCMeta metaclass by default.
            # TODO: add a metaclass conflict check if there is another metaclass.
            abc_meta = self.named_type_or_none("abc.ABCMeta", [])
            if abc_meta is not None:  # May be None in tests with incomplete lib-stub.
                defn.info.metaclass_type = abc_meta
    if defn.info.metaclass_type and defn.info.metaclass_type.type.has_base("enum.EnumMeta"):
        defn.info.is_enum = True
        if defn.type_vars:
            self.fail("Enum class cannot be generic", defn)

</t>
<t tx="ekr.20220525082935.68">#
# Imports
#

</t>
<t tx="ekr.20220525082935.69">def visit_import(self, i: Import) -&gt; None:
    self.statement = i
    for id, as_id in i.ids:
        # Modules imported in a stub file without using 'import X as X' won't get exported
        # When implicit re-exporting is disabled, we have the same behavior as stubs.
        use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
        if as_id is not None:
            base_id = id
            imported_id = as_id
            module_public = use_implicit_reexport or id.split(".")[-1] == as_id
        else:
            base_id = id.split(".")[0]
            imported_id = base_id
            module_public = use_implicit_reexport
        self.add_module_symbol(
            base_id,
            imported_id,
            context=i,
            module_public=module_public,
            module_hidden=not module_public,
        )

</t>
<t tx="ekr.20220525082935.7">def prepare_file(self, file_node: MypyFile) -&gt; None:
    """Prepare a freshly parsed file for semantic analysis."""
    if "builtins" in self.modules:
        file_node.names["__builtins__"] = SymbolTableNode(GDEF, self.modules["builtins"])
    if file_node.fullname == "builtins":
        self.prepare_builtins_namespace(file_node)
    if file_node.fullname == "typing":
        self.prepare_typing_namespace(file_node, type_aliases)
    if file_node.fullname == "typing_extensions":
        self.prepare_typing_namespace(file_node, typing_extensions_aliases)

</t>
<t tx="ekr.20220525082935.70">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    self.statement = imp
    module_id = self.correct_relative_import(imp)
    module = self.modules.get(module_id)
    for id, as_id in imp.names:
        fullname = module_id + "." + id
        self.set_future_import_flags(fullname)
        if module is None:
            node = None
        elif module_id == self.cur_mod_id and fullname in self.modules:
            # Submodule takes precedence over definition in surround package, for
            # compatibility with runtime semantics in typical use cases. This
            # could more precisely model runtime semantics by taking into account
            # the line number beyond which the local definition should take
            # precedence, but doesn't seem to be important in most use cases.
            node = SymbolTableNode(GDEF, self.modules[fullname])
        else:
            if id == as_id == "__all__" and module_id in self.export_map:
                self.all_exports[:] = self.export_map[module_id]
            node = module.names.get(id)

        missing_submodule = False
        imported_id = as_id or id

        # Modules imported in a stub file without using 'from Y import X as X' will
        # not get exported.
        # When implicit re-exporting is disabled, we have the same behavior as stubs.
        use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
        module_public = use_implicit_reexport or (as_id is not None and id == as_id)

        # If the module does not contain a symbol with the name 'id',
        # try checking if it's a module instead.
        if not node:
            mod = self.modules.get(fullname)
            if mod is not None:
                kind = self.current_symbol_kind()
                node = SymbolTableNode(kind, mod)
            elif fullname in self.missing_modules:
                missing_submodule = True
        # If it is still not resolved, check for a module level __getattr__
        if (
            module
            and not node
            and (module.is_stub or self.options.python_version &gt;= (3, 7))
            and "__getattr__" in module.names
        ):
            # We store the fullname of the original definition so that we can
            # detect whether two imported names refer to the same thing.
            fullname = module_id + "." + id
            gvar = self.create_getattr_var(module.names["__getattr__"], imported_id, fullname)
            if gvar:
                self.add_symbol(
                    imported_id,
                    gvar,
                    imp,
                    module_public=module_public,
                    module_hidden=not module_public,
                )
                continue

        if node and not node.module_hidden:
            self.process_imported_symbol(
                node, module_id, id, imported_id, fullname, module_public, context=imp
            )
        elif module and not missing_submodule:
            # Target module exists but the imported name is missing or hidden.
            self.report_missing_module_attribute(
                module_id,
                id,
                imported_id,
                module_public=module_public,
                module_hidden=not module_public,
                context=imp,
            )
        else:
            # Import of a missing (sub)module.
            self.add_unknown_imported_symbol(
                imported_id,
                imp,
                target_name=fullname,
                module_public=module_public,
                module_hidden=not module_public,
            )

</t>
<t tx="ekr.20220525082935.71">def process_imported_symbol(
    self,
    node: SymbolTableNode,
    module_id: str,
    id: str,
    imported_id: str,
    fullname: str,
    module_public: bool,
    context: ImportBase,
) -&gt; None:
    module_hidden = not module_public and (
        # `from package import submodule` should work regardless of whether package
        # re-exports submodule, so we shouldn't hide it
        not isinstance(node.node, MypyFile)
        or fullname not in self.modules
        # but given `from somewhere import random_unrelated_module` we should hide
        # random_unrelated_module
        or not fullname.startswith(self.cur_mod_id + ".")
    )

    if isinstance(node.node, PlaceholderNode):
        if self.final_iteration:
            self.report_missing_module_attribute(
                module_id,
                id,
                imported_id,
                module_public=module_public,
                module_hidden=module_hidden,
                context=context,
            )
            return
        else:
            # This might become a type.
            self.mark_incomplete(
                imported_id,
                node.node,
                module_public=module_public,
                module_hidden=module_hidden,
                becomes_typeinfo=True,
            )
    existing_symbol = self.globals.get(imported_id)
    if (
        existing_symbol
        and not isinstance(existing_symbol.node, PlaceholderNode)
        and not isinstance(node.node, PlaceholderNode)
    ):
        # Import can redefine a variable. They get special treatment.
        if self.process_import_over_existing_name(imported_id, existing_symbol, node, context):
            return
    if existing_symbol and isinstance(node.node, PlaceholderNode):
        # Imports are special, some redefinitions are allowed, so wait until
        # we know what is the new symbol node.
        return
    # NOTE: we take the original node even for final `Var`s. This is to support
    # a common pattern when constants are re-exported (same applies to import *).
    self.add_imported_symbol(
        imported_id, node, context, module_public=module_public, module_hidden=module_hidden
    )

</t>
<t tx="ekr.20220525082935.72">def report_missing_module_attribute(
    self,
    import_id: str,
    source_id: str,
    imported_id: str,
    module_public: bool,
    module_hidden: bool,
    context: Node,
) -&gt; None:
    # Missing attribute.
    if self.is_incomplete_namespace(import_id):
        # We don't know whether the name will be there, since the namespace
        # is incomplete. Defer the current target.
        self.mark_incomplete(
            imported_id, context, module_public=module_public, module_hidden=module_hidden
        )
        return
    message = f'Module "{import_id}" has no attribute "{source_id}"'
    # Suggest alternatives, if any match is found.
    module = self.modules.get(import_id)
    if module:
        if not self.options.implicit_reexport and source_id in module.names.keys():
            message = (
                'Module "{}" does not explicitly export attribute "{}"'
                "; implicit reexport disabled".format(import_id, source_id)
            )
        else:
            alternatives = set(module.names.keys()).difference({source_id})
            matches = best_matches(source_id, alternatives)[:3]
            if matches:
                suggestion = f"; maybe {pretty_seq(matches, 'or')}?"
                message += f"{suggestion}"
    self.fail(message, context, code=codes.ATTR_DEFINED)
    self.add_unknown_imported_symbol(
        imported_id,
        context,
        target_name=None,
        module_public=module_public,
        module_hidden=not module_public,
    )

    if import_id == "typing":
        # The user probably has a missing definition in a test fixture. Let's verify.
        fullname = f"builtins.{source_id.lower()}"
        if (
            self.lookup_fully_qualified_or_none(fullname) is None
            and fullname in SUGGESTED_TEST_FIXTURES
        ):
            # Yes. Generate a helpful note.
            self.msg.add_fixture_note(fullname, context)

</t>
<t tx="ekr.20220525082935.73">def process_import_over_existing_name(
    self,
    imported_id: str,
    existing_symbol: SymbolTableNode,
    module_symbol: SymbolTableNode,
    import_node: ImportBase,
) -&gt; bool:
    if existing_symbol.node is module_symbol.node:
        # We added this symbol on previous iteration.
        return False
    if existing_symbol.kind in (LDEF, GDEF, MDEF) and isinstance(
        existing_symbol.node, (Var, FuncDef, TypeInfo, Decorator, TypeAlias)
    ):
        # This is a valid import over an existing definition in the file. Construct a dummy
        # assignment that we'll use to type check the import.
        lvalue = NameExpr(imported_id)
        lvalue.kind = existing_symbol.kind
        lvalue.node = existing_symbol.node
        rvalue = NameExpr(imported_id)
        rvalue.kind = module_symbol.kind
        rvalue.node = module_symbol.node
        if isinstance(rvalue.node, TypeAlias):
            # Suppress bogus errors from the dummy assignment if rvalue is an alias.
            # Otherwise mypy may complain that alias is invalid in runtime context.
            rvalue.is_alias_rvalue = True
        assignment = AssignmentStmt([lvalue], rvalue)
        for node in assignment, lvalue, rvalue:
            node.set_line(import_node)
        import_node.assignments.append(assignment)
        return True
    return False

</t>
<t tx="ekr.20220525082935.74">def correct_relative_import(self, node: ImportFrom | ImportAll) -&gt; str:
    import_id, ok = correct_relative_import(
        self.cur_mod_id, node.relative, node.id, self.cur_mod_node.is_package_init_file()
    )
    if not ok:
        self.fail("Relative import climbs too many namespaces", node)
    return import_id

</t>
<t tx="ekr.20220525082935.75">def visit_import_all(self, i: ImportAll) -&gt; None:
    i_id = self.correct_relative_import(i)
    if i_id in self.modules:
        m = self.modules[i_id]
        if self.is_incomplete_namespace(i_id):
            # Any names could be missing from the current namespace if the target module
            # namespace is incomplete.
            self.mark_incomplete("*", i)
        for name, node in m.names.items():
            fullname = i_id + "." + name
            self.set_future_import_flags(fullname)
            if node is None:
                continue
            # if '__all__' exists, all nodes not included have had module_public set to
            # False, and we can skip checking '_' because it's been explicitly included.
            if node.module_public and (not name.startswith("_") or "__all__" in m.names):
                if isinstance(node.node, MypyFile):
                    # Star import of submodule from a package, add it as a dependency.
                    self.imports.add(node.node.fullname)
                existing_symbol = self.lookup_current_scope(name)
                if existing_symbol and not isinstance(node.node, PlaceholderNode):
                    # Import can redefine a variable. They get special treatment.
                    if self.process_import_over_existing_name(name, existing_symbol, node, i):
                        continue
                # `from x import *` always reexports symbols
                self.add_imported_symbol(
                    name, node, i, module_public=True, module_hidden=False
                )

    else:
        # Don't add any dummy symbols for 'from x import *' if 'x' is unknown.
        pass

</t>
<t tx="ekr.20220525082935.76">#
# Assignment
#

</t>
<t tx="ekr.20220525082935.77">def visit_assignment_expr(self, s: AssignmentExpr) -&gt; None:
    s.value.accept(self)
    self.analyze_lvalue(s.target, escape_comprehensions=True, has_explicit_value=True)

</t>
<t tx="ekr.20220525082935.78">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    self.statement = s

    # Special case assignment like X = X.
    if self.analyze_identity_global_assignment(s):
        return

    tag = self.track_incomplete_refs()

    # Here we have a chicken and egg problem: at this stage we can't call
    # can_be_type_alias(), because we have not enough information about rvalue.
    # But we can't use a full visit because it may emit extra incomplete refs (namely
    # when analysing any type applications there) thus preventing the further analysis.
    # To break the tie, we first analyse rvalue partially, if it can be a type alias.
    with self.basic_type_applications_set(s):
        s.rvalue.accept(self)
    if self.found_incomplete_ref(tag) or self.should_wait_rhs(s.rvalue):
        # Initializer couldn't be fully analyzed. Defer the current node and give up.
        # Make sure that if we skip the definition of some local names, they can't be
        # added later in this scope, since an earlier definition should take precedence.
        for expr in names_modified_by_assignment(s):
            self.mark_incomplete(expr.name, expr)
        return
    if self.can_possibly_be_index_alias(s):
        # Now re-visit those rvalues that were we skipped type applications above.
        # This should be safe as generally semantic analyzer is idempotent.
        s.rvalue.accept(self)

    # The r.h.s. is now ready to be classified, first check if it is a special form:
    special_form = False
    # * type alias
    if self.check_and_set_up_type_alias(s):
        s.is_alias_def = True
        special_form = True
    # * type variable definition
    elif self.process_typevar_declaration(s):
        special_form = True
    elif self.process_paramspec_declaration(s):
        special_form = True
    elif self.process_typevartuple_declaration(s):
        special_form = True
    # * type constructors
    elif self.analyze_namedtuple_assign(s):
        special_form = True
    elif self.analyze_typeddict_assign(s):
        special_form = True
    elif self.newtype_analyzer.process_newtype_declaration(s):
        special_form = True
    elif self.analyze_enum_assign(s):
        special_form = True

    if special_form:
        self.record_special_form_lvalue(s)
        return
    # Clear the alias flag if assignment turns out not a special form after all. It
    # may be set to True while there were still placeholders due to forward refs.
    s.is_alias_def = False

    # OK, this is a regular assignment, perform the necessary analysis steps.
    s.is_final_def = self.unwrap_final(s)
    self.analyze_lvalues(s)
    self.check_final_implicit_def(s)
    self.store_final_status(s)
    self.check_classvar(s)
    self.process_type_annotation(s)
    self.apply_dynamic_class_hook(s)
    if not s.type:
        self.process_module_assignment(s.lvalues, s.rvalue, s)
    self.process__all__(s)
    self.process__deletable__(s)
    self.process__slots__(s)

</t>
<t tx="ekr.20220525082935.79">def analyze_identity_global_assignment(self, s: AssignmentStmt) -&gt; bool:
    """Special case 'X = X' in global scope.

    This allows supporting some important use cases.

    Return true if special casing was applied.
    """
    if not isinstance(s.rvalue, NameExpr) or len(s.lvalues) != 1:
        # Not of form 'X = X'
        return False
    lvalue = s.lvalues[0]
    if not isinstance(lvalue, NameExpr) or s.rvalue.name != lvalue.name:
        # Not of form 'X = X'
        return False
    if self.type is not None or self.is_func_scope():
        # Not in global scope
        return False
    # It's an assignment like 'X = X' in the global scope.
    name = lvalue.name
    sym = self.lookup(name, s)
    if sym is None:
        if self.final_iteration:
            # Fall back to normal assignment analysis.
            return False
        else:
            self.defer()
            return True
    else:
        if sym.node is None:
            # Something special -- fall back to normal assignment analysis.
            return False
        if name not in self.globals:
            # The name is from builtins. Add an alias to the current module.
            self.add_symbol(name, sym.node, s)
        if not isinstance(sym.node, PlaceholderNode):
            for node in s.rvalue, lvalue:
                node.node = sym.node
                node.kind = GDEF
                node.fullname = sym.node.fullname
        return True

</t>
<t tx="ekr.20220525082935.8">def prepare_typing_namespace(self, file_node: MypyFile, aliases: dict[str, str]) -&gt; None:
    """Remove dummy alias definitions such as List = TypeAlias(object) from typing.

    They will be replaced with real aliases when corresponding targets are ready.
    """
    # This is all pretty unfortunate. typeshed now has a
    # sys.version_info check for OrderedDict, and we shouldn't
    # take it out, because it is correct and a typechecker should
    # use that as a source of truth. But instead we rummage
    # through IfStmts to remove the info first.  (I tried to
    # remove this whole machinery and ran into issues with the
    # builtins/typing import cycle.)
    def helper(defs: list[Statement]) -&gt; None:
        for stmt in defs.copy():
            if isinstance(stmt, IfStmt):
                for body in stmt.body:
                    helper(body.body)
                if stmt.else_body:
                    helper(stmt.else_body.body)
            if (
                isinstance(stmt, AssignmentStmt)
                and len(stmt.lvalues) == 1
                and isinstance(stmt.lvalues[0], NameExpr)
            ):
                # Assignment to a simple name, remove it if it is a dummy alias.
                if f"{file_node.fullname}.{stmt.lvalues[0].name}" in aliases:
                    defs.remove(stmt)

    helper(file_node.defs)

</t>
<t tx="ekr.20220525082935.80">def should_wait_rhs(self, rv: Expression) -&gt; bool:
    """Can we already classify this r.h.s. of an assignment or should we wait?

    This returns True if we don't have enough information to decide whether
    an assignment is just a normal variable definition or a special form.
    Always return False if this is a final iteration. This will typically cause
    the lvalue to be classified as a variable plus emit an error.
    """
    if self.final_iteration:
        # No chance, nothing has changed.
        return False
    if isinstance(rv, NameExpr):
        n = self.lookup(rv.name, rv)
        if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
            return True
    elif isinstance(rv, MemberExpr):
        fname = get_member_expr_fullname(rv)
        if fname:
            n = self.lookup_qualified(fname, rv, suppress_errors=True)
            if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
                return True
    elif isinstance(rv, IndexExpr) and isinstance(rv.base, RefExpr):
        return self.should_wait_rhs(rv.base)
    elif isinstance(rv, CallExpr) and isinstance(rv.callee, RefExpr):
        # This is only relevant for builtin SCC where things like 'TypeVar'
        # may be not ready.
        return self.should_wait_rhs(rv.callee)
    return False

</t>
<t tx="ekr.20220525082935.81">def can_be_type_alias(self, rv: Expression, allow_none: bool = False) -&gt; bool:
    """Is this a valid r.h.s. for an alias definition?

    Note: this function should be only called for expressions where self.should_wait_rhs()
    returns False.
    """
    if isinstance(rv, RefExpr) and self.is_type_ref(rv, bare=True):
        return True
    if isinstance(rv, IndexExpr) and self.is_type_ref(rv.base, bare=False):
        return True
    if self.is_none_alias(rv):
        return True
    if allow_none and isinstance(rv, NameExpr) and rv.fullname == "builtins.None":
        return True
    if isinstance(rv, OpExpr) and rv.op == "|":
        if self.is_stub_file:
            return True
        if self.can_be_type_alias(rv.left, allow_none=True) and self.can_be_type_alias(
            rv.right, allow_none=True
        ):
            return True
    return False

def can_possibly_be_index_alias(self, s: AssignmentStmt) -&gt; bool:
    """Like can_be_type_alias(), but simpler and doesn't require analyzed rvalue.

    Instead, use lvalues/annotations structure to figure out whether this can
    potentially be a type alias definition. Another difference from above function
    is that we are only interested IndexExpr and OpExpr rvalues, since only those
    can be potentially recursive (things like `A = A` are never valid).
    """
    if len(s.lvalues) &gt; 1:
        return False
    if not isinstance(s.lvalues[0], NameExpr):
        return False
    if s.unanalyzed_type is not None and not self.is_pep_613(s):
        return False
    if not isinstance(s.rvalue, (IndexExpr, OpExpr)):
        return False
    # Something that looks like Foo = Bar[Baz, ...]
    return True

@contextmanager
def basic_type_applications_set(self, s: AssignmentStmt) -&gt; Iterator[None]:
    old = self.basic_type_applications
    # As an optimization, only use the double visit logic if this
    # can possibly be a recursive type alias.
    self.basic_type_applications = self.can_possibly_be_index_alias(s)
    try:
        yield
    finally:
        self.basic_type_applications = old

</t>
<t tx="ekr.20220525082935.82">def is_type_ref(self, rv: Expression, bare: bool = False) -&gt; bool:
    """Does this expression refer to a type?

    This includes:
      * Special forms, like Any or Union
      * Classes (except subscripted enums)
      * Other type aliases
      * PlaceholderNodes with becomes_typeinfo=True (these can be not ready class
        definitions, and not ready aliases).

    If bare is True, this is not a base of an index expression, so some special
    forms are not valid (like a bare Union).

    Note: This method should be only used in context of a type alias definition.
    This method can only return True for RefExprs, to check if C[int] is a valid
    target for type alias call this method on expr.base (i.e. on C in C[int]).
    See also can_be_type_alias().
    """
    if not isinstance(rv, RefExpr):
        return False
    if isinstance(rv.node, TypeVarLikeExpr):
        self.fail(f'Type variable "{rv.fullname}" is invalid as target for type alias', rv)
        return False

    if bare:
        # These three are valid even if bare, for example
        # A = Tuple is just equivalent to A = Tuple[Any, ...].
        valid_refs = {"typing.Any", "typing.Tuple", "typing.Callable"}
    else:
        valid_refs = type_constructors

    if isinstance(rv.node, TypeAlias) or rv.fullname in valid_refs:
        return True
    if isinstance(rv.node, TypeInfo):
        if bare:
            return True
        # Assignment color = Color['RED'] defines a variable, not an alias.
        return not rv.node.is_enum
    if isinstance(rv.node, Var):
        return rv.node.fullname in NEVER_NAMES

    if isinstance(rv, NameExpr):
        n = self.lookup(rv.name, rv)
        if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
            return True
    elif isinstance(rv, MemberExpr):
        fname = get_member_expr_fullname(rv)
        if fname:
            # The r.h.s. for variable definitions may not be a type reference but just
            # an instance attribute, so suppress the errors.
            n = self.lookup_qualified(fname, rv, suppress_errors=True)
            if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
                return True
    return False

</t>
<t tx="ekr.20220525082935.83">def is_none_alias(self, node: Expression) -&gt; bool:
    """Is this a r.h.s. for a None alias?

    We special case the assignments like Void = type(None), to allow using
    Void in type annotations.
    """
    if isinstance(node, CallExpr):
        if (
            isinstance(node.callee, NameExpr)
            and len(node.args) == 1
            and isinstance(node.args[0], NameExpr)
        ):
            call = self.lookup_qualified(node.callee.name, node.callee)
            arg = self.lookup_qualified(node.args[0].name, node.args[0])
            if (
                call is not None
                and call.node
                and call.node.fullname == "builtins.type"
                and arg is not None
                and arg.node
                and arg.node.fullname == "builtins.None"
            ):
                return True
    return False

</t>
<t tx="ekr.20220525082935.84">def record_special_form_lvalue(self, s: AssignmentStmt) -&gt; None:
    """Record minimal necessary information about l.h.s. of a special form.

    This exists mostly for compatibility with the old semantic analyzer.
    """
    lvalue = s.lvalues[0]
    assert isinstance(lvalue, NameExpr)
    lvalue.is_special_form = True
    if self.current_symbol_kind() == GDEF:
        lvalue.fullname = self.qualified_name(lvalue.name)
    lvalue.kind = self.current_symbol_kind()

</t>
<t tx="ekr.20220525082935.85">def analyze_enum_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines an Enum."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, EnumCallExpr):
        # Already analyzed enum -- nothing to do here.
        return True
    return self.enum_call_analyzer.process_enum_call(s, self.is_func_scope())

</t>
<t tx="ekr.20220525082935.86">def analyze_namedtuple_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines a namedtuple."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, NamedTupleExpr):
        if s.rvalue.analyzed.info.tuple_type and not has_placeholder(
            s.rvalue.analyzed.info.tuple_type
        ):
            return True  # This is a valid and analyzed named tuple definition, nothing to do here.
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    namespace = self.qualified_name(name)
    with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
        internal_name, info, tvar_defs = self.named_tuple_analyzer.check_namedtuple(
            s.rvalue, name, self.is_func_scope()
        )
        if internal_name is None:
            return False
        if isinstance(lvalue, MemberExpr):
            self.fail("NamedTuple type as an attribute is not supported", lvalue)
            return False
        if internal_name != name:
            self.fail(
                'First argument to namedtuple() should be "{}", not "{}"'.format(
                    name, internal_name
                ),
                s.rvalue,
                code=codes.NAME_MATCH,
            )
            return True
        # Yes, it's a valid namedtuple, but defer if it is not ready.
        if not info:
            self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
        else:
            self.setup_type_vars(info.defn, tvar_defs)
            self.setup_alias_type_vars(info.defn)
        return True

</t>
<t tx="ekr.20220525082935.87">def analyze_typeddict_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines a typed dict."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, TypedDictExpr):
        if s.rvalue.analyzed.info.typeddict_type and not has_placeholder(
            s.rvalue.analyzed.info.typeddict_type
        ):
            # This is a valid and analyzed typed dict definition, nothing to do here.
            return True
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    namespace = self.qualified_name(name)
    with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
        is_typed_dict, info, tvar_defs = self.typed_dict_analyzer.check_typeddict(
            s.rvalue, name, self.is_func_scope()
        )
        if not is_typed_dict:
            return False
        if isinstance(lvalue, MemberExpr):
            self.fail("TypedDict type as attribute is not supported", lvalue)
            return False
        # Yes, it's a valid typed dict, but defer if it is not ready.
        if not info:
            self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
        else:
            defn = info.defn
            self.setup_type_vars(defn, tvar_defs)
            self.setup_alias_type_vars(defn)
        return True

</t>
<t tx="ekr.20220525082935.88">def analyze_lvalues(self, s: AssignmentStmt) -&gt; None:
    # We cannot use s.type, because analyze_simple_literal_type() will set it.
    explicit = s.unanalyzed_type is not None
    if self.is_final_type(s.unanalyzed_type):
        # We need to exclude bare Final.
        assert isinstance(s.unanalyzed_type, UnboundType)
        if not s.unanalyzed_type.args:
            explicit = False

    if s.rvalue:
        if isinstance(s.rvalue, TempNode):
            has_explicit_value = not s.rvalue.no_rhs
        else:
            has_explicit_value = True
    else:
        has_explicit_value = False

    for lval in s.lvalues:
        self.analyze_lvalue(
            lval,
            explicit_type=explicit,
            is_final=s.is_final_def,
            has_explicit_value=has_explicit_value,
        )

</t>
<t tx="ekr.20220525082935.89">def apply_dynamic_class_hook(self, s: AssignmentStmt) -&gt; None:
    if not isinstance(s.rvalue, CallExpr):
        return
    fname = None
    call = s.rvalue
    while True:
        if isinstance(call.callee, RefExpr):
            fname = call.callee.fullname
        # check if method call
        if fname is None and isinstance(call.callee, MemberExpr):
            callee_expr = call.callee.expr
            if isinstance(callee_expr, RefExpr) and callee_expr.fullname:
                method_name = call.callee.name
                fname = callee_expr.fullname + "." + method_name
            elif isinstance(callee_expr, CallExpr):
                # check if chain call
                call = callee_expr
                continue
        break
    if not fname:
        return
    hook = self.plugin.get_dynamic_class_hook(fname)
    if not hook:
        return
    for lval in s.lvalues:
        if not isinstance(lval, NameExpr):
            continue
        hook(DynamicClassDefContext(call, lval.name, self))

</t>
<t tx="ekr.20220525082935.9">def prepare_builtins_namespace(self, file_node: MypyFile) -&gt; None:
    """Add certain special-cased definitions to the builtins module.

    Some definitions are too special or fundamental to be processed
    normally from the AST.
    """
    names = file_node.names

    # Add empty definition for core built-in classes, since they are required for basic
    # operation. These will be completed later on.
    for name in CORE_BUILTIN_CLASSES:
        cdef = ClassDef(name, Block([]))  # Dummy ClassDef, will be replaced later
        info = TypeInfo(SymbolTable(), cdef, "builtins")
        info._fullname = f"builtins.{name}"
        names[name] = SymbolTableNode(GDEF, info)

    bool_info = names["bool"].node
    assert isinstance(bool_info, TypeInfo)
    bool_type = Instance(bool_info, [])

    special_var_types: list[tuple[str, Type]] = [
        ("None", NoneType()),
        # reveal_type is a mypy-only function that gives an error with
        # the type of its arg.
        ("reveal_type", AnyType(TypeOfAny.special_form)),
        # reveal_locals is a mypy-only function that gives an error with the types of
        # locals
        ("reveal_locals", AnyType(TypeOfAny.special_form)),
        ("True", bool_type),
        ("False", bool_type),
        ("__debug__", bool_type),
    ]

    for name, typ in special_var_types:
        v = Var(name, typ)
        v._fullname = f"builtins.{name}"
        file_node.names[name] = SymbolTableNode(GDEF, v)

</t>
<t tx="ekr.20220525082935.90">def unwrap_final(self, s: AssignmentStmt) -&gt; bool:
    """Strip Final[...] if present in an assignment.

    This is done to invoke type inference during type checking phase for this
    assignment. Also, Final[...] doesn't affect type in any way -- it is rather an
    access qualifier for given `Var`.

    Also perform various consistency checks.

    Returns True if Final[...] was present.
    """
    if not s.unanalyzed_type or not self.is_final_type(s.unanalyzed_type):
        return False
    assert isinstance(s.unanalyzed_type, UnboundType)
    if len(s.unanalyzed_type.args) &gt; 1:
        self.fail("Final[...] takes at most one type argument", s.unanalyzed_type)
    invalid_bare_final = False
    if not s.unanalyzed_type.args:
        s.type = None
        if isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs:
            invalid_bare_final = True
            self.fail("Type in Final[...] can only be omitted if there is an initializer", s)
    else:
        s.type = s.unanalyzed_type.args[0]

    if s.type is not None and self.is_classvar(s.type):
        self.fail("Variable should not be annotated with both ClassVar and Final", s)
        return False

    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], RefExpr):
        self.fail("Invalid final declaration", s)
        return False
    lval = s.lvalues[0]
    assert isinstance(lval, RefExpr)

    # Reset inferred status if it was set due to simple literal rvalue on previous iteration.
    # TODO: this is a best-effort quick fix, we should avoid the need to manually sync this,
    # see https://github.com/python/mypy/issues/6458.
    if lval.is_new_def:
        lval.is_inferred_def = s.type is None

    if self.loop_depth &gt; 0:
        self.fail("Cannot use Final inside a loop", s)
    if self.type and self.type.is_protocol:
        self.msg.protocol_members_cant_be_final(s)
    if (
        isinstance(s.rvalue, TempNode)
        and s.rvalue.no_rhs
        and not self.is_stub_file
        and not self.is_class_scope()
    ):
        if not invalid_bare_final:  # Skip extra error messages.
            self.msg.final_without_value(s)
    return True

</t>
<t tx="ekr.20220525082935.91">def check_final_implicit_def(self, s: AssignmentStmt) -&gt; None:
    """Do basic checks for final declaration on self in __init__.

    Additional re-definition checks are performed by `analyze_lvalue`.
    """
    if not s.is_final_def:
        return
    lval = s.lvalues[0]
    assert isinstance(lval, RefExpr)
    if isinstance(lval, MemberExpr):
        if not self.is_self_member_ref(lval):
            self.fail("Final can be only applied to a name or an attribute on self", s)
            s.is_final_def = False
            return
        else:
            assert self.function_stack
            if self.function_stack[-1].name != "__init__":
                self.fail("Can only declare a final attribute in class body or __init__", s)
                s.is_final_def = False
                return

</t>
<t tx="ekr.20220525082935.92">def store_final_status(self, s: AssignmentStmt) -&gt; None:
    """If this is a locally valid final declaration, set the corresponding flag on `Var`."""
    if s.is_final_def:
        if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
            node = s.lvalues[0].node
            if isinstance(node, Var):
                node.is_final = True
                node.final_value = self.unbox_literal(s.rvalue)
                if self.is_class_scope() and (
                    isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs
                ):
                    node.final_unset_in_class = True
    else:
        for lval in self.flatten_lvalues(s.lvalues):
            # Special case: we are working with an `Enum`:
            #
            #   class MyEnum(Enum):
            #       key = 'some value'
            #
            # Here `key` is implicitly final. In runtime, code like
            #
            #     MyEnum.key = 'modified'
            #
            # will fail with `AttributeError: Cannot reassign members.`
            # That's why we need to replicate this.
            if (
                isinstance(lval, NameExpr)
                and isinstance(self.type, TypeInfo)
                and self.type.is_enum
            ):
                cur_node = self.type.names.get(lval.name, None)
                if (
                    cur_node
                    and isinstance(cur_node.node, Var)
                    and not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)
                ):
                    # Double underscored members are writable on an `Enum`.
                    # (Except read-only `__members__` but that is handled in type checker)
                    cur_node.node.is_final = s.is_final_def = not is_dunder(cur_node.node.name)

            # Special case: deferred initialization of a final attribute in __init__.
            # In this case we just pretend this is a valid final definition to suppress
            # errors about assigning to final attribute.
            if isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                assert self.type, "Self member outside a class"
                cur_node = self.type.names.get(lval.name, None)
                if cur_node and isinstance(cur_node.node, Var) and cur_node.node.is_final:
                    assert self.function_stack
                    top_function = self.function_stack[-1]
                    if (
                        top_function.name == "__init__"
                        and cur_node.node.final_unset_in_class
                        and not cur_node.node.final_set_in_init
                        and not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)
                    ):
                        cur_node.node.final_set_in_init = True
                        s.is_final_def = True

</t>
<t tx="ekr.20220525082935.93">def flatten_lvalues(self, lvalues: list[Expression]) -&gt; list[Expression]:
    res: list[Expression] = []
    for lv in lvalues:
        if isinstance(lv, (TupleExpr, ListExpr)):
            res.extend(self.flatten_lvalues(lv.items))
        else:
            res.append(lv)
    return res

</t>
<t tx="ekr.20220525082935.94">def unbox_literal(self, e: Expression) -&gt; int | float | bool | str | None:
    if isinstance(e, (IntExpr, FloatExpr, StrExpr)):
        return e.value
    elif isinstance(e, NameExpr) and e.name in ("True", "False"):
        return True if e.name == "True" else False
    return None

</t>
<t tx="ekr.20220525082935.95">def process_type_annotation(self, s: AssignmentStmt) -&gt; None:
    """Analyze type annotation or infer simple literal type."""
    &lt;&lt; define callers &amp; callerName &gt;&gt;
    if s.type:
        lvalue = s.lvalues[-1]
        allow_tuple_literal = isinstance(lvalue, TupleExpr)
        analyzed = self.anal_type(s.type, allow_tuple_literal=allow_tuple_literal)
        # Don't store not ready types (including placeholders).
        if analyzed is None or has_placeholder(analyzed):
            self.defer(s)
            return
        s.type = analyzed
        if (
            self.type
            and self.type.is_protocol
            and isinstance(lvalue, NameExpr)
            and isinstance(s.rvalue, TempNode)
            and s.rvalue.no_rhs
        ):
            if isinstance(lvalue.node, Var):
                lvalue.node.is_abstract_var = True
    else:
        if (
            self.type
            and self.type.is_protocol
            and self.is_annotated_protocol_member(s)
            and not self.is_func_scope()
        ):
            self.fail("All protocol members must have explicitly declared types", s)
        # Set the type if the rvalue is a simple literal (even if the above error occurred).
        if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
            ref_expr = s.lvalues[0]
            safe_literal_inference = True
            if self.type and isinstance(ref_expr, NameExpr) and len(self.type.mro) &gt; 1:
                # Check if there is a definition in supertype. If yes, we can't safely
                # decide here what to infer: int or Literal[42].
                safe_literal_inference = self.type.mro[1].get(ref_expr.name) is None
            if safe_literal_inference and ref_expr.is_inferred_def:
                s.type = self.analyze_simple_literal_type(s.rvalue, s.is_final_def)
    if s.type:
        # Store type into nodes.
        for lvalue in s.lvalues:
            self.store_declared_types(lvalue, s.type)

</t>
<t tx="ekr.20220525082935.96">def is_annotated_protocol_member(self, s: AssignmentStmt) -&gt; bool:
    """Check whether a protocol member is annotated.

    There are some exceptions that can be left unannotated, like ``__slots__``."""
    return any(
        (isinstance(lv, NameExpr) and lv.name != "__slots__" and lv.is_inferred_def)
        for lv in s.lvalues
    )

</t>
<t tx="ekr.20220525082935.97">def analyze_simple_literal_type(self, rvalue: Expression, is_final: bool) -&gt; Type | None:
    """Return builtins.int if rvalue is an int literal, etc.
    If this is a 'Final' context, we return "Literal[...]" instead."""
    if self.options.semantic_analysis_only or self.function_stack:
        # Skip this if we're only doing the semantic analysis pass.
        # This is mostly to avoid breaking unit tests.
        # Also skip inside a function; this is to avoid confusing
        # the code that handles dead code due to isinstance()
        # inside type variables with value restrictions (like
        # AnyStr).
        return None
    if isinstance(rvalue, FloatExpr):
        return self.named_type_or_none("builtins.float")

    value: LiteralValue | None = None
    type_name: str | None = None
    if isinstance(rvalue, IntExpr):
        value, type_name = rvalue.value, "builtins.int"
    if isinstance(rvalue, StrExpr):
        value, type_name = rvalue.value, "builtins.str"
    if isinstance(rvalue, BytesExpr):
        value, type_name = rvalue.value, "builtins.bytes"

    if type_name is not None:
        assert value is not None
        typ = self.named_type_or_none(type_name)
        if typ and is_final:
            return typ.copy_modified(
                last_known_value=LiteralType(
                    value=value, fallback=typ, line=typ.line, column=typ.column
                )
            )
        return typ

    return None

</t>
<t tx="ekr.20220525082935.98">def analyze_alias(
    self, rvalue: Expression, allow_placeholder: bool = False
) -&gt; tuple[Type | None, list[str], set[str], list[str]]:
    """Check if 'rvalue' is a valid type allowed for aliasing (e.g. not a type variable).

    If yes, return the corresponding type, a list of
    qualified type variable names for generic aliases, a set of names the alias depends on,
    and a list of type variables if the alias is generic.
    An schematic example for the dependencies:
        A = int
        B = str
        analyze_alias(Dict[A, B])[2] == {'__main__.A', '__main__.B'}
    """
    dynamic = bool(self.function_stack and self.function_stack[-1].is_dynamic())
    global_scope = not self.type and not self.function_stack
    res = analyze_type_alias(
        rvalue,
        self,
        self.tvar_scope,
        self.plugin,
        self.options,
        self.is_typeshed_stub_file,
        allow_placeholder=allow_placeholder,
        in_dynamic_func=dynamic,
        global_scope=global_scope,
    )
    typ: Type | None = None
    if res:
        typ, depends_on = res
        found_type_vars = typ.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
        alias_tvars = [name for (name, node) in found_type_vars]
        qualified_tvars = [node.fullname for (name, node) in found_type_vars]
    else:
        alias_tvars = []
        depends_on = set()
        qualified_tvars = []
    return typ, alias_tvars, depends_on, qualified_tvars

def is_pep_613(self, s: AssignmentStmt) -&gt; bool:
    if s.unanalyzed_type is not None and isinstance(s.unanalyzed_type, UnboundType):
        lookup = self.lookup_qualified(s.unanalyzed_type.name, s, suppress_errors=True)
        if lookup and lookup.fullname in TYPE_ALIAS_NAMES:
            return True
    return False

</t>
<t tx="ekr.20220525082935.99">def check_and_set_up_type_alias(self, s: AssignmentStmt) -&gt; bool:
    """Check if assignment creates a type alias and set it up as needed.

    Return True if it is a type alias (even if the target is not ready),
    or False otherwise.

    Note: the resulting types for subscripted (including generic) aliases
    are also stored in rvalue.analyzed.
    """
    if s.invalid_recursive_alias:
        return True
    lvalue = s.lvalues[0]
    if len(s.lvalues) &gt; 1 or not isinstance(lvalue, NameExpr):
        # First rule: Only simple assignments like Alias = ... create aliases.
        return False

    pep_613 = self.is_pep_613(s)
    if not pep_613 and s.unanalyzed_type is not None:
        # Second rule: Explicit type (cls: Type[A] = A) always creates variable, not alias.
        # unless using PEP 613 `cls: TypeAlias = A`
        return False

    if isinstance(s.rvalue, CallExpr) and s.rvalue.analyzed:
        return False

    existing = self.current_symbol_table().get(lvalue.name)
    # Third rule: type aliases can't be re-defined. For example:
    #     A: Type[float] = int
    #     A = float  # OK, but this doesn't define an alias
    #     B = int
    #     B = float  # Error!
    # Don't create an alias in these cases:
    if existing and (
        isinstance(existing.node, Var)  # existing variable
        or (isinstance(existing.node, TypeAlias) and not s.is_alias_def)  # existing alias
        or (isinstance(existing.node, PlaceholderNode) and existing.node.node.line &lt; s.line)
    ):  # previous incomplete definition
        # TODO: find a more robust way to track the order of definitions.
        # Note: if is_alias_def=True, this is just a node from previous iteration.
        if isinstance(existing.node, TypeAlias) and not s.is_alias_def:
            self.fail(
                'Cannot assign multiple types to name "{}"'
                ' without an explicit "Type[...]" annotation'.format(lvalue.name),
                lvalue,
            )
        return False

    non_global_scope = self.type or self.is_func_scope()
    if not pep_613 and isinstance(s.rvalue, RefExpr) and non_global_scope:
        # Fourth rule (special case): Non-subscripted right hand side creates a variable
        # at class and function scopes. For example:
        #
        #   class Model:
        #       ...
        #   class C:
        #       model = Model # this is automatically a variable with type 'Type[Model]'
        #
        # without this rule, this typical use case will require a lot of explicit
        # annotations (see the second rule).
        return False
    rvalue = s.rvalue
    if not pep_613 and not self.can_be_type_alias(rvalue):
        return False

    if existing and not isinstance(existing.node, (PlaceholderNode, TypeAlias)):
        # Cannot redefine existing node as type alias.
        return False

    res: Type | None = None
    if self.is_none_alias(rvalue):
        res = NoneType()
        alias_tvars: list[str] = []
        depends_on: set[str] = set()
        qualified_tvars: list[str] = []
    else:
        tag = self.track_incomplete_refs()
        res, alias_tvars, depends_on, qualified_tvars = self.analyze_alias(
            rvalue, allow_placeholder=True
        )
        if not res:
            return False
        if not self.options.disable_recursive_aliases and not self.is_func_scope():
            # Only marking incomplete for top-level placeholders makes recursive aliases like
            # `A = Sequence[str | A]` valid here, similar to how we treat base classes in class
            # definitions, allowing `class str(Sequence[str]): ...`
            incomplete_target = isinstance(res, ProperType) and isinstance(
                res, PlaceholderType
            )
        else:
            incomplete_target = has_placeholder(res)
        if self.found_incomplete_ref(tag) or incomplete_target:
            # Since we have got here, we know this must be a type alias (incomplete refs
            # may appear in nested positions), therefore use becomes_typeinfo=True.
            self.mark_incomplete(lvalue.name, rvalue, becomes_typeinfo=True)
            return True
    self.add_type_alias_deps(depends_on)
    # In addition to the aliases used, we add deps on unbound
    # type variables, since they are erased from target type.
    self.add_type_alias_deps(qualified_tvars)
    # The above are only direct deps on other aliases.
    # For subscripted aliases, type deps from expansion are added in deps.py
    # (because the type is stored).
    check_for_explicit_any(res, self.options, self.is_typeshed_stub_file, self.msg, context=s)
    # When this type alias gets "inlined", the Any is not explicit anymore,
    # so we need to replace it with non-explicit Anys.
    res = make_any_non_explicit(res)
    # Note: with the new (lazy) type alias representation we only need to set no_args to True
    # if the expected number of arguments is non-zero, so that aliases like A = List work.
    # However, eagerly expanding aliases like Text = str is a nice performance optimization.
    no_args = isinstance(res, Instance) and not res.args  # type: ignore[misc]
    fix_instance_types(res, self.fail, self.note, self.options.python_version)
    # Aliases defined within functions can't be accessed outside
    # the function, since the symbol table will no longer
    # exist. Work around by expanding them eagerly when used.
    eager = self.is_func_scope()
    alias_node = TypeAlias(
        res,
        self.qualified_name(lvalue.name),
        s.line,
        s.column,
        alias_tvars=alias_tvars,
        no_args=no_args,
        eager=eager,
    )
    if isinstance(s.rvalue, (IndexExpr, CallExpr)):  # CallExpr is for `void = type(None)`
        s.rvalue.analyzed = TypeAliasExpr(alias_node)
        s.rvalue.analyzed.line = s.line
        # we use the column from resulting target, to get better location for errors
        s.rvalue.analyzed.column = res.column
    elif isinstance(s.rvalue, RefExpr):
        s.rvalue.is_alias_rvalue = True

    if existing:
        # An alias gets updated.
        updated = False
        if isinstance(existing.node, TypeAlias):
            if existing.node.target != res:
                # Copy expansion to the existing alias, this matches how we update base classes
                # for a TypeInfo _in place_ if there are nested placeholders.
                existing.node.target = res
                existing.node.alias_tvars = alias_tvars
                existing.node.no_args = no_args
                updated = True
        else:
            # Otherwise just replace existing placeholder with type alias.
            existing.node = alias_node
            updated = True
        if updated:
            if self.final_iteration:
                self.cannot_resolve_name(lvalue.name, "name", s)
                return True
            else:
                # We need to defer so that this change can get propagated to base classes.
                self.defer(s, force_progress=True)
    else:
        self.add_symbol(lvalue.name, alias_node, s)
    if isinstance(rvalue, RefExpr) and isinstance(rvalue.node, TypeAlias):
        alias_node.normalized = rvalue.node.normalized
    current_node = existing.node if existing else alias_node
    assert isinstance(current_node, TypeAlias)
    self.disable_invalid_recursive_aliases(s, current_node)
    if self.is_class_scope():
        assert self.type is not None
        if self.type.is_protocol:
            self.fail("Type aliases are prohibited in protocol bodies", s)
            if not lvalue.name[0].isupper():
                self.note("Use variable annotation syntax to define protocol members", s)
    return True

</t>
<t tx="ekr.20220525082936.34">@mypyc_attr(allow_interpreted_subclasses=True)
class TypeTranslator(TypeVisitor[Type]):
    """Identity type transformation.

    Subclass this and override some methods to implement a non-trivial
    transformation.
    """

    @others
</t>
<t tx="ekr.20220525082936.35">def visit_unbound_type(self, t: UnboundType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.36">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.37">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.38">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.39">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.40">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.41">def visit_instance(self, t: Instance) -&gt; Type:
    last_known_value: LiteralType | None = None
    if t.last_known_value is not None:
        raw_last_known_value = t.last_known_value.accept(self)
        assert isinstance(raw_last_known_value, LiteralType)  # type: ignore[misc]
        last_known_value = raw_last_known_value
    return Instance(
        typ=t.type,
        args=self.translate_types(t.args),
        line=t.line,
        column=t.column,
        last_known_value=last_known_value,
    )

</t>
<t tx="ekr.20220525082936.42">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.43">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.44">def visit_parameters(self, t: Parameters) -&gt; Type:
    return t.copy_modified(arg_types=self.translate_types(t.arg_types))

</t>
<t tx="ekr.20220525082936.45">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.46">def visit_partial_type(self, t: PartialType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.47">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    return UnpackType(t.type.accept(self))

</t>
<t tx="ekr.20220525082936.48">def visit_callable_type(self, t: CallableType) -&gt; Type:
    return t.copy_modified(
        arg_types=self.translate_types(t.arg_types),
        ret_type=t.ret_type.accept(self),
        variables=self.translate_variables(t.variables),
    )

</t>
<t tx="ekr.20220525082936.49">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    return TupleType(
        self.translate_types(t.items),
        # TODO: This appears to be unsafe.
        cast(Any, t.partial_fallback.accept(self)),
        t.line,
        t.column,
    )

</t>
<t tx="ekr.20220525082936.50">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = {item_name: item_type.accept(self) for (item_name, item_type) in t.items.items()}
    return TypedDictType(
        items,
        t.required_keys,
        # TODO: This appears to be unsafe.
        cast(Any, t.fallback.accept(self)),
        t.line,
        t.column,
    )

</t>
<t tx="ekr.20220525082936.51">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    fallback = t.fallback.accept(self)
    assert isinstance(fallback, Instance)  # type: ignore[misc]
    return LiteralType(value=t.value, fallback=fallback, line=t.line, column=t.column)

</t>
<t tx="ekr.20220525082936.52">def visit_union_type(self, t: UnionType) -&gt; Type:
    return UnionType(self.translate_types(t.items), t.line, t.column)

</t>
<t tx="ekr.20220525082936.53">def translate_types(self, types: Iterable[Type]) -&gt; list[Type]:
    return [t.accept(self) for t in types]

</t>
<t tx="ekr.20220525082936.54">def translate_variables(
    self, variables: Sequence[TypeVarLikeType]
) -&gt; Sequence[TypeVarLikeType]:
    return variables

</t>
<t tx="ekr.20220525082936.55">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    items: list[CallableType] = []
    for item in t.items:
        new = item.accept(self)
        assert isinstance(new, CallableType)  # type: ignore[misc]
        items.append(new)
    return Overloaded(items=items)

</t>
<t tx="ekr.20220525082936.56">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(t.item.accept(self), line=t.line, column=t.column)

</t>
<t tx="ekr.20220525082936.57">@abstractmethod
def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # This method doesn't have a default implementation for type translators,
    # because type aliases are special: some information is contained in the
    # TypeAlias node, and we normally don't generate new nodes. Every subclass
    # must implement this depending on its semantics.
    pass


</t>
<t tx="ekr.20220526075331.1"></t>
<t tx="ekr.20220530121430.1">@language rest
@nowrap

Contributing (testing): https://github.com/python/mypy/blob/master/CONTRIBUTING.md

Adding tests: https://github.com/python/mypy/blob/master/test-data/unit/README.md</t>
<t tx="ekr.20220530121437.1">Adding tests: https://github.com/python/mypy/blob/master/test-data/unit/README.md

To add a simple unit test for a new feature you developed, create test-data/unit/check-*.test file
with a name that roughly relates to the feature you added.

If you added a new check-*.test file, add it to the list of files in mypy/test/testcheck.py.

Add the test in this format anywhere in the file:

[case testNewSyntaxBasics]
# flags: --python-version 3.6
x: int
x = 5
y: int = 5

a: str
a = 5  # E: Incompatible types in assignment (expression has type "int", variable has type "str")
b: str = 5  # E: Incompatible types in assignment (expression has type "int", variable has type "str")

zzz: int
zzz: str  # E: Name "zzz" already defined

- no code here is executed, just type checked
- optional # flags: indicates which flags to use for this unit test
- # E: abc... indicates that this line should result in type check error with text "abc..."
- note a space after E: and flags:
- # E:12 adds column number to the expected error
- use \ to escape the # character and indicate that the rest of the line is part of the error message
- repeating # E: several times in one line indicates multiple expected errors in one line
- W: ... and N: ... works exactly like E: ..., but report a warning and a note respectively
- lines that don't contain the above should cause no type check errors
- optional [builtins fixtures/...] tells the type checker to use stubs from the indicated file
  (see Fixtures section below)
- optional [out] is an alternative to the # E: notation:
  it indicates that any text after it contains the expected type checking error messages.
  Usually, # E: is preferred because it makes it easier to associate the errors with the code
  generating them at a glance, and to change the code of the test without having to change line numbers in [out]
- an empty [out] section has no effect
- to add tests for a feature that hasn't been implemented yet, append -xfail to the end of the test name
- to run just this test, use pytest -n0 -k testNewSyntaxBasics
</t>
<t tx="ekr.20220531170425.1">=============================================================== short test summary info ===============================================================
FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testException
FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testTryExcept
FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testExceptionAtModuleTopLevel
FAILED mypyc/test/test_run.py::TestRun::run-loops.test::testForIterable
FAILED mypyc/test/test_run.py::TestRun::run-classes.test::testProperty
FAILED mypyc/test/test_run.py::TestRun::run-classes.test::testSubclassUninitAttr
FAILED mypyc/test/test_run.py::TestRun::run-generators.test::testYieldThrow
FAILED mypyc/test/test_run.py::TestRun::run-multimodule.test::testMultiModuleTraceback
FAILED mypyc/test/test_run.py::TestRunMultiFile::run-multimodule.test::testMultiModuleTraceback_multi
FAILED mypyc/test/test_run.py::TestRunSeparate::run-multimodule.test::testMultiModuleTraceback_separate
======================================== 10 failed, 10100 passed, 380 skipped, 9 xfailed in 388.00s (0:06:28) =========================================
</t>
<t tx="ekr.20220531174005.1">
--- Tests of mypy issue #12352: https://github.com/python/mypy/issues/12352

[case testDefaultString]
# flags: --disallow-untyped-defs
--- def f1(a):
---    pass
--- reveal_type(f1)  # N: Revealed type is "def (a: Any) -&gt; Any"

def f1_str(a="abc") -&gt; None:
    pass

--- reveal_type(f1_str)  # N: Revealed type is "def (a: Any =) -&gt; None"

[builtins fixtures/tuple.pyi]
</t>
<t tx="ekr.20220531174305.1">@language python

# Use cd-mypy.cmd first. It does an activate, which is essential.

g.cls()
import subprocess

# mypy.cmd contains:
# mypy ekr_test.py --show-traceback --pdb %*

if 0:  # Standalone test.
    command = 'mypy ekr_test.py'
else:
    # Must add the test file to mypy/test/testcheck.py
    test = 'mypy/test/testcheck.py::TypeCheckSuite::check-default-annotations.test'
    pdb = '' # '--pdb'
    command = f"pytest {test} {pdb}"
proc = subprocess.Popen(command, shell=True)
assert(proc)

# Print statements don't seem to be useful: they are printed after all the tests run!

# proc.communicate()
# print('done')</t>
<t tx="ekr.20220601070439.1"># Use cd-mypy.cmd first. It does an activate, which is essential.

g.cls()
import subprocess
subprocess.Popen('pytest', shell=True)

# Expect the following failures:

# FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testExceptionAtModuleTopLevel
# FAILED mypyc/test/test_run.py::TestRun::run-loops.test::testForIterable
# FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testException
# FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testTryExcept
# FAILED mypyc/test/test_run.py::TestRun::run-classes.test::testSubclassUninitAttr
# FAILED mypyc/test/test_run.py::TestRun::run-generators.test::testYieldThrow
# FAILED mypyc/test/test_run.py::TestRun::run-classes.test::testProperty
# FAILED mypyc/test/test_run.py::TestRun::run-multimodule.test::testMultiModuleTraceback
# FAILED mypyc/test/test_run.py::TestRunMultiFile::run-multimodule.test::testMultiModuleTraceback_multi
# FAILED mypyc/test/test_run.py::TestRunSeparate::run-multimodule.test::testMultiModuleTraceback_separate

# 10 failed, 10101 passed, 380 skipped, 9 xfailed in 449.48s (0:07:29)
</t>
<t tx="ekr.20220601131028.1"></t>
<t tx="ekr.20220601131038.1"></t>
<t tx="ekr.20220601131042.1">test-one
test-all
</t>
<t tx="ekr.20220603080610.1">@language python
@tabwidth -4
@nosearch
@path C:/Repos/ekr-mypy2
"""Stand-alone test file for issue #12352"""

def ekr_f_annotated(ekr_a: str) -&gt; None:
    pass
    
# Passes with legacy mypy.
def ekr_f_annotated_initialized(ekr_a: str="abc") -&gt; None:
    pass

# Fails with legacy mypy.
# Change this case!
def ekr_f_not_annotated(ekr_a="abc", i=1, f=0.1, aBool=True, ekr_b=b's' ) -&gt; None:
    pass
    
# Later
def ekr_f_not_annotated2(b: int, ekr_a="abc") -&gt; None:
    pass
    
# a: str="abc"
# b="xyz"
# c: str
</t>
<t tx="ekr.20220603081942.1"></t>
<t tx="ekr.20220603103251.1">@language python
@nowrap

# 
# State.compute_dependencies: Every module implicitly depends on 'builtins'
# 'c:\\repos\\ekr-mypy\\mypy\\typeshed\\stdlib\\builtins.pyi'</t>
<t tx="ekr.20220605090347.1"></t>
<t tx="ekr.20220606071921.1"></t>
<t tx="ekr.20220606071936.1">c.backup_helper(sub_dir='mypy.leo')
</t>
<t tx="ekr.20220606085857.1"></t>
<t tx="ekr.20220606101206.1">@nosearch</t>
<t tx="ekr.20220607072154.1">@nosearch

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept, visit_if_stmt, visit_block, accept, accept,
visit_assignment_stmt, check_and_set_up_type_alias, analyze_alias, analyze_type_alias, accept,
visit_assignment_stmt, anal_array,
anal_type, accept, visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, process_typevar_declaration, process_typevar_parameters, expr_to_analyzed_type,
anal_type, accept, visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept, visit_assignment_stmt, anal_array,
anal_type, accept, visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept, visit_assignment_stmt, anal_array,
anal_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, check_and_set_up_type_alias, analyze_alias, analyze_type_alias, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, analyze_type_with_type_info, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, check_and_set_up_type_alias, analyze_alias, analyze_type_alias, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type,
anal_type, accept,
visit_assignment_stmt, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept, visit_if_stmt, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, check_and_set_up_type_alias, analyze_alias, analyze_type_alias, accept,
visit_assignment_stmt, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, accept, visit_index_expr, accept, visit_tuple_expr, accept, visit_index_expr, analyze_type_application, analyze_type_application_args,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, check_and_set_up_type_alias, analyze_alias, analyze_type_alias, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, analyze_type_with_type_info, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels,
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, check_classvar,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, analyze_type_with_type_info, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels,
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, analyze_type_with_type_info, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels,
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_if_stmt, visit_block_maybe, visit_block, accept, accept,
visit_if_stmt, visit_block, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept, visit_if_stmt, visit_block, accept, accept,
visit_assignment_stmt, check_and_set_up_type_alias, analyze_alias, analyze_type_alias, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept, visit_if_stmt, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_top_levels, 
semantic_analyze_target, refresh_partial, refresh_top_level, accept, accept,
visit_if_stmt, visit_block_maybe, visit_block, accept, accept,
visit_if_stmt, visit_block, accept, accept,
visit_class_def, analyze_class, analyze_class_body_common, accept, visit_block, accept, accept,
visit_assignment_stmt, process_type_annotation,
anal_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function,
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_var_defs, &lt;listcomp&gt;, anal_var_def, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, analyze_type_with_type_info, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
anal_type, accept,
visit_assignment_stmt, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept, visit_overloaded_func_def, analyze_overloaded_func_def, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, analyze_overload_sigs_and_impl, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept,
visit_assignment_stmt, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, analyze_overload_sigs_and_impl, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept,
visit_assignment_stmt, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept,
visit_assignment_stmt, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, analyze_overload_sigs_and_impl, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_var_defs, &lt;listcomp&gt;, anal_var_def, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, analyze_overload_sigs_and_impl, accept, visit_decorator, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_overloaded_func_def, analyze_overloaded_func_def, analyze_property_with_multi_part_definition, accept,
visit_func_def, analyze_func_def, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_array,
anal_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type,
anal_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type, anal_type_guard, anal_type_guard_arg,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type

callers:
&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch, process_graph, process_stale_scc,
semantic_analysis_for_scc, process_functions, process_top_level_function, 
semantic_analyze_target, refresh_partial, accept, accept,
visit_func_def, analyze_func_def, visit_callable_type,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type, anal_type_guard_arg,
anal_type, accept,
visit_unbound_type, visit_unbound_type_nonoptional, try_analyze_special_unbound_type,
analyze_callable_type, accept, visit_callable_type
</t>
<t tx="ekr.20220608151139.1"></t>
<t tx="ekr.20220612050429.1">def callers(n: int=4) -&gt; str:
    """
    Return a string containing a comma-separated list of the callers
    of the function that called g.callerList.
    """
    i, result = 2, []
    while True:
        s = callerName(n=i)
        if s:
            result.append(s)
        if not s or len(result) &gt;= n:
            break
        i += 1
    return ', '.join(reversed(result))

def callerName(n: int) -&gt; str:
    """Get the function name from the call stack."""
    import sys
    try:
        f1 = sys._getframe(n)
        code1 = f1.f_code
        return code1.co_name
    except Exception:
        return ''
</t>
<t tx="ekr.20220612061447.1"></t>
<t tx="ekr.20220613154216.1"></t>
<t tx="ekr.20220614100511.1">"""The semantic analyzer.

Bind names to definitions and do various other simple consistency
checks.  Populate symbol tables.  The semantic analyzer also detects
special forms which reuse generic syntax such as NamedTuple and
cast().  Multiple analysis iterations may be needed to analyze forward
references and import cycles. Each iteration "fills in" additional
bindings and references until everything has been bound.

For example, consider this program:

  x = 1
  y = x

Here semantic analysis would detect that the assignment 'x = 1'
defines a new variable, the type of which is to be inferred (in a
later pass; type inference or type checking is not part of semantic
analysis).  Also, it would bind both references to 'x' to the same
module-level variable (Var) node.  The second assignment would also
be analyzed, and the type of 'y' marked as being inferred.

Semantic analysis of types is implemented in typeanal.py.

See semanal_main.py for the top-level logic.

Some important properties:

* After semantic analysis is complete, no PlaceholderNode and
  PlaceholderType instances should remain. During semantic analysis,
  if we encounter one of these, the current target should be deferred.

* A TypeInfo is only created once we know certain basic information about
  a type, such as the MRO, existence of a Tuple base class (e.g., for named
  tuples), and whether we have a TypedDict. We use a temporary
  PlaceholderNode node in the symbol table if some such information is
  missing.

* For assignments, we only add a non-placeholder symbol table entry once
  we know the sort of thing being defined (variable, NamedTuple, type alias,
  etc.).

* Every part of the analysis step must support multiple iterations over
  the same AST nodes, and each iteration must be able to fill in arbitrary
  things that were missing or incomplete in previous iterations.

* Changes performed by the analysis need to be reversible, since mypy
  daemon strips and reuses existing ASTs (to improve performance and/or
  reduce memory use).
"""
</t>
<t tx="ekr.20220614100550.1">"""Top-level logic for the semantic analyzer.

The semantic analyzer binds names, resolves imports, detects various
special constructs that don't have dedicated AST nodes after parse
(such as 'cast' which looks like a call), populates symbol tables, and
performs various simple consistency checks.

Semantic analysis of each SCC (strongly connected component; import
cycle) is performed in one unit. Each module is analyzed as multiple
separate *targets*; the module top level is one target and each function
is a target. Nested functions are not separate targets, however. This is
mostly identical to targets used by mypy daemon (but classes aren't
targets in semantic analysis).

We first analyze each module top level in an SCC. If we encounter some
names that we can't bind because the target of the name may not have
been processed yet, we *defer* the current target for further
processing. Deferred targets will be analyzed additional times until
everything can be bound, or we reach a maximum number of iterations.

We keep track of a set of incomplete namespaces, i.e. namespaces that we
haven't finished populating yet. References to these namespaces cause a
deferral if they can't be satisfied. Initially every module in the SCC
will be incomplete.
"""
</t>
<t tx="ekr.20220614100654.1"></t>
<t tx="ekr.20220614100953.1"></t>
<t tx="ekr.20220614101046.1"></t>
<t tx="ekr.20220614101128.1"></t>
<t tx="ekr.20220614101220.1"></t>
<t tx="ekr.20220614101246.1"></t>
<t tx="ekr.20220614101407.1"></t>
<t tx="ekr.20220617075745.1">"""Plugin system for extending mypy.

At large scale the plugin system works as following:

* Plugins are collected from the corresponding mypy config file option
  (either via paths to Python files, or installed Python modules)
  and imported using importlib.

* Every module should get an entry point function (called 'plugin' by default,
  but may be overridden in the config file) that should accept a single string
  argument that is a full mypy version (includes git commit hash for dev
  versions) and return a subclass of mypy.plugins.Plugin.

* All plugin class constructors should match the signature of mypy.plugin.Plugin
  (i.e. should accept an mypy.options.Options object), and *must* call
  super().__init__().

* At several steps during semantic analysis and type checking mypy calls
  special `get_xxx` methods on user plugins with a single string argument that
  is a fully qualified name (full name) of a relevant definition
  (see mypy.plugin.Plugin method docstrings for details).

* The plugins are called in the order they are passed in the config option.
  Every plugin must decide whether to act on a given full name. The first
  plugin that returns non-None object will be used.

* The above decision should be made using the limited common API specified by
  mypy.plugin.CommonPluginApi.

* The callback returned by the plugin will be called with a larger context that
  includes relevant current state (e.g. a default return type, or a default
  attribute type) and a wider relevant API provider (e.g.
  SemanticAnalyzerPluginInterface or CheckerPluginInterface).

* The result of this is used for further processing. See various `XxxContext`
  named tuples for details about which information is given to each hook.

Plugin developers should ensure that their plugins work well in incremental and
daemon modes. In particular, plugins should not hold global state, and should
always call add_plugin_dependency() in plugin hooks called during semantic
analysis. See the method docstring for more details.

There is no dedicated cache storage for plugins, but plugins can store
per-TypeInfo data in a special .metadata attribute that is serialized to the
mypy caches between incremental runs. To avoid collisions between plugins, they
are encouraged to store their state under a dedicated key coinciding with
plugin name in the metadata dictionary. Every value stored there must be
JSON-serializable.

## Notes about the semantic analyzer

Mypy 0.710 introduced a new semantic analyzer that changed how plugins are
expected to work in several notable ways (from mypy 0.730 the old semantic
analyzer is no longer available):

1. The order of processing AST nodes in modules is different. The old semantic
   analyzer processed modules in textual order, one module at a time. The new
   semantic analyzer first processes the module top levels, including bodies of
   any top-level classes and classes nested within classes. ("Top-level" here
   means "not nested within a function/method".) Functions and methods are
   processed only after module top levels have been finished. If there is an
   import cycle, all module top levels in the cycle are processed before
   processing any functions or methods. Each unit of processing (a module top
   level or a function/method) is called a *target*.

   This also means that function signatures in the same module have not been
   analyzed yet when analyzing the module top level. If you need access to
   a function signature, you'll need to explicitly analyze the signature first
   using `anal_type()`.

2. Each target can be processed multiple times. This may happen if some forward
   references are not ready yet, for example. This means that semantic analyzer
   related plugin hooks can be called multiple times for the same full name.
   These plugin methods must thus be idempotent.

3. The `anal_type` API function returns None if some part of the type is not
   available yet. If this happens, the current target being analyzed will be
   *deferred*, which means that it will be processed again soon, in the hope
   that additional dependencies will be available. This may happen if there are
   forward references to types or inter-module references to types within an
   import cycle.

   Note that if there is a circular definition, mypy may decide to stop
   processing to avoid an infinite number of iterations. When this happens,
   `anal_type` will generate an error and return an `AnyType` type object
   during the final iteration (instead of None).

4. There is a new API method `defer()`. This can be used to explicitly request
   the current target to be reprocessed one more time. You don't need this
   to call this if `anal_type` returns None, however.

5. There is a new API property `final_iteration`, which is true once mypy
   detected no progress during the previous iteration or if the maximum
   semantic analysis iteration count has been reached. You must never
   defer during the final iteration, as it will cause a crash.

6. The `node` attribute of SymbolTableNode objects may contain a reference to
   a PlaceholderNode object. This object means that this definition has not
   been fully processed yet. If you encounter a PlaceholderNode, you should
   defer unless it's the final iteration. If it's the final iteration, you
   should generate an error message. It usually means that there's a cyclic
   definition that cannot be resolved by mypy. PlaceholderNodes can only refer
   to references inside an import cycle. If you are looking up things from
   another module, such as the builtins, that is outside the current module or
   import cycle, you can safely assume that you won't receive a placeholder.

When testing your plugin, you should have a test case that forces a module top
level to be processed multiple times. The easiest way to do this is to include
a forward reference to a class in a top-level annotation. Example:

    c: C  # Forward reference causes second analysis pass
    class C: pass

Note that a forward reference in a function signature won't trigger another
pass, since all functions are processed only after the top level has been fully
analyzed.

You can use `api.options.new_semantic_analyzer` to check whether the new
semantic analyzer is enabled (it's always true in mypy 0.730 and later).
"""
</t>
<t tx="ekr.20220617084344.1">sources, options = process_options(args, stdout=stdout, stderr=stderr, fscache=fscache)
if clean_exit:
    options.fast_exit = False

formatter = util.FancyFormatter(stdout, stderr, options.hide_error_codes)

if options.install_types and (stdout is not sys.stdout or stderr is not sys.stderr):
    # Since --install-types performs user input, we want regular stdout and stderr.
    fail("error: --install-types not supported in this mode of running mypy", stderr, options)

if options.non_interactive and not options.install_types:
    fail("error: --non-interactive is only supported with --install-types", stderr, options)

if options.install_types and not options.incremental:
    fail(
        "error: --install-types not supported with incremental mode disabled", stderr, options
    )

if options.install_types and options.python_executable is None:
    fail(
        "error: --install-types not supported without python executable or site packages",
        stderr,
        options,
    )

</t>
<t tx="ekr.20220617085103.1">if options.non_interactive:
    missing_pkgs = read_types_packages_to_install(options.cache_dir, after_run=True)
    if missing_pkgs:
        # Install missing type packages and rerun build.
        install_types(formatter, options, after_run=True, non_interactive=True)
        fscache.flush()
        print()
        res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)
    show_messages(messages, stderr, formatter, options)

if MEM_PROFILE:
    from mypy.memprofile import print_memory_profile

    print_memory_profile()

code = 0
if messages:
    code = 2 if blockers else 1
if options.error_summary:
    n_errors, n_notes, n_files = util.count_stats(messages)
    if n_errors:
        summary = formatter.format_error(
            n_errors, n_files, len(sources), blockers=blockers, use_color=options.color_output
        )
        stdout.write(summary + "\n")
    # Only notes should also output success
    elif not messages or n_notes == len(messages):
        stdout.write(formatter.format_success(len(sources), options.color_output) + "\n")
    stdout.flush()

if options.install_types and not options.non_interactive:
    result = install_types(formatter, options, after_run=True, non_interactive=False)
    if result:
        print()
        print("note: Run mypy again for up-to-date results with installed types")
        code = 2

if options.fast_exit:
    # Exit without freeing objects -- it's faster.
    #
    # NOTE: We don't flush all open files on exit (or run other destructors)!
    util.hard_exit(code)
elif code:
    sys.exit(code)
</t>
<t tx="ekr.20220617085407.1">if (
    options.warn_unused_configs
    and options.unused_configs
    and not options.incremental
    and not options.non_interactive
):
    print(
        "Warning: unused section(s) in %s: %s"
        % (
            options.config_file,
            get_config_module_names(
                options.config_file,
                [
                    glob
                    for glob in options.per_module_options.keys()
                    if glob in options.unused_configs
                ],
            ),
        ),
        file=stderr,
    )
</t>
<t tx="ekr.20220617085508.1">"""Analyze a program.

A single call to build performs parsing, semantic analysis and optionally
type checking for the program *and* all imported modules, recursively.

Return BuildResult if successful or only non-blocking errors were found;
otherwise raise CompileError.

If a flush_errors callback is provided, all error messages will be
passed to it and the errors and messages fields of BuildResult and
CompileError (respectively) will be empty. Otherwise those fields will
report any error messages.

Args:
  sources: list of sources to build
  options: build options
  alt_lib_path: an additional directory for looking up library modules
    (takes precedence over other directories)
  flush_errors: optional function to flush errors after a file is processed
  fscache: optionally a file-system cacher

"""
</t>
<t tx="ekr.20220617090951.1"># This is a kind of unfortunate hack to work around some of fine-grained's
# fragility: if we have loaded less than 50% of the specified files from
# cache in fine-grained cache mode, load the graph again honestly.
# In this case, we just turn the cache off entirely, so we don't need
# to worry about some files being loaded and some from cache and so
# that fine-grained mode never *writes* to the cache.
if manager.use_fine_grained_cache() and len(graph) &lt; 0.50 * len(sources):
    manager.log("Redoing load_graph without cache because too much was missing")
    manager.cache_enabled = False
    graph = load_graph(sources, manager)

t1 = time.time()
manager.add_stats(
    graph_size=len(graph),
    stubs_found=sum(g.path is not None and g.path.endswith(".pyi") for g in graph.values()),
    graph_load_time=(t1 - t0),
    fm_cache_size=len(manager.find_module_cache.results),
)
if not graph:
    print("Nothing to do?!", file=stdout)
    return graph
manager.log(f"Loaded graph with {len(graph)} nodes ({t1 - t0:.3f} sec)")
if manager.options.dump_graph:
    dump_graph(graph, stdout)
    return graph

# Fine grained dependencies that didn't have an associated module in the build
# are serialized separately, so we read them after we load the graph.
# We need to read them both for running in daemon mode and if we are generating
# a fine-grained cache (so that we can properly update them incrementally).
# The `read_deps_cache` will also validate
# the deps cache against the loaded individual cache files.
if manager.options.cache_fine_grained or manager.use_fine_grained_cache():
    t2 = time.time()
    fg_deps_meta = read_deps_cache(manager, graph)
    manager.add_stats(load_fg_deps_time=time.time() - t2)
    if fg_deps_meta is not None:
        manager.fg_deps_meta = fg_deps_meta
    elif manager.stats.get("fresh_metas", 0) &gt; 0:
        # Clear the stats so we don't infinite loop because of positive fresh_metas
        manager.stats.clear()
        # There were some cache files read, but no fine-grained dependencies loaded.
        manager.log("Error reading fine-grained dependencies cache -- aborting cache load")
        manager.cache_enabled = False
        manager.log("Falling back to full run -- reloading graph...")
        return dispatch(sources, manager, stdout)

</t>
<t tx="ekr.20220617091557.1">if len(fresh_scc_queue) &gt; 0:
    manager.log(f"Processing {len(fresh_scc_queue)} queued fresh SCCs")
    # Defer processing fresh SCCs until we actually run into a stale SCC
    # and need the earlier modules to be loaded.
    #
    # Note that `process_graph` may end with us not having processed every
    # single fresh SCC. This is intentional -- we don't need those modules
    # loaded if there are no more stale SCCs to be rechecked.
    #
    # Also note we shouldn't have to worry about transitive_error here,
    # since modules with transitive errors aren't written to the cache,
    # and if any dependencies were changed, this SCC would be stale.
    # (Also, in quick_and_dirty mode we don't care about transitive errors.)
    #
    # TODO: see if it's possible to determine if we need to process only a
    # _subset_ of the past SCCs instead of having to process them all.
    for prev_scc in fresh_scc_queue:
        process_fresh_modules(graph, prev_scc, manager)
    fresh_scc_queue = []
size = len(scc)
if size == 1:
    manager.log(f"Processing SCC singleton ({scc_str}) as {fresh_msg}")
else:
    manager.log("Processing SCC of size %d (%s) as %s" % (size, scc_str, fresh_msg))
</t>
<t tx="ekr.20220617091730.1"># Order the SCC's nodes using a heuristic.
# Note that ascc is a set, and scc is a list.
scc = order_ascc(graph, ascc)
# Make the order of the SCC that includes 'builtins' and 'typing',
# among other things, predictable. Various things may  break if
# the order changes.
if "builtins" in ascc:
    scc = sorted(scc, reverse=True)
    # If builtins is in the list, move it last.  (This is a bit of
    # a hack, but it's necessary because the builtins module is
    # part of a small cycle involving at least {builtins, abc,
    # typing}.  Of these, builtins must be processed last or else
    # some builtin objects will be incompletely processed.)
    scc.remove("builtins")
    scc.append("builtins")
if manager.options.verbosity &gt;= 2:
    for id in scc:
        manager.trace(
            f"Priorities for {id}:",
            " ".join(
                "%s:%d" % (x, graph[id].priorities[x])
                for x in graph[id].dependencies
                if x in ascc and x in graph[id].priorities
            ),
        )
# Because the SCCs are presented in topological sort order, we
# don't need to look at dependencies recursively for staleness
# -- the immediate dependencies are sufficient.
stale_scc = {id for id in scc if not graph[id].is_fresh()}
fresh = not stale_scc
deps = set()
for id in scc:
    deps.update(graph[id].dependencies)
deps -= ascc
stale_deps = {id for id in deps if id in graph and not graph[id].is_interface_fresh()}
fresh = fresh and not stale_deps
undeps = set()
if fresh:
    # Check if any dependencies that were suppressed according
    # to the cache have been added back in this run.
    # NOTE: Newly suppressed dependencies are handled by is_fresh().
    for id in scc:
        undeps.update(graph[id].suppressed)
    undeps &amp;= graph.keys()
    if undeps:
        fresh = False
if fresh:
    # All cache files are fresh.  Check that no dependency's
    # cache file is newer than any scc node's cache file.
    oldest_in_scc = min(graph[id].xmeta.data_mtime for id in scc)
    viable = {id for id in stale_deps if graph[id].meta is not None}
    newest_in_deps = (
        0 if not viable else max(graph[dep].xmeta.data_mtime for dep in viable)
    )
    if manager.options.verbosity &gt;= 3:  # Dump all mtimes for extreme debugging.
        all_ids = sorted(ascc | viable, key=lambda id: graph[id].xmeta.data_mtime)
        for id in all_ids:
            if id in scc:
                if graph[id].xmeta.data_mtime &lt; newest_in_deps:
                    key = "*id:"
                else:
                    key = "id:"
            else:
                if graph[id].xmeta.data_mtime &gt; oldest_in_scc:
                    key = "+dep:"
                else:
                    key = "dep:"
            manager.trace(" %5s %.0f %s" % (key, graph[id].xmeta.data_mtime, id))
    # If equal, give the benefit of the doubt, due to 1-sec time granularity
    # (on some platforms).
    if oldest_in_scc &lt; newest_in_deps:
        fresh = False
        fresh_msg = f"out of date by {newest_in_deps - oldest_in_scc:.0f} seconds"
    else:
        fresh_msg = "fresh"
elif undeps:
    fresh_msg = f"stale due to changed suppression ({' '.join(sorted(undeps))})"
elif stale_scc:
    fresh_msg = "inherently stale"
    if stale_scc != ascc:
        fresh_msg += f" ({' '.join(sorted(stale_scc))})"
    if stale_deps:
        fresh_msg += f" with stale deps ({' '.join(sorted(stale_deps))})"
else:
    fresh_msg = f"stale due to deps ({' '.join(sorted(stale_deps))})"

# Initialize transitive_error for all SCC members from union
# of transitive_error of dependencies.
if any(graph[dep].transitive_error for dep in deps if dep in graph):
    for id in scc:
        graph[id].transitive_error = True

</t>
<t tx="ekr.20220617092314.1"># Update plugins snapshot.
write_plugins_snapshot(manager)
manager.old_plugins_snapshot = manager.plugins_snapshot
if manager.options.cache_fine_grained or manager.options.fine_grained_incremental:
    # If we are running a daemon or are going to write cache for further fine grained use,
    # then we need to collect fine grained protocol dependencies.
    # Since these are a global property of the program, they are calculated after we
    # processed the whole graph.
    TypeState.add_all_protocol_deps(manager.fg_deps)
    if not manager.options.fine_grained_incremental:
        rdeps = generate_deps_for_cache(manager, graph)
        write_deps_cache(rdeps, manager, graph)

</t>
<t tx="ekr.20220617092355.1">if manager.options.dump_deps:
    # This speeds up startup a little when not using the daemon mode.
    from mypy.server.deps import dump_all_dependencies

    dump_all_dependencies(
        manager.modules, manager.all_types, manager.options.python_version, manager.options
    )
</t>
<t tx="ekr.20220617092600.1">
# Construct a build manager object to hold state during the build.
#
# Ignore current directory prefix in error messages.
manager = BuildManager(
    data_dir,
    search_paths,
    ignore_prefix=os.getcwd(),
    source_set=source_set,
    reports=reports,
    options=options,
    version_id=__version__,
    plugin=plugin,
    plugins_snapshot=snapshot,
    errors=errors,
    flush_errors=flush_errors,
    fscache=fscache,
    stdout=stdout,
    stderr=stderr,
)
manager.trace(repr(options))

</t>
<t tx="ekr.20220617092625.1">errors = Errors(
    options.show_error_context,
    options.show_column_numbers,
    options.hide_error_codes,
    options.pretty,
    options.show_error_end,
    lambda path: read_py_file(path, cached_read),
    options.show_absolute_path,
    options.many_errors_threshold,
    options,
)
</t>
<t tx="ekr.20220617092846.1">t0 = time.time()
manager.metastore.commit()
manager.add_stats(cache_commit_time=time.time() - t0)
manager.log(
    "Build finished in %.3f seconds with %d modules, and %d errors"
    % (
        time.time() - manager.start_time,
        len(manager.modules),
        manager.errors.num_messages(),
    )
)
manager.dump_stats()
if reports is not None:
    # Finish the HTML or XML reports even if CompileError was raised.
    reports.finish()
if not cache_dir_existed and os.path.isdir(options.cache_dir):
    add_catch_all_gitignore(options.cache_dir)
    exclude_from_backups(options.cache_dir)
if os.path.isdir(options.cache_dir):
    record_missing_stub_packages(options.cache_dir, manager.missing_stub_packages)


</t>
<t tx="ekr.20220617093043.1">sccs_left = len(fresh_scc_queue)
nodes_left = sum(len(scc) for scc in fresh_scc_queue)
manager.add_stats(sccs_left=sccs_left, nodes_left=nodes_left)
if sccs_left:
    manager.log(
        "{} fresh SCCs ({} nodes) left in queue (and will remain unprocessed)".format(
            sccs_left, nodes_left
        )
    )
    manager.trace(str(fresh_scc_queue))
else:
    manager.log("No fresh SCCs left in queue")


</t>
<t tx="ekr.20220619062539.1"></t>
<t tx="ekr.20220619083632.1">@nosearch

# Ignore Case, Head, Body

# found 6 nodes</t>
<t tx="ekr.20220620055103.1">@nosearch</t>
<t tx="ekr.20220620060013.1">@nosearch

# Ignore Case, Head, Body

# found 6 nodes</t>
<t tx="ekr.20220620060013.10">def callers(n: int=4) -&gt; str:
    """
    Return a string containing a comma-separated list of the callers
    of the function that called g.callerList.
    """
    i, result = 2, []
    while True:
        s = callerName(n=i)
        if s:
            result.append(s)
        if not s or len(result) &gt;= n:
            break
        i += 1
    return ', '.join(reversed(result))

def callerName(n: int) -&gt; str:
    """Get the function name from the call stack."""
    import sys
    try:
        f1 = sys._getframe(n)
        code1 = f1.f_code
        return code1.co_name
    except Exception:
        return ''
</t>
<t tx="ekr.20220620060013.11">def visit_func_def(self, defn: FuncDef) -&gt; None:
    
    ###
    # semantic_analysis_for_scc calls this *twice* for top-level functions:
    # process_top_levels(graph, scc, patches)
    # process_functions(graph, scc, patches)
        
    trace_tag = 'SA.visit_func_def:'
    module_name = self.cur_mod_id
    trace = False and module_name.startswith('ekr')
    if trace:  ###
        print(f"{trace_tag} {defn._name}")  # defn.fullname exists only on the second call.
        print(defn)
        print('')
        ### import pdb ; pdb.set_trace(header=trace_tag)  ### SA.visit_func_def

    self.statement = defn

    # Visit default values because they may contain assignment expressions.
    for arg in defn.arguments:
        if arg.initializer:
            arg.initializer.accept(self)

    defn.is_conditional = self.block_depth[-1] &gt; 0

    # Set full names even for those definitions that aren't added
    # to a symbol table. For example, for overload items.
    defn._fullname = self.qualified_name(defn.name)

    # We don't add module top-level functions to symbol tables
    # when we analyze their bodies in the second phase on analysis,
    # since they were added in the first phase. Nested functions
    # get always added, since they aren't separate targets.
    if not self.recurse_into_functions or len(self.function_stack) &gt; 0:
        if not defn.is_decorated and not defn.is_overload:
            self.add_function_to_symbol_table(defn)

    if not self.recurse_into_functions:
        return

    with self.scope.function_scope(defn):
        self.analyze_func_def(defn)

</t>
<t tx="ekr.20220620060013.12">ekr_call_set = set()

def visit_callable_type(self, t: CallableType, nested: bool = True) -&gt; Type:
    # Every Callable can bind its own type variables, if they're not in the outer scope
    
    ### Called by Sa.analyze_func_def
    ### t.definition is a FuncDef.

    trace = False and t.definition and t.definition._name.startswith('ekr_')
    trace_tag = 'TA.visit_callable_type:'
    if t.definition:
        arguments = t.definition.arguments
        initializers = [z.initializer for z in arguments]
        annotations = [z.type_annotation for z in arguments]
        assert len(t.arg_names) == len(initializers), (t.arg_names, initializers)
    else:
        arguments = []
        initializers = []
        annotations = []
    # Supposedly t.definition is only for traces???
    if trace and t.definition:
        # t.definition is either None or is a FuncDef!
        arguments_s = [z.variable.name for z in arguments]  # A list of Argument objects.
        initializers_s = [str(z) for z in initializers]
        annotations_s = [f"{z.__class__.__name__} = {z}" for z in annotations]
        # print(f"{trace_tag} t.definition: {t.definition}") # Prints parse tree.
        print(f"{trace_tag}                  t: {t.__class__.__name__} {t}")
        print(f"{trace_tag} t.definition._name: {t.definition._name}")
        print(f"{trace_tag}          arguments: {arguments_s}")
        print(f"{trace_tag}        annotations: {annotations_s}")
        print(f"{trace_tag}       initializers: {initializers_s}")
        print(f"{trace_tag}        t.arg_kinds: {t.arg_kinds}")
        print(f"{trace_tag}        t.arg_names: {t.arg_names}")
        print(f"{trace_tag}        t.arg_types: {t.arg_types}")
        print(f"{trace_tag}        t.variables: {t.variables}")

    with self.tvar_scope_frame():
        if self.defining_alias:
            variables = t.variables
        else:
            variables = self.bind_function_type_variables(t, t)
        special = self.anal_type_guard(t.ret_type)
        arg_kinds = t.arg_kinds
        if len(arg_kinds) &gt;= 2 and arg_kinds[-2] == ARG_STAR and arg_kinds[-1] == ARG_STAR2:
            arg_types = self.anal_array(t.arg_types[:-2], nested=nested) + [
                self.anal_star_arg_type(t.arg_types[-2], ARG_STAR, nested=nested),
                self.anal_star_arg_type(t.arg_types[-1], ARG_STAR2, nested=nested),
            ]
        else:
            arg_types = self.anal_array(t.arg_types, nested=nested)
            
        if initializers:  ###
            n_args = len(t.arg_kinds)
            assert len(t.arg_names) == n_args, (n_args, t.arg_names, t.arg_kinds)
            for i, arg_name in enumerate(t.arg_names):
                arg_type = t.arg_types[i]
                arg_kind = t.arg_kinds[i]
                initializer = initializers[i]
                annotation = annotations[i]
                # Only initialized args will have arg_kind == ArgKind.ARG_OPT
                if trace:
                    arg_s = f"arg {i}"
                    arg_type_s = f"arg type {i}"
                    annotation_s = f"Annotation {i}"
                    print(f"{trace_tag} {arg_s:&gt;18}: kind: {arg_kind} name: {arg_name}")
                    print(f"{trace_tag} {arg_type_s:&gt;18}: {arg_type.__class__.__name__} {arg_type}")
                    print(f"{trace_tag} {annotation_s:&gt;18}: {annotation.__class__.__name__} {annotation}")
                
                if arg_kind == ArgKind.ARG_OPT and not annotation:
                    ###
                    ### Create an annotation in the FuncDef (t.definition.arguments)
                    ###
                    assert initializer
                    if trace:
                        print(f"{trace_tag} {'***** TO DO':&gt;18}:")
                        initializer_s = f"Initializer {i}"
                        print(f"{trace_tag} {initializer_s:&gt;18}: {initializer}")
                    # Call the *global* expr_to_unanalyzed_type. SA is not available.
                    from mypy.exprtotype import expr_to_unanalyzed_type
                    new_type = expr_to_unanalyzed_type(initializer)
                    ### import pdb ; pdb.set_trace()  ###
                    assert isinstance(new_type, UnboundType), repr(new_type)
                    if trace:
                        print(f"{trace_tag} {'new type':&gt;18}: {new_type.__class__.__name__} {new_type}")
                    ### Experimental.
                        # arguments = t.definition.arguments
                        # annotations = [z.type_annotation for z in arguments]
                    t.definition.arguments[i].type_annotation = new_type
                    arg_types[i] = new_type
                        # t.arg_types[i] = new_type
                        # t.definition.arguments = arguments
                    if trace: ###
                        print(f"{trace_tag} {'NEW RET':&gt;18}: {t.__class__.__name__} {t}")
                    return t  ### Hack!

        ret = t.copy_modified(arg_types=arg_types,
                              ret_type=self.anal_type(t.ret_type, nested=nested),
                              # If the fallback isn't filled in yet,
                              # its type will be the falsey FakeInfo
                              fallback=(t.fallback if t.fallback.type
                                        else self.named_type('builtins.function')),
                              variables=self.anal_var_defs(variables),
                              type_guard=special,
                              )
                              
        assert isinstance(ret, CallableType), ret.__class__.__name__ ###
        if trace:  ###
            print(f"{trace_tag} {'ret':&gt;18}: {ret}")
            print('')
    return ret

</t>
<t tx="ekr.20220620060013.13">def check_for_missing_annotations(self, fdef: FuncItem) -&gt; None:

    trace = False  ###
    trace_tag = 'check_for_missing_annotations'
    # Check for functions with unspecified/not fully specified types.
    def is_unannotated_any(t: Type) -&gt; bool:
        if not isinstance(t, ProperType):
            return False
        return isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated

    has_explicit_annotation = (isinstance(fdef.type, CallableType)
                               and any(not is_unannotated_any(t)
                                       for t in fdef.type.arg_types + [fdef.type.ret_type]))

    show_untyped = not self.is_typeshed_stub or self.options.warn_incomplete_stub
    check_incomplete_defs = self.options.disallow_incomplete_defs and has_explicit_annotation
    if show_untyped and (self.options.disallow_untyped_defs or check_incomplete_defs):
        if fdef.type is None and self.options.disallow_untyped_defs:
            if (not fdef.arguments or (len(fdef.arguments) == 1 and
                    (fdef.arg_names[0] == 'self' or fdef.arg_names[0] == 'cls'))):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
                if not has_return_statement(fdef) and not fdef.is_generator:
                    self.note('Use "-&gt; None" if function does not return a value', fdef,
                              code=codes.NO_UNTYPED_DEF)
            else:
                self.fail(message_registry.FUNCTION_TYPE_EXPECTED, fdef)
        elif isinstance(fdef.type, CallableType):
            ret_type = get_proper_type(fdef.type.ret_type)
            if is_unannotated_any(ret_type):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_generator:
                if is_unannotated_any(self.get_generator_return_type(ret_type,
                                                                     fdef.is_coroutine)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_coroutine and isinstance(ret_type, Instance):
                if is_unannotated_any(self.get_coroutine_return_type(ret_type)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            if any(is_unannotated_any(t) for t in fdef.type.arg_types):
                if trace:
                    print(trace_tag, fdef.__class__.__name__, fdef.type.arg_types)  ###
                self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)

</t>
<t tx="ekr.20220620060013.14"># Str(string s)
def visit_Str(self, n: Str) -&gt; Type:
    # Note: we transform these fallback types into the correct types in
    # 'typeanal.py' -- specifically in the named_type_with_normalized_str method.
    # If we're analyzing Python 3, that function will translate 'builtins.unicode'
    # into 'builtins.str'. In contrast, if we're analyzing Python 2 code, we'll
    # translate 'builtins.bytes' in the method below into 'builtins.str'.

    # Do a getattr because the field doesn't exist in 3.8 (where
    # this method doesn't actually ever run.) We can't just do
    # an attribute access with a `# type: ignore` because it would be
    # unused on &lt; 3.8.
    kind: str = getattr(n, "kind")  # noqa
    
    ### print('TypeConverter.visit_Str', kind, n)  ###

    if 'u' in kind or self.assume_str_is_unicode:
        return parse_type_string(n.s, 'builtins.unicode', self.line, n.col_offset,
                                 assume_str_is_unicode=self.assume_str_is_unicode)
    else:
        return parse_type_string(n.s, 'builtins.str', self.line, n.col_offset,
                                 assume_str_is_unicode=self.assume_str_is_unicode)

</t>
<t tx="ekr.20220620060013.15">def visit_callable_type(self, t: CallableType) -&gt; str:
    param_spec = t.param_spec()
    if param_spec is not None:
        num_skip = 2
    else:
        num_skip = 0

    s = ''
    bare_asterisk = False
    for i in range(len(t.arg_types) - num_skip):
        if s != '':
            s += ', '
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += '*, '
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += '*'
        if t.arg_kinds[i] == ARG_STAR2:
            s += '**'
        name = t.arg_names[i]
        if name:
            s += name + ': '
        s += t.arg_types[i].accept(self)
        if t.arg_kinds[i].is_optional():
            ### s += ' ='
            s += ' &lt;OPT2&gt;'

    if param_spec is not None:
        n = param_spec.name
        if s:
            s += ', '
        s += f'*{n}.args, **{n}.kwargs'

    s = f'({s})'

    if not isinstance(get_proper_type(t.ret_type), NoneType):
        if t.type_guard is not None:
            s += f' -&gt; TypeGuard[{t.type_guard.accept(self)}]'
        else:
            s += f' -&gt; {t.ret_type.accept(self)}'

    if t.variables:
        vs = []
        for var in t.variables:
            if isinstance(var, TypeVarType):
                # We reimplement TypeVarType.__repr__ here in order to support id_mapper.
                if var.values:
                    vals = f"({', '.join(val.accept(self) for val in var.values)})"
                    vs.append(f'{var.name} in {vals}')
                elif not is_named_instance(var.upper_bound, 'builtins.object'):
                    vs.append(f'{var.name} &lt;: {var.upper_bound.accept(self)}')
                else:
                    vs.append(var.name)
            else:
                # For other TypeVarLikeTypes, just use the name
                vs.append(var.name)
        s = f"[{', '.join(vs)}] {s}"

    return f'def {s}'

</t>
<t tx="ekr.20220620060013.16">def visit_parameters(self, t: Parameters) -&gt; str:
    # This is copied from visit_callable -- is there a way to decrease duplication?
    if t.is_ellipsis_args:
        return '...'

    s = ''
    bare_asterisk = False
    for i in range(len(t.arg_types)):
        if s != '':
            s += ', '
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += '*, '
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += '*'
        if t.arg_kinds[i] == ARG_STAR2:
            s += '**'
        name = t.arg_names[i]
        if name:
            s += f'{name}: '
        r = t.arg_types[i].accept(self)

        s += r

        if t.arg_kinds[i].is_optional():
            ### s += ' ='
            s += ' &lt;OPT1&gt;'

    return f'[{s}]'

</t>
<t tx="ekr.20220620060013.2">def do_func_def(self, n: Union[ast3.FunctionDef, ast3.AsyncFunctionDef],
                is_coroutine: bool = False) -&gt; Union[FuncDef, Decorator]:
    """Helper shared between visit_FunctionDef and visit_AsyncFunctionDef."""
    
    if n.name.startswith('ekr_'):  ###
        print('')
        print('ASTConverter.do_func_def', n.name)
    
    self.class_and_function_stack.append('F')
    no_type_check = bool(n.decorator_list and
                         any(is_no_type_check_decorator(d) for d in n.decorator_list))

    lineno = n.lineno
    args = self.transform_args(n.args, lineno, no_type_check=no_type_check)
    if special_function_elide_names(n.name):
        for arg in args:
            arg.pos_only = True

    arg_kinds = [arg.kind for arg in args]
    arg_names = [None if arg.pos_only else arg.variable.name for arg in args]

    arg_types: List[Optional[Type]] = []
    if no_type_check:
        arg_types = [None] * len(args)
        return_type = None
    elif n.type_comment is not None:
        try:
            func_type_ast = ast3_parse(n.type_comment, '&lt;func_type&gt;', 'func_type')
            assert isinstance(func_type_ast, FunctionType)
            # for ellipsis arg
            if (len(func_type_ast.argtypes) == 1 and
                    isinstance(func_type_ast.argtypes[0], ast3_Ellipsis)):
                if n.returns:
                    # PEP 484 disallows both type annotations and type comments
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                arg_types = [a.type_annotation
                             if a.type_annotation is not None
                             else AnyType(TypeOfAny.unannotated)
                             for a in args]
            else:
                # PEP 484 disallows both type annotations and type comments
                if n.returns or any(a.type_annotation is not None for a in args):
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                translated_args = (TypeConverter(self.errors,
                                                 line=lineno,
                                                 override_column=n.col_offset)
                                   .translate_expr_list(func_type_ast.argtypes))
                arg_types = [a if a is not None else AnyType(TypeOfAny.unannotated)
                            for a in translated_args]
            return_type = TypeConverter(self.errors,
                                        line=lineno).visit(func_type_ast.returns)

            # add implicit self type
            if self.in_method_scope() and len(arg_types) &lt; len(args):
                arg_types.insert(0, AnyType(TypeOfAny.special_form))
        except SyntaxError:
            stripped_type = n.type_comment.split("#", 2)[0].strip()
            err_msg = f'{TYPE_COMMENT_SYNTAX_ERROR} "{stripped_type}"'
            self.fail(err_msg, lineno, n.col_offset)
            if n.type_comment and n.type_comment[0] not in ["(", "#"]:
                self.note('Suggestion: wrap argument types in parentheses',
                          lineno, n.col_offset)
            arg_types = [AnyType(TypeOfAny.from_error)] * len(args)
            return_type = AnyType(TypeOfAny.from_error)
    else:
        arg_types = [a.type_annotation for a in args]
        return_type = TypeConverter(self.errors, line=n.returns.lineno
                                    if n.returns else lineno).visit(n.returns)

    for arg, arg_type in zip(args, arg_types):
        self.set_type_optional(arg_type, arg.initializer)

    func_type = None
    if any(arg_types) or return_type:
        if len(arg_types) != 1 and any(isinstance(t, EllipsisType)
                                       for t in arg_types):
            self.fail("Ellipses cannot accompany other argument types "
                      "in function type signature", lineno, n.col_offset)
        elif len(arg_types) &gt; len(arg_kinds):
            self.fail('Type signature has too many arguments', lineno, n.col_offset,
                      blocker=False)
        elif len(arg_types) &lt; len(arg_kinds):
            self.fail('Type signature has too few arguments', lineno, n.col_offset,
                      blocker=False)
        else:
            func_type = CallableType([a if a is not None else
                                      AnyType(TypeOfAny.unannotated) for a in arg_types],
                                     arg_kinds,
                                     arg_names,
                                     return_type if return_type is not None else
                                     AnyType(TypeOfAny.unannotated),
                                     _dummy_fallback)

    func_def = FuncDef(
        n.name,
        args,
        self.as_required_block(n.body, lineno),
        func_type)
    if isinstance(func_def.type, CallableType):
        # semanal.py does some in-place modifications we want to avoid
        func_def.unanalyzed_type = func_def.type.copy_modified()
    if is_coroutine:
        func_def.is_coroutine = True
    if func_type is not None:
        func_type.definition = func_def
        func_type.line = lineno

    if n.decorator_list:
        if sys.version_info &lt; (3, 8):
            # Before 3.8, [typed_]ast the line number points to the first decorator.
            # In 3.8, it points to the 'def' line, where we want it.
            lineno += len(n.decorator_list)
            end_lineno: Optional[int] = None
        else:
            # Set end_lineno to the old pre-3.8 lineno, in order to keep
            # existing "# type: ignore" comments working:
            end_lineno = n.decorator_list[0].lineno + len(n.decorator_list)

        var = Var(func_def.name)
        var.is_ready = False
        var.set_line(lineno)

        func_def.is_decorated = True
        func_def.set_line(lineno, n.col_offset, end_lineno)
        func_def.body.set_line(lineno)  # TODO: Why?

        deco = Decorator(func_def, self.translate_expr_list(n.decorator_list), var)
        first = n.decorator_list[0]
        deco.set_line(first.lineno, first.col_offset)
        retval: Union[FuncDef, Decorator] = deco
    else:
        # FuncDef overrides set_line -- can't use self.set_line
        func_def.set_line(lineno, n.col_offset)
        retval = func_def
    self.class_and_function_stack.pop()
    return retval

</t>
<t tx="ekr.20220620060013.3">def make_argument(self, arg: ast3.arg, default: Optional[ast3.expr], kind: ArgKind,
                  no_type_check: bool, pos_only: bool = False) -&gt; Argument:
    trace = 'ekr_' in arg.arg
    trace_tag = 'ASTConverter.make_argument'
    if no_type_check:
        arg_type = None
    else:
        annotation = arg.annotation
        type_comment = arg.type_comment
        if annotation is not None and type_comment is not None:
            self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, arg.lineno, arg.col_offset)
        arg_type = None
        if annotation is not None:
            arg_type = TypeConverter(self.errors, line=arg.lineno).visit(annotation)
        elif isinstance(default, ast3.Constant):  ### New, experimental.
            trace = True
            arg_class = default.value.__class__.__name__
            if arg_class in ('bool', 'bytes', 'float', 'int', 'str'):
                arg_type = UnboundType(arg_class)
        else:
            arg_type = self.translate_type_comment(arg, type_comment)
    if argument_elide_name(arg.arg):
        pos_only = True
        
    ###
    # ast.arg: A single argument in a list.
    # arg.arg: argument name
    # arg.annotation, annotation, such as a Str or Name node.
    # arg.type_comment: optional type comment.

    if trace:
        if isinstance(default, ast3.Constant) and not annotation:
            print(f"{trace_tag} {arg.arg:&gt;7}: annotate using {arg_type!s:16} = ast.Constant({default.value})")
        else:
            ann_s = f"ast.Name({annotation.id})" if isinstance(annotation, ast3.Name) else repr(annotation)
            print(f"{trace_tag} {arg.arg:&gt;7}: annotation: {ann_s} default: {default.__class__.__name__}")

    return Argument(Var(arg.arg), arg_type, self.visit(default), kind, pos_only)

</t>
<t tx="ekr.20220620060013.4">def transform_args(self,
                   args: ast3.arguments,
                   line: int,
                   no_type_check: bool = False,
                   ) -&gt; List[Argument]:
                       
    ### arguments = (
    #   arg* posonlyargs, arg* args, arg? vararg, arg* kwonlyargs,
    #   expr* kw_defaults, arg? kwarg, expr* defaults
    #  )
    
    # ast.arg: A single argument in a list.
    # arg.arg: argument name
    # arg.annotation, annotation, such as a Str or Name node.
    # arg.type_comment: optional type comment.
                       
    trace = False and any('ekr_' in z.arg for z in args.args)
    if trace:
        import ast  # assumes python 3.8+
        trace_tag = 'ASTConverter.transform_args'
        if any(z.annotation for z in args.args):
            print(trace_tag)
            for z in args.args:
                if z.annotation:
                    ann_s = f"ast.Name: {z.annotation.id}" if isinstance(z.annotation, ast.Name) else z.annotation.__class__.__name__
                    print(f"  {z.arg} annotation: {ann_s}")
        
    new_args = []
    names: List[ast3.arg] = []
    posonlyargs = getattr(args, "posonlyargs", cast(List[ast3.arg], []))
    args_args = posonlyargs + args.args
    args_defaults = args.defaults
    num_no_defaults = len(args_args) - len(args_defaults)
    # positional arguments without defaults
    for i, a in enumerate(args_args[:num_no_defaults]):
        pos_only = i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, None, ARG_POS, no_type_check, pos_only))
        names.append(a)

    # positional arguments with defaults
    for i, (a, d) in enumerate(zip(args_args[num_no_defaults:], args_defaults)):
        pos_only = num_no_defaults + i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, d, ARG_OPT, no_type_check, pos_only))
        names.append(a)

    # *arg
    if args.vararg is not None:
        new_args.append(self.make_argument(args.vararg, None, ARG_STAR, no_type_check))
        names.append(args.vararg)

    # keyword-only arguments with defaults
    for a, kd in zip(args.kwonlyargs, args.kw_defaults):
        new_args.append(self.make_argument(
            a,
            kd,
            ARG_NAMED if kd is None else ARG_NAMED_OPT,
            no_type_check))
        names.append(a)

    # **kwarg
    if args.kwarg is not None:
        new_args.append(self.make_argument(args.kwarg, None, ARG_STAR2, no_type_check))
        names.append(args.kwarg)

    check_arg_names([arg.variable.name for arg in new_args], names, self.fail_arg)

    return new_args

</t>
<t tx="ekr.20220620060013.5">def expr_to_unanalyzed_type(expr: Expression,
                            options: Optional[Options] = None,
                            allow_new_syntax: bool = False,
                            _parent: Optional[Expression] = None) -&gt; ProperType:
    """Translate an expression to the corresponding type.

    The result is not semantically analyzed. It can be UnboundType or TypeList.
    Raise TypeTranslationError if the expression cannot represent a type.

    If allow_new_syntax is True, allow all type syntax independent of the target
    Python version (used in stubs).
    """
    # The `parent` parameter is used in recursive calls to provide context for
    # understanding whether an CallableArgument is ok.
    name: Optional[str] = None
    if isinstance(expr, NameExpr):
        name = expr.name
        if name == 'True':
            return RawExpressionType(True, 'builtins.bool', line=expr.line, column=expr.column)
        elif name == 'False':
            return RawExpressionType(False, 'builtins.bool', line=expr.line, column=expr.column)
        else:
            return UnboundType(name, line=expr.line, column=expr.column)
    elif isinstance(expr, MemberExpr):
        fullname = get_member_expr_fullname(expr)
        if fullname:
            return UnboundType(fullname, line=expr.line, column=expr.column)
        else:
            raise TypeTranslationError()
    elif isinstance(expr, IndexExpr):
        base = expr_to_unanalyzed_type(expr.base, options, allow_new_syntax, expr)
        if isinstance(base, UnboundType):
            if base.args:
                raise TypeTranslationError()
            if isinstance(expr.index, TupleExpr):
                args = expr.index.items
            else:
                args = [expr.index]

            if isinstance(expr.base, RefExpr) and expr.base.fullname in ANNOTATED_TYPE_NAMES:
                # TODO: this is not the optimal solution as we are basically getting rid
                # of the Annotation definition and only returning the type information,
                # losing all the annotations.

                return expr_to_unanalyzed_type(args[0], options, allow_new_syntax, expr)
            else:
                base.args = tuple(expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
                                  for arg in args)
            if not base.args:
                base.empty_tuple_index = True
            return base
        else:
            raise TypeTranslationError()
    elif (isinstance(expr, OpExpr)
          and expr.op == '|'
          and ((options and options.python_version &gt;= (3, 10)) or allow_new_syntax)):
        return UnionType([expr_to_unanalyzed_type(expr.left, options, allow_new_syntax),
                          expr_to_unanalyzed_type(expr.right, options, allow_new_syntax)])
    elif isinstance(expr, CallExpr) and isinstance(_parent, ListExpr):
        c = expr.callee
        names = []
        # Go through the dotted member expr chain to get the full arg
        # constructor name to look up
        while True:
            if isinstance(c, NameExpr):
                names.append(c.name)
                break
            elif isinstance(c, MemberExpr):
                names.append(c.name)
                c = c.expr
            else:
                raise TypeTranslationError()
        arg_const = '.'.join(reversed(names))

        # Go through the constructor args to get its name and type.
        name = None
        default_type = AnyType(TypeOfAny.unannotated)
        typ: Type = default_type
        for i, arg in enumerate(expr.args):
            if expr.arg_names[i] is not None:
                if expr.arg_names[i] == "name":
                    if name is not None:
                        # Two names
                        raise TypeTranslationError()
                    name = _extract_argument_name(arg)
                    continue
                elif expr.arg_names[i] == "type":
                    if typ is not default_type:
                        # Two types
                        raise TypeTranslationError()
                    typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
                    continue
                else:
                    raise TypeTranslationError()
            elif i == 0:
                typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
            elif i == 1:
                name = _extract_argument_name(arg)
            else:
                raise TypeTranslationError()
        return CallableArgument(typ, name, arg_const, expr.line, expr.column)
    elif isinstance(expr, ListExpr):
        return TypeList([expr_to_unanalyzed_type(t, options, allow_new_syntax, expr)
                         for t in expr.items],
                        line=expr.line, column=expr.column)
    elif isinstance(expr, StrExpr):
        ### print('expr_to_unanalyzed_type', expr.value)  ###
        return parse_type_string(expr.value, 'builtins.str', expr.line, expr.column,
                                 assume_str_is_unicode=expr.from_python_3)
    elif isinstance(expr, BytesExpr):
        return parse_type_string(expr.value, 'builtins.bytes', expr.line, expr.column,
                                 assume_str_is_unicode=False)
    elif isinstance(expr, UnicodeExpr):
        return parse_type_string(expr.value, 'builtins.unicode', expr.line, expr.column,
                                 assume_str_is_unicode=True)
    elif isinstance(expr, UnaryExpr):
        typ = expr_to_unanalyzed_type(expr.expr, options, allow_new_syntax)
        if isinstance(typ, RawExpressionType):
            if isinstance(typ.literal_value, int) and expr.op == '-':
                typ.literal_value *= -1
                return typ
        raise TypeTranslationError()
    elif isinstance(expr, IntExpr):
        return RawExpressionType(expr.value, 'builtins.int', line=expr.line, column=expr.column)
    elif isinstance(expr, FloatExpr):
        # Floats are not valid parameters for RawExpressionType , so we just
        # pass in 'None' for now. We'll report the appropriate error at a later stage.
        return RawExpressionType(None, 'builtins.float', line=expr.line, column=expr.column)
    elif isinstance(expr, ComplexExpr):
        # Same thing as above with complex numbers.
        return RawExpressionType(None, 'builtins.complex', line=expr.line, column=expr.column)
    elif isinstance(expr, EllipsisExpr):
        return EllipsisType(expr.line)
    else:
        raise TypeTranslationError()
</t>
<t tx="ekr.20220620060013.6"># Note that all __init__ args must have default values
def __init__(self,
             name: str = '',              # Function name
             arguments: Optional[List[Argument]] = None,
             body: Optional['Block'] = None,
             typ: 'Optional[mypy.types.FunctionLike]' = None) -&gt; None:
    super().__init__(arguments, body, typ)
    self._name = name
    self.is_decorated = False
    self.is_conditional = False  # Defined conditionally (within block)?
    self.is_abstract = False
    self.is_final = False
    # Original conditional definition
    self.original_def: Union[None, FuncDef, Var, Decorator] = None

    if False and name.startswith('ekr_'):  ###
        print('FuncDef.__init__', name, self)  # call repr(self) *last*.
        print('')
        ### import pdb ; pdb.set_trace()  ### FuncDef.__init__
</t>
<t tx="ekr.20220620060013.7">def process_stale_scc(graph: Graph, scc: List[str], manager: BuildManager) -&gt; None:
    """Process the modules in one SCC from source code.

    Exception: If quick_and_dirty is set, use the cache for fresh modules.
    """
    trace = False  ###
    stale = scc
    if trace:  ###
        print(f"process_stale_scc: includes ekr_test: {'ekr_test' in scc} len(scc): {len(scc)}\n")
        if 0: ### Experimental: fails in semantic_analyze_target
            if 'ekr_test' not in scc:
                return

    for id in stale:
        # We may already have parsed the module, or not.
        # If the former, parse_file() is a no-op.
        graph[id].parse_file()
    if 'typing' in scc:
        # For historical reasons we need to manually add typing aliases
        # for built-in generic collections, see docstring of
        # SemanticAnalyzerPass2.add_builtin_aliases for details.
        typing_mod = graph['typing'].tree
        assert typing_mod, "The typing module was not parsed"
    mypy.semanal_main.semantic_analysis_for_scc(graph, scc, manager.errors)

    # Track what modules aren't yet done so we can finish them as soon
    # as possible, saving memory.
    unfinished_modules = set(stale)
    for id in stale:
        graph[id].type_check_first_pass()
        if not graph[id].type_checker().deferred_nodes:
            unfinished_modules.discard(id)
            graph[id].finish_passes()

    while unfinished_modules:
        for id in stale:
            if id not in unfinished_modules:
                continue
            if not graph[id].type_check_second_pass():
                unfinished_modules.discard(id)
                graph[id].finish_passes()
    for id in stale:
        graph[id].generate_unused_ignore_notes()
        graph[id].generate_ignore_without_code_notes()
    if any(manager.errors.is_errors_for_file(graph[id].xpath) for id in stale):
        for id in stale:
            graph[id].transitive_error = True
    for id in stale:
        manager.flush_errors(manager.errors.file_messages(graph[id].xpath), False)
        graph[id].write_cache()
        graph[id].mark_as_rechecked()


</t>
<t tx="ekr.20220620060013.8">def analyze_func_def(self, defn: FuncDef) -&gt; None:

    trace_tag = 'SA.analyze_func_def:'
    module_name = self.cur_mod_id
    trace = False and module_name.startswith('ekr')
    if trace:
        # ast = self.modules.get(module_name)
        initializers_s = ','.join(str(z.initializer) for z in defn.arguments)
        print(
            # f"{trace_tag}            defn: {defn.__class__.__name__}\n"
            f"{trace_tag}  defn._fullname: {defn._fullname}\n"
            f"{trace_tag}       arguments: {defn.arguments}\n"
            f"{trace_tag}    initializers: {initializers_s}\n"
            f"{trace_tag}            defn: {defn.__class__.__name__} {defn}\n"
            # f"{trace_tag} self.cur_mod_id: {module_name}\n"
            # f"{trace_tag}  AST (MypyFile): {ast.__class__.__name__}\n"
        )

    self.function_stack.append(defn)
    
    if defn.type:
        assert isinstance(defn.type, CallableType)
        self.update_function_type_variables(defn.type, defn)
    self.function_stack.pop()

    if self.is_class_scope():
        # Method definition
        assert self.type is not None
        defn.info = self.type
        if defn.type is not None and defn.name in ('__init__', '__init_subclass__'):
            assert isinstance(defn.type, CallableType)
            if isinstance(get_proper_type(defn.type.ret_type), AnyType):
                defn.type = defn.type.copy_modified(ret_type=NoneType())
        self.prepare_method_signature(defn, self.type)

    # Analyze function signature
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        if defn.type:
            self.check_classvar_in_signature(defn.type)
            assert isinstance(defn.type, CallableType)
            # Signature must be analyzed in the surrounding scope so that
            # class-level imported names and type variables are in scope.
            analyzer = self.type_analyzer()
            tag = self.track_incomplete_refs()
            result = analyzer.visit_callable_type(defn.type, nested=False)
            # Don't store not ready types (including placeholders).
            if False and trace:
                import pdb ; pdb.set_trace()  ###
            if self.found_incomplete_ref(tag) or has_placeholder(result):
                self.defer(defn)
                return
            assert isinstance(result, ProperType)
            defn.type = result
            self.add_type_alias_deps(analyzer.aliases_used)
            self.check_function_signature(defn)
            if isinstance(defn, FuncDef):
                assert isinstance(defn.type, CallableType)
                defn.type = set_callable_name(defn.type, defn)

    self.analyze_arg_initializers(defn)
    self.analyze_function_body(defn)
    if (defn.is_coroutine and
            isinstance(defn.type, CallableType) and
            self.wrapped_coro_return_types.get(defn) != defn.type):
        if defn.is_async_generator:
            # Async generator types are handled elsewhere
            pass
        else:
            # A coroutine defined as `async def foo(...) -&gt; T: ...`
            # has external return type `Coroutine[Any, Any, T]`.
            any_type = AnyType(TypeOfAny.special_form)
            ret_type = self.named_type_or_none('typing.Coroutine',
                                               [any_type, any_type, defn.type.ret_type])
            assert ret_type is not None, "Internal error: typing.Coroutine not found"
            defn.type = defn.type.copy_modified(ret_type=ret_type)
            self.wrapped_coro_return_types[defn] = defn.type

</t>
<t tx="ekr.20220620060013.9">def process_type_annotation(self, s: AssignmentStmt) -&gt; None:
    """Analyze type annotation or infer simple literal type."""
    &lt;&lt; define callers &amp; callerName &gt;&gt;
    if 1:
        trace = False
    else:
        trace_tag = 'SA.process_type_annotation'
        module_name = self.cur_mod_id
        trace = module_name.startswith('ekr')  ###
    
    if trace:
        # s.type: UnboundType for annotated assignments, else None.
        # print(f"{trace_tag}      s: {s}")
        print(f"{trace_tag} s.type: {s.type.__class__.__name__} {s.type}")

    if s.type:
        lvalue = s.lvalues[-1]
        allow_tuple_literal = isinstance(lvalue, TupleExpr)
        analyzed = self.anal_type(s.type, allow_tuple_literal=allow_tuple_literal)
        # Don't store not ready types (including placeholders).
        if analyzed is None or has_placeholder(analyzed):
            return
        s.type = analyzed
        if trace:
            print(
                # An ast.NameExpr node.
                f"{trace_tag} Adjust: lvalue {lvalue.__class__.__name__} =&gt; "
                # an Instance, a subclass of ProperType and Type.
                f"{analyzed.__class__.__name__} = {analyzed}")
        if (self.type and self.type.is_protocol and isinstance(lvalue, NameExpr) and
                isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs):
            if isinstance(lvalue.node, Var):
                lvalue.node.is_abstract_var = True
    else:
        if (self.type and self.type.is_protocol and
                self.is_annotated_protocol_member(s) and not self.is_func_scope()):
            self.fail('All protocol members must have explicitly declared types', s)
        # Set the type if the rvalue is a simple literal (even if the above error occurred).
        if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
            if s.lvalues[0].is_inferred_def:
                s.type = self.analyze_simple_literal_type(s.rvalue, s.is_final_def)
    if s.type:
        # Store type into nodes.
        for lvalue in s.lvalues:
            self.store_declared_types(lvalue, s.type)
            
    if trace:
        print('')  ###

</t>
<t tx="ekr.20220621111917.1">(venv) C:\Repos\ekr-mypy&gt;pytest
=============================================== short test summary info ===============================================

---- new failures
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-classes.test::testInitSubclassUnannotatedMulti
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-fastparse.test::testFastParsePerArgumentAnnotations
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-fastparse.test::testFastParsePerArgumentAnnotationsWithReturn
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-functions.test::testEllipsisWithArbitraryArgsOnBareFunctionWithDefaults
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-columns.test::testColumnsMethodDefaultArgumentsAndSignatureAsComment
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-functions.test::testDefaultArgumentsAndSignatureAsComment
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-functions.test::testMethodDefaultArgumentsAndSignatureAsComment
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-attr.test::testAttrsUsingBadConverter
FAILED mypy/test/testcheck.py::TypeCheckSuite::check-attr.test::testAttrsUsingBadConverterReprocess
FAILED mypy/test/testparse.py::ParserSuite::parse.test::testDefaultArgs
FAILED mypy/test/testparse.py::ParserSuite::parse.test::testBareAsteriskInFuncDef
FAILED mypy/test/teststubgen.py::StubgenPythonSuite::stubgen.test::testDefaultArgInt
FAILED mypy/test/teststubgen.py::StubgenPythonSuite::stubgen.test::testDefaultArgBool
FAILED mypy/test/teststubgen.py::StubgenPythonSuite::stubgen.test::testDefaultArgStr
FAILED mypy/test/teststubgen.py::StubgenPythonSuite::stubgen.test::testDefaultArgBytes
FAILED mypy/test/teststubgen.py::StubgenPythonSuite::stubgen.test::testDefaultArgFloat
FAILED mypy/test/teststubgen.py::StubgenPythonSuite::stubgen.test::testVarArgsWithKwVarArgs
FAILED mypy/test/teststubgen.py::StubgenPythonSuite::stubgen.test::testKeywordOnlyArg

----------- previous failures
FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testException
FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testTryExcept
FAILED mypyc/test/test_run.py::TestRun::run-exceptions.test::testExceptionAtModuleTopLevel
FAILED mypyc/test/test_run.py::TestRun::run-loops.test::testForIterable
FAILED mypyc/test/test_run.py::TestRun::run-classes.test::testProperty
FAILED mypyc/test/test_run.py::TestRun::run-classes.test::testSubclassUninitAttr
FAILED mypyc/test/test_run.py::TestRun::run-generators.test::testYieldThrow
FAILED mypyc/test/test_run.py::TestRun::run-multimodule.test::testMultiModuleTraceback
FAILED mypyc/test/test_run.py::TestRunMultiFile::run-multimodule.test::testMultiModuleTraceback_multi
FAILED mypyc/test/test_run.py::TestRunSeparate::run-multimodule.test::testMultiModuleTraceback_separate

======================== 28 failed, 10083 passed, 380 skipped, 9 xfailed in 484.32s (0:08:04) =========================
</t>
<t tx="ekr.20220621112034.1">====================================================== FAILURES =======================================================
__________________________________________ testInitSubclassUnannotatedMulti ___________________________________________
[gw6] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-classes.test:6845:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-classes.test, line 6845)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
Actual:
  main:7: error: "List[Type[A]]" has no attribute "append" (diff)

_________________________________________ testFastParsePerArgumentAnnotations _________________________________________
[gw4] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-fastparse.test:140:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-fastparse.test, line 140)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  main:15: note: Revealed type is "__main__.A"
  main:16: note: Revealed type is "Union[__main__.B, None]" (diff)
  main:17: note: Revealed type is "builtins.tuple[__main__.C, ...]"
  main:18: note: Revealed type is "Union[__main__.D, None]" (diff)
  main:19: note: Revealed type is "__main__.E"
  main:20: note: Revealed type is "builtins.dict[builtins.str, __main__.F]"
Actual:
  main:15: note: Revealed type is "__main__.A"
  main:16: note: Revealed type is "Any"         (diff)
  main:17: note: Revealed type is "builtins.tuple[__main__.C, ...]"
  main:18: note: Revealed type is "Any"         (diff)
  main:19: note: Revealed type is "__main__.E"
  main:20: note: Revealed type is "builtins.dict[builtins.str, __main__.F]"

Alignment of first line difference:
  E: ...ote: Revealed type is "Union[__main__.B, None]"
  A: ...ote: Revealed type is "Any"
                               ^
____________________________________ testFastParsePerArgumentAnnotationsWithReturn ____________________________________
[gw4] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-fastparse.test:164:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-fastparse.test, line 164)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  main:16: note: Revealed type is "__main__.A"
  main:17: note: Revealed type is "Union[__main__.B, None]" (diff)
  main:18: note: Revealed type is "builtins.tuple[__main__.C, ...]"
  main:19: note: Revealed type is "Union[__main__.D, None]" (diff)
  main:20: note: Revealed type is "__main__.E"
  main:21: note: Revealed type is "builtins.dict[builtins.str, __main__.F]"
  main:22: error: Incompatible return value type (got "str", expected "int")
Actual:
  main:16: note: Revealed type is "__main__.A"
  main:17: note: Revealed type is "Any"         (diff)
  main:18: note: Revealed type is "builtins.tuple[__main__.C, ...]"
  main:19: note: Revealed type is "Any"         (diff)
  main:20: note: Revealed type is "__main__.E"
  main:21: note: Revealed type is "builtins.dict[builtins.str, __main__.F]"
  main:22: error: Incompatible return value type (got "str", expected "int")

Alignment of first line difference:
  E: ...ote: Revealed type is "Union[__main__.B, None]"
  A: ...ote: Revealed type is "Any"
                               ^
_______________________________ testEllipsisWithArbitraryArgsOnBareFunctionWithDefaults _______________________________
[gw2] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-functions.test:2023:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-functions.test, line 2023)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
Actual:
  main:3: error: Argument 2 to "f" has incompatible type "str"; expected "int" (diff)
  main:3: error: Argument 3 to "f" has incompatible type "List[&lt;nothing&gt;]"; expected "str" (diff)
  main:4: error: Argument "y" to "f" has incompatible type "str"; expected "int" (diff)
  main:4: error: Argument "z" to "f" has incompatible type "List[&lt;nothing&gt;]"; expected "str" (diff)

_______________________________ testColumnsMethodDefaultArgumentsAndSignatureAsComment ________________________________
[gw7] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-columns.test:18:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-columns.test, line 18)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  main:7:7: error: Argument 1 to "f" of "A" has incompatible type "str"; expected "int" (diff)
  main:8:10: error: Argument 2 to "f" of "A" has incompatible type "int"; expected "str" (diff)
  main:9:2: error: Too many arguments for "f" of "A" (diff)
Actual:
  main:3:5: error: Function has duplicate type signatures (diff)

Alignment of first line difference:
  E: main:7:7: error: Argument 1 to "f" of "A" has incompatible type "str"; e...
  A: main:3:5: error: Function has duplicate type signatures...
          ^
______________________________________ testDefaultArgumentsAndSignatureAsComment ______________________________________
[gw7] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-functions.test:579:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-functions.test, line 579)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  main:6: error: Argument 1 to "f" has incompatible type "str"; expected "int" (diff)
Actual:
  main:2: error: Function has duplicate type signatures (diff)

Alignment of first line difference:
  E: main:6: error: Argument 1 to "f" has incompatible type "str"; expected "...
  A: main:2: error: Function has duplicate type signatures...
          ^
___________________________________ testMethodDefaultArgumentsAndSignatureAsComment ___________________________________
[gw7] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-functions.test:587:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-functions.test, line 587)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  main:7: error: Argument 1 to "f" of "A" has incompatible type "str"; expected "int" (diff)
Actual:
  main:3: error: Function has duplicate type signatures (diff)

Alignment of first line difference:
  E: main:7: error: Argument 1 to "f" of "A" has incompatible type "str"; exp...
  A: main:3: error: Function has duplicate type signatures...
          ^
_____________________________________________ testAttrsUsingBadConverter ______________________________________________
[gw2] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-attr.test:816:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-attr.test, line 816)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  main:16: error: Cannot determine __init__ type from converter (diff)
  main:16: error: Argument "converter" has incompatible type "Callable[[], str]"; expected "Callable[[Any], str]" (diff)
  main:17: error: Cannot determine __init__ type from converter (diff)
  main:17: error: Argument "converter" has incompatible type overloaded function; expected "Callable[[Any], int]" (diff)
  ...
Actual:
  main:10: error: Overloaded function implementation does not accept all possible arguments of signature 2 (diff)
  main:16: error: Cannot determine __init__ type from converter (diff)
  main:16: error: Argument "converter" has incompatible type "Callable[[], str]"; expected "Callable[[Any], str]" (diff)
  main:17: error: Cannot determine __init__ type from converter (diff)
  main:17: error: Argument "converter" has incompatible type overloaded function; expected "Callable[[Any], int]" (diff)
  ...

Alignment of first line difference:
  E: main:16: error: Cannot determine __init__ type from converter...
  A: main:10: error: Overloaded function implementation does not accept all p...
           ^
_________________________________________ testAttrsUsingBadConverterReprocess _________________________________________
[gw2] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\check-attr.test:843:
C:\Repos\ekr-mypy\mypy\test\testcheck.py:151: in run_case
    self.run_case_once(testcase)
C:\Repos\ekr-mypy\mypy\test\testcheck.py:239: in run_case_once
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))
E   AssertionError: Unexpected type checker output (C:\Repos\ekr-mypy\test-data\unit\check-attr.test, line 843)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  main:17: error: Cannot determine __init__ type from converter (diff)
  main:17: error: Argument "converter" has incompatible type "Callable[[], str]"; expected "Callable[[Any], str]" (diff)
  main:18: error: Cannot determine __init__ type from converter (diff)
  main:18: error: Argument "converter" has incompatible type overloaded function; expected "Callable[[Any], int]" (diff)
  ...
Actual:
  main:11: error: Overloaded function implementation does not accept all possible arguments of signature 2 (diff)
  main:17: error: Cannot determine __init__ type from converter (diff)
  main:17: error: Argument "converter" has incompatible type "Callable[[], str]"; expected "Callable[[Any], str]" (diff)
  main:18: error: Cannot determine __init__ type from converter (diff)
  main:18: error: Argument "converter" has incompatible type overloaded function; expected "Callable[[Any], int]" (diff)
  ...

Alignment of first line difference:
  E: main:17: error: Cannot determine __init__ type from converter...
  A: main:11: error: Overloaded function implementation does not accept all p...
           ^
___________________________________________________ testDefaultArgs ___________________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\parse.test:1064:
C:\Repos\ekr-mypy\mypy\test\testparse.py:25: in run_case
    test_parser(testcase)
C:\Repos\ekr-mypy\mypy\test\testparse.py:51: in test_parser
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid parser output (C:\Repos\ekr-mypy\test-data\unit\parse.test, line 1064)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  ...
      Args(
        default(
          Var(x)
          IntExpr(1)))
      Block:1(                                  (diff)
        PassStmt:2()))                          (diff)
    FuncDef:3(                                  (diff)
      g                                         (diff)
  ...
Actual:
  ...
      Args(
        default(
          Var(x)
          IntExpr(1)))
      def (x: int? =) -&gt; Any                    (diff)
      Block:1(                                  (diff)
        PassStmt:2()))                          (diff)
    FuncDef:3(                                  (diff)
      g                                         (diff)
  ...

Alignment of first line difference:
  E:     Block:1(
  A:     def (x: int? =) -&gt; Any
         ^
______________________________________________ testBareAsteriskInFuncDef ______________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\parse.test:2277:
C:\Repos\ekr-mypy\mypy\test\testparse.py:25: in run_case
    test_parser(testcase)
C:\Repos\ekr-mypy\mypy\test\testparse.py:51: in test_parser
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid parser output (C:\Repos\ekr-mypy\test-data\unit\parse.test, line 2277)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  ...
        Var(x)
        default(
          Var(y)
          IntExpr(1)))
      Block:1(                                  (diff)
        PassStmt:1())))                         (diff)
Actual:
  ...
        Var(x)
        default(
          Var(y)
          IntExpr(1)))
      def (x: Any, *, y: int? =) -&gt; Any         (diff)
      Block:1(                                  (diff)
        PassStmt:1())))                         (diff)

Alignment of first line difference:
  E:     Block:1(
  A:     def (x: Any, *, y: int? =) -&gt; Any
         ^
__________________________________________________ testDefaultArgInt __________________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\stubgen.test:21:
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:563: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:596: in run_case_inner
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\test-data\unit\stubgen.test, line 21)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  def f(a, b: int = ...) -&gt; None: ...           (diff)
  def g(b: int = ..., c: int = ...) -&gt; None: ... (diff)
Actual:
  def f(a, b: int = ...): ...                   (diff)
  def g(b: int = ..., c: int = ...): ...        (diff)

Alignment of first line difference:
  E: def f(a, b: int = ...) -&gt; None: ...
  A: def f(a, b: int = ...): ...
                           ^
_________________________________________________ testDefaultArgBool __________________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\stubgen.test:35:
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:563: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:596: in run_case_inner
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\test-data\unit\stubgen.test, line 35)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  def f(x: bool = ..., y: bool = ...) -&gt; None: ... (diff)
Actual:
  def f(x: bool = ..., y: bool = ...): ...      (diff)

Alignment of first line difference:
  E: ...ool = ..., y: bool = ...) -&gt; None: ...
  A: ...ool = ..., y: bool = ...): ...
                                 ^
__________________________________________________ testDefaultArgStr __________________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\stubgen.test:40:
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:563: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:596: in run_case_inner
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\test-data\unit\stubgen.test, line 40)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  def f(x: str = ...) -&gt; None: ...              (diff)
Actual:
  def f(x: str = ...): ...                      (diff)

Alignment of first line difference:
  E: def f(x: str = ...) -&gt; None: ...
  A: def f(x: str = ...): ...
                        ^
_________________________________________________ testDefaultArgBytes _________________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\stubgen.test:45:
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:563: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:596: in run_case_inner
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\test-data\unit\stubgen.test, line 45)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  def f(x: bytes = ...) -&gt; None: ...            (diff)
Actual:
  def f(x: bytes = ...): ...                    (diff)

Alignment of first line difference:
  E: def f(x: bytes = ...) -&gt; None: ...
  A: def f(x: bytes = ...): ...
                          ^
_________________________________________________ testDefaultArgFloat _________________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\stubgen.test:50:
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:563: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:596: in run_case_inner
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\test-data\unit\stubgen.test, line 50)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  def f(x: float = ...) -&gt; None: ...            (diff)
Actual:
  def f(x: float = ...): ...                    (diff)

Alignment of first line difference:
  E: def f(x: float = ...) -&gt; None: ...
  A: def f(x: float = ...): ...
                          ^
______________________________________________ testVarArgsWithKwVarArgs _______________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\stubgen.test:106:
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:563: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:596: in run_case_inner
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\test-data\unit\stubgen.test, line 106)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  def f(a, *b, **c) -&gt; None: ...
  def g(a, *b, c: int = ...) -&gt; None: ...       (diff)
  def h(a, *b, c: int = ..., **d) -&gt; None: ...  (diff)
  def i(a, *, b: int = ...) -&gt; None: ...        (diff)
  def j(a, *, b: int = ..., **c) -&gt; None: ...   (diff)
Actual:
  def f(a, *b, **c) -&gt; None: ...
  def g(a, *b, c: int = ...): ...               (diff)
  def h(a, *b, c: int = ..., **d): ...          (diff)
  def i(a, *, b: int = ...): ...                (diff)
  def j(a, *, b: int = ..., **c): ...           (diff)

Alignment of first line difference:
  E: def g(a, *b, c: int = ...) -&gt; None: ...
  A: def g(a, *b, c: int = ...): ...
                               ^
_________________________________________________ testKeywordOnlyArg __________________________________________________
[gw5] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\test-data\unit\stubgen.test:297:
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:563: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypy\test\teststubgen.py:596: in run_case_inner
    assert_string_arrays_equal(testcase.output, a,
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\test-data\unit\stubgen.test, line 297)
------------------------------------------------ Captured stderr call -------------------------------------------------
Expected:
  def f(x, *, y: int = ...) -&gt; None: ...        (diff)
  def g(x, *, y: int = ..., z: int = ...) -&gt; None: ... (diff)
Actual:
  def f(x, *, y: int = ...): ...                (diff)
  def g(x, *, y: int = ..., z: int = ...): ...  (diff)

Alignment of first line difference:
  E: def f(x, *, y: int = ...) -&gt; None: ...
  A: def f(x, *, y: int = ...): ...
                              ^
____________________________________________________ testException ____________________________________________________
[gw7] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-exceptions.test:3:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-exceptions.test, line 3)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-h2drmyy_\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\__native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 4, in &lt;module&gt;       (diff)
      f([])
    File "native.py", line 3, in f
      g(x)
    File "native.py", line 6, in g
      x[5] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "driver.py", line 8, in &lt;module&gt;       (diff)
      r1()
    File "native.py", line 10, in r1
      q1()
    File "native.py", line 13, in q1
      raise Exception("test")
  Exception: test
  Traceback (most recent call last):
    File "driver.py", line 12, in &lt;module&gt;      (diff)
      r2()
    File "native.py", line 16, in r2
      q2()
    File "native.py", line 19, in q2
      raise Exception
  Exception
  Traceback (most recent call last):
    File "driver.py", line 16, in &lt;module&gt;      (diff)
      hey()
    File "native.py", line 26, in hey
      A()
    File "native.py", line 23, in __init__
  ...
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-h2drmyy_\tmp\driver.py", line 4, in &lt;module&gt; (diff)
      f([])
    File "native.py", line 3, in f
      g(x)
    File "native.py", line 6, in g
      x[5] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-h2drmyy_\tmp\driver.py", line 8, in &lt;module&gt; (diff)
      r1()
    File "native.py", line 10, in r1
      q1()
    File "native.py", line 13, in q1
      raise Exception("test")
  Exception: test
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-h2drmyy_\tmp\driver.py", line 12, in &lt;module&gt; (diff)
      r2()
    File "native.py", line 16, in r2
      q2()
    File "native.py", line 19, in q2
      raise Exception
  Exception
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-h2drmyy_\tmp\driver.py", line 16, in &lt;module&gt; (diff)
      hey()
    File "native.py", line 26, in hey
      A()
    File "native.py", line 23, in __init__
  ...

Alignment of first line difference:
  E:   File "driver.py", line 4, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-h2drmyy_\tmp\driver.py...
             ^
____________________________________________________ testTryExcept ____________________________________________________
[gw7] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-exceptions.test:84:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-exceptions.test, line 84)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-y57ytsho\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\__native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  == i ==
  &lt;class 'IndexError'&gt;
  Traceback (most recent call last):
    File "driver.py", line 6, in &lt;module&gt;       (diff)
      i()
    File "native.py", line 44, in i
      r(0)
    File "native.py", line 15, in r
      [0][1]
  IndexError: list index out of range
  == k ==
  Traceback (most recent call last):
    File "native.py", line 59, in k
      r(1)
    File "native.py", line 17, in r
      raise Exception('hi')
  Exception: hi

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File "driver.py", line 12, in &lt;module&gt;      (diff)
      k()
    File "native.py", line 61, in k
      r(0)
    File "native.py", line 15, in r
  ...
Actual:
  == i ==
  &lt;class 'IndexError'&gt;
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-y57ytsho\tmp\driver.py", line 6, in &lt;module&gt; (diff)
      i()
    File "native.py", line 44, in i
      r(0)
    File "native.py", line 15, in r
      [0][1]
  IndexError: list index out of range
  == k ==
  Traceback (most recent call last):
    File "native.py", line 59, in k
      r(1)
    File "native.py", line 17, in r
      raise Exception('hi')
  Exception: hi

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-y57ytsho\tmp\driver.py", line 12, in &lt;module&gt; (diff)
      k()
    File "native.py", line 61, in k
      r(0)
    File "native.py", line 15, in r
  ...

Alignment of first line difference:
  E:   File "driver.py", line 6, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-y57ytsho\tmp\driver.py...
             ^
____________________________________________ testExceptionAtModuleTopLevel ____________________________________________
[gw7] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-exceptions.test:425:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-exceptions.test, line 425)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-rmokog3l\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\__native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 3, in &lt;module&gt;       (diff)
      import native
    File "native.py", line 6, in &lt;module&gt;
      f(y)
  TypeError: int object expected; got str
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-rmokog3l\tmp\driver.py", line 3, in &lt;module&gt; (diff)
      import native
    File "native.py", line 6, in &lt;module&gt;
      f(y)
  TypeError: int object expected; got str

Alignment of first line difference:
  E:   File "driver.py", line 3, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-rmokog3l\tmp\driver.py...
             ^
___________________________________________________ testForIterable ___________________________________________________
[gw4] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-loops.test:278:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-loops.test, line 278)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-cadczimy\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\__native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 16, in &lt;module&gt;      (diff)
      iterate_over_any(5)
    File "native.py", line 3, in iterate_over_any
      for element in a:
  TypeError: 'int' object is not iterable
  Traceback (most recent call last):
    File "driver.py", line 20, in &lt;module&gt;      (diff)
      iterate_over_iterable(broken_generator(5))
    File "native.py", line 7, in iterate_over_iterable
      for element in iterable:
    File "driver.py", line 8, in broken_generator (diff)
      raise Exception('Exception Manually Raised')
  Exception: Exception Manually Raised
  Traceback (most recent call last):
    File "driver.py", line 24, in &lt;module&gt;      (diff)
      iterate_and_delete(d)
    File "native.py", line 11, in iterate_and_delete
      for key in d:
  RuntimeError: dictionary changed size during iteration
  ...
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-cadczimy\tmp\driver.py", line 16, in &lt;module&gt; (diff)
      iterate_over_any(5)
    File "native.py", line 3, in iterate_over_any
      for element in a:
  TypeError: 'int' object is not iterable
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-cadczimy\tmp\driver.py", line 20, in &lt;module&gt; (diff)
      iterate_over_iterable(broken_generator(5))
    File "native.py", line 7, in iterate_over_iterable
      for element in iterable:
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-cadczimy\tmp\driver.py", line 8, in broken_generator (diff)
      raise Exception('Exception Manually Raised')
  Exception: Exception Manually Raised
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-cadczimy\tmp\driver.py", line 24, in &lt;module&gt; (diff)
      iterate_and_delete(d)
    File "native.py", line 11, in iterate_and_delete
      for key in d:
  RuntimeError: dictionary changed size during iteration
  ...

Alignment of first line difference:
  E:   File "driver.py", line 16, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-cadczimy\tmp\driver.py...
             ^
_______________________________________________ testSubclassUninitAttr ________________________________________________
[gw2] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-classes.test:636:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-classes.test, line 636)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-mj76bqqz\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\__native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 4, in &lt;module&gt;       (diff)
      A().x
  AttributeError: attribute 'x' of 'X' undefined
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-mj76bqqz\tmp\driver.py", line 4, in &lt;module&gt; (diff)
      A().x
  AttributeError: attribute 'x' of 'X' undefined

Alignment of first line difference:
  E:   File "driver.py", line 4, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-mj76bqqz\tmp\driver.py...
             ^
____________________________________________________ testProperty _____________________________________________________
[gw4] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-classes.test:1604:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-classes.test, line 1604)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-cjop2hh8\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\__native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 5, in &lt;module&gt;       (diff)
      print (x.rankine)
    File "native.py", line 16, in rankine
      raise NotImplementedError
  NotImplementedError
  ...
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-cjop2hh8\tmp\driver.py", line 5, in &lt;module&gt; (diff)
      print (x.rankine)
    File "native.py", line 16, in rankine
      raise NotImplementedError
  NotImplementedError
  ...

Alignment of first line difference:
  E:   File "driver.py", line 5, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-cjop2hh8\tmp\driver.py...
             ^
___________________________________________________ testYieldThrow ____________________________________________________
[gw7] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-generators.test:248:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-generators.test, line 248)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-nje4s332\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\__native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  ...
    File "native.py", line 9, in generator
      yield 2
    File "native.py", line 8, in generator
      yield 1
    File "driver.py", line 31, in &lt;module&gt;      (diff)
      raise Exception
    File "native.py", line 10, in generator
      yield 3
    File "native.py", line 30, in wrapper
  ...
Actual:
  ...
    File "native.py", line 9, in generator
      yield 2
    File "native.py", line 8, in generator
      yield 1
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-nje4s332\tmp\driver.py", line 31, in &lt;module&gt; (diff)
      raise Exception
    File "native.py", line 10, in generator
      yield 3
    File "native.py", line 30, in wrapper
  ...

Alignment of first line difference:
  E:   File "driver.py", line 31, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-nje4s332\tmp\driver.py...
             ^
______________________________________________ testMultiModuleTraceback _______________________________________________
[gw3] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-multimodule.test:251:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-multimodule.test, line 251)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building '3ef2a66b8671abc76f6e__mypyc' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native_3ef2a66b8671abc76f6e.c /Fobuild\temp.win-amd64-3.10\Release\build\__native_3ef2a66b8671abc76f6e.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native_3ef2a66b8671abc76f6e.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-xdoy5st0\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_3ef2a66b8671abc76f6e__mypyc build\temp.win-amd64-3.10\Release\build\__native_3ef2a66b8671abc76f6e.obj /OUT:build\lib.win-amd64-3.10\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.exp
Generating code
Finished generating code
building 'native' extension
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\native.c /Fobuild\temp.win-amd64-3.10\Release\build\native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
native.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
building 'other' extension
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\other.c /Fobuild\temp.win-amd64-3.10\Release\build\other.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
other.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_other build\temp.win-amd64-3.10\Release\build\other.obj /OUT:build\lib.win-amd64-3.10\other.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.pyd -&gt;
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
copying build\lib.win-amd64-3.10\other.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 6, in &lt;module&gt;       (diff)
      other.fail2()
    File "other.py", line 3, in fail2
      x[2] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "driver.py", line 12, in &lt;module&gt;      (diff)
      native.fail()
    File "native.py", line 4, in fail
      fail2()
    File "other.py", line 3, in fail2
  ...
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-xdoy5st0\tmp\driver.py", line 6, in &lt;module&gt; (diff)
      other.fail2()
    File "other.py", line 3, in fail2
      x[2] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-xdoy5st0\tmp\driver.py", line 12, in &lt;module&gt; (diff)
      native.fail()
    File "native.py", line 4, in fail
      fail2()
    File "other.py", line 3, in fail2
  ...

Alignment of first line difference:
  E:   File "driver.py", line 6, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-xdoy5st0\tmp\driver.py...
             ^
___________________________________________ testMultiModuleTraceback_multi ____________________________________________
[gw4] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-multimodule.test:251:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-multimodule.test, line 251)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building '3ef2a66b8671abc76f6e__mypyc' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native_3ef2a66b8671abc76f6e.c /Fobuild\temp.win-amd64-3.10\Release\build\__native_3ef2a66b8671abc76f6e.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
__native_3ef2a66b8671abc76f6e.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native_native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native_native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
__native_native.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native_other.c /Fobuild\temp.win-amd64-3.10\Release\build\__native_other.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
__native_other.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\bytes_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\bytes_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
bytes_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\dict_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\dict_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
dict_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\exc_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\exc_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
exc_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\generic_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\generic_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
generic_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\getargs.c /Fobuild\temp.win-amd64-3.10\Release\build\getargs.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
getargs.c
build\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\getargsfast.c /Fobuild\temp.win-amd64-3.10\Release\build\getargsfast.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
getargsfast.c
build\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\init.c /Fobuild\temp.win-amd64-3.10\Release\build\init.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
init.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\int_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\int_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
int_ops.c
build\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
build\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\list_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\list_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
list_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\misc_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\misc_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
misc_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\set_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\set_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
set_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\str_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\str_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
str_ops.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\tuple_ops.c /Fobuild\temp.win-amd64-3.10\Release\build\tuple_ops.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
tuple_ops.c
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-djg_ofgm\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_3ef2a66b8671abc76f6e__mypyc build\temp.win-amd64-3.10\Release\build\__native_3ef2a66b8671abc76f6e.obj build\temp.win-amd64-3.10\Release\build\__native_native.obj build\temp.win-amd64-3.10\Release\build\__native_other.obj build\temp.win-amd64-3.10\Release\build\bytes_ops.obj build\temp.win-amd64-3.10\Release\build\dict_ops.obj build\temp.win-amd64-3.10\Release\build\exc_ops.obj build\temp.win-amd64-3.10\Release\build\generic_ops.obj build\temp.win-amd64-3.10\Release\build\getargs.obj build\temp.win-amd64-3.10\Release\build\getargsfast.obj build\temp.win-amd64-3.10\Release\build\init.obj build\temp.win-amd64-3.10\Release\build\int_ops.obj build\temp.win-amd64-3.10\Release\build\list_ops.obj build\temp.win-amd64-3.10\Release\build\misc_ops.obj build\temp.win-amd64-3.10\Release\build\set_ops.obj build\temp.win-amd64-3.10\Release\build\str_ops.obj build\temp.win-amd64-3.10\Release\build\tuple_ops.obj /OUT:build\lib.win-amd64-3.10\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.exp
LINK : /LTCG specified but no code generation required; remove /LTCG from the link command line to improve linker performance
building 'native' extension
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\native.c /Fobuild\temp.win-amd64-3.10\Release\build\native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
native.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
LINK : /LTCG specified but no code generation required; remove /LTCG from the link command line to improve linker performance
building 'other' extension
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\other.c /Fobuild\temp.win-amd64-3.10\Release\build\other.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146 /GL- /wd9025
other.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_other build\temp.win-amd64-3.10\Release\build\other.obj /OUT:build\lib.win-amd64-3.10\other.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.exp
LINK : /LTCG specified but no code generation required; remove /LTCG from the link command line to improve linker performance
copying build\lib.win-amd64-3.10\3ef2a66b8671abc76f6e__mypyc.cp310-win_amd64.pyd -&gt;
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
copying build\lib.win-amd64-3.10\other.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/GL' with '/GL-'
cl : Command line warning D9014 : invalid value '9025' for '/wd'; assuming '5999'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 6, in &lt;module&gt;       (diff)
      other.fail2()
    File "other.py", line 3, in fail2
      x[2] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "driver.py", line 12, in &lt;module&gt;      (diff)
      native.fail()
    File "native.py", line 4, in fail
      fail2()
    File "other.py", line 3, in fail2
  ...
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-djg_ofgm\tmp\driver.py", line 6, in &lt;module&gt; (diff)
      other.fail2()
    File "other.py", line 3, in fail2
      x[2] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-djg_ofgm\tmp\driver.py", line 12, in &lt;module&gt; (diff)
      native.fail()
    File "native.py", line 4, in fail
      fail2()
    File "other.py", line 3, in fail2
  ...

Alignment of first line difference:
  E:   File "driver.py", line 6, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-djg_ofgm\tmp\driver.py...
             ^
__________________________________________ testMultiModuleTraceback_separate __________________________________________
[gw1] win32 -- Python 3.10.4 C:\Repos\ekr-mypy\venv\Scripts\python.exe
data: C:\Repos\ekr-mypy\mypyc\test-data\run-multimodule.test:251:
C:\Repos\ekr-mypy\mypyc\test\test_run.py:136: in run_case
    self.run_case_inner(testcase)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:152: in run_case_inner
    self.run_case_step(testcase, step)
C:\Repos\ekr-mypy\mypyc\test\test_run.py:316: in run_case_step
    assert_test_output(testcase, outlines, msg, expected)
E   AssertionError: Invalid output (C:\Repos\ekr-mypy\mypyc\test-data\run-multimodule.test, line 251)
------------------------------------------------ Captured stdout call -------------------------------------------------
running build_ext
building 'native__mypyc' extension
creating build\temp.win-amd64-3.10
creating build\temp.win-amd64-3.10\Release
creating build\temp.win-amd64-3.10\Release\build
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native_native.c /Fobuild\temp.win-amd64-3.10\Release\build\__native_native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native_native.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
creating C:\Users\Dev\AppData\Local\Temp\mypy-test-rujd5r59\tmp\build\lib.win-amd64-3.10
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native__mypyc build\temp.win-amd64-3.10\Release\build\__native_native.obj /OUT:build\lib.win-amd64-3.10\native__mypyc.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native__mypyc.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native__mypyc.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native__mypyc.cp310-win_amd64.exp
Generating code
Finished generating code
building 'native' extension
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\native.c /Fobuild\temp.win-amd64-3.10\Release\build\native.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
native.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_native build\temp.win-amd64-3.10\Release\build\native.obj /OUT:build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\native.cp310-win_amd64.exp
Generating code
Finished generating code
building 'other__mypyc' extension
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\mypyc\lib-rt -Ibuild -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\__native_other.c /Fobuild\temp.win-amd64-3.10\Release\build\__native_other.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
__native_other.c
C:\Repos\ekr-mypy\mypyc\lib-rt\getargs.c(342): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\getargsfast.c(452): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(279): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
C:\Repos\ekr-mypy\mypyc\lib-rt\int_ops.c(344): warning C4244: '=': conversion from 'Py_ssize_t' to 'digit', possible loss of data
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_other__mypyc build\temp.win-amd64-3.10\Release\build\__native_other.obj /OUT:build\lib.win-amd64-3.10\other__mypyc.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\other__mypyc.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\other__mypyc.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\other__mypyc.cp310-win_amd64.exp
Generating code
Finished generating code
building 'other' extension
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Repos\ekr-mypy\venv\include -IC:\Python\Python3.10\include -IC:\Python\Python3.10\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt /Tcbuild\other.c /Fobuild\temp.win-amd64-3.10\Release\build\other.obj /Od /DEBUG:FASTLINK /wd4102 /wd4101 /wd4146
other.c
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Repos\ekr-mypy\venv\libs /LIBPATH:C:\Python\Python3.10\libs /LIBPATH:C:\Python\Python3.10 /LIBPATH:C:\Repos\ekr-mypy\venv\PCbuild\amd64 /LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64 /LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64 /EXPORT:PyInit_other build\temp.win-amd64-3.10\Release\build\other.obj /OUT:build\lib.win-amd64-3.10\other.cp310-win_amd64.pyd /IMPLIB:build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.lib
   Creating library build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.lib and object build\temp.win-amd64-3.10\Release\build\other.cp310-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-3.10\native__mypyc.cp310-win_amd64.pyd -&gt;
copying build\lib.win-amd64-3.10\native.cp310-win_amd64.pyd -&gt;
copying build\lib.win-amd64-3.10\other__mypyc.cp310-win_amd64.pyd -&gt;
copying build\lib.win-amd64-3.10\other.cp310-win_amd64.pyd -&gt;
------------------------------------------------ Captured stderr call -------------------------------------------------
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
cl : Command line warning D9025 : overriding '/Ox' with '/Od'
Expected:
  Traceback (most recent call last):
    File "driver.py", line 6, in &lt;module&gt;       (diff)
      other.fail2()
    File "other.py", line 3, in fail2
      x[2] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "driver.py", line 12, in &lt;module&gt;      (diff)
      native.fail()
    File "native.py", line 4, in fail
      fail2()
    File "other.py", line 3, in fail2
  ...
Actual:
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-rujd5r59\tmp\driver.py", line 6, in &lt;module&gt; (diff)
      other.fail2()
    File "other.py", line 3, in fail2
      x[2] = 2
  IndexError: list assignment index out of range
  Traceback (most recent call last):
    File "C:\Users\Dev\AppData\Local\Temp\mypy-test-rujd5r59\tmp\driver.py", line 12, in &lt;module&gt; (diff)
      native.fail()
    File "native.py", line 4, in fail
      fail2()
    File "other.py", line 3, in fail2
  ...

Alignment of first line difference:
  E:   File "driver.py", line 6, in &lt;module&gt;...
  A:   File "C:\Users\Dev\AppData\Local\Temp\mypy-test-rujd5r59\tmp\driver.py...
             ^
</t>
<t tx="ekr.20220925073606.1"></t>
<t tx="ekr.20220930225101.1"></t>
<t tx="ekr.20221001054943.1">@language rest
@wrap

- Be able to single-step through unit tests.

@language python
</t>
<t tx="ekr.20221003060449.1"></t>
<t tx="ekr.20221003060506.1">@language rest
@wrap

A "visit" method (including another accept!) always follows accept. I understand tree traversal, so all my former uncertainties disappeared.
Understanding Node.__str__ and Type.__repr__

These are condensed, high-level, representations of Nodes and Types. Aha: there is seldom any need to worry about the underlying details! For example, here are the latest traces of ekr_test.py:

    analyze_func_def: f1_str_annotated
    analyze_func_def: tag: 0

    visit_callable_type:   func name: (t.definition._name) f1_str_annotated
    visit_callable_type:     callers: visit_func_def, analyze_func_def, visit_callable_type
    visit_callable_type:           t: CallableType def (ekr_a: str? =) -&gt; None?
    visit_callable_type: t.arg_types: [str?]
    visit_callable_type:         ret: def (ekr_a: builtins.str =)

    analyze_func_def: f2_str_not_annotated
    analyze_func_def: tag: 0

    ekr_test.py:10: error: Function is missing a type annotation for one or more arguments
       [no-untyped-def]
    Found 1 error in 1 file (checked 1 source file)

</t>
<t tx="ekr.20221003060650.1">@language rest
@wrap

Aha: in visit_callable_type, t.definition is either None or a FuncDef.
The PR would not be possible without the link back to the FuncDef.
t alone does not provide enough data.
Nodes are static; Types are dynamic

Nodes objects represent ast nodes.
Traversing ast trees has static-level complexity.
Type object represent computed types.
Computing Type objects is much harder.

So I'll focus my attention primarily to Type objects.
</t>
<t tx="ekr.20221003060901.1">@language rest
@wrap


[mypy issue #12352](https://github.com/python/mypy/issues/12352) suggests that there should be no need for annotations for kwargs initialized to known constant values.

For example::

    def bool_args(ok=True, die=False): pass
    def int_arg(i=1): pass
    def float_arg(ratio=1.0): pass
    def string_arg(s="Hello World"): pass

In this example, the **natural annotations** are as follows::

    def bool_args(ok: bool=True, die: bool=False): pass
    def int_arg(i:int=1): pass
    def float_arg(ratio: float=1.0): pass
    def string_arg(s:str="Hello World"): pass

The issue proposes a non-trivial change to mypy's type inference. The proposal suggests that mypy act *as if* a natural annotation exists for an initialized kwarg. **Note**: mypy will (must!) check the new (natural) annotation, and mypy must complain if the natural annotation is not correct.

**Pitch**

At present, with `disallow_untyped_defs = True` and `disallow_incomplete_defs = False`, mypy complains that the signatures above lack annotations. But the annotations unnecessarily clutter code and provide no new information to human readers. It should be relatively straightforward to infer the types of the kwargs based on the obvious types of the constants!

This feature would fix my biggest gripe about mypy.

**Summary of investigations**

My first idea was to modify only TypeChecker.check_for_missing_annotations, and perhaps its caller, check_func_def. But this idea has no chance of working!</t>
<t tx="ekr.20221003061049.1">@language rest
@nowrap


I add the following traces to SemanticAnalyzer.analyze_func_def::


    ekr_call_set = set()
    ekr_name_set = set()

    &lt;&lt; define callers and callerName &gt;&gt;

    def analyze_func_def(self, defn: FuncDef) -&gt; None:

        key, trace_tag = defn._name, 'analyze_func_def:'
        callers = self.callers(60)
        trace = True or key.startswith(('f1_', 'f2_'))
        if trace:
            if key in self.ekr_name_set and callers in self.ekr_call_set:
                pass
            elif callers in self.ekr_call_set:
                print(f"{trace_tag} {key}")
            else:
                self.ekr_call_set.add(callers)
                print('')
                print(f"{trace_tag} {key} callers: {callers}\n")
            self.ekr_name_set.add(key)
`
Here are the resulting traces, slightly reformatted,
with the names of the "extra" functions omitted::

    analyze_func_def: __getitem__ callers:
    &lt;module&gt;, console_entry, main, run_build, build, _build, dispatch,
    process_graph, process_stale_scc, semantic_analysis_for_scc,
    process_functions, process_top_level_function, semantic_analyze_target, refresh_partial, accept, accept,
    visit_func_def, analyze_func_def
    ...
    analyze_func_def: __round__ callers:
    &lt;module&gt;, console_entry, main, run_build, build, _build, dispatch,
    process_graph, process_stale_scc, semantic_analysis_for_scc,
    process_functions, process_top_level_function, semantic_analyze_target, refresh_partial, accept, accept,
    visit_overloaded_func_def, analyze_overloaded_func_def, accept, visit_decorator, accept,
    visit_func_def, analyze_func_def
    ...
    analyze_func_def: __round__ callers:
    &lt;module&gt;, console_entry, main, run_build, build, _build, dispatch,
    process_graph, process_stale_scc, semantic_analysis_for_scc,
    process_functions, process_top_level_function, semantic_analyze_target, refresh_partial, accept, accept,
    visit_overloaded_func_def, analyze_overloaded_func_def, analyze_overload_sigs_and_impl, accept, visit_decorator, accept, visit_func_def, analyze_func_def
    ...
    analyze_func_def: _length_ callers:
    &lt;module&gt;, console_entry, main, run_build, build, _build, dispatch,
    process_graph, process_stale_scc, semantic_analysis_for_scc,
    process_functions, process_top_level_function, semantic_analyze_target, refresh_partial, accept, accept,
    visit_overloaded_func_def, analyze_overloaded_func_def, analyze_property_with_multi_part_definition, accept, visit_func_def, analyze_func_def
    ...
    analyze_func_def: f1_str_annotated
    visit_callable_type:   func name: (t.definition._name) f1_str_annotated
    visit_callable_type:     callers: visit_func_def, analyze_func_def, visit_callable_type
    visit_callable_type:           t: CallableType def (ekr_a: str? =) -&gt; None?
    visit_callable_type: t.arg_types: [str?]
    visit_callable_type:         ret: def (ekr_a: builtins.str =)

    analyze_func_def: f2_str_not_annotated
    ekr_test.py:9: error: Function is missing a type annotation for one or more arguments
    Found 1 error in 1 file (checked 1 source file)


All the caller traces start with:

&lt;module&gt;, console_entry, main, run_build, build, _build, dispatch,
process_graph, process_stale_scc, semantic_analysis_for_scc,
process_functions, process_top_level_function, semantic_analyze_target, refresh_partial, accept, accept,

The differences are in the following calls to visit_func_def or visit_overloaded_func_def, so the *reason* for all the calls lies in the *common* part of callers, probably in one of these:
    `run_build, build, _build, dispatch, process_graph, process_stale_scc`

Tracing shows two calls to process_stall_scc:

    scc [
       'typing_extensions', 'typing', 'types', 'sys', 'subprocess', 'posixpath', 'pickle', 'pathlib',
       'os.path', 'os', 'ntpath', 'mmap', 'io',
       'importlib.metadata._meta', 'importlib.metadata', 'importlib.machinery', 'importlib.abc', 'importlib',
        'genericpath',
        'email.policy', 'email.message', 'email.header', 'email.errors', 'email.contentmanager', 'email.charset', 'email',
        'ctypes', 'contextlib', 'collections.abc', 'collections', 'codecs',
        'array', 'abc', '_winapi', '_typeshed', '_collections_abc', '_ast', 'builtins'
    ]

    process_stale_scc: scc ['ekr_test']


So clearly the startup logic is analyzing a lot of modules!

More tracing reveals that the **load_graph** function in build.py returns a list of *many* modules, given only ekr_test.py as input.

**Summary**: It's still not clear *why* mypy analyzes all these modules, or whether command-line modules might disable the analysis.</t>
<t tx="ekr.20221004064034.10"># Later
def ekr_f_not_annotated2(b: int, ekr_a="abc") -&gt; None:
    pass
    
</t>
<t tx="ekr.20221004064034.100">def get_commits_starting_at(repo_folder_path: str, start_commit: str) -&gt; list[tuple[str, str]]:
    print(f"Fetching commits starting at {start_commit}")
    return get_commits(repo_folder_path, f"{start_commit}^..HEAD")


</t>
<t tx="ekr.20221004064034.1000">def __init__(self, options: Options, status_file: str, timeout: int | None = None) -&gt; None:
    """Initialize the server with the desired mypy flags."""
    self.options = options
    # Snapshot the options info before we muck with it, to detect changes
    self.options_snapshot = options.snapshot()
    self.timeout = timeout
    self.fine_grained_manager: FineGrainedBuildManager | None = None

    if os.path.isfile(status_file):
        os.unlink(status_file)

    self.fscache = FileSystemCache()

    options.raise_exceptions = True
    options.incremental = True
    options.fine_grained_incremental = True
    options.show_traceback = True
    if options.use_fine_grained_cache:
        # Using fine_grained_cache implies generating and caring
        # about the fine grained cache
        options.cache_fine_grained = True
    else:
        options.cache_dir = os.devnull
    # Fine-grained incremental doesn't support general partial types
    # (details in https://github.com/python/mypy/issues/4492)
    options.local_partial_types = True
    self.status_file = status_file

    # Since the object is created in the parent process we can check
    # the output terminal options here.
    self.formatter = FancyFormatter(sys.stdout, sys.stderr, options.hide_error_codes)

</t>
<t tx="ekr.20221004064034.1001">def _response_metadata(self) -&gt; dict[str, str]:
    py_version = f"{self.options.python_version[0]}_{self.options.python_version[1]}"
    return {"platform": self.options.platform, "python_version": py_version}

</t>
<t tx="ekr.20221004064034.1002">def serve(self) -&gt; None:
    """Serve requests, synchronously (no thread or fork)."""
    command = None
    server = IPCServer(CONNECTION_NAME, self.timeout)
    try:
        with open(self.status_file, "w") as f:
            json.dump({"pid": os.getpid(), "connection_name": server.connection_name}, f)
            f.write("\n")  # I like my JSON with a trailing newline
        while True:
            with server:
                data = receive(server)
                debug_stdout = io.StringIO()
                sys.stdout = debug_stdout
                resp: dict[str, Any] = {}
                if "command" not in data:
                    resp = {"error": "No command found in request"}
                else:
                    command = data["command"]
                    if not isinstance(command, str):
                        resp = {"error": "Command is not a string"}
                    else:
                        command = data.pop("command")
                        try:
                            resp = self.run_command(command, data)
                        except Exception:
                            # If we are crashing, report the crash to the client
                            tb = traceback.format_exception(*sys.exc_info())
                            resp = {"error": "Daemon crashed!\n" + "".join(tb)}
                            resp.update(self._response_metadata())
                            resp["stdout"] = debug_stdout.getvalue()
                            server.write(json.dumps(resp).encode("utf8"))
                            raise
                resp["stdout"] = debug_stdout.getvalue()
                try:
                    resp.update(self._response_metadata())
                    server.write(json.dumps(resp).encode("utf8"))
                except OSError:
                    pass  # Maybe the client hung up
                if command == "stop":
                    reset_global_state()
                    sys.exit(0)
    finally:
        # If the final command is something other than a clean
        # stop, remove the status file. (We can't just
        # simplify the logic and always remove the file, since
        # that could cause us to remove a future server's
        # status file.)
        if command != "stop":
            os.unlink(self.status_file)
        try:
            server.cleanup()  # try to remove the socket dir on Linux
        except OSError:
            pass
        exc_info = sys.exc_info()
        if exc_info[0] and exc_info[0] is not SystemExit:
            traceback.print_exception(*exc_info)

</t>
<t tx="ekr.20221004064034.1003">def run_command(self, command: str, data: dict[str, object]) -&gt; dict[str, object]:
    """Run a specific command from the registry."""
    key = "cmd_" + command
    method = getattr(self.__class__, key, None)
    if method is None:
        return {"error": f"Unrecognized command '{command}'"}
    else:
        if command not in {"check", "recheck", "run"}:
            # Only the above commands use some error formatting.
            del data["is_tty"]
            del data["terminal_width"]
        ret = method(self, **data)
        assert isinstance(ret, dict)
        return ret

</t>
<t tx="ekr.20221004064034.1004"># Command functions (run in the server via RPC).

</t>
<t tx="ekr.20221004064034.1005">def cmd_status(self, fswatcher_dump_file: str | None = None) -&gt; dict[str, object]:
    """Return daemon status."""
    res: dict[str, object] = {}
    res.update(get_meminfo())
    if fswatcher_dump_file:
        data = self.fswatcher.dump_file_data() if hasattr(self, "fswatcher") else {}
        # Using .dumps and then writing was noticeably faster than using dump
        s = json.dumps(data)
        with open(fswatcher_dump_file, "w") as f:
            f.write(s)
    return res

</t>
<t tx="ekr.20221004064034.1006">def cmd_stop(self) -&gt; dict[str, object]:
    """Stop daemon."""
    # We need to remove the status file *before* we complete the
    # RPC. Otherwise a race condition exists where a subsequent
    # command can see a status file from a dying server and think
    # it is a live one.
    os.unlink(self.status_file)
    return {}

</t>
<t tx="ekr.20221004064034.1007">def cmd_run(
    self,
    version: str,
    args: Sequence[str],
    export_types: bool,
    is_tty: bool,
    terminal_width: int,
) -&gt; dict[str, object]:
    """Check a list of files, triggering a restart if needed."""
    stderr = io.StringIO()
    stdout = io.StringIO()
    try:
        # Process options can exit on improper arguments, so we need to catch that and
        # capture stderr so the client can report it
        with redirect_stderr(stderr):
            with redirect_stdout(stdout):
                sources, options = mypy.main.process_options(
                    ["-i"] + list(args),
                    require_targets=True,
                    server_options=True,
                    fscache=self.fscache,
                    program="mypy-daemon",
                    header=argparse.SUPPRESS,
                )
        # Signal that we need to restart if the options have changed
        if self.options_snapshot != options.snapshot():
            return {"restart": "configuration changed"}
        if __version__ != version:
            return {"restart": "mypy version changed"}
        if self.fine_grained_manager:
            manager = self.fine_grained_manager.manager
            start_plugins_snapshot = manager.plugins_snapshot
            _, current_plugins_snapshot = mypy.build.load_plugins(
                options, manager.errors, sys.stdout, extra_plugins=()
            )
            if current_plugins_snapshot != start_plugins_snapshot:
                return {"restart": "plugins changed"}
    except InvalidSourceList as err:
        return {"out": "", "err": str(err), "status": 2}
    except SystemExit as e:
        return {"out": stdout.getvalue(), "err": stderr.getvalue(), "status": e.code}
    return self.check(sources, export_types, is_tty, terminal_width)

</t>
<t tx="ekr.20221004064034.1008">def cmd_check(
    self, files: Sequence[str], export_types: bool, is_tty: bool, terminal_width: int
) -&gt; dict[str, object]:
    """Check a list of files."""
    try:
        sources = create_source_list(files, self.options, self.fscache)
    except InvalidSourceList as err:
        return {"out": "", "err": str(err), "status": 2}
    return self.check(sources, export_types, is_tty, terminal_width)

</t>
<t tx="ekr.20221004064034.1009">def cmd_recheck(
    self,
    is_tty: bool,
    terminal_width: int,
    export_types: bool,
    remove: list[str] | None = None,
    update: list[str] | None = None,
) -&gt; dict[str, object]:
    """Check the same list of files we checked most recently.

    If remove/update is given, they modify the previous list;
    if all are None, stat() is called for each file in the previous list.
    """
    t0 = time.time()
    if not self.fine_grained_manager:
        return {"error": "Command 'recheck' is only valid after a 'check' command"}
    sources = self.previous_sources
    if remove:
        removals = set(remove)
        sources = [s for s in sources if s.path and s.path not in removals]
    if update:
        known = {s.path for s in sources if s.path}
        added = [p for p in update if p not in known]
        try:
            added_sources = create_source_list(added, self.options, self.fscache)
        except InvalidSourceList as err:
            return {"out": "", "err": str(err), "status": 2}
        sources = sources + added_sources  # Make a copy!
    t1 = time.time()
    manager = self.fine_grained_manager.manager
    manager.log(f"fine-grained increment: cmd_recheck: {t1 - t0:.3f}s")
    self.options.export_types = export_types
    if not self.following_imports():
        messages = self.fine_grained_increment(sources, remove, update)
    else:
        assert remove is None and update is None
        messages = self.fine_grained_increment_follow_imports(sources)
    res = self.increment_output(messages, sources, is_tty, terminal_width)
    self.flush_caches()
    self.update_stats(res)
    return res

</t>
<t tx="ekr.20221004064034.101">def get_nth_commit(repo_folder_path: str, n: int) -&gt; tuple[str, str]:
    print(f"Fetching last {n} commits (or all, if there are fewer commits than n)")
    return get_commits(repo_folder_path, f"-{n}")[0]


</t>
<t tx="ekr.20221004064034.1010">def check(
    self, sources: list[BuildSource], export_types: bool, is_tty: bool, terminal_width: int
) -&gt; dict[str, Any]:
    """Check using fine-grained incremental mode.

    If is_tty is True format the output nicely with colors and summary line
    (unless disabled in self.options). Also pass the terminal_width to formatter.
    """
    self.options.export_types = export_types
    if not self.fine_grained_manager:
        res = self.initialize_fine_grained(sources, is_tty, terminal_width)
    else:
        if not self.following_imports():
            messages = self.fine_grained_increment(sources)
        else:
            messages = self.fine_grained_increment_follow_imports(sources)
        res = self.increment_output(messages, sources, is_tty, terminal_width)
    self.flush_caches()
    self.update_stats(res)
    return res

</t>
<t tx="ekr.20221004064034.1011">def flush_caches(self) -&gt; None:
    self.fscache.flush()
    if self.fine_grained_manager:
        self.fine_grained_manager.flush_cache()

</t>
<t tx="ekr.20221004064034.1012">def update_stats(self, res: dict[str, Any]) -&gt; None:
    if self.fine_grained_manager:
        manager = self.fine_grained_manager.manager
        manager.dump_stats()
        res["stats"] = manager.stats
        manager.stats = {}

</t>
<t tx="ekr.20221004064034.1013">def following_imports(self) -&gt; bool:
    """Are we following imports?"""
    # TODO: What about silent?
    return self.options.follow_imports == "normal"

</t>
<t tx="ekr.20221004064034.1014">def initialize_fine_grained(
    self, sources: list[BuildSource], is_tty: bool, terminal_width: int
) -&gt; dict[str, Any]:
    self.fswatcher = FileSystemWatcher(self.fscache)
    t0 = time.time()
    self.update_sources(sources)
    t1 = time.time()
    try:
        result = mypy.build.build(sources=sources, options=self.options, fscache=self.fscache)
    except mypy.errors.CompileError as e:
        output = "".join(s + "\n" for s in e.messages)
        if e.use_stdout:
            out, err = output, ""
        else:
            out, err = "", output
        return {"out": out, "err": err, "status": 2}
    messages = result.errors
    self.fine_grained_manager = FineGrainedBuildManager(result)

    if self.following_imports():
        sources = find_all_sources_in_build(self.fine_grained_manager.graph, sources)
        self.update_sources(sources)

    self.previous_sources = sources

    # If we are using the fine-grained cache, build hasn't actually done
    # the typechecking on the updated files yet.
    # Run a fine-grained update starting from the cached data
    if result.used_cache:
        t2 = time.time()
        # Pull times and hashes out of the saved_cache and stick them into
        # the fswatcher, so we pick up the changes.
        for state in self.fine_grained_manager.graph.values():
            meta = state.meta
            if meta is None:
                continue
            assert state.path is not None
            self.fswatcher.set_file_data(
                state.path,
                FileData(st_mtime=float(meta.mtime), st_size=meta.size, hash=meta.hash),
            )

        changed, removed = self.find_changed(sources)
        changed += self.find_added_suppressed(
            self.fine_grained_manager.graph,
            set(),
            self.fine_grained_manager.manager.search_paths,
        )

        # Find anything that has had its dependency list change
        for state in self.fine_grained_manager.graph.values():
            if not state.is_fresh():
                assert state.path is not None
                changed.append((state.id, state.path))

        t3 = time.time()
        # Run an update
        messages = self.fine_grained_manager.update(changed, removed)

        if self.following_imports():
            # We need to do another update to any new files found by following imports.
            messages = self.fine_grained_increment_follow_imports(sources)

        t4 = time.time()
        self.fine_grained_manager.manager.add_stats(
            update_sources_time=t1 - t0,
            build_time=t2 - t1,
            find_changes_time=t3 - t2,
            fg_update_time=t4 - t3,
            files_changed=len(removed) + len(changed),
        )

    else:
        # Stores the initial state of sources as a side effect.
        self.fswatcher.find_changed()

    if MEM_PROFILE:
        from mypy.memprofile import print_memory_profile

        print_memory_profile(run_gc=False)

    status = 1 if messages else 0
    messages = self.pretty_messages(messages, len(sources), is_tty, terminal_width)
    return {"out": "".join(s + "\n" for s in messages), "err": "", "status": status}

</t>
<t tx="ekr.20221004064034.1015">def fine_grained_increment(
    self,
    sources: list[BuildSource],
    remove: list[str] | None = None,
    update: list[str] | None = None,
) -&gt; list[str]:
    """Perform a fine-grained type checking increment.

    If remove and update are None, determine changed paths by using
    fswatcher. Otherwise, assume that only these files have changes.

    Args:
        sources: sources passed on the command line
        remove: paths of files that have been removed
        update: paths of files that have been changed or created
    """
    assert self.fine_grained_manager is not None
    manager = self.fine_grained_manager.manager

    t0 = time.time()
    if remove is None and update is None:
        # Use the fswatcher to determine which files were changed
        # (updated or added) or removed.
        self.update_sources(sources)
        changed, removed = self.find_changed(sources)
    else:
        # Use the remove/update lists to update fswatcher.
        # This avoids calling stat() for unchanged files.
        changed, removed = self.update_changed(sources, remove or [], update or [])
    changed += self.find_added_suppressed(
        self.fine_grained_manager.graph, set(), manager.search_paths
    )
    manager.search_paths = compute_search_paths(sources, manager.options, manager.data_dir)
    t1 = time.time()
    manager.log(f"fine-grained increment: find_changed: {t1 - t0:.3f}s")
    messages = self.fine_grained_manager.update(changed, removed)
    t2 = time.time()
    manager.log(f"fine-grained increment: update: {t2 - t1:.3f}s")
    manager.add_stats(
        find_changes_time=t1 - t0,
        fg_update_time=t2 - t1,
        files_changed=len(removed) + len(changed),
    )

    self.previous_sources = sources
    return messages

</t>
<t tx="ekr.20221004064034.1016">def fine_grained_increment_follow_imports(self, sources: list[BuildSource]) -&gt; list[str]:
    """Like fine_grained_increment, but follow imports."""
    t0 = time.time()

    # TODO: Support file events

    assert self.fine_grained_manager is not None
    fine_grained_manager = self.fine_grained_manager
    graph = fine_grained_manager.graph
    manager = fine_grained_manager.manager

    orig_modules = list(graph.keys())

    self.update_sources(sources)
    changed_paths = self.fswatcher.find_changed()
    manager.search_paths = compute_search_paths(sources, manager.options, manager.data_dir)

    t1 = time.time()
    manager.log(f"fine-grained increment: find_changed: {t1 - t0:.3f}s")

    seen = {source.module for source in sources}

    # Find changed modules reachable from roots (or in roots) already in graph.
    changed, new_files = self.find_reachable_changed_modules(
        sources, graph, seen, changed_paths
    )
    sources.extend(new_files)

    # Process changes directly reachable from roots.
    messages = fine_grained_manager.update(changed, [])

    # Follow deps from changed modules (still within graph).
    worklist = changed[:]
    while worklist:
        module = worklist.pop()
        if module[0] not in graph:
            continue
        sources2 = self.direct_imports(module, graph)
        # Filter anything already seen before. This prevents
        # infinite looping if there are any self edges. (Self
        # edges are maybe a bug, but...)
        sources2 = [source for source in sources2 if source.module not in seen]
        changed, new_files = self.find_reachable_changed_modules(
            sources2, graph, seen, changed_paths
        )
        self.update_sources(new_files)
        messages = fine_grained_manager.update(changed, [])
        worklist.extend(changed)

    t2 = time.time()

    def refresh_file(module: str, path: str) -&gt; list[str]:
        return fine_grained_manager.update([(module, path)], [])

    for module_id, state in list(graph.items()):
        new_messages = refresh_suppressed_submodules(
            module_id, state.path, fine_grained_manager.deps, graph, self.fscache, refresh_file
        )
        if new_messages is not None:
            messages = new_messages

    t3 = time.time()

    # There may be new files that became available, currently treated as
    # suppressed imports. Process them.
    while True:
        new_unsuppressed = self.find_added_suppressed(graph, seen, manager.search_paths)
        if not new_unsuppressed:
            break
        new_files = [BuildSource(mod[1], mod[0]) for mod in new_unsuppressed]
        sources.extend(new_files)
        self.update_sources(new_files)
        messages = fine_grained_manager.update(new_unsuppressed, [])

        for module_id, path in new_unsuppressed:
            new_messages = refresh_suppressed_submodules(
                module_id, path, fine_grained_manager.deps, graph, self.fscache, refresh_file
            )
            if new_messages is not None:
                messages = new_messages

    t4 = time.time()

    # Find all original modules in graph that were not reached -- they are deleted.
    to_delete = []
    for module_id in orig_modules:
        if module_id not in graph:
            continue
        if module_id not in seen:
            module_path = graph[module_id].path
            assert module_path is not None
            to_delete.append((module_id, module_path))
    if to_delete:
        messages = fine_grained_manager.update([], to_delete)

    fix_module_deps(graph)

    self.previous_sources = find_all_sources_in_build(graph)
    self.update_sources(self.previous_sources)

    # Store current file state as side effect
    self.fswatcher.find_changed()

    t5 = time.time()

    manager.log(f"fine-grained increment: update: {t5 - t1:.3f}s")
    manager.add_stats(
        find_changes_time=t1 - t0,
        fg_update_time=t2 - t1,
        refresh_suppressed_time=t3 - t2,
        find_added_supressed_time=t4 - t3,
        cleanup_time=t5 - t4,
    )

    return messages

</t>
<t tx="ekr.20221004064034.1017">def find_reachable_changed_modules(
    self,
    roots: list[BuildSource],
    graph: mypy.build.Graph,
    seen: set[str],
    changed_paths: AbstractSet[str],
) -&gt; tuple[list[tuple[str, str]], list[BuildSource]]:
    """Follow imports within graph from given sources until hitting changed modules.

    If we find a changed module, we can't continue following imports as the imports
    may have changed.

    Args:
        roots: modules where to start search from
        graph: module graph to use for the search
        seen: modules we've seen before that won't be visited (mutated here!!)
        changed_paths: which paths have changed (stop search here and return any found)

    Return (encountered reachable changed modules,
            unchanged files not in sources_set traversed).
    """
    changed = []
    new_files = []
    worklist = roots[:]
    seen.update(source.module for source in worklist)
    while worklist:
        nxt = worklist.pop()
        if nxt.module not in seen:
            seen.add(nxt.module)
            new_files.append(nxt)
        if nxt.path in changed_paths:
            assert nxt.path is not None  # TODO
            changed.append((nxt.module, nxt.path))
        elif nxt.module in graph:
            state = graph[nxt.module]
            for dep in state.dependencies:
                if dep not in seen:
                    seen.add(dep)
                    worklist.append(BuildSource(graph[dep].path, graph[dep].id))
    return changed, new_files

</t>
<t tx="ekr.20221004064034.1018">def direct_imports(
    self, module: tuple[str, str], graph: mypy.build.Graph
) -&gt; list[BuildSource]:
    """Return the direct imports of module not included in seen."""
    state = graph[module[0]]
    return [BuildSource(graph[dep].path, dep) for dep in state.dependencies]

</t>
<t tx="ekr.20221004064034.1019">def find_added_suppressed(
    self, graph: mypy.build.Graph, seen: set[str], search_paths: SearchPaths
) -&gt; list[tuple[str, str]]:
    """Find suppressed modules that have been added (and not included in seen).

    Args:
        seen: reachable modules we've seen before (mutated here!!)

    Return suppressed, added modules.
    """
    all_suppressed = set()
    for state in graph.values():
        all_suppressed |= state.suppressed_set

    # Filter out things that shouldn't actually be considered suppressed.
    #
    # TODO: Figure out why these are treated as suppressed
    all_suppressed = {
        module
        for module in all_suppressed
        if module not in graph and not ignore_suppressed_imports(module)
    }

    # Optimization: skip top-level packages that are obviously not
    # there, to avoid calling the relatively slow find_module()
    # below too many times.
    packages = {module.split(".", 1)[0] for module in all_suppressed}
    packages = filter_out_missing_top_level_packages(packages, search_paths, self.fscache)

    # TODO: Namespace packages

    finder = FindModuleCache(search_paths, self.fscache, self.options)

    found = []

    for module in all_suppressed:
        top_level_pkg = module.split(".", 1)[0]
        if top_level_pkg not in packages:
            # Fast path: non-existent top-level package
            continue
        result = finder.find_module(module, fast_path=True)
        if isinstance(result, str) and module not in seen:
            # When not following imports, we only follow imports to .pyi files.
            if not self.following_imports() and not result.endswith(".pyi"):
                continue
            found.append((module, result))
            seen.add(module)

    return found

</t>
<t tx="ekr.20221004064034.102">def run_mypy(
    target_file_path: str | None,
    mypy_cache_path: str,
    mypy_script: str | None,
    *,
    incremental: bool = False,
    daemon: bool = False,
    verbose: bool = False,
) -&gt; tuple[float, str, dict[str, Any]]:
    """Runs mypy against `target_file_path` and returns what mypy prints to stdout as a string.

    If `incremental` is set to True, this function will use store and retrieve all caching data
    inside `mypy_cache_path`. If `verbose` is set to True, this function will pass the "-v -v"
    flags to mypy to make it output debugging information.

    If `daemon` is True, we use daemon mode; the daemon must be started and stopped by the caller.
    """
    stats: dict[str, Any] = {}
    if daemon:
        command = DAEMON_CMD + ["check", "-v"]
    else:
        if mypy_script is None:
            command = ["python3", "-m", "mypy"]
        else:
            command = [mypy_script]
        command.extend(["--cache-dir", mypy_cache_path])
        if incremental:
            command.append("--incremental")
        if verbose:
            command.extend(["-v", "-v"])
    if target_file_path is not None:
        command.append(target_file_path)
    start = time.time()
    output, stderr, _ = execute(command, False)
    if stderr != "":
        output = stderr
    else:
        if daemon:
            output, stats = filter_daemon_stats(output)
    runtime = time.time() - start
    return runtime, output, stats


</t>
<t tx="ekr.20221004064034.1020">def increment_output(
    self, messages: list[str], sources: list[BuildSource], is_tty: bool, terminal_width: int
) -&gt; dict[str, Any]:
    status = 1 if messages else 0
    messages = self.pretty_messages(messages, len(sources), is_tty, terminal_width)
    return {"out": "".join(s + "\n" for s in messages), "err": "", "status": status}

</t>
<t tx="ekr.20221004064034.1021">def pretty_messages(
    self,
    messages: list[str],
    n_sources: int,
    is_tty: bool = False,
    terminal_width: int | None = None,
) -&gt; list[str]:
    use_color = self.options.color_output and is_tty
    fit_width = self.options.pretty and is_tty
    if fit_width:
        messages = self.formatter.fit_in_terminal(
            messages, fixed_terminal_width=terminal_width
        )
    if self.options.error_summary:
        summary: str | None = None
        n_errors, n_notes, n_files = count_stats(messages)
        if n_errors:
            summary = self.formatter.format_error(
                n_errors, n_files, n_sources, use_color=use_color
            )
        elif not messages or n_notes == len(messages):
            summary = self.formatter.format_success(n_sources, use_color)
        if summary:
            # Create new list to avoid appending multiple summaries on successive runs.
            messages = messages + [summary]
    if use_color:
        messages = [self.formatter.colorize(m) for m in messages]
    return messages

</t>
<t tx="ekr.20221004064034.1022">def update_sources(self, sources: list[BuildSource]) -&gt; None:
    paths = [source.path for source in sources if source.path is not None]
    if self.following_imports():
        # Filter out directories (used for namespace packages).
        paths = [path for path in paths if self.fscache.isfile(path)]
    self.fswatcher.add_watched_paths(paths)

</t>
<t tx="ekr.20221004064034.1023">def update_changed(
    self, sources: list[BuildSource], remove: list[str], update: list[str]
) -&gt; ChangesAndRemovals:

    changed_paths = self.fswatcher.update_changed(remove, update)
    return self._find_changed(sources, changed_paths)

</t>
<t tx="ekr.20221004064034.1024">def find_changed(self, sources: list[BuildSource]) -&gt; ChangesAndRemovals:
    changed_paths = self.fswatcher.find_changed()
    return self._find_changed(sources, changed_paths)

</t>
<t tx="ekr.20221004064034.1025">def _find_changed(
    self, sources: list[BuildSource], changed_paths: AbstractSet[str]
) -&gt; ChangesAndRemovals:
    # Find anything that has been added or modified
    changed = [
        (source.module, source.path)
        for source in sources
        if source.path and source.path in changed_paths
    ]

    # Now find anything that has been removed from the build
    modules = {source.module for source in sources}
    omitted = [source for source in self.previous_sources if source.module not in modules]
    removed = []
    for source in omitted:
        path = source.path
        assert path
        removed.append((source.module, path))

    # Find anything that has had its module path change because of added or removed __init__s
    last = {s.path: s.module for s in self.previous_sources}
    for s in sources:
        assert s.path
        if s.path in last and last[s.path] != s.module:
            # Mark it as removed from its old name and changed at its new name
            removed.append((last[s.path], s.path))
            changed.append((s.module, s.path))

    return changed, removed

</t>
<t tx="ekr.20221004064034.1026">def cmd_inspect(
    self,
    show: str,
    location: str,
    verbosity: int = 0,
    limit: int = 0,
    include_span: bool = False,
    include_kind: bool = False,
    include_object_attrs: bool = False,
    union_attrs: bool = False,
    force_reload: bool = False,
) -&gt; dict[str, object]:
    """Locate and inspect expression(s)."""
    if sys.version_info &lt; (3, 8):
        return {"error": 'Python 3.8 required for "inspect" command'}
    if not self.fine_grained_manager:
        return {
            "error": 'Command "inspect" is only valid after a "check" command'
            " (that produces no parse errors)"
        }
    engine = InspectionEngine(
        self.fine_grained_manager,
        verbosity=verbosity,
        limit=limit,
        include_span=include_span,
        include_kind=include_kind,
        include_object_attrs=include_object_attrs,
        union_attrs=union_attrs,
        force_reload=force_reload,
    )
    old_inspections = self.options.inspections
    self.options.inspections = True
    try:
        if show == "type":
            result = engine.get_type(location)
        elif show == "attrs":
            result = engine.get_attrs(location)
        elif show == "definition":
            result = engine.get_definition(location)
        else:
            assert False, "Unknown inspection kind"
    finally:
        self.options.inspections = old_inspections
    if "out" in result:
        assert isinstance(result["out"], str)
        result["out"] += "\n"
    return result

</t>
<t tx="ekr.20221004064034.1027">def cmd_suggest(self, function: str, callsites: bool, **kwargs: Any) -&gt; dict[str, object]:
    """Suggest a signature for a function."""
    if not self.fine_grained_manager:
        return {
            "error": "Command 'suggest' is only valid after a 'check' command"
            " (that produces no parse errors)"
        }
    engine = SuggestionEngine(self.fine_grained_manager, **kwargs)
    try:
        if callsites:
            out = engine.suggest_callsites(function)
        else:
            out = engine.suggest(function)
    except SuggestionFailure as err:
        return {"error": str(err)}
    else:
        if not out:
            out = "No suggestions\n"
        elif not out.endswith("\n"):
            out += "\n"
        return {"out": out, "err": "", "status": 0}
    finally:
        self.flush_caches()

</t>
<t tx="ekr.20221004064034.1028">def cmd_hang(self) -&gt; dict[str, object]:
    """Hang for 100 seconds, as a debug hack."""
    time.sleep(100)
    return {}


</t>
<t tx="ekr.20221004064034.1029"># Misc utilities.


MiB: Final = 2**20


</t>
<t tx="ekr.20221004064034.103">def filter_daemon_stats(output: str) -&gt; tuple[str, dict[str, Any]]:
    stats: dict[str, Any] = {}
    lines = output.splitlines()
    output_lines = []
    for line in lines:
        m = re.match(r"(\w+)\s+:\s+(.*)", line)
        if m:
            key, value = m.groups()
            stats[key] = value
        else:
            output_lines.append(line)
    if output_lines:
        output_lines.append("\n")
    return "\n".join(output_lines), stats


</t>
<t tx="ekr.20221004064034.1030">def get_meminfo() -&gt; dict[str, Any]:
    res: dict[str, Any] = {}
    try:
        import psutil
    except ImportError:
        res["memory_psutil_missing"] = (
            "psutil not found, run pip install mypy[dmypy] "
            "to install the needed components for dmypy"
        )
    else:
        process = psutil.Process()
        meminfo = process.memory_info()
        res["memory_rss_mib"] = meminfo.rss / MiB
        res["memory_vms_mib"] = meminfo.vms / MiB
        if sys.platform == "win32":
            res["memory_maxrss_mib"] = meminfo.peak_wset / MiB
        else:
            # See https://stackoverflow.com/questions/938733/total-memory-used-by-python-process
            import resource  # Since it doesn't exist on Windows.

            rusage = resource.getrusage(resource.RUSAGE_SELF)
            if sys.platform == "darwin":
                factor = 1
            else:
                factor = 1024  # Linux
            res["memory_maxrss_mib"] = rusage.ru_maxrss * factor / MiB
    return res


</t>
<t tx="ekr.20221004064034.1031">def find_all_sources_in_build(
    graph: mypy.build.Graph, extra: Sequence[BuildSource] = ()
) -&gt; list[BuildSource]:
    result = list(extra)
    seen = {source.module for source in result}
    for module, state in graph.items():
        if module not in seen:
            result.append(BuildSource(state.path, module))
    return result


</t>
<t tx="ekr.20221004064034.1032">def fix_module_deps(graph: mypy.build.Graph) -&gt; None:
    """After an incremental update, update module dependencies to reflect the new state.

    This can make some suppressed dependencies non-suppressed, and vice versa (if modules
    have been added to or removed from the build).
    """
    for module, state in graph.items():
        new_suppressed = []
        new_dependencies = []
        for dep in state.dependencies + state.suppressed:
            if dep in graph:
                new_dependencies.append(dep)
            else:
                new_suppressed.append(dep)
        state.dependencies = new_dependencies
        state.dependencies_set = set(new_dependencies)
        state.suppressed = new_suppressed
        state.suppressed_set = set(new_suppressed)


</t>
<t tx="ekr.20221004064034.1033">def filter_out_missing_top_level_packages(
    packages: set[str], search_paths: SearchPaths, fscache: FileSystemCache
) -&gt; set[str]:
    """Quickly filter out obviously missing top-level packages.

    Return packages with entries that can't be found removed.

    This is approximate: some packages that aren't actually valid may be
    included. However, all potentially valid packages must be returned.
    """
    # Start with a empty set and add all potential top-level packages.
    found = set()
    paths = (
        search_paths.python_path
        + search_paths.mypy_path
        + search_paths.package_path
        + search_paths.typeshed_path
    )
    for p in paths:
        try:
            entries = fscache.listdir(p)
        except Exception:
            entries = []
        for entry in entries:
            # The code is hand-optimized for mypyc since this may be somewhat
            # performance-critical.
            if entry.endswith(".py"):
                entry = entry[:-3]
            elif entry.endswith(".pyi"):
                entry = entry[:-4]
            elif entry.endswith("-stubs"):
                # Possible PEP 561 stub package
                entry = entry[:-6]
            if entry in packages:
                found.add(entry)
    return found
</t>
<t tx="ekr.20221004064034.1034">@path C:/Repos/ekr-mypy2/mypy/
"""Shared code between dmypy.py and dmypy_server.py.

This should be pretty lightweight and not depend on other mypy code (other than ipc).
"""

from __future__ import annotations

import json
from typing import Any
from typing_extensions import Final

from mypy.ipc import IPCBase

DEFAULT_STATUS_FILE: Final = ".dmypy.json"


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1035">def receive(connection: IPCBase) -&gt; Any:
    """Receive JSON data from a connection until EOF.

    Raise OSError if the data received is not valid JSON or if it is
    not a dict.
    """
    bdata = connection.read()
    if not bdata:
        raise OSError("No data received")
    try:
        data = json.loads(bdata.decode("utf8"))
    except Exception as e:
        raise OSError("Data received is not valid JSON") from e
    if not isinstance(data, dict):
        raise OSError(f"Data received is not a dict ({type(data)})")
    return data
</t>
<t tx="ekr.20221004064034.1036">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Callable, Container, cast

from mypy.nodes import ARG_STAR, ARG_STAR2
from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeTranslator,
    TypeType,
    TypeVarId,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1037">def erase_type(typ: Type) -&gt; ProperType:
    """Erase any type variables from a type.

    Also replace tuple types with the corresponding concrete types.

    Examples:
      A -&gt; A
      B[X] -&gt; B[Any]
      Tuple[A, B] -&gt; tuple
      Callable[[A1, A2, ...], R] -&gt; Callable[..., Any]
      Type[X] -&gt; Type[Any]
    """
    typ = get_proper_type(typ)
    return typ.accept(EraseTypeVisitor())


</t>
<t tx="ekr.20221004064034.1038">class EraseTypeVisitor(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20221004064034.1039">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    # TODO: replace with an assert after UnboundType can't leak from semantic analysis.
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064034.104">def start_daemon(mypy_cache_path: str) -&gt; None:
    cmd = DAEMON_CMD + [
        "restart",
        "--log-file",
        "./@incr-chk-logs",
        "--",
        "--cache-dir",
        mypy_cache_path,
    ]
    execute(cmd)


</t>
<t tx="ekr.20221004064034.1040">def visit_any(self, t: AnyType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064034.1041">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064034.1042">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064034.1043">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064034.1044">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # Should not get here.
    raise RuntimeError()

</t>
<t tx="ekr.20221004064034.1045">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064034.1046">def visit_instance(self, t: Instance) -&gt; ProperType:
    return Instance(t.type, [AnyType(TypeOfAny.special_form)] * len(t.args), t.line)

</t>
<t tx="ekr.20221004064034.1047">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.1048">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.1049">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    raise RuntimeError("Parameters should have been bound to a class")

</t>
<t tx="ekr.20221004064034.105">def stop_daemon() -&gt; None:
    execute(DAEMON_CMD + ["stop"])


</t>
<t tx="ekr.20221004064034.1050">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.1051">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.1052">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    # We must preserve the fallback type for overload resolution to work.
    any_type = AnyType(TypeOfAny.special_form)
    return CallableType(
        arg_types=[any_type, any_type],
        arg_kinds=[ARG_STAR, ARG_STAR2],
        arg_names=[None, None],
        ret_type=any_type,
        fallback=t.fallback,
        is_ellipsis_args=True,
        implicit=True,
    )

</t>
<t tx="ekr.20221004064034.1053">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20221004064034.1054">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    return t.partial_fallback.accept(self)

</t>
<t tx="ekr.20221004064034.1055">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20221004064034.1056">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    # The fallback for literal types should always be either
    # something like int or str, or an enum class -- types that
    # don't contain any TypeVars. So there's no need to visit it.
    return t

</t>
<t tx="ekr.20221004064034.1057">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    erased_items = [erase_type(item) for item in t.items]
    from mypy.typeops import make_simplified_union

    return make_simplified_union(erased_items)

</t>
<t tx="ekr.20221004064034.1058">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    return TypeType.make_normalized(t.item.accept(self), line=t.line)

</t>
<t tx="ekr.20221004064034.1059">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    raise RuntimeError("Type aliases should be expanded before accepting this visitor")


</t>
<t tx="ekr.20221004064034.106">def load_cache(incremental_cache_path: str = CACHE_PATH) -&gt; JsonDict:
    if os.path.exists(incremental_cache_path):
        with open(incremental_cache_path) as stream:
            cache = json.load(stream)
            assert isinstance(cache, dict)
            return cache
    else:
        return {}


</t>
<t tx="ekr.20221004064034.1060">def erase_typevars(t: Type, ids_to_erase: Container[TypeVarId] | None = None) -&gt; Type:
    """Replace all type variables in a type with any,
    or just the ones in the provided collection.
    """

    @others
    return t.accept(TypeVarEraser(erase_id, AnyType(TypeOfAny.special_form)))


</t>
<t tx="ekr.20221004064034.1061">def erase_id(id: TypeVarId) -&gt; bool:
    if ids_to_erase is None:
        return True
    return id in ids_to_erase

</t>
<t tx="ekr.20221004064034.1062">def replace_meta_vars(t: Type, target_type: Type) -&gt; Type:
    """Replace unification variables in a type with the target type."""
    return t.accept(TypeVarEraser(lambda id: id.is_meta_var(), target_type))


</t>
<t tx="ekr.20221004064034.1063">class TypeVarEraser(TypeTranslator):
    """Implementation of type erasure"""

    @others
</t>
<t tx="ekr.20221004064034.1064">def __init__(self, erase_id: Callable[[TypeVarId], bool], replacement: Type) -&gt; None:
    self.erase_id = erase_id
    self.replacement = replacement

</t>
<t tx="ekr.20221004064034.1065">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20221004064034.1066">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20221004064034.1067">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20221004064034.1068">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Type alias target can't contain bound type variables, so
    # it is safe to just erase the arguments.
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20221004064034.1069">def remove_instance_last_known_values(t: Type) -&gt; Type:
    return t.accept(LastKnownValueEraser())


</t>
<t tx="ekr.20221004064034.107">def save_cache(cache: JsonDict, incremental_cache_path: str = CACHE_PATH) -&gt; None:
    with open(incremental_cache_path, "w") as stream:
        json.dump(cache, stream, indent=2)


</t>
<t tx="ekr.20221004064034.1070">class LastKnownValueEraser(TypeTranslator):
    """Removes the Literal[...] type that may be associated with any
    Instance types."""

    @others
</t>
<t tx="ekr.20221004064034.1071">def visit_instance(self, t: Instance) -&gt; Type:
    if not t.last_known_value and not t.args:
        return t
    return t.copy_modified(args=[a.accept(self) for a in t.args], last_known_value=None)

</t>
<t tx="ekr.20221004064034.1072">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Type aliases can't contain literal values, because they are
    # always constructed as explicit types.
    return t

</t>
<t tx="ekr.20221004064034.1073">def visit_union_type(self, t: UnionType) -&gt; Type:
    new = cast(UnionType, super().visit_union_type(t))
    # Erasure can result in many duplicate items; merge them.
    # Call make_simplified_union only on lists of instance types
    # that all have the same fullname, to avoid simplifying too
    # much.
    instances = [item for item in new.items if isinstance(get_proper_type(item), Instance)]
    # Avoid merge in simple cases such as optional types.
    if len(instances) &gt; 1:
        instances_by_name: dict[str, list[Instance]] = {}
        p_new_items = get_proper_types(new.items)
        for p_item in p_new_items:
            if isinstance(p_item, Instance) and not p_item.args:
                instances_by_name.setdefault(p_item.type.fullname, []).append(p_item)
        merged: list[Type] = []
        for item in new.items:
            orig_item = item
            item = get_proper_type(item)
            if isinstance(item, Instance) and not item.args:
                types = instances_by_name.get(item.type.fullname)
                if types is not None:
                    if len(types) == 1:
                        merged.append(item)
                    else:
                        from mypy.typeops import make_simplified_union

                        merged.append(make_simplified_union(types))
                        del instances_by_name[item.type.fullname]
            else:
                merged.append(orig_item)
        return UnionType.make_union(merged)
    return new
</t>
<t tx="ekr.20221004064034.1074">@path C:/Repos/ekr-mypy2/mypy/
"""Classification of possible errors mypy can detect.

These can be used for filtering specific errors.
"""

from __future__ import annotations

from typing_extensions import Final

error_codes: dict[str, ErrorCode] = {}


@others
ATTR_DEFINED: Final = ErrorCode("attr-defined", "Check that attribute exists", "General")
NAME_DEFINED: Final = ErrorCode("name-defined", "Check that name is defined", "General")
CALL_ARG: Final[ErrorCode] = ErrorCode(
    "call-arg", "Check number, names and kinds of arguments in calls", "General"
)
ARG_TYPE: Final = ErrorCode("arg-type", "Check argument types in calls", "General")
CALL_OVERLOAD: Final = ErrorCode(
    "call-overload", "Check that an overload variant matches arguments", "General"
)
VALID_TYPE: Final[ErrorCode] = ErrorCode(
    "valid-type", "Check that type (annotation) is valid", "General"
)
VAR_ANNOTATED: Final = ErrorCode(
    "var-annotated", "Require variable annotation if type can't be inferred", "General"
)
OVERRIDE: Final = ErrorCode(
    "override", "Check that method override is compatible with base class", "General"
)
RETURN: Final[ErrorCode] = ErrorCode(
    "return", "Check that function always returns a value", "General"
)
RETURN_VALUE: Final[ErrorCode] = ErrorCode(
    "return-value", "Check that return value is compatible with signature", "General"
)
ASSIGNMENT: Final[ErrorCode] = ErrorCode(
    "assignment", "Check that assigned value is compatible with target", "General"
)
TYPE_ARG: Final = ErrorCode("type-arg", "Check that generic type arguments are present", "General")
TYPE_VAR: Final = ErrorCode("type-var", "Check that type variable values are valid", "General")
UNION_ATTR: Final = ErrorCode(
    "union-attr", "Check that attribute exists in each item of a union", "General"
)
INDEX: Final = ErrorCode("index", "Check indexing operations", "General")
OPERATOR: Final = ErrorCode("operator", "Check that operator is valid for operands", "General")
LIST_ITEM: Final = ErrorCode(
    "list-item", "Check list items in a list expression [item, ...]", "General"
)
DICT_ITEM: Final = ErrorCode(
    "dict-item", "Check dict items in a dict expression {key: value, ...}", "General"
)
TYPEDDICT_ITEM: Final = ErrorCode(
    "typeddict-item", "Check items when constructing TypedDict", "General"
)
HAS_TYPE: Final = ErrorCode(
    "has-type", "Check that type of reference can be determined", "General"
)
IMPORT: Final = ErrorCode(
    "import", "Require that imported module can be found or has stubs", "General"
)
NO_REDEF: Final = ErrorCode("no-redef", "Check that each name is defined once", "General")
FUNC_RETURNS_VALUE: Final = ErrorCode(
    "func-returns-value", "Check that called function returns a value in value context", "General"
)
ABSTRACT: Final = ErrorCode(
    "abstract", "Prevent instantiation of classes with abstract attributes", "General"
)
TYPE_ABSTRACT: Final = ErrorCode(
    "type-abstract", "Require only concrete classes where Type[...] is expected", "General"
)
VALID_NEWTYPE: Final = ErrorCode(
    "valid-newtype", "Check that argument 2 to NewType is valid", "General"
)
STRING_FORMATTING: Final = ErrorCode(
    "str-format", "Check that string formatting/interpolation is type-safe", "General"
)
STR_BYTES_PY3: Final = ErrorCode(
    "str-bytes-safe", "Warn about dangerous coercions related to bytes and string types", "General"
)
EXIT_RETURN: Final = ErrorCode(
    "exit-return", "Warn about too general return type for '__exit__'", "General"
)
LITERAL_REQ: Final = ErrorCode("literal-required", "Check that value is a literal", "General")
UNUSED_COROUTINE: Final = ErrorCode(
    "unused-coroutine", "Ensure that all coroutines are used", "General"
)
# TODO: why do we need the explicit type here? Without it mypyc CI builds fail with
# mypy/message_registry.py:37: error: Cannot determine type of "EMPTY_BODY"  [has-type]
EMPTY_BODY: Final[ErrorCode] = ErrorCode(
    "empty-body",
    "A dedicated error code to opt out return errors for empty/trivial bodies",
    "General",
)
SAFE_SUPER: Final = ErrorCode(
    "safe-super", "Warn about calls to abstract methods with empty/trivial bodies", "General"
)

# These error codes aren't enabled by default.
NO_UNTYPED_DEF: Final[ErrorCode] = ErrorCode(
    "no-untyped-def", "Check that every function has an annotation", "General"
)
NO_UNTYPED_CALL: Final = ErrorCode(
    "no-untyped-call",
    "Disallow calling functions without type annotations from annotated functions",
    "General",
)
REDUNDANT_CAST: Final = ErrorCode(
    "redundant-cast", "Check that cast changes type of expression", "General"
)
ASSERT_TYPE: Final = ErrorCode("assert-type", "Check that assert_type() call succeeds", "General")
COMPARISON_OVERLAP: Final = ErrorCode(
    "comparison-overlap", "Check that types in comparisons and 'in' expressions overlap", "General"
)
NO_ANY_UNIMPORTED: Final = ErrorCode(
    "no-any-unimported", 'Reject "Any" types from unfollowed imports', "General"
)
NO_ANY_RETURN: Final = ErrorCode(
    "no-any-return",
    'Reject returning value with "Any" type if return type is not "Any"',
    "General",
)
UNREACHABLE: Final = ErrorCode(
    "unreachable", "Warn about unreachable statements or expressions", "General"
)
PARTIALLY_DEFINED: Final[ErrorCode] = ErrorCode(
    "partially-defined",
    "Warn about variables that are defined only in some execution paths",
    "General",
    default_enabled=False,
)
REDUNDANT_EXPR: Final = ErrorCode(
    "redundant-expr", "Warn about redundant expressions", "General", default_enabled=False
)
TRUTHY_BOOL: Final[ErrorCode] = ErrorCode(
    "truthy-bool",
    "Warn about expressions that could always evaluate to true in boolean contexts",
    "General",
    default_enabled=False,
)
NAME_MATCH: Final = ErrorCode(
    "name-match", "Check that type definition has consistent naming", "General"
)
NO_OVERLOAD_IMPL: Final = ErrorCode(
    "no-overload-impl",
    "Check that overloaded functions outside stub files have an implementation",
    "General",
)
IGNORE_WITHOUT_CODE: Final = ErrorCode(
    "ignore-without-code",
    "Warn about '# type: ignore' comments which do not have error codes",
    "General",
    default_enabled=False,
)
UNUSED_AWAITABLE: Final = ErrorCode(
    "unused-awaitable",
    "Ensure that all awaitable values are used",
    "General",
    default_enabled=False,
)


# Syntax errors are often blocking.
SYNTAX: Final = ErrorCode("syntax", "Report syntax errors", "General")

# This is an internal marker code for a whole-file ignore. It is not intended to
# be user-visible.
FILE: Final = ErrorCode("file", "Internal marker for a whole file being ignored", "General")
del error_codes[FILE.code]

# This is a catch-all for remaining uncategorized errors.
MISC: Final = ErrorCode("misc", "Miscellaneous other checks", "General")
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1075">class ErrorCode:
    @others
</t>
<t tx="ekr.20221004064034.1076">def __init__(
    self, code: str, description: str, category: str, default_enabled: bool = True
) -&gt; None:
    self.code = code
    self.description = description
    self.category = category
    self.default_enabled = default_enabled
    error_codes[code] = self

</t>
<t tx="ekr.20221004064034.1077">def __str__(self) -&gt; str:
    return f"&lt;ErrorCode {self.code}&gt;"


</t>
<t tx="ekr.20221004064034.1078">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import os.path
import sys
import traceback
from collections import defaultdict
from typing import Callable, NoReturn, Optional, TextIO, Tuple, TypeVar
from typing_extensions import Final, Literal, TypeAlias as _TypeAlias

from mypy import errorcodes as codes
from mypy.errorcodes import IMPORT, ErrorCode
from mypy.message_registry import ErrorMessage
from mypy.options import Options
from mypy.scope import Scope
from mypy.util import DEFAULT_SOURCE_OFFSET, is_typeshed_file
from mypy.version import __version__ as mypy_version

T = TypeVar("T")

allowed_duplicates: Final = ["@overload", "Got:", "Expected:"]

# Keep track of the original error code when the error code of a message is changed.
# This is used to give notes about out-of-date "type: ignore" comments.
original_error_codes: Final = {codes.LITERAL_REQ: codes.MISC}


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1079">class ErrorInfo:
    """Representation of a single error message."""

    # Description of a sequence of imports that refer to the source file
    # related to this error. Each item is a (path, line number) tuple.
    import_ctx: list[tuple[str, int]]

    # The path to source file that was the source of this error.
    file = ""

    # The fully-qualified id of the source module for this error.
    module: str | None = None

    # The name of the type in which this error is located at.
    type: str | None = ""  # Unqualified, may be None

    # The name of the function or member in which this error is located at.
    function_or_member: str | None = ""  # Unqualified, may be None

    # The line number related to this error within file.
    line = 0  # -1 if unknown

    # The column number related to this error with file.
    column = 0  # -1 if unknown

    # The end line number related to this error within file.
    end_line = 0  # -1 if unknown

    # The end column number related to this error with file.
    end_column = 0  # -1 if unknown

    # Either 'error' or 'note'
    severity = ""

    # The error message.
    message = ""

    # The error code.
    code: ErrorCode | None = None

    # If True, we should halt build after the file that generated this error.
    blocker = False

    # Only report this particular messages once per program.
    only_once = False

    # Do not remove duplicate copies of this message (ignored if only_once is True).
    allow_dups = False

    # Actual origin of the error message as tuple (path, line number, end line number)
    # If end line number is unknown, use line number.
    origin: tuple[str, int, int]

    # Fine-grained incremental target where this was reported
    target: str | None = None

    # If True, don't show this message in output, but still record the error (needed
    # by mypy daemon)
    hidden = False

    @others
</t>
<t tx="ekr.20221004064034.108">def set_expected(
    commits: list[tuple[str, str]],
    cache: JsonDict,
    temp_repo_path: str,
    target_file_path: str | None,
    mypy_cache_path: str,
    mypy_script: str | None,
) -&gt; None:
    """Populates the given `cache` with the expected results for all of the given `commits`.

    This function runs mypy on the `target_file_path` inside the `temp_repo_path`, and stores
    the result in the `cache`.

    If `cache` already contains results for a particular commit, this function will
    skip evaluating that commit and move on to the next."""
    for commit_id, message in commits:
        if commit_id in cache:
            print(f'Skipping commit (already cached): {commit_id}: "{message}"')
        else:
            print(f'Caching expected output for commit {commit_id}: "{message}"')
            execute(["git", "-C", temp_repo_path, "checkout", commit_id])
            runtime, output, stats = run_mypy(
                target_file_path, mypy_cache_path, mypy_script, incremental=False
            )
            cache[commit_id] = {"runtime": runtime, "output": output}
            if output == "":
                print(f"    Clean output ({runtime:.3f} sec)")
            else:
                print(f"    Output ({runtime:.3f} sec)")
                print_offset(output, 8)
    print()


</t>
<t tx="ekr.20221004064034.1080">def __init__(
    self,
    import_ctx: list[tuple[str, int]],
    file: str,
    module: str | None,
    typ: str | None,
    function_or_member: str | None,
    line: int,
    column: int,
    end_line: int,
    end_column: int,
    severity: str,
    message: str,
    code: ErrorCode | None,
    blocker: bool,
    only_once: bool,
    allow_dups: bool,
    origin: tuple[str, int, int] | None = None,
    target: str | None = None,
) -&gt; None:
    self.import_ctx = import_ctx
    self.file = file
    self.module = module
    self.type = typ
    self.function_or_member = function_or_member
    self.line = line
    self.column = column
    self.end_line = end_line
    self.end_column = end_column
    self.severity = severity
    self.message = message
    self.code = code
    self.blocker = blocker
    self.only_once = only_once
    self.allow_dups = allow_dups
    self.origin = origin or (file, line, line)
    self.target = target


</t>
<t tx="ekr.20221004064034.1081"># Type used internally to represent errors:
#   (path, line, column, end_line, end_column, severity, message, allow_dups, code)
ErrorTuple: _TypeAlias = Tuple[
    Optional[str], int, int, int, int, str, str, bool, Optional[ErrorCode]
]


</t>
<t tx="ekr.20221004064034.1082">class ErrorWatcher:
    """Context manager that can be used to keep track of new errors recorded
    around a given operation.

    Errors maintain a stack of such watchers. The handler is called starting
    at the top of the stack, and is propagated down the stack unless filtered
    out by one of the ErrorWatcher instances.
    """

    @others
</t>
<t tx="ekr.20221004064034.1083">def __init__(
    self,
    errors: Errors,
    *,
    filter_errors: bool | Callable[[str, ErrorInfo], bool] = False,
    save_filtered_errors: bool = False,
):
    self.errors = errors
    self._has_new_errors = False
    self._filter = filter_errors
    self._filtered: list[ErrorInfo] | None = [] if save_filtered_errors else None

</t>
<t tx="ekr.20221004064034.1084">def __enter__(self) -&gt; ErrorWatcher:
    self.errors._watchers.append(self)
    return self

</t>
<t tx="ekr.20221004064034.1085">def __exit__(self, exc_type: object, exc_val: object, exc_tb: object) -&gt; Literal[False]:
    last = self.errors._watchers.pop()
    assert last == self
    return False

</t>
<t tx="ekr.20221004064034.1086">def on_error(self, file: str, info: ErrorInfo) -&gt; bool:
    """Handler called when a new error is recorded.

    The default implementation just sets the has_new_errors flag

    Return True to filter out the error, preventing it from being seen by other
    ErrorWatcher further down the stack and from being recorded by Errors
    """
    self._has_new_errors = True
    if isinstance(self._filter, bool):
        should_filter = self._filter
    elif callable(self._filter):
        should_filter = self._filter(file, info)
    else:
        raise AssertionError(f"invalid error filter: {type(self._filter)}")
    if should_filter and self._filtered is not None:
        self._filtered.append(info)

    return should_filter

</t>
<t tx="ekr.20221004064034.1087">def has_new_errors(self) -&gt; bool:
    return self._has_new_errors

</t>
<t tx="ekr.20221004064034.1088">def filtered_errors(self) -&gt; list[ErrorInfo]:
    assert self._filtered is not None
    return self._filtered


</t>
<t tx="ekr.20221004064034.1089">class Errors:
    """Container for compile errors.

    This class generates and keeps tracks of compile errors and the
    current error context (nested imports).
    """

    # Map from files to generated error messages. Is an OrderedDict so
    # that it can be used to order messages based on the order the
    # files were processed.
    error_info_map: dict[str, list[ErrorInfo]]

    # optimization for legacy codebases with many files with errors
    has_blockers: set[str]

    # Files that we have reported the errors for
    flushed_files: set[str]

    # Current error context: nested import context/stack, as a list of (path, line) pairs.
    import_ctx: list[tuple[str, int]]

    # Path name prefix that is removed from all paths, if set.
    ignore_prefix: str | None = None

    # Path to current file.
    file: str = ""

    # Ignore some errors on these lines of each file
    # (path -&gt; line -&gt; error-codes)
    ignored_lines: dict[str, dict[int, list[str]]]

    # Lines on which an error was actually ignored.
    used_ignored_lines: dict[str, dict[int, list[str]]]

    # Files where all errors should be ignored.
    ignored_files: set[str]

    # Collection of reported only_once messages.
    only_once_messages: set[str]

    # Set to True to show "In function "foo":" messages.
    show_error_context: bool = False

    # Set to True to show column numbers in error messages.
    show_column_numbers: bool = False

    # Set to True to show end line and end column in error messages.
    # Ths implies `show_column_numbers`.
    show_error_end: bool = False

    # Set to True to show absolute file paths in error messages.
    show_absolute_path: bool = False

    # State for keeping track of the current fine-grained incremental mode target.
    # (See mypy.server.update for more about targets.)
    # Current module id.
    target_module: str | None = None
    scope: Scope | None = None

    # Have we seen an import-related error so far? If yes, we filter out other messages
    # in some cases to avoid reporting huge numbers of errors.
    seen_import_error = False

    _watchers: list[ErrorWatcher] = []

    @others
</t>
<t tx="ekr.20221004064034.109">def test_incremental(
    commits: list[tuple[str, str]],
    cache: JsonDict,
    temp_repo_path: str,
    target_file_path: str | None,
    mypy_cache_path: str,
    *,
    mypy_script: str | None = None,
    daemon: bool = False,
    exit_on_error: bool = False,
) -&gt; None:
    """Runs incremental mode on all `commits` to verify the output matches the expected output.

    This function runs mypy on the `target_file_path` inside the `temp_repo_path`. The
    expected output must be stored inside of the given `cache`.
    """
    print("Note: first commit is evaluated twice to warm up cache")
    commits = [commits[0]] + commits
    overall_stats: dict[str, float] = {}
    for commit_id, message in commits:
        print(f'Now testing commit {commit_id}: "{message}"')
        execute(["git", "-C", temp_repo_path, "checkout", commit_id])
        runtime, output, stats = run_mypy(
            target_file_path, mypy_cache_path, mypy_script, incremental=True, daemon=daemon
        )
        relevant_stats = combine_stats(overall_stats, stats)
        expected_runtime: float = cache[commit_id]["runtime"]
        expected_output: str = cache[commit_id]["output"]
        if output != expected_output:
            print("    Output does not match expected result!")
            print(f"    Expected output ({expected_runtime:.3f} sec):")
            print_offset(expected_output, 8)
            print(f"    Actual output: ({runtime:.3f} sec):")
            print_offset(output, 8)
            if exit_on_error:
                break
        else:
            print("    Output matches expected result!")
            print(f"    Incremental: {runtime:.3f} sec")
            print(f"    Original:    {expected_runtime:.3f} sec")
            if relevant_stats:
                print(f"    Stats:       {relevant_stats}")
    if overall_stats:
        print("Overall stats:", overall_stats)


</t>
<t tx="ekr.20221004064034.1090">def __init__(
    self,
    show_error_context: bool = False,
    show_column_numbers: bool = False,
    hide_error_codes: bool = False,
    pretty: bool = False,
    show_error_end: bool = False,
    read_source: Callable[[str], list[str] | None] | None = None,
    show_absolute_path: bool = False,
    many_errors_threshold: int = -1,
    options: Options | None = None,
) -&gt; None:
    self.show_error_context = show_error_context
    self.show_column_numbers = show_column_numbers
    self.hide_error_codes = hide_error_codes
    self.show_absolute_path = show_absolute_path
    self.pretty = pretty
    self.show_error_end = show_error_end
    if show_error_end:
        assert show_column_numbers, "Inconsistent formatting, must be prevented by argparse"
    # We use fscache to read source code when showing snippets.
    self.read_source = read_source
    self.many_errors_threshold = many_errors_threshold
    self.options = options
    self.initialize()

</t>
<t tx="ekr.20221004064034.1091">def initialize(self) -&gt; None:
    self.error_info_map = {}
    self.flushed_files = set()
    self.import_ctx = []
    self.function_or_member = [None]
    self.ignored_lines = {}
    self.used_ignored_lines = defaultdict(lambda: defaultdict(list))
    self.ignored_files = set()
    self.only_once_messages = set()
    self.has_blockers = set()
    self.scope = None
    self.target_module = None
    self.seen_import_error = False

</t>
<t tx="ekr.20221004064034.1092">def reset(self) -&gt; None:
    self.initialize()

</t>
<t tx="ekr.20221004064034.1093">def set_ignore_prefix(self, prefix: str) -&gt; None:
    """Set path prefix that will be removed from all paths."""
    prefix = os.path.normpath(prefix)
    # Add separator to the end, if not given.
    if os.path.basename(prefix) != "":
        prefix += os.sep
    self.ignore_prefix = prefix

</t>
<t tx="ekr.20221004064034.1094">def simplify_path(self, file: str) -&gt; str:
    if self.show_absolute_path:
        return os.path.abspath(file)
    else:
        file = os.path.normpath(file)
        return remove_path_prefix(file, self.ignore_prefix)

</t>
<t tx="ekr.20221004064034.1095">def set_file(
    self, file: str, module: str | None, options: Options, scope: Scope | None = None
) -&gt; None:
    """Set the path and module id of the current file."""
    # The path will be simplified later, in render_messages. That way
    #  * 'file' is always a key that uniquely identifies a source file
    #    that mypy read (simplified paths might not be unique); and
    #  * we only have to simplify in one place, while still supporting
    #    reporting errors for files other than the one currently being
    #    processed.
    self.file = file
    self.target_module = module
    self.scope = scope
    self.options = options

</t>
<t tx="ekr.20221004064034.1096">def set_file_ignored_lines(
    self, file: str, ignored_lines: dict[int, list[str]], ignore_all: bool = False
) -&gt; None:
    self.ignored_lines[file] = ignored_lines
    if ignore_all:
        self.ignored_files.add(file)

</t>
<t tx="ekr.20221004064034.1097">def current_target(self) -&gt; str | None:
    """Retrieves the current target from the associated scope.

    If there is no associated scope, use the target module."""
    if self.scope is not None:
        return self.scope.current_target()
    return self.target_module

</t>
<t tx="ekr.20221004064034.1098">def current_module(self) -&gt; str | None:
    return self.target_module

</t>
<t tx="ekr.20221004064034.1099">def import_context(self) -&gt; list[tuple[str, int]]:
    """Return a copy of the import context."""
    return self.import_ctx[:]

</t>
<t tx="ekr.20221004064034.11">@path C:/Repos/ekr-mypy2/
#!/usr/bin/env python3

from __future__ import annotations

import subprocess
from subprocess import Popen
from sys import argv, executable, exit

# Slow test suites
CMDLINE = "PythonCmdline"
PEP561 = "PEP561Suite"
EVALUATION = "PythonEvaluation"
DAEMON = "testdaemon"
STUBGEN_CMD = "StubgenCmdLine"
STUBGEN_PY = "StubgenPythonSuite"
MYPYC_RUN = "TestRun"
MYPYC_RUN_MULTI = "TestRunMultiFile"
MYPYC_EXTERNAL = "TestExternal"
MYPYC_COMMAND_LINE = "TestCommandLine"
ERROR_STREAM = "ErrorStreamSuite"


ALL_NON_FAST = [
    CMDLINE,
    PEP561,
    EVALUATION,
    DAEMON,
    STUBGEN_CMD,
    STUBGEN_PY,
    MYPYC_RUN,
    MYPYC_RUN_MULTI,
    MYPYC_EXTERNAL,
    MYPYC_COMMAND_LINE,
    ERROR_STREAM,
]


# This must be enabled by explicitly including 'pytest-extra' on the command line
PYTEST_OPT_IN = [PEP561]


# These must be enabled by explicitly including 'mypyc-extra' on the command line.
MYPYC_OPT_IN = [MYPYC_RUN, MYPYC_RUN_MULTI]


# We split the pytest run into three parts to improve test
# parallelization. Each run should have tests that each take a roughly similar
# time to run.
cmds = {
    # Self type check
    "self": [executable, "-m", "mypy", "--config-file", "mypy_self_check.ini", "-p", "mypy"],
    # Lint
    "lint": ["flake8", "-j0"],
    "format-black": ["black", "."],
    "format-isort": ["isort", "."],
    # Fast test cases only (this is the bulk of the test suite)
    "pytest-fast": ["pytest", "-q", "-k", f"not ({' or '.join(ALL_NON_FAST)})"],
    # Test cases that invoke mypy (with small inputs)
    "pytest-cmdline": [
        "pytest",
        "-q",
        "-k",
        " or ".join([CMDLINE, EVALUATION, STUBGEN_CMD, STUBGEN_PY]),
    ],
    # Test cases that may take seconds to run each
    "pytest-slow": [
        "pytest",
        "-q",
        "-k",
        " or ".join([DAEMON, MYPYC_EXTERNAL, MYPYC_COMMAND_LINE, ERROR_STREAM]),
    ],
    # Test cases that might take minutes to run
    "pytest-extra": ["pytest", "-q", "-k", " or ".join(PYTEST_OPT_IN)],
    # Mypyc tests that aren't run by default, since they are slow and rarely
    # fail for commits that don't touch mypyc
    "mypyc-extra": ["pytest", "-q", "-k", " or ".join(MYPYC_OPT_IN)],
}

# Stop run immediately if these commands fail
FAST_FAIL = ["self", "lint"]

EXTRA_COMMANDS = ("pytest-extra", "mypyc-extra")
DEFAULT_COMMANDS = [cmd for cmd in cmds if cmd not in EXTRA_COMMANDS]

assert all(cmd in cmds for cmd in FAST_FAIL)


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.110">def combine_stats(overall_stats: dict[str, float], new_stats: dict[str, Any]) -&gt; dict[str, float]:
    INTERESTING_KEYS = ["build_time", "gc_time"]
    # For now, we only support float keys
    relevant_stats: dict[str, float] = {}
    for key in INTERESTING_KEYS:
        if key in new_stats:
            value = float(new_stats[key])
            relevant_stats[key] = value
            overall_stats[key] = overall_stats.get(key, 0.0) + value
    return relevant_stats


</t>
<t tx="ekr.20221004064034.1100">def set_import_context(self, ctx: list[tuple[str, int]]) -&gt; None:
    """Replace the entire import context with a new value."""
    self.import_ctx = ctx[:]

</t>
<t tx="ekr.20221004064034.1101">def report(
    self,
    line: int,
    column: int | None,
    message: str,
    code: ErrorCode | None = None,
    *,
    blocker: bool = False,
    severity: str = "error",
    file: str | None = None,
    only_once: bool = False,
    allow_dups: bool = False,
    origin_span: tuple[int, int] | None = None,
    offset: int = 0,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    """Report message at the given line using the current error context.

    Args:
        line: line number of error
        column: column number of error
        message: message to report
        code: error code (defaults to 'misc'; not shown for notes)
        blocker: if True, don't continue analysis after this error
        severity: 'error' or 'note'
        file: if non-None, override current file as context
        only_once: if True, only report this exact message once per build
        allow_dups: if True, allow duplicate copies of this message (ignored if only_once)
        origin_span: if non-None, override current context as origin
                     (type: ignores have effect here)
        end_line: if non-None, override current context as end
    """
    if self.scope:
        type = self.scope.current_type_name()
        if self.scope.ignored &gt; 0:
            type = None  # Omit type context if nested function
        function = self.scope.current_function_name()
    else:
        type = None
        function = None

    if column is None:
        column = -1
    if end_column is None:
        if column == -1:
            end_column = -1
        else:
            end_column = column + 1

    if file is None:
        file = self.file
    if offset:
        message = " " * offset + message

    if origin_span is None:
        origin_span = (line, line)

    if end_line is None:
        end_line = line

    code = code or (codes.MISC if not blocker else None)

    info = ErrorInfo(
        self.import_context(),
        file,
        self.current_module(),
        type,
        function,
        line,
        column,
        end_line,
        end_column,
        severity,
        message,
        code,
        blocker,
        only_once,
        allow_dups,
        origin=(self.file, *origin_span),
        target=self.current_target(),
    )
    self.add_error_info(info)

</t>
<t tx="ekr.20221004064034.1102">def _add_error_info(self, file: str, info: ErrorInfo) -&gt; None:
    assert file not in self.flushed_files
    # process the stack of ErrorWatchers before modifying any internal state
    # in case we need to filter out the error entirely
    if self._filter_error(file, info):
        return
    if file not in self.error_info_map:
        self.error_info_map[file] = []
    self.error_info_map[file].append(info)
    if info.blocker:
        self.has_blockers.add(file)
    if info.code is IMPORT:
        self.seen_import_error = True

</t>
<t tx="ekr.20221004064034.1103">def _filter_error(self, file: str, info: ErrorInfo) -&gt; bool:
    """
    process ErrorWatcher stack from top to bottom,
    stopping early if error needs to be filtered out
    """
    i = len(self._watchers)
    while i &gt; 0:
        i -= 1
        w = self._watchers[i]
        if w.on_error(file, info):
            return True
    return False

</t>
<t tx="ekr.20221004064034.1104">def add_error_info(self, info: ErrorInfo) -&gt; None:
    file, line, end_line = info.origin
    # process the stack of ErrorWatchers before modifying any internal state
    # in case we need to filter out the error entirely
    # NB: we need to do this both here and in _add_error_info, otherwise we
    # might incorrectly update the sets of ignored or only_once messages
    if self._filter_error(file, info):
        return
    if not info.blocker:  # Blockers cannot be ignored
        if file in self.ignored_lines:
            # Check each line in this context for "type: ignore" comments.
            # line == end_line for most nodes, so we only loop once.
            for scope_line in range(line, end_line + 1):
                if self.is_ignored_error(scope_line, info, self.ignored_lines[file]):
                    # Annotation requests us to ignore all errors on this line.
                    self.used_ignored_lines[file][scope_line].append(
                        (info.code or codes.MISC).code
                    )
                    return
        if file in self.ignored_files:
            return
    if info.only_once:
        if info.message in self.only_once_messages:
            return
        self.only_once_messages.add(info.message)
    if self.seen_import_error and info.code is not IMPORT and self.has_many_errors():
        # Missing stubs can easily cause thousands of errors about
        # Any types, especially when upgrading to mypy 0.900,
        # which no longer bundles third-party library stubs. Avoid
        # showing too many errors to make it easier to see
        # import-related errors.
        info.hidden = True
        self.report_hidden_errors(info)
    self._add_error_info(file, info)
    ignored_codes = self.ignored_lines.get(file, {}).get(info.line, [])
    if ignored_codes and info.code:
        # Something is ignored on the line, but not this error, so maybe the error
        # code is incorrect.
        msg = f'Error code "{info.code.code}" not covered by "type: ignore" comment'
        if info.code in original_error_codes:
            # If there seems to be a "type: ignore" with a stale error
            # code, report a more specific note.
            old_code = original_error_codes[info.code].code
            if old_code in ignored_codes:
                msg = (
                    f'Error code changed to {info.code.code}; "type: ignore" comment '
                    + "may be out of date"
                )
        note = ErrorInfo(
            info.import_ctx,
            info.file,
            info.module,
            info.type,
            info.function_or_member,
            info.line,
            info.column,
            info.end_line,
            info.end_column,
            "note",
            msg,
            code=None,
            blocker=False,
            only_once=False,
            allow_dups=False,
        )
        self._add_error_info(file, note)

</t>
<t tx="ekr.20221004064034.1105">def has_many_errors(self) -&gt; bool:
    if self.many_errors_threshold &lt; 0:
        return False
    if len(self.error_info_map) &gt;= self.many_errors_threshold:
        return True
    if (
        sum(len(errors) for errors in self.error_info_map.values())
        &gt;= self.many_errors_threshold
    ):
        return True
    return False

</t>
<t tx="ekr.20221004064034.1106">def report_hidden_errors(self, info: ErrorInfo) -&gt; None:
    message = (
        "(Skipping most remaining errors due to unresolved imports or missing stubs; "
        + "fix these first)"
    )
    if message in self.only_once_messages:
        return
    self.only_once_messages.add(message)
    new_info = ErrorInfo(
        import_ctx=info.import_ctx,
        file=info.file,
        module=info.module,
        typ=None,
        function_or_member=None,
        line=info.line,
        column=info.column,
        end_line=info.end_line,
        end_column=info.end_column,
        severity="note",
        message=message,
        code=None,
        blocker=False,
        only_once=True,
        allow_dups=False,
        origin=info.origin,
        target=info.target,
    )
    self._add_error_info(info.origin[0], new_info)

</t>
<t tx="ekr.20221004064034.1107">def is_ignored_error(self, line: int, info: ErrorInfo, ignores: dict[int, list[str]]) -&gt; bool:
    if info.blocker:
        # Blocking errors can never be ignored
        return False
    if info.code and not self.is_error_code_enabled(info.code):
        return True
    if line not in ignores:
        return False
    if not ignores[line]:
        # Empty list means that we ignore all errors
        return True
    if info.code and self.is_error_code_enabled(info.code):
        return info.code.code in ignores[line]
    return False

</t>
<t tx="ekr.20221004064034.1108">def is_error_code_enabled(self, error_code: ErrorCode) -&gt; bool:
    if self.options:
        current_mod_disabled = self.options.disabled_error_codes
        current_mod_enabled = self.options.enabled_error_codes
    else:
        current_mod_disabled = set()
        current_mod_enabled = set()

    if error_code in current_mod_disabled:
        return False
    elif error_code in current_mod_enabled:
        return True
    else:
        return error_code.default_enabled

</t>
<t tx="ekr.20221004064034.1109">def clear_errors_in_targets(self, path: str, targets: set[str]) -&gt; None:
    """Remove errors in specific fine-grained targets within a file."""
    if path in self.error_info_map:
        new_errors = []
        has_blocker = False
        for info in self.error_info_map[path]:
            if info.target not in targets:
                new_errors.append(info)
                has_blocker |= info.blocker
            elif info.only_once:
                self.only_once_messages.remove(info.message)
        self.error_info_map[path] = new_errors
        if not has_blocker and path in self.has_blockers:
            self.has_blockers.remove(path)

</t>
<t tx="ekr.20221004064034.111">def cleanup(temp_repo_path: str, mypy_cache_path: str) -&gt; None:
    delete_folder(temp_repo_path)
    delete_folder(mypy_cache_path)


</t>
<t tx="ekr.20221004064034.1110">def generate_unused_ignore_errors(self, file: str) -&gt; None:
    if (
        is_typeshed_file(self.options.abs_custom_typeshed_dir if self.options else None, file)
        or file in self.ignored_files
    ):
        return
    ignored_lines = self.ignored_lines[file]
    used_ignored_lines = self.used_ignored_lines[file]
    for line, ignored_codes in ignored_lines.items():
        used_ignored_codes = used_ignored_lines[line]
        unused_ignored_codes = set(ignored_codes) - set(used_ignored_codes)
        # `ignore` is used
        if len(ignored_codes) == 0 and len(used_ignored_codes) &gt; 0:
            continue
        # All codes appearing in `ignore[...]` are used
        if len(ignored_codes) &gt; 0 and len(unused_ignored_codes) == 0:
            continue
        # Display detail only when `ignore[...]` specifies more than one error code
        unused_codes_message = ""
        if len(ignored_codes) &gt; 1 and len(unused_ignored_codes) &gt; 0:
            unused_codes_message = f"[{', '.join(sorted(unused_ignored_codes))}]"
        message = f'Unused "type: ignore{unused_codes_message}" comment'
        # Don't use report since add_error_info will ignore the error!
        info = ErrorInfo(
            self.import_context(),
            file,
            self.current_module(),
            None,
            None,
            line,
            -1,
            line,
            -1,
            "error",
            message,
            None,
            False,
            False,
            False,
        )
        self._add_error_info(file, info)

</t>
<t tx="ekr.20221004064034.1111">def generate_ignore_without_code_errors(
    self, file: str, is_warning_unused_ignores: bool
) -&gt; None:
    if (
        is_typeshed_file(self.options.abs_custom_typeshed_dir if self.options else None, file)
        or file in self.ignored_files
    ):
        return

    used_ignored_lines = self.used_ignored_lines[file]

    # If the whole file is ignored, ignore it.
    if used_ignored_lines:
        _, used_codes = min(used_ignored_lines.items())
        if codes.FILE.code in used_codes:
            return

    for line, ignored_codes in self.ignored_lines[file].items():
        if ignored_codes:
            continue

        # If the ignore is itself unused and that would be warned about, let
        # that error stand alone
        if is_warning_unused_ignores and not used_ignored_lines[line]:
            continue

        codes_hint = ""
        ignored_codes = sorted(set(used_ignored_lines[line]))
        if ignored_codes:
            codes_hint = f' (consider "type: ignore[{", ".join(ignored_codes)}]" instead)'

        message = f'"type: ignore" comment without error code{codes_hint}'
        # Don't use report since add_error_info will ignore the error!
        info = ErrorInfo(
            self.import_context(),
            file,
            self.current_module(),
            None,
            None,
            line,
            -1,
            line,
            -1,
            "error",
            message,
            codes.IGNORE_WITHOUT_CODE,
            False,
            False,
            False,
        )
        self._add_error_info(file, info)

</t>
<t tx="ekr.20221004064034.1112">def num_messages(self) -&gt; int:
    """Return the number of generated messages."""
    return sum(len(x) for x in self.error_info_map.values())

</t>
<t tx="ekr.20221004064034.1113">def is_errors(self) -&gt; bool:
    """Are there any generated messages?"""
    return bool(self.error_info_map)

</t>
<t tx="ekr.20221004064034.1114">def is_blockers(self) -&gt; bool:
    """Are the any errors that are blockers?"""
    return bool(self.has_blockers)

</t>
<t tx="ekr.20221004064034.1115">def blocker_module(self) -&gt; str | None:
    """Return the module with a blocking error, or None if not possible."""
    for path in self.has_blockers:
        for err in self.error_info_map[path]:
            if err.blocker:
                return err.module
    return None

</t>
<t tx="ekr.20221004064034.1116">def is_errors_for_file(self, file: str) -&gt; bool:
    """Are there any errors for the given file?"""
    return file in self.error_info_map

</t>
<t tx="ekr.20221004064034.1117">def raise_error(self, use_stdout: bool = True) -&gt; NoReturn:
    """Raise a CompileError with the generated messages.

    Render the messages suitable for displaying.
    """
    # self.new_messages() will format all messages that haven't already
    # been returned from a file_messages() call.
    raise CompileError(
        self.new_messages(), use_stdout=use_stdout, module_with_blocker=self.blocker_module()
    )

</t>
<t tx="ekr.20221004064034.1118">def format_messages(
    self, error_info: list[ErrorInfo], source_lines: list[str] | None
) -&gt; list[str]:
    """Return a string list that represents the error messages.

    Use a form suitable for displaying to the user. If self.pretty
    is True also append a relevant trimmed source code line (only for
    severity 'error').
    """
    a: list[str] = []
    error_info = [info for info in error_info if not info.hidden]
    errors = self.render_messages(self.sort_messages(error_info))
    errors = self.remove_duplicates(errors)
    for (
        file,
        line,
        column,
        end_line,
        end_column,
        severity,
        message,
        allow_dups,
        code,
    ) in errors:
        s = ""
        if file is not None:
            if self.show_column_numbers and line &gt;= 0 and column &gt;= 0:
                srcloc = f"{file}:{line}:{1 + column}"
                if self.show_error_end and end_line &gt;= 0 and end_column &gt;= 0:
                    srcloc += f":{end_line}:{end_column}"
            elif line &gt;= 0:
                srcloc = f"{file}:{line}"
            else:
                srcloc = file
            s = f"{srcloc}: {severity}: {message}"
        else:
            s = message
        if not self.hide_error_codes and code and severity != "note":
            # If note has an error code, it is related to a previous error. Avoid
            # displaying duplicate error codes.
            s = f"{s}  [{code.code}]"
        a.append(s)
        if self.pretty:
            # Add source code fragment and a location marker.
            if severity == "error" and source_lines and line &gt; 0:
                source_line = source_lines[line - 1]
                source_line_expanded = source_line.expandtabs()
                if column &lt; 0:
                    # Something went wrong, take first non-empty column.
                    column = len(source_line) - len(source_line.lstrip())

                # Shifts column after tab expansion
                column = len(source_line[:column].expandtabs())
                end_column = len(source_line[:end_column].expandtabs())

                # Note, currently coloring uses the offset to detect source snippets,
                # so these offsets should not be arbitrary.
                a.append(" " * DEFAULT_SOURCE_OFFSET + source_line_expanded)
                marker = "^"
                if end_line == line and end_column &gt; column:
                    marker = f'^{"~" * (end_column - column - 1)}'
                a.append(" " * (DEFAULT_SOURCE_OFFSET + column) + marker)
    return a

</t>
<t tx="ekr.20221004064034.1119">def file_messages(self, path: str) -&gt; list[str]:
    """Return a string list of new error messages from a given file.

    Use a form suitable for displaying to the user.
    """
    if path not in self.error_info_map:
        return []
    self.flushed_files.add(path)
    source_lines = None
    if self.pretty:
        assert self.read_source
        source_lines = self.read_source(path)
    return self.format_messages(self.error_info_map[path], source_lines)

</t>
<t tx="ekr.20221004064034.112">def test_repo(
    target_repo_url: str,
    temp_repo_path: str,
    target_file_path: str | None,
    mypy_path: str,
    incremental_cache_path: str,
    mypy_cache_path: str,
    range_type: str,
    range_start: str,
    branch: str,
    params: Namespace,
) -&gt; None:
    """Tests incremental mode against the repo specified in `target_repo_url`.

    This algorithm runs in five main stages:

    1.  Clones `target_repo_url` into the `temp_repo_path` folder locally,
        checking out the specified `branch` if applicable.
    2.  Examines the repo's history to get the list of all commits to
        to test incremental mode on.
    3.  Runs mypy WITHOUT incremental mode against the `target_file_path` (which is
        assumed to be located inside the `temp_repo_path`), testing each commit
        discovered in stage two.
        -   If the results of running mypy WITHOUT incremental mode on a
            particular commit are already cached inside the `incremental_cache_path`,
            skip that commit to save time.
        -   Cache the results after finishing.
    4.  Rewind back to the first commit, and run mypy WITH incremental mode
        against the `target_file_path` commit-by-commit, and compare to the expected
        results found in stage 3.
    5.  Delete all unnecessary temp files.
    """
    # Stage 1: Clone repo and get ready to being testing
    ensure_environment_is_ready(mypy_path, temp_repo_path, mypy_cache_path)
    initialize_repo(target_repo_url, temp_repo_path, branch)

    # Stage 2: Get all commits we want to test
    if range_type == "last":
        start_commit = get_nth_commit(temp_repo_path, int(range_start))[0]
    elif range_type == "commit":
        start_commit = range_start
    else:
        raise RuntimeError(f"Invalid option: {range_type}")
    commits = get_commits_starting_at(temp_repo_path, start_commit)
    if params.limit:
        commits = commits[: params.limit]
    if params.sample:
        seed = params.seed or base64.urlsafe_b64encode(os.urandom(15)).decode("ascii")
        random.seed(seed)
        commits = random.sample(commits, params.sample)
        print("Sampled down to %d commits using random seed %s" % (len(commits), seed))

    # Stage 3: Find and cache expected results for each commit (without incremental mode)
    cache = load_cache(incremental_cache_path)
    set_expected(
        commits,
        cache,
        temp_repo_path,
        target_file_path,
        mypy_cache_path,
        mypy_script=params.mypy_script,
    )
    save_cache(cache, incremental_cache_path)

    # Stage 4: Rewind and re-run mypy (with incremental mode enabled)
    if params.daemon:
        print("Starting daemon")
        start_daemon(mypy_cache_path)
    test_incremental(
        commits,
        cache,
        temp_repo_path,
        target_file_path,
        mypy_cache_path,
        mypy_script=params.mypy_script,
        daemon=params.daemon,
        exit_on_error=params.exit_on_error,
    )

    # Stage 5: Remove temp files, stop daemon
    if not params.keep_temporary_files:
        cleanup(temp_repo_path, mypy_cache_path)
    if params.daemon:
        print("Stopping daemon")
        stop_daemon()


</t>
<t tx="ekr.20221004064034.1120">def new_messages(self) -&gt; list[str]:
    """Return a string list of new error messages.

    Use a form suitable for displaying to the user.
    Errors from different files are ordered based on the order in which
    they first generated an error.
    """
    msgs = []
    for path in self.error_info_map.keys():
        if path not in self.flushed_files:
            msgs.extend(self.file_messages(path))
    return msgs

</t>
<t tx="ekr.20221004064034.1121">def targets(self) -&gt; set[str]:
    """Return a set of all targets that contain errors."""
    # TODO: Make sure that either target is always defined or that not being defined
    #       is okay for fine-grained incremental checking.
    return {
        info.target for errs in self.error_info_map.values() for info in errs if info.target
    }

</t>
<t tx="ekr.20221004064034.1122">def render_messages(self, errors: list[ErrorInfo]) -&gt; list[ErrorTuple]:
    """Translate the messages into a sequence of tuples.

    Each tuple is of form (path, line, col, severity, message, allow_dups, code).
    The rendered sequence includes information about error contexts.
    The path item may be None. If the line item is negative, the
    line number is not defined for the tuple.
    """
    result: list[ErrorTuple] = []
    prev_import_context: list[tuple[str, int]] = []
    prev_function_or_member: str | None = None
    prev_type: str | None = None

    for e in errors:
        # Report module import context, if different from previous message.
        if not self.show_error_context:
            pass
        elif e.import_ctx != prev_import_context:
            last = len(e.import_ctx) - 1
            i = last
            while i &gt;= 0:
                path, line = e.import_ctx[i]
                fmt = "{}:{}: note: In module imported here"
                if i &lt; last:
                    fmt = "{}:{}: note: ... from here"
                if i &gt; 0:
                    fmt += ","
                else:
                    fmt += ":"
                # Remove prefix to ignore from path (if present) to
                # simplify path.
                path = remove_path_prefix(path, self.ignore_prefix)
                result.append(
                    (None, -1, -1, -1, -1, "note", fmt.format(path, line), e.allow_dups, None)
                )
                i -= 1

        file = self.simplify_path(e.file)

        # Report context within a source file.
        if not self.show_error_context:
            pass
        elif e.function_or_member != prev_function_or_member or e.type != prev_type:
            if e.function_or_member is None:
                if e.type is None:
                    result.append(
                        (file, -1, -1, -1, -1, "note", "At top level:", e.allow_dups, None)
                    )
                else:
                    result.append(
                        (
                            file,
                            -1,
                            -1,
                            -1,
                            -1,
                            "note",
                            f'In class "{e.type}":',
                            e.allow_dups,
                            None,
                        )
                    )
            else:
                if e.type is None:
                    result.append(
                        (
                            file,
                            -1,
                            -1,
                            -1,
                            -1,
                            "note",
                            f'In function "{e.function_or_member}":',
                            e.allow_dups,
                            None,
                        )
                    )
                else:
                    result.append(
                        (
                            file,
                            -1,
                            -1,
                            -1,
                            -1,
                            "note",
                            'In member "{}" of class "{}":'.format(
                                e.function_or_member, e.type
                            ),
                            e.allow_dups,
                            None,
                        )
                    )
        elif e.type != prev_type:
            if e.type is None:
                result.append(
                    (file, -1, -1, -1, -1, "note", "At top level:", e.allow_dups, None)
                )
            else:
                result.append(
                    (file, -1, -1, -1, -1, "note", f'In class "{e.type}":', e.allow_dups, None)
                )

        if isinstance(e.message, ErrorMessage):
            result.append(
                (
                    file,
                    e.line,
                    e.column,
                    e.end_line,
                    e.end_column,
                    e.severity,
                    e.message.value,
                    e.allow_dups,
                    e.code,
                )
            )
        else:
            result.append(
                (
                    file,
                    e.line,
                    e.column,
                    e.end_line,
                    e.end_column,
                    e.severity,
                    e.message,
                    e.allow_dups,
                    e.code,
                )
            )

        prev_import_context = e.import_ctx
        prev_function_or_member = e.function_or_member
        prev_type = e.type

    return result

</t>
<t tx="ekr.20221004064034.1123">def sort_messages(self, errors: list[ErrorInfo]) -&gt; list[ErrorInfo]:
    """Sort an array of error messages locally by line number.

    I.e., sort a run of consecutive messages with the same
    context by line number, but otherwise retain the general
    ordering of the messages.
    """
    result: list[ErrorInfo] = []
    i = 0
    while i &lt; len(errors):
        i0 = i
        # Find neighbouring errors with the same context and file.
        while (
            i + 1 &lt; len(errors)
            and errors[i + 1].import_ctx == errors[i].import_ctx
            and errors[i + 1].file == errors[i].file
        ):
            i += 1
        i += 1

        # Sort the errors specific to a file according to line number and column.
        a = sorted(errors[i0:i], key=lambda x: (x.line, x.column))
        result.extend(a)
    return result

</t>
<t tx="ekr.20221004064034.1124">def remove_duplicates(self, errors: list[ErrorTuple]) -&gt; list[ErrorTuple]:
    """Remove duplicates from a sorted error list."""
    res: list[ErrorTuple] = []
    i = 0
    while i &lt; len(errors):
        dup = False
        # Use slightly special formatting for member conflicts reporting.
        conflicts_notes = False
        j = i - 1
        # Find duplicates, unless duplicates are allowed.
        if not errors[i][7]:
            while j &gt;= 0 and errors[j][0] == errors[i][0]:
                if errors[j][6].strip() == "Got:":
                    conflicts_notes = True
                j -= 1
            j = i - 1
            while j &gt;= 0 and errors[j][0] == errors[i][0] and errors[j][1] == errors[i][1]:
                if (
                    errors[j][5] == errors[i][5]
                    and
                    # Allow duplicate notes in overload conflicts reporting.
                    not (
                        (errors[i][5] == "note" and errors[i][6].strip() in allowed_duplicates)
                        or (errors[i][6].strip().startswith("def ") and conflicts_notes)
                    )
                    and errors[j][6] == errors[i][6]
                ):  # ignore column
                    dup = True
                    break
                j -= 1
        if not dup:
            res.append(errors[i])
        i += 1
    return res


</t>
<t tx="ekr.20221004064034.1125">class CompileError(Exception):
    """Exception raised when there is a compile error.

    It can be a parse, semantic analysis, type check or other
    compilation-related error.

    CompileErrors raised from an errors object carry all of the
    messages that have not been reported out by error streaming.
    This is patched up by build.build to contain either all error
    messages (if errors were streamed) or none (if they were not).

    """

    messages: list[str]
    use_stdout = False
    # Can be set in case there was a module with a blocking error
    module_with_blocker: str | None = None

    @others
</t>
<t tx="ekr.20221004064034.1126">def __init__(
    self, messages: list[str], use_stdout: bool = False, module_with_blocker: str | None = None
) -&gt; None:
    super().__init__("\n".join(messages))
    self.messages = messages
    self.use_stdout = use_stdout
    self.module_with_blocker = module_with_blocker


</t>
<t tx="ekr.20221004064034.1127">def remove_path_prefix(path: str, prefix: str | None) -&gt; str:
    """If path starts with prefix, return copy of path with the prefix removed.
    Otherwise, return path. If path is None, return None.
    """
    if prefix is not None and path.startswith(prefix):
        return path[len(prefix) :]
    else:
        return path


</t>
<t tx="ekr.20221004064034.1128">def report_internal_error(
    err: Exception,
    file: str | None,
    line: int,
    errors: Errors,
    options: Options,
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
) -&gt; NoReturn:
    """Report internal error and exit.

    This optionally starts pdb or shows a traceback.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr
    # Dump out errors so far, they often provide a clue.
    # But catch unexpected errors rendering them.
    try:
        for msg in errors.new_messages():
            print(msg)
    except Exception as e:
        print("Failed to dump errors:", repr(e), file=stderr)

    # Compute file:line prefix for official-looking error messages.
    if file:
        if line:
            prefix = f"{file}:{line}: "
        else:
            prefix = f"{file}: "
    else:
        prefix = ""

    # Print "INTERNAL ERROR" message.
    print(
        f"{prefix}error: INTERNAL ERROR --",
        "Please try using mypy master on GitHub:\n"
        "https://mypy.readthedocs.io/en/stable/common_issues.html"
        "#using-a-development-mypy-build",
        file=stderr,
    )
    if options.show_traceback:
        print("Please report a bug at https://github.com/python/mypy/issues", file=stderr)
    else:
        print(
            "If this issue continues with mypy master, "
            "please report a bug at https://github.com/python/mypy/issues",
            file=stderr,
        )
    print(f"version: {mypy_version}", file=stderr)

    # If requested, drop into pdb. This overrides show_tb.
    if options.pdb:
        print("Dropping into pdb", file=stderr)
        import pdb

        pdb.post_mortem(sys.exc_info()[2])

    # If requested, print traceback, else print note explaining how to get one.
    if options.raise_exceptions:
        raise err
    if not options.show_traceback:
        if not options.pdb:
            print(
                "{}: note: please use --show-traceback to print a traceback "
                "when reporting a bug".format(prefix),
                file=stderr,
            )
    else:
        tb = traceback.extract_stack()[:-2]
        tb2 = traceback.extract_tb(sys.exc_info()[2])
        print("Traceback (most recent call last):")
        for s in traceback.format_list(tb + tb2):
            print(s.rstrip("\n"))
        print(f"{type(err).__name__}: {err}", file=stdout)
        print(f"{prefix}: note: use --pdb to drop into pdb", file=stderr)

    # Exit.  The caller has nothing more to say.
    # We use exit code 2 to signal that this is no ordinary error.
    raise SystemExit(2)
</t>
<t tx="ekr.20221004064034.1129">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Iterable, Mapping, Sequence, TypeVar, cast, overload

from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecFlavor,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
)
from mypy.typevartuples import split_with_instance, split_with_prefix_and_suffix


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.113">def main() -&gt; None:
    help_factory: Any = lambda prog: RawDescriptionHelpFormatter(prog=prog, max_help_position=32)
    parser = ArgumentParser(
        prog="incremental_checker", description=__doc__, formatter_class=help_factory
    )

    parser.add_argument(
        "range_type",
        metavar="START_TYPE",
        choices=["last", "commit"],
        help="must be one of 'last' or 'commit'",
    )
    parser.add_argument(
        "range_start",
        metavar="COMMIT_ID_OR_NUMBER",
        help="the commit id to start from, or the number of commits to move back (see above)",
    )
    parser.add_argument(
        "-r",
        "--repo_url",
        default=MYPY_REPO_URL,
        metavar="URL",
        help="the repo to clone and run tests on",
    )
    parser.add_argument(
        "-f",
        "--file-path",
        default=MYPY_TARGET_FILE,
        metavar="FILE",
        help="the name of the file or directory to typecheck",
    )
    parser.add_argument(
        "-x", "--exit-on-error", action="store_true", help="Exits as soon as an error occurs"
    )
    parser.add_argument(
        "--keep-temporary-files", action="store_true", help="Keep temporary files on exit"
    )
    parser.add_argument(
        "--cache-path",
        default=CACHE_PATH,
        metavar="DIR",
        help="sets a custom location to store cache data",
    )
    parser.add_argument(
        "--branch",
        default=None,
        metavar="NAME",
        help="check out and test a custom branch uses the default if not specified",
    )
    parser.add_argument("--sample", type=int, help="use a random sample of size SAMPLE")
    parser.add_argument("--seed", type=str, help="random seed")
    parser.add_argument(
        "--limit", type=int, help="maximum number of commits to use (default until end)"
    )
    parser.add_argument("--mypy-script", type=str, help="alternate mypy script to run")
    parser.add_argument(
        "--daemon",
        action="store_true",
        help="use mypy daemon instead of incremental (highly experimental)",
    )

    if len(sys.argv[1:]) == 0:
        parser.print_help()
        parser.exit()

    params = parser.parse_args(sys.argv[1:])

    # Make all paths absolute so we avoid having to worry about being in the right folder

    # The path to this specific script (incremental_checker.py).
    script_path = os.path.abspath(sys.argv[0])

    # The path to the mypy repo.
    mypy_path = os.path.abspath(os.path.dirname(os.path.dirname(script_path)))

    # The folder the cloned repo will reside in.
    temp_repo_path = os.path.abspath(os.path.join(mypy_path, "tmp_repo"))

    # The particular file or package to typecheck inside the repo.
    if params.file_path:
        target_file_path = os.path.abspath(os.path.join(temp_repo_path, params.file_path))
    else:
        # Allow `-f ''` to clear target_file_path.
        target_file_path = None

    # The path to where the incremental checker cache data is stored.
    incremental_cache_path = os.path.abspath(params.cache_path)

    # The path to store the mypy incremental mode cache data
    mypy_cache_path = os.path.abspath(os.path.join(mypy_path, "misc", ".mypy_cache"))

    print(f"Assuming mypy is located at {mypy_path}")
    print(f"Temp repo will be cloned at {temp_repo_path}")
    print(f"Testing file/dir located at {target_file_path}")
    print(f"Using cache data located at {incremental_cache_path}")
    print()

    test_repo(
        params.repo_url,
        temp_repo_path,
        target_file_path,
        mypy_path,
        incremental_cache_path,
        mypy_cache_path,
        params.range_type,
        params.range_start,
        params.branch,
        params,
    )


</t>
<t tx="ekr.20221004064034.1130">@overload
def expand_type(typ: ProperType, env: Mapping[TypeVarId, Type]) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20221004064034.1131">@overload
def expand_type(typ: Type, env: Mapping[TypeVarId, Type]) -&gt; Type:
    ...


</t>
<t tx="ekr.20221004064034.1132">def expand_type(typ: Type, env: Mapping[TypeVarId, Type]) -&gt; Type:
    """Substitute any type variable references in a type given by a type
    environment.
    """
    return typ.accept(ExpandTypeVisitor(env))


</t>
<t tx="ekr.20221004064034.1133">@overload
def expand_type_by_instance(typ: ProperType, instance: Instance) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20221004064034.1134">@overload
def expand_type_by_instance(typ: Type, instance: Instance) -&gt; Type:
    ...


</t>
<t tx="ekr.20221004064034.1135">def expand_type_by_instance(typ: Type, instance: Instance) -&gt; Type:
    """Substitute type variables in type using values from an Instance.
    Type variables are considered to be bound by the class declaration."""
    if not instance.args:
        return typ
    else:
        variables: dict[TypeVarId, Type] = {}
        if instance.type.has_type_var_tuple_type:
            assert instance.type.type_var_tuple_prefix is not None
            assert instance.type.type_var_tuple_suffix is not None

            args_prefix, args_middle, args_suffix = split_with_instance(instance)
            tvars_prefix, tvars_middle, tvars_suffix = split_with_prefix_and_suffix(
                tuple(instance.type.defn.type_vars),
                instance.type.type_var_tuple_prefix,
                instance.type.type_var_tuple_suffix,
            )
            variables = {tvars_middle[0].id: TypeList(list(args_middle))}
            instance_args = args_prefix + args_suffix
            tvars = tvars_prefix + tvars_suffix
        else:
            tvars = tuple(instance.type.defn.type_vars)
            instance_args = instance.args

        for binder, arg in zip(tvars, instance_args):
            variables[binder.id] = arg

        return expand_type(typ, variables)


</t>
<t tx="ekr.20221004064034.1136">F = TypeVar("F", bound=FunctionLike)


</t>
<t tx="ekr.20221004064034.1137">def freshen_function_type_vars(callee: F) -&gt; F:
    """Substitute fresh type variables for generic function type variables."""
    if isinstance(callee, CallableType):
        if not callee.is_generic():
            return cast(F, callee)
        tvs = []
        tvmap: dict[TypeVarId, Type] = {}
        for v in callee.variables:
            if isinstance(v, TypeVarType):
                tv: TypeVarLikeType = TypeVarType.new_unification_variable(v)
            elif isinstance(v, TypeVarTupleType):
                assert isinstance(v, TypeVarTupleType)
                tv = TypeVarTupleType.new_unification_variable(v)
            else:
                assert isinstance(v, ParamSpecType)
                tv = ParamSpecType.new_unification_variable(v)
            tvs.append(tv)
            tvmap[v.id] = tv
        fresh = cast(CallableType, expand_type(callee, tvmap)).copy_modified(variables=tvs)
        return cast(F, fresh)
    else:
        assert isinstance(callee, Overloaded)
        fresh_overload = Overloaded([freshen_function_type_vars(item) for item in callee.items])
        return cast(F, fresh_overload)


</t>
<t tx="ekr.20221004064034.1138">class ExpandTypeVisitor(TypeVisitor[Type]):
    """Visitor that substitutes type variables with values."""

    variables: Mapping[TypeVarId, Type]  # TypeVar id -&gt; TypeVar value

    @others
</t>
<t tx="ekr.20221004064034.1139">def __init__(self, variables: Mapping[TypeVarId, Type]) -&gt; None:
    self.variables = variables

</t>
<t tx="ekr.20221004064034.114">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3

from __future__ import annotations

import os
import shutil
import statistics
import subprocess
import textwrap
import time
from typing import Callable


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1140">def visit_unbound_type(self, t: UnboundType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064034.1141">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064034.1142">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064034.1143">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064034.1144">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064034.1145">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    # Should not get here.
    raise RuntimeError()

</t>
<t tx="ekr.20221004064034.1146">def visit_instance(self, t: Instance) -&gt; Type:
    args = self.expand_types_with_unpack(list(t.args))
    if isinstance(args, list):
        return t.copy_modified(args=args)
    else:
        return args

</t>
<t tx="ekr.20221004064034.1147">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    repl = self.variables.get(t.id, t)
    if isinstance(repl, ProperType) and isinstance(repl, Instance):
        # TODO: do we really need to do this?
        # If I try to remove this special-casing ~40 tests fail on reveal_type().
        return repl.copy_modified(last_known_value=None)
    return repl

</t>
<t tx="ekr.20221004064034.1148">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    repl = get_proper_type(self.variables.get(t.id, t))
    if isinstance(repl, Instance):
        # TODO: what does prefix mean in this case?
        # TODO: why does this case even happen? Instances aren't plural.
        return repl
    elif isinstance(repl, ParamSpecType):
        return repl.copy_modified(
            flavor=t.flavor,
            prefix=t.prefix.copy_modified(
                arg_types=t.prefix.arg_types + repl.prefix.arg_types,
                arg_kinds=t.prefix.arg_kinds + repl.prefix.arg_kinds,
                arg_names=t.prefix.arg_names + repl.prefix.arg_names,
            ),
        )
    elif isinstance(repl, Parameters) or isinstance(repl, CallableType):
        # if the paramspec is *P.args or **P.kwargs:
        if t.flavor != ParamSpecFlavor.BARE:
            assert isinstance(repl, CallableType), "Should not be able to get here."
            # Is this always the right thing to do?
            param_spec = repl.param_spec()
            if param_spec:
                return param_spec.with_flavor(t.flavor)
            else:
                return repl
        else:
            return Parameters(
                t.prefix.arg_types + repl.arg_types,
                t.prefix.arg_kinds + repl.arg_kinds,
                t.prefix.arg_names + repl.arg_names,
                variables=[*t.prefix.variables, *repl.variables],
            )
    else:
        # TODO: should this branch be removed? better not to fail silently
        return repl

</t>
<t tx="ekr.20221004064034.1149">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064034.115">class Command:
    def __init__(self, setup: Callable[[], None], command: Callable[[], None]) -&gt; None:
        self.setup = setup
        self.command = command


</t>
<t tx="ekr.20221004064034.1150">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    # It is impossible to reasonally implement visit_unpack_type, because
    # unpacking inherently expands to something more like a list of types.
    #
    # Relevant sections that can call unpack should call expand_unpack()
    # instead.
    assert False, "Mypy bug: unpacking must happen at a higher level"

</t>
<t tx="ekr.20221004064034.1151">def expand_unpack(self, t: UnpackType) -&gt; list[Type] | Instance | AnyType | None:
    """May return either a list of types to unpack to, any, or a single
    variable length tuple. The latter may not be valid in all contexts.
    """
    if isinstance(t.type, TypeVarTupleType):
        repl = get_proper_type(self.variables.get(t.type.id, t))
        if isinstance(repl, TupleType):
            return repl.items
        if isinstance(repl, TypeList):
            return repl.items
        elif isinstance(repl, Instance) and repl.type.fullname == "builtins.tuple":
            return repl
        elif isinstance(repl, AnyType):
            # tuple[Any, ...] would be better, but we don't have
            # the type info to construct that type here.
            return repl
        elif isinstance(repl, TypeVarTupleType):
            return [UnpackType(typ=repl)]
        elif isinstance(repl, UnpackType):
            return [repl]
        elif isinstance(repl, UninhabitedType):
            return None
        else:
            raise NotImplementedError(f"Invalid type replacement to expand: {repl}")
    else:
        raise NotImplementedError(f"Invalid type to expand: {t.type}")

</t>
<t tx="ekr.20221004064034.1152">def visit_parameters(self, t: Parameters) -&gt; Type:
    return t.copy_modified(arg_types=self.expand_types(t.arg_types))

</t>
<t tx="ekr.20221004064034.1153">def visit_callable_type(self, t: CallableType) -&gt; Type:
    param_spec = t.param_spec()
    if param_spec is not None:
        repl = get_proper_type(self.variables.get(param_spec.id))
        # If a ParamSpec in a callable type is substituted with a
        # callable type, we can't use normal substitution logic,
        # since ParamSpec is actually split into two components
        # *P.args and **P.kwargs in the original type. Instead, we
        # must expand both of them with all the argument types,
        # kinds and names in the replacement. The return type in
        # the replacement is ignored.
        if isinstance(repl, CallableType) or isinstance(repl, Parameters):
            # Substitute *args: P.args, **kwargs: P.kwargs
            prefix = param_spec.prefix
            # we need to expand the types in the prefix, so might as well
            # not get them in the first place
            t = t.expand_param_spec(repl, no_prefix=True)
            return t.copy_modified(
                arg_types=self.expand_types(prefix.arg_types) + t.arg_types,
                arg_kinds=prefix.arg_kinds + t.arg_kinds,
                arg_names=prefix.arg_names + t.arg_names,
                ret_type=t.ret_type.accept(self),
                type_guard=(t.type_guard.accept(self) if t.type_guard is not None else None),
            )

    return t.copy_modified(
        arg_types=self.expand_types(t.arg_types),
        ret_type=t.ret_type.accept(self),
        type_guard=(t.type_guard.accept(self) if t.type_guard is not None else None),
    )

</t>
<t tx="ekr.20221004064034.1154">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    items: list[CallableType] = []
    for item in t.items:
        new_item = item.accept(self)
        assert isinstance(new_item, ProperType)
        assert isinstance(new_item, CallableType)
        items.append(new_item)
    return Overloaded(items)

</t>
<t tx="ekr.20221004064034.1155">def expand_types_with_unpack(
    self, typs: Sequence[Type]
) -&gt; list[Type] | AnyType | UninhabitedType | Instance:
    """Expands a list of types that has an unpack.

    In corner cases, this can return a type rather than a list, in which case this
    indicates use of Any or some error occurred earlier. In this case callers should
    simply propagate the resulting type.
    """
    items: list[Type] = []
    for item in typs:
        if isinstance(item, UnpackType):
            unpacked_items = self.expand_unpack(item)
            if unpacked_items is None:
                # TODO: better error, something like tuple of unknown?
                return UninhabitedType()
            elif isinstance(unpacked_items, Instance):
                if len(typs) == 1:
                    return unpacked_items
                else:
                    assert False, "Invalid unpack of variable length tuple"
            elif isinstance(unpacked_items, AnyType):
                return unpacked_items
            else:
                items.extend(unpacked_items)
        else:
            # Must preserve original aliases when possible.
            items.append(item.accept(self))
    return items

</t>
<t tx="ekr.20221004064034.1156">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    items = self.expand_types_with_unpack(t.items)
    if isinstance(items, list):
        fallback = t.partial_fallback.accept(self)
        fallback = get_proper_type(fallback)
        if not isinstance(fallback, Instance):
            fallback = t.partial_fallback
        return t.copy_modified(items=items, fallback=fallback)
    else:
        return items

</t>
<t tx="ekr.20221004064034.1157">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    fallback = t.fallback.accept(self)
    fallback = get_proper_type(fallback)
    if not isinstance(fallback, Instance):
        fallback = t.fallback
    return t.copy_modified(item_types=self.expand_types(t.items.values()), fallback=fallback)

</t>
<t tx="ekr.20221004064034.1158">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    # TODO: Verify this implementation is correct
    return t

</t>
<t tx="ekr.20221004064034.1159">def visit_union_type(self, t: UnionType) -&gt; Type:
    # After substituting for type variables in t.items,
    # some of the resulting types might be subtypes of others.
    from mypy.typeops import make_simplified_union  # asdf

    return make_simplified_union(self.expand_types(t.items), t.line, t.column)

</t>
<t tx="ekr.20221004064034.116">def print_offset(text: str, indent_length: int = 4) -&gt; None:
    print()
    print(textwrap.indent(text, " " * indent_length))
    print()


</t>
<t tx="ekr.20221004064034.1160">def visit_partial_type(self, t: PartialType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064034.1161">def visit_type_type(self, t: TypeType) -&gt; Type:
    # TODO: Verify that the new item type is valid (instance or
    # union of instances or Any).  Sadly we can't report errors
    # here yet.
    item = t.item.accept(self)
    return TypeType.make_normalized(item)

</t>
<t tx="ekr.20221004064034.1162">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Target of the type alias cannot contain type variables,
    # so we just expand the arguments.
    return t.copy_modified(args=self.expand_types(t.args))

</t>
<t tx="ekr.20221004064034.1163">def expand_types(self, types: Iterable[Type]) -&gt; list[Type]:
    a: list[Type] = []
    for t in types:
        a.append(t.accept(self))
    return a
</t>
<t tx="ekr.20221004064034.1164">@path C:/Repos/ekr-mypy2/mypy/
"""Translate an Expression to a Type value."""

from __future__ import annotations

from mypy.fastparse import parse_type_string
from mypy.nodes import (
    BytesExpr,
    CallExpr,
    ComplexExpr,
    EllipsisExpr,
    Expression,
    FloatExpr,
    IndexExpr,
    IntExpr,
    ListExpr,
    MemberExpr,
    NameExpr,
    OpExpr,
    RefExpr,
    StrExpr,
    TupleExpr,
    UnaryExpr,
    get_member_expr_fullname,
)
from mypy.options import Options
from mypy.types import (
    ANNOTATED_TYPE_NAMES,
    AnyType,
    CallableArgument,
    EllipsisType,
    ProperType,
    RawExpressionType,
    Type,
    TypeList,
    TypeOfAny,
    UnboundType,
    UnionType,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1165">class TypeTranslationError(Exception):
    """Exception raised when an expression is not valid as a type."""


</t>
<t tx="ekr.20221004064034.1166">def _extract_argument_name(expr: Expression) -&gt; str | None:
    if isinstance(expr, NameExpr) and expr.name == "None":
        return None
    elif isinstance(expr, StrExpr):
        return expr.value
    else:
        raise TypeTranslationError()


</t>
<t tx="ekr.20221004064034.1167">def expr_to_unanalyzed_type(
    expr: Expression,
    options: Options | None = None,
    allow_new_syntax: bool = False,
    _parent: Expression | None = None,
) -&gt; ProperType:
    """Translate an expression to the corresponding type.

    The result is not semantically analyzed. It can be UnboundType or TypeList.
    Raise TypeTranslationError if the expression cannot represent a type.

    If allow_new_syntax is True, allow all type syntax independent of the target
    Python version (used in stubs).
    """
    # The `parent` parameter is used in recursive calls to provide context for
    # understanding whether an CallableArgument is ok.
    name: str | None = None
    if isinstance(expr, NameExpr):
        name = expr.name
        if name == "True":
            return RawExpressionType(True, "builtins.bool", line=expr.line, column=expr.column)
        elif name == "False":
            return RawExpressionType(False, "builtins.bool", line=expr.line, column=expr.column)
        else:
            return UnboundType(name, line=expr.line, column=expr.column)
    elif isinstance(expr, MemberExpr):
        fullname = get_member_expr_fullname(expr)
        if fullname:
            return UnboundType(fullname, line=expr.line, column=expr.column)
        else:
            raise TypeTranslationError()
    elif isinstance(expr, IndexExpr):
        base = expr_to_unanalyzed_type(expr.base, options, allow_new_syntax, expr)
        if isinstance(base, UnboundType):
            if base.args:
                raise TypeTranslationError()
            if isinstance(expr.index, TupleExpr):
                args = expr.index.items
            else:
                args = [expr.index]

            if isinstance(expr.base, RefExpr) and expr.base.fullname in ANNOTATED_TYPE_NAMES:
                # TODO: this is not the optimal solution as we are basically getting rid
                # of the Annotation definition and only returning the type information,
                # losing all the annotations.

                return expr_to_unanalyzed_type(args[0], options, allow_new_syntax, expr)
            else:
                base.args = tuple(
                    expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr) for arg in args
                )
            if not base.args:
                base.empty_tuple_index = True
            return base
        else:
            raise TypeTranslationError()
    elif (
        isinstance(expr, OpExpr)
        and expr.op == "|"
        and ((options and options.python_version &gt;= (3, 10)) or allow_new_syntax)
    ):
        return UnionType(
            [
                expr_to_unanalyzed_type(expr.left, options, allow_new_syntax),
                expr_to_unanalyzed_type(expr.right, options, allow_new_syntax),
            ]
        )
    elif isinstance(expr, CallExpr) and isinstance(_parent, ListExpr):
        c = expr.callee
        names = []
        # Go through the dotted member expr chain to get the full arg
        # constructor name to look up
        while True:
            if isinstance(c, NameExpr):
                names.append(c.name)
                break
            elif isinstance(c, MemberExpr):
                names.append(c.name)
                c = c.expr
            else:
                raise TypeTranslationError()
        arg_const = ".".join(reversed(names))

        # Go through the constructor args to get its name and type.
        name = None
        default_type = AnyType(TypeOfAny.unannotated)
        typ: Type = default_type
        for i, arg in enumerate(expr.args):
            if expr.arg_names[i] is not None:
                if expr.arg_names[i] == "name":
                    if name is not None:
                        # Two names
                        raise TypeTranslationError()
                    name = _extract_argument_name(arg)
                    continue
                elif expr.arg_names[i] == "type":
                    if typ is not default_type:
                        # Two types
                        raise TypeTranslationError()
                    typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
                    continue
                else:
                    raise TypeTranslationError()
            elif i == 0:
                typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
            elif i == 1:
                name = _extract_argument_name(arg)
            else:
                raise TypeTranslationError()
        return CallableArgument(typ, name, arg_const, expr.line, expr.column)
    elif isinstance(expr, ListExpr):
        return TypeList(
            [expr_to_unanalyzed_type(t, options, allow_new_syntax, expr) for t in expr.items],
            line=expr.line,
            column=expr.column,
        )
    elif isinstance(expr, StrExpr):
        return parse_type_string(expr.value, "builtins.str", expr.line, expr.column)
    elif isinstance(expr, BytesExpr):
        return parse_type_string(expr.value, "builtins.bytes", expr.line, expr.column)
    elif isinstance(expr, UnaryExpr):
        typ = expr_to_unanalyzed_type(expr.expr, options, allow_new_syntax)
        if isinstance(typ, RawExpressionType):
            if isinstance(typ.literal_value, int) and expr.op == "-":
                typ.literal_value *= -1
                return typ
        raise TypeTranslationError()
    elif isinstance(expr, IntExpr):
        return RawExpressionType(expr.value, "builtins.int", line=expr.line, column=expr.column)
    elif isinstance(expr, FloatExpr):
        # Floats are not valid parameters for RawExpressionType , so we just
        # pass in 'None' for now. We'll report the appropriate error at a later stage.
        return RawExpressionType(None, "builtins.float", line=expr.line, column=expr.column)
    elif isinstance(expr, ComplexExpr):
        # Same thing as above with complex numbers.
        return RawExpressionType(None, "builtins.complex", line=expr.line, column=expr.column)
    elif isinstance(expr, EllipsisExpr):
        return EllipsisType(expr.line)
    else:
        raise TypeTranslationError()
</t>
<t tx="ekr.20221004064034.1168">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import copy
import re
import sys
import warnings
from typing import Any, Callable, List, Optional, Sequence, TypeVar, Union, cast
from typing_extensions import Final, Literal, overload

from mypy import defaults, errorcodes as codes, message_registry
from mypy.errors import Errors
from mypy.nodes import (
    ARG_NAMED,
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    ArgKind,
    Argument,
    AssertStmt,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    ContinueStmt,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    FakeInfo,
    FloatExpr,
    ForStmt,
    FuncDef,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportBase,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    OverloadPart,
    PassStmt,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    Statement,
    StrExpr,
    SuperExpr,
    TempNode,
    TryStmt,
    TupleExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
    check_arg_names,
)
from mypy.options import Options
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.reachability import infer_reachability_of_if_statement, mark_block_unreachable
from mypy.sharedparse import argument_elide_name, special_function_elide_names
from mypy.types import (
    AnyType,
    CallableArgument,
    CallableType,
    EllipsisType,
    Instance,
    ProperType,
    RawExpressionType,
    TupleType,
    Type,
    TypeList,
    TypeOfAny,
    UnboundType,
    UnionType,
)
from mypy.util import bytes_to_human_readable_repr, unnamed_function

try:
    # pull this into a final variable to make mypyc be quiet about the
    # the default argument warning
    PY_MINOR_VERSION: Final = sys.version_info[1]

    # Check if we can use the stdlib ast module instead of typed_ast.
    if sys.version_info &gt;= (3, 8):
        import ast as ast3

        assert (
            "kind" in ast3.Constant._fields
        ), f"This 3.8.0 alpha ({sys.version.split()[0]}) is too old; 3.8.0a3 required"
        # TODO: Num, Str, Bytes, NameConstant, Ellipsis are deprecated in 3.8.
        # TODO: Index, ExtSlice are deprecated in 3.9.
        from ast import (
            AST,
            Attribute,
            Bytes,
            Call,
            Ellipsis as ast3_Ellipsis,
            Expression as ast3_Expression,
            FunctionType,
            Index,
            Name,
            NameConstant,
            Num,
            Starred,
            Str,
            UnaryOp,
            USub,
        )

        def ast3_parse(
            source: str | bytes, filename: str, mode: str, feature_version: int = PY_MINOR_VERSION
        ) -&gt; AST:
            return ast3.parse(
                source,
                filename,
                mode,
                type_comments=True,  # This works the magic
                feature_version=feature_version,
            )

        NamedExpr = ast3.NamedExpr
        Constant = ast3.Constant
    else:
        from typed_ast import ast3
        from typed_ast.ast3 import (
            AST,
            Attribute,
            Bytes,
            Call,
            Ellipsis as ast3_Ellipsis,
            Expression as ast3_Expression,
            FunctionType,
            Index,
            Name,
            NameConstant,
            Num,
            Starred,
            Str,
            UnaryOp,
            USub,
        )

        def ast3_parse(
            source: str | bytes, filename: str, mode: str, feature_version: int = PY_MINOR_VERSION
        ) -&gt; AST:
            return ast3.parse(source, filename, mode, feature_version=feature_version)

        # These don't exist before 3.8
        NamedExpr = Any
        Constant = Any

    if sys.version_info &gt;= (3, 10):
        Match = ast3.Match
        MatchValue = ast3.MatchValue
        MatchSingleton = ast3.MatchSingleton
        MatchSequence = ast3.MatchSequence
        MatchStar = ast3.MatchStar
        MatchMapping = ast3.MatchMapping
        MatchClass = ast3.MatchClass
        MatchAs = ast3.MatchAs
        MatchOr = ast3.MatchOr
        AstNode = Union[ast3.expr, ast3.stmt, ast3.pattern, ast3.ExceptHandler]
    else:
        Match = Any
        MatchValue = Any
        MatchSingleton = Any
        MatchSequence = Any
        MatchStar = Any
        MatchMapping = Any
        MatchClass = Any
        MatchAs = Any
        MatchOr = Any
        AstNode = Union[ast3.expr, ast3.stmt, ast3.ExceptHandler]
except ImportError:
    try:
        from typed_ast import ast35  # type: ignore[attr-defined]  # noqa: F401
    except ImportError:
        print(
            "The typed_ast package is not installed.\n"
            "You can install it with `python3 -m pip install typed-ast`.",
            file=sys.stderr,
        )
    else:
        print(
            "You need a more recent version of the typed_ast package.\n"
            "You can update to the latest version with "
            "`python3 -m pip install -U typed-ast`.",
            file=sys.stderr,
        )
    sys.exit(1)

N = TypeVar("N", bound=Node)

# There is no way to create reasonable fallbacks at this stage,
# they must be patched later.
MISSING_FALLBACK: Final = FakeInfo("fallback can't be filled out until semanal")
_dummy_fallback: Final = Instance(MISSING_FALLBACK, [], -1)

TYPE_COMMENT_SYNTAX_ERROR: Final = "syntax error in type comment"

INVALID_TYPE_IGNORE: Final = 'Invalid "type: ignore" comment'

TYPE_IGNORE_PATTERN: Final = re.compile(r"[^#]*#\s*type:\s*ignore\s*(.*)")


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1169">def parse(
    source: str | bytes,
    fnam: str,
    module: str | None,
    errors: Errors | None = None,
    options: Options | None = None,
) -&gt; MypyFile:

    """Parse a source file, without doing any semantic analysis.

    Return the parse tree. If errors is not provided, raise ParseError
    on failure. Otherwise, use the errors object to report parse errors.
    """
    raise_on_error = False
    if options is None:
        options = Options()
    if errors is None:
        errors = Errors(hide_error_codes=options.hide_error_codes)
        raise_on_error = True
    errors.set_file(fnam, module, options=options)
    is_stub_file = fnam.endswith(".pyi")
    if is_stub_file:
        feature_version = defaults.PYTHON3_VERSION[1]
        if options.python_version[0] == 3 and options.python_version[1] &gt; feature_version:
            feature_version = options.python_version[1]
    else:
        assert options.python_version[0] &gt;= 3
        feature_version = options.python_version[1]
    try:
        # Disable deprecation warnings about \u
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=DeprecationWarning)
            ast = ast3_parse(source, fnam, "exec", feature_version=feature_version)

        tree = ASTConverter(options=options, is_stub=is_stub_file, errors=errors).visit(ast)
        tree.path = fnam
        tree.is_stub = is_stub_file
    except SyntaxError as e:
        # alias to please mypyc
        is_py38_or_earlier = sys.version_info &lt; (3, 9)
        if is_py38_or_earlier and e.filename == "&lt;fstring&gt;":
            # In Python 3.8 and earlier, syntax errors in f-strings have lineno relative to the
            # start of the f-string. This would be misleading, as mypy will report the error as the
            # lineno within the file.
            e.lineno = None
        message = e.msg
        if feature_version &gt; sys.version_info.minor and message.startswith("invalid syntax"):
            python_version_str = f"{options.python_version[0]}.{options.python_version[1]}"
            message += f"; you likely need to run mypy using Python {python_version_str} or newer"
        errors.report(
            e.lineno if e.lineno is not None else -1,
            e.offset,
            message,
            blocker=True,
            code=codes.SYNTAX,
        )
        tree = MypyFile([], [], False, {})

    if raise_on_error and errors.is_errors():
        errors.raise_error()

    assert isinstance(tree, MypyFile)
    return tree


</t>
<t tx="ekr.20221004064034.117">def delete_folder(folder_path: str) -&gt; None:
    if os.path.exists(folder_path):
        shutil.rmtree(folder_path)


</t>
<t tx="ekr.20221004064034.1170">def parse_type_ignore_tag(tag: str | None) -&gt; list[str] | None:
    """Parse optional "[code, ...]" tag after "# type: ignore".

    Return:
     * [] if no tag was found (ignore all errors)
     * list of ignored error codes if a tag was found
     * None if the tag was invalid.
    """
    if not tag or tag.strip() == "" or tag.strip().startswith("#"):
        # No tag -- ignore all errors.
        return []
    m = re.match(r"\s*\[([^]#]*)\]\s*(#.*)?$", tag)
    if m is None:
        # Invalid "# type: ignore" comment.
        return None
    return [code.strip() for code in m.group(1).split(",")]


</t>
<t tx="ekr.20221004064034.1171">def parse_type_comment(
    type_comment: str, line: int, column: int, errors: Errors | None
) -&gt; tuple[list[str] | None, ProperType | None]:
    """Parse type portion of a type comment (+ optional type ignore).

    Return (ignore info, parsed type).
    """
    try:
        typ = ast3_parse(type_comment, "&lt;type_comment&gt;", "eval")
    except SyntaxError:
        if errors is not None:
            stripped_type = type_comment.split("#", 2)[0].strip()
            err_msg = f'{TYPE_COMMENT_SYNTAX_ERROR} "{stripped_type}"'
            errors.report(line, column, err_msg, blocker=True, code=codes.SYNTAX)
            return None, None
        else:
            raise
    else:
        extra_ignore = TYPE_IGNORE_PATTERN.match(type_comment)
        if extra_ignore:
            # Typeshed has a non-optional return type for group!
            tag: str | None = cast(Any, extra_ignore).group(1)
            ignored: list[str] | None = parse_type_ignore_tag(tag)
            if ignored is None:
                if errors is not None:
                    errors.report(line, column, INVALID_TYPE_IGNORE, code=codes.SYNTAX)
                else:
                    raise SyntaxError
        else:
            ignored = None
        assert isinstance(typ, ast3_Expression)
        converted = TypeConverter(
            errors, line=line, override_column=column, is_evaluated=False
        ).visit(typ.body)
        return ignored, converted


</t>
<t tx="ekr.20221004064034.1172">def parse_type_string(
    expr_string: str, expr_fallback_name: str, line: int, column: int
) -&gt; ProperType:
    """Parses a type that was originally present inside of an explicit string.

    For example, suppose we have the type `Foo["blah"]`. We should parse the
    string expression "blah" using this function.
    """
    try:
        _, node = parse_type_comment(expr_string.strip(), line=line, column=column, errors=None)
        if isinstance(node, UnboundType) and node.original_str_expr is None:
            node.original_str_expr = expr_string
            node.original_str_fallback = expr_fallback_name
            return node
        elif isinstance(node, UnionType):
            return node
        else:
            return RawExpressionType(expr_string, expr_fallback_name, line, column)
    except (SyntaxError, ValueError):
        # Note: the parser will raise a `ValueError` instead of a SyntaxError if
        # the string happens to contain things like \x00.
        return RawExpressionType(expr_string, expr_fallback_name, line, column)


</t>
<t tx="ekr.20221004064034.1173">def is_no_type_check_decorator(expr: ast3.expr) -&gt; bool:
    if isinstance(expr, Name):
        return expr.id == "no_type_check"
    elif isinstance(expr, Attribute):
        if isinstance(expr.value, Name):
            return expr.value.id == "typing" and expr.attr == "no_type_check"
    return False


</t>
<t tx="ekr.20221004064034.1174">class ASTConverter:
    @others
</t>
<t tx="ekr.20221004064034.1175">def __init__(self, options: Options, is_stub: bool, errors: Errors) -&gt; None:
    # 'C' for class, 'F' for function
    self.class_and_function_stack: list[Literal["C", "F"]] = []
    self.imports: list[ImportBase] = []

    self.options = options
    self.is_stub = is_stub
    self.errors = errors

    self.type_ignores: dict[int, list[str]] = {}

    # Cache of visit_X methods keyed by type of visited object
    self.visitor_cache: dict[type, Callable[[AST | None], Any]] = {}

</t>
<t tx="ekr.20221004064034.1176">def note(self, msg: str, line: int, column: int) -&gt; None:
    self.errors.report(line, column, msg, severity="note", code=codes.SYNTAX)

</t>
<t tx="ekr.20221004064034.1177">def fail(
    self,
    msg: str,
    line: int,
    column: int,
    blocker: bool = True,
    code: codes.ErrorCode = codes.SYNTAX,
) -&gt; None:
    if blocker or not self.options.ignore_errors:
        self.errors.report(line, column, msg, blocker=blocker, code=code)

</t>
<t tx="ekr.20221004064034.1178">def fail_merge_overload(self, node: IfStmt) -&gt; None:
    self.fail(
        "Condition can't be inferred, unable to merge overloads",
        line=node.line,
        column=node.column,
        blocker=False,
        code=codes.MISC,
    )

</t>
<t tx="ekr.20221004064034.1179">def visit(self, node: AST | None) -&gt; Any:
    if node is None:
        return None
    typeobj = type(node)
    visitor = self.visitor_cache.get(typeobj)
    if visitor is None:
        method = "visit_" + node.__class__.__name__
        visitor = getattr(self, method)
        self.visitor_cache[typeobj] = visitor
    return visitor(node)

</t>
<t tx="ekr.20221004064034.118">def execute(command: list[str]) -&gt; None:
    proc = subprocess.Popen(
        " ".join(command), stderr=subprocess.PIPE, stdout=subprocess.PIPE, shell=True
    )
    stdout_bytes, stderr_bytes = proc.communicate()
    stdout, stderr = stdout_bytes.decode("utf-8"), stderr_bytes.decode("utf-8")
    if proc.returncode != 0:
        print("EXECUTED COMMAND:", repr(command))
        print("RETURN CODE:", proc.returncode)
        print()
        print("STDOUT:")
        print_offset(stdout)
        print("STDERR:")
        print_offset(stderr)
        raise RuntimeError("Unexpected error from external tool.")


</t>
<t tx="ekr.20221004064034.1180">def set_line(self, node: N, n: AstNode) -&gt; N:
    node.line = n.lineno
    node.column = n.col_offset
    node.end_line = getattr(n, "end_lineno", None)
    node.end_column = getattr(n, "end_col_offset", None)

    return node

</t>
<t tx="ekr.20221004064034.1181">def translate_opt_expr_list(self, l: Sequence[AST | None]) -&gt; list[Expression | None]:
    res: list[Expression | None] = []
    for e in l:
        exp = self.visit(e)
        res.append(exp)
    return res

</t>
<t tx="ekr.20221004064034.1182">def translate_expr_list(self, l: Sequence[AST]) -&gt; list[Expression]:
    return cast(List[Expression], self.translate_opt_expr_list(l))

</t>
<t tx="ekr.20221004064034.1183">def get_lineno(self, node: ast3.expr | ast3.stmt) -&gt; int:
    if (
        isinstance(node, (ast3.AsyncFunctionDef, ast3.ClassDef, ast3.FunctionDef))
        and node.decorator_list
    ):
        return node.decorator_list[0].lineno
    return node.lineno

</t>
<t tx="ekr.20221004064034.1184">def translate_stmt_list(
    self, stmts: Sequence[ast3.stmt], ismodule: bool = False
) -&gt; list[Statement]:
    # A "# type: ignore" comment before the first statement of a module
    # ignores the whole module:
    if (
        ismodule
        and stmts
        and self.type_ignores
        and min(self.type_ignores) &lt; self.get_lineno(stmts[0])
    ):
        if self.type_ignores[min(self.type_ignores)]:
            self.fail(
                (
                    "type ignore with error code is not supported for modules; "
                    "use `# mypy: disable-error-code=...`"
                ),
                line=min(self.type_ignores),
                column=0,
                blocker=False,
            )
        self.errors.used_ignored_lines[self.errors.file][min(self.type_ignores)].append(
            codes.FILE.code
        )
        block = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        mark_block_unreachable(block)
        return [block]

    res: list[Statement] = []
    for stmt in stmts:
        node = self.visit(stmt)
        res.append(node)

    return res

</t>
<t tx="ekr.20221004064034.1185">def translate_type_comment(
    self, n: ast3.stmt | ast3.arg, type_comment: str | None
) -&gt; ProperType | None:
    if type_comment is None:
        return None
    else:
        lineno = n.lineno
        extra_ignore, typ = parse_type_comment(type_comment, lineno, n.col_offset, self.errors)
        if extra_ignore is not None:
            self.type_ignores[lineno] = extra_ignore
        return typ

</t>
<t tx="ekr.20221004064034.1186">op_map: Final[dict[type[AST], str]] = {
    ast3.Add: "+",
    ast3.Sub: "-",
    ast3.Mult: "*",
    ast3.MatMult: "@",
    ast3.Div: "/",
    ast3.Mod: "%",
    ast3.Pow: "**",
    ast3.LShift: "&lt;&lt;",
    ast3.RShift: "&gt;&gt;",
    ast3.BitOr: "|",
    ast3.BitXor: "^",
    ast3.BitAnd: "&amp;",
    ast3.FloorDiv: "//",
}

</t>
<t tx="ekr.20221004064034.1187">def from_operator(self, op: ast3.operator) -&gt; str:
    op_name = ASTConverter.op_map.get(type(op))
    if op_name is None:
        raise RuntimeError("Unknown operator " + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20221004064034.1188">comp_op_map: Final[dict[type[AST], str]] = {
    ast3.Gt: "&gt;",
    ast3.Lt: "&lt;",
    ast3.Eq: "==",
    ast3.GtE: "&gt;=",
    ast3.LtE: "&lt;=",
    ast3.NotEq: "!=",
    ast3.Is: "is",
    ast3.IsNot: "is not",
    ast3.In: "in",
    ast3.NotIn: "not in",
}

</t>
<t tx="ekr.20221004064034.1189">def from_comp_operator(self, op: ast3.cmpop) -&gt; str:
    op_name = ASTConverter.comp_op_map.get(type(op))
    if op_name is None:
        raise RuntimeError("Unknown comparison operator " + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20221004064034.119">def trial(num_trials: int, command: Command) -&gt; list[float]:
    trials = []
    for i in range(num_trials):
        command.setup()
        start = time.time()
        command.command()
        delta = time.time() - start
        trials.append(delta)
    return trials


</t>
<t tx="ekr.20221004064034.1190">def as_block(self, stmts: list[ast3.stmt], lineno: int) -&gt; Block | None:
    b = None
    if stmts:
        b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        b.set_line(lineno)
    return b

</t>
<t tx="ekr.20221004064034.1191">def as_required_block(self, stmts: list[ast3.stmt], lineno: int) -&gt; Block:
    assert stmts  # must be non-empty
    b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
    # TODO: in most call sites line is wrong (includes first line of enclosing statement)
    # TODO: also we need to set the column, and the end position here.
    b.set_line(lineno)
    return b

</t>
<t tx="ekr.20221004064034.1192">def fix_function_overloads(self, stmts: list[Statement]) -&gt; list[Statement]:
    ret: list[Statement] = []
    current_overload: list[OverloadPart] = []
    current_overload_name: str | None = None
    seen_unconditional_func_def = False
    last_if_stmt: IfStmt | None = None
    last_if_overload: Decorator | FuncDef | OverloadedFuncDef | None = None
    last_if_stmt_overload_name: str | None = None
    last_if_unknown_truth_value: IfStmt | None = None
    skipped_if_stmts: list[IfStmt] = []
    for stmt in stmts:
        if_overload_name: str | None = None
        if_block_with_overload: Block | None = None
        if_unknown_truth_value: IfStmt | None = None
        if isinstance(stmt, IfStmt) and seen_unconditional_func_def is False:
            # Check IfStmt block to determine if function overloads can be merged
            if_overload_name = self._check_ifstmt_for_overloads(stmt, current_overload_name)
            if if_overload_name is not None:
                (
                    if_block_with_overload,
                    if_unknown_truth_value,
                ) = self._get_executable_if_block_with_overloads(stmt)

        if (
            current_overload_name is not None
            and isinstance(stmt, (Decorator, FuncDef))
            and stmt.name == current_overload_name
        ):
            if last_if_stmt is not None:
                skipped_if_stmts.append(last_if_stmt)
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if last_if_unknown_truth_value:
                self.fail_merge_overload(last_if_unknown_truth_value)
                last_if_unknown_truth_value = None
            current_overload.append(stmt)
            if isinstance(stmt, FuncDef):
                seen_unconditional_func_def = True
        elif (
            current_overload_name is not None
            and isinstance(stmt, IfStmt)
            and if_overload_name == current_overload_name
        ):
            # IfStmt only contains stmts relevant to current_overload.
            # Check if stmts are reachable and add them to current_overload,
            # otherwise skip IfStmt to allow subsequent overload
            # or function definitions.
            skipped_if_stmts.append(stmt)
            if if_block_with_overload is None:
                if if_unknown_truth_value is not None:
                    self.fail_merge_overload(if_unknown_truth_value)
                continue
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if isinstance(if_block_with_overload.body[-1], OverloadedFuncDef):
                skipped_if_stmts.extend(cast(List[IfStmt], if_block_with_overload.body[:-1]))
                current_overload.extend(if_block_with_overload.body[-1].items)
            else:
                current_overload.append(
                    cast(Union[Decorator, FuncDef], if_block_with_overload.body[0])
                )
        else:
            if last_if_stmt is not None:
                ret.append(last_if_stmt)
                last_if_stmt_overload_name = current_overload_name
                last_if_stmt, last_if_overload = None, None
                last_if_unknown_truth_value = None

            if current_overload and current_overload_name == last_if_stmt_overload_name:
                # Remove last stmt (IfStmt) from ret if the overload names matched
                # Only happens if no executable block had been found in IfStmt
                skipped_if_stmts.append(cast(IfStmt, ret.pop()))
            if current_overload and skipped_if_stmts:
                # Add bare IfStmt (without overloads) to ret
                # Required for mypy to be able to still check conditions
                for if_stmt in skipped_if_stmts:
                    self._strip_contents_from_if_stmt(if_stmt)
                    ret.append(if_stmt)
                skipped_if_stmts = []
            if len(current_overload) == 1:
                ret.append(current_overload[0])
            elif len(current_overload) &gt; 1:
                ret.append(OverloadedFuncDef(current_overload))

            # If we have multiple decorated functions named "_" next to each, we want to treat
            # them as a series of regular FuncDefs instead of one OverloadedFuncDef because
            # most of mypy/mypyc assumes that all the functions in an OverloadedFuncDef are
            # related, but multiple underscore functions next to each other aren't necessarily
            # related
            seen_unconditional_func_def = False
            if isinstance(stmt, Decorator) and not unnamed_function(stmt.name):
                current_overload = [stmt]
                current_overload_name = stmt.name
            elif isinstance(stmt, IfStmt) and if_overload_name is not None:
                current_overload = []
                current_overload_name = if_overload_name
                last_if_stmt = stmt
                last_if_stmt_overload_name = None
                if if_block_with_overload is not None:
                    skipped_if_stmts.extend(
                        cast(List[IfStmt], if_block_with_overload.body[:-1])
                    )
                    last_if_overload = cast(
                        Union[Decorator, FuncDef, OverloadedFuncDef],
                        if_block_with_overload.body[-1],
                    )
                last_if_unknown_truth_value = if_unknown_truth_value
            else:
                current_overload = []
                current_overload_name = None
                ret.append(stmt)

    if current_overload and skipped_if_stmts:
        # Add bare IfStmt (without overloads) to ret
        # Required for mypy to be able to still check conditions
        for if_stmt in skipped_if_stmts:
            self._strip_contents_from_if_stmt(if_stmt)
            ret.append(if_stmt)
    if len(current_overload) == 1:
        ret.append(current_overload[0])
    elif len(current_overload) &gt; 1:
        ret.append(OverloadedFuncDef(current_overload))
    elif last_if_overload is not None:
        ret.append(last_if_overload)
    elif last_if_stmt is not None:
        ret.append(last_if_stmt)
    return ret

</t>
<t tx="ekr.20221004064034.1193">def _check_ifstmt_for_overloads(
    self, stmt: IfStmt, current_overload_name: str | None = None
) -&gt; str | None:
    """Check if IfStmt contains only overloads with the same name.
    Return overload_name if found, None otherwise.
    """
    # Check that block only contains a single Decorator, FuncDef, or OverloadedFuncDef.
    # Multiple overloads have already been merged as OverloadedFuncDef.
    if not (
        len(stmt.body[0].body) == 1
        and (
            isinstance(stmt.body[0].body[0], (Decorator, OverloadedFuncDef))
            or current_overload_name is not None
            and isinstance(stmt.body[0].body[0], FuncDef)
        )
        or len(stmt.body[0].body) &gt; 1
        and isinstance(stmt.body[0].body[-1], OverloadedFuncDef)
        and all(self._is_stripped_if_stmt(if_stmt) for if_stmt in stmt.body[0].body[:-1])
    ):
        return None

    overload_name = cast(
        Union[Decorator, FuncDef, OverloadedFuncDef], stmt.body[0].body[-1]
    ).name
    if stmt.else_body is None:
        return overload_name

    if len(stmt.else_body.body) == 1:
        # For elif: else_body contains an IfStmt itself -&gt; do a recursive check.
        if (
            isinstance(stmt.else_body.body[0], (Decorator, FuncDef, OverloadedFuncDef))
            and stmt.else_body.body[0].name == overload_name
        ):
            return overload_name
        if (
            isinstance(stmt.else_body.body[0], IfStmt)
            and self._check_ifstmt_for_overloads(stmt.else_body.body[0], current_overload_name)
            == overload_name
        ):
            return overload_name

    return None

</t>
<t tx="ekr.20221004064034.1194">def _get_executable_if_block_with_overloads(
    self, stmt: IfStmt
) -&gt; tuple[Block | None, IfStmt | None]:
    """Return block from IfStmt that will get executed.

    Return
        0 -&gt; A block if sure that alternative blocks are unreachable.
        1 -&gt; An IfStmt if the reachability of it can't be inferred,
             i.e. the truth value is unknown.
    """
    infer_reachability_of_if_statement(stmt, self.options)
    if stmt.else_body is None and stmt.body[0].is_unreachable is True:
        # always False condition with no else
        return None, None
    if (
        stmt.else_body is None
        or stmt.body[0].is_unreachable is False
        and stmt.else_body.is_unreachable is False
    ):
        # The truth value is unknown, thus not conclusive
        return None, stmt
    if stmt.else_body.is_unreachable is True:
        # else_body will be set unreachable if condition is always True
        return stmt.body[0], None
    if stmt.body[0].is_unreachable is True:
        # body will be set unreachable if condition is always False
        # else_body can contain an IfStmt itself (for elif) -&gt; do a recursive check
        if isinstance(stmt.else_body.body[0], IfStmt):
            return self._get_executable_if_block_with_overloads(stmt.else_body.body[0])
        return stmt.else_body, None
    return None, stmt

</t>
<t tx="ekr.20221004064034.1195">def _strip_contents_from_if_stmt(self, stmt: IfStmt) -&gt; None:
    """Remove contents from IfStmt.

    Needed to still be able to check the conditions after the contents
    have been merged with the surrounding function overloads.
    """
    if len(stmt.body) == 1:
        stmt.body[0].body = []
    if stmt.else_body and len(stmt.else_body.body) == 1:
        if isinstance(stmt.else_body.body[0], IfStmt):
            self._strip_contents_from_if_stmt(stmt.else_body.body[0])
        else:
            stmt.else_body.body = []

</t>
<t tx="ekr.20221004064034.1196">def _is_stripped_if_stmt(self, stmt: Statement) -&gt; bool:
    """Check stmt to make sure it is a stripped IfStmt.

    See also: _strip_contents_from_if_stmt
    """
    if not isinstance(stmt, IfStmt):
        return False

    if not (len(stmt.body) == 1 and len(stmt.body[0].body) == 0):
        # Body not empty
        return False

    if not stmt.else_body or len(stmt.else_body.body) == 0:
        # No or empty else_body
        return True

    # For elif, IfStmt are stored recursively in else_body
    return self._is_stripped_if_stmt(stmt.else_body.body[0])

</t>
<t tx="ekr.20221004064034.1197">def in_method_scope(self) -&gt; bool:
    return self.class_and_function_stack[-2:] == ["C", "F"]

</t>
<t tx="ekr.20221004064034.1198">def translate_module_id(self, id: str) -&gt; str:
    """Return the actual, internal module id for a source text id."""
    if id == self.options.custom_typing_module:
        return "typing"
    return id

</t>
<t tx="ekr.20221004064034.1199">def visit_Module(self, mod: ast3.Module) -&gt; MypyFile:
    self.type_ignores = {}
    for ti in mod.type_ignores:
        parsed = parse_type_ignore_tag(ti.tag)  # type: ignore[attr-defined]
        if parsed is not None:
            self.type_ignores[ti.lineno] = parsed
        else:
            self.fail(INVALID_TYPE_IGNORE, ti.lineno, -1, blocker=False)
    body = self.fix_function_overloads(self.translate_stmt_list(mod.body, ismodule=True))
    return MypyFile(body, self.imports, False, self.type_ignores)

</t>
<t tx="ekr.20221004064034.12">def run_cmd(name: str) -&gt; int:
    status = 0
    cmd = cmds[name]
    print(f"run {name}: {cmd}")
    proc = subprocess.run(cmd, stderr=subprocess.STDOUT)
    if proc.returncode:
        print("\nFAILED: %s" % name)
        status = proc.returncode
        if name in FAST_FAIL:
            exit(status)
    return status


</t>
<t tx="ekr.20221004064034.120">def report(name: str, times: list[float]) -&gt; None:
    print(f"{name}:")
    print(f"  Times: {times}")
    print(f"  Mean:  {statistics.mean(times)}")
    print(f"  Stdev: {statistics.stdev(times)}")
    print()


</t>
<t tx="ekr.20221004064034.1200"># --- stmt ---
# FunctionDef(identifier name, arguments args,
#             stmt* body, expr* decorator_list, expr? returns, string? type_comment)
# arguments = (arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults,
#              arg? kwarg, expr* defaults)
def visit_FunctionDef(self, n: ast3.FunctionDef) -&gt; FuncDef | Decorator:
    return self.do_func_def(n)

</t>
<t tx="ekr.20221004064034.1201"># AsyncFunctionDef(identifier name, arguments args,
#                  stmt* body, expr* decorator_list, expr? returns, string? type_comment)
def visit_AsyncFunctionDef(self, n: ast3.AsyncFunctionDef) -&gt; FuncDef | Decorator:
    return self.do_func_def(n, is_coroutine=True)

</t>
<t tx="ekr.20221004064034.1202">def do_func_def(
    self, n: ast3.FunctionDef | ast3.AsyncFunctionDef, is_coroutine: bool = False
) -&gt; FuncDef | Decorator:
    """Helper shared between visit_FunctionDef and visit_AsyncFunctionDef."""
    self.class_and_function_stack.append("F")
    no_type_check = bool(
        n.decorator_list and any(is_no_type_check_decorator(d) for d in n.decorator_list)
    )

    lineno = n.lineno
    args = self.transform_args(n.args, lineno, no_type_check=no_type_check)
    if special_function_elide_names(n.name):
        for arg in args:
            arg.pos_only = True

    arg_kinds = [arg.kind for arg in args]
    arg_names = [None if arg.pos_only else arg.variable.name for arg in args]

    arg_types: list[Type | None] = []
    if no_type_check:
        arg_types = [None] * len(args)
        return_type = None
    elif n.type_comment is not None:
        try:
            func_type_ast = ast3_parse(n.type_comment, "&lt;func_type&gt;", "func_type")
            assert isinstance(func_type_ast, FunctionType)
            # for ellipsis arg
            if len(func_type_ast.argtypes) == 1 and isinstance(
                func_type_ast.argtypes[0], ast3_Ellipsis
            ):
                if n.returns:
                    # PEP 484 disallows both type annotations and type comments
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                arg_types = [
                    a.type_annotation
                    if a.type_annotation is not None
                    else AnyType(TypeOfAny.unannotated)
                    for a in args
                ]
            else:
                # PEP 484 disallows both type annotations and type comments
                if n.returns or any(a.type_annotation is not None for a in args):
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                translated_args: list[Type] = TypeConverter(
                    self.errors, line=lineno, override_column=n.col_offset
                ).translate_expr_list(func_type_ast.argtypes)
                # Use a cast to work around `list` invariance
                arg_types = cast(List[Optional[Type]], translated_args)
            return_type = TypeConverter(self.errors, line=lineno).visit(func_type_ast.returns)

            # add implicit self type
            if self.in_method_scope() and len(arg_types) &lt; len(args):
                arg_types.insert(0, AnyType(TypeOfAny.special_form))
        except SyntaxError:
            stripped_type = n.type_comment.split("#", 2)[0].strip()
            err_msg = f'{TYPE_COMMENT_SYNTAX_ERROR} "{stripped_type}"'
            self.fail(err_msg, lineno, n.col_offset)
            if n.type_comment and n.type_comment[0] not in ["(", "#"]:
                self.note(
                    "Suggestion: wrap argument types in parentheses", lineno, n.col_offset
                )
            arg_types = [AnyType(TypeOfAny.from_error)] * len(args)
            return_type = AnyType(TypeOfAny.from_error)
    else:
        arg_types = [a.type_annotation for a in args]
        return_type = TypeConverter(
            self.errors, line=n.returns.lineno if n.returns else lineno
        ).visit(n.returns)

    for arg, arg_type in zip(args, arg_types):
        self.set_type_optional(arg_type, arg.initializer)

    func_type = None
    if any(arg_types) or return_type:
        if len(arg_types) != 1 and any(isinstance(t, EllipsisType) for t in arg_types):
            self.fail(
                "Ellipses cannot accompany other argument types in function type signature",
                lineno,
                n.col_offset,
            )
        elif len(arg_types) &gt; len(arg_kinds):
            self.fail(
                "Type signature has too many arguments", lineno, n.col_offset, blocker=False
            )
        elif len(arg_types) &lt; len(arg_kinds):
            self.fail(
                "Type signature has too few arguments", lineno, n.col_offset, blocker=False
            )
        else:
            func_type = CallableType(
                [a if a is not None else AnyType(TypeOfAny.unannotated) for a in arg_types],
                arg_kinds,
                arg_names,
                return_type if return_type is not None else AnyType(TypeOfAny.unannotated),
                _dummy_fallback,
            )

    # End position is always the same.
    end_line = getattr(n, "end_lineno", None)
    end_column = getattr(n, "end_col_offset", None)

    func_def = FuncDef(n.name, args, self.as_required_block(n.body, lineno), func_type)
    if isinstance(func_def.type, CallableType):
        # semanal.py does some in-place modifications we want to avoid
        func_def.unanalyzed_type = func_def.type.copy_modified()
    if is_coroutine:
        func_def.is_coroutine = True
    if func_type is not None:
        func_type.definition = func_def
        func_type.line = lineno

    if n.decorator_list:
        if sys.version_info &lt; (3, 8):
            # Before 3.8, [typed_]ast the line number points to the first decorator.
            # In 3.8, it points to the 'def' line, where we want it.
            deco_line = lineno
            lineno += len(n.decorator_list)  # this is only approximately true
        else:
            # Set deco_line to the old pre-3.8 lineno, in order to keep
            # existing "# type: ignore" comments working:
            deco_line = n.decorator_list[0].lineno

        var = Var(func_def.name)
        var.is_ready = False
        var.set_line(lineno)

        func_def.is_decorated = True
        func_def.deco_line = deco_line
        func_def.set_line(lineno, n.col_offset, end_line, end_column)
        # Set the line again after we updated it (to make value same in Python 3.7/3.8)
        # Note that TODOs in as_required_block() apply here as well.
        func_def.body.set_line(lineno)

        deco = Decorator(func_def, self.translate_expr_list(n.decorator_list), var)
        first = n.decorator_list[0]
        deco.set_line(first.lineno, first.col_offset, end_line, end_column)
        retval: FuncDef | Decorator = deco
    else:
        # FuncDef overrides set_line -- can't use self.set_line
        func_def.set_line(lineno, n.col_offset, end_line, end_column)
        retval = func_def
    self.class_and_function_stack.pop()
    return retval

</t>
<t tx="ekr.20221004064034.1203">def set_type_optional(self, type: Type | None, initializer: Expression | None) -&gt; None:
    if not self.options.implicit_optional:
        return
    # Indicate that type should be wrapped in an Optional if arg is initialized to None.
    optional = isinstance(initializer, NameExpr) and initializer.name == "None"
    if isinstance(type, UnboundType):
        type.optional = optional

</t>
<t tx="ekr.20221004064034.1204">def transform_args(
    self, args: ast3.arguments, line: int, no_type_check: bool = False
) -&gt; list[Argument]:
    new_args = []
    names: list[ast3.arg] = []
    posonlyargs = getattr(args, "posonlyargs", cast(List[ast3.arg], []))
    args_args = posonlyargs + args.args
    args_defaults = args.defaults
    num_no_defaults = len(args_args) - len(args_defaults)
    # positional arguments without defaults
    for i, a in enumerate(args_args[:num_no_defaults]):
        pos_only = i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, None, ARG_POS, no_type_check, pos_only))
        names.append(a)

    # positional arguments with defaults
    for i, (a, d) in enumerate(zip(args_args[num_no_defaults:], args_defaults)):
        pos_only = num_no_defaults + i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, d, ARG_OPT, no_type_check, pos_only))
        names.append(a)

    # *arg
    if args.vararg is not None:
        new_args.append(self.make_argument(args.vararg, None, ARG_STAR, no_type_check))
        names.append(args.vararg)

    # keyword-only arguments with defaults
    for a, kd in zip(args.kwonlyargs, args.kw_defaults):
        new_args.append(
            self.make_argument(
                a, kd, ARG_NAMED if kd is None else ARG_NAMED_OPT, no_type_check
            )
        )
        names.append(a)

    # **kwarg
    if args.kwarg is not None:
        new_args.append(self.make_argument(args.kwarg, None, ARG_STAR2, no_type_check))
        names.append(args.kwarg)

    check_arg_names([arg.variable.name for arg in new_args], names, self.fail_arg)

    return new_args

</t>
<t tx="ekr.20221004064034.1205">def make_argument(
    self,
    arg: ast3.arg,
    default: ast3.expr | None,
    kind: ArgKind,
    no_type_check: bool,
    pos_only: bool = False,
) -&gt; Argument:
    if no_type_check:
        arg_type = None
    else:
        annotation = arg.annotation
        type_comment = arg.type_comment
        if annotation is not None and type_comment is not None:
            self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, arg.lineno, arg.col_offset)
        arg_type = None
        if annotation is not None:
            arg_type = TypeConverter(self.errors, line=arg.lineno).visit(annotation)
        else:
            arg_type = self.translate_type_comment(arg, type_comment)
    if argument_elide_name(arg.arg):
        pos_only = True

    return Argument(Var(arg.arg), arg_type, self.visit(default), kind, pos_only)

</t>
<t tx="ekr.20221004064034.1206">def fail_arg(self, msg: str, arg: ast3.arg) -&gt; None:
    self.fail(msg, arg.lineno, arg.col_offset)

</t>
<t tx="ekr.20221004064034.1207"># ClassDef(identifier name,
#  expr* bases,
#  keyword* keywords,
#  stmt* body,
#  expr* decorator_list)
def visit_ClassDef(self, n: ast3.ClassDef) -&gt; ClassDef:
    self.class_and_function_stack.append("C")
    keywords = [(kw.arg, self.visit(kw.value)) for kw in n.keywords if kw.arg]

    cdef = ClassDef(
        n.name,
        self.as_required_block(n.body, n.lineno),
        None,
        self.translate_expr_list(n.bases),
        metaclass=dict(keywords).get("metaclass"),
        keywords=keywords,
    )
    cdef.decorators = self.translate_expr_list(n.decorator_list)
    # Set lines to match the old mypy 0.700 lines, in order to keep
    # existing "# type: ignore" comments working:
    if sys.version_info &lt; (3, 8):
        cdef.line = n.lineno + len(n.decorator_list)
        cdef.deco_line = n.lineno
    else:
        cdef.line = n.lineno
        cdef.deco_line = n.decorator_list[0].lineno if n.decorator_list else None
    cdef.column = n.col_offset
    cdef.end_line = getattr(n, "end_lineno", None)
    cdef.end_column = getattr(n, "end_col_offset", None)
    self.class_and_function_stack.pop()
    return cdef

</t>
<t tx="ekr.20221004064034.1208"># Return(expr? value)
def visit_Return(self, n: ast3.Return) -&gt; ReturnStmt:
    node = ReturnStmt(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1209"># Delete(expr* targets)
def visit_Delete(self, n: ast3.Delete) -&gt; DelStmt:
    if len(n.targets) &gt; 1:
        tup = TupleExpr(self.translate_expr_list(n.targets))
        tup.set_line(n.lineno)
        node = DelStmt(tup)
    else:
        node = DelStmt(self.visit(n.targets[0]))
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.121">def main() -&gt; None:
    trials = 3

    print("Testing baseline")
    baseline = trial(
        trials, Command(lambda: None, lambda: execute(["python3", "-m", "mypy", "mypy"]))
    )
    report("Baseline", baseline)

    print("Testing cold cache")
    cold_cache = trial(
        trials,
        Command(
            lambda: delete_folder(".mypy_cache"),
            lambda: execute(["python3", "-m", "mypy", "-i", "mypy"]),
        ),
    )
    report("Cold cache", cold_cache)

    print("Testing warm cache")
    execute(["python3", "-m", "mypy", "-i", "mypy"])
    warm_cache = trial(
        trials, Command(lambda: None, lambda: execute(["python3", "-m", "mypy", "-i", "mypy"]))
    )
    report("Warm cache", warm_cache)


</t>
<t tx="ekr.20221004064034.1210"># Assign(expr* targets, expr? value, string? type_comment, expr? annotation)
def visit_Assign(self, n: ast3.Assign) -&gt; AssignmentStmt:
    lvalues = self.translate_expr_list(n.targets)
    rvalue = self.visit(n.value)
    typ = self.translate_type_comment(n, n.type_comment)
    s = AssignmentStmt(lvalues, rvalue, type=typ, new_syntax=False)
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.1211"># AnnAssign(expr target, expr annotation, expr? value, int simple)
def visit_AnnAssign(self, n: ast3.AnnAssign) -&gt; AssignmentStmt:
    line = n.lineno
    if n.value is None:  # always allow 'x: int'
        rvalue: Expression = TempNode(AnyType(TypeOfAny.special_form), no_rhs=True)
        rvalue.line = line
        rvalue.column = n.col_offset
    else:
        rvalue = self.visit(n.value)
    typ = TypeConverter(self.errors, line=line).visit(n.annotation)
    assert typ is not None
    typ.column = n.annotation.col_offset
    s = AssignmentStmt([self.visit(n.target)], rvalue, type=typ, new_syntax=True)
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.1212"># AugAssign(expr target, operator op, expr value)
def visit_AugAssign(self, n: ast3.AugAssign) -&gt; OperatorAssignmentStmt:
    s = OperatorAssignmentStmt(
        self.from_operator(n.op), self.visit(n.target), self.visit(n.value)
    )
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.1213"># For(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_For(self, n: ast3.For) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(
        self.visit(n.target),
        self.visit(n.iter),
        self.as_required_block(n.body, n.lineno),
        self.as_block(n.orelse, n.lineno),
        target_type,
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1214"># AsyncFor(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_AsyncFor(self, n: ast3.AsyncFor) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(
        self.visit(n.target),
        self.visit(n.iter),
        self.as_required_block(n.body, n.lineno),
        self.as_block(n.orelse, n.lineno),
        target_type,
    )
    node.is_async = True
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1215"># While(expr test, stmt* body, stmt* orelse)
def visit_While(self, n: ast3.While) -&gt; WhileStmt:
    node = WhileStmt(
        self.visit(n.test),
        self.as_required_block(n.body, n.lineno),
        self.as_block(n.orelse, n.lineno),
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1216"># If(expr test, stmt* body, stmt* orelse)
def visit_If(self, n: ast3.If) -&gt; IfStmt:
    lineno = n.lineno
    node = IfStmt(
        [self.visit(n.test)],
        [self.as_required_block(n.body, lineno)],
        self.as_block(n.orelse, lineno),
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1217"># With(withitem* items, stmt* body, string? type_comment)
def visit_With(self, n: ast3.With) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = WithStmt(
        [self.visit(i.context_expr) for i in n.items],
        [self.visit(i.optional_vars) for i in n.items],
        self.as_required_block(n.body, n.lineno),
        target_type,
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1218"># AsyncWith(withitem* items, stmt* body, string? type_comment)
def visit_AsyncWith(self, n: ast3.AsyncWith) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    s = WithStmt(
        [self.visit(i.context_expr) for i in n.items],
        [self.visit(i.optional_vars) for i in n.items],
        self.as_required_block(n.body, n.lineno),
        target_type,
    )
    s.is_async = True
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.1219"># Raise(expr? exc, expr? cause)
def visit_Raise(self, n: ast3.Raise) -&gt; RaiseStmt:
    node = RaiseStmt(self.visit(n.exc), self.visit(n.cause))
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.122">@path C:/Repos/ekr-mypy2/misc/
from __future__ import annotations

from typing import Callable

from mypy.checker import TypeChecker
from mypy.nodes import TypeInfo
from mypy.plugin import FunctionContext, Plugin
from mypy.subtypes import is_proper_subtype
from mypy.types import (
    AnyType,
    CallableType,
    FunctionLike,
    Instance,
    NoneTyp,
    ProperType,
    TupleType,
    Type,
    UnionType,
    get_proper_type,
    get_proper_types,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1220"># Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)
def visit_Try(self, n: ast3.Try) -&gt; TryStmt:
    vs = [
        self.set_line(NameExpr(h.name), h) if h.name is not None else None for h in n.handlers
    ]
    types = [self.visit(h.type) for h in n.handlers]
    handlers = [self.as_required_block(h.body, h.lineno) for h in n.handlers]

    node = TryStmt(
        self.as_required_block(n.body, n.lineno),
        vs,
        types,
        handlers,
        self.as_block(n.orelse, n.lineno),
        self.as_block(n.finalbody, n.lineno),
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1221"># Assert(expr test, expr? msg)
def visit_Assert(self, n: ast3.Assert) -&gt; AssertStmt:
    node = AssertStmt(self.visit(n.test), self.visit(n.msg))
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1222"># Import(alias* names)
def visit_Import(self, n: ast3.Import) -&gt; Import:
    names: list[tuple[str, str | None]] = []
    for alias in n.names:
        name = self.translate_module_id(alias.name)
        asname = alias.asname
        if asname is None and name != alias.name:
            # if the module name has been translated (and it's not already
            # an explicit import-as), make it an implicit import-as the
            # original name
            asname = alias.name
        names.append((name, asname))
    i = Import(names)
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20221004064034.1223"># ImportFrom(identifier? module, alias* names, int? level)
def visit_ImportFrom(self, n: ast3.ImportFrom) -&gt; ImportBase:
    assert n.level is not None
    if len(n.names) == 1 and n.names[0].name == "*":
        mod = n.module if n.module is not None else ""
        i: ImportBase = ImportAll(mod, n.level)
    else:
        i = ImportFrom(
            self.translate_module_id(n.module) if n.module is not None else "",
            n.level,
            [(a.name, a.asname) for a in n.names],
        )
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20221004064034.1224"># Global(identifier* names)
def visit_Global(self, n: ast3.Global) -&gt; GlobalDecl:
    g = GlobalDecl(n.names)
    return self.set_line(g, n)

</t>
<t tx="ekr.20221004064034.1225"># Nonlocal(identifier* names)
def visit_Nonlocal(self, n: ast3.Nonlocal) -&gt; NonlocalDecl:
    d = NonlocalDecl(n.names)
    return self.set_line(d, n)

</t>
<t tx="ekr.20221004064034.1226"># Expr(expr value)
def visit_Expr(self, n: ast3.Expr) -&gt; ExpressionStmt:
    value = self.visit(n.value)
    node = ExpressionStmt(value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1227"># Pass
def visit_Pass(self, n: ast3.Pass) -&gt; PassStmt:
    s = PassStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.1228"># Break
def visit_Break(self, n: ast3.Break) -&gt; BreakStmt:
    s = BreakStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.1229"># Continue
def visit_Continue(self, n: ast3.Continue) -&gt; ContinueStmt:
    s = ContinueStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.123">class ProperTypePlugin(Plugin):
    """
    A plugin to ensure that every type is expanded before doing any special-casing.

    This solves the problem that we have hundreds of call sites like:

        if isinstance(typ, UnionType):
            ...  # special-case union

    But after introducing a new type TypeAliasType (and removing immediate expansion)
    all these became dangerous because typ may be e.g. an alias to union.
    """

    @others
</t>
<t tx="ekr.20221004064034.1230"># --- expr ---

</t>
<t tx="ekr.20221004064034.1231">def visit_NamedExpr(self, n: NamedExpr) -&gt; AssignmentExpr:
    s = AssignmentExpr(self.visit(n.target), self.visit(n.value))
    return self.set_line(s, n)

</t>
<t tx="ekr.20221004064034.1232"># BoolOp(boolop op, expr* values)
def visit_BoolOp(self, n: ast3.BoolOp) -&gt; OpExpr:
    # mypy translates (1 and 2 and 3) as (1 and (2 and 3))
    assert len(n.values) &gt;= 2
    op_node = n.op
    if isinstance(op_node, ast3.And):
        op = "and"
    elif isinstance(op_node, ast3.Or):
        op = "or"
    else:
        raise RuntimeError("unknown BoolOp " + str(type(n)))

    # potentially inefficient!
    return self.group(op, self.translate_expr_list(n.values), n)

</t>
<t tx="ekr.20221004064034.1233">def group(self, op: str, vals: list[Expression], n: ast3.expr) -&gt; OpExpr:
    if len(vals) == 2:
        e = OpExpr(op, vals[0], vals[1])
    else:
        e = OpExpr(op, vals[0], self.group(op, vals[1:], n))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1234"># BinOp(expr left, operator op, expr right)
def visit_BinOp(self, n: ast3.BinOp) -&gt; OpExpr:
    op = self.from_operator(n.op)

    if op is None:
        raise RuntimeError("cannot translate BinOp " + str(type(n.op)))

    e = OpExpr(op, self.visit(n.left), self.visit(n.right))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1235"># UnaryOp(unaryop op, expr operand)
def visit_UnaryOp(self, n: ast3.UnaryOp) -&gt; UnaryExpr:
    op = None
    if isinstance(n.op, ast3.Invert):
        op = "~"
    elif isinstance(n.op, ast3.Not):
        op = "not"
    elif isinstance(n.op, ast3.UAdd):
        op = "+"
    elif isinstance(n.op, ast3.USub):
        op = "-"

    if op is None:
        raise RuntimeError("cannot translate UnaryOp " + str(type(n.op)))

    e = UnaryExpr(op, self.visit(n.operand))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1236"># Lambda(arguments args, expr body)
def visit_Lambda(self, n: ast3.Lambda) -&gt; LambdaExpr:
    body = ast3.Return(n.body)
    body.lineno = n.body.lineno
    body.col_offset = n.body.col_offset

    e = LambdaExpr(
        self.transform_args(n.args, n.lineno), self.as_required_block([body], n.lineno)
    )
    e.set_line(n.lineno, n.col_offset)  # Overrides set_line -- can't use self.set_line
    return e

</t>
<t tx="ekr.20221004064034.1237"># IfExp(expr test, expr body, expr orelse)
def visit_IfExp(self, n: ast3.IfExp) -&gt; ConditionalExpr:
    e = ConditionalExpr(self.visit(n.test), self.visit(n.body), self.visit(n.orelse))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1238"># Dict(expr* keys, expr* values)
def visit_Dict(self, n: ast3.Dict) -&gt; DictExpr:
    e = DictExpr(
        list(zip(self.translate_opt_expr_list(n.keys), self.translate_expr_list(n.values)))
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1239"># Set(expr* elts)
def visit_Set(self, n: ast3.Set) -&gt; SetExpr:
    e = SetExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.124">def get_function_hook(self, fullname: str) -&gt; Callable[[FunctionContext], Type] | None:
    if fullname == "builtins.isinstance":
        return isinstance_proper_hook
    if fullname == "mypy.types.get_proper_type":
        return proper_type_hook
    if fullname == "mypy.types.get_proper_types":
        return proper_types_hook
    return None


</t>
<t tx="ekr.20221004064034.1240"># ListComp(expr elt, comprehension* generators)
def visit_ListComp(self, n: ast3.ListComp) -&gt; ListComprehension:
    e = ListComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1241"># SetComp(expr elt, comprehension* generators)
def visit_SetComp(self, n: ast3.SetComp) -&gt; SetComprehension:
    e = SetComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1242"># DictComp(expr key, expr value, comprehension* generators)
def visit_DictComp(self, n: ast3.DictComp) -&gt; DictionaryComprehension:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = DictionaryComprehension(
        self.visit(n.key), self.visit(n.value), targets, iters, ifs_list, is_async
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1243"># GeneratorExp(expr elt, comprehension* generators)
def visit_GeneratorExp(self, n: ast3.GeneratorExp) -&gt; GeneratorExpr:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = GeneratorExpr(self.visit(n.elt), targets, iters, ifs_list, is_async)
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1244"># Await(expr value)
def visit_Await(self, n: ast3.Await) -&gt; AwaitExpr:
    v = self.visit(n.value)
    e = AwaitExpr(v)
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1245"># Yield(expr? value)
def visit_Yield(self, n: ast3.Yield) -&gt; YieldExpr:
    e = YieldExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1246"># YieldFrom(expr value)
def visit_YieldFrom(self, n: ast3.YieldFrom) -&gt; YieldFromExpr:
    e = YieldFromExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1247"># Compare(expr left, cmpop* ops, expr* comparators)
def visit_Compare(self, n: ast3.Compare) -&gt; ComparisonExpr:
    operators = [self.from_comp_operator(o) for o in n.ops]
    operands = self.translate_expr_list([n.left] + n.comparators)
    e = ComparisonExpr(operators, operands)
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1248"># Call(expr func, expr* args, keyword* keywords)
# keyword = (identifier? arg, expr value)
def visit_Call(self, n: Call) -&gt; CallExpr:
    args = n.args
    keywords = n.keywords
    keyword_names = [k.arg for k in keywords]
    arg_types = self.translate_expr_list(
        [a.value if isinstance(a, Starred) else a for a in args] + [k.value for k in keywords]
    )
    arg_kinds = [ARG_STAR if type(a) is Starred else ARG_POS for a in args] + [
        ARG_STAR2 if arg is None else ARG_NAMED for arg in keyword_names
    ]
    e = CallExpr(
        self.visit(n.func),
        arg_types,
        arg_kinds,
        cast("List[Optional[str]]", [None] * len(args)) + keyword_names,
    )
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1249"># Constant(object value) -- a constant, in Python 3.8.
def visit_Constant(self, n: Constant) -&gt; Any:
    val = n.value
    e: Any = None
    if val is None:
        e = NameExpr("None")
    elif isinstance(val, str):
        e = StrExpr(n.s)
    elif isinstance(val, bytes):
        e = BytesExpr(bytes_to_human_readable_repr(n.s))
    elif isinstance(val, bool):  # Must check before int!
        e = NameExpr(str(val))
    elif isinstance(val, int):
        e = IntExpr(val)
    elif isinstance(val, float):
        e = FloatExpr(val)
    elif isinstance(val, complex):
        e = ComplexExpr(val)
    elif val is Ellipsis:
        e = EllipsisExpr()
    else:
        raise RuntimeError("Constant not implemented for " + str(type(val)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.125">def isinstance_proper_hook(ctx: FunctionContext) -&gt; Type:
    if len(ctx.arg_types) != 2 or not ctx.arg_types[1]:
        return ctx.default_return_type

    right = get_proper_type(ctx.arg_types[1][0])
    for arg in ctx.arg_types[0]:
        if (
            is_improper_type(arg) or isinstance(get_proper_type(arg), AnyType)
        ) and is_dangerous_target(right):
            if is_special_target(right):
                return ctx.default_return_type
            ctx.api.fail(
                "Never apply isinstance() to unexpanded types;"
                " use mypy.types.get_proper_type() first",
                ctx.context,
            )
            ctx.api.note(  # type: ignore[attr-defined]
                "If you pass on the original type"
                " after the check, always use its unexpanded version",
                ctx.context,
            )
    return ctx.default_return_type


</t>
<t tx="ekr.20221004064034.1250"># Num(object n) -- a number as a PyObject.
def visit_Num(self, n: ast3.Num) -&gt; IntExpr | FloatExpr | ComplexExpr:
    # The n field has the type complex, but complex isn't *really*
    # a parent of int and float, and this causes isinstance below
    # to think that the complex branch is always picked. Avoid
    # this by throwing away the type.
    val: object = n.n
    if isinstance(val, int):
        e: IntExpr | FloatExpr | ComplexExpr = IntExpr(val)
    elif isinstance(val, float):
        e = FloatExpr(val)
    elif isinstance(val, complex):
        e = ComplexExpr(val)
    else:
        raise RuntimeError("num not implemented for " + str(type(val)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1251"># Str(string s)
def visit_Str(self, n: Str) -&gt; StrExpr:
    e = StrExpr(n.s)
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1252"># JoinedStr(expr* values)
def visit_JoinedStr(self, n: ast3.JoinedStr) -&gt; Expression:
    # Each of n.values is a str or FormattedValue; we just concatenate
    # them all using ''.join.
    empty_string = StrExpr("")
    empty_string.set_line(n.lineno, n.col_offset)
    strs_to_join = ListExpr(self.translate_expr_list(n.values))
    strs_to_join.set_line(empty_string)
    # Don't make unnecessary join call if there is only one str to join
    if len(strs_to_join.items) == 1:
        return self.set_line(strs_to_join.items[0], n)
    join_method = MemberExpr(empty_string, "join")
    join_method.set_line(empty_string)
    result_expression = CallExpr(join_method, [strs_to_join], [ARG_POS], [None])
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20221004064034.1253"># FormattedValue(expr value)
def visit_FormattedValue(self, n: ast3.FormattedValue) -&gt; Expression:
    # A FormattedValue is a component of a JoinedStr, or it can exist
    # on its own. We translate them to individual '{}'.format(value)
    # calls. Format specifier and conversion information is passed along
    # to allow mypyc to support f-strings with format specifiers and conversions.
    val_exp = self.visit(n.value)
    val_exp.set_line(n.lineno, n.col_offset)
    conv_str = "" if n.conversion is None or n.conversion &lt; 0 else "!" + chr(n.conversion)
    format_string = StrExpr("{" + conv_str + ":{}}")
    format_spec_exp = self.visit(n.format_spec) if n.format_spec is not None else StrExpr("")
    format_string.set_line(n.lineno, n.col_offset)
    format_method = MemberExpr(format_string, "format")
    format_method.set_line(format_string)
    result_expression = CallExpr(
        format_method, [val_exp, format_spec_exp], [ARG_POS, ARG_POS], [None, None]
    )
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20221004064034.1254"># Bytes(bytes s)
def visit_Bytes(self, n: ast3.Bytes) -&gt; BytesExpr | StrExpr:
    e = BytesExpr(bytes_to_human_readable_repr(n.s))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1255"># NameConstant(singleton value)
def visit_NameConstant(self, n: NameConstant) -&gt; NameExpr:
    e = NameExpr(str(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1256"># Ellipsis
def visit_Ellipsis(self, n: ast3_Ellipsis) -&gt; EllipsisExpr:
    e = EllipsisExpr()
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1257"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; MemberExpr | SuperExpr:
    value = n.value
    member_expr = MemberExpr(self.visit(value), n.attr)
    obj = member_expr.expr
    if (
        isinstance(obj, CallExpr)
        and isinstance(obj.callee, NameExpr)
        and obj.callee.name == "super"
    ):
        e: MemberExpr | SuperExpr = SuperExpr(member_expr.name, obj)
    else:
        e = member_expr
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1258"># Subscript(expr value, slice slice, expr_context ctx)
def visit_Subscript(self, n: ast3.Subscript) -&gt; IndexExpr:
    e = IndexExpr(self.visit(n.value), self.visit(n.slice))
    self.set_line(e, n)
    # alias to please mypyc
    is_py38_or_earlier = sys.version_info &lt; (3, 9)
    if isinstance(n.slice, ast3.Slice) or (
        is_py38_or_earlier and isinstance(n.slice, ast3.ExtSlice)
    ):
        # Before Python 3.9, Slice has no line/column in the raw ast. To avoid incompatibility
        # visit_Slice doesn't set_line, even in Python 3.9 on.
        # ExtSlice also has no line/column info. In Python 3.9 on, line/column is set for
        # e.index when visiting n.slice.
        e.index.line = e.line
        e.index.column = e.column
    return e

</t>
<t tx="ekr.20221004064034.1259"># Starred(expr value, expr_context ctx)
def visit_Starred(self, n: Starred) -&gt; StarExpr:
    e = StarExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.126">def is_special_target(right: ProperType) -&gt; bool:
    """Whitelist some special cases for use in isinstance() with improper types."""
    if isinstance(right, FunctionLike) and right.is_type_obj():
        if right.type_object().fullname == "builtins.tuple":
            # Used with Union[Type, Tuple[Type, ...]].
            return True
        if right.type_object().fullname in (
            "mypy.types.Type",
            "mypy.types.ProperType",
            "mypy.types.TypeAliasType",
        ):
            # Special case: things like assert isinstance(typ, ProperType) are always OK.
            return True
        if right.type_object().fullname in (
            "mypy.types.UnboundType",
            "mypy.types.TypeVarLikeType",
            "mypy.types.TypeVarType",
            "mypy.types.UnpackType",
            "mypy.types.TypeVarTupleType",
            "mypy.types.ParamSpecType",
            "mypy.types.RawExpressionType",
            "mypy.types.EllipsisType",
            "mypy.types.StarType",
            "mypy.types.TypeList",
            "mypy.types.CallableArgument",
            "mypy.types.PartialType",
            "mypy.types.ErasedType",
            "mypy.types.DeletedType",
            "mypy.types.RequiredType",
        ):
            # Special case: these are not valid targets for a type alias and thus safe.
            # TODO: introduce a SyntheticType base to simplify this?
            return True
    elif isinstance(right, TupleType):
        return all(is_special_target(t) for t in get_proper_types(right.items))
    return False


</t>
<t tx="ekr.20221004064034.1260"># Name(identifier id, expr_context ctx)
def visit_Name(self, n: Name) -&gt; NameExpr:
    e = NameExpr(n.id)
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1261"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast3.List) -&gt; ListExpr | TupleExpr:
    expr_list: list[Expression] = [self.visit(e) for e in n.elts]
    if isinstance(n.ctx, ast3.Store):
        # [x, y] = z and (x, y) = z means exactly the same thing
        e: ListExpr | TupleExpr = TupleExpr(expr_list)
    else:
        e = ListExpr(expr_list)
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1262"># Tuple(expr* elts, expr_context ctx)
def visit_Tuple(self, n: ast3.Tuple) -&gt; TupleExpr:
    e = TupleExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20221004064034.1263"># --- slice ---

</t>
<t tx="ekr.20221004064034.1264"># Slice(expr? lower, expr? upper, expr? step)
def visit_Slice(self, n: ast3.Slice) -&gt; SliceExpr:
    return SliceExpr(self.visit(n.lower), self.visit(n.upper), self.visit(n.step))

</t>
<t tx="ekr.20221004064034.1265"># ExtSlice(slice* dims)
def visit_ExtSlice(self, n: ast3.ExtSlice) -&gt; TupleExpr:
    # cast for mypyc's benefit on Python 3.9
    return TupleExpr(self.translate_expr_list(cast(Any, n).dims))

</t>
<t tx="ekr.20221004064034.1266"># Index(expr value)
def visit_Index(self, n: Index) -&gt; Node:
    # cast for mypyc's benefit on Python 3.9
    value = self.visit(cast(Any, n).value)
    assert isinstance(value, Node)
    return value

</t>
<t tx="ekr.20221004064034.1267"># Match(expr subject, match_case* cases) # python 3.10 and later
def visit_Match(self, n: Match) -&gt; MatchStmt:
    node = MatchStmt(
        self.visit(n.subject),
        [self.visit(c.pattern) for c in n.cases],
        [self.visit(c.guard) for c in n.cases],
        [self.as_required_block(c.body, n.lineno) for c in n.cases],
    )
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1268">def visit_MatchValue(self, n: MatchValue) -&gt; ValuePattern:
    node = ValuePattern(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1269">def visit_MatchSingleton(self, n: MatchSingleton) -&gt; SingletonPattern:
    node = SingletonPattern(n.value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.127">def is_improper_type(typ: Type) -&gt; bool:
    """Is this a type that is not a subtype of ProperType?"""
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        info = typ.type
        return info.has_base("mypy.types.Type") and not info.has_base("mypy.types.ProperType")
    if isinstance(typ, UnionType):
        return any(is_improper_type(t) for t in typ.items)
    return False


</t>
<t tx="ekr.20221004064034.1270">def visit_MatchSequence(self, n: MatchSequence) -&gt; SequencePattern:
    patterns = [self.visit(p) for p in n.patterns]
    stars = [p for p in patterns if isinstance(p, StarredPattern)]
    assert len(stars) &lt; 2

    node = SequencePattern(patterns)
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1271">def visit_MatchStar(self, n: MatchStar) -&gt; StarredPattern:
    if n.name is None:
        node = StarredPattern(None)
    else:
        node = StarredPattern(NameExpr(n.name))

    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1272">def visit_MatchMapping(self, n: MatchMapping) -&gt; MappingPattern:
    keys = [self.visit(k) for k in n.keys]
    values = [self.visit(v) for v in n.patterns]

    if n.rest is None:
        rest = None
    else:
        rest = NameExpr(n.rest)

    node = MappingPattern(keys, values, rest)
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1273">def visit_MatchClass(self, n: MatchClass) -&gt; ClassPattern:
    class_ref = self.visit(n.cls)
    assert isinstance(class_ref, RefExpr)
    positionals = [self.visit(p) for p in n.patterns]
    keyword_keys = n.kwd_attrs
    keyword_values = [self.visit(p) for p in n.kwd_patterns]

    node = ClassPattern(class_ref, positionals, keyword_keys, keyword_values)
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1274"># MatchAs(expr pattern, identifier name)
def visit_MatchAs(self, n: MatchAs) -&gt; AsPattern:
    if n.name is None:
        name = None
    else:
        name = NameExpr(n.name)
        name = self.set_line(name, n)
    node = AsPattern(self.visit(n.pattern), name)
    return self.set_line(node, n)

</t>
<t tx="ekr.20221004064034.1275"># MatchOr(expr* pattern)
def visit_MatchOr(self, n: MatchOr) -&gt; OrPattern:
    node = OrPattern([self.visit(pattern) for pattern in n.patterns])
    return self.set_line(node, n)


</t>
<t tx="ekr.20221004064034.1276">class TypeConverter:
    @others
</t>
<t tx="ekr.20221004064034.1277">def __init__(
    self,
    errors: Errors | None,
    line: int = -1,
    override_column: int = -1,
    is_evaluated: bool = True,
) -&gt; None:
    self.errors = errors
    self.line = line
    self.override_column = override_column
    self.node_stack: list[AST] = []
    self.is_evaluated = is_evaluated

</t>
<t tx="ekr.20221004064034.1278">def convert_column(self, column: int) -&gt; int:
    """Apply column override if defined; otherwise return column.

    Column numbers are sometimes incorrect in the AST and the column
    override can be used to work around that.
    """
    if self.override_column &lt; 0:
        return column
    else:
        return self.override_column

</t>
<t tx="ekr.20221004064034.1279">def invalid_type(self, node: AST, note: str | None = None) -&gt; RawExpressionType:
    """Constructs a type representing some expression that normally forms an invalid type.
    For example, if we see a type hint that says "3 + 4", we would transform that
    expression into a RawExpressionType.

    The semantic analysis layer will report an "Invalid type" error when it
    encounters this type, along with the given note if one is provided.

    See RawExpressionType's docstring for more details on how it's used.
    """
    return RawExpressionType(
        None, "typing.Any", line=self.line, column=getattr(node, "col_offset", -1), note=note
    )

</t>
<t tx="ekr.20221004064034.128">def is_dangerous_target(typ: ProperType) -&gt; bool:
    """Is this a dangerous target (right argument) for an isinstance() check?"""
    if isinstance(typ, TupleType):
        return any(is_dangerous_target(get_proper_type(t)) for t in typ.items)
    if isinstance(typ, CallableType) and typ.is_type_obj():
        return typ.type_object().has_base("mypy.types.Type")
    return False


</t>
<t tx="ekr.20221004064034.1280">@overload
def visit(self, node: ast3.expr) -&gt; ProperType:
    ...

</t>
<t tx="ekr.20221004064034.1281">@overload
def visit(self, node: AST | None) -&gt; ProperType | None:
    ...

</t>
<t tx="ekr.20221004064034.1282">def visit(self, node: AST | None) -&gt; ProperType | None:
    """Modified visit -- keep track of the stack of nodes"""
    if node is None:
        return None
    self.node_stack.append(node)
    try:
        method = "visit_" + node.__class__.__name__
        visitor = getattr(self, method, None)
        if visitor is not None:
            typ = visitor(node)
            assert isinstance(typ, ProperType)
            return typ
        else:
            return self.invalid_type(node)
    finally:
        self.node_stack.pop()

</t>
<t tx="ekr.20221004064034.1283">def parent(self) -&gt; AST | None:
    """Return the AST node above the one we are processing"""
    if len(self.node_stack) &lt; 2:
        return None
    return self.node_stack[-2]

</t>
<t tx="ekr.20221004064034.1284">def fail(self, msg: str, line: int, column: int) -&gt; None:
    if self.errors:
        self.errors.report(line, column, msg, blocker=True, code=codes.SYNTAX)

</t>
<t tx="ekr.20221004064034.1285">def note(self, msg: str, line: int, column: int) -&gt; None:
    if self.errors:
        self.errors.report(line, column, msg, severity="note", code=codes.SYNTAX)

</t>
<t tx="ekr.20221004064034.1286">def translate_expr_list(self, l: Sequence[ast3.expr]) -&gt; list[Type]:
    return [self.visit(e) for e in l]

</t>
<t tx="ekr.20221004064034.1287">def visit_Call(self, e: Call) -&gt; Type:
    # Parse the arg constructor
    f = e.func
    constructor = stringify_name(f)

    if not isinstance(self.parent(), ast3.List):
        note = None
        if constructor:
            note = "Suggestion: use {0}[...] instead of {0}(...)".format(constructor)
        return self.invalid_type(e, note=note)
    if not constructor:
        self.fail("Expected arg constructor name", e.lineno, e.col_offset)

    name: str | None = None
    default_type = AnyType(TypeOfAny.special_form)
    typ: Type = default_type
    for i, arg in enumerate(e.args):
        if i == 0:
            converted = self.visit(arg)
            assert converted is not None
            typ = converted
        elif i == 1:
            name = self._extract_argument_name(arg)
        else:
            self.fail("Too many arguments for argument constructor", f.lineno, f.col_offset)
    for k in e.keywords:
        value = k.value
        if k.arg == "name":
            if name is not None:
                self.fail(
                    '"{}" gets multiple values for keyword argument "name"'.format(
                        constructor
                    ),
                    f.lineno,
                    f.col_offset,
                )
            name = self._extract_argument_name(value)
        elif k.arg == "type":
            if typ is not default_type:
                self.fail(
                    '"{}" gets multiple values for keyword argument "type"'.format(
                        constructor
                    ),
                    f.lineno,
                    f.col_offset,
                )
            converted = self.visit(value)
            assert converted is not None
            typ = converted
        else:
            self.fail(
                f'Unexpected argument "{k.arg}" for argument constructor',
                value.lineno,
                value.col_offset,
            )
    return CallableArgument(typ, name, constructor, e.lineno, e.col_offset)

</t>
<t tx="ekr.20221004064034.1288">def translate_argument_list(self, l: Sequence[ast3.expr]) -&gt; TypeList:
    return TypeList([self.visit(e) for e in l], line=self.line)

</t>
<t tx="ekr.20221004064034.1289">def _extract_argument_name(self, n: ast3.expr) -&gt; str | None:
    if isinstance(n, Str):
        return n.s.strip()
    elif isinstance(n, NameConstant) and str(n.value) == "None":
        return None
    self.fail(
        f"Expected string literal for argument name, got {type(n).__name__}", self.line, 0
    )
    return None

</t>
<t tx="ekr.20221004064034.129">def proper_type_hook(ctx: FunctionContext) -&gt; Type:
    """Check if this get_proper_type() call is not redundant."""
    arg_types = ctx.arg_types[0]
    if arg_types:
        arg_type = get_proper_type(arg_types[0])
        proper_type = get_proper_type_instance(ctx)
        if is_proper_subtype(arg_type, UnionType.make_union([NoneTyp(), proper_type])):
            # Minimize amount of spurious errors from overload machinery.
            # TODO: call the hook on the overload as a whole?
            if isinstance(arg_type, (UnionType, Instance)):
                ctx.api.fail("Redundant call to get_proper_type()", ctx.context)
    return ctx.default_return_type


</t>
<t tx="ekr.20221004064034.1290">def visit_Name(self, n: Name) -&gt; Type:
    return UnboundType(n.id, line=self.line, column=self.convert_column(n.col_offset))

</t>
<t tx="ekr.20221004064034.1291">def visit_BinOp(self, n: ast3.BinOp) -&gt; Type:
    if not isinstance(n.op, ast3.BitOr):
        return self.invalid_type(n)

    left = self.visit(n.left)
    right = self.visit(n.right)
    return UnionType(
        [left, right],
        line=self.line,
        column=self.convert_column(n.col_offset),
        is_evaluated=self.is_evaluated,
        uses_pep604_syntax=True,
    )

</t>
<t tx="ekr.20221004064034.1292">def visit_NameConstant(self, n: NameConstant) -&gt; Type:
    if isinstance(n.value, bool):
        return RawExpressionType(n.value, "builtins.bool", line=self.line)
    else:
        return UnboundType(str(n.value), line=self.line, column=n.col_offset)

</t>
<t tx="ekr.20221004064034.1293"># Only for 3.8 and newer
def visit_Constant(self, n: Constant) -&gt; Type:
    val = n.value
    if val is None:
        # None is a type.
        return UnboundType("None", line=self.line)
    if isinstance(val, str):
        # Parse forward reference.
        return parse_type_string(n.s, "builtins.str", self.line, n.col_offset)
    if val is Ellipsis:
        # '...' is valid in some types.
        return EllipsisType(line=self.line)
    if isinstance(val, bool):
        # Special case for True/False.
        return RawExpressionType(val, "builtins.bool", line=self.line)
    if isinstance(val, (int, float, complex)):
        return self.numeric_type(val, n)
    if isinstance(val, bytes):
        contents = bytes_to_human_readable_repr(val)
        return RawExpressionType(contents, "builtins.bytes", self.line, column=n.col_offset)
    # Everything else is invalid.
    return self.invalid_type(n)

</t>
<t tx="ekr.20221004064034.1294"># UnaryOp(op, operand)
def visit_UnaryOp(self, n: UnaryOp) -&gt; Type:
    # We support specifically Literal[-4] and nothing else.
    # For example, Literal[+4] or Literal[~6] is not supported.
    typ = self.visit(n.operand)
    if isinstance(typ, RawExpressionType) and isinstance(n.op, USub):
        if isinstance(typ.literal_value, int):
            typ.literal_value *= -1
            return typ
    return self.invalid_type(n)

</t>
<t tx="ekr.20221004064034.1295">def numeric_type(self, value: object, n: AST) -&gt; Type:
    # The node's field has the type complex, but complex isn't *really*
    # a parent of int and float, and this causes isinstance below
    # to think that the complex branch is always picked. Avoid
    # this by throwing away the type.
    if isinstance(value, int):
        numeric_value: int | None = value
        type_name = "builtins.int"
    else:
        # Other kinds of numbers (floats, complex) are not valid parameters for
        # RawExpressionType so we just pass in 'None' for now. We'll report the
        # appropriate error at a later stage.
        numeric_value = None
        type_name = f"builtins.{type(value).__name__}"
    return RawExpressionType(
        numeric_value, type_name, line=self.line, column=getattr(n, "col_offset", -1)
    )

</t>
<t tx="ekr.20221004064034.1296"># These next three methods are only used if we are on python &lt;
# 3.8, using typed_ast.  They are defined unconditionally because
# mypyc can't handle conditional method definitions.

</t>
<t tx="ekr.20221004064034.1297"># Num(number n)
def visit_Num(self, n: Num) -&gt; Type:
    return self.numeric_type(n.n, n)

</t>
<t tx="ekr.20221004064034.1298"># Str(string s)
def visit_Str(self, n: Str) -&gt; Type:
    return parse_type_string(n.s, "builtins.str", self.line, n.col_offset)

</t>
<t tx="ekr.20221004064034.1299"># Bytes(bytes s)
def visit_Bytes(self, n: Bytes) -&gt; Type:
    contents = bytes_to_human_readable_repr(n.s)
    return RawExpressionType(contents, "builtins.bytes", self.line, column=n.col_offset)

</t>
<t tx="ekr.20221004064034.13">def start_background_cmd(name: str) -&gt; Popen:
    cmd = cmds[name]
    proc = subprocess.Popen(cmd, stderr=subprocess.STDOUT, stdout=subprocess.PIPE)
    return proc


</t>
<t tx="ekr.20221004064034.130">def proper_types_hook(ctx: FunctionContext) -&gt; Type:
    """Check if this get_proper_types() call is not redundant."""
    arg_types = ctx.arg_types[0]
    if arg_types:
        arg_type = arg_types[0]
        proper_type = get_proper_type_instance(ctx)
        item_type = UnionType.make_union([NoneTyp(), proper_type])
        ok_type = ctx.api.named_generic_type("typing.Iterable", [item_type])
        if is_proper_subtype(arg_type, ok_type):
            ctx.api.fail("Redundant call to get_proper_types()", ctx.context)
    return ctx.default_return_type


</t>
<t tx="ekr.20221004064034.1300">def visit_Index(self, n: ast3.Index) -&gt; Type:
    # cast for mypyc's benefit on Python 3.9
    value = self.visit(cast(Any, n).value)
    assert isinstance(value, Type)
    return value

</t>
<t tx="ekr.20221004064034.1301">def visit_Slice(self, n: ast3.Slice) -&gt; Type:
    return self.invalid_type(n, note="did you mean to use ',' instead of ':' ?")

</t>
<t tx="ekr.20221004064034.1302"># Subscript(expr value, slice slice, expr_context ctx)  # Python 3.8 and before
# Subscript(expr value, expr slice, expr_context ctx)  # Python 3.9 and later
def visit_Subscript(self, n: ast3.Subscript) -&gt; Type:
    if sys.version_info &gt;= (3, 9):  # Really 3.9a5 or later
        sliceval: Any = n.slice
    # Python 3.8 or earlier use a different AST structure for subscripts
    elif isinstance(n.slice, ast3.Index):
        sliceval: Any = n.slice.value
    elif isinstance(n.slice, ast3.Slice):
        sliceval = copy.deepcopy(n.slice)  # so we don't mutate passed AST
        if getattr(sliceval, "col_offset", None) is None:
            # Fix column information so that we get Python 3.9+ message order
            sliceval.col_offset = sliceval.lower.col_offset
    else:
        assert isinstance(n.slice, ast3.ExtSlice)
        dims = copy.deepcopy(n.slice.dims)
        for s in dims:
            if getattr(s, "col_offset", None) is None:
                if isinstance(s, ast3.Index):
                    s.col_offset = s.value.col_offset  # type: ignore[attr-defined]
                elif isinstance(s, ast3.Slice):
                    assert s.lower is not None
                    s.col_offset = s.lower.col_offset  # type: ignore[attr-defined]
        sliceval = ast3.Tuple(dims, n.ctx)

    empty_tuple_index = False
    if isinstance(sliceval, ast3.Tuple):
        params = self.translate_expr_list(sliceval.elts)
        if len(sliceval.elts) == 0:
            empty_tuple_index = True
    else:
        params = [self.visit(sliceval)]

    value = self.visit(n.value)
    if isinstance(value, UnboundType) and not value.args:
        return UnboundType(
            value.name,
            params,
            line=self.line,
            column=value.column,
            empty_tuple_index=empty_tuple_index,
        )
    else:
        return self.invalid_type(n)

</t>
<t tx="ekr.20221004064034.1303">def visit_Tuple(self, n: ast3.Tuple) -&gt; Type:
    return TupleType(
        self.translate_expr_list(n.elts),
        _dummy_fallback,
        implicit=True,
        line=self.line,
        column=self.convert_column(n.col_offset),
    )

</t>
<t tx="ekr.20221004064034.1304"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; Type:
    before_dot = self.visit(n.value)

    if isinstance(before_dot, UnboundType) and not before_dot.args:
        return UnboundType(f"{before_dot.name}.{n.attr}", line=self.line)
    else:
        return self.invalid_type(n)

</t>
<t tx="ekr.20221004064034.1305"># Ellipsis
def visit_Ellipsis(self, n: ast3_Ellipsis) -&gt; Type:
    return EllipsisType(line=self.line)

</t>
<t tx="ekr.20221004064034.1306"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast3.List) -&gt; Type:
    assert isinstance(n.ctx, ast3.Load)
    return self.translate_argument_list(n.elts)


</t>
<t tx="ekr.20221004064034.1307">def stringify_name(n: AST) -&gt; str | None:
    if isinstance(n, Name):
        return n.id
    elif isinstance(n, Attribute):
        sv = stringify_name(n.value)
        if sv is not None:
            return f"{sv}.{n.attr}"
    return None  # Can't do it.
</t>
<t tx="ekr.20221004064034.1308">@path C:/Repos/ekr-mypy2/mypy/
"""Routines for finding the sources that mypy will check"""

from __future__ import annotations

import functools
import os
from typing import Sequence
from typing_extensions import Final

from mypy.fscache import FileSystemCache
from mypy.modulefinder import PYTHON_EXTENSIONS, BuildSource, matches_exclude, mypy_path
from mypy.options import Options

PY_EXTENSIONS: Final = tuple(PYTHON_EXTENSIONS)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1309">class InvalidSourceList(Exception):
    """Exception indicating a problem in the list of sources given to mypy."""


</t>
<t tx="ekr.20221004064034.131">def get_proper_type_instance(ctx: FunctionContext) -&gt; Instance:
    checker = ctx.api
    assert isinstance(checker, TypeChecker)
    types = checker.modules["mypy.types"]
    proper_type_info = types.names["ProperType"]
    assert isinstance(proper_type_info.node, TypeInfo)
    return Instance(proper_type_info.node, [])


</t>
<t tx="ekr.20221004064034.1310">def create_source_list(
    paths: Sequence[str],
    options: Options,
    fscache: FileSystemCache | None = None,
    allow_empty_dir: bool = False,
) -&gt; list[BuildSource]:
    """From a list of source files/directories, makes a list of BuildSources.

    Raises InvalidSourceList on errors.
    """
    fscache = fscache or FileSystemCache()
    finder = SourceFinder(fscache, options)

    sources = []
    for path in paths:
        path = os.path.normpath(path)
        if path.endswith(PY_EXTENSIONS):
            # Can raise InvalidSourceList if a directory doesn't have a valid module name.
            name, base_dir = finder.crawl_up(path)
            sources.append(BuildSource(path, name, None, base_dir))
        elif fscache.isdir(path):
            sub_sources = finder.find_sources_in_dir(path)
            if not sub_sources and not allow_empty_dir:
                raise InvalidSourceList(f"There are no .py[i] files in directory '{path}'")
            sources.extend(sub_sources)
        else:
            mod = os.path.basename(path) if options.scripts_are_modules else None
            sources.append(BuildSource(path, mod, None))
    return sources


</t>
<t tx="ekr.20221004064034.1311">def keyfunc(name: str) -&gt; tuple[bool, int, str]:
    """Determines sort order for directory listing.

    The desirable properties are:
    1) foo &lt; foo.pyi &lt; foo.py
    2) __init__.py[i] &lt; foo
    """
    base, suffix = os.path.splitext(name)
    for i, ext in enumerate(PY_EXTENSIONS):
        if suffix == ext:
            return (base != "__init__", i, base)
    return (base != "__init__", -1, name)


</t>
<t tx="ekr.20221004064034.1312">def normalise_package_base(root: str) -&gt; str:
    if not root:
        root = os.curdir
    root = os.path.abspath(root)
    if root.endswith(os.sep):
        root = root[:-1]
    return root


</t>
<t tx="ekr.20221004064034.1313">def get_explicit_package_bases(options: Options) -&gt; list[str] | None:
    """Returns explicit package bases to use if the option is enabled, or None if disabled.

    We currently use MYPYPATH and the current directory as the package bases. In the future,
    when --namespace-packages is the default could also use the values passed with the
    --package-root flag, see #9632.

    Values returned are normalised so we can use simple string comparisons in
    SourceFinder.is_explicit_package_base
    """
    if not options.explicit_package_bases:
        return None
    roots = mypy_path() + options.mypy_path + [os.getcwd()]
    return [normalise_package_base(root) for root in roots]


</t>
<t tx="ekr.20221004064034.1314">class SourceFinder:
    @others
</t>
<t tx="ekr.20221004064034.1315">def __init__(self, fscache: FileSystemCache, options: Options) -&gt; None:
    self.fscache = fscache
    self.explicit_package_bases = get_explicit_package_bases(options)
    self.namespace_packages = options.namespace_packages
    self.exclude = options.exclude
    self.verbosity = options.verbosity

</t>
<t tx="ekr.20221004064034.1316">def is_explicit_package_base(self, path: str) -&gt; bool:
    assert self.explicit_package_bases
    return normalise_package_base(path) in self.explicit_package_bases

</t>
<t tx="ekr.20221004064034.1317">def find_sources_in_dir(self, path: str) -&gt; list[BuildSource]:
    sources = []

    seen: set[str] = set()
    names = sorted(self.fscache.listdir(path), key=keyfunc)
    for name in names:
        # Skip certain names altogether
        if name in ("__pycache__", "site-packages", "node_modules") or name.startswith("."):
            continue
        subpath = os.path.join(path, name)

        if matches_exclude(subpath, self.exclude, self.fscache, self.verbosity &gt;= 2):
            continue

        if self.fscache.isdir(subpath):
            sub_sources = self.find_sources_in_dir(subpath)
            if sub_sources:
                seen.add(name)
                sources.extend(sub_sources)
        else:
            stem, suffix = os.path.splitext(name)
            if stem not in seen and suffix in PY_EXTENSIONS:
                seen.add(stem)
                module, base_dir = self.crawl_up(subpath)
                sources.append(BuildSource(subpath, module, None, base_dir))

    return sources

</t>
<t tx="ekr.20221004064034.1318">def crawl_up(self, path: str) -&gt; tuple[str, str]:
    """Given a .py[i] filename, return module and base directory.

    For example, given "xxx/yyy/foo/bar.py", we might return something like:
    ("foo.bar", "xxx/yyy")

    If namespace packages is off, we crawl upwards until we find a directory without
    an __init__.py

    If namespace packages is on, we crawl upwards until the nearest explicit base directory.
    Failing that, we return one past the highest directory containing an __init__.py

    We won't crawl past directories with invalid package names.
    The base directory returned is an absolute path.
    """
    path = os.path.abspath(path)
    parent, filename = os.path.split(path)

    module_name = strip_py(filename) or filename

    parent_module, base_dir = self.crawl_up_dir(parent)
    if module_name == "__init__":
        return parent_module, base_dir

    # Note that module_name might not actually be a valid identifier, but that's okay
    # Ignoring this possibility sidesteps some search path confusion
    module = module_join(parent_module, module_name)
    return module, base_dir

</t>
<t tx="ekr.20221004064034.1319">def crawl_up_dir(self, dir: str) -&gt; tuple[str, str]:
    return self._crawl_up_helper(dir) or ("", dir)

</t>
<t tx="ekr.20221004064034.132">def plugin(version: str) -&gt; type[ProperTypePlugin]:
    return ProperTypePlugin
</t>
<t tx="ekr.20221004064034.1320">@functools.lru_cache()  # noqa: B019
def _crawl_up_helper(self, dir: str) -&gt; tuple[str, str] | None:
    """Given a directory, maybe returns module and base directory.

    We return a non-None value if we were able to find something clearly intended as a base
    directory (as adjudicated by being an explicit base directory or by containing a package
    with __init__.py).

    This distinction is necessary for namespace packages, so that we know when to treat
    ourselves as a subpackage.
    """
    # stop crawling if we're an explicit base directory
    if self.explicit_package_bases is not None and self.is_explicit_package_base(dir):
        return "", dir

    parent, name = os.path.split(dir)
    if name.endswith("-stubs"):
        name = name[:-6]  # PEP-561 stub-only directory

    # recurse if there's an __init__.py
    init_file = self.get_init_file(dir)
    if init_file is not None:
        if not name.isidentifier():
            # in most cases the directory name is invalid, we'll just stop crawling upwards
            # but if there's an __init__.py in the directory, something is messed up
            raise InvalidSourceList(f"{name} is not a valid Python package name")
        # we're definitely a package, so we always return a non-None value
        mod_prefix, base_dir = self.crawl_up_dir(parent)
        return module_join(mod_prefix, name), base_dir

    # stop crawling if we're out of path components or our name is an invalid identifier
    if not name or not parent or not name.isidentifier():
        return None

    # stop crawling if namespace packages is off (since we don't have an __init__.py)
    if not self.namespace_packages:
        return None

    # at this point: namespace packages is on, we don't have an __init__.py and we're not an
    # explicit base directory
    result = self._crawl_up_helper(parent)
    if result is None:
        # we're not an explicit base directory and we don't have an __init__.py
        # and none of our parents are either, so return
        return None
    # one of our parents was an explicit base directory or had an __init__.py, so we're
    # definitely a subpackage! chain our name to the module.
    mod_prefix, base_dir = result
    return module_join(mod_prefix, name), base_dir

</t>
<t tx="ekr.20221004064034.1321">def get_init_file(self, dir: str) -&gt; str | None:
    """Check whether a directory contains a file named __init__.py[i].

    If so, return the file's name (with dir prefixed).  If not, return None.

    This prefers .pyi over .py (because of the ordering of PY_EXTENSIONS).
    """
    for ext in PY_EXTENSIONS:
        f = os.path.join(dir, "__init__" + ext)
        if self.fscache.isfile(f):
            return f
        if ext == ".py" and self.fscache.init_under_package_root(f):
            return f
    return None


</t>
<t tx="ekr.20221004064034.1322">def module_join(parent: str, child: str) -&gt; str:
    """Join module ids, accounting for a possibly empty parent."""
    if parent:
        return parent + "." + child
    return child


</t>
<t tx="ekr.20221004064034.1323">def strip_py(arg: str) -&gt; str | None:
    """Strip a trailing .py or .pyi suffix.

    Return None if no such suffix is found.
    """
    for ext in PY_EXTENSIONS:
        if arg.endswith(ext):
            return arg[: -len(ext)]
    return None
</t>
<t tx="ekr.20221004064034.1324">@path C:/Repos/ekr-mypy2/mypy/
"""Fix up various things after deserialization."""

from __future__ import annotations

from typing import Any
from typing_extensions import Final

from mypy.lookup import lookup_fully_qualified
from mypy.nodes import (
    Block,
    ClassDef,
    Decorator,
    FuncDef,
    MypyFile,
    OverloadedFuncDef,
    ParamSpecExpr,
    SymbolTable,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    TypeVarTupleExpr,
    Var,
)
from mypy.types import (
    NOT_READY,
    AnyType,
    CallableType,
    Instance,
    LiteralType,
    Overloaded,
    Parameters,
    ParamSpecType,
    TupleType,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UnionType,
    UnpackType,
)
from mypy.visitor import NodeVisitor


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1325"># N.B: we do a allow_missing fixup when fixing up a fine-grained
# incremental cache load (since there may be cross-refs into deleted
# modules)
def fixup_module(tree: MypyFile, modules: dict[str, MypyFile], allow_missing: bool) -&gt; None:
    node_fixer = NodeFixer(modules, allow_missing)
    node_fixer.visit_symbol_table(tree.names, tree.fullname)


</t>
<t tx="ekr.20221004064034.1326"># TODO: Fix up .info when deserializing, i.e. much earlier.
class NodeFixer(NodeVisitor[None]):
    current_info: TypeInfo | None = None

    @others
</t>
<t tx="ekr.20221004064034.1327">def __init__(self, modules: dict[str, MypyFile], allow_missing: bool) -&gt; None:
    self.modules = modules
    self.allow_missing = allow_missing
    self.type_fixer = TypeFixer(self.modules, allow_missing)

</t>
<t tx="ekr.20221004064034.1328"># NOTE: This method isn't (yet) part of the NodeVisitor API.
def visit_type_info(self, info: TypeInfo) -&gt; None:
    save_info = self.current_info
    try:
        self.current_info = info
        if info.defn:
            info.defn.accept(self)
        if info.names:
            self.visit_symbol_table(info.names, info.fullname)
        if info.bases:
            for base in info.bases:
                base.accept(self.type_fixer)
        if info._promote:
            for p in info._promote:
                p.accept(self.type_fixer)
        if info.tuple_type:
            info.tuple_type.accept(self.type_fixer)
            info.update_tuple_type(info.tuple_type)
        if info.typeddict_type:
            info.typeddict_type.accept(self.type_fixer)
            info.update_typeddict_type(info.typeddict_type)
        if info.declared_metaclass:
            info.declared_metaclass.accept(self.type_fixer)
        if info.metaclass_type:
            info.metaclass_type.accept(self.type_fixer)
        if info._mro_refs:
            info.mro = [
                lookup_fully_qualified_typeinfo(
                    self.modules, name, allow_missing=self.allow_missing
                )
                for name in info._mro_refs
            ]
            info._mro_refs = None
    finally:
        self.current_info = save_info

</t>
<t tx="ekr.20221004064034.1329"># NOTE: This method *definitely* isn't part of the NodeVisitor API.
def visit_symbol_table(self, symtab: SymbolTable, table_fullname: str) -&gt; None:
    # Copy the items because we may mutate symtab.
    for key, value in list(symtab.items()):
        cross_ref = value.cross_ref
        if cross_ref is not None:  # Fix up cross-reference.
            value.cross_ref = None
            if cross_ref in self.modules:
                value.node = self.modules[cross_ref]
            else:
                stnode = lookup_fully_qualified(
                    cross_ref, self.modules, raise_on_missing=not self.allow_missing
                )
                if stnode is not None:
                    assert stnode.node is not None, (table_fullname + "." + key, cross_ref)
                    value.node = stnode.node
                elif not self.allow_missing:
                    assert False, f"Could not find cross-ref {cross_ref}"
                else:
                    # We have a missing crossref in allow missing mode, need to put something
                    value.node = missing_info(self.modules)
        else:
            if isinstance(value.node, TypeInfo):
                # TypeInfo has no accept().  TODO: Add it?
                self.visit_type_info(value.node)
            elif value.node is not None:
                value.node.accept(self)
            else:
                assert False, f"Unexpected empty node {key!r}: {value}"

</t>
<t tx="ekr.20221004064034.133">@path C:/Repos/ekr-mypy2/misc/
"""Sync stdlib stubs (and a few other files) from typeshed.

Usage:

  python3 misc/sync-typeshed.py [--commit hash] [--typeshed-dir dir]

By default, sync to the latest typeshed commit.
"""

from __future__ import annotations

import argparse
import os
import shutil
import subprocess
import sys
import tempfile
import textwrap


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1330">def visit_func_def(self, func: FuncDef) -&gt; None:
    if self.current_info is not None:
        func.info = self.current_info
    if func.type is not None:
        func.type.accept(self.type_fixer)

</t>
<t tx="ekr.20221004064034.1331">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    if self.current_info is not None:
        o.info = self.current_info
    if o.type:
        o.type.accept(self.type_fixer)
    for item in o.items:
        item.accept(self)
    if o.impl:
        o.impl.accept(self)

</t>
<t tx="ekr.20221004064034.1332">def visit_decorator(self, d: Decorator) -&gt; None:
    if self.current_info is not None:
        d.var.info = self.current_info
    if d.func:
        d.func.accept(self)
    if d.var:
        d.var.accept(self)
    for node in d.decorators:
        node.accept(self)

</t>
<t tx="ekr.20221004064034.1333">def visit_class_def(self, c: ClassDef) -&gt; None:
    for v in c.type_vars:
        if isinstance(v, TypeVarType):
            for value in v.values:
                value.accept(self.type_fixer)
            v.upper_bound.accept(self.type_fixer)

</t>
<t tx="ekr.20221004064034.1334">def visit_type_var_expr(self, tv: TypeVarExpr) -&gt; None:
    for value in tv.values:
        value.accept(self.type_fixer)
    tv.upper_bound.accept(self.type_fixer)

</t>
<t tx="ekr.20221004064034.1335">def visit_paramspec_expr(self, p: ParamSpecExpr) -&gt; None:
    p.upper_bound.accept(self.type_fixer)

</t>
<t tx="ekr.20221004064034.1336">def visit_type_var_tuple_expr(self, tv: TypeVarTupleExpr) -&gt; None:
    tv.upper_bound.accept(self.type_fixer)

</t>
<t tx="ekr.20221004064034.1337">def visit_var(self, v: Var) -&gt; None:
    if self.current_info is not None:
        v.info = self.current_info
    if v.type is not None:
        v.type.accept(self.type_fixer)

</t>
<t tx="ekr.20221004064034.1338">def visit_type_alias(self, a: TypeAlias) -&gt; None:
    a.target.accept(self.type_fixer)


</t>
<t tx="ekr.20221004064034.1339">class TypeFixer(TypeVisitor[None]):
    @others
</t>
<t tx="ekr.20221004064034.134">def check_state() -&gt; None:
    if not os.path.isfile("README.md"):
        sys.exit("error: The current working directory must be the mypy repository root")
    out = subprocess.check_output(["git", "status", "-s", os.path.join("mypy", "typeshed")])
    if out:
        # If there are local changes under mypy/typeshed, they would be lost.
        sys.exit('error: Output of "git status -s mypy/typeshed" must be empty')


</t>
<t tx="ekr.20221004064034.1340">def __init__(self, modules: dict[str, MypyFile], allow_missing: bool) -&gt; None:
    self.modules = modules
    self.allow_missing = allow_missing

</t>
<t tx="ekr.20221004064034.1341">def visit_instance(self, inst: Instance) -&gt; None:
    # TODO: Combine Instances that are exactly the same?
    type_ref = inst.type_ref
    if type_ref is None:
        return  # We've already been here.
    inst.type_ref = None
    inst.type = lookup_fully_qualified_typeinfo(
        self.modules, type_ref, allow_missing=self.allow_missing
    )
    # TODO: Is this needed or redundant?
    # Also fix up the bases, just in case.
    for base in inst.type.bases:
        if base.type is NOT_READY:
            base.accept(self)
    for a in inst.args:
        a.accept(self)
    if inst.last_known_value is not None:
        inst.last_known_value.accept(self)

</t>
<t tx="ekr.20221004064034.1342">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    type_ref = t.type_ref
    if type_ref is None:
        return  # We've already been here.
    t.type_ref = None
    t.alias = lookup_fully_qualified_alias(
        self.modules, type_ref, allow_missing=self.allow_missing
    )
    for a in t.args:
        a.accept(self)

</t>
<t tx="ekr.20221004064034.1343">def visit_any(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20221004064034.1344">def visit_callable_type(self, ct: CallableType) -&gt; None:
    if ct.fallback:
        ct.fallback.accept(self)
    for argt in ct.arg_types:
        # argt may be None, e.g. for __self in NamedTuple constructors.
        if argt is not None:
            argt.accept(self)
    if ct.ret_type is not None:
        ct.ret_type.accept(self)
    for v in ct.variables:
        v.accept(self)
    for arg in ct.bound_args:
        if arg:
            arg.accept(self)
    if ct.type_guard is not None:
        ct.type_guard.accept(self)

</t>
<t tx="ekr.20221004064034.1345">def visit_overloaded(self, t: Overloaded) -&gt; None:
    for ct in t.items:
        ct.accept(self)

</t>
<t tx="ekr.20221004064034.1346">def visit_erased_type(self, o: Any) -&gt; None:
    # This type should exist only temporarily during type inference
    raise RuntimeError("Shouldn't get here", o)

</t>
<t tx="ekr.20221004064034.1347">def visit_deleted_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20221004064034.1348">def visit_none_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20221004064034.1349">def visit_uninhabited_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20221004064034.135">def update_typeshed(typeshed_dir: str, commit: str | None) -&gt; str:
    """Update contents of local typeshed copy.

    Return the normalized typeshed commit hash.
    """
    assert os.path.isdir(os.path.join(typeshed_dir, "stdlib"))
    assert os.path.isdir(os.path.join(typeshed_dir, "stubs"))
    if commit:
        subprocess.run(["git", "checkout", commit], check=True, cwd=typeshed_dir)
    commit = git_head_commit(typeshed_dir)
    stdlib_dir = os.path.join("mypy", "typeshed", "stdlib")
    # Remove existing stubs.
    shutil.rmtree(stdlib_dir)
    # Copy new stdlib stubs.
    shutil.copytree(os.path.join(typeshed_dir, "stdlib"), stdlib_dir)
    # Copy mypy_extensions stubs. We don't want to use a stub package, since it's
    # treated specially by mypy and we make assumptions about what's there.
    stubs_dir = os.path.join("mypy", "typeshed", "stubs")
    shutil.rmtree(stubs_dir)
    os.makedirs(stubs_dir)
    shutil.copytree(
        os.path.join(typeshed_dir, "stubs", "mypy-extensions"),
        os.path.join(stubs_dir, "mypy-extensions"),
    )
    shutil.copy(os.path.join(typeshed_dir, "LICENSE"), os.path.join("mypy", "typeshed"))
    return commit


</t>
<t tx="ekr.20221004064034.1350">def visit_partial_type(self, o: Any) -&gt; None:
    raise RuntimeError("Shouldn't get here", o)

</t>
<t tx="ekr.20221004064034.1351">def visit_tuple_type(self, tt: TupleType) -&gt; None:
    if tt.items:
        for it in tt.items:
            it.accept(self)
    if tt.partial_fallback is not None:
        tt.partial_fallback.accept(self)

</t>
<t tx="ekr.20221004064034.1352">def visit_typeddict_type(self, tdt: TypedDictType) -&gt; None:
    if tdt.items:
        for it in tdt.items.values():
            it.accept(self)
    if tdt.fallback is not None:
        if tdt.fallback.type_ref is not None:
            if (
                lookup_fully_qualified(
                    tdt.fallback.type_ref,
                    self.modules,
                    raise_on_missing=not self.allow_missing,
                )
                is None
            ):
                # We reject fake TypeInfos for TypedDict fallbacks because
                # the latter are used in type checking and must be valid.
                tdt.fallback.type_ref = "typing._TypedDict"
        tdt.fallback.accept(self)

</t>
<t tx="ekr.20221004064034.1353">def visit_literal_type(self, lt: LiteralType) -&gt; None:
    lt.fallback.accept(self)

</t>
<t tx="ekr.20221004064034.1354">def visit_type_var(self, tvt: TypeVarType) -&gt; None:
    if tvt.values:
        for vt in tvt.values:
            vt.accept(self)
    if tvt.upper_bound is not None:
        tvt.upper_bound.accept(self)

</t>
<t tx="ekr.20221004064034.1355">def visit_param_spec(self, p: ParamSpecType) -&gt; None:
    p.upper_bound.accept(self)

</t>
<t tx="ekr.20221004064034.1356">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; None:
    t.upper_bound.accept(self)

</t>
<t tx="ekr.20221004064034.1357">def visit_unpack_type(self, u: UnpackType) -&gt; None:
    u.type.accept(self)

</t>
<t tx="ekr.20221004064034.1358">def visit_parameters(self, p: Parameters) -&gt; None:
    for argt in p.arg_types:
        if argt is not None:
            argt.accept(self)
    for var in p.variables:
        var.accept(self)

</t>
<t tx="ekr.20221004064034.1359">def visit_unbound_type(self, o: UnboundType) -&gt; None:
    for a in o.args:
        a.accept(self)

</t>
<t tx="ekr.20221004064034.136">def git_head_commit(repo: str) -&gt; str:
    commit = subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=repo).decode("ascii")
    return commit.strip()


</t>
<t tx="ekr.20221004064034.1360">def visit_union_type(self, ut: UnionType) -&gt; None:
    if ut.items:
        for it in ut.items:
            it.accept(self)

</t>
<t tx="ekr.20221004064034.1361">def visit_void(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20221004064034.1362">def visit_type_type(self, t: TypeType) -&gt; None:
    t.item.accept(self)


</t>
<t tx="ekr.20221004064034.1363">def lookup_fully_qualified_typeinfo(
    modules: dict[str, MypyFile], name: str, *, allow_missing: bool
) -&gt; TypeInfo:
    stnode = lookup_fully_qualified(name, modules, raise_on_missing=not allow_missing)
    node = stnode.node if stnode else None
    if isinstance(node, TypeInfo):
        return node
    else:
        # Looks like a missing TypeInfo during an initial daemon load, put something there
        assert (
            allow_missing
        ), "Should never get here in normal mode, got {}:{} instead of TypeInfo".format(
            type(node).__name__, node.fullname if node else ""
        )
        return missing_info(modules)


</t>
<t tx="ekr.20221004064034.1364">def lookup_fully_qualified_alias(
    modules: dict[str, MypyFile], name: str, *, allow_missing: bool
) -&gt; TypeAlias:
    stnode = lookup_fully_qualified(name, modules, raise_on_missing=not allow_missing)
    node = stnode.node if stnode else None
    if isinstance(node, TypeAlias):
        return node
    elif isinstance(node, TypeInfo):
        if node.special_alias:
            # Already fixed up.
            return node.special_alias
        if node.tuple_type:
            alias = TypeAlias.from_tuple_type(node)
        elif node.typeddict_type:
            alias = TypeAlias.from_typeddict_type(node)
        else:
            assert allow_missing
            return missing_alias()
        node.special_alias = alias
        return alias
    else:
        # Looks like a missing TypeAlias during an initial daemon load, put something there
        assert (
            allow_missing
        ), "Should never get here in normal mode, got {}:{} instead of TypeAlias".format(
            type(node).__name__, node.fullname if node else ""
        )
        return missing_alias()


</t>
<t tx="ekr.20221004064034.1365">_SUGGESTION: Final = "&lt;missing {}: *should* have gone away during fine-grained update&gt;"


</t>
<t tx="ekr.20221004064034.1366">def missing_info(modules: dict[str, MypyFile]) -&gt; TypeInfo:
    suggestion = _SUGGESTION.format("info")
    dummy_def = ClassDef(suggestion, Block([]))
    dummy_def.fullname = suggestion

    info = TypeInfo(SymbolTable(), dummy_def, "&lt;missing&gt;")
    obj_type = lookup_fully_qualified_typeinfo(modules, "builtins.object", allow_missing=False)
    info.bases = [Instance(obj_type, [])]
    info.mro = [info, obj_type]
    return info


</t>
<t tx="ekr.20221004064034.1367">def missing_alias() -&gt; TypeAlias:
    suggestion = _SUGGESTION.format("alias")
    return TypeAlias(AnyType(TypeOfAny.special_form), suggestion, line=-1, column=-1)
</t>
<t tx="ekr.20221004064034.1368">@path C:/Repos/ekr-mypy2/mypy/
"""Generic node traverser visitor"""

from __future__ import annotations

from mypy.nodes import Block, MypyFile
from mypy.traverser import TraverserVisitor


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1369">class TreeFreer(TraverserVisitor):
    def visit_block(self, block: Block) -&gt; None:
        super().visit_block(block)
        block.body.clear()


</t>
<t tx="ekr.20221004064034.137">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--commit",
        default=None,
        help="Typeshed commit (default to latest master if using a repository clone)",
    )
    parser.add_argument(
        "--typeshed-dir",
        default=None,
        help="Location of typeshed (default to a temporary repository clone)",
    )
    args = parser.parse_args()
    check_state()
    print("Update contents of mypy/typeshed from typeshed? [yN] ", end="")
    answer = input()
    if answer.lower() != "y":
        sys.exit("Aborting")

    if not args.typeshed_dir:
        # Clone typeshed repo if no directory given.
        with tempfile.TemporaryDirectory() as tempdir:
            print(f"Cloning typeshed in {tempdir}...")
            subprocess.run(
                ["git", "clone", "https://github.com/python/typeshed.git"], check=True, cwd=tempdir
            )
            repo = os.path.join(tempdir, "typeshed")
            commit = update_typeshed(repo, args.commit)
    else:
        commit = update_typeshed(args.typeshed_dir, args.commit)

    assert commit

    # Create a commit
    message = textwrap.dedent(
        """\
        Sync typeshed

        Source commit:
        https://github.com/python/typeshed/commit/{commit}
        """.format(
            commit=commit
        )
    )
    subprocess.run(["git", "add", "--all", os.path.join("mypy", "typeshed")], check=True)
    subprocess.run(["git", "commit", "-m", message], check=True)
    print("Created typeshed sync commit.")


</t>
<t tx="ekr.20221004064034.1370">def free_tree(tree: MypyFile) -&gt; None:
    """Free all the ASTs associated with a module.

    This needs to be done recursively, since symbol tables contain
    references to definitions, so those won't be freed but we want their
    contents to be.
    """
    tree.accept(TreeFreer())
    tree.defs.clear()
</t>
<t tx="ekr.20221004064034.1371">@path C:/Repos/ekr-mypy2/mypy/
"""Interface for accessing the file system with automatic caching.

The idea is to cache the results of any file system state reads during
a single transaction. This has two main benefits:

* This avoids redundant syscalls, as we won't perform the same OS
  operations multiple times.

* This makes it easier to reason about concurrent FS updates, as different
  operations targeting the same paths can't report different state during
  a transaction.

Note that this only deals with reading state, not writing.

Properties maintained by the API:

* The contents of the file are always from the same or later time compared
  to the reported mtime of the file, even if mtime is queried after reading
  a file.

* Repeating an operation produces the same result as the first one during
  a transaction.

* Call flush() to start a new transaction (flush the caches).

The API is a bit limited. It's easy to add new cached operations, however.
You should perform all file system reads through the API to actually take
advantage of the benefits.
"""

from __future__ import annotations

import os
import stat

from mypy_extensions import mypyc_attr

from mypy.util import hash_digest


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1372">@mypyc_attr(allow_interpreted_subclasses=True)  # for tests
class FileSystemCache:
    @others
</t>
<t tx="ekr.20221004064034.1373">def __init__(self) -&gt; None:
    # The package root is not flushed with the caches.
    # It is set by set_package_root() below.
    self.package_root: list[str] = []
    self.flush()

</t>
<t tx="ekr.20221004064034.1374">def set_package_root(self, package_root: list[str]) -&gt; None:
    self.package_root = package_root

</t>
<t tx="ekr.20221004064034.1375">def flush(self) -&gt; None:
    """Start another transaction and empty all caches."""
    self.stat_cache: dict[str, os.stat_result] = {}
    self.stat_error_cache: dict[str, OSError] = {}
    self.listdir_cache: dict[str, list[str]] = {}
    self.listdir_error_cache: dict[str, OSError] = {}
    self.isfile_case_cache: dict[str, bool] = {}
    self.exists_case_cache: dict[str, bool] = {}
    self.read_cache: dict[str, bytes] = {}
    self.read_error_cache: dict[str, Exception] = {}
    self.hash_cache: dict[str, str] = {}
    self.fake_package_cache: set[str] = set()

</t>
<t tx="ekr.20221004064034.1376">def stat(self, path: str) -&gt; os.stat_result:
    if path in self.stat_cache:
        return self.stat_cache[path]
    if path in self.stat_error_cache:
        raise copy_os_error(self.stat_error_cache[path])
    try:
        st = os.stat(path)
    except OSError as err:
        if self.init_under_package_root(path):
            try:
                return self._fake_init(path)
            except OSError:
                pass
        # Take a copy to get rid of associated traceback and frame objects.
        # Just assigning to __traceback__ doesn't free them.
        self.stat_error_cache[path] = copy_os_error(err)
        raise err
    self.stat_cache[path] = st
    return st

</t>
<t tx="ekr.20221004064034.1377">def init_under_package_root(self, path: str) -&gt; bool:
    """Is this path an __init__.py under a package root?

    This is used to detect packages that don't contain __init__.py
    files, which is needed to support Bazel.  The function should
    only be called for non-existing files.

    It will return True if it refers to a __init__.py file that
    Bazel would create, so that at runtime Python would think the
    directory containing it is a package.  For this to work you
    must pass one or more package roots using the --package-root
    flag.

    As an exceptional case, any directory that is a package root
    itself will not be considered to contain a __init__.py file.
    This is different from the rules Bazel itself applies, but is
    necessary for mypy to properly distinguish packages from other
    directories.

    See https://docs.bazel.build/versions/master/be/python.html,
    where this behavior is described under legacy_create_init.
    """
    if not self.package_root:
        return False
    dirname, basename = os.path.split(path)
    if basename != "__init__.py":
        return False
    if not os.path.basename(dirname).isidentifier():
        # Can't put an __init__.py in a place that's not an identifier
        return False
    try:
        st = self.stat(dirname)
    except OSError:
        return False
    else:
        if not stat.S_ISDIR(st.st_mode):
            return False
    ok = False
    drive, path = os.path.splitdrive(path)  # Ignore Windows drive name
    if os.path.isabs(path):
        path = os.path.relpath(path)
    path = os.path.normpath(path)
    for root in self.package_root:
        if path.startswith(root):
            if path == root + basename:
                # A package root itself is never a package.
                ok = False
                break
            else:
                ok = True
    return ok

</t>
<t tx="ekr.20221004064034.1378">def _fake_init(self, path: str) -&gt; os.stat_result:
    """Prime the cache with a fake __init__.py file.

    This makes code that looks for path believe an empty file by
    that name exists.  Should only be called after
    init_under_package_root() returns True.
    """
    dirname, basename = os.path.split(path)
    assert basename == "__init__.py", path
    assert not os.path.exists(path), path  # Not cached!
    dirname = os.path.normpath(dirname)
    st = self.stat(dirname)  # May raise OSError
    # Get stat result as a list so we can modify it.
    seq: list[float] = list(st)
    seq[stat.ST_MODE] = stat.S_IFREG | 0o444
    seq[stat.ST_INO] = 1
    seq[stat.ST_NLINK] = 1
    seq[stat.ST_SIZE] = 0
    st = os.stat_result(seq)
    self.stat_cache[path] = st
    # Make listdir() and read() also pretend this file exists.
    self.fake_package_cache.add(dirname)
    return st

</t>
<t tx="ekr.20221004064034.1379">def listdir(self, path: str) -&gt; list[str]:
    path = os.path.normpath(path)
    if path in self.listdir_cache:
        res = self.listdir_cache[path]
        # Check the fake cache.
        if path in self.fake_package_cache and "__init__.py" not in res:
            res.append("__init__.py")  # Updates the result as well as the cache
        return res
    if path in self.listdir_error_cache:
        raise copy_os_error(self.listdir_error_cache[path])
    try:
        results = os.listdir(path)
    except OSError as err:
        # Like above, take a copy to reduce memory use.
        self.listdir_error_cache[path] = copy_os_error(err)
        raise err
    self.listdir_cache[path] = results
    # Check the fake cache.
    if path in self.fake_package_cache and "__init__.py" not in results:
        results.append("__init__.py")
    return results

</t>
<t tx="ekr.20221004064034.138">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
"""Upload mypy packages to PyPI.

You must first tag the release, use `git push --tags` and wait for the wheel build in CI to complete.

"""

from __future__ import annotations

import argparse
import contextlib
import json
import re
import shutil
import subprocess
import tarfile
import tempfile
import venv
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, Iterator
from urllib.request import urlopen

BASE = "https://api.github.com/repos"
REPO = "mypyc/mypy_mypyc-wheels"


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1380">def isfile(self, path: str) -&gt; bool:
    try:
        st = self.stat(path)
    except OSError:
        return False
    return stat.S_ISREG(st.st_mode)

</t>
<t tx="ekr.20221004064034.1381">def isfile_case(self, path: str, prefix: str) -&gt; bool:
    """Return whether path exists and is a file.

    On case-insensitive filesystems (like Mac or Windows) this returns
    False if the case of path's last component does not exactly match
    the case found in the filesystem.

    We check also the case of other path components up to prefix.
    For example, if path is 'user-stubs/pack/mod.pyi' and prefix is 'user-stubs',
    we check that the case of 'pack' and 'mod.py' matches exactly, 'user-stubs' will be
    case insensitive on case insensitive filesystems.

    The caller must ensure that prefix is a valid file system prefix of path.
    """
    if not self.isfile(path):
        # Fast path
        return False
    if path in self.isfile_case_cache:
        return self.isfile_case_cache[path]
    head, tail = os.path.split(path)
    if not tail:
        self.isfile_case_cache[path] = False
        return False
    try:
        names = self.listdir(head)
        # This allows one to check file name case sensitively in
        # case-insensitive filesystems.
        res = tail in names
    except OSError:
        res = False
    if res:
        # Also recursively check the other path components in case sensitive way.
        res = self.exists_case(head, prefix)
    self.isfile_case_cache[path] = res
    return res

</t>
<t tx="ekr.20221004064034.1382">def exists_case(self, path: str, prefix: str) -&gt; bool:
    """Return whether path exists - checking path components in case sensitive
    fashion, up to prefix.
    """
    if path in self.exists_case_cache:
        return self.exists_case_cache[path]
    head, tail = os.path.split(path)
    if not head.startswith(prefix) or not tail:
        # Only perform the check for paths under prefix.
        self.exists_case_cache[path] = True
        return True
    try:
        names = self.listdir(head)
        # This allows one to check file name case sensitively in
        # case-insensitive filesystems.
        res = tail in names
    except OSError:
        res = False
    if res:
        # Also recursively check other path components.
        res = self.exists_case(head, prefix)
    self.exists_case_cache[path] = res
    return res

</t>
<t tx="ekr.20221004064034.1383">def isdir(self, path: str) -&gt; bool:
    try:
        st = self.stat(path)
    except OSError:
        return False
    return stat.S_ISDIR(st.st_mode)

</t>
<t tx="ekr.20221004064034.1384">def exists(self, path: str) -&gt; bool:
    try:
        self.stat(path)
    except FileNotFoundError:
        return False
    return True

</t>
<t tx="ekr.20221004064034.1385">def read(self, path: str) -&gt; bytes:
    if path in self.read_cache:
        return self.read_cache[path]
    if path in self.read_error_cache:
        raise self.read_error_cache[path]

    # Need to stat first so that the contents of file are from no
    # earlier instant than the mtime reported by self.stat().
    self.stat(path)

    dirname, basename = os.path.split(path)
    dirname = os.path.normpath(dirname)
    # Check the fake cache.
    if basename == "__init__.py" and dirname in self.fake_package_cache:
        data = b""
    else:
        try:
            with open(path, "rb") as f:
                data = f.read()
        except OSError as err:
            self.read_error_cache[path] = err
            raise

    self.read_cache[path] = data
    self.hash_cache[path] = hash_digest(data)
    return data

</t>
<t tx="ekr.20221004064034.1386">def hash_digest(self, path: str) -&gt; str:
    if path not in self.hash_cache:
        self.read(path)
    return self.hash_cache[path]

</t>
<t tx="ekr.20221004064034.1387">def samefile(self, f1: str, f2: str) -&gt; bool:
    s1 = self.stat(f1)
    s2 = self.stat(f2)
    return os.path.samestat(s1, s2)


</t>
<t tx="ekr.20221004064034.1388">def copy_os_error(e: OSError) -&gt; OSError:
    new = OSError(*e.args)
    new.errno = e.errno
    new.strerror = e.strerror
    new.filename = e.filename
    if e.filename2:
        new.filename2 = e.filename2
    return new
</t>
<t tx="ekr.20221004064034.1389">@path C:/Repos/ekr-mypy2/mypy/
"""Watch parts of the file system for changes."""

from __future__ import annotations

from typing import AbstractSet, Iterable, NamedTuple

from mypy.fscache import FileSystemCache


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.139">def is_whl_or_tar(name: str) -&gt; bool:
    return name.endswith(".tar.gz") or name.endswith(".whl")


</t>
<t tx="ekr.20221004064034.1390">class FileData(NamedTuple):
    st_mtime: float
    st_size: int
    hash: str


</t>
<t tx="ekr.20221004064034.1391">class FileSystemWatcher:
    """Watcher for file system changes among specific paths.

    All file system access is performed using FileSystemCache. We
    detect changed files by stat()ing them all and comparing hashes
    of potentially changed files. If a file has both size and mtime
    unmodified, the file is assumed to be unchanged.

    An important goal of this class is to make it easier to eventually
    use file system events to detect file changes.

    Note: This class doesn't flush the file system cache. If you don't
    manually flush it, changes won't be seen.
    """

    # TODO: Watching directories?
    # TODO: Handle non-files

    @others
</t>
<t tx="ekr.20221004064034.1392">def __init__(self, fs: FileSystemCache) -&gt; None:
    self.fs = fs
    self._paths: set[str] = set()
    self._file_data: dict[str, FileData | None] = {}

</t>
<t tx="ekr.20221004064034.1393">def dump_file_data(self) -&gt; dict[str, tuple[float, int, str]]:
    return {k: v for k, v in self._file_data.items() if v is not None}

</t>
<t tx="ekr.20221004064034.1394">def set_file_data(self, path: str, data: FileData) -&gt; None:
    self._file_data[path] = data

</t>
<t tx="ekr.20221004064034.1395">def add_watched_paths(self, paths: Iterable[str]) -&gt; None:
    for path in paths:
        if path not in self._paths:
            # By storing None this path will get reported as changed by
            # find_changed if it exists.
            self._file_data[path] = None
    self._paths |= set(paths)

</t>
<t tx="ekr.20221004064034.1396">def remove_watched_paths(self, paths: Iterable[str]) -&gt; None:
    for path in paths:
        if path in self._file_data:
            del self._file_data[path]
    self._paths -= set(paths)

</t>
<t tx="ekr.20221004064034.1397">def _update(self, path: str) -&gt; None:
    st = self.fs.stat(path)
    hash_digest = self.fs.hash_digest(path)
    self._file_data[path] = FileData(st.st_mtime, st.st_size, hash_digest)

</t>
<t tx="ekr.20221004064034.1398">def _find_changed(self, paths: Iterable[str]) -&gt; AbstractSet[str]:
    changed = set()
    for path in paths:
        old = self._file_data[path]
        try:
            st = self.fs.stat(path)
        except FileNotFoundError:
            if old is not None:
                # File was deleted.
                changed.add(path)
                self._file_data[path] = None
        else:
            if old is None:
                # File is new.
                changed.add(path)
                self._update(path)
            # Round mtimes down, to match the mtimes we write to meta files
            elif st.st_size != old.st_size or int(st.st_mtime) != int(old.st_mtime):
                # Only look for changes if size or mtime has changed as an
                # optimization, since calculating hash is expensive.
                new_hash = self.fs.hash_digest(path)
                self._update(path)
                if st.st_size != old.st_size or new_hash != old.hash:
                    # Changed file.
                    changed.add(path)
    return changed

</t>
<t tx="ekr.20221004064034.1399">def find_changed(self) -&gt; AbstractSet[str]:
    """Return paths that have changes since the last call, in the watched set."""
    return self._find_changed(self._paths)

</t>
<t tx="ekr.20221004064034.14">def wait_background_cmd(name: str, proc: Popen) -&gt; int:
    output = proc.communicate()[0]
    status = proc.returncode
    print(f"run {name}: {cmds[name]}")
    if status:
        print(output.decode().rstrip())
        print("\nFAILED:", name)
        if name in FAST_FAIL:
            exit(status)
    return status


</t>
<t tx="ekr.20221004064034.140">def get_release_for_tag(tag: str) -&gt; dict[str, Any]:
    with urlopen(f"{BASE}/{REPO}/releases/tags/{tag}") as f:
        data = json.load(f)
    assert isinstance(data, dict)
    assert data["tag_name"] == tag
    return data


</t>
<t tx="ekr.20221004064034.1400">def update_changed(self, remove: list[str], update: list[str]) -&gt; AbstractSet[str]:
    """Alternative to find_changed() given explicit changes.

    This only calls self.fs.stat() on added or updated files, not
    on all files.  It believes all other files are unchanged!

    Implies add_watched_paths() for add and update, and
    remove_watched_paths() for remove.
    """
    self.remove_watched_paths(remove)
    self.add_watched_paths(update)
    return self._find_changed(update)
</t>
<t tx="ekr.20221004064034.1401">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import gc
import time
from typing import Mapping


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1402">class GcLogger:
    """Context manager to log GC stats and overall time."""

    @others
</t>
<t tx="ekr.20221004064034.1403">def __enter__(self) -&gt; GcLogger:
    self.gc_start_time: float | None = None
    self.gc_time = 0.0
    self.gc_calls = 0
    self.gc_collected = 0
    self.gc_uncollectable = 0
    gc.callbacks.append(self.gc_callback)
    self.start_time = time.time()
    return self

</t>
<t tx="ekr.20221004064034.1404">def gc_callback(self, phase: str, info: Mapping[str, int]) -&gt; None:
    if phase == "start":
        assert self.gc_start_time is None, "Start phase out of sequence"
        self.gc_start_time = time.time()
    elif phase == "stop":
        assert self.gc_start_time is not None, "Stop phase out of sequence"
        self.gc_calls += 1
        self.gc_time += time.time() - self.gc_start_time
        self.gc_start_time = None
        self.gc_collected += info["collected"]
        self.gc_uncollectable += info["uncollectable"]
    else:
        assert False, f"Unrecognized gc phase ({phase!r})"

</t>
<t tx="ekr.20221004064034.1405">def __exit__(self, *args: object) -&gt; None:
    while self.gc_callback in gc.callbacks:
        gc.callbacks.remove(self.gc_callback)

</t>
<t tx="ekr.20221004064034.1406">def get_stats(self) -&gt; Mapping[str, float]:
    end_time = time.time()
    result = {}
    result["gc_time"] = self.gc_time
    result["gc_calls"] = self.gc_calls
    result["gc_collected"] = self.gc_collected
    result["gc_uncollectable"] = self.gc_uncollectable
    result["build_time"] = end_time - self.start_time
    return result
</t>
<t tx="ekr.20221004064034.1407">@path C:/Repos/ekr-mypy2/mypy/
"""Git utilities."""

# Used also from setup.py, so don't pull in anything additional here (like mypy or typing):
from __future__ import annotations

import os
import subprocess


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1408">def is_git_repo(dir: str) -&gt; bool:
    """Is the given directory version-controlled with git?"""
    return os.path.exists(os.path.join(dir, ".git"))


</t>
<t tx="ekr.20221004064034.1409">def have_git() -&gt; bool:
    """Can we run the git executable?"""
    try:
        subprocess.check_output(["git", "--help"])
        return True
    except subprocess.CalledProcessError:
        return False
    except OSError:
        return False


</t>
<t tx="ekr.20221004064034.141">def download_asset(asset: dict[str, Any], dst: Path) -&gt; Path:
    name = asset["name"]
    assert isinstance(name, str)
    download_url = asset["browser_download_url"]
    assert is_whl_or_tar(name)
    with urlopen(download_url) as src_file:
        with open(dst / name, "wb") as dst_file:
            shutil.copyfileobj(src_file, dst_file)
    return dst / name


</t>
<t tx="ekr.20221004064034.1410">def git_revision(dir: str) -&gt; bytes:
    """Get the SHA-1 of the HEAD of a git repository."""
    return subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=dir).strip()


</t>
<t tx="ekr.20221004064034.1411">def is_dirty(dir: str) -&gt; bool:
    """Check whether a git repository has uncommitted changes."""
    output = subprocess.check_output(["git", "status", "-uno", "--porcelain"], cwd=dir)
    return output.strip() != b""
</t>
<t tx="ekr.20221004064034.1412">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Iterable, Set

import mypy.types as types
from mypy.types import TypeVisitor
from mypy.util import split_module_names


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1413">def extract_module_names(type_name: str | None) -&gt; list[str]:
    """Returns the module names of a fully qualified type name."""
    if type_name is not None:
        # Discard the first one, which is just the qualified name of the type
        possible_module_names = split_module_names(type_name)
        return possible_module_names[1:]
    else:
        return []


</t>
<t tx="ekr.20221004064034.1414">class TypeIndirectionVisitor(TypeVisitor[Set[str]]):
    """Returns all module references within a particular type."""

    @others
</t>
<t tx="ekr.20221004064034.1415">def __init__(self) -&gt; None:
    self.cache: dict[types.Type, set[str]] = {}
    self.seen_aliases: set[types.TypeAliasType] = set()

</t>
<t tx="ekr.20221004064034.1416">def find_modules(self, typs: Iterable[types.Type]) -&gt; set[str]:
    self.seen_aliases.clear()
    return self._visit(typs)

</t>
<t tx="ekr.20221004064034.1417">def _visit(self, typ_or_typs: types.Type | Iterable[types.Type]) -&gt; set[str]:
    typs = [typ_or_typs] if isinstance(typ_or_typs, types.Type) else typ_or_typs
    output: set[str] = set()
    for typ in typs:
        if isinstance(typ, types.TypeAliasType):
            # Avoid infinite recursion for recursive type aliases.
            if typ in self.seen_aliases:
                continue
            self.seen_aliases.add(typ)
        if typ in self.cache:
            modules = self.cache[typ]
        else:
            modules = typ.accept(self)
            self.cache[typ] = set(modules)
        output.update(modules)
    return output

</t>
<t tx="ekr.20221004064034.1418">def visit_unbound_type(self, t: types.UnboundType) -&gt; set[str]:
    return self._visit(t.args)

</t>
<t tx="ekr.20221004064034.1419">def visit_any(self, t: types.AnyType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20221004064034.142">def download_all_release_assets(release: dict[str, Any], dst: Path) -&gt; None:
    print("Downloading assets...")
    with ThreadPoolExecutor() as e:
        for asset in e.map(lambda asset: download_asset(asset, dst), release["assets"]):
            print(f"Downloaded {asset}")


</t>
<t tx="ekr.20221004064034.1420">def visit_none_type(self, t: types.NoneType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20221004064034.1421">def visit_uninhabited_type(self, t: types.UninhabitedType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20221004064034.1422">def visit_erased_type(self, t: types.ErasedType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20221004064034.1423">def visit_deleted_type(self, t: types.DeletedType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20221004064034.1424">def visit_type_var(self, t: types.TypeVarType) -&gt; set[str]:
    return self._visit(t.values) | self._visit(t.upper_bound)

</t>
<t tx="ekr.20221004064034.1425">def visit_param_spec(self, t: types.ParamSpecType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20221004064034.1426">def visit_type_var_tuple(self, t: types.TypeVarTupleType) -&gt; set[str]:
    return self._visit(t.upper_bound)

</t>
<t tx="ekr.20221004064034.1427">def visit_unpack_type(self, t: types.UnpackType) -&gt; set[str]:
    return t.type.accept(self)

</t>
<t tx="ekr.20221004064034.1428">def visit_parameters(self, t: types.Parameters) -&gt; set[str]:
    return self._visit(t.arg_types)

</t>
<t tx="ekr.20221004064034.1429">def visit_instance(self, t: types.Instance) -&gt; set[str]:
    out = self._visit(t.args)
    if t.type:
        # Uses of a class depend on everything in the MRO,
        # as changes to classes in the MRO can add types to methods,
        # change property types, change the MRO itself, etc.
        for s in t.type.mro:
            out.update(split_module_names(s.module_name))
        if t.type.metaclass_type is not None:
            out.update(split_module_names(t.type.metaclass_type.type.module_name))
    return out

</t>
<t tx="ekr.20221004064034.143">def check_sdist(dist: Path, version: str) -&gt; None:
    tarfiles = list(dist.glob("*.tar.gz"))
    assert len(tarfiles) == 1
    sdist = tarfiles[0]
    assert version in sdist.name
    with tarfile.open(sdist) as f:
        version_py = f.extractfile(f"{sdist.name[:-len('.tar.gz')]}/mypy/version.py")
        assert version_py is not None
        version_py_contents = version_py.read().decode("utf-8")

        # strip a git hash from our version, if necessary, since that's not present in version.py
        match = re.match(r"(.*\+dev).*$", version)
        hashless_version = match.group(1) if match else version

        assert (
            f"'{hashless_version}'" in version_py_contents
        ), "Version does not match version.py in sdist"


</t>
<t tx="ekr.20221004064034.1430">def visit_callable_type(self, t: types.CallableType) -&gt; set[str]:
    out = self._visit(t.arg_types) | self._visit(t.ret_type)
    if t.definition is not None:
        out.update(extract_module_names(t.definition.fullname))
    return out

</t>
<t tx="ekr.20221004064034.1431">def visit_overloaded(self, t: types.Overloaded) -&gt; set[str]:
    return self._visit(t.items) | self._visit(t.fallback)

</t>
<t tx="ekr.20221004064034.1432">def visit_tuple_type(self, t: types.TupleType) -&gt; set[str]:
    return self._visit(t.items) | self._visit(t.partial_fallback)

</t>
<t tx="ekr.20221004064034.1433">def visit_typeddict_type(self, t: types.TypedDictType) -&gt; set[str]:
    return self._visit(t.items.values()) | self._visit(t.fallback)

</t>
<t tx="ekr.20221004064034.1434">def visit_literal_type(self, t: types.LiteralType) -&gt; set[str]:
    return self._visit(t.fallback)

</t>
<t tx="ekr.20221004064034.1435">def visit_union_type(self, t: types.UnionType) -&gt; set[str]:
    return self._visit(t.items)

</t>
<t tx="ekr.20221004064034.1436">def visit_partial_type(self, t: types.PartialType) -&gt; set[str]:
    return set()

</t>
<t tx="ekr.20221004064034.1437">def visit_type_type(self, t: types.TypeType) -&gt; set[str]:
    return self._visit(t.item)

</t>
<t tx="ekr.20221004064034.1438">def visit_type_alias_type(self, t: types.TypeAliasType) -&gt; set[str]:
    return self._visit(types.get_proper_type(t))
</t>
<t tx="ekr.20221004064034.1439">@path C:/Repos/ekr-mypy2/mypy/
"""Utilities for type argument inference."""

from __future__ import annotations

from typing import NamedTuple, Sequence

from mypy.constraints import (
    SUBTYPE_OF,
    SUPERTYPE_OF,
    infer_constraints,
    infer_constraints_for_callable,
)
from mypy.nodes import ArgKind
from mypy.solve import solve_constraints
from mypy.types import CallableType, Instance, Type, TypeVarId


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.144">def spot_check_dist(dist: Path, version: str) -&gt; None:
    items = [item for item in dist.iterdir() if is_whl_or_tar(item.name)]
    assert len(items) &gt; 10
    assert all(version in item.name for item in items)
    assert any(item.name.endswith("py3-none-any.whl") for item in items)


</t>
<t tx="ekr.20221004064034.1440">class ArgumentInferContext(NamedTuple):
    """Type argument inference context.

    We need this because we pass around ``Mapping`` and ``Iterable`` types.
    These types are only known by ``TypeChecker`` itself.
    It is required for ``*`` and ``**`` argument inference.

    https://github.com/python/mypy/issues/11144
    """

    mapping_type: Instance
    iterable_type: Instance


</t>
<t tx="ekr.20221004064034.1441">def infer_function_type_arguments(
    callee_type: CallableType,
    arg_types: Sequence[Type | None],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
    context: ArgumentInferContext,
    strict: bool = True,
) -&gt; list[Type | None]:
    """Infer the type arguments of a generic function.

    Return an array of lower bound types for the type variables -1 (at
    index 0), -2 (at index 1), etc. A lower bound is None if a value
    could not be inferred.

    Arguments:
      callee_type: the target generic function
      arg_types: argument types at the call site (each optional; if None,
                 we are not considering this argument in the current pass)
      arg_kinds: nodes.ARG_* values for arg_types
      formal_to_actual: mapping from formal to actual variable indices
    """
    # Infer constraints.
    constraints = infer_constraints_for_callable(
        callee_type, arg_types, arg_kinds, formal_to_actual, context
    )

    # Solve constraints.
    type_vars = callee_type.type_var_ids()
    return solve_constraints(type_vars, constraints, strict)


</t>
<t tx="ekr.20221004064034.1442">def infer_type_arguments(
    type_var_ids: list[TypeVarId], template: Type, actual: Type, is_supertype: bool = False
) -&gt; list[Type | None]:
    # Like infer_function_type_arguments, but only match a single type
    # against a generic type.
    constraints = infer_constraints(template, actual, SUPERTYPE_OF if is_supertype else SUBTYPE_OF)
    return solve_constraints(type_var_ids, constraints)
</t>
<t tx="ekr.20221004064034.1443">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import os
from collections import defaultdict
from functools import cmp_to_key
from typing import Callable

from mypy.build import State
from mypy.find_sources import InvalidSourceList, SourceFinder
from mypy.messages import format_type
from mypy.modulefinder import PYTHON_EXTENSIONS
from mypy.nodes import (
    LDEF,
    Decorator,
    Expression,
    FuncBase,
    MemberExpr,
    MypyFile,
    Node,
    OverloadedFuncDef,
    RefExpr,
    SymbolNode,
    TypeInfo,
    Var,
)
from mypy.server.update import FineGrainedBuildManager
from mypy.traverser import ExtendedTraverserVisitor
from mypy.typeops import tuple_fallback
from mypy.types import (
    FunctionLike,
    Instance,
    LiteralType,
    ProperType,
    TupleType,
    TypedDictType,
    TypeVarType,
    UnionType,
    get_proper_type,
)
from mypy.typevars import fill_typevars_with_any


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.1444">def node_starts_after(o: Node, line: int, column: int) -&gt; bool:
    return o.line &gt; line or o.line == line and o.column &gt; column


</t>
<t tx="ekr.20221004064034.1445">def node_ends_before(o: Node, line: int, column: int) -&gt; bool:
    # Unfortunately, end positions for some statements are a mess,
    # e.g. overloaded functions, so we return False when we don't know.
    if o.end_line is not None and o.end_column is not None:
        if o.end_line &lt; line or o.end_line == line and o.end_column &lt; column:
            return True
    return False


</t>
<t tx="ekr.20221004064034.1446">def expr_span(expr: Expression) -&gt; str:
    """Format expression span as in mypy error messages."""
    return f"{expr.line}:{expr.column + 1}:{expr.end_line}:{expr.end_column}"


</t>
<t tx="ekr.20221004064034.1447">def get_instance_fallback(typ: ProperType) -&gt; list[Instance]:
    """Returns the Instance fallback for this type if one exists or None."""
    if isinstance(typ, Instance):
        return [typ]
    elif isinstance(typ, TupleType):
        return [tuple_fallback(typ)]
    elif isinstance(typ, TypedDictType):
        return [typ.fallback]
    elif isinstance(typ, FunctionLike):
        return [typ.fallback]
    elif isinstance(typ, LiteralType):
        return [typ.fallback]
    elif isinstance(typ, TypeVarType):
        if typ.values:
            res = []
            for t in typ.values:
                res.extend(get_instance_fallback(get_proper_type(t)))
            return res
        return get_instance_fallback(get_proper_type(typ.upper_bound))
    elif isinstance(typ, UnionType):
        res = []
        for t in typ.items:
            res.extend(get_instance_fallback(get_proper_type(t)))
        return res
    return []


</t>
<t tx="ekr.20221004064034.1448">def find_node(name: str, info: TypeInfo) -&gt; Var | FuncBase | None:
    """Find the node defining member 'name' in given TypeInfo."""
    # TODO: this code shares some logic with checkmember.py
    method = info.get_method(name)
    if method:
        if isinstance(method, Decorator):
            return method.var
        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            return dec.var
        return method
    else:
        # don't have such method, maybe variable?
        node = info.get(name)
        v = node.node if node else None
        if isinstance(v, Var):
            return v
    return None


</t>
<t tx="ekr.20221004064034.1449">def find_module_by_fullname(fullname: str, modules: dict[str, State]) -&gt; State | None:
    """Find module by a node fullname.

    This logic mimics the one we use in fixup, so should be good enough.
    """
    head = fullname
    # Special case: a module symbol is considered to be defined in itself, not in enclosing
    # package, since this is what users want when clicking go to definition on a module.
    if head in modules:
        return modules[head]
    while True:
        if "." not in head:
            return None
        head, tail = head.rsplit(".", maxsplit=1)
        mod = modules.get(head)
        if mod is not None:
            return mod


</t>
<t tx="ekr.20221004064034.145">@contextlib.contextmanager
def tmp_twine() -&gt; Iterator[Path]:
    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_venv_dir = Path(tmp_dir) / "venv"
        venv.create(tmp_venv_dir, with_pip=True)
        pip_exe = tmp_venv_dir / "bin" / "pip"
        subprocess.check_call([pip_exe, "install", "twine"])
        yield tmp_venv_dir / "bin" / "twine"


</t>
<t tx="ekr.20221004064034.1450">class SearchVisitor(ExtendedTraverserVisitor):
    """Visitor looking for an expression whose span matches given one exactly."""

    @others
</t>
<t tx="ekr.20221004064034.1451">def __init__(self, line: int, column: int, end_line: int, end_column: int) -&gt; None:
    self.line = line
    self.column = column
    self.end_line = end_line
    self.end_column = end_column
    self.result: Expression | None = None

</t>
<t tx="ekr.20221004064034.1452">def visit(self, o: Node) -&gt; bool:
    if node_starts_after(o, self.line, self.column):
        return False
    if node_ends_before(o, self.end_line, self.end_column):
        return False
    if (
        o.line == self.line
        and o.end_line == self.end_line
        and o.column == self.column
        and o.end_column == self.end_column
    ):
        if isinstance(o, Expression):
            self.result = o
    return self.result is None


</t>
<t tx="ekr.20221004064034.1453">def find_by_location(
    tree: MypyFile, line: int, column: int, end_line: int, end_column: int
) -&gt; Expression | None:
    """Find an expression matching given span, or None if not found."""
    if end_line &lt; line:
        raise ValueError('"end_line" must not be before "line"')
    if end_line == line and end_column &lt;= column:
        raise ValueError('"end_column" must be after "column"')
    visitor = SearchVisitor(line, column, end_line, end_column)
    tree.accept(visitor)
    return visitor.result


</t>
<t tx="ekr.20221004064034.1454">class SearchAllVisitor(ExtendedTraverserVisitor):
    """Visitor looking for all expressions whose spans enclose given position."""

    @others
</t>
<t tx="ekr.20221004064034.1455">def __init__(self, line: int, column: int) -&gt; None:
    self.line = line
    self.column = column
    self.result: list[Expression] = []

</t>
<t tx="ekr.20221004064034.1456">def visit(self, o: Node) -&gt; bool:
    if node_starts_after(o, self.line, self.column):
        return False
    if node_ends_before(o, self.line, self.column):
        return False
    if isinstance(o, Expression):
        self.result.append(o)
    return True


</t>
<t tx="ekr.20221004064034.1457">def find_all_by_location(tree: MypyFile, line: int, column: int) -&gt; list[Expression]:
    """Find all expressions enclosing given position starting from innermost."""
    visitor = SearchAllVisitor(line, column)
    tree.accept(visitor)
    return list(reversed(visitor.result))


</t>
<t tx="ekr.20221004064034.1458">class InspectionEngine:
    """Engine for locating and statically inspecting expressions."""

    @others
</t>
<t tx="ekr.20221004064034.1459">def __init__(
    self,
    fg_manager: FineGrainedBuildManager,
    *,
    verbosity: int = 0,
    limit: int = 0,
    include_span: bool = False,
    include_kind: bool = False,
    include_object_attrs: bool = False,
    union_attrs: bool = False,
    force_reload: bool = False,
) -&gt; None:
    self.fg_manager = fg_manager
    self.finder = SourceFinder(
        self.fg_manager.manager.fscache, self.fg_manager.manager.options
    )
    self.verbosity = verbosity
    self.limit = limit
    self.include_span = include_span
    self.include_kind = include_kind
    self.include_object_attrs = include_object_attrs
    self.union_attrs = union_attrs
    self.force_reload = force_reload
    # Module for which inspection was requested.
    self.module: State | None = None

</t>
<t tx="ekr.20221004064034.146">def upload_dist(dist: Path, dry_run: bool = True) -&gt; None:
    with tmp_twine() as twine:
        files = [item for item in dist.iterdir() if is_whl_or_tar(item.name)]
        cmd: list[Any] = [twine, "upload"]
        cmd += files
        if dry_run:
            print("[dry run] " + " ".join(map(str, cmd)))
        else:
            print(" ".join(map(str, cmd)))
            subprocess.check_call(cmd)


</t>
<t tx="ekr.20221004064034.1460">def parse_location(self, location: str) -&gt; tuple[str, list[int]]:
    if location.count(":") not in [2, 4]:
        raise ValueError("Format should be file:line:column[:end_line:end_column]")
    parts = location.split(":")
    module, *rest = parts
    return module, [int(p) for p in rest]

</t>
<t tx="ekr.20221004064034.1461">def reload_module(self, state: State) -&gt; None:
    """Reload given module while temporary exporting types."""
    old = self.fg_manager.manager.options.export_types
    self.fg_manager.manager.options.export_types = True
    try:
        self.fg_manager.flush_cache()
        assert state.path is not None
        self.fg_manager.update([(state.id, state.path)], [])
    finally:
        self.fg_manager.manager.options.export_types = old

</t>
<t tx="ekr.20221004064034.1462">def expr_type(self, expression: Expression) -&gt; tuple[str, bool]:
    """Format type for an expression using current options.

    If type is known, second item returned is True. If type is not known, an error
    message is returned instead, and second item returned is False.
    """
    expr_type = self.fg_manager.manager.all_types.get(expression)
    if expr_type is None:
        return self.missing_type(expression), False

    type_str = format_type(expr_type, verbosity=self.verbosity)
    return self.add_prefixes(type_str, expression), True

</t>
<t tx="ekr.20221004064034.1463">def object_type(self) -&gt; Instance:
    builtins = self.fg_manager.graph["builtins"].tree
    assert builtins is not None
    object_node = builtins.names["object"].node
    assert isinstance(object_node, TypeInfo)
    return Instance(object_node, [])

</t>
<t tx="ekr.20221004064034.1464">def collect_attrs(self, instances: list[Instance]) -&gt; dict[TypeInfo, list[str]]:
    """Collect attributes from all union/typevar variants."""

    def item_attrs(attr_dict: dict[TypeInfo, list[str]]) -&gt; set[str]:
        attrs = set()
        for base in attr_dict:
            attrs |= set(attr_dict[base])
        return attrs

    def cmp_types(x: TypeInfo, y: TypeInfo) -&gt; int:
        if x in y.mro:
            return 1
        if y in x.mro:
            return -1
        return 0

    # First gather all attributes for every union variant.
    assert instances
    all_attrs = []
    for instance in instances:
        attrs = {}
        mro = instance.type.mro
        if not self.include_object_attrs:
            mro = mro[:-1]
        for base in mro:
            attrs[base] = sorted(base.names)
        all_attrs.append(attrs)

    # Find attributes valid for all variants in a union or type variable.
    intersection = item_attrs(all_attrs[0])
    for item in all_attrs[1:]:
        intersection &amp;= item_attrs(item)

    # Combine attributes from all variants into a single dict while
    # also removing invalid attributes (unless using --union-attrs).
    combined_attrs = defaultdict(list)
    for item in all_attrs:
        for base in item:
            if base in combined_attrs:
                continue
            for name in item[base]:
                if self.union_attrs or name in intersection:
                    combined_attrs[base].append(name)

    # Sort bases by MRO, unrelated will appear in the order they appeared as union variants.
    sorted_bases = sorted(combined_attrs.keys(), key=cmp_to_key(cmp_types))
    result = {}
    for base in sorted_bases:
        if not combined_attrs[base]:
            # Skip bases where everytihng was filtered out.
            continue
        result[base] = combined_attrs[base]
    return result

</t>
<t tx="ekr.20221004064034.1465">def _fill_from_dict(
    self, attrs_strs: list[str], attrs_dict: dict[TypeInfo, list[str]]
) -&gt; None:
    for base in attrs_dict:
        cls_name = base.name if self.verbosity &lt; 1 else base.fullname
        attrs = [f'"{attr}"' for attr in attrs_dict[base]]
        attrs_strs.append(f'"{cls_name}": [{", ".join(attrs)}]')

</t>
<t tx="ekr.20221004064034.1466">def expr_attrs(self, expression: Expression) -&gt; tuple[str, bool]:
    """Format attributes that are valid for a given expression.

    If expression type is not an Instance, try using fallback. Attributes are
    returned as a JSON (ordered by MRO) that maps base class name to list of
    attributes. Attributes may appear in multiple bases if overridden (we simply
    follow usual mypy logic for creating new Vars etc).
    """
    expr_type = self.fg_manager.manager.all_types.get(expression)
    if expr_type is None:
        return self.missing_type(expression), False

    expr_type = get_proper_type(expr_type)
    instances = get_instance_fallback(expr_type)
    if not instances:
        # Everything is an object in Python.
        instances = [self.object_type()]

    attrs_dict = self.collect_attrs(instances)

    # Special case: modules have names apart from those from ModuleType.
    if isinstance(expression, RefExpr) and isinstance(expression.node, MypyFile):
        node = expression.node
        names = sorted(node.names)
        if "__builtins__" in names:
            # This is just to make tests stable. No one will really need ths name.
            names.remove("__builtins__")
        mod_dict = {f'"&lt;{node.fullname}&gt;"': [f'"{name}"' for name in names]}
    else:
        mod_dict = {}

    # Special case: for class callables, prepend with the class attributes.
    # TODO: also handle cases when such callable appears in a union.
    if isinstance(expr_type, FunctionLike) and expr_type.is_type_obj():
        template = fill_typevars_with_any(expr_type.type_object())
        class_dict = self.collect_attrs(get_instance_fallback(template))
    else:
        class_dict = {}

    # We don't use JSON dump to be sure keys order is always preserved.
    base_attrs = []
    if mod_dict:
        for mod in mod_dict:
            base_attrs.append(f'{mod}: [{", ".join(mod_dict[mod])}]')
    self._fill_from_dict(base_attrs, class_dict)
    self._fill_from_dict(base_attrs, attrs_dict)
    return self.add_prefixes(f'{{{", ".join(base_attrs)}}}', expression), True

</t>
<t tx="ekr.20221004064034.1467">def format_node(self, module: State, node: FuncBase | SymbolNode) -&gt; str:
    return f"{module.path}:{node.line}:{node.column + 1}:{node.name}"

</t>
<t tx="ekr.20221004064034.1468">def collect_nodes(self, expression: RefExpr) -&gt; list[FuncBase | SymbolNode]:
    """Collect nodes that can be referred to by an expression.

    Note: it can be more than one for example in case of a union attribute.
    """
    node: FuncBase | SymbolNode | None = expression.node
    nodes: list[FuncBase | SymbolNode]
    if node is None:
        # Tricky case: instance attribute
        if isinstance(expression, MemberExpr) and expression.kind is None:
            base_type = self.fg_manager.manager.all_types.get(expression.expr)
            if base_type is None:
                return []

            # Now we use the base type to figure out where the attribute is defined.
            base_type = get_proper_type(base_type)
            instances = get_instance_fallback(base_type)
            nodes = []
            for instance in instances:
                node = find_node(expression.name, instance.type)
                if node:
                    nodes.append(node)
            if not nodes:
                # Try checking class namespace if attribute is on a class object.
                if isinstance(base_type, FunctionLike) and base_type.is_type_obj():
                    instances = get_instance_fallback(
                        fill_typevars_with_any(base_type.type_object())
                    )
                    for instance in instances:
                        node = find_node(expression.name, instance.type)
                        if node:
                            nodes.append(node)
                else:
                    # Still no luck, give up.
                    return []
        else:
            return []
    else:
        # Easy case: a module-level definition
        nodes = [node]
    return nodes

</t>
<t tx="ekr.20221004064034.1469">def modules_for_nodes(
    self, nodes: list[FuncBase | SymbolNode], expression: RefExpr
) -&gt; tuple[dict[FuncBase | SymbolNode, State], bool]:
    """Gather modules where given nodes where defined.

    Also check if they need to be refreshed (cached nodes may have
    lines/columns missing).
    """
    modules = {}
    reload_needed = False
    for node in nodes:
        module = find_module_by_fullname(node.fullname, self.fg_manager.graph)
        if not module:
            if expression.kind == LDEF and self.module:
                module = self.module
            else:
                continue
        modules[node] = module
        if not module.tree or module.tree.is_cache_skeleton or self.force_reload:
            reload_needed |= not module.tree or module.tree.is_cache_skeleton
            self.reload_module(module)
    return modules, reload_needed

</t>
<t tx="ekr.20221004064034.147">def upload_to_pypi(version: str, dry_run: bool = True) -&gt; None:
    assert re.match(r"v?0\.[0-9]{3}(\+\S+)?$", version)
    if "dev" in version:
        assert dry_run, "Must use --dry-run with dev versions of mypy"
    if version.startswith("v"):
        version = version[1:]

    target_dir = tempfile.mkdtemp()
    dist = Path(target_dir) / "dist"
    dist.mkdir()
    print(f"Temporary target directory: {target_dir}")

    release = get_release_for_tag(f"v{version}")
    download_all_release_assets(release, dist)

    spot_check_dist(dist, version)
    check_sdist(dist, version)
    upload_dist(dist, dry_run)
    print("&lt;&lt; All done! &gt;&gt;")


</t>
<t tx="ekr.20221004064034.1470">def expression_def(self, expression: Expression) -&gt; tuple[str, bool]:
    """Find and format definition location for an expression.

    If it is not a RefExpr, it is effectively skipped by returning an
    empty result.
    """
    if not isinstance(expression, RefExpr):
        # If there are no suitable matches at all, we return error later.
        return "", True

    nodes = self.collect_nodes(expression)

    if not nodes:
        return self.missing_node(expression), False

    modules, reload_needed = self.modules_for_nodes(nodes, expression)
    if reload_needed:
        # TODO: line/column are not stored in cache for vast majority of symbol nodes.
        # Adding them will make thing faster, but will have visible memory impact.
        nodes = self.collect_nodes(expression)
        modules, reload_needed = self.modules_for_nodes(nodes, expression)
        assert not reload_needed

    result = []
    for node in modules:
        result.append(self.format_node(modules[node], node))

    if not result:
        return self.missing_node(expression), False

    return self.add_prefixes(", ".join(result), expression), True

</t>
<t tx="ekr.20221004064034.1471">def missing_type(self, expression: Expression) -&gt; str:
    alt_suggestion = ""
    if not self.force_reload:
        alt_suggestion = " or try --force-reload"
    return (
        f'No known type available for "{type(expression).__name__}"'
        f" (maybe unreachable{alt_suggestion})"
    )

</t>
<t tx="ekr.20221004064034.1472">def missing_node(self, expression: Expression) -&gt; str:
    return (
        f'Cannot find definition for "{type(expression).__name__}"'
        f" at {expr_span(expression)}"
    )

</t>
<t tx="ekr.20221004064034.1473">def add_prefixes(self, result: str, expression: Expression) -&gt; str:
    prefixes = []
    if self.include_kind:
        prefixes.append(f"{type(expression).__name__}")
    if self.include_span:
        prefixes.append(expr_span(expression))
    if prefixes:
        prefix = ":".join(prefixes) + " -&gt; "
    else:
        prefix = ""
    return prefix + result

</t>
<t tx="ekr.20221004064034.1474">def run_inspection_by_exact_location(
    self,
    tree: MypyFile,
    line: int,
    column: int,
    end_line: int,
    end_column: int,
    method: Callable[[Expression], tuple[str, bool]],
) -&gt; dict[str, object]:
    """Get type of an expression matching a span.

    Type or error is returned as a standard daemon response dict.
    """
    try:
        expression = find_by_location(tree, line, column - 1, end_line, end_column)
    except ValueError as err:
        return {"error": str(err)}

    if expression is None:
        span = f"{line}:{column}:{end_line}:{end_column}"
        return {"out": f"Can't find expression at span {span}", "err": "", "status": 1}

    inspection_str, success = method(expression)
    return {"out": inspection_str, "err": "", "status": 0 if success else 1}

</t>
<t tx="ekr.20221004064034.1475">def run_inspection_by_position(
    self,
    tree: MypyFile,
    line: int,
    column: int,
    method: Callable[[Expression], tuple[str, bool]],
) -&gt; dict[str, object]:
    """Get types of all expressions enclosing a position.

    Types and/or errors are returned as a standard daemon response dict.
    """
    expressions = find_all_by_location(tree, line, column - 1)
    if not expressions:
        position = f"{line}:{column}"
        return {
            "out": f"Can't find any expressions at position {position}",
            "err": "",
            "status": 1,
        }

    inspection_strs = []
    status = 0
    for expression in expressions:
        inspection_str, success = method(expression)
        if not success:
            status = 1
        if inspection_str:
            inspection_strs.append(inspection_str)
    if self.limit:
        inspection_strs = inspection_strs[: self.limit]
    return {"out": "\n".join(inspection_strs), "err": "", "status": status}

</t>
<t tx="ekr.20221004064034.1476">def find_module(self, file: str) -&gt; tuple[State | None, dict[str, object]]:
    """Find module by path, or return a suitable error message.

    Note we don't use exceptions to simplify handling 1 vs 2 statuses.
    """
    if not any(file.endswith(ext) for ext in PYTHON_EXTENSIONS):
        return None, {"error": "Source file is not a Python file"}

    try:
        module, _ = self.finder.crawl_up(os.path.normpath(file))
    except InvalidSourceList:
        return None, {"error": "Invalid source file name: " + file}

    state = self.fg_manager.graph.get(module)
    self.module = state
    return (
        state,
        {"out": f"Unknown module: {module}", "err": "", "status": 1} if state is None else {},
    )

</t>
<t tx="ekr.20221004064034.1477">def run_inspection(
    self, location: str, method: Callable[[Expression], tuple[str, bool]]
) -&gt; dict[str, object]:
    """Top-level logic to inspect expression(s) at a location.

    This can be re-used by various simple inspections.
    """
    try:
        file, pos = self.parse_location(location)
    except ValueError as err:
        return {"error": str(err)}

    state, err_dict = self.find_module(file)
    if state is None:
        assert err_dict
        return err_dict

    # Force reloading to load from cache, account for any edits, etc.
    if not state.tree or state.tree.is_cache_skeleton or self.force_reload:
        self.reload_module(state)
    assert state.tree is not None

    if len(pos) == 4:
        # Full span, return an exact match only.
        line, column, end_line, end_column = pos
        return self.run_inspection_by_exact_location(
            state.tree, line, column, end_line, end_column, method
        )
    assert len(pos) == 2
    # Inexact location, return all expressions.
    line, column = pos
    return self.run_inspection_by_position(state.tree, line, column, method)

</t>
<t tx="ekr.20221004064034.1478">def get_type(self, location: str) -&gt; dict[str, object]:
    """Get types of expression(s) at a location."""
    return self.run_inspection(location, self.expr_type)

</t>
<t tx="ekr.20221004064034.1479">def get_attrs(self, location: str) -&gt; dict[str, object]:
    """Get attributes of expression(s) at a location."""
    return self.run_inspection(location, self.expr_attrs)

</t>
<t tx="ekr.20221004064034.148">def main() -&gt; None:
    parser = argparse.ArgumentParser(description="PyPI mypy package uploader")
    parser.add_argument(
        "--dry-run", action="store_true", default=False, help="Don't actually upload packages"
    )
    parser.add_argument("version", help="mypy version to release")
    args = parser.parse_args()

    upload_to_pypi(args.version, args.dry_run)


</t>
<t tx="ekr.20221004064034.1480">def get_definition(self, location: str) -&gt; dict[str, object]:
    """Get symbol definitions of expression(s) at a location."""
    result = self.run_inspection(location, self.expression_def)
    if "out" in result and not result["out"]:
        # None of the expressions found turns out to be a RefExpr.
        _, location = location.split(":", maxsplit=1)
        result["out"] = f"No name or member expressions at {location}"
        result["status"] = 1
    return result
</t>
<t tx="ekr.20221004064034.1481">@path C:/Repos/ekr-mypy2/mypy/
"""Cross platform abstractions for inter-process communication

On Unix, this uses AF_UNIX sockets.
On Windows, this uses NamedPipes.
"""

from __future__ import annotations

import base64
import os
import shutil
import sys
import tempfile
from types import TracebackType
from typing import Callable
from typing_extensions import Final

if sys.platform == "win32":
    # This may be private, but it is needed for IPC on Windows, and is basically stable
    import ctypes

    import _winapi

    _IPCHandle = int

    kernel32 = ctypes.windll.kernel32
    DisconnectNamedPipe: Callable[[_IPCHandle], int] = kernel32.DisconnectNamedPipe
    FlushFileBuffers: Callable[[_IPCHandle], int] = kernel32.FlushFileBuffers
else:
    import socket

    _IPCHandle = socket.socket


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.149"></t>
<t tx="ekr.20221004064034.15">def main() -&gt; None:
    prog, *args = argv

    if not set(args).issubset(cmds):
        print("usage:", prog, " ".join(f"[{k}]" for k in cmds))
        print()
        print(
            "Run the given tests. If given no arguments, run everything except"
            + " pytest-extra and mypyc-extra."
        )
        exit(1)

    if not args:
        args = DEFAULT_COMMANDS[:]

    status = 0

    if "self" in args and "lint" in args:
        # Perform lint and self check in parallel as it's faster.
        proc = start_background_cmd("lint")
        cmd_status = run_cmd("self")
        if cmd_status:
            status = cmd_status
        cmd_status = wait_background_cmd("lint", proc)
        if cmd_status:
            status = cmd_status
        args = [arg for arg in args if arg not in ("self", "lint")]

    for arg in args:
        cmd_status = run_cmd(arg)
        if cmd_status:
            status = cmd_status

    exit(status)


</t>
<t tx="ekr.20221004064034.150">@path C:/Repos/ekr-mypy2/mypy/
"""This module makes it possible to use mypy as part of a Python application.

Since mypy still changes, the API was kept utterly simple and non-intrusive.
It just mimics command line activation without starting a new interpreter.
So the normal docs about the mypy command line apply.
Changes in the command line version of mypy will be immediately usable.

Just import this module and then call the 'run' function with a parameter of
type List[str], containing what normally would have been the command line
arguments to mypy.

Function 'run' returns a Tuple[str, str, int], namely
(&lt;normal_report&gt;, &lt;error_report&gt;, &lt;exit_status&gt;),
in which &lt;normal_report&gt; is what mypy normally writes to sys.stdout,
&lt;error_report&gt; is what mypy normally writes to sys.stderr and exit_status is
the exit status mypy normally returns to the operating system.

Any pretty formatting is left to the caller.

The 'run_dmypy' function is similar, but instead mimics invocation of
dmypy. Note that run_dmypy is not thread-safe and modifies sys.stdout
and sys.stderr during its invocation.

Note that these APIs don't support incremental generation of error
messages.

Trivial example of code using this module:

import sys
from mypy import api

result = api.run(sys.argv[1:])

if result[0]:
    print('\nType checking report:\n')
    print(result[0])  # stdout

if result[1]:
    print('\nError report:\n')
    print(result[1])  # stderr

print('\nExit status:', result[2])

"""

from __future__ import annotations

import sys
from io import StringIO
from typing import Callable, TextIO, cast


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.151">def _run(main_wrapper: Callable[[TextIO, TextIO], None]) -&gt; tuple[str, str, int]:

    stdout = StringIO()
    stderr = StringIO()

    try:
        main_wrapper(stdout, stderr)
        exit_status = 0
    except SystemExit as system_exit:
        exit_status = cast(int, system_exit.code)

    return stdout.getvalue(), stderr.getvalue(), exit_status


</t>
<t tx="ekr.20221004064034.152">def run(args: list[str]) -&gt; tuple[str, str, int]:
    # Lazy import to avoid needing to import all of mypy to call run_dmypy
    from mypy.main import main

    return _run(
        lambda stdout, stderr: main(args=args, stdout=stdout, stderr=stderr, clean_exit=True)
    )


</t>
<t tx="ekr.20221004064034.153">def run_dmypy(args: list[str]) -&gt; tuple[str, str, int]:
    from mypy.dmypy.client import main

    @others
    return _run(f)
</t>
<t tx="ekr.20221004064034.154"># A bunch of effort has been put into threading stdout and stderr
# through the main API to avoid the threadsafety problems of
# modifying sys.stdout/sys.stderr, but that hasn't been done for
# the dmypy client, so we just do the non-threadsafe thing.
def f(stdout: TextIO, stderr: TextIO) -&gt; None:
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    try:
        sys.stdout = stdout
        sys.stderr = stderr
        main(args)
    finally:
        sys.stdout = old_stdout
        sys.stderr = old_stderr

</t>
<t tx="ekr.20221004064034.155">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Callable, Sequence

import mypy.subtypes
from mypy.expandtype import expand_type
from mypy.nodes import Context
from mypy.types import (
    AnyType,
    CallableType,
    Parameters,
    ParamSpecType,
    PartialType,
    Type,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    get_proper_type,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.156">def get_target_type(
    tvar: TypeVarLikeType,
    type: Type,
    callable: CallableType,
    report_incompatible_typevar_value: Callable[[CallableType, Type, str, Context], None],
    context: Context,
    skip_unsatisfied: bool,
) -&gt; Type | None:
    if isinstance(tvar, ParamSpecType):
        return type
    if isinstance(tvar, TypeVarTupleType):
        return type
    assert isinstance(tvar, TypeVarType)
    values = tvar.values
    p_type = get_proper_type(type)
    if values:
        if isinstance(p_type, AnyType):
            return type
        if isinstance(p_type, TypeVarType) and p_type.values:
            # Allow substituting T1 for T if every allowed value of T1
            # is also a legal value of T.
            if all(any(mypy.subtypes.is_same_type(v, v1) for v in values) for v1 in p_type.values):
                return type
        matching = []
        for value in values:
            if mypy.subtypes.is_subtype(type, value):
                matching.append(value)
        if matching:
            best = matching[0]
            # If there are more than one matching value, we select the narrowest
            for match in matching[1:]:
                if mypy.subtypes.is_subtype(match, best):
                    best = match
            return best
        if skip_unsatisfied:
            return None
        report_incompatible_typevar_value(callable, type, tvar.name, context)
    else:
        upper_bound = tvar.upper_bound
        if not mypy.subtypes.is_subtype(type, upper_bound):
            if skip_unsatisfied:
                return None
            report_incompatible_typevar_value(callable, type, tvar.name, context)
    return type


</t>
<t tx="ekr.20221004064034.157">def apply_generic_arguments(
    callable: CallableType,
    orig_types: Sequence[Type | None],
    report_incompatible_typevar_value: Callable[[CallableType, Type, str, Context], None],
    context: Context,
    skip_unsatisfied: bool = False,
) -&gt; CallableType:
    """Apply generic type arguments to a callable type.

    For example, applying [int] to 'def [T] (T) -&gt; T' results in
    'def (int) -&gt; int'.

    Note that each type can be None; in this case, it will not be applied.

    If `skip_unsatisfied` is True, then just skip the types that don't satisfy type variable
    bound or constraints, instead of giving an error.
    """
    tvars = callable.variables
    assert len(tvars) == len(orig_types)
    # Check that inferred type variable values are compatible with allowed
    # values and bounds.  Also, promote subtype values to allowed values.
    # Create a map from type variable id to target type.
    id_to_type: dict[TypeVarId, Type] = {}

    for tvar, type in zip(tvars, orig_types):
        assert not isinstance(type, PartialType), "Internal error: must never apply partial type"
        if type is None:
            continue

        target_type = get_target_type(
            tvar, type, callable, report_incompatible_typevar_value, context, skip_unsatisfied
        )
        if target_type is not None:
            id_to_type[tvar.id] = target_type

    param_spec = callable.param_spec()
    if param_spec is not None:
        nt = id_to_type.get(param_spec.id)
        if nt is not None:
            nt = get_proper_type(nt)
            if isinstance(nt, CallableType) or isinstance(nt, Parameters):
                callable = callable.expand_param_spec(nt)

    # Apply arguments to argument types.
    arg_types = [expand_type(at, id_to_type) for at in callable.arg_types]

    # Apply arguments to TypeGuard if any.
    if callable.type_guard is not None:
        type_guard = expand_type(callable.type_guard, id_to_type)
    else:
        type_guard = None

    # The callable may retain some type vars if only some were applied.
    remaining_tvars = [tv for tv in tvars if tv.id not in id_to_type]

    return callable.copy_modified(
        arg_types=arg_types,
        ret_type=expand_type(callable.ret_type, id_to_type),
        variables=remaining_tvars,
        type_guard=type_guard,
    )
</t>
<t tx="ekr.20221004064034.158">@path C:/Repos/ekr-mypy2/mypy/
"""Utilities for mapping between actual and formal arguments (and their types)."""

from __future__ import annotations

from typing import TYPE_CHECKING, Callable, Sequence

from mypy import nodes
from mypy.maptype import map_instance_to_supertype
from mypy.types import (
    AnyType,
    Instance,
    ParamSpecType,
    TupleType,
    Type,
    TypedDictType,
    TypeOfAny,
    get_proper_type,
)

if TYPE_CHECKING:
    from mypy.infer import ArgumentInferContext


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.159">def map_actuals_to_formals(
    actual_kinds: list[nodes.ArgKind],
    actual_names: Sequence[str | None] | None,
    formal_kinds: list[nodes.ArgKind],
    formal_names: Sequence[str | None],
    actual_arg_type: Callable[[int], Type],
) -&gt; list[list[int]]:
    """Calculate mapping between actual (caller) args and formals.

    The result contains a list of caller argument indexes mapping to each
    callee argument index, indexed by callee index.

    The caller_arg_type argument should evaluate to the type of the actual
    argument type with the given index.
    """
    nformals = len(formal_kinds)
    formal_to_actual: list[list[int]] = [[] for i in range(nformals)]
    ambiguous_actual_kwargs: list[int] = []
    fi = 0
    for ai, actual_kind in enumerate(actual_kinds):
        if actual_kind == nodes.ARG_POS:
            if fi &lt; nformals:
                if not formal_kinds[fi].is_star():
                    formal_to_actual[fi].append(ai)
                    fi += 1
                elif formal_kinds[fi] == nodes.ARG_STAR:
                    formal_to_actual[fi].append(ai)
        elif actual_kind == nodes.ARG_STAR:
            # We need to know the actual type to map varargs.
            actualt = get_proper_type(actual_arg_type(ai))
            if isinstance(actualt, TupleType):
                # A tuple actual maps to a fixed number of formals.
                for _ in range(len(actualt.items)):
                    if fi &lt; nformals:
                        if formal_kinds[fi] != nodes.ARG_STAR2:
                            formal_to_actual[fi].append(ai)
                        else:
                            break
                        if formal_kinds[fi] != nodes.ARG_STAR:
                            fi += 1
            else:
                # Assume that it is an iterable (if it isn't, there will be
                # an error later).
                while fi &lt; nformals:
                    if formal_kinds[fi].is_named(star=True):
                        break
                    else:
                        formal_to_actual[fi].append(ai)
                    if formal_kinds[fi] == nodes.ARG_STAR:
                        break
                    fi += 1
        elif actual_kind.is_named():
            assert actual_names is not None, "Internal error: named kinds without names given"
            name = actual_names[ai]
            if name in formal_names:
                formal_to_actual[formal_names.index(name)].append(ai)
            elif nodes.ARG_STAR2 in formal_kinds:
                formal_to_actual[formal_kinds.index(nodes.ARG_STAR2)].append(ai)
        else:
            assert actual_kind == nodes.ARG_STAR2
            actualt = get_proper_type(actual_arg_type(ai))
            if isinstance(actualt, TypedDictType):
                for name in actualt.items:
                    if name in formal_names:
                        formal_to_actual[formal_names.index(name)].append(ai)
                    elif nodes.ARG_STAR2 in formal_kinds:
                        formal_to_actual[formal_kinds.index(nodes.ARG_STAR2)].append(ai)
            else:
                # We don't exactly know which **kwargs are provided by the
                # caller, so we'll defer until all the other unambiguous
                # actuals have been processed
                ambiguous_actual_kwargs.append(ai)

    if ambiguous_actual_kwargs:
        # Assume the ambiguous kwargs will fill the remaining arguments.
        #
        # TODO: If there are also tuple varargs, we might be missing some potential
        #       matches if the tuple was short enough to not match everything.
        unmatched_formals = [
            fi
            for fi in range(nformals)
            if (
                formal_names[fi]
                and (
                    not formal_to_actual[fi]
                    or actual_kinds[formal_to_actual[fi][0]] == nodes.ARG_STAR
                )
                and formal_kinds[fi] != nodes.ARG_STAR
            )
            or formal_kinds[fi] == nodes.ARG_STAR2
        ]
        for ai in ambiguous_actual_kwargs:
            for fi in unmatched_formals:
                formal_to_actual[fi].append(ai)

    return formal_to_actual


</t>
<t tx="ekr.20221004064034.160">def map_formals_to_actuals(
    actual_kinds: list[nodes.ArgKind],
    actual_names: Sequence[str | None] | None,
    formal_kinds: list[nodes.ArgKind],
    formal_names: list[str | None],
    actual_arg_type: Callable[[int], Type],
) -&gt; list[list[int]]:
    """Calculate the reverse mapping of map_actuals_to_formals."""
    formal_to_actual = map_actuals_to_formals(
        actual_kinds, actual_names, formal_kinds, formal_names, actual_arg_type
    )
    # Now reverse the mapping.
    actual_to_formal: list[list[int]] = [[] for _ in actual_kinds]
    for formal, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            actual_to_formal[actual].append(formal)
    return actual_to_formal


</t>
<t tx="ekr.20221004064034.161">class ArgTypeExpander:
    """Utility class for mapping actual argument types to formal arguments.

    One of the main responsibilities is to expand caller tuple *args and TypedDict
    **kwargs, and to keep track of which tuple/TypedDict items have already been
    consumed.

    Example:

       def f(x: int, *args: str) -&gt; None: ...
       f(*(1, 'x', 1.1))

    We'd call expand_actual_type three times:

      1. The first call would provide 'int' as the actual type of 'x' (from '1').
      2. The second call would provide 'str' as one of the actual types for '*args'.
      2. The third call would provide 'float' as one of the actual types for '*args'.

    A single instance can process all the arguments for a single call. Each call
    needs a separate instance since instances have per-call state.
    """

    @others
</t>
<t tx="ekr.20221004064034.162">def __init__(self, context: ArgumentInferContext) -&gt; None:
    # Next tuple *args index to use.
    self.tuple_index = 0
    # Keyword arguments in TypedDict **kwargs used.
    self.kwargs_used: set[str] = set()
    # Type context for `*` and `**` arg kinds.
    self.context = context

</t>
<t tx="ekr.20221004064034.163">def expand_actual_type(
    self,
    actual_type: Type,
    actual_kind: nodes.ArgKind,
    formal_name: str | None,
    formal_kind: nodes.ArgKind,
) -&gt; Type:
    """Return the actual (caller) type(s) of a formal argument with the given kinds.

    If the actual argument is a tuple *args, return the next individual tuple item that
    maps to the formal arg.

    If the actual argument is a TypedDict **kwargs, return the next matching typed dict
    value type based on formal argument name and kind.

    This is supposed to be called for each formal, in order. Call multiple times per
    formal if multiple actuals map to a formal.
    """
    original_actual = actual_type
    actual_type = get_proper_type(actual_type)
    if actual_kind == nodes.ARG_STAR:
        if isinstance(actual_type, Instance) and actual_type.args:
            from mypy.subtypes import is_subtype

            if is_subtype(actual_type, self.context.iterable_type):
                return map_instance_to_supertype(
                    actual_type, self.context.iterable_type.type
                ).args[0]
            else:
                # We cannot properly unpack anything other
                # than `Iterable` type with `*`.
                # Just return `Any`, other parts of code would raise
                # a different error for improper use.
                return AnyType(TypeOfAny.from_error)
        elif isinstance(actual_type, TupleType):
            # Get the next tuple item of a tuple *arg.
            if self.tuple_index &gt;= len(actual_type.items):
                # Exhausted a tuple -- continue to the next *args.
                self.tuple_index = 1
            else:
                self.tuple_index += 1
            return actual_type.items[self.tuple_index - 1]
        elif isinstance(actual_type, ParamSpecType):
            # ParamSpec is valid in *args but it can't be unpacked.
            return actual_type
        else:
            return AnyType(TypeOfAny.from_error)
    elif actual_kind == nodes.ARG_STAR2:
        from mypy.subtypes import is_subtype

        if isinstance(actual_type, TypedDictType):
            if formal_kind != nodes.ARG_STAR2 and formal_name in actual_type.items:
                # Lookup type based on keyword argument name.
                assert formal_name is not None
            else:
                # Pick an arbitrary item if no specified keyword is expected.
                formal_name = (set(actual_type.items.keys()) - self.kwargs_used).pop()
            self.kwargs_used.add(formal_name)
            return actual_type.items[formal_name]
        elif (
            isinstance(actual_type, Instance)
            and len(actual_type.args) &gt; 1
            and is_subtype(actual_type, self.context.mapping_type)
        ):
            # Only `Mapping` type can be unpacked with `**`.
            # Other types will produce an error somewhere else.
            return map_instance_to_supertype(actual_type, self.context.mapping_type.type).args[
                1
            ]
        elif isinstance(actual_type, ParamSpecType):
            # ParamSpec is valid in **kwargs but it can't be unpacked.
            return actual_type
        else:
            return AnyType(TypeOfAny.from_error)
    else:
        # No translation for other kinds -- 1:1 mapping.
        return original_actual
</t>
<t tx="ekr.20221004064034.164">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from collections import defaultdict
from contextlib import contextmanager
from typing import DefaultDict, Iterator, List, Optional, Tuple, Union, cast
from typing_extensions import TypeAlias as _TypeAlias

from mypy.erasetype import remove_instance_last_known_values
from mypy.join import join_simple
from mypy.literals import Key, literal, literal_hash, subkeys
from mypy.nodes import Expression, IndexExpr, MemberExpr, NameExpr, RefExpr, TypeInfo, Var
from mypy.subtypes import is_same_type, is_subtype
from mypy.types import (
    AnyType,
    NoneType,
    PartialType,
    Type,
    TypeOfAny,
    TypeType,
    UnionType,
    get_proper_type,
)
from mypy.typevars import fill_typevars_with_any

BindableExpression: _TypeAlias = Union[IndexExpr, MemberExpr, NameExpr]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.165">class Frame:
    """A Frame represents a specific point in the execution of a program.
    It carries information about the current types of expressions at
    that point, arising either from assignments to those expressions
    or the result of isinstance checks. It also records whether it is
    possible to reach that point at all.

    This information is not copied into a new Frame when it is pushed
    onto the stack, so a given Frame only has information about types
    that were assigned in that frame.
    """

    @others
</t>
<t tx="ekr.20221004064034.166">def __init__(self, id: int, conditional_frame: bool = False) -&gt; None:
    self.id = id
    self.types: dict[Key, Type] = {}
    self.unreachable = False
    self.conditional_frame = conditional_frame

    # Should be set only if we're entering a frame where it's not
    # possible to accurately determine whether or not contained
    # statements will be unreachable or not.
    #
    # Long-term, we should improve mypy to the point where we no longer
    # need this field.
    self.suppress_unreachable_warnings = False


</t>
<t tx="ekr.20221004064034.167">Assigns = DefaultDict[Expression, List[Tuple[Type, Optional[Type]]]]


</t>
<t tx="ekr.20221004064034.168">class ConditionalTypeBinder:
    """Keep track of conditional types of variables.

    NB: Variables are tracked by literal expression, so it is possible
    to confuse the binder; for example,

    ```
    class A:
        a = None          # type: Union[int, str]
    x = A()
    lst = [x]
    reveal_type(x.a)      # Union[int, str]
    x.a = 1
    reveal_type(x.a)      # int
    reveal_type(lst[0].a) # Union[int, str]
    lst[0].a = 'a'
    reveal_type(x.a)      # int
    reveal_type(lst[0].a) # str
    ```
    """

    # Stored assignments for situations with tuple/list lvalue and rvalue of union type.
    # This maps an expression to a list of bound types for every item in the union type.
    type_assignments: Assigns | None = None

    @others
</t>
<t tx="ekr.20221004064034.169">def __init__(self) -&gt; None:
    self.next_id = 1

    # The stack of frames currently used.  These map
    # literal_hash(expr) -- literals like 'foo.bar' --
    # to types. The last element of this list is the
    # top-most, current frame. Each earlier element
    # records the state as of when that frame was last
    # on top of the stack.
    self.frames = [Frame(self._get_id())]

    # For frames higher in the stack, we record the set of
    # Frames that can escape there, either by falling off
    # the end of the frame or by a loop control construct
    # or raised exception. The last element of self.frames
    # has no corresponding element in this list.
    self.options_on_return: list[list[Frame]] = []

    # Maps literal_hash(expr) to get_declaration(expr)
    # for every expr stored in the binder
    self.declarations: dict[Key, Type | None] = {}
    # Set of other keys to invalidate if a key is changed, e.g. x -&gt; {x.a, x[0]}
    # Whenever a new key (e.g. x.a.b) is added, we update this
    self.dependencies: dict[Key, set[Key]] = {}

    # Whether the last pop changed the newly top frame on exit
    self.last_pop_changed = False

    self.try_frames: set[int] = set()
    self.break_frames: list[int] = []
    self.continue_frames: list[int] = []

</t>
<t tx="ekr.20221004064034.170">def _get_id(self) -&gt; int:
    self.next_id += 1
    return self.next_id

</t>
<t tx="ekr.20221004064034.171">def _add_dependencies(self, key: Key, value: Key | None = None) -&gt; None:
    if value is None:
        value = key
    else:
        self.dependencies.setdefault(key, set()).add(value)
    for elt in subkeys(key):
        self._add_dependencies(elt, value)

</t>
<t tx="ekr.20221004064034.172">def push_frame(self, conditional_frame: bool = False) -&gt; Frame:
    """Push a new frame into the binder."""
    f = Frame(self._get_id(), conditional_frame)
    self.frames.append(f)
    self.options_on_return.append([])
    return f

</t>
<t tx="ekr.20221004064034.173">def _put(self, key: Key, type: Type, index: int = -1) -&gt; None:
    self.frames[index].types[key] = type

</t>
<t tx="ekr.20221004064034.174">def _get(self, key: Key, index: int = -1) -&gt; Type | None:
    if index &lt; 0:
        index += len(self.frames)
    for i in range(index, -1, -1):
        if key in self.frames[i].types:
            return self.frames[i].types[key]
    return None

</t>
<t tx="ekr.20221004064034.175">def put(self, expr: Expression, typ: Type) -&gt; None:
    if not isinstance(expr, (IndexExpr, MemberExpr, NameExpr)):
        return
    if not literal(expr):
        return
    key = literal_hash(expr)
    assert key is not None, "Internal error: binder tried to put non-literal"
    if key not in self.declarations:
        self.declarations[key] = get_declaration(expr)
        self._add_dependencies(key)
    self._put(key, typ)

</t>
<t tx="ekr.20221004064034.176">def unreachable(self) -&gt; None:
    self.frames[-1].unreachable = True

</t>
<t tx="ekr.20221004064034.177">def suppress_unreachable_warnings(self) -&gt; None:
    self.frames[-1].suppress_unreachable_warnings = True

</t>
<t tx="ekr.20221004064034.178">def get(self, expr: Expression) -&gt; Type | None:
    key = literal_hash(expr)
    assert key is not None, "Internal error: binder tried to get non-literal"
    return self._get(key)

</t>
<t tx="ekr.20221004064034.179">def is_unreachable(self) -&gt; bool:
    # TODO: Copy the value of unreachable into new frames to avoid
    # this traversal on every statement?
    return any(f.unreachable for f in self.frames)

</t>
<t tx="ekr.20221004064034.180">def is_unreachable_warning_suppressed(self) -&gt; bool:
    # TODO: See todo in 'is_unreachable'
    return any(f.suppress_unreachable_warnings for f in self.frames)

</t>
<t tx="ekr.20221004064034.181">def cleanse(self, expr: Expression) -&gt; None:
    """Remove all references to a Node from the binder."""
    key = literal_hash(expr)
    assert key is not None, "Internal error: binder tried cleanse non-literal"
    self._cleanse_key(key)

</t>
<t tx="ekr.20221004064034.182">def _cleanse_key(self, key: Key) -&gt; None:
    """Remove all references to a key from the binder."""
    for frame in self.frames:
        if key in frame.types:
            del frame.types[key]

</t>
<t tx="ekr.20221004064034.183">def update_from_options(self, frames: list[Frame]) -&gt; bool:
    """Update the frame to reflect that each key will be updated
    as in one of the frames.  Return whether any item changes.

    If a key is declared as AnyType, only update it if all the
    options are the same.
    """

    frames = [f for f in frames if not f.unreachable]
    changed = False
    keys = {key for f in frames for key in f.types}

    for key in keys:
        current_value = self._get(key)
        resulting_values = [f.types.get(key, current_value) for f in frames]
        if any(x is None for x in resulting_values):
            # We didn't know anything about key before
            # (current_value must be None), and we still don't
            # know anything about key in at least one possible frame.
            continue

        type = resulting_values[0]
        assert type is not None
        declaration_type = get_proper_type(self.declarations.get(key))
        if isinstance(declaration_type, AnyType):
            # At this point resulting values can't contain None, see continue above
            if not all(is_same_type(type, cast(Type, t)) for t in resulting_values[1:]):
                type = AnyType(TypeOfAny.from_another_any, source_any=declaration_type)
        else:
            for other in resulting_values[1:]:
                assert other is not None
                type = join_simple(self.declarations[key], type, other)
        if current_value is None or not is_same_type(type, current_value):
            self._put(key, type)
            changed = True

    self.frames[-1].unreachable = not frames

    return changed

</t>
<t tx="ekr.20221004064034.184">def pop_frame(self, can_skip: bool, fall_through: int) -&gt; Frame:
    """Pop a frame and return it.

    See frame_context() for documentation of fall_through.
    """

    if fall_through &gt; 0:
        self.allow_jump(-fall_through)

    result = self.frames.pop()
    options = self.options_on_return.pop()

    if can_skip:
        options.insert(0, self.frames[-1])

    self.last_pop_changed = self.update_from_options(options)

    return result

</t>
<t tx="ekr.20221004064034.185">@contextmanager
def accumulate_type_assignments(self) -&gt; Iterator[Assigns]:
    """Push a new map to collect assigned types in multiassign from union.

    If this map is not None, actual binding is deferred until all items in
    the union are processed (a union of collected items is later bound
    manually by the caller).
    """
    old_assignments = None
    if self.type_assignments is not None:
        old_assignments = self.type_assignments
    self.type_assignments = defaultdict(list)
    yield self.type_assignments
    self.type_assignments = old_assignments

</t>
<t tx="ekr.20221004064034.186">def assign_type(
    self, expr: Expression, type: Type, declared_type: Type | None, restrict_any: bool = False
) -&gt; None:
    # We should erase last known value in binder, because if we are using it,
    # it means that the target is not final, and therefore can't hold a literal.
    type = remove_instance_last_known_values(type)

    if self.type_assignments is not None:
        # We are in a multiassign from union, defer the actual binding,
        # just collect the types.
        self.type_assignments[expr].append((type, declared_type))
        return
    if not isinstance(expr, (IndexExpr, MemberExpr, NameExpr)):
        return None
    if not literal(expr):
        return
    self.invalidate_dependencies(expr)

    if declared_type is None:
        # Not sure why this happens.  It seems to mainly happen in
        # member initialization.
        return
    if not is_subtype(type, declared_type):
        # Pretty sure this is only happens when there's a type error.

        # Ideally this function wouldn't be called if the
        # expression has a type error, though -- do other kinds of
        # errors cause this function to get called at invalid
        # times?
        return

    p_declared = get_proper_type(declared_type)
    p_type = get_proper_type(type)
    enclosing_type = get_proper_type(self.most_recent_enclosing_type(expr, type))
    if isinstance(enclosing_type, AnyType) and not restrict_any:
        # If x is Any and y is int, after x = y we do not infer that x is int.
        # This could be changed.
        # Instead, since we narrowed type from Any in a recent frame (probably an
        # isinstance check), but now it is reassigned, we broaden back
        # to Any (which is the most recent enclosing type)
        self.put(expr, enclosing_type)
    # As a special case, when assigning Any to a variable with a
    # declared Optional type that has been narrowed to None,
    # replace all the Nones in the declared Union type with Any.
    # This overrides the normal behavior of ignoring Any assignments to variables
    # in order to prevent false positives.
    # (See discussion in #3526)
    elif (
        isinstance(p_type, AnyType)
        and isinstance(p_declared, UnionType)
        and any(isinstance(get_proper_type(item), NoneType) for item in p_declared.items)
        and isinstance(
            get_proper_type(self.most_recent_enclosing_type(expr, NoneType())), NoneType
        )
    ):
        # Replace any Nones in the union type with Any
        new_items = [
            type if isinstance(get_proper_type(item), NoneType) else item
            for item in p_declared.items
        ]
        self.put(expr, UnionType(new_items))
    elif isinstance(p_type, AnyType) and not (
        isinstance(p_declared, UnionType)
        and any(isinstance(get_proper_type(item), AnyType) for item in p_declared.items)
    ):
        # Assigning an Any value doesn't affect the type to avoid false negatives, unless
        # there is an Any item in a declared union type.
        self.put(expr, declared_type)
    else:
        self.put(expr, type)

    for i in self.try_frames:
        # XXX This should probably not copy the entire frame, but
        # just copy this variable into a single stored frame.
        self.allow_jump(i)

</t>
<t tx="ekr.20221004064034.187">def invalidate_dependencies(self, expr: BindableExpression) -&gt; None:
    """Invalidate knowledge of types that include expr, but not expr itself.

    For example, when expr is foo.bar, invalidate foo.bar.baz.

    It is overly conservative: it invalidates globally, including
    in code paths unreachable from here.
    """
    key = literal_hash(expr)
    assert key is not None
    for dep in self.dependencies.get(key, set()):
        self._cleanse_key(dep)

</t>
<t tx="ekr.20221004064034.188">def most_recent_enclosing_type(self, expr: BindableExpression, type: Type) -&gt; Type | None:
    type = get_proper_type(type)
    if isinstance(type, AnyType):
        return get_declaration(expr)
    key = literal_hash(expr)
    assert key is not None
    enclosers = [get_declaration(expr)] + [
        f.types[key] for f in self.frames if key in f.types and is_subtype(type, f.types[key])
    ]
    return enclosers[-1]

</t>
<t tx="ekr.20221004064034.189">def allow_jump(self, index: int) -&gt; None:
    # self.frames and self.options_on_return have different lengths
    # so make sure the index is positive
    if index &lt; 0:
        index += len(self.options_on_return)
    frame = Frame(self._get_id())
    for f in self.frames[index + 1 :]:
        frame.types.update(f.types)
        if f.unreachable:
            frame.unreachable = True
    self.options_on_return[index].append(frame)

</t>
<t tx="ekr.20221004064034.190">def handle_break(self) -&gt; None:
    self.allow_jump(self.break_frames[-1])
    self.unreachable()

</t>
<t tx="ekr.20221004064034.191">def handle_continue(self) -&gt; None:
    self.allow_jump(self.continue_frames[-1])
    self.unreachable()

</t>
<t tx="ekr.20221004064034.192">@contextmanager
def frame_context(
    self,
    *,
    can_skip: bool,
    fall_through: int = 1,
    break_frame: int = 0,
    continue_frame: int = 0,
    conditional_frame: bool = False,
    try_frame: bool = False,
) -&gt; Iterator[Frame]:
    """Return a context manager that pushes/pops frames on enter/exit.

    If can_skip is True, control flow is allowed to bypass the
    newly-created frame.

    If fall_through &gt; 0, then it will allow control flow that
    falls off the end of the frame to escape to its ancestor
    `fall_through` levels higher. Otherwise control flow ends
    at the end of the frame.

    If break_frame &gt; 0, then 'break' statements within this frame
    will jump out to the frame break_frame levels higher than the
    frame created by this call to frame_context. Similarly for
    continue_frame and 'continue' statements.

    If try_frame is true, then execution is allowed to jump at any
    point within the newly created frame (or its descendants) to
    its parent (i.e., to the frame that was on top before this
    call to frame_context).

    After the context manager exits, self.last_pop_changed indicates
    whether any types changed in the newly-topmost frame as a result
    of popping this frame.
    """
    assert len(self.frames) &gt; 1

    if break_frame:
        self.break_frames.append(len(self.frames) - break_frame)
    if continue_frame:
        self.continue_frames.append(len(self.frames) - continue_frame)
    if try_frame:
        self.try_frames.add(len(self.frames) - 1)

    new_frame = self.push_frame(conditional_frame)
    if try_frame:
        # An exception may occur immediately
        self.allow_jump(-1)
    yield new_frame
    self.pop_frame(can_skip, fall_through)

    if break_frame:
        self.break_frames.pop()
    if continue_frame:
        self.continue_frames.pop()
    if try_frame:
        self.try_frames.remove(len(self.frames) - 1)

</t>
<t tx="ekr.20221004064034.193">@contextmanager
def top_frame_context(self) -&gt; Iterator[Frame]:
    """A variant of frame_context for use at the top level of
    a namespace (module, function, or class).
    """
    assert len(self.frames) == 1
    yield self.push_frame()
    self.pop_frame(True, 0)


</t>
<t tx="ekr.20221004064034.194">def get_declaration(expr: BindableExpression) -&gt; Type | None:
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, Var):
            type = expr.node.type
            if not isinstance(get_proper_type(type), PartialType):
                return type
        elif isinstance(expr.node, TypeInfo):
            return TypeType(fill_typevars_with_any(expr.node))
    return None
</t>
<t tx="ekr.20221004064034.195">@path C:/Repos/ekr-mypy2/mypy/
"""A Bogus[T] type alias for marking when we subvert the type system

We need this for compiling with mypyc, which inserts runtime
typechecks that cause problems when we subvert the type system. So
when compiling with mypyc, we turn those places into Any, while
keeping the types around for normal typechecks.

Since this causes the runtime types to be Any, this is best used
in places where efficient access to properties is not important.
For those cases some other technique should be used.
"""

from __future__ import annotations

from typing import Any, TypeVar

from mypy_extensions import FlexibleAlias

T = TypeVar("T")

# This won't ever be true at runtime, but we consider it true during
# mypyc compilations.
MYPYC = False
if MYPYC:
    Bogus = FlexibleAlias[T, Any]
else:
    Bogus = FlexibleAlias[T, T]
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.196">@path C:/Repos/ekr-mypy2/mypy/
"""Facilities to analyze entire programs, including imported modules.

Parse and analyze the source files of a program in the correct order
(based on file dependencies), and collect the results.

This module only directs a build, which is performed in multiple passes per
file.  The individual passes are implemented in separate modules.

The function build() is the main interface to this module.
"""
# TODO: More consistent terminology, e.g. path/fnam, module/id, state/file

from __future__ import annotations

import contextlib
import errno
import gc
import json
import os
import platform
import re
import stat
import sys
import time
import types
from typing import (
    TYPE_CHECKING,
    AbstractSet,
    Any,
    Callable,
    ClassVar,
    Dict,
    Iterable,
    Iterator,
    Mapping,
    NamedTuple,
    NoReturn,
    Sequence,
    TextIO,
    TypeVar,
)
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy_extensions import TypedDict

import mypy.semanal_main
from mypy.checker import TypeChecker
from mypy.errors import CompileError, ErrorInfo, Errors, report_internal_error
from mypy.indirection import TypeIndirectionVisitor
from mypy.messages import MessageBuilder
from mypy.nodes import Import, ImportAll, ImportBase, ImportFrom, MypyFile, SymbolTable
from mypy.partially_defined import PartiallyDefinedVariableVisitor
from mypy.semanal import SemanticAnalyzer
from mypy.semanal_pass1 import SemanticAnalyzerPreAnalysis
from mypy.util import (
    DecodeError,
    decode_python_encoding,
    get_mypy_comments,
    get_top_two_prefixes,
    hash_digest,
    is_stub_package_file,
    is_sub_path,
    is_typeshed_file,
    module_prefix,
    read_py_file,
    time_ref,
    time_spent_us,
)

if TYPE_CHECKING:
    from mypy.report import Reports  # Avoid unconditional slow import

from mypy import errorcodes as codes
from mypy.config_parser import parse_mypy_comments
from mypy.fixup import fixup_module
from mypy.freetree import free_tree
from mypy.fscache import FileSystemCache
from mypy.metastore import FilesystemMetadataStore, MetadataStore, SqliteMetadataStore
from mypy.modulefinder import (
    BuildSource as BuildSource,
    BuildSourceSet as BuildSourceSet,
    FindModuleCache,
    ModuleNotFoundReason,
    ModuleSearchResult,
    SearchPaths,
    compute_search_paths,
)
from mypy.nodes import Expression
from mypy.options import Options
from mypy.parse import parse
from mypy.plugin import ChainedPlugin, Plugin, ReportConfigContext
from mypy.plugins.default import DefaultPlugin
from mypy.renaming import LimitedVariableRenameVisitor, VariableRenameVisitor
from mypy.stats import dump_type_stats
from mypy.stubinfo import (
    is_legacy_bundled_package,
    legacy_bundled_packages,
    non_bundled_packages,
    stub_package_name,
)
from mypy.types import Type
from mypy.typestate import TypeState, reset_global_state
from mypy.version import __version__

# Switch to True to produce debug output related to fine-grained incremental
# mode only that is useful during development. This produces only a subset of
# output compared to --verbose output. We use a global flag to enable this so
# that it's easy to enable this when running tests.
DEBUG_FINE_GRAINED: Final = False

# These modules are special and should always come from typeshed.
CORE_BUILTIN_MODULES: Final = {
    "builtins",
    "typing",
    "types",
    "typing_extensions",
    "mypy_extensions",
    "_importlib_modulespec",
    "sys",
    "abc",
}


Graph: _TypeAlias = Dict[str, "State"]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.197"># TODO: Get rid of BuildResult.  We might as well return a BuildManager.
class BuildResult:
    """The result of a successful build.

    Attributes:
      manager: The build manager.
      files:   Dictionary from module name to related AST node.
      types:   Dictionary from parse tree node to its inferred type.
      used_cache: Whether the build took advantage of a pre-existing cache
      errors:  List of error messages.
    """

    @others
</t>
<t tx="ekr.20221004064034.198">def __init__(self, manager: BuildManager, graph: Graph) -&gt; None:
    self.manager = manager
    self.graph = graph
    self.files = manager.modules
    self.types = manager.all_types  # Non-empty if export_types True in options
    self.used_cache = manager.cache_enabled
    self.errors: list[str] = []  # Filled in by build if desired


</t>
<t tx="ekr.20221004064034.199">def build(
    sources: list[BuildSource],
    options: Options,
    alt_lib_path: str | None = None,
    flush_errors: Callable[[list[str], bool], None] | None = None,
    fscache: FileSystemCache | None = None,
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
    extra_plugins: Sequence[Plugin] | None = None,
) -&gt; BuildResult:
    """Analyze a program.

    A single call to build performs parsing, semantic analysis and optionally
    type checking for the program *and* all imported modules, recursively.

    Return BuildResult if successful or only non-blocking errors were found;
    otherwise raise CompileError.

    If a flush_errors callback is provided, all error messages will be
    passed to it and the errors and messages fields of BuildResult and
    CompileError (respectively) will be empty. Otherwise those fields will
    report any error messages.

    Args:
      sources: list of sources to build
      options: build options
      alt_lib_path: an additional directory for looking up library modules
        (takes precedence over other directories)
      flush_errors: optional function to flush errors after a file is processed
      fscache: optionally a file-system cacher

    """
    # If we were not given a flush_errors, we use one that will populate those
    # fields for callers that want the traditional API.
    messages = []

    @others
    flush_errors = flush_errors or default_flush_errors
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr
    extra_plugins = extra_plugins or []
    
    ### from leo.core import leoGlobals as g  ###
    ### g.printObj([repr(z) for z in sources])
    
    ### import pdb ; pdb.set_trace()  ###

    try:
        result = _build(
            sources, options, alt_lib_path, flush_errors, fscache, stdout, stderr, extra_plugins
        )
        result.errors = messages
        return result
    except CompileError as e:
        # CompileErrors raised from an errors object carry all of the
        # messages that have not been reported out by error streaming.
        # Patch it up to contain either none or all none of the messages,
        # depending on whether we are flushing errors.
        serious = not e.use_stdout
        flush_errors(e.messages, serious)
        e.messages = messages
        raise


</t>
<t tx="ekr.20221004064034.200">def default_flush_errors(new_messages: list[str], is_serious: bool) -&gt; None:
    messages.extend(new_messages)

</t>
<t tx="ekr.20221004064034.201">def _build(
    sources: list[BuildSource],
    options: Options,
    alt_lib_path: str | None,
    flush_errors: Callable[[list[str], bool], None],
    fscache: FileSystemCache | None,
    stdout: TextIO,
    stderr: TextIO,
    extra_plugins: Sequence[Plugin],
) -&gt; BuildResult:
    if platform.python_implementation() == "CPython":
        # This seems the most reasonable place to tune garbage collection.
        gc.set_threshold(150 * 1000)

    data_dir = default_data_dir()
    fscache = fscache or FileSystemCache()

    search_paths = compute_search_paths(sources, options, data_dir, alt_lib_path)

    reports = None
    if options.report_dirs:
        # Import lazily to avoid slowing down startup.
        from mypy.report import Reports

        reports = Reports(data_dir, options.report_dirs)

    source_set = BuildSourceSet(sources)
    cached_read = fscache.read
    errors = Errors(
        options.show_error_context,
        options.show_column_numbers,
        options.hide_error_codes,
        options.pretty,
        options.show_error_end,
        lambda path: read_py_file(path, cached_read),
        options.show_absolute_path,
        options.many_errors_threshold,
        options,
    )
    plugin, snapshot = load_plugins(options, errors, stdout, extra_plugins)

    # Add catch-all .gitignore to cache dir if we created it
    cache_dir_existed = os.path.isdir(options.cache_dir)

    # Construct a build manager object to hold state during the build.
    #
    # Ignore current directory prefix in error messages.
    manager = BuildManager(
        data_dir,
        search_paths,
        ignore_prefix=os.getcwd(),
        source_set=source_set,
        reports=reports,
        options=options,
        version_id=__version__,
        plugin=plugin,
        plugins_snapshot=snapshot,
        errors=errors,
        flush_errors=flush_errors,
        fscache=fscache,
        stdout=stdout,
        stderr=stderr,
    )
    manager.trace(repr(options))

    reset_global_state()
    try:
        graph = dispatch(sources, manager, stdout)
        if not options.fine_grained_incremental:
            TypeState.reset_all_subtype_caches()
        if options.timing_stats is not None:
            dump_timing_stats(options.timing_stats, graph)
        return BuildResult(manager, graph)
    finally:
        t0 = time.time()
        manager.metastore.commit()
        manager.add_stats(cache_commit_time=time.time() - t0)
        manager.log(
            "Build finished in %.3f seconds with %d modules, and %d errors"
            % (
                time.time() - manager.start_time,
                len(manager.modules),
                manager.errors.num_messages(),
            )
        )
        manager.dump_stats()
        if reports is not None:
            # Finish the HTML or XML reports even if CompileError was raised.
            reports.finish()
        if not cache_dir_existed and os.path.isdir(options.cache_dir):
            add_catch_all_gitignore(options.cache_dir)
            exclude_from_backups(options.cache_dir)
        if os.path.isdir(options.cache_dir):
            record_missing_stub_packages(options.cache_dir, manager.missing_stub_packages)


</t>
<t tx="ekr.20221004064034.202">def default_data_dir() -&gt; str:
    """Returns directory containing typeshed directory."""
    return os.path.dirname(__file__)


</t>
<t tx="ekr.20221004064034.203">def normpath(path: str, options: Options) -&gt; str:
    """Convert path to absolute; but to relative in bazel mode.

    (Bazel's distributed cache doesn't like filesystem metadata to
    end up in output files.)
    """
    # TODO: Could we always use relpath?  (A worry in non-bazel
    # mode would be that a moved file may change its full module
    # name without changing its size, mtime or hash.)
    if options.bazel:
        return os.path.relpath(path)
    else:
        return os.path.abspath(path)


</t>
<t tx="ekr.20221004064034.204">class CacheMeta(NamedTuple):
    id: str
    path: str
    mtime: int
    size: int
    hash: str
    dependencies: list[str]  # names of imported modules
    data_mtime: int  # mtime of data_json
    data_json: str  # path of &lt;id&gt;.data.json
    suppressed: list[str]  # dependencies that weren't imported
    options: dict[str, object] | None  # build options
    # dep_prios and dep_lines are in parallel with dependencies + suppressed
    dep_prios: list[int]
    dep_lines: list[int]
    interface_hash: str  # hash representing the public interface
    version_id: str  # mypy version for cache invalidation
    ignore_all: bool  # if errors were ignored
    plugin_data: Any  # config data from plugins


</t>
<t tx="ekr.20221004064034.205"># NOTE: dependencies + suppressed == all reachable imports;
# suppressed contains those reachable imports that were prevented by
# silent mode or simply not found.


# Metadata for the fine-grained dependencies file associated with a module.
FgDepMeta = TypedDict("FgDepMeta", {"path": str, "mtime": int})


</t>
<t tx="ekr.20221004064034.206">def cache_meta_from_dict(meta: dict[str, Any], data_json: str) -&gt; CacheMeta:
    """Build a CacheMeta object from a json metadata dictionary

    Args:
      meta: JSON metadata read from the metadata cache file
      data_json: Path to the .data.json file containing the AST trees
    """
    sentinel: Any = None  # Values to be validated by the caller
    return CacheMeta(
        meta.get("id", sentinel),
        meta.get("path", sentinel),
        int(meta["mtime"]) if "mtime" in meta else sentinel,
        meta.get("size", sentinel),
        meta.get("hash", sentinel),
        meta.get("dependencies", []),
        int(meta["data_mtime"]) if "data_mtime" in meta else sentinel,
        data_json,
        meta.get("suppressed", []),
        meta.get("options"),
        meta.get("dep_prios", []),
        meta.get("dep_lines", []),
        meta.get("interface_hash", ""),
        meta.get("version_id", sentinel),
        meta.get("ignore_all", True),
        meta.get("plugin_data", None),
    )


</t>
<t tx="ekr.20221004064034.207"># Priorities used for imports.  (Here, top-level includes inside a class.)
# These are used to determine a more predictable order in which the
# nodes in an import cycle are processed.
PRI_HIGH: Final = 5  # top-level "from X import blah"
PRI_MED: Final = 10  # top-level "import X"
PRI_LOW: Final = 20  # either form inside a function
PRI_MYPY: Final = 25  # inside "if MYPY" or "if TYPE_CHECKING"
PRI_INDIRECT: Final = 30  # an indirect dependency
PRI_ALL: Final = 99  # include all priorities


</t>
<t tx="ekr.20221004064034.208">def import_priority(imp: ImportBase, toplevel_priority: int) -&gt; int:
    """Compute import priority from an import node."""
    if not imp.is_top_level:
        # Inside a function
        return PRI_LOW
    if imp.is_mypy_only:
        # Inside "if MYPY" or "if typing.TYPE_CHECKING"
        return max(PRI_MYPY, toplevel_priority)
    # A regular import; priority determined by argument.
    return toplevel_priority


</t>
<t tx="ekr.20221004064034.209">def load_plugins_from_config(
    options: Options, errors: Errors, stdout: TextIO
) -&gt; tuple[list[Plugin], dict[str, str]]:
    """Load all configured plugins.

    Return a list of all the loaded plugins from the config file.
    The second return value is a snapshot of versions/hashes of loaded user
    plugins (for cache validation).
    """
    import importlib

    snapshot: dict[str, str] = {}

    if not options.config_file:
        return [], snapshot

    line = find_config_file_line_number(options.config_file, "mypy", "plugins")
    if line == -1:
        line = 1  # We need to pick some line number that doesn't look too confusing

    @others
    custom_plugins: list[Plugin] = []
    errors.set_file(options.config_file, None, options)
    for plugin_path in options.plugins:
        func_name = "plugin"
        plugin_dir: str | None = None
        if ":" in os.path.basename(plugin_path):
            plugin_path, func_name = plugin_path.rsplit(":", 1)
        if plugin_path.endswith(".py"):
            # Plugin paths can be relative to the config file location.
            plugin_path = os.path.join(os.path.dirname(options.config_file), plugin_path)
            if not os.path.isfile(plugin_path):
                plugin_error(f'Can\'t find plugin "{plugin_path}"')
            # Use an absolute path to avoid populating the cache entry
            # for 'tmp' during tests, since it will be different in
            # different tests.
            plugin_dir = os.path.abspath(os.path.dirname(plugin_path))
            fnam = os.path.basename(plugin_path)
            module_name = fnam[:-3]
            sys.path.insert(0, plugin_dir)
        elif re.search(r"[\\/]", plugin_path):
            fnam = os.path.basename(plugin_path)
            plugin_error(f'Plugin "{fnam}" does not have a .py extension')
        else:
            module_name = plugin_path

        try:
            module = importlib.import_module(module_name)
        except Exception as exc:
            plugin_error(f'Error importing plugin "{plugin_path}": {exc}')
        finally:
            if plugin_dir is not None:
                assert sys.path[0] == plugin_dir
                del sys.path[0]

        if not hasattr(module, func_name):
            plugin_error(
                'Plugin "{}" does not define entry point function "{}"'.format(
                    plugin_path, func_name
                )
            )

        try:
            plugin_type = getattr(module, func_name)(__version__)
        except Exception:
            print(f"Error calling the plugin(version) entry point of {plugin_path}\n", file=stdout)
            raise  # Propagate to display traceback

        if not isinstance(plugin_type, type):
            plugin_error(
                'Type object expected as the return value of "plugin"; got {!r} (in {})'.format(
                    plugin_type, plugin_path
                )
            )
        if not issubclass(plugin_type, Plugin):
            plugin_error(
                'Return value of "plugin" must be a subclass of "mypy.plugin.Plugin" '
                "(in {})".format(plugin_path)
            )
        try:
            custom_plugins.append(plugin_type(options))
            snapshot[module_name] = take_module_snapshot(module)
        except Exception:
            print(f"Error constructing plugin instance of {plugin_type.__name__}\n", file=stdout)
            raise  # Propagate to display traceback

    return custom_plugins, snapshot


</t>
<t tx="ekr.20221004064034.210">def plugin_error(message: str) -&gt; NoReturn:
    errors.report(line, 0, message)
    errors.raise_error(use_stdout=False)

</t>
<t tx="ekr.20221004064034.211">def load_plugins(
    options: Options, errors: Errors, stdout: TextIO, extra_plugins: Sequence[Plugin]
) -&gt; tuple[Plugin, dict[str, str]]:
    """Load all configured plugins.

    Return a plugin that encapsulates all plugins chained together. Always
    at least include the default plugin (it's last in the chain).
    The second return value is a snapshot of versions/hashes of loaded user
    plugins (for cache validation).
    """
    custom_plugins, snapshot = load_plugins_from_config(options, errors, stdout)

    custom_plugins += extra_plugins

    default_plugin: Plugin = DefaultPlugin(options)
    if not custom_plugins:
        return default_plugin, snapshot

    # Custom plugins take precedence over the default plugin.
    return ChainedPlugin(options, custom_plugins + [default_plugin]), snapshot


</t>
<t tx="ekr.20221004064034.212">def take_module_snapshot(module: types.ModuleType) -&gt; str:
    """Take plugin module snapshot by recording its version and hash.

    We record _both_ hash and the version to detect more possible changes
    (e.g. if there is a change in modules imported by a plugin).
    """
    if hasattr(module, "__file__"):
        assert module.__file__ is not None
        with open(module.__file__, "rb") as f:
            digest = hash_digest(f.read())
    else:
        digest = "unknown"
    ver = getattr(module, "__version__", "none")
    return f"{ver}:{digest}"


</t>
<t tx="ekr.20221004064034.213">def find_config_file_line_number(path: str, section: str, setting_name: str) -&gt; int:
    """Return the approximate location of setting_name within mypy config file.

    Return -1 if can't determine the line unambiguously.
    """
    in_desired_section = False
    try:
        results = []
        with open(path, encoding="UTF-8") as f:
            for i, line in enumerate(f):
                line = line.strip()
                if line.startswith("[") and line.endswith("]"):
                    current_section = line[1:-1].strip()
                    in_desired_section = current_section == section
                elif in_desired_section and re.match(rf"{setting_name}\s*=", line):
                    results.append(i + 1)
        if len(results) == 1:
            return results[0]
    except OSError:
        pass
    return -1


</t>
<t tx="ekr.20221004064034.214">class BuildManager:
    """This class holds shared state for building a mypy program.

    It is used to coordinate parsing, import processing, semantic
    analysis and type checking.  The actual build steps are carried
    out by dispatch().

    Attributes:
      data_dir:        Mypy data directory (contains stubs)
      search_paths:    SearchPaths instance indicating where to look for modules
      modules:         Mapping of module ID to MypyFile (shared by the passes)
      semantic_analyzer:
                       Semantic analyzer, pass 2
      all_types:       Map {Expression: Type} from all modules (enabled by export_types)
      options:         Build options
      missing_modules: Set of modules that could not be imported encountered so far
      stale_modules:   Set of modules that needed to be rechecked (only used by tests)
      fg_deps_meta:    Metadata for fine-grained dependencies caches associated with modules
      fg_deps:         A fine-grained dependency map
      version_id:      The current mypy version (based on commit id when possible)
      plugin:          Active mypy plugin(s)
      plugins_snapshot:
                       Snapshot of currently active user plugins (versions and hashes)
      old_plugins_snapshot:
                       Plugins snapshot from previous incremental run (or None in
                       non-incremental mode and if cache was not found)
      errors:          Used for reporting all errors
      flush_errors:    A function for processing errors after each SCC
      cache_enabled:   Whether cache is being read. This is set based on options,
                       but is disabled if fine-grained cache loading fails
                       and after an initial fine-grained load. This doesn't
                       determine whether we write cache files or not.
      quickstart_state:
                       A cache of filename -&gt; mtime/size/hash info used to avoid
                       needing to hash source files when using a cache with mismatching mtimes
      stats:           Dict with various instrumentation numbers, it is used
                       not only for debugging, but also required for correctness,
                       in particular to check consistency of the fine-grained dependency cache.
      fscache:         A file system cacher
      ast_cache:       AST cache to speed up mypy daemon
    """

    @others
</t>
<t tx="ekr.20221004064034.215">def __init__(
    self,
    data_dir: str,
    search_paths: SearchPaths,
    ignore_prefix: str,
    source_set: BuildSourceSet,
    reports: Reports | None,
    options: Options,
    version_id: str,
    plugin: Plugin,
    plugins_snapshot: dict[str, str],
    errors: Errors,
    flush_errors: Callable[[list[str], bool], None],
    fscache: FileSystemCache,
    stdout: TextIO,
    stderr: TextIO,
) -&gt; None:
    self.stats: dict[str, Any] = {}  # Values are ints or floats
    self.stdout = stdout
    self.stderr = stderr
    self.start_time = time.time()
    self.data_dir = data_dir
    self.errors = errors
    self.errors.set_ignore_prefix(ignore_prefix)
    self.search_paths = search_paths
    self.source_set = source_set
    self.reports = reports
    self.options = options
    self.version_id = version_id
    self.modules: dict[str, MypyFile] = {}
    self.missing_modules: set[str] = set()
    self.fg_deps_meta: dict[str, FgDepMeta] = {}
    # fg_deps holds the dependencies of every module that has been
    # processed. We store this in BuildManager so that we can compute
    # dependencies as we go, which allows us to free ASTs and type information,
    # saving a ton of memory on net.
    self.fg_deps: dict[str, set[str]] = {}
    # Always convert the plugin to a ChainedPlugin so that it can be manipulated if needed
    if not isinstance(plugin, ChainedPlugin):
        plugin = ChainedPlugin(options, [plugin])
    self.plugin = plugin
    # Set of namespaces (module or class) that are being populated during semantic
    # analysis and may have missing definitions.
    self.incomplete_namespaces: set[str] = set()
    self.semantic_analyzer = SemanticAnalyzer(
        self.modules,
        self.missing_modules,
        self.incomplete_namespaces,
        self.errors,
        self.plugin,
    )
    self.all_types: dict[Expression, Type] = {}  # Enabled by export_types
    self.indirection_detector = TypeIndirectionVisitor()
    self.stale_modules: set[str] = set()
    self.rechecked_modules: set[str] = set()
    self.flush_errors = flush_errors
    has_reporters = reports is not None and reports.reporters
    self.cache_enabled = (
        options.incremental
        and (not options.fine_grained_incremental or options.use_fine_grained_cache)
        and not has_reporters
    )
    self.fscache = fscache
    self.find_module_cache = FindModuleCache(
        self.search_paths, self.fscache, self.options, source_set=self.source_set
    )
    for module in CORE_BUILTIN_MODULES:
        if options.use_builtins_fixtures:
            continue
        if module == "_importlib_modulespec":
            continue
        path = self.find_module_cache.find_module(module)
        if not isinstance(path, str):
            raise CompileError(
                [f"Failed to find builtin module {module}, perhaps typeshed is broken?"]
            )
        if is_typeshed_file(options.abs_custom_typeshed_dir, path) or is_stub_package_file(
            path
        ):
            continue

        raise CompileError(
            [
                f'mypy: "{os.path.relpath(path)}" shadows library module "{module}"',
                f'note: A user-defined top-level module with name "{module}" is not supported',
            ]
        )

    self.metastore = create_metastore(options)

    # a mapping from source files to their corresponding shadow files
    # for efficient lookup
    self.shadow_map: dict[str, str] = {}
    if self.options.shadow_file is not None:
        self.shadow_map = {
            source_file: shadow_file for (source_file, shadow_file) in self.options.shadow_file
        }
    # a mapping from each file being typechecked to its possible shadow file
    self.shadow_equivalence_map: dict[str, str | None] = {}
    self.plugin = plugin
    self.plugins_snapshot = plugins_snapshot
    self.old_plugins_snapshot = read_plugins_snapshot(self)
    self.quickstart_state = read_quickstart_file(options, self.stdout)
    # Fine grained targets (module top levels and top level functions) processed by
    # the semantic analyzer, used only for testing. Currently used only by the new
    # semantic analyzer.
    self.processed_targets: list[str] = []
    # Missing stub packages encountered.
    self.missing_stub_packages: set[str] = set()
    # Cache for mypy ASTs that have completed semantic analysis
    # pass 1. When multiple files are added to the build in a
    # single daemon increment, only one of the files gets added
    # per step and the others are discarded. This gets repeated
    # until all the files have been added. This means that a
    # new file can be processed O(n**2) times. This cache
    # avoids most of this redundant work.
    self.ast_cache: dict[str, tuple[MypyFile, list[ErrorInfo]]] = {}

</t>
<t tx="ekr.20221004064034.216">def dump_stats(self) -&gt; None:
    if self.options.dump_build_stats:
        print("Stats:")
        for key, value in sorted(self.stats_summary().items()):
            print(f"{key + ':':24}{value}")

</t>
<t tx="ekr.20221004064034.217">def use_fine_grained_cache(self) -&gt; bool:
    return self.cache_enabled and self.options.use_fine_grained_cache

</t>
<t tx="ekr.20221004064034.218">def maybe_swap_for_shadow_path(self, path: str) -&gt; str:
    if not self.shadow_map:
        return path

    path = normpath(path, self.options)

    previously_checked = path in self.shadow_equivalence_map
    if not previously_checked:
        for source, shadow in self.shadow_map.items():
            if self.fscache.samefile(path, source):
                self.shadow_equivalence_map[path] = shadow
                break
            else:
                self.shadow_equivalence_map[path] = None

    shadow_file = self.shadow_equivalence_map.get(path)
    return shadow_file if shadow_file else path

</t>
<t tx="ekr.20221004064034.219">def get_stat(self, path: str) -&gt; os.stat_result:
    return self.fscache.stat(self.maybe_swap_for_shadow_path(path))

</t>
<t tx="ekr.20221004064034.220">def getmtime(self, path: str) -&gt; int:
    """Return a file's mtime; but 0 in bazel mode.

    (Bazel's distributed cache doesn't like filesystem metadata to
    end up in output files.)
    """
    if self.options.bazel:
        return 0
    else:
        return int(self.metastore.getmtime(path))

</t>
<t tx="ekr.20221004064034.221">def all_imported_modules_in_file(self, file: MypyFile) -&gt; list[tuple[int, str, int]]:
    """Find all reachable import statements in a file.

    Return list of tuples (priority, module id, import line number)
    for all modules imported in file; lower numbers == higher priority.

    Can generate blocking errors on bogus relative imports.
    """

    def correct_rel_imp(imp: ImportFrom | ImportAll) -&gt; str:
        """Function to correct for relative imports."""
        file_id = file.fullname
        rel = imp.relative
        if rel == 0:
            return imp.id
        if os.path.basename(file.path).startswith("__init__."):
            rel -= 1
        if rel != 0:
            file_id = ".".join(file_id.split(".")[:-rel])
        new_id = file_id + "." + imp.id if imp.id else file_id

        if not new_id:
            self.errors.set_file(file.path, file.name, self.options)
            self.errors.report(
                imp.line, 0, "No parent module -- cannot perform relative import", blocker=True
            )

        return new_id

    res: list[tuple[int, str, int]] = []
    for imp in file.imports:
        if not imp.is_unreachable:
            if isinstance(imp, Import):
                pri = import_priority(imp, PRI_MED)
                ancestor_pri = import_priority(imp, PRI_LOW)
                for id, _ in imp.ids:
                    res.append((pri, id, imp.line))
                    ancestor_parts = id.split(".")[:-1]
                    ancestors = []
                    for part in ancestor_parts:
                        ancestors.append(part)
                        res.append((ancestor_pri, ".".join(ancestors), imp.line))
            elif isinstance(imp, ImportFrom):
                cur_id = correct_rel_imp(imp)
                all_are_submodules = True
                # Also add any imported names that are submodules.
                pri = import_priority(imp, PRI_MED)
                for name, __ in imp.names:
                    sub_id = cur_id + "." + name
                    if self.is_module(sub_id):
                        res.append((pri, sub_id, imp.line))
                    else:
                        all_are_submodules = False
                # Add cur_id as a dependency, even if all of the
                # imports are submodules. Processing import from will try
                # to look through cur_id, so we should depend on it.
                # As a workaround for for some bugs in cycle handling (#4498),
                # if all of the imports are submodules, do the import at a lower
                # priority.
                pri = import_priority(imp, PRI_HIGH if not all_are_submodules else PRI_LOW)
                res.append((pri, cur_id, imp.line))
            elif isinstance(imp, ImportAll):
                pri = import_priority(imp, PRI_HIGH)
                res.append((pri, correct_rel_imp(imp), imp.line))

    # Sort such that module (e.g. foo.bar.baz) comes before its ancestors (e.g. foo
    # and foo.bar) so that, if FindModuleCache finds the target module in a
    # package marked with py.typed underneath a namespace package installed in
    # site-packages, (gasp), that cache's knowledge of the ancestors
    # (aka FindModuleCache.ns_ancestors) can be primed when it is asked to find
    # the parent.
    res.sort(key=lambda x: -x[1].count("."))
    return res

</t>
<t tx="ekr.20221004064034.222">def is_module(self, id: str) -&gt; bool:
    """Is there a file in the file system corresponding to module id?"""
    return find_module_simple(id, self) is not None

</t>
<t tx="ekr.20221004064034.223">def parse_file(
    self, id: str, path: str, source: str, ignore_errors: bool, options: Options
) -&gt; MypyFile:
    """Parse the source of a file with the given name.

    Raise CompileError if there is a parse error.
    """
    t0 = time.time()
    tree = parse(source, path, id, self.errors, options=options)
    tree._fullname = id
    self.add_stats(
        files_parsed=1,
        modules_parsed=int(not tree.is_stub),
        stubs_parsed=int(tree.is_stub),
        parse_time=time.time() - t0,
    )

    if self.errors.is_blockers():
        self.log("Bailing due to parse errors")
        self.errors.raise_error()

    self.errors.set_file_ignored_lines(path, tree.ignored_lines, ignore_errors)
    return tree

</t>
<t tx="ekr.20221004064034.224">def load_fine_grained_deps(self, id: str) -&gt; dict[str, set[str]]:
    t0 = time.time()
    if id in self.fg_deps_meta:
        # TODO: Assert deps file wasn't changed.
        deps = json.loads(self.metastore.read(self.fg_deps_meta[id]["path"]))
    else:
        deps = {}
    val = {k: set(v) for k, v in deps.items()}
    self.add_stats(load_fg_deps_time=time.time() - t0)
    return val

</t>
<t tx="ekr.20221004064034.225">def report_file(
    self, file: MypyFile, type_map: dict[Expression, Type], options: Options
) -&gt; None:
    if self.reports is not None and self.source_set.is_source(file):
        self.reports.file(file, self.modules, type_map, options)

</t>
<t tx="ekr.20221004064034.226">def verbosity(self) -&gt; int:
    return self.options.verbosity

</t>
<t tx="ekr.20221004064034.227">def log(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 1:
        if message:
            print("LOG: ", *message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20221004064034.228">def log_fine_grained(self, *message: str) -&gt; None:
    import mypy.build

    if self.verbosity() &gt;= 1:
        self.log("fine-grained:", *message)
    elif mypy.build.DEBUG_FINE_GRAINED:
        # Output log in a simplified format that is quick to browse.
        if message:
            print(*message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20221004064034.229">def trace(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 2:
        print("TRACE:", *message, file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20221004064034.230">def add_stats(self, **kwds: Any) -&gt; None:
    for key, value in kwds.items():
        if key in self.stats:
            self.stats[key] += value
        else:
            self.stats[key] = value

</t>
<t tx="ekr.20221004064034.231">def stats_summary(self) -&gt; Mapping[str, object]:
    return self.stats


</t>
<t tx="ekr.20221004064034.232">def deps_to_json(x: dict[str, set[str]]) -&gt; str:
    return json.dumps({k: list(v) for k, v in x.items()})


</t>
<t tx="ekr.20221004064034.233"># File for storing metadata about all the fine-grained dependency caches
DEPS_META_FILE: Final = "@deps.meta.json"
# File for storing fine-grained dependencies that didn't a parent in the build
DEPS_ROOT_FILE: Final = "@root.deps.json"

# The name of the fake module used to store fine-grained dependencies that
# have no other place to go.
FAKE_ROOT_MODULE: Final = "@root"


</t>
<t tx="ekr.20221004064034.234">def write_deps_cache(
    rdeps: dict[str, dict[str, set[str]]], manager: BuildManager, graph: Graph
) -&gt; None:
    """Write cache files for fine-grained dependencies.

    Serialize fine-grained dependencies map for fine grained mode.

    Dependencies on some module 'm' is stored in the dependency cache
    file m.deps.json.  This entails some spooky action at a distance:
    if module 'n' depends on 'm', that produces entries in m.deps.json.
    When there is a dependency on a module that does not exist in the
    build, it is stored with its first existing parent module. If no
    such module exists, it is stored with the fake module FAKE_ROOT_MODULE.

    This means that the validity of the fine-grained dependency caches
    are a global property, so we store validity checking information for
    fine-grained dependencies in a global cache file:
     * We take a snapshot of current sources to later check consistency
       between the fine-grained dependency cache and module cache metadata
     * We store the mtime of all of the dependency files to verify they
       haven't changed
    """
    metastore = manager.metastore

    error = False

    fg_deps_meta = manager.fg_deps_meta.copy()

    for id in rdeps:
        if id != FAKE_ROOT_MODULE:
            _, _, deps_json = get_cache_names(id, graph[id].xpath, manager.options)
        else:
            deps_json = DEPS_ROOT_FILE
        assert deps_json
        manager.log("Writing deps cache", deps_json)
        if not manager.metastore.write(deps_json, deps_to_json(rdeps[id])):
            manager.log(f"Error writing fine-grained deps JSON file {deps_json}")
            error = True
        else:
            fg_deps_meta[id] = {"path": deps_json, "mtime": manager.getmtime(deps_json)}

    meta_snapshot: dict[str, str] = {}
    for id, st in graph.items():
        # If we didn't parse a file (so it doesn't have a
        # source_hash), then it must be a module with a fresh cache,
        # so use the hash from that.
        if st.source_hash:
            hash = st.source_hash
        else:
            assert st.meta, "Module must be either parsed or cached"
            hash = st.meta.hash
        meta_snapshot[id] = hash

    meta = {"snapshot": meta_snapshot, "deps_meta": fg_deps_meta}

    if not metastore.write(DEPS_META_FILE, json.dumps(meta)):
        manager.log(f"Error writing fine-grained deps meta JSON file {DEPS_META_FILE}")
        error = True

    if error:
        manager.errors.set_file(_cache_dir_prefix(manager.options), None, manager.options)
        manager.errors.report(0, 0, "Error writing fine-grained dependencies cache", blocker=True)


</t>
<t tx="ekr.20221004064034.235">def invert_deps(deps: dict[str, set[str]], graph: Graph) -&gt; dict[str, dict[str, set[str]]]:
    """Splits fine-grained dependencies based on the module of the trigger.

    Returns a dictionary from module ids to all dependencies on that
    module. Dependencies not associated with a module in the build will be
    associated with the nearest parent module that is in the build, or the
    fake module FAKE_ROOT_MODULE if none are.
    """
    # Lazy import to speed up startup
    from mypy.server.target import trigger_to_target

    # Prepopulate the map for all the modules that have been processed,
    # so that we always generate files for processed modules (even if
    # there aren't any dependencies to them.)
    rdeps: dict[str, dict[str, set[str]]] = {id: {} for id, st in graph.items() if st.tree}
    for trigger, targets in deps.items():
        module = module_prefix(graph, trigger_to_target(trigger))
        if not module or not graph[module].tree:
            module = FAKE_ROOT_MODULE

        mod_rdeps = rdeps.setdefault(module, {})
        mod_rdeps.setdefault(trigger, set()).update(targets)

    return rdeps


</t>
<t tx="ekr.20221004064034.236">def generate_deps_for_cache(manager: BuildManager, graph: Graph) -&gt; dict[str, dict[str, set[str]]]:
    """Generate fine-grained dependencies into a form suitable for serializing.

    This does a couple things:
    1. Splits fine-grained deps based on the module of the trigger
    2. For each module we generated fine-grained deps for, load any previous
       deps and merge them in.

    Returns a dictionary from module ids to all dependencies on that
    module. Dependencies not associated with a module in the build will be
    associated with the nearest parent module that is in the build, or the
    fake module FAKE_ROOT_MODULE if none are.
    """
    from mypy.server.deps import merge_dependencies  # Lazy import to speed up startup

    # Split the dependencies out into based on the module that is depended on.
    rdeps = invert_deps(manager.fg_deps, graph)

    # We can't just clobber existing dependency information, so we
    # load the deps for every module we've generated new dependencies
    # to and merge the new deps into them.
    for module, mdeps in rdeps.items():
        old_deps = manager.load_fine_grained_deps(module)
        merge_dependencies(old_deps, mdeps)

    return rdeps


</t>
<t tx="ekr.20221004064034.237">PLUGIN_SNAPSHOT_FILE: Final = "@plugins_snapshot.json"


</t>
<t tx="ekr.20221004064034.238">def write_plugins_snapshot(manager: BuildManager) -&gt; None:
    """Write snapshot of versions and hashes of currently active plugins."""
    if not manager.metastore.write(PLUGIN_SNAPSHOT_FILE, json.dumps(manager.plugins_snapshot)):
        manager.errors.set_file(_cache_dir_prefix(manager.options), None, manager.options)
        manager.errors.report(0, 0, "Error writing plugins snapshot", blocker=True)


</t>
<t tx="ekr.20221004064034.239">def read_plugins_snapshot(manager: BuildManager) -&gt; dict[str, str] | None:
    """Read cached snapshot of versions and hashes of plugins from previous run."""
    snapshot = _load_json_file(
        PLUGIN_SNAPSHOT_FILE,
        manager,
        log_success="Plugins snapshot ",
        log_error="Could not load plugins snapshot: ",
    )
    if snapshot is None:
        return None
    if not isinstance(snapshot, dict):
        manager.log(f"Could not load plugins snapshot: cache is not a dict: {type(snapshot)}")
        return None
    return snapshot


</t>
<t tx="ekr.20221004064034.240">def read_quickstart_file(
    options: Options, stdout: TextIO
) -&gt; dict[str, tuple[float, int, str]] | None:
    quickstart: dict[str, tuple[float, int, str]] | None = None
    if options.quickstart_file:
        # This is very "best effort". If the file is missing or malformed,
        # just ignore it.
        raw_quickstart: dict[str, Any] = {}
        try:
            with open(options.quickstart_file) as f:
                raw_quickstart = json.load(f)

            quickstart = {}
            for file, (x, y, z) in raw_quickstart.items():
                quickstart[file] = (x, y, z)
        except Exception as e:
            print(f"Warning: Failed to load quickstart file: {str(e)}\n", file=stdout)
    return quickstart


</t>
<t tx="ekr.20221004064034.241">def read_deps_cache(manager: BuildManager, graph: Graph) -&gt; dict[str, FgDepMeta] | None:
    """Read and validate the fine-grained dependencies cache.

    See the write_deps_cache documentation for more information on
    the details of the cache.

    Returns None if the cache was invalid in some way.
    """
    deps_meta = _load_json_file(
        DEPS_META_FILE,
        manager,
        log_success="Deps meta ",
        log_error="Could not load fine-grained dependency metadata: ",
    )
    if deps_meta is None:
        return None
    meta_snapshot = deps_meta["snapshot"]
    # Take a snapshot of the source hashes from all of the metas we found.
    # (Including the ones we rejected because they were out of date.)
    # We use this to verify that they match up with the proto_deps.
    current_meta_snapshot = {
        id: st.meta_source_hash for id, st in graph.items() if st.meta_source_hash is not None
    }

    common = set(meta_snapshot.keys()) &amp; set(current_meta_snapshot.keys())
    if any(meta_snapshot[id] != current_meta_snapshot[id] for id in common):
        # TODO: invalidate also if options changed (like --strict-optional)?
        manager.log("Fine-grained dependencies cache inconsistent, ignoring")
        return None

    module_deps_metas = deps_meta["deps_meta"]
    assert isinstance(module_deps_metas, dict)
    if not manager.options.skip_cache_mtime_checks:
        for id, meta in module_deps_metas.items():
            try:
                matched = manager.getmtime(meta["path"]) == meta["mtime"]
            except FileNotFoundError:
                matched = False
            if not matched:
                manager.log(f"Invalid or missing fine-grained deps cache: {meta['path']}")
                return None

    return module_deps_metas


</t>
<t tx="ekr.20221004064034.242">def _load_json_file(
    file: str, manager: BuildManager, log_success: str, log_error: str
) -&gt; dict[str, Any] | None:
    """A simple helper to read a JSON file with logging."""
    t0 = time.time()
    try:
        data = manager.metastore.read(file)
    except OSError:
        manager.log(log_error + file)
        return None
    manager.add_stats(metastore_read_time=time.time() - t0)
    # Only bother to compute the log message if we are logging it, since it could be big
    if manager.verbosity() &gt;= 2:
        manager.trace(log_success + data.rstrip())
    try:
        t1 = time.time()
        result = json.loads(data)
        manager.add_stats(data_json_load_time=time.time() - t1)
    except json.JSONDecodeError:
        manager.errors.set_file(file, None, manager.options)
        manager.errors.report(
            -1,
            -1,
            "Error reading JSON file;"
            " you likely have a bad cache.\n"
            "Try removing the {cache_dir} directory"
            " and run mypy again.".format(cache_dir=manager.options.cache_dir),
            blocker=True,
        )
        return None
    else:
        assert isinstance(result, dict)
        return result


</t>
<t tx="ekr.20221004064034.243">def _cache_dir_prefix(options: Options) -&gt; str:
    """Get current cache directory (or file if id is given)."""
    if options.bazel:
        # This is needed so the cache map works.
        return os.curdir
    cache_dir = options.cache_dir
    pyversion = options.python_version
    base = os.path.join(cache_dir, "%d.%d" % pyversion)
    return base


</t>
<t tx="ekr.20221004064034.244">def add_catch_all_gitignore(target_dir: str) -&gt; None:
    """Add catch-all .gitignore to an existing directory.

    No-op if the .gitignore already exists.
    """
    gitignore = os.path.join(target_dir, ".gitignore")
    try:
        with open(gitignore, "x") as f:
            print("# Automatically created by mypy", file=f)
            print("*", file=f)
    except FileExistsError:
        pass


</t>
<t tx="ekr.20221004064034.245">def exclude_from_backups(target_dir: str) -&gt; None:
    """Exclude the directory from various archives and backups supporting CACHEDIR.TAG.

    If the CACHEDIR.TAG file exists the function is a no-op.
    """
    cachedir_tag = os.path.join(target_dir, "CACHEDIR.TAG")
    try:
        with open(cachedir_tag, "x") as f:
            f.write(
                """Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag automatically created by mypy.
# For information about cache directory tags see https://bford.info/cachedir/
"""
            )
    except FileExistsError:
        pass


</t>
<t tx="ekr.20221004064034.246">def create_metastore(options: Options) -&gt; MetadataStore:
    """Create the appropriate metadata store."""
    if options.sqlite_cache:
        mds: MetadataStore = SqliteMetadataStore(_cache_dir_prefix(options))
    else:
        mds = FilesystemMetadataStore(_cache_dir_prefix(options))
    return mds


</t>
<t tx="ekr.20221004064034.247">def get_cache_names(id: str, path: str, options: Options) -&gt; tuple[str, str, str | None]:
    """Return the file names for the cache files.

    Args:
      id: module ID
      path: module path
      cache_dir: cache directory
      pyversion: Python version (major, minor)

    Returns:
      A tuple with the file names to be used for the meta JSON, the
      data JSON, and the fine-grained deps JSON, respectively.
    """
    if options.cache_map:
        pair = options.cache_map.get(normpath(path, options))
    else:
        pair = None
    if pair is not None:
        # The cache map paths were specified relative to the base directory,
        # but the filesystem metastore APIs operates relative to the cache
        # prefix directory.
        # Solve this by rewriting the paths as relative to the root dir.
        # This only makes sense when using the filesystem backed cache.
        root = _cache_dir_prefix(options)
        return (os.path.relpath(pair[0], root), os.path.relpath(pair[1], root), None)
    prefix = os.path.join(*id.split("."))
    is_package = os.path.basename(path).startswith("__init__.py")
    if is_package:
        prefix = os.path.join(prefix, "__init__")

    deps_json = None
    if options.cache_fine_grained:
        deps_json = prefix + ".deps.json"
    return (prefix + ".meta.json", prefix + ".data.json", deps_json)


</t>
<t tx="ekr.20221004064034.248">def find_cache_meta(id: str, path: str, manager: BuildManager) -&gt; CacheMeta | None:
    """Find cache data for a module.

    Args:
      id: module ID
      path: module path
      manager: the build manager (for pyversion, log/trace, and build options)

    Returns:
      A CacheMeta instance if the cache data was found and appears
      valid; otherwise None.
    """
    # TODO: May need to take more build options into account
    meta_json, data_json, _ = get_cache_names(id, path, manager.options)
    manager.trace(f"Looking for {id} at {meta_json}")
    t0 = time.time()
    meta = _load_json_file(
        meta_json, manager, log_success=f"Meta {id} ", log_error=f"Could not load cache for {id}: "
    )
    t1 = time.time()
    if meta is None:
        return None
    if not isinstance(meta, dict):
        manager.log(f"Could not load cache for {id}: meta cache is not a dict: {repr(meta)}")
        return None
    m = cache_meta_from_dict(meta, data_json)
    t2 = time.time()
    manager.add_stats(
        load_meta_time=t2 - t0, load_meta_load_time=t1 - t0, load_meta_from_dict_time=t2 - t1
    )

    # Don't check for path match, that is dealt with in validate_meta().
    #
    # TODO: these `type: ignore`s wouldn't be necessary
    # if the type annotations for CacheMeta were more accurate
    # (all of these attributes can be `None`)
    if (
        m.id != id
        or m.mtime is None  # type: ignore[redundant-expr]
        or m.size is None  # type: ignore[redundant-expr]
        or m.dependencies is None  # type: ignore[redundant-expr]
        or m.data_mtime is None
    ):
        manager.log(f"Metadata abandoned for {id}: attributes are missing")
        return None

    # Ignore cache if generated by an older mypy version.
    if (
        (m.version_id != manager.version_id and not manager.options.skip_version_check)
        or m.options is None
        or len(m.dependencies) + len(m.suppressed) != len(m.dep_prios)
        or len(m.dependencies) + len(m.suppressed) != len(m.dep_lines)
    ):
        manager.log(f"Metadata abandoned for {id}: new attributes are missing")
        return None

    # Ignore cache if (relevant) options aren't the same.
    # Note that it's fine to mutilate cached_options since it's only used here.
    cached_options = m.options
    current_options = manager.options.clone_for_module(id).select_options_affecting_cache()
    if manager.options.skip_version_check:
        # When we're lax about version we're also lax about platform.
        cached_options["platform"] = current_options["platform"]
    if "debug_cache" in cached_options:
        # Older versions included debug_cache, but it's silly to compare it.
        del cached_options["debug_cache"]
    if cached_options != current_options:
        manager.log(f"Metadata abandoned for {id}: options differ")
        if manager.options.verbosity &gt;= 2:
            for key in sorted(set(cached_options) | set(current_options)):
                if cached_options.get(key) != current_options.get(key):
                    manager.trace(
                        "    {}: {} != {}".format(
                            key, cached_options.get(key), current_options.get(key)
                        )
                    )
        return None
    if manager.old_plugins_snapshot and manager.plugins_snapshot:
        # Check if plugins are still the same.
        if manager.plugins_snapshot != manager.old_plugins_snapshot:
            manager.log(f"Metadata abandoned for {id}: plugins differ")
            return None
    # So that plugins can return data with tuples in it without
    # things silently always invalidating modules, we round-trip
    # the config data. This isn't beautiful.
    plugin_data = json.loads(
        json.dumps(manager.plugin.report_config_data(ReportConfigContext(id, path, is_check=True)))
    )
    if m.plugin_data != plugin_data:
        manager.log(f"Metadata abandoned for {id}: plugin configuration differs")
        return None

    manager.add_stats(fresh_metas=1)
    return m


</t>
<t tx="ekr.20221004064034.249">def validate_meta(
    meta: CacheMeta | None, id: str, path: str | None, ignore_all: bool, manager: BuildManager
) -&gt; CacheMeta | None:
    """Checks whether the cached AST of this module can be used.

    Returns:
      None, if the cached AST is unusable.
      Original meta, if mtime/size matched.
      Meta with mtime updated to match source file, if hash/size matched but mtime/path didn't.
    """
    # This requires two steps. The first one is obvious: we check that the module source file
    # contents is the same as it was when the cache data file was created. The second one is not
    # too obvious: we check that the cache data file mtime has not changed; it is needed because
    # we use cache data file mtime to propagate information about changes in the dependencies.

    if meta is None:
        manager.log(f"Metadata not found for {id}")
        return None

    if meta.ignore_all and not ignore_all:
        manager.log(f"Metadata abandoned for {id}: errors were previously ignored")
        return None

    t0 = time.time()
    bazel = manager.options.bazel
    assert path is not None, "Internal error: meta was provided without a path"
    if not manager.options.skip_cache_mtime_checks:
        # Check data_json; assume if its mtime matches it's good.
        try:
            data_mtime = manager.getmtime(meta.data_json)
        except OSError:
            manager.log(f"Metadata abandoned for {id}: failed to stat data_json")
            return None
        if data_mtime != meta.data_mtime:
            manager.log(f"Metadata abandoned for {id}: data cache is modified")
            return None

    if bazel:
        # Normalize path under bazel to make sure it isn't absolute
        path = normpath(path, manager.options)
    try:
        st = manager.get_stat(path)
    except OSError:
        return None
    if not stat.S_ISDIR(st.st_mode) and not stat.S_ISREG(st.st_mode):
        manager.log(f"Metadata abandoned for {id}: file or directory {path} does not exist")
        return None

    manager.add_stats(validate_stat_time=time.time() - t0)

    # When we are using a fine-grained cache, we want our initial
    # build() to load all of the cache information and then do a
    # fine-grained incremental update to catch anything that has
    # changed since the cache was generated. We *don't* want to do a
    # coarse-grained incremental rebuild, so we accept the cache
    # metadata even if it doesn't match the source file.
    #
    # We still *do* the mtime/hash checks, however, to enable
    # fine-grained mode to take advantage of the mtime-updating
    # optimization when mtimes differ but hashes match.  There is
    # essentially no extra time cost to computing the hash here, since
    # it will be cached and will be needed for finding changed files
    # later anyways.
    fine_grained_cache = manager.use_fine_grained_cache()

    size = st.st_size
    # Bazel ensures the cache is valid.
    if size != meta.size and not bazel and not fine_grained_cache:
        manager.log(f"Metadata abandoned for {id}: file {path} has different size")
        return None

    # Bazel ensures the cache is valid.
    mtime = 0 if bazel else int(st.st_mtime)
    if not bazel and (mtime != meta.mtime or path != meta.path):
        if manager.quickstart_state and path in manager.quickstart_state:
            # If the mtime and the size of the file recorded in the quickstart dump matches
            # what we see on disk, we know (assume) that the hash matches the quickstart
            # data as well. If that hash matches the hash in the metadata, then we know
            # the file is up to date even though the mtime is wrong, without needing to hash it.
            qmtime, qsize, qhash = manager.quickstart_state[path]
            if int(qmtime) == mtime and qsize == size and qhash == meta.hash:
                manager.log(f"Metadata fresh (by quickstart) for {id}: file {path}")
                meta = meta._replace(mtime=mtime, path=path)
                return meta

        t0 = time.time()
        try:
            # dir means it is a namespace package
            if stat.S_ISDIR(st.st_mode):
                source_hash = ""
            else:
                source_hash = manager.fscache.hash_digest(path)
        except (OSError, UnicodeDecodeError, DecodeError):
            return None
        manager.add_stats(validate_hash_time=time.time() - t0)
        if source_hash != meta.hash:
            if fine_grained_cache:
                manager.log(f"Using stale metadata for {id}: file {path}")
                return meta
            else:
                manager.log(f"Metadata abandoned for {id}: file {path} has different hash")
                return None
        else:
            t0 = time.time()
            # Optimization: update mtime and path (otherwise, this mismatch will reappear).
            meta = meta._replace(mtime=mtime, path=path)
            # Construct a dict we can pass to json.dumps() (compare to write_cache()).
            meta_dict = {
                "id": id,
                "path": path,
                "mtime": mtime,
                "size": size,
                "hash": source_hash,
                "data_mtime": meta.data_mtime,
                "dependencies": meta.dependencies,
                "suppressed": meta.suppressed,
                "options": (manager.options.clone_for_module(id).select_options_affecting_cache()),
                "dep_prios": meta.dep_prios,
                "dep_lines": meta.dep_lines,
                "interface_hash": meta.interface_hash,
                "version_id": manager.version_id,
                "ignore_all": meta.ignore_all,
                "plugin_data": meta.plugin_data,
            }
            if manager.options.debug_cache:
                meta_str = json.dumps(meta_dict, indent=2, sort_keys=True)
            else:
                meta_str = json.dumps(meta_dict)
            meta_json, _, _ = get_cache_names(id, path, manager.options)
            manager.log(
                "Updating mtime for {}: file {}, meta {}, mtime {}".format(
                    id, path, meta_json, meta.mtime
                )
            )
            t1 = time.time()
            manager.metastore.write(meta_json, meta_str)  # Ignore errors, just an optimization.
            manager.add_stats(validate_update_time=time.time() - t1, validate_munging_time=t1 - t0)
            return meta

    # It's a match on (id, path, size, hash, mtime).
    manager.log(f"Metadata fresh for {id}: file {path}")
    return meta


</t>
<t tx="ekr.20221004064034.25"></t>
<t tx="ekr.20221004064034.250">def compute_hash(text: str) -&gt; str:
    # We use a crypto hash instead of the builtin hash(...) function
    # because the output of hash(...)  can differ between runs due to
    # hash randomization (enabled by default in Python 3.3).  See the
    # note in
    # https://docs.python.org/3/reference/datamodel.html#object.__hash__.
    return hash_digest(text.encode("utf-8"))


</t>
<t tx="ekr.20221004064034.251">def json_dumps(obj: Any, debug_cache: bool) -&gt; str:
    if debug_cache:
        return json.dumps(obj, indent=2, sort_keys=True)
    else:
        return json.dumps(obj, sort_keys=True)


</t>
<t tx="ekr.20221004064034.252">def write_cache(
    id: str,
    path: str,
    tree: MypyFile,
    dependencies: list[str],
    suppressed: list[str],
    dep_prios: list[int],
    dep_lines: list[int],
    old_interface_hash: str,
    source_hash: str,
    ignore_all: bool,
    manager: BuildManager,
) -&gt; tuple[str, CacheMeta | None]:
    """Write cache files for a module.

    Note that this mypy's behavior is still correct when any given
    write_cache() call is replaced with a no-op, so error handling
    code that bails without writing anything is okay.

    Args:
      id: module ID
      path: module path
      tree: the fully checked module data
      dependencies: module IDs on which this module depends
      suppressed: module IDs which were suppressed as dependencies
      dep_prios: priorities (parallel array to dependencies)
      dep_lines: import line locations (parallel array to dependencies)
      old_interface_hash: the hash from the previous version of the data cache file
      source_hash: the hash of the source code
      ignore_all: the ignore_all flag for this module
      manager: the build manager (for pyversion, log/trace)

    Returns:
      A tuple containing the interface hash and CacheMeta
      corresponding to the metadata that was written (the latter may
      be None if the cache could not be written).
    """
    metastore = manager.metastore
    # For Bazel we use relative paths and zero mtimes.
    bazel = manager.options.bazel

    # Obtain file paths.
    meta_json, data_json, _ = get_cache_names(id, path, manager.options)
    manager.log(f"Writing {id} {path} {meta_json} {data_json}")

    # Update tree.path so that in bazel mode it's made relative (since
    # sometimes paths leak out).
    if bazel:
        tree.path = path

    # Serialize data and analyze interface
    data = tree.serialize()
    data_str = json_dumps(data, manager.options.debug_cache)
    interface_hash = compute_hash(data_str)

    plugin_data = manager.plugin.report_config_data(ReportConfigContext(id, path, is_check=False))

    # Obtain and set up metadata
    try:
        st = manager.get_stat(path)
    except OSError as err:
        manager.log(f"Cannot get stat for {path}: {err}")
        # Remove apparently-invalid cache files.
        # (This is purely an optimization.)
        for filename in [data_json, meta_json]:
            try:
                os.remove(filename)
            except OSError:
                pass
        # Still return the interface hash we computed.
        return interface_hash, None

    # Write data cache file, if applicable
    # Note that for Bazel we don't record the data file's mtime.
    if old_interface_hash == interface_hash:
        manager.trace(f"Interface for {id} is unchanged")
    else:
        manager.trace(f"Interface for {id} has changed")
        if not metastore.write(data_json, data_str):
            # Most likely the error is the replace() call
            # (see https://github.com/python/mypy/issues/3215).
            manager.log(f"Error writing data JSON file {data_json}")
            # Let's continue without writing the meta file.  Analysis:
            # If the replace failed, we've changed nothing except left
            # behind an extraneous temporary file; if the replace
            # worked but the getmtime() call failed, the meta file
            # will be considered invalid on the next run because the
            # data_mtime field won't match the data file's mtime.
            # Both have the effect of slowing down the next run a
            # little bit due to an out-of-date cache file.
            return interface_hash, None

    try:
        data_mtime = manager.getmtime(data_json)
    except OSError:
        manager.log(f"Error in os.stat({data_json!r}), skipping cache write")
        return interface_hash, None

    mtime = 0 if bazel else int(st.st_mtime)
    size = st.st_size
    # Note that the options we store in the cache are the options as
    # specified by the command line/config file and *don't* reflect
    # updates made by inline config directives in the file. This is
    # important, or otherwise the options would never match when
    # verifying the cache.
    options = manager.options.clone_for_module(id)
    assert source_hash is not None
    meta = {
        "id": id,
        "path": path,
        "mtime": mtime,
        "size": size,
        "hash": source_hash,
        "data_mtime": data_mtime,
        "dependencies": dependencies,
        "suppressed": suppressed,
        "options": options.select_options_affecting_cache(),
        "dep_prios": dep_prios,
        "dep_lines": dep_lines,
        "interface_hash": interface_hash,
        "version_id": manager.version_id,
        "ignore_all": ignore_all,
        "plugin_data": plugin_data,
    }

    # Write meta cache file
    meta_str = json_dumps(meta, manager.options.debug_cache)
    if not metastore.write(meta_json, meta_str):
        # Most likely the error is the replace() call
        # (see https://github.com/python/mypy/issues/3215).
        # The next run will simply find the cache entry out of date.
        manager.log(f"Error writing meta JSON file {meta_json}")

    return interface_hash, cache_meta_from_dict(meta, data_json)


</t>
<t tx="ekr.20221004064034.253">def delete_cache(id: str, path: str, manager: BuildManager) -&gt; None:
    """Delete cache files for a module.

    The cache files for a module are deleted when mypy finds errors there.
    This avoids inconsistent states with cache files from different mypy runs,
    see #4043 for an example.
    """
    # We don't delete .deps files on errors, since the dependencies
    # are mostly generated from other files and the metadata is
    # tracked separately.
    meta_path, data_path, _ = get_cache_names(id, path, manager.options)
    cache_paths = [meta_path, data_path]
    manager.log(f"Deleting {id} {path} {' '.join(x for x in cache_paths if x)}")

    for filename in cache_paths:
        try:
            manager.metastore.remove(filename)
        except OSError as e:
            if e.errno != errno.ENOENT:
                manager.log(f"Error deleting cache file {filename}: {e.strerror}")


</t>
<t tx="ekr.20221004064034.254">"""Dependency manager.

Design
======

Ideally
-------

A. Collapse cycles (each SCC -- strongly connected component --
   becomes one "supernode").

B. Topologically sort nodes based on dependencies.

C. Process from leaves towards roots.

Wrinkles
--------

a. Need to parse source modules to determine dependencies.

b. Processing order for modules within an SCC.

c. Must order mtimes of files to decide whether to re-process; depends
   on clock never resetting.

d. from P import M; checks filesystem whether module P.M exists in
   filesystem.

e. Race conditions, where somebody modifies a file while we're
   processing. Solved by using a FileSystemCache.


Steps
-----

1. For each explicitly given module find the source file location.

2. For each such module load and check the cache metadata, and decide
   whether it's valid.

3. Now recursively (or iteratively) find dependencies and add those to
   the graph:

   - for cached nodes use the list of dependencies from the cache
     metadata (this will be valid even if we later end up re-parsing
     the same source);

   - for uncached nodes parse the file and process all imports found,
     taking care of (a) above.

Step 3 should also address (d) above.

Once step 3 terminates we have the entire dependency graph, and for
each module we've either loaded the cache metadata or parsed the
source code.  (However, we may still need to parse those modules for
which we have cache metadata but that depend, directly or indirectly,
on at least one module for which the cache metadata is stale.)

Now we can execute steps A-C from the first section.  Finding SCCs for
step A shouldn't be hard; there's a recipe here:
http://code.activestate.com/recipes/578507/.  There's also a plethora
of topsort recipes, e.g. http://code.activestate.com/recipes/577413/.

For single nodes, processing is simple.  If the node was cached, we
deserialize the cache data and fix up cross-references.  Otherwise, we
do semantic analysis followed by type checking.  We also handle (c)
above; if a module has valid cache data *but* any of its
dependencies was processed from source, then the module should be
processed from source.

A relatively simple optimization (outside SCCs) we might do in the
future is as follows: if a node's cache data is valid, but one or more
of its dependencies are out of date so we have to re-parse the node
from source, once we have fully type-checked the node, we can decide
whether its symbol table actually changed compared to the cache data
(by reading the cache data and comparing it to the data we would be
writing).  If there is no change we can declare the node up to date,
and any node that depends (and for which we have cached data, and
whose other dependencies are up to date) on it won't need to be
re-parsed from source.

Import cycles
-------------

Finally we have to decide how to handle (c), import cycles.  Here
we'll need a modified version of the original state machine
(build.py), but we only need to do this per SCC, and we won't have to
deal with changes to the list of nodes while we're processing it.

If all nodes in the SCC have valid cache metadata and all dependencies
outside the SCC are still valid, we can proceed as follows:

  1. Load cache data for all nodes in the SCC.

  2. Fix up cross-references for all nodes in the SCC.

Otherwise, the simplest (but potentially slow) way to proceed is to
invalidate all cache data in the SCC and re-parse all nodes in the SCC
from source.  We can do this as follows:

  1. Parse source for all nodes in the SCC.

  2. Semantic analysis for all nodes in the SCC.

  3. Type check all nodes in the SCC.

(If there are more passes the process is the same -- each pass should
be done for all nodes before starting the next pass for any nodes in
the SCC.)

We could process the nodes in the SCC in any order.  For sentimental
reasons, I've decided to process them in the reverse order in which we
encountered them when originally constructing the graph.  That's how
the old build.py deals with cycles, and at least this reproduces the
previous implementation more accurately.

Can we do better than re-parsing all nodes in the SCC when any of its
dependencies are out of date?  It's doubtful.  The optimization
mentioned at the end of the previous section would require re-parsing
and type-checking a node and then comparing its symbol table to the
cached data; but because the node is part of a cycle we can't
technically type-check it until the semantic analysis of all other
nodes in the cycle has completed.  (This is an important issue because
Dropbox has a very large cycle in production code.  But I'd like to
deal with it later.)

Additional wrinkles
-------------------

During implementation more wrinkles were found.

- When a submodule of a package (e.g. x.y) is encountered, the parent
  package (e.g. x) must also be loaded, but it is not strictly a
  dependency.  See State.add_ancestors() below.
"""


</t>
<t tx="ekr.20221004064034.255">class ModuleNotFound(Exception):
    """Control flow exception to signal that a module was not found."""


</t>
<t tx="ekr.20221004064034.256">class State:
    """The state for a module.

    The source is only used for the -c command line option; in that
    case path is None.  Otherwise source is None and path isn't.
    """

    manager: BuildManager
    order_counter: ClassVar[int] = 0
    order: int  # Order in which modules were encountered
    id: str  # Fully qualified module name
    path: str | None = None  # Path to module source
    abspath: str | None = None  # Absolute path to module source
    xpath: str  # Path or '&lt;string&gt;'
    source: str | None = None  # Module source code
    source_hash: str | None = None  # Hash calculated based on the source code
    meta_source_hash: str | None = None  # Hash of the source given in the meta, if any
    meta: CacheMeta | None = None
    data: str | None = None
    tree: MypyFile | None = None
    # We keep both a list and set of dependencies. A set because it makes it efficient to
    # prevent duplicates and the list because I am afraid of changing the order of
    # iteration over dependencies.
    # They should be managed with add_dependency and suppress_dependency.
    dependencies: list[str]  # Modules directly imported by the module
    dependencies_set: set[str]  # The same but as a set for deduplication purposes
    suppressed: list[str]  # Suppressed/missing dependencies
    suppressed_set: set[str]  # Suppressed/missing dependencies
    priorities: dict[str, int]

    # Map each dependency to the line number where it is first imported
    dep_line_map: dict[str, int]

    # Parent package, its parent, etc.
    ancestors: list[str] | None = None

    # List of (path, line number) tuples giving context for import
    import_context: list[tuple[str, int]]

    # The State from which this module was imported, if any
    caller_state: State | None = None

    # If caller_state is set, the line number in the caller where the import occurred
    caller_line = 0

    # If True, indicate that the public interface of this module is unchanged
    externally_same = True

    # Contains a hash of the public interface in incremental mode
    interface_hash: str = ""

    # Options, specialized for this file
    options: Options

    # Whether to ignore all errors
    ignore_all = False

    # Whether the module has an error or any of its dependencies have one.
    transitive_error = False

    # Errors reported before semantic analysis, to allow fine-grained
    # mode to keep reporting them.
    early_errors: list[ErrorInfo]

    # Type checker used for checking this file.  Use type_checker() for
    # access and to construct this on demand.
    _type_checker: TypeChecker | None = None

    fine_grained_deps_loaded = False

    # Cumulative time spent on this file, in microseconds (for profiling stats)
    time_spent_us: int = 0

    @others
</t>
<t tx="ekr.20221004064034.257">def __init__(
    self,
    id: str | None,
    path: str | None,
    source: str | None,
    manager: BuildManager,
    caller_state: State | None = None,
    caller_line: int = 0,
    ancestor_for: State | None = None,
    root_source: bool = False,
    # If `temporary` is True, this State is being created to just
    # quickly parse/load the tree, without an intention to further
    # process it. With this flag, any changes to external state as well
    # as error reporting should be avoided.
    temporary: bool = False,
) -&gt; None:
    if not temporary:
        assert id or path or source is not None, "Neither id, path nor source given"
    self.manager = manager
    State.order_counter += 1
    self.order = State.order_counter
    self.caller_state = caller_state
    self.caller_line = caller_line
    if caller_state:
        self.import_context = caller_state.import_context[:]
        self.import_context.append((caller_state.xpath, caller_line))
    else:
        self.import_context = []
    self.id = id or "__main__"
    self.options = manager.options.clone_for_module(self.id)
    self.early_errors = []
    self._type_checker = None
    if not path and source is None:
        assert id is not None
        try:
            path, follow_imports = find_module_and_diagnose(
                manager,
                id,
                self.options,
                caller_state,
                caller_line,
                ancestor_for,
                root_source,
                skip_diagnose=temporary,
            )
        except ModuleNotFound:
            if not temporary:
                manager.missing_modules.add(id)
            raise
        if follow_imports == "silent":
            self.ignore_all = True
    elif path and is_silent_import_module(manager, path):
        self.ignore_all = True
    self.path = path
    if path:
        self.abspath = os.path.abspath(path)
    self.xpath = path or "&lt;string&gt;"
    if path and source is None and self.manager.cache_enabled:
        self.meta = find_cache_meta(self.id, path, manager)
        # TODO: Get mtime if not cached.
        if self.meta is not None:
            self.interface_hash = self.meta.interface_hash
            self.meta_source_hash = self.meta.hash
    if path and source is None and self.manager.fscache.isdir(path):
        source = ""
    self.source = source
    self.add_ancestors()
    t0 = time.time()
    self.meta = validate_meta(self.meta, self.id, self.path, self.ignore_all, manager)
    self.manager.add_stats(validate_meta_time=time.time() - t0)
    if self.meta:
        # Make copies, since we may modify these and want to
        # compare them to the originals later.
        self.dependencies = list(self.meta.dependencies)
        self.dependencies_set = set(self.dependencies)
        self.suppressed = list(self.meta.suppressed)
        self.suppressed_set = set(self.suppressed)
        all_deps = self.dependencies + self.suppressed
        assert len(all_deps) == len(self.meta.dep_prios)
        self.priorities = {id: pri for id, pri in zip(all_deps, self.meta.dep_prios)}
        assert len(all_deps) == len(self.meta.dep_lines)
        self.dep_line_map = {id: line for id, line in zip(all_deps, self.meta.dep_lines)}
        if temporary:
            self.load_tree(temporary=True)
        if not manager.use_fine_grained_cache():
            # Special case: if there were a previously missing package imported here
            # and it is not present, then we need to re-calculate dependencies.
            # This is to support patterns like this:
            #     from missing_package import missing_module  # type: ignore
            # At first mypy doesn't know that `missing_module` is a module
            # (it may be a variable, a class, or a function), so it is not added to
            # suppressed dependencies. Therefore, when the package with module is added,
            # we need to re-calculate dependencies.
            # NOTE: see comment below for why we skip this in fine grained mode.
            if exist_added_packages(self.suppressed, manager, self.options):
                self.parse_file()  # This is safe because the cache is anyway stale.
                self.compute_dependencies()
    else:
        # When doing a fine-grained cache load, pretend we only
        # know about modules that have cache information and defer
        # handling new modules until the fine-grained update.
        if manager.use_fine_grained_cache():
            manager.log(f"Deferring module to fine-grained update {path} ({id})")
            raise ModuleNotFound

        # Parse the file (and then some) to get the dependencies.
        self.parse_file()
        self.compute_dependencies()

</t>
<t tx="ekr.20221004064034.258">@property
def xmeta(self) -&gt; CacheMeta:
    assert self.meta, "missing meta on allegedly fresh module"
    return self.meta

</t>
<t tx="ekr.20221004064034.259">def add_ancestors(self) -&gt; None:
    if self.path is not None:
        _, name = os.path.split(self.path)
        base, _ = os.path.splitext(name)
        if "." in base:
            # This is just a weird filename, don't add anything
            self.ancestors = []
            return
    # All parent packages are new ancestors.
    ancestors = []
    parent = self.id
    while "." in parent:
        parent, _ = parent.rsplit(".", 1)
        ancestors.append(parent)
    self.ancestors = ancestors

</t>
<t tx="ekr.20221004064034.26">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python

from __future__ import annotations

import json
import os
import os.path
from collections import Counter
from typing import Any, Dict, Iterable
from typing_extensions import Final, TypeAlias as _TypeAlias

ROOT: Final = ".mypy_cache/3.5"

JsonDict: _TypeAlias = Dict[str, Any]


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.260">def is_fresh(self) -&gt; bool:
    """Return whether the cache data for this file is fresh."""
    # NOTE: self.dependencies may differ from
    # self.meta.dependencies when a dependency is dropped due to
    # suppression by silent mode.  However when a suppressed
    # dependency is added back we find out later in the process.
    return (
        self.meta is not None
        and self.is_interface_fresh()
        and self.dependencies == self.meta.dependencies
    )

</t>
<t tx="ekr.20221004064034.261">def is_interface_fresh(self) -&gt; bool:
    return self.externally_same

</t>
<t tx="ekr.20221004064034.262">def mark_as_rechecked(self) -&gt; None:
    """Marks this module as having been fully re-analyzed by the type-checker."""
    self.manager.rechecked_modules.add(self.id)

</t>
<t tx="ekr.20221004064034.263">def mark_interface_stale(self, *, on_errors: bool = False) -&gt; None:
    """Marks this module as having a stale public interface, and discards the cache data."""
    self.externally_same = False
    if not on_errors:
        self.manager.stale_modules.add(self.id)

</t>
<t tx="ekr.20221004064034.264">def check_blockers(self) -&gt; None:
    """Raise CompileError if a blocking error is detected."""
    if self.manager.errors.is_blockers():
        self.manager.log("Bailing due to blocking errors")
        self.manager.errors.raise_error()

</t>
<t tx="ekr.20221004064034.265">@contextlib.contextmanager
def wrap_context(self, check_blockers: bool = True) -&gt; Iterator[None]:
    """Temporarily change the error import context to match this state.

    Also report an internal error if an unexpected exception was raised
    and raise an exception on a blocking error, unless
    check_blockers is False. Skipping blocking error reporting is used
    in the semantic analyzer so that we can report all blocking errors
    for a file (across multiple targets) to maintain backward
    compatibility.
    """
    save_import_context = self.manager.errors.import_context()
    self.manager.errors.set_import_context(self.import_context)
    try:
        yield
    except CompileError:
        raise
    except Exception as err:
        report_internal_error(
            err,
            self.path,
            0,
            self.manager.errors,
            self.options,
            self.manager.stdout,
            self.manager.stderr,
        )
    self.manager.errors.set_import_context(save_import_context)
    # TODO: Move this away once we've removed the old semantic analyzer?
    if check_blockers:
        self.check_blockers()

</t>
<t tx="ekr.20221004064034.266">def load_fine_grained_deps(self) -&gt; dict[str, set[str]]:
    return self.manager.load_fine_grained_deps(self.id)

</t>
<t tx="ekr.20221004064034.267">def load_tree(self, temporary: bool = False) -&gt; None:
    assert (
        self.meta is not None
    ), "Internal error: this method must be called only for cached modules"

    data = _load_json_file(
        self.meta.data_json, self.manager, "Load tree ", "Could not load tree: "
    )
    if data is None:
        return None

    t0 = time.time()
    # TODO: Assert data file wasn't changed.
    self.tree = MypyFile.deserialize(data)
    t1 = time.time()
    self.manager.add_stats(deserialize_time=t1 - t0)
    if not temporary:
        self.manager.modules[self.id] = self.tree
        self.manager.add_stats(fresh_trees=1)

</t>
<t tx="ekr.20221004064034.268">def fix_cross_refs(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We need to set allow_missing when doing a fine grained cache
    # load because we need to gracefully handle missing modules.
    fixup_module(self.tree, self.manager.modules, self.options.use_fine_grained_cache)

</t>
<t tx="ekr.20221004064034.269"># Methods for processing modules from source code.

</t>
<t tx="ekr.20221004064034.27">class CacheData:
    @others
</t>
<t tx="ekr.20221004064034.270">def parse_file(self) -&gt; None:
    """Parse file and run first pass of semantic analysis.

    Everything done here is local to the file. Don't depend on imported
    modules in any way. Also record module dependencies based on imports.
    """
    if self.tree is not None:
        # The file was already parsed (in __init__()).
        return

    manager = self.manager

    # Can we reuse a previously parsed AST? This avoids redundant work in daemon.
    cached = self.id in manager.ast_cache
    modules = manager.modules
    if not cached:
        manager.log(f"Parsing {self.xpath} ({self.id})")
    else:
        manager.log(f"Using cached AST for {self.xpath} ({self.id})")

    t0 = time_ref()

    with self.wrap_context():
        source = self.source
        self.source = None  # We won't need it again.
        if self.path and source is None:
            try:
                path = manager.maybe_swap_for_shadow_path(self.path)
                source = decode_python_encoding(manager.fscache.read(path))
                self.source_hash = manager.fscache.hash_digest(path)
            except OSError as ioerr:
                # ioerr.strerror differs for os.stat failures between Windows and
                # other systems, but os.strerror(ioerr.errno) does not, so we use that.
                # (We want the error messages to be platform-independent so that the
                # tests have predictable output.)
                raise CompileError(
                    [
                        "mypy: can't read file '{}': {}".format(
                            self.path, os.strerror(ioerr.errno)
                        )
                    ],
                    module_with_blocker=self.id,
                ) from ioerr
            except (UnicodeDecodeError, DecodeError) as decodeerr:
                if self.path.endswith(".pyd"):
                    err = f"mypy: stubgen does not support .pyd files: '{self.path}'"
                else:
                    err = f"mypy: can't decode file '{self.path}': {str(decodeerr)}"
                raise CompileError([err], module_with_blocker=self.id) from decodeerr
        elif self.path and self.manager.fscache.isdir(self.path):
            source = ""
            self.source_hash = ""
        else:
            assert source is not None
            self.source_hash = compute_hash(source)

        self.parse_inline_configuration(source)
        if not cached:
            self.tree = manager.parse_file(
                self.id,
                self.xpath,
                source,
                self.ignore_all or self.options.ignore_errors,
                self.options,
            )

        else:
            # Reuse a cached AST
            self.tree = manager.ast_cache[self.id][0]
            manager.errors.set_file_ignored_lines(
                self.xpath,
                self.tree.ignored_lines,
                self.ignore_all or self.options.ignore_errors,
            )

    self.time_spent_us += time_spent_us(t0)

    if not cached:
        # Make a copy of any errors produced during parse time so that
        # fine-grained mode can repeat them when the module is
        # reprocessed.
        self.early_errors = list(manager.errors.error_info_map.get(self.xpath, []))
    else:
        self.early_errors = manager.ast_cache[self.id][1]

    modules[self.id] = self.tree

    if not cached:
        self.semantic_analysis_pass1()

    self.check_blockers()

    manager.ast_cache[self.id] = (self.tree, self.early_errors)

</t>
<t tx="ekr.20221004064034.271">def parse_inline_configuration(self, source: str) -&gt; None:
    """Check for inline mypy: options directive and parse them."""
    flags = get_mypy_comments(source)
    if flags:
        changes, config_errors = parse_mypy_comments(flags, self.options)
        self.options = self.options.apply_changes(changes)
        self.manager.errors.set_file(self.xpath, self.id, self.options)
        for lineno, error in config_errors:
            self.manager.errors.report(lineno, 0, error)

</t>
<t tx="ekr.20221004064034.272">def semantic_analysis_pass1(self) -&gt; None:
    """Perform pass 1 of semantic analysis, which happens immediately after parsing.

    This pass can't assume that any other modules have been processed yet.
    """
    options = self.options
    assert self.tree is not None

    t0 = time_ref()

    # Do the first pass of semantic analysis: analyze the reachability
    # of blocks and import statements. We must do this before
    # processing imports, since this may mark some import statements as
    # unreachable.
    #
    # TODO: This should not be considered as a semantic analysis
    #     pass -- it's an independent pass.
    analyzer = SemanticAnalyzerPreAnalysis()
    with self.wrap_context():
        analyzer.visit_file(self.tree, self.xpath, self.id, options)
    # TODO: Do this while constructing the AST?
    self.tree.names = SymbolTable()
    if not self.tree.is_stub:
        # Always perform some low-key variable renaming
        self.tree.accept(LimitedVariableRenameVisitor())
        if options.allow_redefinition:
            # Perform more renaming across the AST to allow variable redefinitions
            self.tree.accept(VariableRenameVisitor())
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20221004064034.273">def add_dependency(self, dep: str) -&gt; None:
    if dep not in self.dependencies_set:
        self.dependencies.append(dep)
        self.dependencies_set.add(dep)
    if dep in self.suppressed_set:
        self.suppressed.remove(dep)
        self.suppressed_set.remove(dep)

</t>
<t tx="ekr.20221004064034.274">def suppress_dependency(self, dep: str) -&gt; None:
    if dep in self.dependencies_set:
        self.dependencies.remove(dep)
        self.dependencies_set.remove(dep)
    if dep not in self.suppressed_set:
        self.suppressed.append(dep)
        self.suppressed_set.add(dep)

</t>
<t tx="ekr.20221004064034.275">def compute_dependencies(self) -&gt; None:
    """Compute a module's dependencies after parsing it.

    This is used when we parse a file that we didn't have
    up-to-date cache information for. When we have an up-to-date
    cache, we just use the cached info.
    """
    manager = self.manager
    assert self.tree is not None

    # Compute (direct) dependencies.
    # Add all direct imports (this is why we needed the first pass).
    # Also keep track of each dependency's source line.
    # Missing dependencies will be moved from dependencies to
    # suppressed when they fail to be loaded in load_graph.

    self.dependencies = []
    self.dependencies_set = set()
    self.suppressed = []
    self.suppressed_set = set()
    self.priorities = {}  # id -&gt; priority
    self.dep_line_map = {}  # id -&gt; line
    dep_entries = manager.all_imported_modules_in_file(
        self.tree
    ) + self.manager.plugin.get_additional_deps(self.tree)
    for pri, id, line in dep_entries:
        self.priorities[id] = min(pri, self.priorities.get(id, PRI_ALL))
        if id == self.id:
            continue
        self.add_dependency(id)
        if id not in self.dep_line_map:
            self.dep_line_map[id] = line
    # Every module implicitly depends on builtins.
    if self.id != "builtins":
        self.add_dependency("builtins")

    self.check_blockers()  # Can fail due to bogus relative imports

</t>
<t tx="ekr.20221004064034.276">def type_check_first_pass(self) -&gt; None:
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        self.type_checker().check_first_pass()
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20221004064034.277">def type_checker(self) -&gt; TypeChecker:
    if not self._type_checker:
        assert self.tree is not None, "Internal error: must be called on parsed file only"
        manager = self.manager
        self._type_checker = TypeChecker(
            manager.errors,
            manager.modules,
            self.options,
            self.tree,
            self.xpath,
            manager.plugin,
        )
    return self._type_checker

</t>
<t tx="ekr.20221004064034.278">def type_map(self) -&gt; dict[Expression, Type]:
    # We can extract the master type map directly since at this
    # point no temporary type maps can be active.
    assert len(self.type_checker()._type_maps) == 1
    return self.type_checker()._type_maps[0]

</t>
<t tx="ekr.20221004064034.279">def type_check_second_pass(self) -&gt; bool:
    if self.options.semantic_analysis_only:
        return False
    t0 = time_ref()
    with self.wrap_context():
        result = self.type_checker().check_second_pass()
    self.time_spent_us += time_spent_us(t0)
    return result

</t>
<t tx="ekr.20221004064034.28">def __init__(
    self,
    filename: str,
    data_json: JsonDict,
    meta_json: JsonDict,
    data_size: int,
    meta_size: int,
) -&gt; None:
    self.filename = filename
    self.data = data_json
    self.meta = meta_json
    self.data_size = data_size
    self.meta_size = meta_size

</t>
<t tx="ekr.20221004064034.280">def detect_partially_defined_vars(self, type_map: dict[Expression, Type]) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    manager = self.manager
    if manager.errors.is_error_code_enabled(codes.PARTIALLY_DEFINED):
        manager.errors.set_file(self.xpath, self.tree.fullname, options=manager.options)
        self.tree.accept(
            PartiallyDefinedVariableVisitor(
                MessageBuilder(manager.errors, manager.modules), type_map
            )
        )

</t>
<t tx="ekr.20221004064034.281">def finish_passes(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    manager = self.manager
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        # Some tests (and tools) want to look at the set of all types.
        options = manager.options
        if options.export_types:
            manager.all_types.update(self.type_map())

        # We should always patch indirect dependencies, even in full (non-incremental) builds,
        # because the cache still may be written, and it must be correct.
        self._patch_indirect_dependencies(self.type_checker().module_refs, self.type_map())

        if self.options.dump_inference_stats:
            dump_type_stats(
                self.tree,
                self.xpath,
                modules=self.manager.modules,
                inferred=True,
                typemap=self.type_map(),
            )
        manager.report_file(self.tree, self.type_map(), self.options)

        self.update_fine_grained_deps(self.manager.fg_deps)
        self.free_state()
        if not manager.options.fine_grained_incremental and not manager.options.preserve_asts:
            free_tree(self.tree)
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20221004064034.282">def free_state(self) -&gt; None:
    if self._type_checker:
        self._type_checker.reset()
        self._type_checker = None

</t>
<t tx="ekr.20221004064034.283">def _patch_indirect_dependencies(
    self, module_refs: set[str], type_map: dict[Expression, Type]
) -&gt; None:
    types = set(type_map.values())
    assert None not in types
    valid = self.valid_references()

    encountered = self.manager.indirection_detector.find_modules(types) | module_refs
    extra = encountered - valid

    for dep in sorted(extra):
        if dep not in self.manager.modules:
            continue
        if dep not in self.suppressed_set and dep not in self.manager.missing_modules:
            self.add_dependency(dep)
            self.priorities[dep] = PRI_INDIRECT
        elif dep not in self.suppressed_set and dep in self.manager.missing_modules:
            self.suppress_dependency(dep)

</t>
<t tx="ekr.20221004064034.284">def compute_fine_grained_deps(self) -&gt; dict[str, set[str]]:
    assert self.tree is not None
    if self.id in ("builtins", "typing", "types", "sys", "_typeshed"):
        # We don't track changes to core parts of typeshed -- the
        # assumption is that they are only changed as part of mypy
        # updates, which will invalidate everything anyway. These
        # will always be processed in the initial non-fine-grained
        # build. Other modules may be brought in as a result of an
        # fine-grained increment, and we may need these
        # dependencies then to handle cyclic imports.
        return {}
    from mypy.server.deps import get_dependencies  # Lazy import to speed up startup

    return get_dependencies(
        target=self.tree,
        type_map=self.type_map(),
        python_version=self.options.python_version,
        options=self.manager.options,
    )

</t>
<t tx="ekr.20221004064034.285">def update_fine_grained_deps(self, deps: dict[str, set[str]]) -&gt; None:
    options = self.manager.options
    if options.cache_fine_grained or options.fine_grained_incremental:
        from mypy.server.deps import merge_dependencies  # Lazy import to speed up startup

        merge_dependencies(self.compute_fine_grained_deps(), deps)
        TypeState.update_protocol_deps(deps)

</t>
<t tx="ekr.20221004064034.286">def valid_references(self) -&gt; set[str]:
    assert self.ancestors is not None
    valid_refs = set(self.dependencies + self.suppressed + self.ancestors)
    valid_refs.add(self.id)

    if "os" in valid_refs:
        valid_refs.add("os.path")

    return valid_refs

</t>
<t tx="ekr.20221004064034.287">def write_cache(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We don't support writing cache files in fine-grained incremental mode.
    if (
        not self.path
        or self.options.cache_dir == os.devnull
        or self.options.fine_grained_incremental
    ):
        return
    is_errors = self.transitive_error
    if is_errors:
        delete_cache(self.id, self.path, self.manager)
        self.meta = None
        self.mark_interface_stale(on_errors=True)
        return
    dep_prios = self.dependency_priorities()
    dep_lines = self.dependency_lines()
    assert self.source_hash is not None
    assert len(set(self.dependencies)) == len(
        self.dependencies
    ), f"Duplicates in dependencies list for {self.id} ({self.dependencies})"
    new_interface_hash, self.meta = write_cache(
        self.id,
        self.path,
        self.tree,
        list(self.dependencies),
        list(self.suppressed),
        dep_prios,
        dep_lines,
        self.interface_hash,
        self.source_hash,
        self.ignore_all,
        self.manager,
    )
    if new_interface_hash == self.interface_hash:
        self.manager.log(f"Cached module {self.id} has same interface")
    else:
        self.manager.log(f"Cached module {self.id} has changed interface")
        self.mark_interface_stale()
        self.interface_hash = new_interface_hash

</t>
<t tx="ekr.20221004064034.288">def verify_dependencies(self, suppressed_only: bool = False) -&gt; None:
    """Report errors for import targets in modules that don't exist.

    If suppressed_only is set, only check suppressed dependencies.
    """
    manager = self.manager
    assert self.ancestors is not None
    if suppressed_only:
        all_deps = self.suppressed
    else:
        # Strip out indirect dependencies. See comment in build.load_graph().
        dependencies = [
            dep for dep in self.dependencies if self.priorities.get(dep) != PRI_INDIRECT
        ]
        all_deps = dependencies + self.suppressed + self.ancestors
    for dep in all_deps:
        if dep in manager.modules:
            continue
        options = manager.options.clone_for_module(dep)
        if options.ignore_missing_imports:
            continue
        line = self.dep_line_map.get(dep, 1)
        try:
            if dep in self.ancestors:
                state: State | None = None
                ancestor: State | None = self
            else:
                state, ancestor = self, None
            # Called just for its side effects of producing diagnostics.
            find_module_and_diagnose(
                manager,
                dep,
                options,
                caller_state=state,
                caller_line=line,
                ancestor_for=ancestor,
            )
        except (ModuleNotFound, CompileError):
            # Swallow up any ModuleNotFounds or CompilerErrors while generating
            # a diagnostic. CompileErrors may get generated in
            # fine-grained mode when an __init__.py is deleted, if a module
            # that was in that package has targets reprocessed before
            # it is renamed.
            pass

</t>
<t tx="ekr.20221004064034.289">def dependency_priorities(self) -&gt; list[int]:
    return [self.priorities.get(dep, PRI_HIGH) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20221004064034.29">@property
def total_size(self) -&gt; int:
    return self.data_size + self.meta_size


</t>
<t tx="ekr.20221004064034.290">def dependency_lines(self) -&gt; list[int]:
    return [self.dep_line_map.get(dep, 1) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20221004064034.291">def generate_unused_ignore_notes(self) -&gt; None:
    if self.options.warn_unused_ignores:
        # If this file was initially loaded from the cache, it may have suppressed
        # dependencies due to imports with ignores on them. We need to generate
        # those errors to avoid spuriously flagging them as unused ignores.
        if self.meta:
            self.verify_dependencies(suppressed_only=True)
        self.manager.errors.generate_unused_ignore_errors(self.xpath)

</t>
<t tx="ekr.20221004064034.292">def generate_ignore_without_code_notes(self) -&gt; None:
    if self.manager.errors.is_error_code_enabled(codes.IGNORE_WITHOUT_CODE):
        self.manager.errors.generate_ignore_without_code_errors(
            self.xpath, self.options.warn_unused_ignores
        )


</t>
<t tx="ekr.20221004064034.293"># Module import and diagnostic glue


</t>
<t tx="ekr.20221004064034.294">def find_module_and_diagnose(
    manager: BuildManager,
    id: str,
    options: Options,
    caller_state: State | None = None,
    caller_line: int = 0,
    ancestor_for: State | None = None,
    root_source: bool = False,
    skip_diagnose: bool = False,
) -&gt; tuple[str, str]:
    """Find a module by name, respecting follow_imports and producing diagnostics.

    If the module is not found, then the ModuleNotFound exception is raised.

    Args:
      id: module to find
      options: the options for the module being loaded
      caller_state: the state of the importing module, if applicable
      caller_line: the line number of the import
      ancestor_for: the child module this is an ancestor of, if applicable
      root_source: whether this source was specified on the command line
      skip_diagnose: skip any error diagnosis and reporting (but ModuleNotFound is
          still raised if the module is missing)

    The specified value of follow_imports for a module can be overridden
    if the module is specified on the command line or if it is a stub,
    so we compute and return the "effective" follow_imports of the module.

    Returns a tuple containing (file path, target's effective follow_imports setting)
    """
    result = find_module_with_reason(id, manager)
    if isinstance(result, str):
        # For non-stubs, look at options.follow_imports:
        # - normal (default) -&gt; fully analyze
        # - silent -&gt; analyze but silence errors
        # - skip -&gt; don't analyze, make the type Any
        follow_imports = options.follow_imports
        if (
            root_source  # Honor top-level modules
            or (
                result.endswith(".pyi")  # Stubs are always normal
                and not options.follow_imports_for_stubs  # except when they aren't
            )
            or id in mypy.semanal_main.core_modules  # core is always normal
        ):
            follow_imports = "normal"
        if skip_diagnose:
            pass
        elif follow_imports == "silent":
            # Still import it, but silence non-blocker errors.
            manager.log(f"Silencing {result} ({id})")
        elif follow_imports == "skip" or follow_imports == "error":
            # In 'error' mode, produce special error messages.
            if id not in manager.missing_modules:
                manager.log(f"Skipping {result} ({id})")
            if follow_imports == "error":
                if ancestor_for:
                    skipping_ancestor(manager, id, result, ancestor_for)
                else:
                    skipping_module(manager, caller_line, caller_state, id, result)
            raise ModuleNotFound
        if is_silent_import_module(manager, result):
            follow_imports = "silent"
        return (result, follow_imports)
    else:
        # Could not find a module.  Typically the reason is a
        # misspelled module name, missing stub, module not in
        # search path or the module has not been installed.

        ignore_missing_imports = options.ignore_missing_imports
        top_level, second_level = get_top_two_prefixes(id)
        # Don't honor a global (not per-module) ignore_missing_imports
        # setting for modules that used to have bundled stubs, as
        # otherwise updating mypy can silently result in new false
        # negatives. (Unless there are stubs but they are incomplete.)
        global_ignore_missing_imports = manager.options.ignore_missing_imports
        if (
            (is_legacy_bundled_package(top_level) or is_legacy_bundled_package(second_level))
            and global_ignore_missing_imports
            and not options.ignore_missing_imports_per_module
            and result is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
        ):
            ignore_missing_imports = False

        if skip_diagnose:
            raise ModuleNotFound
        if caller_state:
            if not (ignore_missing_imports or in_partial_package(id, manager)):
                module_not_found(manager, caller_line, caller_state, id, result)
            raise ModuleNotFound
        elif root_source:
            # If we can't find a root source it's always fatal.
            # TODO: This might hide non-fatal errors from
            # root sources processed earlier.
            raise CompileError([f"mypy: can't find module '{id}'"])
        else:
            raise ModuleNotFound


</t>
<t tx="ekr.20221004064034.295">def exist_added_packages(suppressed: list[str], manager: BuildManager, options: Options) -&gt; bool:
    """Find if there are any newly added packages that were previously suppressed.

    Exclude everything not in build for follow-imports=skip.
    """
    for dep in suppressed:
        if dep in manager.source_set.source_modules:
            # We don't need to add any special logic for this. If a module
            # is added to build, importers will be invalidated by normal mechanism.
            continue
        path = find_module_simple(dep, manager)
        if not path:
            continue
        if options.follow_imports == "skip" and (
            not path.endswith(".pyi") or options.follow_imports_for_stubs
        ):
            continue
        if "__init__.py" in path:
            # It is better to have a bit lenient test, this will only slightly reduce
            # performance, while having a too strict test may affect correctness.
            return True
    return False


</t>
<t tx="ekr.20221004064034.296">def find_module_simple(id: str, manager: BuildManager) -&gt; str | None:
    """Find a filesystem path for module `id` or `None` if not found."""
    x = find_module_with_reason(id, manager)
    if isinstance(x, ModuleNotFoundReason):
        return None
    return x


</t>
<t tx="ekr.20221004064034.297">def find_module_with_reason(id: str, manager: BuildManager) -&gt; ModuleSearchResult:
    """Find a filesystem path for module `id` or the reason it can't be found."""
    t0 = time.time()
    x = manager.find_module_cache.find_module(id)
    manager.add_stats(find_module_time=time.time() - t0, find_module_calls=1)
    return x


</t>
<t tx="ekr.20221004064034.298">def in_partial_package(id: str, manager: BuildManager) -&gt; bool:
    """Check if a missing module can potentially be a part of a package.

    This checks if there is any existing parent __init__.pyi stub that
    defines a module-level __getattr__ (a.k.a. partial stub package).
    """
    while "." in id:
        parent, _ = id.rsplit(".", 1)
        if parent in manager.modules:
            parent_mod: MypyFile | None = manager.modules[parent]
        else:
            # Parent is not in build, try quickly if we can find it.
            try:
                parent_st = State(
                    id=parent, path=None, source=None, manager=manager, temporary=True
                )
            except (ModuleNotFound, CompileError):
                parent_mod = None
            else:
                parent_mod = parent_st.tree
        if parent_mod is not None:
            if parent_mod.is_partial_stub_package:
                return True
            else:
                # Bail out soon, complete subpackage found
                return False
        id = parent
    return False


</t>
<t tx="ekr.20221004064034.299">def module_not_found(
    manager: BuildManager,
    line: int,
    caller_state: State,
    target: str,
    reason: ModuleNotFoundReason,
) -&gt; None:
    errors = manager.errors
    save_import_context = errors.import_context()
    errors.set_import_context(caller_state.import_context)
    errors.set_file(caller_state.xpath, caller_state.id, caller_state.options)
    if target == "builtins":
        errors.report(
            line, 0, "Cannot find 'builtins' module. Typeshed appears broken!", blocker=True
        )
        errors.raise_error()
    else:
        daemon = manager.options.fine_grained_incremental
        msg, notes = reason.error_message_templates(daemon)
        errors.report(line, 0, msg.format(module=target), code=codes.IMPORT)
        top_level, second_level = get_top_two_prefixes(target)
        if second_level in legacy_bundled_packages or second_level in non_bundled_packages:
            top_level = second_level
        for note in notes:
            if "{stub_dist}" in note:
                note = note.format(stub_dist=stub_package_name(top_level))
            errors.report(line, 0, note, severity="note", only_once=True, code=codes.IMPORT)
        if reason is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
            manager.missing_stub_packages.add(stub_package_name(top_level))
    errors.set_import_context(save_import_context)


</t>
<t tx="ekr.20221004064034.3">@path C:/Repos/ekr-mypy2/
from __future__ import annotations

import os.path

pytest_plugins = ["mypy.test.data"]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.30">def extract_classes(chunks: Iterable[CacheData]) -&gt; Iterable[JsonDict]:
    @others
    yield from extract([chunk.data for chunk in chunks])


</t>
<t tx="ekr.20221004064034.300">def skipping_module(
    manager: BuildManager, line: int, caller_state: State | None, id: str, path: str
) -&gt; None:
    """Produce an error for an import ignored due to --follow_imports=error"""
    assert caller_state, (id, path)
    save_import_context = manager.errors.import_context()
    manager.errors.set_import_context(caller_state.import_context)
    manager.errors.set_file(caller_state.xpath, caller_state.id, manager.options)
    manager.errors.report(line, 0, f'Import of "{id}" ignored', severity="error")
    manager.errors.report(
        line,
        0,
        "(Using --follow-imports=error, module not passed on command line)",
        severity="note",
        only_once=True,
    )
    manager.errors.set_import_context(save_import_context)


</t>
<t tx="ekr.20221004064034.301">def skipping_ancestor(manager: BuildManager, id: str, path: str, ancestor_for: State) -&gt; None:
    """Produce an error for an ancestor ignored due to --follow_imports=error"""
    # TODO: Read the path (the __init__.py file) and return
    # immediately if it's empty or only contains comments.
    # But beware, some package may be the ancestor of many modules,
    # so we'd need to cache the decision.
    manager.errors.set_import_context([])
    manager.errors.set_file(ancestor_for.xpath, ancestor_for.id, manager.options)
    manager.errors.report(
        -1, -1, f'Ancestor package "{id}" ignored', severity="error", only_once=True
    )
    manager.errors.report(
        -1,
        -1,
        "(Using --follow-imports=error, submodule passed on command line)",
        severity="note",
        only_once=True,
    )


</t>
<t tx="ekr.20221004064034.302">def log_configuration(manager: BuildManager, sources: list[BuildSource]) -&gt; None:
    """Output useful configuration information to LOG and TRACE"""

    manager.log()
    configuration_vars = [
        ("Mypy Version", __version__),
        ("Config File", (manager.options.config_file or "Default")),
        ("Configured Executable", manager.options.python_executable or "None"),
        ("Current Executable", sys.executable),
        ("Cache Dir", manager.options.cache_dir),
        ("Compiled", str(not __file__.endswith(".py"))),
        ("Exclude", manager.options.exclude),
    ]

    for conf_name, conf_value in configuration_vars:
        manager.log(f"{conf_name + ':':24}{conf_value}")

    for source in sources:
        manager.log(f"{'Found source:':24}{source}")

    # Complete list of searched paths can get very long, put them under TRACE
    for path_type, paths in manager.search_paths._asdict().items():
        if not paths:
            manager.trace(f"No {path_type}")
            continue

        manager.trace(f"{path_type}:")

        for pth in paths:
            manager.trace(f"    {pth}")


</t>
<t tx="ekr.20221004064034.303"># The driver


</t>
<t tx="ekr.20221004064034.304">def dispatch(sources: list[BuildSource], manager: BuildManager, stdout: TextIO) -&gt; Graph:
    log_configuration(manager, sources)

    t0 = time.time()
    graph = load_graph(sources, manager)

    # This is a kind of unfortunate hack to work around some of fine-grained's
    # fragility: if we have loaded less than 50% of the specified files from
    # cache in fine-grained cache mode, load the graph again honestly.
    # In this case, we just turn the cache off entirely, so we don't need
    # to worry about some files being loaded and some from cache and so
    # that fine-grained mode never *writes* to the cache.
    if manager.use_fine_grained_cache() and len(graph) &lt; 0.50 * len(sources):
        manager.log("Redoing load_graph without cache because too much was missing")
        manager.cache_enabled = False
        graph = load_graph(sources, manager)

    t1 = time.time()
    manager.add_stats(
        graph_size=len(graph),
        stubs_found=sum(g.path is not None and g.path.endswith(".pyi") for g in graph.values()),
        graph_load_time=(t1 - t0),
        fm_cache_size=len(manager.find_module_cache.results),
    )
    if not graph:
        print("Nothing to do?!", file=stdout)
        return graph
    manager.log(f"Loaded graph with {len(graph)} nodes ({t1 - t0:.3f} sec)")
    if manager.options.dump_graph:
        dump_graph(graph, stdout)
        return graph

    # Fine grained dependencies that didn't have an associated module in the build
    # are serialized separately, so we read them after we load the graph.
    # We need to read them both for running in daemon mode and if we are generating
    # a fine-grained cache (so that we can properly update them incrementally).
    # The `read_deps_cache` will also validate
    # the deps cache against the loaded individual cache files.
    if manager.options.cache_fine_grained or manager.use_fine_grained_cache():
        t2 = time.time()
        fg_deps_meta = read_deps_cache(manager, graph)
        manager.add_stats(load_fg_deps_time=time.time() - t2)
        if fg_deps_meta is not None:
            manager.fg_deps_meta = fg_deps_meta
        elif manager.stats.get("fresh_metas", 0) &gt; 0:
            # Clear the stats so we don't infinite loop because of positive fresh_metas
            manager.stats.clear()
            # There were some cache files read, but no fine-grained dependencies loaded.
            manager.log("Error reading fine-grained dependencies cache -- aborting cache load")
            manager.cache_enabled = False
            manager.log("Falling back to full run -- reloading graph...")
            return dispatch(sources, manager, stdout)

    # If we are loading a fine-grained incremental mode cache, we
    # don't want to do a real incremental reprocess of the
    # graph---we'll handle it all later.
    if not manager.use_fine_grained_cache():
        process_graph(graph, manager)
        # Update plugins snapshot.
        write_plugins_snapshot(manager)
        manager.old_plugins_snapshot = manager.plugins_snapshot
        if manager.options.cache_fine_grained or manager.options.fine_grained_incremental:
            # If we are running a daemon or are going to write cache for further fine grained use,
            # then we need to collect fine grained protocol dependencies.
            # Since these are a global property of the program, they are calculated after we
            # processed the whole graph.
            TypeState.add_all_protocol_deps(manager.fg_deps)
            if not manager.options.fine_grained_incremental:
                rdeps = generate_deps_for_cache(manager, graph)
                write_deps_cache(rdeps, manager, graph)

    if manager.options.dump_deps:
        # This speeds up startup a little when not using the daemon mode.
        from mypy.server.deps import dump_all_dependencies

        dump_all_dependencies(
            manager.modules, manager.all_types, manager.options.python_version, manager.options
        )
    return graph


</t>
<t tx="ekr.20221004064034.305">class NodeInfo:
    """Some info about a node in the graph of SCCs."""

    @others
</t>
<t tx="ekr.20221004064034.306">def __init__(self, index: int, scc: list[str]) -&gt; None:
    self.node_id = "n%d" % index
    self.scc = scc
    self.sizes: dict[str, int] = {}  # mod -&gt; size in bytes
    self.deps: dict[str, int] = {}  # node_id -&gt; pri

</t>
<t tx="ekr.20221004064034.307">def dumps(self) -&gt; str:
    """Convert to JSON string."""
    total_size = sum(self.sizes.values())
    return "[{}, {}, {},\n     {},\n     {}]".format(
        json.dumps(self.node_id),
        json.dumps(total_size),
        json.dumps(self.scc),
        json.dumps(self.sizes),
        json.dumps(self.deps),
    )


</t>
<t tx="ekr.20221004064034.308">def dump_timing_stats(path: str, graph: Graph) -&gt; None:
    """
    Dump timing stats for each file in the given graph
    """
    with open(path, "w") as f:
        for k in sorted(graph.keys()):
            v = graph[k]
            f.write(f"{v.id} {v.time_spent_us}\n")


</t>
<t tx="ekr.20221004064034.309">def dump_graph(graph: Graph, stdout: TextIO | None = None) -&gt; None:
    """Dump the graph as a JSON string to stdout.

    This copies some of the work by process_graph()
    (sorted_components() and order_ascc()).
    """
    stdout = stdout or sys.stdout
    nodes = []
    sccs = sorted_components(graph)
    for i, ascc in enumerate(sccs):
        scc = order_ascc(graph, ascc)
        node = NodeInfo(i, scc)
        nodes.append(node)
    inv_nodes = {}  # module -&gt; node_id
    for node in nodes:
        for mod in node.scc:
            inv_nodes[mod] = node.node_id
    for node in nodes:
        for mod in node.scc:
            state = graph[mod]
            size = 0
            if state.path:
                try:
                    size = os.path.getsize(state.path)
                except os.error:
                    pass
            node.sizes[mod] = size
            for dep in state.dependencies:
                if dep in state.priorities:
                    pri = state.priorities[dep]
                    if dep in inv_nodes:
                        dep_id = inv_nodes[dep]
                        if dep_id != node.node_id and (
                            dep_id not in node.deps or pri &lt; node.deps[dep_id]
                        ):
                            node.deps[dep_id] = pri
    print("[" + ",\n ".join(node.dumps() for node in nodes) + "\n]", file=stdout)


</t>
<t tx="ekr.20221004064034.31">def extract(chunks: Iterable[JsonDict]) -&gt; Iterable[JsonDict]:
    for chunk in chunks:
        if isinstance(chunk, dict):
            yield chunk
            yield from extract(chunk.values())
        elif isinstance(chunk, list):
            yield from extract(chunk)

</t>
<t tx="ekr.20221004064034.310">def load_graph(
    sources: list[BuildSource],
    manager: BuildManager,
    old_graph: Graph | None = None,
    new_modules: list[State] | None = None,
) -&gt; Graph:
    """Given some source files, load the full dependency graph.

    If an old_graph is passed in, it is used as the starting point and
    modified during graph loading.

    If a new_modules is passed in, any modules that are loaded are
    added to the list. This is an argument and not a return value
    so that the caller can access it even if load_graph fails.

    As this may need to parse files, this can raise CompileError in case
    there are syntax errors.
    """

    graph: Graph = old_graph if old_graph is not None else {}

    # The deque is used to implement breadth-first traversal.
    # TODO: Consider whether to go depth-first instead.  This may
    # affect the order in which we process files within import cycles.
    new = new_modules if new_modules is not None else []
    entry_points: set[str] = set()
    # Seed the graph with the initial root sources.
    for bs in sources:
        try:
            st = State(
                id=bs.module, path=bs.path, source=bs.text, manager=manager, root_source=True
            )
        except ModuleNotFound:
            continue
        if st.id in graph:
            manager.errors.set_file(st.xpath, st.id, manager.options)
            manager.errors.report(
                -1,
                -1,
                f'Duplicate module named "{st.id}" (also at "{graph[st.id].xpath}")',
                blocker=True,
            )
            manager.errors.report(
                -1,
                -1,
                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "  # noqa: E501
                "for more info",
                severity="note",
            )
            manager.errors.report(
                -1,
                -1,
                "Common resolutions include: a) using `--exclude` to avoid checking one of them, "
                "b) adding `__init__.py` somewhere, c) using `--explicit-package-bases` or "
                "adjusting MYPYPATH",
                severity="note",
            )

            manager.errors.raise_error()
        graph[st.id] = st
        new.append(st)
        entry_points.add(bs.module)

    # Note: Running this each time could be slow in the daemon. If it's a problem, we
    # can do more work to maintain this incrementally.
    seen_files = {st.abspath: st for st in graph.values() if st.path}

    # Collect dependencies.  We go breadth-first.
    # More nodes might get added to new as we go, but that's fine.
    for st in new:
        assert st.ancestors is not None
        # Strip out indirect dependencies.  These will be dealt with
        # when they show up as direct dependencies, and there's a
        # scenario where they hurt:
        # - Suppose A imports B and B imports C.
        # - Suppose on the next round:
        #   - C is deleted;
        #   - B is updated to remove the dependency on C;
        #   - A is unchanged.
        # - In this case A's cached *direct* dependencies are still valid
        #   (since direct dependencies reflect the imports found in the source)
        #   but A's cached *indirect* dependency on C is wrong.
        dependencies = [dep for dep in st.dependencies if st.priorities.get(dep) != PRI_INDIRECT]
        if not manager.use_fine_grained_cache():
            # TODO: Ideally we could skip here modules that appeared in st.suppressed
            # because they are not in build with `follow-imports=skip`.
            # This way we could avoid overhead of cloning options in `State.__init__()`
            # below to get the option value. This is quite minor performance loss however.
            added = [dep for dep in st.suppressed if find_module_simple(dep, manager)]
        else:
            # During initial loading we don't care about newly added modules,
            # they will be taken care of during fine grained update. See also
            # comment about this in `State.__init__()`.
            added = []
        for dep in st.ancestors + dependencies + st.suppressed:
            ignored = dep in st.suppressed_set and dep not in entry_points
            if ignored and dep not in added:
                manager.missing_modules.add(dep)
            elif dep not in graph:
                try:
                    if dep in st.ancestors:
                        # TODO: Why not 'if dep not in st.dependencies' ?
                        # Ancestors don't have import context.
                        newst = State(
                            id=dep, path=None, source=None, manager=manager, ancestor_for=st
                        )
                    else:
                        newst = State(
                            id=dep,
                            path=None,
                            source=None,
                            manager=manager,
                            caller_state=st,
                            caller_line=st.dep_line_map.get(dep, 1),
                        )
                except ModuleNotFound:
                    if dep in st.dependencies_set:
                        st.suppress_dependency(dep)
                else:
                    if newst.path:
                        newst_path = os.path.abspath(newst.path)

                        if newst_path in seen_files:
                            manager.errors.report(
                                -1,
                                0,
                                "Source file found twice under different module names: "
                                '"{}" and "{}"'.format(seen_files[newst_path].id, newst.id),
                                blocker=True,
                            )
                            manager.errors.report(
                                -1,
                                0,
                                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "  # noqa: E501
                                "for more info",
                                severity="note",
                            )
                            manager.errors.report(
                                -1,
                                0,
                                "Common resolutions include: a) adding `__init__.py` somewhere, "
                                "b) using `--explicit-package-bases` or adjusting MYPYPATH",
                                severity="note",
                            )
                            manager.errors.raise_error()

                        seen_files[newst_path] = newst

                    assert newst.id not in graph, newst.id
                    graph[newst.id] = newst
                    new.append(newst)
            if dep in graph and dep in st.suppressed_set:
                # Previously suppressed file is now visible
                st.add_dependency(dep)
    manager.plugin.set_modules(manager.modules)
    return graph


</t>
<t tx="ekr.20221004064034.311">def process_graph(graph: Graph, manager: BuildManager) -&gt; None:
    """Process everything in dependency order."""
    sccs = sorted_components(graph)
    manager.log("Found %d SCCs; largest has %d nodes" % (len(sccs), max(len(scc) for scc in sccs)))

    fresh_scc_queue: list[list[str]] = []

    # We're processing SCCs from leaves (those without further
    # dependencies) to roots (those from which everything else can be
    # reached).
    for ascc in sccs:
        # Order the SCC's nodes using a heuristic.
        # Note that ascc is a set, and scc is a list.
        scc = order_ascc(graph, ascc)
        # Make the order of the SCC that includes 'builtins' and 'typing',
        # among other things, predictable. Various things may  break if
        # the order changes.
        if "builtins" in ascc:
            scc = sorted(scc, reverse=True)
            # If builtins is in the list, move it last.  (This is a bit of
            # a hack, but it's necessary because the builtins module is
            # part of a small cycle involving at least {builtins, abc,
            # typing}.  Of these, builtins must be processed last or else
            # some builtin objects will be incompletely processed.)
            scc.remove("builtins")
            scc.append("builtins")
        if manager.options.verbosity &gt;= 2:
            for id in scc:
                manager.trace(
                    f"Priorities for {id}:",
                    " ".join(
                        "%s:%d" % (x, graph[id].priorities[x])
                        for x in graph[id].dependencies
                        if x in ascc and x in graph[id].priorities
                    ),
                )
        # Because the SCCs are presented in topological sort order, we
        # don't need to look at dependencies recursively for staleness
        # -- the immediate dependencies are sufficient.
        stale_scc = {id for id in scc if not graph[id].is_fresh()}
        fresh = not stale_scc
        deps = set()
        for id in scc:
            deps.update(graph[id].dependencies)
        deps -= ascc
        stale_deps = {id for id in deps if id in graph and not graph[id].is_interface_fresh()}
        fresh = fresh and not stale_deps
        undeps = set()
        if fresh:
            # Check if any dependencies that were suppressed according
            # to the cache have been added back in this run.
            # NOTE: Newly suppressed dependencies are handled by is_fresh().
            for id in scc:
                undeps.update(graph[id].suppressed)
            undeps &amp;= graph.keys()
            if undeps:
                fresh = False
        if fresh:
            # All cache files are fresh.  Check that no dependency's
            # cache file is newer than any scc node's cache file.
            oldest_in_scc = min(graph[id].xmeta.data_mtime for id in scc)
            viable = {id for id in stale_deps if graph[id].meta is not None}
            newest_in_deps = (
                0 if not viable else max(graph[dep].xmeta.data_mtime for dep in viable)
            )
            if manager.options.verbosity &gt;= 3:  # Dump all mtimes for extreme debugging.
                all_ids = sorted(ascc | viable, key=lambda id: graph[id].xmeta.data_mtime)
                for id in all_ids:
                    if id in scc:
                        if graph[id].xmeta.data_mtime &lt; newest_in_deps:
                            key = "*id:"
                        else:
                            key = "id:"
                    else:
                        if graph[id].xmeta.data_mtime &gt; oldest_in_scc:
                            key = "+dep:"
                        else:
                            key = "dep:"
                    manager.trace(" %5s %.0f %s" % (key, graph[id].xmeta.data_mtime, id))
            # If equal, give the benefit of the doubt, due to 1-sec time granularity
            # (on some platforms).
            if oldest_in_scc &lt; newest_in_deps:
                fresh = False
                fresh_msg = f"out of date by {newest_in_deps - oldest_in_scc:.0f} seconds"
            else:
                fresh_msg = "fresh"
        elif undeps:
            fresh_msg = f"stale due to changed suppression ({' '.join(sorted(undeps))})"
        elif stale_scc:
            fresh_msg = "inherently stale"
            if stale_scc != ascc:
                fresh_msg += f" ({' '.join(sorted(stale_scc))})"
            if stale_deps:
                fresh_msg += f" with stale deps ({' '.join(sorted(stale_deps))})"
        else:
            fresh_msg = f"stale due to deps ({' '.join(sorted(stale_deps))})"

        # Initialize transitive_error for all SCC members from union
        # of transitive_error of dependencies.
        if any(graph[dep].transitive_error for dep in deps if dep in graph):
            for id in scc:
                graph[id].transitive_error = True

        scc_str = " ".join(scc)
        if fresh:
            manager.trace(f"Queuing {fresh_msg} SCC ({scc_str})")
            fresh_scc_queue.append(scc)
        else:
            if len(fresh_scc_queue) &gt; 0:
                manager.log(f"Processing {len(fresh_scc_queue)} queued fresh SCCs")
                # Defer processing fresh SCCs until we actually run into a stale SCC
                # and need the earlier modules to be loaded.
                #
                # Note that `process_graph` may end with us not having processed every
                # single fresh SCC. This is intentional -- we don't need those modules
                # loaded if there are no more stale SCCs to be rechecked.
                #
                # Also note we shouldn't have to worry about transitive_error here,
                # since modules with transitive errors aren't written to the cache,
                # and if any dependencies were changed, this SCC would be stale.
                # (Also, in quick_and_dirty mode we don't care about transitive errors.)
                #
                # TODO: see if it's possible to determine if we need to process only a
                # _subset_ of the past SCCs instead of having to process them all.
                for prev_scc in fresh_scc_queue:
                    process_fresh_modules(graph, prev_scc, manager)
                fresh_scc_queue = []
            size = len(scc)
            if size == 1:
                manager.log(f"Processing SCC singleton ({scc_str}) as {fresh_msg}")
            else:
                manager.log("Processing SCC of size %d (%s) as %s" % (size, scc_str, fresh_msg))
            process_stale_scc(graph, scc, manager)

    sccs_left = len(fresh_scc_queue)
    nodes_left = sum(len(scc) for scc in fresh_scc_queue)
    manager.add_stats(sccs_left=sccs_left, nodes_left=nodes_left)
    if sccs_left:
        manager.log(
            "{} fresh SCCs ({} nodes) left in queue (and will remain unprocessed)".format(
                sccs_left, nodes_left
            )
        )
        manager.trace(str(fresh_scc_queue))
    else:
        manager.log("No fresh SCCs left in queue")


</t>
<t tx="ekr.20221004064034.312">def order_ascc(graph: Graph, ascc: AbstractSet[str], pri_max: int = PRI_ALL) -&gt; list[str]:
    """Come up with the ideal processing order within an SCC.

    Using the priorities assigned by all_imported_modules_in_file(),
    try to reduce the cycle to a DAG, by omitting arcs representing
    dependencies of lower priority.

    In the simplest case, if we have A &lt;--&gt; B where A has a top-level
    "import B" (medium priority) but B only has the reverse "import A"
    inside a function (low priority), we turn the cycle into a DAG by
    dropping the B --&gt; A arc, which leaves only A --&gt; B.

    If all arcs have the same priority, we fall back to sorting by
    reverse global order (the order in which modules were first
    encountered).

    The algorithm is recursive, as follows: when as arcs of different
    priorities are present, drop all arcs of the lowest priority,
    identify SCCs in the resulting graph, and apply the algorithm to
    each SCC thus found.  The recursion is bounded because at each
    recursion the spread in priorities is (at least) one less.

    In practice there are only a few priority levels (less than a
    dozen) and in the worst case we just carry out the same algorithm
    for finding SCCs N times.  Thus the complexity is no worse than
    the complexity of the original SCC-finding algorithm -- see
    strongly_connected_components() below for a reference.
    """
    if len(ascc) == 1:
        return [s for s in ascc]
    pri_spread = set()
    for id in ascc:
        state = graph[id]
        for dep in state.dependencies:
            if dep in ascc:
                pri = state.priorities.get(dep, PRI_HIGH)
                if pri &lt; pri_max:
                    pri_spread.add(pri)
    if len(pri_spread) == 1:
        # Filtered dependencies are uniform -- order by global order.
        return sorted(ascc, key=lambda id: -graph[id].order)
    pri_max = max(pri_spread)
    sccs = sorted_components(graph, ascc, pri_max)
    # The recursion is bounded by the len(pri_spread) check above.
    return [s for ss in sccs for s in order_ascc(graph, ss, pri_max)]


</t>
<t tx="ekr.20221004064034.313">def process_fresh_modules(graph: Graph, modules: list[str], manager: BuildManager) -&gt; None:
    """Process the modules in one group of modules from their cached data.

    This can be used to process an SCC of modules
    This involves loading the tree from JSON and then doing various cleanups.
    """
    t0 = time.time()
    for id in modules:
        graph[id].load_tree()
    t1 = time.time()
    for id in modules:
        graph[id].fix_cross_refs()
    t2 = time.time()
    manager.add_stats(process_fresh_time=t2 - t0, load_tree_time=t1 - t0)


</t>
<t tx="ekr.20221004064034.314">def process_stale_scc(graph: Graph, scc: list[str], manager: BuildManager) -&gt; None:
    """Process the modules in one SCC from source code.

    Exception: If quick_and_dirty is set, use the cache for fresh modules.
    """
    stale = scc
    for id in stale:
        # We may already have parsed the module, or not.
        # If the former, parse_file() is a no-op.
        graph[id].parse_file()
    if "typing" in scc:
        # For historical reasons we need to manually add typing aliases
        # for built-in generic collections, see docstring of
        # SemanticAnalyzerPass2.add_builtin_aliases for details.
        typing_mod = graph["typing"].tree
        assert typing_mod, "The typing module was not parsed"
    mypy.semanal_main.semantic_analysis_for_scc(graph, scc, manager.errors)

    # Track what modules aren't yet done so we can finish them as soon
    # as possible, saving memory.
    unfinished_modules = set(stale)
    for id in stale:
        graph[id].type_check_first_pass()
        if not graph[id].type_checker().deferred_nodes:
            unfinished_modules.discard(id)
            graph[id].detect_partially_defined_vars(graph[id].type_map())
            graph[id].finish_passes()

    while unfinished_modules:
        for id in stale:
            if id not in unfinished_modules:
                continue
            if not graph[id].type_check_second_pass():
                unfinished_modules.discard(id)
                graph[id].detect_partially_defined_vars(graph[id].type_map())
                graph[id].finish_passes()
    for id in stale:
        graph[id].generate_unused_ignore_notes()
        graph[id].generate_ignore_without_code_notes()
    if any(manager.errors.is_errors_for_file(graph[id].xpath) for id in stale):
        for id in stale:
            graph[id].transitive_error = True
    for id in stale:
        manager.flush_errors(manager.errors.file_messages(graph[id].xpath), False)
        graph[id].write_cache()
        graph[id].mark_as_rechecked()


</t>
<t tx="ekr.20221004064034.315">def sorted_components(
    graph: Graph, vertices: AbstractSet[str] | None = None, pri_max: int = PRI_ALL
) -&gt; list[AbstractSet[str]]:
    """Return the graph's SCCs, topologically sorted by dependencies.

    The sort order is from leaves (nodes without dependencies) to
    roots (nodes on which no other nodes depend).

    This works for a subset of the full dependency graph too;
    dependencies that aren't present in graph.keys() are ignored.
    """
    # Compute SCCs.
    if vertices is None:
        vertices = set(graph)
    edges = {id: deps_filtered(graph, vertices, id, pri_max) for id in vertices}
    sccs = list(strongly_connected_components(vertices, edges))
    # Topsort.
    sccsmap = {id: frozenset(scc) for scc in sccs for id in scc}
    data: dict[AbstractSet[str], set[AbstractSet[str]]] = {}
    for scc in sccs:
        deps: set[AbstractSet[str]] = set()
        for id in scc:
            deps.update(sccsmap[x] for x in deps_filtered(graph, vertices, id, pri_max))
        data[frozenset(scc)] = deps
    res = []
    for ready in topsort(data):
        # Sort the sets in ready by reversed smallest State.order.  Examples:
        #
        # - If ready is [{x}, {y}], x.order == 1, y.order == 2, we get
        #   [{y}, {x}].
        #
        # - If ready is [{a, b}, {c, d}], a.order == 1, b.order == 3,
        #   c.order == 2, d.order == 4, the sort keys become [1, 2]
        #   and the result is [{c, d}, {a, b}].
        res.extend(sorted(ready, key=lambda scc: -min(graph[id].order for id in scc)))
    return res


</t>
<t tx="ekr.20221004064034.316">def deps_filtered(graph: Graph, vertices: AbstractSet[str], id: str, pri_max: int) -&gt; list[str]:
    """Filter dependencies for id with pri &lt; pri_max."""
    if id not in vertices:
        return []
    state = graph[id]
    return [
        dep
        for dep in state.dependencies
        if dep in vertices and state.priorities.get(dep, PRI_HIGH) &lt; pri_max
    ]


</t>
<t tx="ekr.20221004064034.317">def strongly_connected_components(
    vertices: AbstractSet[str], edges: dict[str, list[str]]
) -&gt; Iterator[set[str]]:
    """Compute Strongly Connected Components of a directed graph.

    Args:
      vertices: the labels for the vertices
      edges: for each vertex, gives the target vertices of its outgoing edges

    Returns:
      An iterator yielding strongly connected components, each
      represented as a set of vertices.  Each input vertex will occur
      exactly once; vertices not part of a SCC are returned as
      singleton sets.

    From http://code.activestate.com/recipes/578507/.
    """
    identified: set[str] = set()
    stack: list[str] = []
    index: dict[str, int] = {}
    boundaries: list[int] = []

    @others
    for v in vertices:
        if v not in index:
            yield from dfs(v)


</t>
<t tx="ekr.20221004064034.318">def dfs(v: str) -&gt; Iterator[set[str]]:
    index[v] = len(stack)
    stack.append(v)
    boundaries.append(index[v])

    for w in edges[v]:
        if w not in index:
            yield from dfs(w)
        elif w not in identified:
            while index[w] &lt; boundaries[-1]:
                boundaries.pop()

    if boundaries[-1] == index[v]:
        boundaries.pop()
        scc = set(stack[index[v] :])
        del stack[index[v] :]
        identified.update(scc)
        yield scc

</t>
<t tx="ekr.20221004064034.319">T = TypeVar("T")


</t>
<t tx="ekr.20221004064034.32">def load_json(data_path: str, meta_path: str) -&gt; CacheData:
    with open(data_path) as ds:
        data_json = json.load(ds)

    with open(meta_path) as ms:
        meta_json = json.load(ms)

    data_size = os.path.getsize(data_path)
    meta_size = os.path.getsize(meta_path)

    return CacheData(
        data_path.replace(".data.json", ".*.json"), data_json, meta_json, data_size, meta_size
    )


</t>
<t tx="ekr.20221004064034.320">def topsort(data: dict[T, set[T]]) -&gt; Iterable[set[T]]:
    """Topological sort.

    Args:
      data: A map from vertices to all vertices that it has an edge
            connecting it to.  NOTE: This data structure
            is modified in place -- for normalization purposes,
            self-dependencies are removed and entries representing
            orphans are added.

    Returns:
      An iterator yielding sets of vertices that have an equivalent
      ordering.

    Example:
      Suppose the input has the following structure:

        {A: {B, C}, B: {D}, C: {D}}

      This is normalized to:

        {A: {B, C}, B: {D}, C: {D}, D: {}}

      The algorithm will yield the following values:

        {D}
        {B, C}
        {A}

    From http://code.activestate.com/recipes/577413/.
    """
    # TODO: Use a faster algorithm?
    for k, v in data.items():
        v.discard(k)  # Ignore self dependencies.
    for item in set.union(*data.values()) - set(data.keys()):
        data[item] = set()
    while True:
        ready = {item for item, dep in data.items() if not dep}
        if not ready:
            break
        yield ready
        data = {item: (dep - ready) for item, dep in data.items() if item not in ready}
    assert not data, f"A cyclic dependency exists amongst {data!r}"


</t>
<t tx="ekr.20221004064034.321">def missing_stubs_file(cache_dir: str) -&gt; str:
    return os.path.join(cache_dir, "missing_stubs")


</t>
<t tx="ekr.20221004064034.322">def record_missing_stub_packages(cache_dir: str, missing_stub_packages: set[str]) -&gt; None:
    """Write a file containing missing stub packages.

    This allows a subsequent "mypy --install-types" run (without other arguments)
    to install missing stub packages.
    """
    fnam = missing_stubs_file(cache_dir)
    if missing_stub_packages:
        with open(fnam, "w") as f:
            for pkg in sorted(missing_stub_packages):
                f.write(f"{pkg}\n")
    else:
        if os.path.isfile(fnam):
            os.remove(fnam)


</t>
<t tx="ekr.20221004064034.323">def is_silent_import_module(manager: BuildManager, path: str) -&gt; bool:
    if not manager.options.no_silence_site_packages:
        for dir in manager.search_paths.package_path + manager.search_paths.typeshed_path:
            if is_sub_path(path, dir):
                # Silence errors in site-package dirs and typeshed
                return True
    return False
</t>
<t tx="ekr.20221004064034.324">@path C:/Repos/ekr-mypy2/mypy/
"""Mypy type checker."""

from __future__ import annotations

import itertools
from collections import defaultdict
from contextlib import contextmanager, nullcontext
from typing import (
    AbstractSet,
    Any,
    Callable,
    Dict,
    Generic,
    Iterable,
    Iterator,
    Mapping,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
    TypeVar,
    Union,
    cast,
    overload,
)
from typing_extensions import Final, TypeAlias as _TypeAlias

import mypy.checkexpr
from mypy import errorcodes as codes, message_registry, nodes, operators
from mypy.binder import ConditionalTypeBinder, get_declaration
from mypy.checkmember import (
    MemberContext,
    analyze_decorator_or_funcbase_access,
    analyze_descriptor_access,
    analyze_member_access,
    type_object_type,
)
from mypy.checkpattern import PatternChecker
from mypy.constraints import SUPERTYPE_OF
from mypy.erasetype import erase_type, erase_typevars, remove_instance_last_known_values
from mypy.errorcodes import TYPE_VAR, UNUSED_AWAITABLE, UNUSED_COROUTINE, ErrorCode
from mypy.errors import Errors, ErrorWatcher, report_internal_error
from mypy.expandtype import expand_type, expand_type_by_instance
from mypy.join import join_types
from mypy.literals import Key, literal, literal_hash
from mypy.maptype import map_instance_to_supertype
from mypy.meet import is_overlapping_erased_types, is_overlapping_types
from mypy.message_registry import ErrorMessage
from mypy.messages import (
    SUGGESTED_TEST_FIXTURES,
    MessageBuilder,
    append_invariance_notes,
    format_type,
    format_type_bare,
    format_type_distinctly,
    make_inferred_type_note,
    pretty_seq,
)
from mypy.mro import MroError, calculate_mro
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    CONTRAVARIANT,
    COVARIANT,
    FUNC_NO_INFO,
    GDEF,
    IMPLICITLY_ABSTRACT,
    INVARIANT,
    IS_ABSTRACT,
    LDEF,
    LITERAL_TYPE,
    MDEF,
    NOT_ABSTRACT,
    AssertStmt,
    AssignmentExpr,
    AssignmentStmt,
    Block,
    BreakStmt,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    Context,
    ContinueStmt,
    Decorator,
    DelStmt,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    ForStmt,
    FuncBase,
    FuncDef,
    FuncItem,
    IfStmt,
    Import,
    ImportAll,
    ImportBase,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListExpr,
    Lvalue,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    PassStmt,
    PromoteExpr,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    StarExpr,
    Statement,
    SymbolNode,
    SymbolTable,
    SymbolTableNode,
    TempNode,
    TryStmt,
    TupleExpr,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    is_final_node,
)
from mypy.options import Options
from mypy.plugin import CheckerPluginInterface, Plugin
from mypy.scope import Scope
from mypy.semanal import is_trivial_body, refers_to_fullname, set_callable_name
from mypy.semanal_enum import ENUM_BASES, ENUM_SPECIAL_PROPS
from mypy.sharedparse import BINARY_MAGIC_METHODS
from mypy.state import state
from mypy.subtypes import (
    find_member,
    is_callable_compatible,
    is_equivalent,
    is_more_precise,
    is_proper_subtype,
    is_same_type,
    is_subtype,
    restrict_subtype_away,
    unify_generic_callable,
)
from mypy.traverser import all_return_statements, has_return_statement
from mypy.treetransform import TransformVisitor
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type, make_optional_type
from mypy.typeops import (
    bind_self,
    coerce_to_literal,
    custom_special_method,
    erase_def_to_union_or_bound,
    erase_to_bound,
    erase_to_union_or_bound,
    false_only,
    function_type,
    get_type_vars,
    is_literal_type_like,
    is_singleton_type,
    make_simplified_union,
    map_type_from_supertype,
    true_only,
    try_expanding_sum_type_to_union,
    try_getting_int_literals_from_type,
    try_getting_str_literals,
    try_getting_str_literals_from_type,
    tuple_fallback,
)
from mypy.types import (
    OVERLOAD_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    ParamSpecType,
    PartialType,
    ProperType,
    StarType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeGuardedType,
    TypeOfAny,
    TypeQuery,
    TypeTranslator,
    TypeType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    flatten_nested_unions,
    get_proper_type,
    get_proper_types,
    is_literal_type,
    is_named_instance,
    is_optional,
    remove_optional,
    strip_type,
)
from mypy.typetraverser import TypeTraverserVisitor
from mypy.typevars import fill_typevars, fill_typevars_with_any, has_no_typevars
from mypy.util import is_dunder, is_sunder, is_typeshed_file
from mypy.visitor import NodeVisitor

T = TypeVar("T")

DEFAULT_LAST_PASS: Final = 1  # Pass numbers start at 0

DeferredNodeType: _TypeAlias = Union[FuncDef, LambdaExpr, OverloadedFuncDef, Decorator]
FineGrainedDeferredNodeType: _TypeAlias = Union[FuncDef, MypyFile, OverloadedFuncDef]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.325"># A node which is postponed to be processed during the next pass.
# In normal mode one can defer functions and methods (also decorated and/or overloaded)
# and lambda expressions. Nested functions can't be deferred -- only top-level functions
# and methods of classes not defined within a function can be deferred.
class DeferredNode(NamedTuple):
    node: DeferredNodeType
    # And its TypeInfo (for semantic analysis self type handling
    active_typeinfo: TypeInfo | None


</t>
<t tx="ekr.20221004064034.326"># Same as above, but for fine-grained mode targets. Only top-level functions/methods
# and module top levels are allowed as such.
class FineGrainedDeferredNode(NamedTuple):
    node: FineGrainedDeferredNodeType
    active_typeinfo: TypeInfo | None


</t>
<t tx="ekr.20221004064034.327"># Data structure returned by find_isinstance_check representing
# information learned from the truth or falsehood of a condition.  The
# dict maps nodes representing expressions like 'a[0].x' to their
# refined types under the assumption that the condition has a
# particular truth value. A value of None means that the condition can
# never have that truth value.

# NB: The keys of this dict are nodes in the original source program,
# which are compared by reference equality--effectively, being *the
# same* expression of the program, not just two identical expressions
# (such as two references to the same variable). TODO: it would
# probably be better to have the dict keyed by the nodes' literal_hash
# field instead.
TypeMap: _TypeAlias = Optional[Dict[Expression, Type]]


</t>
<t tx="ekr.20221004064034.328"># An object that represents either a precise type or a type with an upper bound;
# it is important for correct type inference with isinstance.
class TypeRange(NamedTuple):
    item: Type
    is_upper_bound: bool  # False =&gt; precise type


</t>
<t tx="ekr.20221004064034.329"># Keeps track of partial types in a single scope. In fine-grained incremental
# mode partial types initially defined at the top level cannot be completed in
# a function, and we use the 'is_function' attribute to enforce this.
class PartialTypeScope(NamedTuple):
    map: dict[Var, Context]
    is_function: bool
    is_local: bool


</t>
<t tx="ekr.20221004064034.33">def get_files(root: str) -&gt; Iterable[CacheData]:
    for (dirpath, dirnames, filenames) in os.walk(root):
        for filename in filenames:
            if filename.endswith(".data.json"):
                meta_filename = filename.replace(".data.json", ".meta.json")
                yield load_json(
                    os.path.join(dirpath, filename), os.path.join(dirpath, meta_filename)
                )


</t>
<t tx="ekr.20221004064034.330">class TypeChecker(NodeVisitor[None], CheckerPluginInterface):
    """Mypy type checker.

    Type check mypy source files that have been semantically analyzed.

    You must create a separate instance for each source file.
    """

    # Are we type checking a stub?
    is_stub = False
    # Error message reporter
    errors: Errors
    # Utility for generating messages
    msg: MessageBuilder
    # Types of type checked nodes. The first item is the "master" type
    # map that will store the final, exported types. Additional items
    # are temporary type maps used during type inference, and these
    # will be eventually popped and either discarded or merged into
    # the master type map.
    #
    # Avoid accessing this directly, but prefer the lookup_type(),
    # has_type() etc. helpers instead.
    _type_maps: list[dict[Expression, Type]]

    # Helper for managing conditional types
    binder: ConditionalTypeBinder
    # Helper for type checking expressions
    expr_checker: mypy.checkexpr.ExpressionChecker

    pattern_checker: PatternChecker

    tscope: Scope
    scope: CheckerScope
    # Stack of function return types
    return_types: list[Type]
    # Flags; true for dynamically typed functions
    dynamic_funcs: list[bool]
    # Stack of collections of variables with partial types
    partial_types: list[PartialTypeScope]
    # Vars for which partial type errors are already reported
    # (to avoid logically duplicate errors with different error context).
    partial_reported: set[Var]
    globals: SymbolTable
    modules: dict[str, MypyFile]
    # Nodes that couldn't be checked because some types weren't available. We'll run
    # another pass and try these again.
    deferred_nodes: list[DeferredNode]
    # Type checking pass number (0 = first pass)
    pass_num = 0
    # Last pass number to take
    last_pass = DEFAULT_LAST_PASS
    # Have we deferred the current function? If yes, don't infer additional
    # types during this pass within the function.
    current_node_deferred = False
    # Is this file a typeshed stub?
    is_typeshed_stub = False
    options: Options
    # Used for collecting inferred attribute types so that they can be checked
    # for consistency.
    inferred_attribute_types: dict[Var, Type] | None = None
    # Don't infer partial None types if we are processing assignment from Union
    no_partial_types: bool = False

    # The set of all dependencies (suppressed or not) that this module accesses, either
    # directly or indirectly.
    module_refs: set[str]

    # A map from variable nodes to a snapshot of the frame ids of the
    # frames that were active when the variable was declared. This can
    # be used to determine nearest common ancestor frame of a variable's
    # declaration and the current frame, which lets us determine if it
    # was declared in a different branch of the same `if` statement
    # (if that frame is a conditional_frame).
    var_decl_frames: dict[Var, set[int]]

    # Plugin that provides special type checking rules for specific library
    # functions such as open(), etc.
    plugin: Plugin

    @others
</t>
<t tx="ekr.20221004064034.331">def __init__(
    self,
    errors: Errors,
    modules: dict[str, MypyFile],
    options: Options,
    tree: MypyFile,
    path: str,
    plugin: Plugin,
) -&gt; None:
    """Construct a type checker.

    Use errors to report type check errors.
    """
    self.errors = errors
    self.modules = modules
    self.options = options
    self.tree = tree
    self.path = path
    self.msg = MessageBuilder(errors, modules)
    self.plugin = plugin
    self.expr_checker = mypy.checkexpr.ExpressionChecker(self, self.msg, self.plugin)
    self.pattern_checker = PatternChecker(self, self.msg, self.plugin)
    self.tscope = Scope()
    self.scope = CheckerScope(tree)
    self.binder = ConditionalTypeBinder()
    self.globals = tree.names
    self.return_types = []
    self.dynamic_funcs = []
    self.partial_types = []
    self.partial_reported = set()
    self.var_decl_frames = {}
    self.deferred_nodes = []
    self._type_maps = [{}]
    self.module_refs = set()
    self.pass_num = 0
    self.current_node_deferred = False
    self.is_stub = tree.is_stub
    self.is_typeshed_stub = is_typeshed_file(options.abs_custom_typeshed_dir, path)
    self.inferred_attribute_types = None

    # If True, process function definitions. If False, don't. This is used
    # for processing module top levels in fine-grained incremental mode.
    self.recurse_into_functions = True
    # This internal flag is used to track whether we a currently type-checking
    # a final declaration (assignment), so that some errors should be suppressed.
    # Should not be set manually, use get_final_context/enter_final_context instead.
    # NOTE: we use the context manager to avoid "threading" an additional `is_final_def`
    # argument through various `checker` and `checkmember` functions.
    self._is_final_def = False

    # This flag is set when we run type-check or attribute access check for the purpose
    # of giving a note on possibly missing "await". It is used to avoid infinite recursion.
    self.checking_missing_await = False

    # While this is True, allow passing an abstract class where Type[T] is expected.
    # although this is technically unsafe, this is desirable in some context, for
    # example when type-checking class decorators.
    self.allow_abstract_call = False

</t>
<t tx="ekr.20221004064034.332">@property
def type_context(self) -&gt; list[Type | None]:
    return self.expr_checker.type_context

</t>
<t tx="ekr.20221004064034.333">def reset(self) -&gt; None:
    """Cleanup stale state that might be left over from a typechecking run.

    This allows us to reuse TypeChecker objects in fine-grained
    incremental mode.
    """
    # TODO: verify this is still actually worth it over creating new checkers
    self.partial_reported.clear()
    self.module_refs.clear()
    self.binder = ConditionalTypeBinder()
    self._type_maps[1:] = []
    self._type_maps[0].clear()
    self.temp_type_map = None
    self.expr_checker.reset()

    assert self.inferred_attribute_types is None
    assert self.partial_types == []
    assert self.deferred_nodes == []
    assert len(self.scope.stack) == 1
    assert self.partial_types == []

</t>
<t tx="ekr.20221004064034.334">def check_first_pass(self) -&gt; None:
    """Type check the entire file, but defer functions with unresolved references.

    Unresolved references are forward references to variables
    whose types haven't been inferred yet.  They may occur later
    in the same file or in a different file that's being processed
    later (usually due to an import cycle).

    Deferred functions will be processed by check_second_pass().
    """
    self.recurse_into_functions = True
    with state.strict_optional_set(self.options.strict_optional):
        self.errors.set_file(
            self.path, self.tree.fullname, scope=self.tscope, options=self.options
        )
        with self.tscope.module_scope(self.tree.fullname):
            with self.enter_partial_types(), self.binder.top_frame_context():
                for d in self.tree.defs:
                    if (
                        self.binder.is_unreachable()
                        and self.should_report_unreachable_issues()
                        and not self.is_raising_or_empty(d)
                    ):
                        self.msg.unreachable_statement(d)
                        break
                    self.accept(d)

            assert not self.current_node_deferred

            all_ = self.globals.get("__all__")
            if all_ is not None and all_.type is not None:
                all_node = all_.node
                assert all_node is not None
                seq_str = self.named_generic_type(
                    "typing.Sequence", [self.named_type("builtins.str")]
                )
                if not is_subtype(all_.type, seq_str):
                    str_seq_s, all_s = format_type_distinctly(seq_str, all_.type)
                    self.fail(
                        message_registry.ALL_MUST_BE_SEQ_STR.format(str_seq_s, all_s), all_node
                    )

</t>
<t tx="ekr.20221004064034.335">def check_second_pass(
    self, todo: Sequence[DeferredNode | FineGrainedDeferredNode] | None = None
) -&gt; bool:
    """Run second or following pass of type checking.

    This goes through deferred nodes, returning True if there were any.
    """
    self.recurse_into_functions = True
    with state.strict_optional_set(self.options.strict_optional):
        if not todo and not self.deferred_nodes:
            return False
        self.errors.set_file(
            self.path, self.tree.fullname, scope=self.tscope, options=self.options
        )
        with self.tscope.module_scope(self.tree.fullname):
            self.pass_num += 1
            if not todo:
                todo = self.deferred_nodes
            else:
                assert not self.deferred_nodes
            self.deferred_nodes = []
            done: set[DeferredNodeType | FineGrainedDeferredNodeType] = set()
            for node, active_typeinfo in todo:
                if node in done:
                    continue
                # This is useful for debugging:
                # print("XXX in pass %d, class %s, function %s" %
                #       (self.pass_num, type_name, node.fullname or node.name))
                done.add(node)
                with self.tscope.class_scope(
                    active_typeinfo
                ) if active_typeinfo else nullcontext():
                    with self.scope.push_class(
                        active_typeinfo
                    ) if active_typeinfo else nullcontext():
                        self.check_partial(node)
        return True

</t>
<t tx="ekr.20221004064034.336">def check_partial(self, node: DeferredNodeType | FineGrainedDeferredNodeType) -&gt; None:
    if isinstance(node, MypyFile):
        self.check_top_level(node)
    else:
        self.recurse_into_functions = True
        if isinstance(node, LambdaExpr):
            self.expr_checker.accept(node)
        else:
            self.accept(node)

</t>
<t tx="ekr.20221004064034.337">def check_top_level(self, node: MypyFile) -&gt; None:
    """Check only the top-level of a module, skipping function definitions."""
    self.recurse_into_functions = False
    with self.enter_partial_types():
        with self.binder.top_frame_context():
            for d in node.defs:
                d.accept(self)

    assert not self.current_node_deferred
    # TODO: Handle __all__

</t>
<t tx="ekr.20221004064034.338">def defer_node(self, node: DeferredNodeType, enclosing_class: TypeInfo | None) -&gt; None:
    """Defer a node for processing during next type-checking pass.

    Args:
        node: function/method being deferred
        enclosing_class: for methods, the class where the method is defined
    NOTE: this can't handle nested functions/methods.
    """
    # We don't freeze the entire scope since only top-level functions and methods
    # can be deferred. Only module/class level scope information is needed.
    # Module-level scope information is preserved in the TypeChecker instance.
    self.deferred_nodes.append(DeferredNode(node, enclosing_class))

</t>
<t tx="ekr.20221004064034.339">def handle_cannot_determine_type(self, name: str, context: Context) -&gt; None:
    node = self.scope.top_non_lambda_function()
    if self.pass_num &lt; self.last_pass and isinstance(node, FuncDef):
        # Don't report an error yet. Just defer. Note that we don't defer
        # lambdas because they are coupled to the surrounding function
        # through the binder and the inferred type of the lambda, so it
        # would get messy.
        enclosing_class = self.scope.enclosing_class()
        self.defer_node(node, enclosing_class)
        # Set a marker so that we won't infer additional types in this
        # function. Any inferred types could be bogus, because there's at
        # least one type that we don't know.
        self.current_node_deferred = True
    else:
        self.msg.cannot_determine_type(name, context)

</t>
<t tx="ekr.20221004064034.34">def pluck(name: str, chunks: Iterable[JsonDict]) -&gt; Iterable[JsonDict]:
    return (chunk for chunk in chunks if chunk[".class"] == name)


</t>
<t tx="ekr.20221004064034.340">def accept(self, stmt: Statement) -&gt; None:
    """Type check a node in the given type context."""
    try:
        stmt.accept(self)
    except Exception as err:
        report_internal_error(err, self.errors.file, stmt.line, self.errors, self.options)

</t>
<t tx="ekr.20221004064034.341">def accept_loop(
    self,
    body: Statement,
    else_body: Statement | None = None,
    *,
    exit_condition: Expression | None = None,
) -&gt; None:
    """Repeatedly type check a loop body until the frame doesn't change.
    If exit_condition is set, assume it must be False on exit from the loop.

    Then check the else_body.
    """
    # The outer frame accumulates the results of all iterations
    with self.binder.frame_context(can_skip=False, conditional_frame=True):
        while True:
            with self.binder.frame_context(can_skip=True, break_frame=2, continue_frame=1):
                self.accept(body)
            if not self.binder.last_pop_changed:
                break
        if exit_condition:
            _, else_map = self.find_isinstance_check(exit_condition)
            self.push_type_map(else_map)
        if else_body:
            self.accept(else_body)

</t>
<t tx="ekr.20221004064034.342">#
# Definitions
#

</t>
<t tx="ekr.20221004064034.343">def visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.tscope.function_scope(defn):
        self._visit_overloaded_func_def(defn)

</t>
<t tx="ekr.20221004064034.344">def _visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    num_abstract = 0
    if not defn.items:
        # In this case we have already complained about none of these being
        # valid overloads.
        return None
    if len(defn.items) == 1:
        self.fail(message_registry.MULTIPLE_OVERLOADS_REQUIRED, defn)

    if defn.is_property:
        # HACK: Infer the type of the property.
        self.visit_decorator(cast(Decorator, defn.items[0]))
    for fdef in defn.items:
        assert isinstance(fdef, Decorator)
        self.check_func_item(fdef.func, name=fdef.func.name, allow_empty=True)
        if fdef.func.abstract_status in (IS_ABSTRACT, IMPLICITLY_ABSTRACT):
            num_abstract += 1
    if num_abstract not in (0, len(defn.items)):
        self.fail(message_registry.INCONSISTENT_ABSTRACT_OVERLOAD, defn)
    if defn.impl:
        defn.impl.accept(self)
    if defn.info:
        self.check_method_override(defn)
        self.check_inplace_operator_method(defn)
    if not defn.is_property:
        self.check_overlapping_overloads(defn)
    return None

</t>
<t tx="ekr.20221004064034.345">def check_overlapping_overloads(self, defn: OverloadedFuncDef) -&gt; None:
    # At this point we should have set the impl already, and all remaining
    # items are decorators

    if self.msg.errors.file in self.msg.errors.ignored_files:
        # This is a little hacky, however, the quadratic check here is really expensive, this
        # method has no side effects, so we should skip it if we aren't going to report
        # anything. In some other places we swallow errors in stubs, but this error is very
        # useful for stubs!
        return

    # Compute some info about the implementation (if it exists) for use below
    impl_type: CallableType | None = None
    if defn.impl:
        if isinstance(defn.impl, FuncDef):
            inner_type: Type | None = defn.impl.type
        elif isinstance(defn.impl, Decorator):
            inner_type = defn.impl.var.type
        else:
            assert False, "Impl isn't the right type"

        # This can happen if we've got an overload with a different
        # decorator or if the implementation is untyped -- we gave up on the types.
        inner_type = get_proper_type(inner_type)
        if inner_type is not None and not isinstance(inner_type, AnyType):
            if isinstance(inner_type, CallableType):
                impl_type = inner_type
            elif isinstance(inner_type, Instance):
                inner_call = get_proper_type(
                    analyze_member_access(
                        name="__call__",
                        typ=inner_type,
                        context=defn.impl,
                        is_lvalue=False,
                        is_super=False,
                        is_operator=True,
                        msg=self.msg,
                        original_type=inner_type,
                        chk=self,
                    )
                )
                if isinstance(inner_call, CallableType):
                    impl_type = inner_call
            if impl_type is None:
                self.msg.not_callable(inner_type, defn.impl)

    is_descriptor_get = defn.info and defn.name == "__get__"
    for i, item in enumerate(defn.items):
        # TODO overloads involving decorators
        assert isinstance(item, Decorator)
        sig1 = self.function_type(item.func)
        assert isinstance(sig1, CallableType)

        for j, item2 in enumerate(defn.items[i + 1 :]):
            assert isinstance(item2, Decorator)
            sig2 = self.function_type(item2.func)
            assert isinstance(sig2, CallableType)

            if not are_argument_counts_overlapping(sig1, sig2):
                continue

            if overload_can_never_match(sig1, sig2):
                self.msg.overloaded_signature_will_never_match(i + 1, i + j + 2, item2.func)
            elif not is_descriptor_get:
                # Note: we force mypy to check overload signatures in strict-optional mode
                # so we don't incorrectly report errors when a user tries typing an overload
                # that happens to have a 'if the argument is None' fallback.
                #
                # For example, the following is fine in strict-optional mode but would throw
                # the unsafe overlap error when strict-optional is disabled:
                #
                #     @overload
                #     def foo(x: None) -&gt; int: ...
                #     @overload
                #     def foo(x: str) -&gt; str: ...
                #
                # See Python 2's map function for a concrete example of this kind of overload.
                with state.strict_optional_set(True):
                    if is_unsafe_overlapping_overload_signatures(sig1, sig2):
                        self.msg.overloaded_signatures_overlap(i + 1, i + j + 2, item.func)

        if impl_type is not None:
            assert defn.impl is not None

            # We perform a unification step that's very similar to what
            # 'is_callable_compatible' would have done if we had set
            # 'unify_generics' to True -- the only difference is that
            # we check and see if the impl_type's return value is a
            # *supertype* of the overload alternative, not a *subtype*.
            #
            # This is to match the direction the implementation's return
            # needs to be compatible in.
            if impl_type.variables:
                impl: CallableType | None = unify_generic_callable(
                    # Normalize both before unifying
                    impl_type.with_unpacked_kwargs(),
                    sig1.with_unpacked_kwargs(),
                    ignore_return=False,
                    return_constraint_direction=SUPERTYPE_OF,
                )
                if impl is None:
                    self.msg.overloaded_signatures_typevar_specific(i + 1, defn.impl)
                    continue
            else:
                impl = impl_type

            # Prevent extra noise from inconsistent use of @classmethod by copying
            # the first arg from the method being checked against.
            if sig1.arg_types and defn.info:
                impl = impl.copy_modified(arg_types=[sig1.arg_types[0]] + impl.arg_types[1:])

            # Is the overload alternative's arguments subtypes of the implementation's?
            if not is_callable_compatible(
                impl, sig1, is_compat=is_subtype, ignore_return=True
            ):
                self.msg.overloaded_signatures_arg_specific(i + 1, defn.impl)

            # Is the overload alternative's return type a subtype of the implementation's?
            if not (
                is_subtype(sig1.ret_type, impl.ret_type)
                or is_subtype(impl.ret_type, sig1.ret_type)
            ):
                self.msg.overloaded_signatures_ret_specific(i + 1, defn.impl)

</t>
<t tx="ekr.20221004064034.346"># Here's the scoop about generators and coroutines.
#
# There are two kinds of generators: classic generators (functions
# with `yield` or `yield from` in the body) and coroutines
# (functions declared with `async def`).  The latter are specified
# in PEP 492 and only available in Python &gt;= 3.5.
#
# Classic generators can be parameterized with three types:
# - ty is the Yield type (the type of y in `yield y`)
# - tc is the type reCeived by yield (the type of c in `c = yield`).
# - tr is the Return type (the type of r in `return r`)
#
# A classic generator must define a return type that's either
# `Generator[ty, tc, tr]`, Iterator[ty], or Iterable[ty] (or
# object or Any).  If tc/tr are not given, both are None.
#
# A coroutine must define a return type corresponding to tr; the
# other two are unconstrained.  The "external" return type (seen
# by the caller) is Awaitable[tr].
#
# In addition, there's the synthetic type AwaitableGenerator: it
# inherits from both Awaitable and Generator and can be used both
# in `yield from` and in `await`.  This type is set automatically
# for functions decorated with `@types.coroutine` or
# `@asyncio.coroutine`.  Its single parameter corresponds to tr.
#
# PEP 525 adds a new type, the asynchronous generator, which was
# first released in Python 3.6. Async generators are `async def`
# functions that can also `yield` values. They can be parameterized
# with two types, ty and tc, because they cannot return a value.
#
# There are several useful methods, each taking a type t and a
# flag c indicating whether it's for a generator or coroutine:
#
# - is_generator_return_type(t, c) returns whether t is a Generator,
#   Iterator, Iterable (if not c), or Awaitable (if c), or
#   AwaitableGenerator (regardless of c).
# - is_async_generator_return_type(t) returns whether t is an
#   AsyncGenerator.
# - get_generator_yield_type(t, c) returns ty.
# - get_generator_receive_type(t, c) returns tc.
# - get_generator_return_type(t, c) returns tr.

</t>
<t tx="ekr.20221004064034.347">def is_generator_return_type(self, typ: Type, is_coroutine: bool) -&gt; bool:
    """Is `typ` a valid type for a generator/coroutine?

    True if `typ` is a *supertype* of Generator or Awaitable.
    Also true it it's *exactly* AwaitableGenerator (modulo type parameters).
    """
    typ = get_proper_type(typ)
    if is_coroutine:
        # This means we're in Python 3.5 or later.
        at = self.named_generic_type("typing.Awaitable", [AnyType(TypeOfAny.special_form)])
        if is_subtype(at, typ):
            return True
    else:
        any_type = AnyType(TypeOfAny.special_form)
        gt = self.named_generic_type("typing.Generator", [any_type, any_type, any_type])
        if is_subtype(gt, typ):
            return True
    return isinstance(typ, Instance) and typ.type.fullname == "typing.AwaitableGenerator"

</t>
<t tx="ekr.20221004064034.348">def is_async_generator_return_type(self, typ: Type) -&gt; bool:
    """Is `typ` a valid type for an async generator?

    True if `typ` is a supertype of AsyncGenerator.
    """
    try:
        any_type = AnyType(TypeOfAny.special_form)
        agt = self.named_generic_type("typing.AsyncGenerator", [any_type, any_type])
    except KeyError:
        # we're running on a version of typing that doesn't have AsyncGenerator yet
        return False
    return is_subtype(agt, typ)

</t>
<t tx="ekr.20221004064034.349">def get_generator_yield_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given the declared return type of a generator (t), return the type it yields (ty)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif not self.is_generator_return_type(
        return_type, is_coroutine
    ) and not self.is_async_generator_return_type(return_type):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == "typing.Awaitable":
        # Awaitable: ty is Any.
        return AnyType(TypeOfAny.special_form)
    elif return_type.args:
        # AwaitableGenerator, Generator, AsyncGenerator, Iterator, or Iterable; ty is args[0].
        ret_type = return_type.args[0]
        # TODO not best fix, better have dedicated yield token
        return ret_type
    else:
        # If the function's declared supertype of Generator has no type
        # parameters (i.e. is `object`), then the yielded values can't
        # be accessed so any type is acceptable.  IOW, ty is Any.
        # (However, see https://github.com/python/mypy/issues/1933)
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.35">def report_counter(counter: Counter[str], amount: int | None = None) -&gt; None:
    for name, count in counter.most_common(amount):
        print(f"    {count: &lt;8} {name}")
    print()


</t>
<t tx="ekr.20221004064034.350">def get_generator_receive_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given a declared generator return type (t), return the type its yield receives (tc)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif not self.is_generator_return_type(
        return_type, is_coroutine
    ) and not self.is_async_generator_return_type(return_type):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == "typing.Awaitable":
        # Awaitable, AwaitableGenerator: tc is Any.
        return AnyType(TypeOfAny.special_form)
    elif (
        return_type.type.fullname in ("typing.Generator", "typing.AwaitableGenerator")
        and len(return_type.args) &gt;= 3
    ):
        # Generator: tc is args[1].
        return return_type.args[1]
    elif return_type.type.fullname == "typing.AsyncGenerator" and len(return_type.args) &gt;= 2:
        return return_type.args[1]
    else:
        # `return_type` is a supertype of Generator, so callers won't be able to send it
        # values.  IOW, tc is None.
        return NoneType()

</t>
<t tx="ekr.20221004064034.351">def get_coroutine_return_type(self, return_type: Type) -&gt; Type:
    return_type = get_proper_type(return_type)
    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    assert isinstance(return_type, Instance), "Should only be called on coroutine functions."
    # Note: return type is the 3rd type parameter of Coroutine.
    return return_type.args[2]

</t>
<t tx="ekr.20221004064034.352">def get_generator_return_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given the declared return type of a generator (t), return the type it returns (tr)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif not self.is_generator_return_type(return_type, is_coroutine):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == "typing.Awaitable" and len(return_type.args) == 1:
        # Awaitable: tr is args[0].
        return return_type.args[0]
    elif (
        return_type.type.fullname in ("typing.Generator", "typing.AwaitableGenerator")
        and len(return_type.args) &gt;= 3
    ):
        # AwaitableGenerator, Generator: tr is args[2].
        return return_type.args[2]
    else:
        # Supertype of Generator (Iterator, Iterable, object): tr is any.
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.353">def visit_func_def(self, defn: FuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.tscope.function_scope(defn):
        self._visit_func_def(defn)

</t>
<t tx="ekr.20221004064034.354">def _visit_func_def(self, defn: FuncDef) -&gt; None:
    """Type check a function definition."""
    self.check_func_item(defn, name=defn.name)
    if defn.info:
        if not defn.is_dynamic() and not defn.is_overload and not defn.is_decorated:
            # If the definition is the implementation for an
            # overload, the legality of the override has already
            # been typechecked, and decorated methods will be
            # checked when the decorator is.
            self.check_method_override(defn)
        self.check_inplace_operator_method(defn)
    if defn.original_def:
        # Override previous definition.
        new_type = self.function_type(defn)
        if isinstance(defn.original_def, FuncDef):
            # Function definition overrides function definition.
            old_type = self.function_type(defn.original_def)
            if not is_same_type(new_type, old_type):
                self.msg.incompatible_conditional_function_def(defn, old_type, new_type)
        else:
            # Function definition overrides a variable initialized via assignment or a
            # decorated function.
            orig_type = defn.original_def.type
            assert orig_type is not None, f"Error checking function redefinition {defn}"
            if isinstance(orig_type, PartialType):
                if orig_type.type is None:
                    # Ah this is a partial type. Give it the type of the function.
                    orig_def = defn.original_def
                    if isinstance(orig_def, Decorator):
                        var = orig_def.var
                    else:
                        var = orig_def
                    partial_types = self.find_partial_types(var)
                    if partial_types is not None:
                        var.type = new_type
                        del partial_types[var]
                else:
                    # Trying to redefine something like partial empty list as function.
                    self.fail(message_registry.INCOMPATIBLE_REDEFINITION, defn)
            else:
                name_expr = NameExpr(defn.name)
                name_expr.node = defn.original_def
                self.binder.assign_type(name_expr, new_type, orig_type)
                self.check_subtype(
                    new_type,
                    orig_type,
                    defn,
                    message_registry.INCOMPATIBLE_REDEFINITION,
                    "redefinition with type",
                    "original type",
                )

</t>
<t tx="ekr.20221004064034.355">def check_func_item(
    self,
    defn: FuncItem,
    type_override: CallableType | None = None,
    name: str | None = None,
    allow_empty: bool = False,
) -&gt; None:
    """Type check a function.

    If type_override is provided, use it as the function type.
    """
    self.dynamic_funcs.append(defn.is_dynamic() and not type_override)

    with self.enter_partial_types(is_function=True):
        typ = self.function_type(defn)
        if type_override:
            typ = type_override.copy_modified(line=typ.line, column=typ.column)
        if isinstance(typ, CallableType):
            with self.enter_attribute_inference_context():
                self.check_func_def(defn, typ, name, allow_empty)
        else:
            raise RuntimeError("Not supported")

    self.dynamic_funcs.pop()
    self.current_node_deferred = False

    if name == "__exit__":
        self.check__exit__return_type(defn)

</t>
<t tx="ekr.20221004064034.356">@contextmanager
def enter_attribute_inference_context(self) -&gt; Iterator[None]:
    old_types = self.inferred_attribute_types
    self.inferred_attribute_types = {}
    yield None
    self.inferred_attribute_types = old_types

</t>
<t tx="ekr.20221004064034.357">def check_func_def(
    self, defn: FuncItem, typ: CallableType, name: str | None, allow_empty: bool = False
) -&gt; None:
    """Type check a function definition."""
    # Expand type variables with value restrictions to ordinary types.
    expanded = self.expand_typevars(defn, typ)
    for item, typ in expanded:
        old_binder = self.binder
        self.binder = ConditionalTypeBinder()
        with self.binder.top_frame_context():
            defn.expanded.append(item)

            # We may be checking a function definition or an anonymous
            # function. In the first case, set up another reference with the
            # precise type.
            if isinstance(item, FuncDef):
                fdef = item
                # Check if __init__ has an invalid return type.
                if (
                    fdef.info
                    and fdef.name in ("__init__", "__init_subclass__")
                    and not isinstance(
                        get_proper_type(typ.ret_type), (NoneType, UninhabitedType)
                    )
                    and not self.dynamic_funcs[-1]
                ):
                    self.fail(
                        message_registry.MUST_HAVE_NONE_RETURN_TYPE.format(fdef.name), item
                    )

                # Check validity of __new__ signature
                if fdef.info and fdef.name == "__new__":
                    self.check___new___signature(fdef, typ)

                self.check_for_missing_annotations(fdef)
                if self.options.disallow_any_unimported:
                    if fdef.type and isinstance(fdef.type, CallableType):
                        ret_type = fdef.type.ret_type
                        if has_any_from_unimported_type(ret_type):
                            self.msg.unimported_type_becomes_any("Return type", ret_type, fdef)
                        for idx, arg_type in enumerate(fdef.type.arg_types):
                            if has_any_from_unimported_type(arg_type):
                                prefix = f'Argument {idx + 1} to "{fdef.name}"'
                                self.msg.unimported_type_becomes_any(prefix, arg_type, fdef)
                check_for_explicit_any(
                    fdef.type, self.options, self.is_typeshed_stub, self.msg, context=fdef
                )

            if name:  # Special method names
                if defn.info and self.is_reverse_op_method(name):
                    self.check_reverse_op_method(item, typ, name, defn)
                elif name in ("__getattr__", "__getattribute__"):
                    self.check_getattr_method(typ, defn, name)
                elif name == "__setattr__":
                    self.check_setattr_method(typ, defn)

            # Refuse contravariant return type variable
            if isinstance(typ.ret_type, TypeVarType):
                if typ.ret_type.variance == CONTRAVARIANT:
                    self.fail(
                        message_registry.RETURN_TYPE_CANNOT_BE_CONTRAVARIANT, typ.ret_type
                    )
                self.check_unbound_return_typevar(typ)

            # Check that Generator functions have the appropriate return type.
            if defn.is_generator:
                if defn.is_async_generator:
                    if not self.is_async_generator_return_type(typ.ret_type):
                        self.fail(
                            message_registry.INVALID_RETURN_TYPE_FOR_ASYNC_GENERATOR, typ
                        )
                else:
                    if not self.is_generator_return_type(typ.ret_type, defn.is_coroutine):
                        self.fail(message_registry.INVALID_RETURN_TYPE_FOR_GENERATOR, typ)

            # Fix the type if decorated with `@types.coroutine` or `@asyncio.coroutine`.
            if defn.is_awaitable_coroutine:
                # Update the return type to AwaitableGenerator.
                # (This doesn't exist in typing.py, only in typing.pyi.)
                t = typ.ret_type
                c = defn.is_coroutine
                ty = self.get_generator_yield_type(t, c)
                tc = self.get_generator_receive_type(t, c)
                if c:
                    tr = self.get_coroutine_return_type(t)
                else:
                    tr = self.get_generator_return_type(t, c)
                ret_type = self.named_generic_type(
                    "typing.AwaitableGenerator", [ty, tc, tr, t]
                )
                typ = typ.copy_modified(ret_type=ret_type)
                defn.type = typ

            # Push return type.
            self.return_types.append(typ.ret_type)

            # Store argument types.
            for i in range(len(typ.arg_types)):
                arg_type = typ.arg_types[i]
                with self.scope.push_function(defn):
                    # We temporary push the definition to get the self type as
                    # visible from *inside* of this function/method.
                    ref_type: Type | None = self.scope.active_self_type()
                if (
                    isinstance(defn, FuncDef)
                    and ref_type is not None
                    and i == 0
                    and not defn.is_static
                    and typ.arg_kinds[0] not in [nodes.ARG_STAR, nodes.ARG_STAR2]
                ):
                    isclass = defn.is_class or defn.name in ("__new__", "__init_subclass__")
                    if isclass:
                        ref_type = mypy.types.TypeType.make_normalized(ref_type)
                    erased = get_proper_type(erase_to_bound(arg_type))
                    if not is_subtype(ref_type, erased, ignore_type_params=True):
                        if (
                            isinstance(erased, Instance)
                            and erased.type.is_protocol
                            or isinstance(erased, TypeType)
                            and isinstance(erased.item, Instance)
                            and erased.item.type.is_protocol
                        ):
                            # We allow the explicit self-type to be not a supertype of
                            # the current class if it is a protocol. For such cases
                            # the consistency check will be performed at call sites.
                            msg = None
                        elif typ.arg_names[i] in {"self", "cls"}:
                            msg = message_registry.ERASED_SELF_TYPE_NOT_SUPERTYPE.format(
                                erased, ref_type
                            )
                        else:
                            msg = message_registry.MISSING_OR_INVALID_SELF_TYPE
                        if msg:
                            self.fail(msg, defn)
                elif isinstance(arg_type, TypeVarType):
                    # Refuse covariant parameter type variables
                    # TODO: check recursively for inner type variables
                    if arg_type.variance == COVARIANT and defn.name not in (
                        "__init__",
                        "__new__",
                    ):
                        ctx: Context = arg_type
                        if ctx.line &lt; 0:
                            ctx = typ
                        self.fail(message_registry.FUNCTION_PARAMETER_CANNOT_BE_COVARIANT, ctx)
                if typ.arg_kinds[i] == nodes.ARG_STAR:
                    if not isinstance(arg_type, ParamSpecType):
                        # builtins.tuple[T] is typing.Tuple[T, ...]
                        arg_type = self.named_generic_type("builtins.tuple", [arg_type])
                elif typ.arg_kinds[i] == nodes.ARG_STAR2:
                    if not isinstance(arg_type, ParamSpecType) and not typ.unpack_kwargs:
                        arg_type = self.named_generic_type(
                            "builtins.dict", [self.str_type(), arg_type]
                        )
                item.arguments[i].variable.type = arg_type

            # Type check initialization expressions.
            body_is_trivial = is_trivial_body(defn.body)
            self.check_default_args(item, body_is_trivial)

        # Type check body in a new scope.
        with self.binder.top_frame_context():
            with self.scope.push_function(defn):
                # We suppress reachability warnings when we use TypeVars with value
                # restrictions: we only want to report a warning if a certain statement is
                # marked as being suppressed in *all* of the expansions, but we currently
                # have no good way of doing this.
                #
                # TODO: Find a way of working around this limitation
                if len(expanded) &gt;= 2:
                    self.binder.suppress_unreachable_warnings()
                self.accept(item.body)
            unreachable = self.binder.is_unreachable()

        if not unreachable:
            if defn.is_generator or is_named_instance(
                self.return_types[-1], "typing.AwaitableGenerator"
            ):
                return_type = self.get_generator_return_type(
                    self.return_types[-1], defn.is_coroutine
                )
            elif defn.is_coroutine:
                return_type = self.get_coroutine_return_type(self.return_types[-1])
            else:
                return_type = self.return_types[-1]
            return_type = get_proper_type(return_type)

            allow_empty = allow_empty or self.options.allow_empty_bodies

            show_error = (
                not body_is_trivial
                or
                # Allow empty bodies for abstract methods, overloads, in tests and stubs.
                (
                    not allow_empty
                    and not (
                        isinstance(defn, FuncDef) and defn.abstract_status != NOT_ABSTRACT
                    )
                    and not self.is_stub
                )
            )

            # Ignore plugin generated methods, these usually don't need any bodies.
            if defn.info is not FUNC_NO_INFO and (
                defn.name not in defn.info.names or defn.info.names[defn.name].plugin_generated
            ):
                show_error = False

            # Ignore also definitions that appear in `if TYPE_CHECKING: ...` blocks.
            # These can't be called at runtime anyway (similar to plugin-generated).
            if isinstance(defn, FuncDef) and defn.is_mypy_only:
                show_error = False

            # We want to minimize the fallout from checking empty bodies
            # that was absent in many mypy versions.
            if body_is_trivial and is_subtype(NoneType(), return_type):
                show_error = False

            may_be_abstract = (
                body_is_trivial
                and defn.info is not FUNC_NO_INFO
                and defn.info.metaclass_type is not None
                and defn.info.metaclass_type.type.has_base("abc.ABCMeta")
            )

            if self.options.warn_no_return:
                if (
                    not self.current_node_deferred
                    and not isinstance(return_type, (NoneType, AnyType))
                    and show_error
                ):
                    # Control flow fell off the end of a function that was
                    # declared to return a non-None type.
                    if isinstance(return_type, UninhabitedType):
                        # This is a NoReturn function
                        msg = message_registry.INVALID_IMPLICIT_RETURN
                    else:
                        msg = message_registry.MISSING_RETURN_STATEMENT
                    if body_is_trivial:
                        msg = msg._replace(code=codes.EMPTY_BODY)
                    self.fail(msg, defn)
                    if may_be_abstract:
                        self.note(message_registry.EMPTY_BODY_ABSTRACT, defn)
            elif show_error:
                msg = message_registry.INCOMPATIBLE_RETURN_VALUE_TYPE
                if body_is_trivial:
                    msg = msg._replace(code=codes.EMPTY_BODY)
                # similar to code in check_return_stmt
                if (
                    not self.check_subtype(
                        subtype_label="implicitly returns",
                        subtype=NoneType(),
                        supertype_label="expected",
                        supertype=return_type,
                        context=defn,
                        msg=msg,
                    )
                    and may_be_abstract
                ):
                    self.note(message_registry.EMPTY_BODY_ABSTRACT, defn)

        self.return_types.pop()

        self.binder = old_binder

</t>
<t tx="ekr.20221004064034.358">def check_unbound_return_typevar(self, typ: CallableType) -&gt; None:
    """Fails when the return typevar is not defined in arguments."""
    if isinstance(typ.ret_type, TypeVarType) and typ.ret_type in typ.variables:
        arg_type_visitor = CollectArgTypeVarTypes()
        for argtype in typ.arg_types:
            argtype.accept(arg_type_visitor)

        if typ.ret_type not in arg_type_visitor.arg_types:
            self.fail(message_registry.UNBOUND_TYPEVAR, typ.ret_type, code=TYPE_VAR)
            upper_bound = get_proper_type(typ.ret_type.upper_bound)
            if not (
                isinstance(upper_bound, Instance)
                and upper_bound.type.fullname == "builtins.object"
            ):
                self.note(
                    "Consider using the upper bound "
                    f"{format_type(typ.ret_type.upper_bound)} instead",
                    context=typ.ret_type,
                )

</t>
<t tx="ekr.20221004064034.359">def check_default_args(self, item: FuncItem, body_is_trivial: bool) -&gt; None:
    for arg in item.arguments:
        if arg.initializer is None:
            continue
        if body_is_trivial and isinstance(arg.initializer, EllipsisExpr):
            continue
        name = arg.variable.name
        msg = "Incompatible default for "
        if name.startswith("__tuple_arg_"):
            msg += f"tuple argument {name[12:]}"
        else:
            msg += f'argument "{name}"'
        if (
            not self.options.implicit_optional
            and isinstance(arg.initializer, NameExpr)
            and arg.initializer.fullname == "builtins.None"
        ):
            notes = [
                "PEP 484 prohibits implicit Optional. "
                "Accordingly, mypy has changed its default to no_implicit_optional=True",
                "Use https://github.com/hauntsaninja/no_implicit_optional to automatically "
                "upgrade your codebase",
            ]
        else:
            notes = None
        self.check_simple_assignment(
            arg.variable.type,
            arg.initializer,
            context=arg.initializer,
            msg=ErrorMessage(msg, code=codes.ASSIGNMENT),
            lvalue_name="argument",
            rvalue_name="default",
            notes=notes,
        )

</t>
<t tx="ekr.20221004064034.36">def report_most_common(chunks: list[JsonDict], amount: int | None = None) -&gt; None:
    report_counter(Counter(str(chunk) for chunk in chunks), amount)


</t>
<t tx="ekr.20221004064034.360">def is_forward_op_method(self, method_name: str) -&gt; bool:
    return method_name in operators.reverse_op_methods

</t>
<t tx="ekr.20221004064034.361">def is_reverse_op_method(self, method_name: str) -&gt; bool:
    return method_name in operators.reverse_op_method_set

</t>
<t tx="ekr.20221004064034.362">def check_for_missing_annotations(self, fdef: FuncItem) -&gt; None:
    # Check for functions with unspecified/not fully specified types.
    def is_unannotated_any(t: Type) -&gt; bool:
        if not isinstance(t, ProperType):
            return False
        return isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated

    has_explicit_annotation = isinstance(fdef.type, CallableType) and any(
        not is_unannotated_any(t) for t in fdef.type.arg_types + [fdef.type.ret_type]
    )

    show_untyped = not self.is_typeshed_stub or self.options.warn_incomplete_stub
    check_incomplete_defs = self.options.disallow_incomplete_defs and has_explicit_annotation
    if show_untyped and (self.options.disallow_untyped_defs or check_incomplete_defs):
        if fdef.type is None and self.options.disallow_untyped_defs:
            if not fdef.arguments or (
                len(fdef.arguments) == 1
                and (fdef.arg_names[0] == "self" or fdef.arg_names[0] == "cls")
            ):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
                if not has_return_statement(fdef) and not fdef.is_generator:
                    self.note(
                        'Use "-&gt; None" if function does not return a value',
                        fdef,
                        code=codes.NO_UNTYPED_DEF,
                    )
            else:
                self.fail(message_registry.FUNCTION_TYPE_EXPECTED, fdef)
        elif isinstance(fdef.type, CallableType):
            ret_type = get_proper_type(fdef.type.ret_type)
            if is_unannotated_any(ret_type):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_generator:
                if is_unannotated_any(
                    self.get_generator_return_type(ret_type, fdef.is_coroutine)
                ):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_coroutine and isinstance(ret_type, Instance):
                if is_unannotated_any(self.get_coroutine_return_type(ret_type)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            if any(is_unannotated_any(t) for t in fdef.type.arg_types):
                self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)

</t>
<t tx="ekr.20221004064034.363">def check___new___signature(self, fdef: FuncDef, typ: CallableType) -&gt; None:
    self_type = fill_typevars_with_any(fdef.info)
    bound_type = bind_self(typ, self_type, is_classmethod=True)
    # Check that __new__ (after binding cls) returns an instance
    # type (or any).
    if fdef.info.is_metaclass():
        # This is a metaclass, so it must return a new unrelated type.
        self.check_subtype(
            bound_type.ret_type,
            self.type_type(),
            fdef,
            message_registry.INVALID_NEW_TYPE,
            "returns",
            "but must return a subtype of",
        )
    elif not isinstance(
        get_proper_type(bound_type.ret_type), (AnyType, Instance, TupleType, UninhabitedType)
    ):
        self.fail(
            message_registry.NON_INSTANCE_NEW_TYPE.format(format_type(bound_type.ret_type)),
            fdef,
        )
    else:
        # And that it returns a subtype of the class
        self.check_subtype(
            bound_type.ret_type,
            self_type,
            fdef,
            message_registry.INVALID_NEW_TYPE,
            "returns",
            "but must return a subtype of",
        )

</t>
<t tx="ekr.20221004064034.364">def check_reverse_op_method(
    self, defn: FuncItem, reverse_type: CallableType, reverse_name: str, context: Context
) -&gt; None:
    """Check a reverse operator method such as __radd__."""
    # Decides whether it's worth calling check_overlapping_op_methods().

    # This used to check for some very obscure scenario.  It now
    # just decides whether it's worth calling
    # check_overlapping_op_methods().

    assert defn.info

    # First check for a valid signature
    method_type = CallableType(
        [AnyType(TypeOfAny.special_form), AnyType(TypeOfAny.special_form)],
        [nodes.ARG_POS, nodes.ARG_POS],
        [None, None],
        AnyType(TypeOfAny.special_form),
        self.named_type("builtins.function"),
    )
    if not is_subtype(reverse_type, method_type):
        self.msg.invalid_signature(reverse_type, context)
        return

    if reverse_name in ("__eq__", "__ne__"):
        # These are defined for all objects =&gt; can't cause trouble.
        return

    # With 'Any' or 'object' return type we are happy, since any possible
    # return value is valid.
    ret_type = get_proper_type(reverse_type.ret_type)
    if isinstance(ret_type, AnyType):
        return
    if isinstance(ret_type, Instance):
        if ret_type.type.fullname == "builtins.object":
            return
    if reverse_type.arg_kinds[0] == ARG_STAR:
        reverse_type = reverse_type.copy_modified(
            arg_types=[reverse_type.arg_types[0]] * 2,
            arg_kinds=[ARG_POS] * 2,
            arg_names=[reverse_type.arg_names[0], "_"],
        )
    assert len(reverse_type.arg_types) &gt;= 2

    forward_name = operators.normal_from_reverse_op[reverse_name]
    forward_inst = get_proper_type(reverse_type.arg_types[1])
    if isinstance(forward_inst, TypeVarType):
        forward_inst = get_proper_type(forward_inst.upper_bound)
    elif isinstance(forward_inst, TupleType):
        forward_inst = tuple_fallback(forward_inst)
    elif isinstance(forward_inst, (FunctionLike, TypedDictType, LiteralType)):
        forward_inst = forward_inst.fallback
    if isinstance(forward_inst, TypeType):
        item = forward_inst.item
        if isinstance(item, Instance):
            opt_meta = item.type.metaclass_type
            if opt_meta is not None:
                forward_inst = opt_meta
    if not (
        isinstance(forward_inst, (Instance, UnionType))
        and forward_inst.has_readable_member(forward_name)
    ):
        return
    forward_base = reverse_type.arg_types[1]
    forward_type = self.expr_checker.analyze_external_member_access(
        forward_name, forward_base, context=defn
    )
    self.check_overlapping_op_methods(
        reverse_type,
        reverse_name,
        defn.info,
        forward_type,
        forward_name,
        forward_base,
        context=defn,
    )

</t>
<t tx="ekr.20221004064034.365">def check_overlapping_op_methods(
    self,
    reverse_type: CallableType,
    reverse_name: str,
    reverse_class: TypeInfo,
    forward_type: Type,
    forward_name: str,
    forward_base: Type,
    context: Context,
) -&gt; None:
    """Check for overlapping method and reverse method signatures.

    This function assumes that:

    -   The reverse method has valid argument count and kinds.
    -   If the reverse operator method accepts some argument of type
        X, the forward operator method also belong to class X.

        For example, if we have the reverse operator `A.__radd__(B)`, then the
        corresponding forward operator must have the type `B.__add__(...)`.
    """

    # Note: Suppose we have two operator methods "A.__rOP__(B) -&gt; R1" and
    # "B.__OP__(C) -&gt; R2". We check if these two methods are unsafely overlapping
    # by using the following algorithm:
    #
    # 1. Rewrite "B.__OP__(C) -&gt; R1"  to "temp1(B, C) -&gt; R1"
    #
    # 2. Rewrite "A.__rOP__(B) -&gt; R2" to "temp2(B, A) -&gt; R2"
    #
    # 3. Treat temp1 and temp2 as if they were both variants in the same
    #    overloaded function. (This mirrors how the Python runtime calls
    #    operator methods: we first try __OP__, then __rOP__.)
    #
    #    If the first signature is unsafely overlapping with the second,
    #    report an error.
    #
    # 4. However, if temp1 shadows temp2 (e.g. the __rOP__ method can never
    #    be called), do NOT report an error.
    #
    #    This behavior deviates from how we handle overloads -- many of the
    #    modules in typeshed seem to define __OP__ methods that shadow the
    #    corresponding __rOP__ method.
    #
    # Note: we do not attempt to handle unsafe overlaps related to multiple
    # inheritance. (This is consistent with how we handle overloads: we also
    # do not try checking unsafe overlaps due to multiple inheritance there.)

    for forward_item in flatten_nested_unions([forward_type]):
        forward_item = get_proper_type(forward_item)
        if isinstance(forward_item, CallableType):
            if self.is_unsafe_overlapping_op(forward_item, forward_base, reverse_type):
                self.msg.operator_method_signatures_overlap(
                    reverse_class, reverse_name, forward_base, forward_name, context
                )
        elif isinstance(forward_item, Overloaded):
            for item in forward_item.items:
                if self.is_unsafe_overlapping_op(item, forward_base, reverse_type):
                    self.msg.operator_method_signatures_overlap(
                        reverse_class, reverse_name, forward_base, forward_name, context
                    )
        elif not isinstance(forward_item, AnyType):
            self.msg.forward_operator_not_callable(forward_name, context)

</t>
<t tx="ekr.20221004064034.366">def is_unsafe_overlapping_op(
    self, forward_item: CallableType, forward_base: Type, reverse_type: CallableType
) -&gt; bool:
    # TODO: check argument kinds?
    if len(forward_item.arg_types) &lt; 1:
        # Not a valid operator method -- can't succeed anyway.
        return False

    # Erase the type if necessary to make sure we don't have a single
    # TypeVar in forward_tweaked. (Having a function signature containing
    # just a single TypeVar can lead to unpredictable behavior.)
    forward_base_erased = forward_base
    if isinstance(forward_base, TypeVarType):
        forward_base_erased = erase_to_bound(forward_base)

    # Construct normalized function signatures corresponding to the
    # operator methods. The first argument is the left operand and the
    # second operand is the right argument -- we switch the order of
    # the arguments of the reverse method.

    forward_tweaked = forward_item.copy_modified(
        arg_types=[forward_base_erased, forward_item.arg_types[0]],
        arg_kinds=[nodes.ARG_POS] * 2,
        arg_names=[None] * 2,
    )
    reverse_tweaked = reverse_type.copy_modified(
        arg_types=[reverse_type.arg_types[1], reverse_type.arg_types[0]],
        arg_kinds=[nodes.ARG_POS] * 2,
        arg_names=[None] * 2,
    )

    reverse_base_erased = reverse_type.arg_types[0]
    if isinstance(reverse_base_erased, TypeVarType):
        reverse_base_erased = erase_to_bound(reverse_base_erased)

    if is_same_type(reverse_base_erased, forward_base_erased):
        return False
    elif is_subtype(reverse_base_erased, forward_base_erased):
        first = reverse_tweaked
        second = forward_tweaked
    else:
        first = forward_tweaked
        second = reverse_tweaked

    return is_unsafe_overlapping_overload_signatures(first, second)

</t>
<t tx="ekr.20221004064034.367">def check_inplace_operator_method(self, defn: FuncBase) -&gt; None:
    """Check an inplace operator method such as __iadd__.

    They cannot arbitrarily overlap with __add__.
    """
    method = defn.name
    if method not in operators.inplace_operator_methods:
        return
    typ = bind_self(self.function_type(defn))
    cls = defn.info
    other_method = "__" + method[3:]
    if cls.has_readable_member(other_method):
        instance = fill_typevars(cls)
        typ2 = get_proper_type(
            self.expr_checker.analyze_external_member_access(other_method, instance, defn)
        )
        fail = False
        if isinstance(typ2, FunctionLike):
            if not is_more_general_arg_prefix(typ, typ2):
                fail = True
        else:
            # TODO overloads
            fail = True
        if fail:
            self.msg.signatures_incompatible(method, other_method, defn)

</t>
<t tx="ekr.20221004064034.368">def check_getattr_method(self, typ: Type, context: Context, name: str) -&gt; None:
    if len(self.scope.stack) == 1:
        # module scope
        if name == "__getattribute__":
            self.fail(message_registry.MODULE_LEVEL_GETATTRIBUTE, context)
            return
        # __getattr__ is fine at the module level as of Python 3.7 (PEP 562). We could
        # show an error for Python &lt; 3.7, but that would be annoying in code that supports
        # both 3.7 and older versions.
        method_type = CallableType(
            [self.named_type("builtins.str")],
            [nodes.ARG_POS],
            [None],
            AnyType(TypeOfAny.special_form),
            self.named_type("builtins.function"),
        )
    elif self.scope.active_class():
        method_type = CallableType(
            [AnyType(TypeOfAny.special_form), self.named_type("builtins.str")],
            [nodes.ARG_POS, nodes.ARG_POS],
            [None, None],
            AnyType(TypeOfAny.special_form),
            self.named_type("builtins.function"),
        )
    else:
        return
    if not is_subtype(typ, method_type):
        self.msg.invalid_signature_for_special_method(typ, context, name)

</t>
<t tx="ekr.20221004064034.369">def check_setattr_method(self, typ: Type, context: Context) -&gt; None:
    if not self.scope.active_class():
        return
    method_type = CallableType(
        [
            AnyType(TypeOfAny.special_form),
            self.named_type("builtins.str"),
            AnyType(TypeOfAny.special_form),
        ],
        [nodes.ARG_POS, nodes.ARG_POS, nodes.ARG_POS],
        [None, None, None],
        NoneType(),
        self.named_type("builtins.function"),
    )
    if not is_subtype(typ, method_type):
        self.msg.invalid_signature_for_special_method(typ, context, "__setattr__")

</t>
<t tx="ekr.20221004064034.37">def compress(chunk: JsonDict) -&gt; JsonDict:
    cache: dict[int, JsonDict] = {}
    counter = 0

    @others
    out = helper(chunk)
    return out


</t>
<t tx="ekr.20221004064034.370">def check_slots_definition(self, typ: Type, context: Context) -&gt; None:
    """Check the type of __slots__."""
    str_type = self.named_type("builtins.str")
    expected_type = UnionType(
        [str_type, self.named_generic_type("typing.Iterable", [str_type])]
    )
    self.check_subtype(
        typ,
        expected_type,
        context,
        message_registry.INVALID_TYPE_FOR_SLOTS,
        "actual type",
        "expected type",
        code=codes.ASSIGNMENT,
    )

</t>
<t tx="ekr.20221004064034.371">def check_match_args(self, var: Var, typ: Type, context: Context) -&gt; None:
    """Check that __match_args__ contains literal strings"""
    if not self.scope.active_class():
        return
    typ = get_proper_type(typ)
    if not isinstance(typ, TupleType) or not all(
        [is_string_literal(item) for item in typ.items]
    ):
        self.msg.note(
            "__match_args__ must be a tuple containing string literals for checking "
            "of match statements to work",
            context,
            code=codes.LITERAL_REQ,
        )

</t>
<t tx="ekr.20221004064034.372">def expand_typevars(
    self, defn: FuncItem, typ: CallableType
) -&gt; list[tuple[FuncItem, CallableType]]:
    # TODO use generator
    subst: list[list[tuple[TypeVarId, Type]]] = []
    tvars = list(typ.variables) or []
    if defn.info:
        # Class type variables
        tvars += defn.info.defn.type_vars or []
    # TODO(PEP612): audit for paramspec
    for tvar in tvars:
        if isinstance(tvar, TypeVarType) and tvar.values:
            subst.append([(tvar.id, value) for value in tvar.values])
    # Make a copy of the function to check for each combination of
    # value restricted type variables. (Except when running mypyc,
    # where we need one canonical version of the function.)
    if subst and not (self.options.mypyc or self.options.inspections):
        result: list[tuple[FuncItem, CallableType]] = []
        for substitutions in itertools.product(*subst):
            mapping = dict(substitutions)
            expanded = cast(CallableType, expand_type(typ, mapping))
            result.append((expand_func(defn, mapping), expanded))
        return result
    else:
        return [(defn, typ)]

</t>
<t tx="ekr.20221004064034.373">def check_method_override(self, defn: FuncDef | OverloadedFuncDef | Decorator) -&gt; None:
    """Check if function definition is compatible with base classes.

    This may defer the method if a signature is not available in at least one base class.
    """
    # Check against definitions in base classes.
    for base in defn.info.mro[1:]:
        if self.check_method_or_accessor_override_for_base(defn, base):
            # Node was deferred, we will have another attempt later.
            return

</t>
<t tx="ekr.20221004064034.374">def check_method_or_accessor_override_for_base(
    self, defn: FuncDef | OverloadedFuncDef | Decorator, base: TypeInfo
) -&gt; bool:
    """Check if method definition is compatible with a base class.

    Return True if the node was deferred because one of the corresponding
    superclass nodes is not ready.
    """
    if base:
        name = defn.name
        base_attr = base.names.get(name)
        if base_attr:
            # First, check if we override a final (always an error, even with Any types).
            if is_final_node(base_attr.node):
                self.msg.cant_override_final(name, base.name, defn)
            # Second, final can't override anything writeable independently of types.
            if defn.is_final:
                self.check_if_final_var_override_writable(name, base_attr.node, defn)

        # Check the type of override.
        if name not in ("__init__", "__new__", "__init_subclass__"):
            # Check method override
            # (__init__, __new__, __init_subclass__ are special).
            if self.check_method_override_for_base_with_name(defn, name, base):
                return True
            if name in operators.inplace_operator_methods:
                # Figure out the name of the corresponding operator method.
                method = "__" + name[3:]
                # An inplace operator method such as __iadd__ might not be
                # always introduced safely if a base class defined __add__.
                # TODO can't come up with an example where this is
                #      necessary; now it's "just in case"
                return self.check_method_override_for_base_with_name(defn, method, base)
    return False

</t>
<t tx="ekr.20221004064034.375">def check_method_override_for_base_with_name(
    self, defn: FuncDef | OverloadedFuncDef | Decorator, name: str, base: TypeInfo
) -&gt; bool:
    """Check if overriding an attribute `name` of `base` with `defn` is valid.

    Return True if the supertype node was not analysed yet, and `defn` was deferred.
    """
    base_attr = base.names.get(name)
    if base_attr:
        # The name of the method is defined in the base class.

        # Point errors at the 'def' line (important for backward compatibility
        # of type ignores).
        if not isinstance(defn, Decorator):
            context = defn
        else:
            context = defn.func

        # Construct the type of the overriding method.
        # TODO: this logic is much less complete than similar one in checkmember.py
        if isinstance(defn, (FuncDef, OverloadedFuncDef)):
            typ: Type = self.function_type(defn)
            override_class_or_static = defn.is_class or defn.is_static
            override_class = defn.is_class
        else:
            assert defn.var.is_ready
            assert defn.var.type is not None
            typ = defn.var.type
            override_class_or_static = defn.func.is_class or defn.func.is_static
            override_class = defn.func.is_class
        typ = get_proper_type(typ)
        if isinstance(typ, FunctionLike) and not is_static(context):
            typ = bind_self(typ, self.scope.active_self_type(), is_classmethod=override_class)
        # Map the overridden method type to subtype context so that
        # it can be checked for compatibility.
        original_type = get_proper_type(base_attr.type)
        original_node = base_attr.node
        # `original_type` can be partial if (e.g.) it is originally an
        # instance variable from an `__init__` block that becomes deferred.
        if original_type is None or isinstance(original_type, PartialType):
            if self.pass_num &lt; self.last_pass:
                # If there are passes left, defer this node until next pass,
                # otherwise try reconstructing the method type from available information.
                self.defer_node(defn, defn.info)
                return True
            elif isinstance(original_node, (FuncDef, OverloadedFuncDef)):
                original_type = self.function_type(original_node)
            elif isinstance(original_node, Decorator):
                original_type = self.function_type(original_node.func)
            elif isinstance(original_node, Var):
                # Super type can define method as an attribute.
                # See https://github.com/python/mypy/issues/10134

                # We also check that sometimes `original_node.type` is None.
                # This is the case when we use something like `__hash__ = None`.
                if original_node.type is not None:
                    original_type = get_proper_type(original_node.type)
                else:
                    original_type = NoneType()
            else:
                # Will always fail to typecheck below, since we know the node is a method
                original_type = NoneType()
        if isinstance(original_node, (FuncDef, OverloadedFuncDef)):
            original_class_or_static = original_node.is_class or original_node.is_static
        elif isinstance(original_node, Decorator):
            fdef = original_node.func
            original_class_or_static = fdef.is_class or fdef.is_static
        else:
            original_class_or_static = False  # a variable can't be class or static

        if isinstance(original_type, FunctionLike):
            original_type = self.bind_and_map_method(base_attr, original_type, defn.info, base)
            if original_node and is_property(original_node):
                original_type = get_property_type(original_type)

        if isinstance(typ, FunctionLike) and is_property(defn):
            typ = get_property_type(typ)
            if (
                isinstance(original_node, Var)
                and not original_node.is_final
                and (not original_node.is_property or original_node.is_settable_property)
                and isinstance(defn, Decorator)
            ):
                # We only give an error where no other similar errors will be given.
                if not isinstance(original_type, AnyType):
                    self.msg.fail(
                        "Cannot override writeable attribute with read-only property",
                        # Give an error on function line to match old behaviour.
                        defn.func,
                        code=codes.OVERRIDE,
                    )

        if isinstance(original_type, AnyType) or isinstance(typ, AnyType):
            pass
        elif isinstance(original_type, FunctionLike) and isinstance(typ, FunctionLike):
            # Check that the types are compatible.
            # TODO overloaded signatures
            self.check_override(
                typ,
                original_type,
                defn.name,
                name,
                base.name,
                original_class_or_static,
                override_class_or_static,
                context,
            )
        elif is_equivalent(original_type, typ):
            # Assume invariance for a non-callable attribute here. Note
            # that this doesn't affect read-only properties which can have
            # covariant overrides.
            #
            pass
        elif (
            original_node
            and not self.is_writable_attribute(original_node)
            and is_subtype(typ, original_type)
        ):
            # If the attribute is read-only, allow covariance
            pass
        else:
            self.msg.signature_incompatible_with_supertype(defn.name, name, base.name, context)
    return False

</t>
<t tx="ekr.20221004064034.376">def bind_and_map_method(
    self, sym: SymbolTableNode, typ: FunctionLike, sub_info: TypeInfo, super_info: TypeInfo
) -&gt; FunctionLike:
    """Bind self-type and map type variables for a method.

    Arguments:
        sym: a symbol that points to method definition
        typ: method type on the definition
        sub_info: class where the method is used
        super_info: class where the method was defined
    """
    if isinstance(sym.node, (FuncDef, OverloadedFuncDef, Decorator)) and not is_static(
        sym.node
    ):
        if isinstance(sym.node, Decorator):
            is_class_method = sym.node.func.is_class
        else:
            is_class_method = sym.node.is_class
        bound = bind_self(typ, self.scope.active_self_type(), is_class_method)
    else:
        bound = typ
    return cast(FunctionLike, map_type_from_supertype(bound, sub_info, super_info))

</t>
<t tx="ekr.20221004064034.377">def get_op_other_domain(self, tp: FunctionLike) -&gt; Type | None:
    if isinstance(tp, CallableType):
        if tp.arg_kinds and tp.arg_kinds[0] == ARG_POS:
            return tp.arg_types[0]
        return None
    elif isinstance(tp, Overloaded):
        raw_items = [self.get_op_other_domain(it) for it in tp.items]
        items = [it for it in raw_items if it]
        if items:
            return make_simplified_union(items)
        return None
    else:
        assert False, "Need to check all FunctionLike subtypes here"

</t>
<t tx="ekr.20221004064034.378">def check_override(
    self,
    override: FunctionLike,
    original: FunctionLike,
    name: str,
    name_in_super: str,
    supertype: str,
    original_class_or_static: bool,
    override_class_or_static: bool,
    node: Context,
) -&gt; None:
    """Check a method override with given signatures.

    Arguments:
      override:  The signature of the overriding method.
      original:  The signature of the original supertype method.
      name:      The name of the subtype. This and the next argument are
                 only used for generating error messages.
      supertype: The name of the supertype.
    """
    # Use boolean variable to clarify code.
    fail = False
    op_method_wider_note = False
    if not is_subtype(override, original, ignore_pos_arg_names=True):
        fail = True
    elif isinstance(override, Overloaded) and self.is_forward_op_method(name):
        # Operator method overrides cannot extend the domain, as
        # this could be unsafe with reverse operator methods.
        original_domain = self.get_op_other_domain(original)
        override_domain = self.get_op_other_domain(override)
        if (
            original_domain
            and override_domain
            and not is_subtype(override_domain, original_domain)
        ):
            fail = True
            op_method_wider_note = True
    if isinstance(override, FunctionLike):
        if original_class_or_static and not override_class_or_static:
            fail = True
        elif isinstance(original, CallableType) and isinstance(override, CallableType):
            if original.type_guard is not None and override.type_guard is None:
                fail = True

    if is_private(name):
        fail = False

    if fail:
        emitted_msg = False

        # Normalize signatures, so we get better diagnostics.
        if isinstance(override, (CallableType, Overloaded)):
            override = override.with_unpacked_kwargs()
        if isinstance(original, (CallableType, Overloaded)):
            original = original.with_unpacked_kwargs()

        if (
            isinstance(override, CallableType)
            and isinstance(original, CallableType)
            and len(override.arg_types) == len(original.arg_types)
            and override.min_args == original.min_args
        ):
            # Give more detailed messages for the common case of both
            # signatures having the same number of arguments and no
            # overloads.

            # override might have its own generic function type
            # variables. If an argument or return type of override
            # does not have the correct subtyping relationship
            # with the original type even after these variables
            # are erased, then it is definitely an incompatibility.

            override_ids = override.type_var_ids()
            type_name = None
            if isinstance(override.definition, FuncDef):
                type_name = override.definition.info.name

            def erase_override(t: Type) -&gt; Type:
                return erase_typevars(t, ids_to_erase=override_ids)

            for i in range(len(override.arg_types)):
                if not is_subtype(
                    original.arg_types[i], erase_override(override.arg_types[i])
                ):
                    arg_type_in_super = original.arg_types[i]
                    self.msg.argument_incompatible_with_supertype(
                        i + 1,
                        name,
                        type_name,
                        name_in_super,
                        arg_type_in_super,
                        supertype,
                        node,
                    )
                    emitted_msg = True

            if not is_subtype(erase_override(override.ret_type), original.ret_type):
                self.msg.return_type_incompatible_with_supertype(
                    name, name_in_super, supertype, original.ret_type, override.ret_type, node
                )
                emitted_msg = True
        elif isinstance(override, Overloaded) and isinstance(original, Overloaded):
            # Give a more detailed message in the case where the user is trying to
            # override an overload, and the subclass's overload is plausible, except
            # that the order of the variants are wrong.
            #
            # For example, if the parent defines the overload f(int) -&gt; int and f(str) -&gt; str
            # (in that order), and if the child swaps the two and does f(str) -&gt; str and
            # f(int) -&gt; int
            order = []
            for child_variant in override.items:
                for i, parent_variant in enumerate(original.items):
                    if is_subtype(child_variant, parent_variant):
                        order.append(i)
                        break

            if len(order) == len(original.items) and order != sorted(order):
                self.msg.overload_signature_incompatible_with_supertype(
                    name, name_in_super, supertype, node
                )
                emitted_msg = True

        if not emitted_msg:
            # Fall back to generic incompatibility message.
            self.msg.signature_incompatible_with_supertype(
                name, name_in_super, supertype, node, original=original, override=override
            )
        if op_method_wider_note:
            self.note(
                "Overloaded operator methods can't have wider argument types in overrides",
                node,
                code=codes.OVERRIDE,
            )

</t>
<t tx="ekr.20221004064034.379">def check__exit__return_type(self, defn: FuncItem) -&gt; None:
    """Generate error if the return type of __exit__ is problematic.

    If __exit__ always returns False but the return type is declared
    as bool, mypy thinks that a with statement may "swallow"
    exceptions even though this is not the case, resulting in
    invalid reachability inference.
    """
    if not defn.type or not isinstance(defn.type, CallableType):
        return

    ret_type = get_proper_type(defn.type.ret_type)
    if not has_bool_item(ret_type):
        return

    returns = all_return_statements(defn)
    if not returns:
        return

    if all(
        isinstance(ret.expr, NameExpr) and ret.expr.fullname == "builtins.False"
        for ret in returns
    ):
        self.msg.incorrect__exit__return(defn)

</t>
<t tx="ekr.20221004064034.38">def helper(chunk: JsonDict) -&gt; JsonDict:
    nonlocal counter
    if not isinstance(chunk, dict):
        return chunk

    if len(chunk) &lt;= 2:
        return chunk
    id = hash(str(chunk))

    if id in cache:
        return cache[id]
    else:
        cache[id] = {".id": counter}
        chunk[".cache_id"] = counter
        counter += 1

    for name in sorted(chunk.keys()):
        value = chunk[name]
        if isinstance(value, list):
            chunk[name] = [helper(child) for child in value]
        elif isinstance(value, dict):
            chunk[name] = helper(value)

    return chunk

</t>
<t tx="ekr.20221004064034.380">def visit_class_def(self, defn: ClassDef) -&gt; None:
    """Type check a class definition."""
    typ = defn.info
    for base in typ.mro[1:]:
        if base.is_final:
            self.fail(message_registry.CANNOT_INHERIT_FROM_FINAL.format(base.name), defn)
    with self.tscope.class_scope(defn.info), self.enter_partial_types(is_class=True):
        old_binder = self.binder
        self.binder = ConditionalTypeBinder()
        with self.binder.top_frame_context():
            with self.scope.push_class(defn.info):
                self.accept(defn.defs)
        self.binder = old_binder
        if not (defn.info.typeddict_type or defn.info.tuple_type or defn.info.is_enum):
            # If it is not a normal class (not a special form) check class keywords.
            self.check_init_subclass(defn)
        if not defn.has_incompatible_baseclass:
            # Otherwise we've already found errors; more errors are not useful
            self.check_multiple_inheritance(typ)
        self.check_metaclass_compatibility(typ)
        self.check_final_deletable(typ)

        if defn.decorators:
            sig: Type = type_object_type(defn.info, self.named_type)
            # Decorators are applied in reverse order.
            for decorator in reversed(defn.decorators):
                if isinstance(decorator, CallExpr) and isinstance(
                    decorator.analyzed, PromoteExpr
                ):
                    # _promote is a special type checking related construct.
                    continue

                dec = self.expr_checker.accept(decorator)
                temp = self.temp_node(sig, context=decorator)
                fullname = None
                if isinstance(decorator, RefExpr):
                    fullname = decorator.fullname

                # TODO: Figure out how to have clearer error messages.
                # (e.g. "class decorator must be a function that accepts a type."
                old_allow_abstract_call = self.allow_abstract_call
                self.allow_abstract_call = True
                sig, _ = self.expr_checker.check_call(
                    dec, [temp], [nodes.ARG_POS], defn, callable_name=fullname
                )
                self.allow_abstract_call = old_allow_abstract_call
            # TODO: Apply the sig to the actual TypeInfo so we can handle decorators
            # that completely swap out the type.  (e.g. Callable[[Type[A]], Type[B]])
    if typ.defn.type_vars:
        for base_inst in typ.bases:
            for base_tvar, base_decl_tvar in zip(
                base_inst.args, base_inst.type.defn.type_vars
            ):
                if (
                    isinstance(base_tvar, TypeVarType)
                    and base_tvar.variance != INVARIANT
                    and isinstance(base_decl_tvar, TypeVarType)
                    and base_decl_tvar.variance != base_tvar.variance
                ):
                    self.fail(
                        f'Variance of TypeVar "{base_tvar.name}" incompatible '
                        "with variance in parent type",
                        context=defn,
                        code=codes.TYPE_VAR,
                    )

    if typ.is_protocol and typ.defn.type_vars:
        self.check_protocol_variance(defn)
    if not defn.has_incompatible_baseclass and defn.info.is_enum:
        self.check_enum(defn)

</t>
<t tx="ekr.20221004064034.381">def check_final_deletable(self, typ: TypeInfo) -&gt; None:
    # These checks are only for mypyc. Only perform some checks that are easier
    # to implement here than in mypyc.
    for attr in typ.deletable_attributes:
        node = typ.names.get(attr)
        if node and isinstance(node.node, Var) and node.node.is_final:
            self.fail(message_registry.CANNOT_MAKE_DELETABLE_FINAL, node.node)

</t>
<t tx="ekr.20221004064034.382">def check_init_subclass(self, defn: ClassDef) -&gt; None:
    """Check that keywords in a class definition are valid arguments for __init_subclass__().

    In this example:
        1   class Base:
        2       def __init_subclass__(cls, thing: int):
        3           pass
        4   class Child(Base, thing=5):
        5       def __init_subclass__(cls):
        6           pass
        7   Child()

    Base.__init_subclass__(thing=5) is called at line 4. This is what we simulate here.
    Child.__init_subclass__ is never called.
    """
    if defn.info.metaclass_type and defn.info.metaclass_type.type.fullname not in (
        "builtins.type",
        "abc.ABCMeta",
    ):
        # We can't safely check situations when both __init_subclass__ and a custom
        # metaclass are present.
        return
    # At runtime, only Base.__init_subclass__ will be called, so
    # we skip the current class itself.
    for base in defn.info.mro[1:]:
        if "__init_subclass__" not in base.names:
            continue
        name_expr = NameExpr(defn.name)
        name_expr.node = base
        callee = MemberExpr(name_expr, "__init_subclass__")
        args = list(defn.keywords.values())
        arg_names: list[str | None] = list(defn.keywords.keys())
        # 'metaclass' keyword is consumed by the rest of the type machinery,
        # and is never passed to __init_subclass__ implementations
        if "metaclass" in arg_names:
            idx = arg_names.index("metaclass")
            arg_names.pop(idx)
            args.pop(idx)
        arg_kinds = [ARG_NAMED] * len(args)
        call_expr = CallExpr(callee, args, arg_kinds, arg_names)
        call_expr.line = defn.line
        call_expr.column = defn.column
        call_expr.end_line = defn.end_line
        self.expr_checker.accept(call_expr, allow_none_return=True, always_allow_any=True)
        # We are only interested in the first Base having __init_subclass__,
        # all other bases have already been checked.
        break

</t>
<t tx="ekr.20221004064034.383">def check_enum(self, defn: ClassDef) -&gt; None:
    assert defn.info.is_enum
    if defn.info.fullname not in ENUM_BASES:
        for sym in defn.info.names.values():
            if (
                isinstance(sym.node, Var)
                and sym.node.has_explicit_value
                and sym.node.name == "__members__"
            ):
                # `__members__` will always be overwritten by `Enum` and is considered
                # read-only so we disallow assigning a value to it
                self.fail(message_registry.ENUM_MEMBERS_ATTR_WILL_BE_OVERRIDEN, sym.node)
    for base in defn.info.mro[1:-1]:  # we don't need self and `object`
        if base.is_enum and base.fullname not in ENUM_BASES:
            self.check_final_enum(defn, base)

    self.check_enum_bases(defn)
    self.check_enum_new(defn)

</t>
<t tx="ekr.20221004064034.384">def check_final_enum(self, defn: ClassDef, base: TypeInfo) -&gt; None:
    for sym in base.names.values():
        if self.is_final_enum_value(sym):
            self.fail(f'Cannot extend enum with existing members: "{base.name}"', defn)
            break

</t>
<t tx="ekr.20221004064034.385">def is_final_enum_value(self, sym: SymbolTableNode) -&gt; bool:
    if isinstance(sym.node, (FuncBase, Decorator)):
        return False  # A method is fine
    if not isinstance(sym.node, Var):
        return True  # Can be a class or anything else

    # Now, only `Var` is left, we need to check:
    # 1. Private name like in `__prop = 1`
    # 2. Dunder name like `__hash__ = some_hasher`
    # 3. Sunder name like `_order_ = 'a, b, c'`
    # 4. If it is a method / descriptor like in `method = classmethod(func)`
    if (
        is_private(sym.node.name)
        or is_dunder(sym.node.name)
        or is_sunder(sym.node.name)
        # TODO: make sure that `x = @class/staticmethod(func)`
        # and `x = property(prop)` both work correctly.
        # Now they are incorrectly counted as enum members.
        or isinstance(get_proper_type(sym.node.type), FunctionLike)
    ):
        return False

    if self.is_stub or sym.node.has_explicit_value:
        return True
    return False

</t>
<t tx="ekr.20221004064034.386">def check_enum_bases(self, defn: ClassDef) -&gt; None:
    """
    Non-enum mixins cannot appear after enum bases; this is disallowed at runtime:

        class Foo: ...
        class Bar(enum.Enum, Foo): ...

    But any number of enum mixins can appear in a class definition
    (even if multiple enum bases define __new__). So this is fine:

        class Foo(enum.Enum):
            def __new__(cls, val): ...
        class Bar(enum.Enum):
            def __new__(cls, val): ...
        class Baz(int, Foo, Bar, enum.Flag): ...
    """
    enum_base: Instance | None = None
    for base in defn.info.bases:
        if enum_base is None and base.type.is_enum:
            enum_base = base
            continue
        elif enum_base is not None and not base.type.is_enum:
            self.fail(f'No non-enum mixin classes are allowed after "{enum_base}"', defn)
            break

</t>
<t tx="ekr.20221004064034.387">def check_enum_new(self, defn: ClassDef) -&gt; None:
    def has_new_method(info: TypeInfo) -&gt; bool:
        new_method = info.get("__new__")
        return bool(
            new_method
            and new_method.node
            and new_method.node.fullname != "builtins.object.__new__"
        )

    has_new = False
    for base in defn.info.bases:
        candidate = False

        if base.type.is_enum:
            # If we have an `Enum`, then we need to check all its bases.
            candidate = any(not b.is_enum and has_new_method(b) for b in base.type.mro[1:-1])
        else:
            candidate = has_new_method(base.type)

        if candidate and has_new:
            self.fail(
                "Only a single data type mixin is allowed for Enum subtypes, "
                'found extra "{}"'.format(base),
                defn,
            )
        elif candidate:
            has_new = True

</t>
<t tx="ekr.20221004064034.388">def check_protocol_variance(self, defn: ClassDef) -&gt; None:
    """Check that protocol definition is compatible with declared
    variances of type variables.

    Note that we also prohibit declaring protocol classes as invariant
    if they are actually covariant/contravariant, since this may break
    transitivity of subtyping, see PEP 544.
    """
    info = defn.info
    object_type = Instance(info.mro[-1], [])
    tvars = info.defn.type_vars
    for i, tvar in enumerate(tvars):
        up_args: list[Type] = [
            object_type if i == j else AnyType(TypeOfAny.special_form)
            for j, _ in enumerate(tvars)
        ]
        down_args: list[Type] = [
            UninhabitedType() if i == j else AnyType(TypeOfAny.special_form)
            for j, _ in enumerate(tvars)
        ]
        up, down = Instance(info, up_args), Instance(info, down_args)
        # TODO: add advanced variance checks for recursive protocols
        if is_subtype(down, up, ignore_declared_variance=True):
            expected = COVARIANT
        elif is_subtype(up, down, ignore_declared_variance=True):
            expected = CONTRAVARIANT
        else:
            expected = INVARIANT
        if isinstance(tvar, TypeVarType) and expected != tvar.variance:
            self.msg.bad_proto_variance(tvar.variance, tvar.name, expected, defn)

</t>
<t tx="ekr.20221004064034.389">def check_multiple_inheritance(self, typ: TypeInfo) -&gt; None:
    """Check for multiple inheritance related errors."""
    if len(typ.bases) &lt;= 1:
        # No multiple inheritance.
        return
    # Verify that inherited attributes are compatible.
    mro = typ.mro[1:]
    for i, base in enumerate(mro):
        # Attributes defined in both the type and base are skipped.
        # Normal checks for attribute compatibility should catch any problems elsewhere.
        non_overridden_attrs = base.names.keys() - typ.names.keys()
        for name in non_overridden_attrs:
            if is_private(name):
                continue
            for base2 in mro[i + 1 :]:
                # We only need to check compatibility of attributes from classes not
                # in a subclass relationship. For subclasses, normal (single inheritance)
                # checks suffice (these are implemented elsewhere).
                if name in base2.names and base2 not in base.mro:
                    self.check_compatibility(name, base, base2, typ)

</t>
<t tx="ekr.20221004064034.39">def decompress(chunk: JsonDict) -&gt; JsonDict:
    cache: dict[int, JsonDict] = {}

    @others
    return helper(chunk)


</t>
<t tx="ekr.20221004064034.390">def determine_type_of_member(self, sym: SymbolTableNode) -&gt; Type | None:
    if sym.type is not None:
        return sym.type
    if isinstance(sym.node, FuncBase):
        return self.function_type(sym.node)
    if isinstance(sym.node, TypeInfo):
        if sym.node.typeddict_type:
            # We special-case TypedDict, because they don't define any constructor.
            return self.expr_checker.typeddict_callable(sym.node)
        else:
            return type_object_type(sym.node, self.named_type)
    if isinstance(sym.node, TypeVarExpr):
        # Use of TypeVars is rejected in an expression/runtime context, so
        # we don't need to check supertype compatibility for them.
        return AnyType(TypeOfAny.special_form)
    if isinstance(sym.node, TypeAlias):
        with self.msg.filter_errors():
            # Suppress any errors, they will be given when analyzing the corresponding node.
            # Here we may have incorrect options and location context.
            return self.expr_checker.alias_type_in_runtime_context(sym.node, ctx=sym.node)
    # TODO: handle more node kinds here.
    return None

</t>
<t tx="ekr.20221004064034.391">def check_compatibility(
    self, name: str, base1: TypeInfo, base2: TypeInfo, ctx: TypeInfo
) -&gt; None:
    """Check if attribute name in base1 is compatible with base2 in multiple inheritance.

    Assume base1 comes before base2 in the MRO, and that base1 and base2 don't have
    a direct subclass relationship (i.e., the compatibility requirement only derives from
    multiple inheritance).

    This check verifies that a definition taken from base1 (and mapped to the current
    class ctx), is type compatible with the definition taken from base2 (also mapped), so
    that unsafe subclassing like this can be detected:
        class A(Generic[T]):
            def foo(self, x: T) -&gt; None: ...

        class B:
            def foo(self, x: str) -&gt; None: ...

        class C(B, A[int]): ...  # this is unsafe because...

        x: A[int] = C()
        x.foo  # ...runtime type is (str) -&gt; None, while static type is (int) -&gt; None
    """
    if name in ("__init__", "__new__", "__init_subclass__"):
        # __init__ and friends can be incompatible -- it's a special case.
        return
    first = base1.names[name]
    second = base2.names[name]
    first_type = get_proper_type(self.determine_type_of_member(first))
    second_type = get_proper_type(self.determine_type_of_member(second))

    if isinstance(first_type, FunctionLike) and isinstance(second_type, FunctionLike):
        if first_type.is_type_obj() and second_type.is_type_obj():
            # For class objects only check the subtype relationship of the classes,
            # since we allow incompatible overrides of '__init__'/'__new__'
            ok = is_subtype(
                left=fill_typevars_with_any(first_type.type_object()),
                right=fill_typevars_with_any(second_type.type_object()),
            )
        else:
            # First bind/map method types when necessary.
            first_sig = self.bind_and_map_method(first, first_type, ctx, base1)
            second_sig = self.bind_and_map_method(second, second_type, ctx, base2)
            ok = is_subtype(first_sig, second_sig, ignore_pos_arg_names=True)
    elif first_type and second_type:
        ok = is_equivalent(first_type, second_type)
        if not ok:
            second_node = base2[name].node
            if isinstance(second_node, Decorator) and second_node.func.is_property:
                ok = is_subtype(first_type, cast(CallableType, second_type).ret_type)
    else:
        if first_type is None:
            self.msg.cannot_determine_type_in_base(name, base1.name, ctx)
        if second_type is None:
            self.msg.cannot_determine_type_in_base(name, base2.name, ctx)
        ok = True
    # Final attributes can never be overridden, but can override
    # non-final read-only attributes.
    if is_final_node(second.node):
        self.msg.cant_override_final(name, base2.name, ctx)
    if is_final_node(first.node):
        self.check_if_final_var_override_writable(name, second.node, ctx)
    # Some attributes like __slots__ and __deletable__ are special, and the type can
    # vary across class hierarchy.
    if isinstance(second.node, Var) and second.node.allow_incompatible_override:
        ok = True
    if not ok:
        self.msg.base_class_definitions_incompatible(name, base1, base2, ctx)

</t>
<t tx="ekr.20221004064034.392">def check_metaclass_compatibility(self, typ: TypeInfo) -&gt; None:
    """Ensures that metaclasses of all parent types are compatible."""
    if (
        typ.is_metaclass()
        or typ.is_protocol
        or typ.is_named_tuple
        or typ.is_enum
        or typ.typeddict_type is not None
    ):
        return  # Reasonable exceptions from this check

    metaclasses = [
        entry.metaclass_type
        for entry in typ.mro[1:-1]
        if entry.metaclass_type
        and not is_named_instance(entry.metaclass_type, "builtins.type")
    ]
    if not metaclasses:
        return
    if typ.metaclass_type is not None and all(
        is_subtype(typ.metaclass_type, meta) for meta in metaclasses
    ):
        return
    self.fail(
        "Metaclass conflict: the metaclass of a derived class must be "
        "a (non-strict) subclass of the metaclasses of all its bases",
        typ,
    )

</t>
<t tx="ekr.20221004064034.393">def visit_import_from(self, node: ImportFrom) -&gt; None:
    self.check_import(node)

</t>
<t tx="ekr.20221004064034.394">def visit_import_all(self, node: ImportAll) -&gt; None:
    self.check_import(node)

</t>
<t tx="ekr.20221004064034.395">def visit_import(self, s: Import) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064034.396">def check_import(self, node: ImportBase) -&gt; None:
    for assign in node.assignments:
        lvalue = assign.lvalues[0]
        lvalue_type, _, __ = self.check_lvalue(lvalue)
        if lvalue_type is None:
            # TODO: This is broken.
            lvalue_type = AnyType(TypeOfAny.special_form)
        message = message_registry.INCOMPATIBLE_IMPORT_OF.format(
            cast(NameExpr, assign.rvalue).name
        )
        self.check_simple_assignment(
            lvalue_type,
            assign.rvalue,
            node,
            msg=message,
            lvalue_name="local name",
            rvalue_name="imported name",
        )

</t>
<t tx="ekr.20221004064034.397">#
# Statements
#

</t>
<t tx="ekr.20221004064034.398">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        # This block was marked as being unreachable during semantic analysis.
        # It turns out any blocks marked in this way are *intentionally* marked
        # as unreachable -- so we don't display an error.
        self.binder.unreachable()
        return
    for s in b.body:
        if self.binder.is_unreachable():
            if self.should_report_unreachable_issues() and not self.is_raising_or_empty(s):
                self.msg.unreachable_statement(s)
            break
        self.accept(s)

</t>
<t tx="ekr.20221004064034.399">def should_report_unreachable_issues(self) -&gt; bool:
    return (
        self.in_checked_function()
        and self.options.warn_unreachable
        and not self.current_node_deferred
        and not self.binder.is_unreachable_warning_suppressed()
    )

</t>
<t tx="ekr.20221004064034.4">def pytest_configure(config):
    mypy_source_root = os.path.dirname(os.path.abspath(__file__))
    if os.getcwd() != mypy_source_root:
        os.chdir(mypy_source_root)


</t>
<t tx="ekr.20221004064034.40">def helper(chunk: JsonDict) -&gt; JsonDict:
    if not isinstance(chunk, dict):
        return chunk
    if ".id" in chunk:
        return cache[chunk[".id"]]

    counter = None
    if ".cache_id" in chunk:
        counter = chunk[".cache_id"]
        del chunk[".cache_id"]

    for name in sorted(chunk.keys()):
        value = chunk[name]
        if isinstance(value, list):
            chunk[name] = [helper(child) for child in value]
        elif isinstance(value, dict):
            chunk[name] = helper(value)

    if counter is not None:
        cache[counter] = chunk

    return chunk

</t>
<t tx="ekr.20221004064034.400">def is_raising_or_empty(self, s: Statement) -&gt; bool:
    """Returns 'true' if the given statement either throws an error of some kind
    or is a no-op.

    We use this function mostly while handling the '--warn-unreachable' flag. When
    that flag is present, we normally report an error on any unreachable statement.
    But if that statement is just something like a 'pass' or a just-in-case 'assert False',
    reporting an error would be annoying.
    """
    if isinstance(s, AssertStmt) and is_false_literal(s.expr):
        return True
    elif isinstance(s, (RaiseStmt, PassStmt)):
        return True
    elif isinstance(s, ExpressionStmt):
        if isinstance(s.expr, EllipsisExpr):
            return True
        elif isinstance(s.expr, CallExpr):
            with self.expr_checker.msg.filter_errors():
                typ = get_proper_type(
                    self.expr_checker.accept(
                        s.expr, allow_none_return=True, always_allow_any=True
                    )
                )

            if isinstance(typ, UninhabitedType):
                return True
    return False

</t>
<t tx="ekr.20221004064034.401">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    """Type check an assignment statement.

    Handle all kinds of assignment statements (simple, indexed, multiple).
    """
    # Avoid type checking type aliases in stubs to avoid false
    # positives about modern type syntax available in stubs such
    # as X | Y.
    if not (s.is_alias_def and self.is_stub):
        with self.enter_final_context(s.is_final_def):
            self.check_assignment(s.lvalues[-1], s.rvalue, s.type is None, s.new_syntax)

    if s.is_alias_def:
        self.check_type_alias_rvalue(s)

    if (
        s.type is not None
        and self.options.disallow_any_unimported
        and has_any_from_unimported_type(s.type)
    ):
        if isinstance(s.lvalues[-1], TupleExpr):
            # This is a multiple assignment. Instead of figuring out which type is problematic,
            # give a generic error message.
            self.msg.unimported_type_becomes_any(
                "A type on this line", AnyType(TypeOfAny.special_form), s
            )
        else:
            self.msg.unimported_type_becomes_any("Type of variable", s.type, s)
    check_for_explicit_any(s.type, self.options, self.is_typeshed_stub, self.msg, context=s)

    if len(s.lvalues) &gt; 1:
        # Chained assignment (e.g. x = y = ...).
        # Make sure that rvalue type will not be reinferred.
        if not self.has_type(s.rvalue):
            self.expr_checker.accept(s.rvalue)
        rvalue = self.temp_node(self.lookup_type(s.rvalue), s)
        for lv in s.lvalues[:-1]:
            with self.enter_final_context(s.is_final_def):
                self.check_assignment(lv, rvalue, s.type is None)

    self.check_final(s)
    if (
        s.is_final_def
        and s.type
        and not has_no_typevars(s.type)
        and self.scope.active_class() is not None
    ):
        self.fail(message_registry.DEPENDENT_FINAL_IN_CLASS_BODY, s)

</t>
<t tx="ekr.20221004064034.402">def check_type_alias_rvalue(self, s: AssignmentStmt) -&gt; None:
    if not (self.is_stub and isinstance(s.rvalue, OpExpr) and s.rvalue.op == "|"):
        # We do this mostly for compatibility with old semantic analyzer.
        # TODO: should we get rid of this?
        alias_type = self.expr_checker.accept(s.rvalue)
    else:
        # Avoid type checking 'X | Y' in stubs, since there can be errors
        # on older Python targets.
        alias_type = AnyType(TypeOfAny.special_form)

        @others
        accept_items(s.rvalue)
    self.store_type(s.lvalues[-1], alias_type)

</t>
<t tx="ekr.20221004064034.403">def accept_items(e: Expression) -&gt; None:
    if isinstance(e, OpExpr) and e.op == "|":
        accept_items(e.left)
        accept_items(e.right)
    else:
        # Nested union types have been converted to type context
        # in semantic analysis (such as in 'list[int | str]'),
        # so we don't need to deal with them here.
        self.expr_checker.accept(e)

</t>
<t tx="ekr.20221004064034.404">def check_assignment(
    self,
    lvalue: Lvalue,
    rvalue: Expression,
    infer_lvalue_type: bool = True,
    new_syntax: bool = False,
) -&gt; None:
    """Type check a single assignment: lvalue = rvalue."""
    if isinstance(lvalue, TupleExpr) or isinstance(lvalue, ListExpr):
        self.check_assignment_to_multiple_lvalues(
            lvalue.items, rvalue, rvalue, infer_lvalue_type
        )
    else:
        self.try_infer_partial_generic_type_from_assignment(lvalue, rvalue, "=")
        lvalue_type, index_lvalue, inferred = self.check_lvalue(lvalue)
        # If we're assigning to __getattr__ or similar methods, check that the signature is
        # valid.
        if isinstance(lvalue, NameExpr) and lvalue.node:
            name = lvalue.node.name
            if name in ("__setattr__", "__getattribute__", "__getattr__"):
                # If an explicit type is given, use that.
                if lvalue_type:
                    signature = lvalue_type
                else:
                    signature = self.expr_checker.accept(rvalue)
                if signature:
                    if name == "__setattr__":
                        self.check_setattr_method(signature, lvalue)
                    else:
                        self.check_getattr_method(signature, lvalue, name)

            if name == "__slots__":
                typ = lvalue_type or self.expr_checker.accept(rvalue)
                self.check_slots_definition(typ, lvalue)
            if name == "__match_args__" and inferred is not None:
                typ = self.expr_checker.accept(rvalue)
                self.check_match_args(inferred, typ, lvalue)

        # Defer PartialType's super type checking.
        if (
            isinstance(lvalue, RefExpr)
            and not (isinstance(lvalue_type, PartialType) and lvalue_type.type is None)
            and not (isinstance(lvalue, NameExpr) and lvalue.name == "__match_args__")
        ):
            if self.check_compatibility_all_supers(lvalue, lvalue_type, rvalue):
                # We hit an error on this line; don't check for any others
                return

        if isinstance(lvalue, MemberExpr) and lvalue.name == "__match_args__":
            self.fail(message_registry.CANNOT_MODIFY_MATCH_ARGS, lvalue)

        if lvalue_type:
            if isinstance(lvalue_type, PartialType) and lvalue_type.type is None:
                # Try to infer a proper type for a variable with a partial None type.
                rvalue_type = self.expr_checker.accept(rvalue)
                if isinstance(get_proper_type(rvalue_type), NoneType):
                    # This doesn't actually provide any additional information -- multiple
                    # None initializers preserve the partial None type.
                    return

                if is_valid_inferred_type(rvalue_type):
                    var = lvalue_type.var
                    partial_types = self.find_partial_types(var)
                    if partial_types is not None:
                        if not self.current_node_deferred:
                            # Partial type can't be final, so strip any literal values.
                            rvalue_type = remove_instance_last_known_values(rvalue_type)
                            inferred_type = make_simplified_union([rvalue_type, NoneType()])
                            self.set_inferred_type(var, lvalue, inferred_type)
                        else:
                            var.type = None
                        del partial_types[var]
                        lvalue_type = var.type
                else:
                    # Try to infer a partial type. No need to check the return value, as
                    # an error will be reported elsewhere.
                    self.infer_partial_type(lvalue_type.var, lvalue, rvalue_type)
                # Handle None PartialType's super type checking here, after it's resolved.
                if isinstance(lvalue, RefExpr) and self.check_compatibility_all_supers(
                    lvalue, lvalue_type, rvalue
                ):
                    # We hit an error on this line; don't check for any others
                    return
            elif (
                is_literal_none(rvalue)
                and isinstance(lvalue, NameExpr)
                and isinstance(lvalue.node, Var)
                and lvalue.node.is_initialized_in_class
                and not new_syntax
            ):
                # Allow None's to be assigned to class variables with non-Optional types.
                rvalue_type = lvalue_type
            elif (
                isinstance(lvalue, MemberExpr) and lvalue.kind is None
            ):  # Ignore member access to modules
                instance_type = self.expr_checker.accept(lvalue.expr)
                rvalue_type, lvalue_type, infer_lvalue_type = self.check_member_assignment(
                    instance_type, lvalue_type, rvalue, context=rvalue
                )
            else:
                # Hacky special case for assigning a literal None
                # to a variable defined in a previous if
                # branch. When we detect this, we'll go back and
                # make the type optional. This is somewhat
                # unpleasant, and a generalization of this would
                # be an improvement!
                if (
                    is_literal_none(rvalue)
                    and isinstance(lvalue, NameExpr)
                    and lvalue.kind == LDEF
                    and isinstance(lvalue.node, Var)
                    and lvalue.node.type
                    and lvalue.node in self.var_decl_frames
                    and not isinstance(get_proper_type(lvalue_type), AnyType)
                ):
                    decl_frame_map = self.var_decl_frames[lvalue.node]
                    # Check if the nearest common ancestor frame for the definition site
                    # and the current site is the enclosing frame of an if/elif/else block.
                    has_if_ancestor = False
                    for frame in reversed(self.binder.frames):
                        if frame.id in decl_frame_map:
                            has_if_ancestor = frame.conditional_frame
                            break
                    if has_if_ancestor:
                        lvalue_type = make_optional_type(lvalue_type)
                        self.set_inferred_type(lvalue.node, lvalue, lvalue_type)

                rvalue_type = self.check_simple_assignment(lvalue_type, rvalue, context=rvalue)

            # Special case: only non-abstract non-protocol classes can be assigned to
            # variables with explicit type Type[A], where A is protocol or abstract.
            p_rvalue_type = get_proper_type(rvalue_type)
            p_lvalue_type = get_proper_type(lvalue_type)
            if (
                isinstance(p_rvalue_type, CallableType)
                and p_rvalue_type.is_type_obj()
                and (
                    p_rvalue_type.type_object().is_abstract
                    or p_rvalue_type.type_object().is_protocol
                )
                and isinstance(p_lvalue_type, TypeType)
                and isinstance(p_lvalue_type.item, Instance)
                and (
                    p_lvalue_type.item.type.is_abstract or p_lvalue_type.item.type.is_protocol
                )
            ):
                self.msg.concrete_only_assign(p_lvalue_type, rvalue)
                return
            if rvalue_type and infer_lvalue_type and not isinstance(lvalue_type, PartialType):
                # Don't use type binder for definitions of special forms, like named tuples.
                if not (isinstance(lvalue, NameExpr) and lvalue.is_special_form):
                    self.binder.assign_type(lvalue, rvalue_type, lvalue_type, False)

        elif index_lvalue:
            self.check_indexed_assignment(index_lvalue, rvalue, lvalue)

        if inferred:
            type_context = self.get_variable_type_context(inferred)
            rvalue_type = self.expr_checker.accept(rvalue, type_context=type_context)
            if not (
                inferred.is_final
                or (isinstance(lvalue, NameExpr) and lvalue.name == "__match_args__")
            ):
                rvalue_type = remove_instance_last_known_values(rvalue_type)
            self.infer_variable_type(inferred, lvalue, rvalue_type, rvalue)
        self.check_assignment_to_slots(lvalue)

</t>
<t tx="ekr.20221004064034.405"># (type, operator) tuples for augmented assignments supported with partial types
partial_type_augmented_ops: Final = {("builtins.list", "+"), ("builtins.set", "|")}

</t>
<t tx="ekr.20221004064034.406">def get_variable_type_context(self, inferred: Var) -&gt; Type | None:
    type_contexts = []
    if inferred.info:
        for base in inferred.info.mro[1:]:
            base_type, base_node = self.lvalue_type_from_base(inferred, base)
            if (
                base_type
                and not (isinstance(base_node, Var) and base_node.invalid_partial_type)
                and not isinstance(base_type, PartialType)
            ):
                type_contexts.append(base_type)
    # Use most derived supertype as type context if available.
    if not type_contexts:
        return None
    candidate = type_contexts[0]
    for other in type_contexts:
        if is_proper_subtype(other, candidate):
            candidate = other
        elif not is_subtype(candidate, other):
            # Multiple incompatible candidates, cannot use any of them as context.
            return None
    return candidate

</t>
<t tx="ekr.20221004064034.407">def try_infer_partial_generic_type_from_assignment(
    self, lvalue: Lvalue, rvalue: Expression, op: str
) -&gt; None:
    """Try to infer a precise type for partial generic type from assignment.

    'op' is '=' for normal assignment and a binary operator ('+', ...) for
    augmented assignment.

    Example where this happens:

        x = []
        if foo():
            x = [1]  # Infer List[int] as type of 'x'
    """
    var = None
    if (
        isinstance(lvalue, NameExpr)
        and isinstance(lvalue.node, Var)
        and isinstance(lvalue.node.type, PartialType)
    ):
        var = lvalue.node
    elif isinstance(lvalue, MemberExpr):
        var = self.expr_checker.get_partial_self_var(lvalue)
    if var is not None:
        typ = var.type
        assert isinstance(typ, PartialType)
        if typ.type is None:
            return
        # Return if this is an unsupported augmented assignment.
        if op != "=" and (typ.type.fullname, op) not in self.partial_type_augmented_ops:
            return
        # TODO: some logic here duplicates the None partial type counterpart
        #       inlined in check_assignment(), see #8043.
        partial_types = self.find_partial_types(var)
        if partial_types is None:
            return
        rvalue_type = self.expr_checker.accept(rvalue)
        rvalue_type = get_proper_type(rvalue_type)
        if isinstance(rvalue_type, Instance):
            if rvalue_type.type == typ.type and is_valid_inferred_type(rvalue_type):
                var.type = rvalue_type
                del partial_types[var]
        elif isinstance(rvalue_type, AnyType):
            var.type = fill_typevars_with_any(typ.type)
            del partial_types[var]

</t>
<t tx="ekr.20221004064034.408">def check_compatibility_all_supers(
    self, lvalue: RefExpr, lvalue_type: Type | None, rvalue: Expression
) -&gt; bool:
    lvalue_node = lvalue.node
    # Check if we are a class variable with at least one base class
    if (
        isinstance(lvalue_node, Var)
        and lvalue.kind in (MDEF, None)
        and len(lvalue_node.info.bases) &gt; 0  # None for Vars defined via self
    ):

        for base in lvalue_node.info.mro[1:]:
            tnode = base.names.get(lvalue_node.name)
            if tnode is not None:
                if not self.check_compatibility_classvar_super(lvalue_node, base, tnode.node):
                    # Show only one error per variable
                    break

                if not self.check_compatibility_final_super(lvalue_node, base, tnode.node):
                    # Show only one error per variable
                    break

        direct_bases = lvalue_node.info.direct_base_classes()
        last_immediate_base = direct_bases[-1] if direct_bases else None

        for base in lvalue_node.info.mro[1:]:
            # The type of "__slots__" and some other attributes usually doesn't need to
            # be compatible with a base class. We'll still check the type of "__slots__"
            # against "object" as an exception.
            if lvalue_node.allow_incompatible_override and not (
                lvalue_node.name == "__slots__" and base.fullname == "builtins.object"
            ):
                continue

            if is_private(lvalue_node.name):
                continue

            base_type, base_node = self.lvalue_type_from_base(lvalue_node, base)
            if isinstance(base_type, PartialType):
                base_type = None

            if base_type:
                assert base_node is not None
                if not self.check_compatibility_super(
                    lvalue, lvalue_type, rvalue, base, base_type, base_node
                ):
                    # Only show one error per variable; even if other
                    # base classes are also incompatible
                    return True
                if base is last_immediate_base:
                    # At this point, the attribute was found to be compatible with all
                    # immediate parents.
                    break
    return False

</t>
<t tx="ekr.20221004064034.409">def check_compatibility_super(
    self,
    lvalue: RefExpr,
    lvalue_type: Type | None,
    rvalue: Expression,
    base: TypeInfo,
    base_type: Type,
    base_node: Node,
) -&gt; bool:
    lvalue_node = lvalue.node
    assert isinstance(lvalue_node, Var)

    # Do not check whether the rvalue is compatible if the
    # lvalue had a type defined; this is handled by other
    # parts, and all we have to worry about in that case is
    # that lvalue is compatible with the base class.
    compare_node = None
    if lvalue_type:
        compare_type = lvalue_type
        compare_node = lvalue.node
    else:
        compare_type = self.expr_checker.accept(rvalue, base_type)
        if isinstance(rvalue, NameExpr):
            compare_node = rvalue.node
            if isinstance(compare_node, Decorator):
                compare_node = compare_node.func

    base_type = get_proper_type(base_type)
    compare_type = get_proper_type(compare_type)
    if compare_type:
        if isinstance(base_type, CallableType) and isinstance(compare_type, CallableType):
            base_static = is_node_static(base_node)
            compare_static = is_node_static(compare_node)

            # In case compare_static is unknown, also check
            # if 'definition' is set. The most common case for
            # this is with TempNode(), where we lose all
            # information about the real rvalue node (but only get
            # the rvalue type)
            if compare_static is None and compare_type.definition:
                compare_static = is_node_static(compare_type.definition)

            # Compare against False, as is_node_static can return None
            if base_static is False and compare_static is False:
                # Class-level function objects and classmethods become bound
                # methods: the former to the instance, the latter to the
                # class
                base_type = bind_self(base_type, self.scope.active_self_type())
                compare_type = bind_self(compare_type, self.scope.active_self_type())

            # If we are a static method, ensure to also tell the
            # lvalue it now contains a static method
            if base_static and compare_static:
                lvalue_node.is_staticmethod = True

        return self.check_subtype(
            compare_type,
            base_type,
            rvalue,
            message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT,
            "expression has type",
            f'base class "{base.name}" defined the type as',
        )
    return True

</t>
<t tx="ekr.20221004064034.41">def main() -&gt; None:
    json_chunks = list(get_files(ROOT))
    class_chunks = list(extract_classes(json_chunks))

    total_size = sum(chunk.total_size for chunk in json_chunks)
    print(f"Total cache size: {total_size / (1024 * 1024):.3f} megabytes")
    print()

    class_name_counter = Counter(chunk[".class"] for chunk in class_chunks)
    print("Most commonly used classes:")
    report_counter(class_name_counter)

    print("Most common literal chunks:")
    report_most_common(class_chunks, 15)

    build = None
    for chunk in json_chunks:
        if "build.*.json" in chunk.filename:
            build = chunk
            break
    assert build is not None
    original = json.dumps(build.data, sort_keys=True)
    print(f"Size of build.data.json, in kilobytes: {len(original) / 1024:.3f}")

    build.data = compress(build.data)
    compressed = json.dumps(build.data, sort_keys=True)
    print(f"Size of compressed build.data.json, in kilobytes: {len(compressed) / 1024:.3f}")

    build.data = decompress(build.data)
    decompressed = json.dumps(build.data, sort_keys=True)
    print(f"Size of decompressed build.data.json, in kilobytes: {len(decompressed) / 1024:.3f}")

    print("Lossless conversion back", original == decompressed)

    """var_chunks = list(pluck("Var", class_chunks))
    report_most_common(var_chunks, 20)
    print()

    #for var in var_chunks:
    #    if var['fullname'] == 'self' and not (isinstance(var['type'], dict) and var['type']['.class'] == 'AnyType'):
    #        print(var)
    #argument_chunks = list(pluck("Argument", class_chunks))

    symbol_table_node_chunks = list(pluck("SymbolTableNode", class_chunks))
    report_most_common(symbol_table_node_chunks, 20)

    print()
    print("Most common")
    report_most_common(class_chunks, 20)
    print()"""


</t>
<t tx="ekr.20221004064034.410">def lvalue_type_from_base(
    self, expr_node: Var, base: TypeInfo
) -&gt; tuple[Type | None, Node | None]:
    """For a NameExpr that is part of a class, walk all base classes and try
    to find the first class that defines a Type for the same name."""
    expr_name = expr_node.name
    base_var = base.names.get(expr_name)

    if base_var:
        base_node = base_var.node
        base_type = base_var.type
        if isinstance(base_node, Decorator):
            base_node = base_node.func
            base_type = base_node.type

        if base_type:
            if not has_no_typevars(base_type):
                self_type = self.scope.active_self_type()
                assert self_type is not None, "Internal error: base lookup outside class"
                if isinstance(self_type, TupleType):
                    instance = tuple_fallback(self_type)
                else:
                    instance = self_type
                itype = map_instance_to_supertype(instance, base)
                base_type = expand_type_by_instance(base_type, itype)

            base_type = get_proper_type(base_type)
            if isinstance(base_type, CallableType) and isinstance(base_node, FuncDef):
                # If we are a property, return the Type of the return
                # value, not the Callable
                if base_node.is_property:
                    base_type = get_proper_type(base_type.ret_type)
            if isinstance(base_type, FunctionLike) and isinstance(
                base_node, OverloadedFuncDef
            ):
                # Same for properties with setter
                if base_node.is_property:
                    base_type = base_type.items[0].ret_type

            return base_type, base_node

    return None, None

</t>
<t tx="ekr.20221004064034.411">def check_compatibility_classvar_super(
    self, node: Var, base: TypeInfo, base_node: Node | None
) -&gt; bool:
    if not isinstance(base_node, Var):
        return True
    if node.is_classvar and not base_node.is_classvar:
        self.fail(message_registry.CANNOT_OVERRIDE_INSTANCE_VAR.format(base.name), node)
        return False
    elif not node.is_classvar and base_node.is_classvar:
        self.fail(message_registry.CANNOT_OVERRIDE_CLASS_VAR.format(base.name), node)
        return False
    return True

</t>
<t tx="ekr.20221004064034.412">def check_compatibility_final_super(
    self, node: Var, base: TypeInfo, base_node: Node | None
) -&gt; bool:
    """Check if an assignment overrides a final attribute in a base class.

    This only checks situations where either a node in base class is not a variable
    but a final method, or where override is explicitly declared as final.
    In these cases we give a more detailed error message. In addition, we check that
    a final variable doesn't override writeable attribute, which is not safe.

    Other situations are checked in `check_final()`.
    """
    if not isinstance(base_node, (Var, FuncBase, Decorator)):
        return True
    if base_node.is_final and (node.is_final or not isinstance(base_node, Var)):
        # Give this error only for explicit override attempt with `Final`, or
        # if we are overriding a final method with variable.
        # Other override attempts will be flagged as assignment to constant
        # in `check_final()`.
        self.msg.cant_override_final(node.name, base.name, node)
        return False
    if node.is_final:
        if base.fullname in ENUM_BASES or node.name in ENUM_SPECIAL_PROPS:
            return True
        self.check_if_final_var_override_writable(node.name, base_node, node)
    return True

</t>
<t tx="ekr.20221004064034.413">def check_if_final_var_override_writable(
    self, name: str, base_node: Node | None, ctx: Context
) -&gt; None:
    """Check that a final variable doesn't override writeable attribute.

    This is done to prevent situations like this:
        class C:
            attr = 1
        class D(C):
            attr: Final = 2

        x: C = D()
        x.attr = 3  # Oops!
    """
    writable = True
    if base_node:
        writable = self.is_writable_attribute(base_node)
    if writable:
        self.msg.final_cant_override_writable(name, ctx)

</t>
<t tx="ekr.20221004064034.414">def get_final_context(self) -&gt; bool:
    """Check whether we a currently checking a final declaration."""
    return self._is_final_def

</t>
<t tx="ekr.20221004064034.415">@contextmanager
def enter_final_context(self, is_final_def: bool) -&gt; Iterator[None]:
    """Store whether the current checked assignment is a final declaration."""
    old_ctx = self._is_final_def
    self._is_final_def = is_final_def
    try:
        yield
    finally:
        self._is_final_def = old_ctx

</t>
<t tx="ekr.20221004064034.416">def check_final(self, s: AssignmentStmt | OperatorAssignmentStmt | AssignmentExpr) -&gt; None:
    """Check if this assignment does not assign to a final attribute.

    This function performs the check only for name assignments at module
    and class scope. The assignments to `obj.attr` and `Cls.attr` are checked
    in checkmember.py.
    """
    if isinstance(s, AssignmentStmt):
        lvs = self.flatten_lvalues(s.lvalues)
    elif isinstance(s, AssignmentExpr):
        lvs = [s.target]
    else:
        lvs = [s.lvalue]
    is_final_decl = s.is_final_def if isinstance(s, AssignmentStmt) else False
    if is_final_decl and self.scope.active_class():
        lv = lvs[0]
        assert isinstance(lv, RefExpr)
        if lv.node is not None:
            assert isinstance(lv.node, Var)
            if (
                lv.node.final_unset_in_class
                and not lv.node.final_set_in_init
                and not self.is_stub
                and  # It is OK to skip initializer in stub files.
                # Avoid extra error messages, if there is no type in Final[...],
                # then we already reported the error about missing r.h.s.
                isinstance(s, AssignmentStmt)
                and s.type is not None
            ):
                self.msg.final_without_value(s)
    for lv in lvs:
        if isinstance(lv, RefExpr) and isinstance(lv.node, Var):
            name = lv.node.name
            cls = self.scope.active_class()
            if cls is not None:
                # These additional checks exist to give more error messages
                # even if the final attribute was overridden with a new symbol
                # (which is itself an error)...
                for base in cls.mro[1:]:
                    sym = base.names.get(name)
                    # We only give this error if base node is variable,
                    # overriding final method will be caught in
                    # `check_compatibility_final_super()`.
                    if sym and isinstance(sym.node, Var):
                        if sym.node.is_final and not is_final_decl:
                            self.msg.cant_assign_to_final(name, sym.node.info is None, s)
                            # ...but only once
                            break
            if lv.node.is_final and not is_final_decl:
                self.msg.cant_assign_to_final(name, lv.node.info is None, s)

</t>
<t tx="ekr.20221004064034.417">def check_assignment_to_slots(self, lvalue: Lvalue) -&gt; None:
    if not isinstance(lvalue, MemberExpr):
        return

    inst = get_proper_type(self.expr_checker.accept(lvalue.expr))
    if not isinstance(inst, Instance):
        return
    if inst.type.slots is None:
        return  # Slots do not exist, we can allow any assignment
    if lvalue.name in inst.type.slots:
        return  # We are assigning to an existing slot
    for base_info in inst.type.mro[:-1]:
        if base_info.names.get("__setattr__") is not None:
            # When type has `__setattr__` defined,
            # we can assign any dynamic value.
            # We exclude object, because it always has `__setattr__`.
            return

    definition = inst.type.get(lvalue.name)
    if definition is None:
        # We don't want to duplicate
        # `"SomeType" has no attribute "some_attr"`
        # error twice.
        return
    if self.is_assignable_slot(lvalue, definition.type):
        return

    self.fail(
        message_registry.NAME_NOT_IN_SLOTS.format(lvalue.name, inst.type.fullname), lvalue
    )

</t>
<t tx="ekr.20221004064034.418">def is_assignable_slot(self, lvalue: Lvalue, typ: Type | None) -&gt; bool:
    if getattr(lvalue, "node", None):
        return False  # This is a definition

    typ = get_proper_type(typ)
    if typ is None or isinstance(typ, AnyType):
        return True  # Any can be literally anything, like `@propery`
    if isinstance(typ, Instance):
        # When working with instances, we need to know if they contain
        # `__set__` special method. Like `@property` does.
        # This makes assigning to properties possible,
        # even without extra slot spec.
        return typ.type.get("__set__") is not None
    if isinstance(typ, FunctionLike):
        return True  # Can be a property, or some other magic
    if isinstance(typ, UnionType):
        return all(self.is_assignable_slot(lvalue, u) for u in typ.items)
    return False

</t>
<t tx="ekr.20221004064034.419">def check_assignment_to_multiple_lvalues(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    context: Context,
    infer_lvalue_type: bool = True,
) -&gt; None:
    if isinstance(rvalue, TupleExpr) or isinstance(rvalue, ListExpr):
        # Recursively go into Tuple or List expression rhs instead of
        # using the type of rhs, because this allowed more fine grained
        # control in cases like: a, b = [int, str] where rhs would get
        # type List[object]
        rvalues: list[Expression] = []
        iterable_type: Type | None = None
        last_idx: int | None = None
        for idx_rval, rval in enumerate(rvalue.items):
            if isinstance(rval, StarExpr):
                typs = get_proper_type(self.expr_checker.visit_star_expr(rval).type)
                if isinstance(typs, TupleType):
                    rvalues.extend([TempNode(typ) for typ in typs.items])
                elif self.type_is_iterable(typs) and isinstance(typs, Instance):
                    if iterable_type is not None and iterable_type != self.iterable_item_type(
                        typs
                    ):
                        self.fail(message_registry.CONTIGUOUS_ITERABLE_EXPECTED, context)
                    else:
                        if last_idx is None or last_idx + 1 == idx_rval:
                            rvalues.append(rval)
                            last_idx = idx_rval
                            iterable_type = self.iterable_item_type(typs)
                        else:
                            self.fail(message_registry.CONTIGUOUS_ITERABLE_EXPECTED, context)
                else:
                    self.fail(message_registry.ITERABLE_TYPE_EXPECTED.format(typs), context)
            else:
                rvalues.append(rval)
        iterable_start: int | None = None
        iterable_end: int | None = None
        for i, rval in enumerate(rvalues):
            if isinstance(rval, StarExpr):
                typs = get_proper_type(self.expr_checker.visit_star_expr(rval).type)
                if self.type_is_iterable(typs) and isinstance(typs, Instance):
                    if iterable_start is None:
                        iterable_start = i
                    iterable_end = i
        if (
            iterable_start is not None
            and iterable_end is not None
            and iterable_type is not None
        ):
            iterable_num = iterable_end - iterable_start + 1
            rvalue_needed = len(lvalues) - (len(rvalues) - iterable_num)
            if rvalue_needed &gt; 0:
                rvalues = (
                    rvalues[0:iterable_start]
                    + [TempNode(iterable_type) for i in range(rvalue_needed)]
                    + rvalues[iterable_end + 1 :]
                )

        if self.check_rvalue_count_in_assignment(lvalues, len(rvalues), context):
            star_index = next(
                (i for i, lv in enumerate(lvalues) if isinstance(lv, StarExpr)), len(lvalues)
            )

            left_lvs = lvalues[:star_index]
            star_lv = (
                cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
            )
            right_lvs = lvalues[star_index + 1 :]

            left_rvs, star_rvs, right_rvs = self.split_around_star(
                rvalues, star_index, len(lvalues)
            )

            lr_pairs = list(zip(left_lvs, left_rvs))
            if star_lv:
                rv_list = ListExpr(star_rvs)
                rv_list.set_line(rvalue)
                lr_pairs.append((star_lv.expr, rv_list))
            lr_pairs.extend(zip(right_lvs, right_rvs))

            for lv, rv in lr_pairs:
                self.check_assignment(lv, rv, infer_lvalue_type)
    else:
        self.check_multi_assignment(lvalues, rvalue, context, infer_lvalue_type)

</t>
<t tx="ekr.20221004064034.42">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
"""Script for applying a cache diff.

With some infrastructure, this can allow for distributing small cache diffs to users in
many cases instead of full cache artifacts.
"""

from __future__ import annotations

import argparse
import json
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mypy.metastore import FilesystemMetadataStore, MetadataStore, SqliteMetadataStore


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.420">def check_rvalue_count_in_assignment(
    self, lvalues: list[Lvalue], rvalue_count: int, context: Context
) -&gt; bool:
    if any(isinstance(lvalue, StarExpr) for lvalue in lvalues):
        if len(lvalues) - 1 &gt; rvalue_count:
            self.msg.wrong_number_values_to_unpack(rvalue_count, len(lvalues) - 1, context)
            return False
    elif rvalue_count != len(lvalues):
        self.msg.wrong_number_values_to_unpack(rvalue_count, len(lvalues), context)
        return False
    return True

</t>
<t tx="ekr.20221004064034.421">def check_multi_assignment(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    context: Context,
    infer_lvalue_type: bool = True,
    rv_type: Type | None = None,
    undefined_rvalue: bool = False,
) -&gt; None:
    """Check the assignment of one rvalue to a number of lvalues."""

    # Infer the type of an ordinary rvalue expression.
    # TODO: maybe elsewhere; redundant.
    rvalue_type = get_proper_type(rv_type or self.expr_checker.accept(rvalue))

    if isinstance(rvalue_type, TypeVarLikeType):
        rvalue_type = get_proper_type(rvalue_type.upper_bound)

    if isinstance(rvalue_type, UnionType):
        # If this is an Optional type in non-strict Optional code, unwrap it.
        relevant_items = rvalue_type.relevant_items()
        if len(relevant_items) == 1:
            rvalue_type = get_proper_type(relevant_items[0])

    if isinstance(rvalue_type, AnyType):
        for lv in lvalues:
            if isinstance(lv, StarExpr):
                lv = lv.expr
            temp_node = self.temp_node(
                AnyType(TypeOfAny.from_another_any, source_any=rvalue_type), context
            )
            self.check_assignment(lv, temp_node, infer_lvalue_type)
    elif isinstance(rvalue_type, TupleType):
        self.check_multi_assignment_from_tuple(
            lvalues, rvalue, rvalue_type, context, undefined_rvalue, infer_lvalue_type
        )
    elif isinstance(rvalue_type, UnionType):
        self.check_multi_assignment_from_union(
            lvalues, rvalue, rvalue_type, context, infer_lvalue_type
        )
    elif isinstance(rvalue_type, Instance) and rvalue_type.type.fullname == "builtins.str":
        self.msg.unpacking_strings_disallowed(context)
    else:
        self.check_multi_assignment_from_iterable(
            lvalues, rvalue_type, context, infer_lvalue_type
        )

</t>
<t tx="ekr.20221004064034.422">def check_multi_assignment_from_union(
    self,
    lvalues: list[Expression],
    rvalue: Expression,
    rvalue_type: UnionType,
    context: Context,
    infer_lvalue_type: bool,
) -&gt; None:
    """Check assignment to multiple lvalue targets when rvalue type is a Union[...].
    For example:

        t: Union[Tuple[int, int], Tuple[str, str]]
        x, y = t
        reveal_type(x)  # Union[int, str]

    The idea in this case is to process the assignment for every item of the union.
    Important note: the types are collected in two places, 'union_types' contains
    inferred types for first assignments, 'assignments' contains the narrowed types
    for binder.
    """
    self.no_partial_types = True
    transposed: tuple[list[Type], ...] = tuple([] for _ in self.flatten_lvalues(lvalues))
    # Notify binder that we want to defer bindings and instead collect types.
    with self.binder.accumulate_type_assignments() as assignments:
        for item in rvalue_type.items:
            # Type check the assignment separately for each union item and collect
            # the inferred lvalue types for each union item.
            self.check_multi_assignment(
                lvalues,
                rvalue,
                context,
                infer_lvalue_type=infer_lvalue_type,
                rv_type=item,
                undefined_rvalue=True,
            )
            for t, lv in zip(transposed, self.flatten_lvalues(lvalues)):
                # We can access _type_maps directly since temporary type maps are
                # only created within expressions.
                t.append(self._type_maps[0].pop(lv, AnyType(TypeOfAny.special_form)))
    union_types = tuple(make_simplified_union(col) for col in transposed)
    for expr, items in assignments.items():
        # Bind a union of types collected in 'assignments' to every expression.
        if isinstance(expr, StarExpr):
            expr = expr.expr

        # TODO: See todo in binder.py, ConditionalTypeBinder.assign_type
        # It's unclear why the 'declared_type' param is sometimes 'None'
        clean_items: list[tuple[Type, Type]] = []
        for type, declared_type in items:
            assert declared_type is not None
            clean_items.append((type, declared_type))

        # TODO: fix signature of zip() in typeshed.
        types, declared_types = cast(Any, zip)(*clean_items)
        self.binder.assign_type(
            expr,
            make_simplified_union(list(types)),
            make_simplified_union(list(declared_types)),
            False,
        )
    for union, lv in zip(union_types, self.flatten_lvalues(lvalues)):
        # Properly store the inferred types.
        _1, _2, inferred = self.check_lvalue(lv)
        if inferred:
            self.set_inferred_type(inferred, lv, union)
        else:
            self.store_type(lv, union)
    self.no_partial_types = False

</t>
<t tx="ekr.20221004064034.423">def flatten_lvalues(self, lvalues: list[Expression]) -&gt; list[Expression]:
    res: list[Expression] = []
    for lv in lvalues:
        if isinstance(lv, (TupleExpr, ListExpr)):
            res.extend(self.flatten_lvalues(lv.items))
        if isinstance(lv, StarExpr):
            # Unwrap StarExpr, since it is unwrapped by other helpers.
            lv = lv.expr
        res.append(lv)
    return res

</t>
<t tx="ekr.20221004064034.424">def check_multi_assignment_from_tuple(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    rvalue_type: TupleType,
    context: Context,
    undefined_rvalue: bool,
    infer_lvalue_type: bool = True,
) -&gt; None:
    if self.check_rvalue_count_in_assignment(lvalues, len(rvalue_type.items), context):
        star_index = next(
            (i for i, lv in enumerate(lvalues) if isinstance(lv, StarExpr)), len(lvalues)
        )

        left_lvs = lvalues[:star_index]
        star_lv = cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
        right_lvs = lvalues[star_index + 1 :]

        if not undefined_rvalue:
            # Infer rvalue again, now in the correct type context.
            lvalue_type = self.lvalue_type_for_inference(lvalues, rvalue_type)
            reinferred_rvalue_type = get_proper_type(
                self.expr_checker.accept(rvalue, lvalue_type)
            )

            if isinstance(reinferred_rvalue_type, UnionType):
                # If this is an Optional type in non-strict Optional code, unwrap it.
                relevant_items = reinferred_rvalue_type.relevant_items()
                if len(relevant_items) == 1:
                    reinferred_rvalue_type = get_proper_type(relevant_items[0])
            if isinstance(reinferred_rvalue_type, UnionType):
                self.check_multi_assignment_from_union(
                    lvalues, rvalue, reinferred_rvalue_type, context, infer_lvalue_type
                )
                return
            if isinstance(reinferred_rvalue_type, AnyType):
                # We can get Any if the current node is
                # deferred. Doing more inference in deferred nodes
                # is hard, so give up for now.  We can also get
                # here if reinferring types above changes the
                # inferred return type for an overloaded function
                # to be ambiguous.
                return
            assert isinstance(reinferred_rvalue_type, TupleType)
            rvalue_type = reinferred_rvalue_type

        left_rv_types, star_rv_types, right_rv_types = self.split_around_star(
            rvalue_type.items, star_index, len(lvalues)
        )

        for lv, rv_type in zip(left_lvs, left_rv_types):
            self.check_assignment(lv, self.temp_node(rv_type, context), infer_lvalue_type)
        if star_lv:
            list_expr = ListExpr(
                [self.temp_node(rv_type, context) for rv_type in star_rv_types]
            )
            list_expr.set_line(context)
            self.check_assignment(star_lv.expr, list_expr, infer_lvalue_type)
        for lv, rv_type in zip(right_lvs, right_rv_types):
            self.check_assignment(lv, self.temp_node(rv_type, context), infer_lvalue_type)

</t>
<t tx="ekr.20221004064034.425">def lvalue_type_for_inference(self, lvalues: list[Lvalue], rvalue_type: TupleType) -&gt; Type:
    star_index = next(
        (i for i, lv in enumerate(lvalues) if isinstance(lv, StarExpr)), len(lvalues)
    )
    left_lvs = lvalues[:star_index]
    star_lv = cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
    right_lvs = lvalues[star_index + 1 :]
    left_rv_types, star_rv_types, right_rv_types = self.split_around_star(
        rvalue_type.items, star_index, len(lvalues)
    )

    type_parameters: list[Type] = []

    def append_types_for_inference(lvs: list[Expression], rv_types: list[Type]) -&gt; None:
        for lv, rv_type in zip(lvs, rv_types):
            sub_lvalue_type, index_expr, inferred = self.check_lvalue(lv)
            if sub_lvalue_type and not isinstance(sub_lvalue_type, PartialType):
                type_parameters.append(sub_lvalue_type)
            else:  # index lvalue
                # TODO Figure out more precise type context, probably
                #      based on the type signature of the _set method.
                type_parameters.append(rv_type)

    append_types_for_inference(left_lvs, left_rv_types)

    if star_lv:
        sub_lvalue_type, index_expr, inferred = self.check_lvalue(star_lv.expr)
        if sub_lvalue_type and not isinstance(sub_lvalue_type, PartialType):
            type_parameters.extend([sub_lvalue_type] * len(star_rv_types))
        else:  # index lvalue
            # TODO Figure out more precise type context, probably
            #      based on the type signature of the _set method.
            type_parameters.extend(star_rv_types)

    append_types_for_inference(right_lvs, right_rv_types)

    return TupleType(type_parameters, self.named_type("builtins.tuple"))

</t>
<t tx="ekr.20221004064034.426">def split_around_star(
    self, items: list[T], star_index: int, length: int
) -&gt; tuple[list[T], list[T], list[T]]:
    """Splits a list of items in three to match another list of length 'length'
    that contains a starred expression at 'star_index' in the following way:

    star_index = 2, length = 5 (i.e., [a,b,*,c,d]), items = [1,2,3,4,5,6,7]
    returns in: ([1,2], [3,4,5], [6,7])
    """
    nr_right_of_star = length - star_index - 1
    right_index = -nr_right_of_star if nr_right_of_star != 0 else len(items)
    left = items[:star_index]
    star = items[star_index:right_index]
    right = items[right_index:]
    return left, star, right

</t>
<t tx="ekr.20221004064034.427">def type_is_iterable(self, type: Type) -&gt; bool:
    type = get_proper_type(type)
    if isinstance(type, CallableType) and type.is_type_obj():
        type = type.fallback
    return is_subtype(
        type, self.named_generic_type("typing.Iterable", [AnyType(TypeOfAny.special_form)])
    )

</t>
<t tx="ekr.20221004064034.428">def check_multi_assignment_from_iterable(
    self,
    lvalues: list[Lvalue],
    rvalue_type: Type,
    context: Context,
    infer_lvalue_type: bool = True,
) -&gt; None:
    rvalue_type = get_proper_type(rvalue_type)
    if self.type_is_iterable(rvalue_type) and isinstance(rvalue_type, Instance):
        item_type = self.iterable_item_type(rvalue_type)
        for lv in lvalues:
            if isinstance(lv, StarExpr):
                items_type = self.named_generic_type("builtins.list", [item_type])
                self.check_assignment(
                    lv.expr, self.temp_node(items_type, context), infer_lvalue_type
                )
            else:
                self.check_assignment(
                    lv, self.temp_node(item_type, context), infer_lvalue_type
                )
    else:
        self.msg.type_not_iterable(rvalue_type, context)

</t>
<t tx="ekr.20221004064034.429">def check_lvalue(self, lvalue: Lvalue) -&gt; tuple[Type | None, IndexExpr | None, Var | None]:
    lvalue_type = None
    index_lvalue = None
    inferred = None

    if self.is_definition(lvalue) and (
        not isinstance(lvalue, NameExpr) or isinstance(lvalue.node, Var)
    ):
        if isinstance(lvalue, NameExpr):
            inferred = cast(Var, lvalue.node)
            assert isinstance(inferred, Var)
        else:
            assert isinstance(lvalue, MemberExpr)
            self.expr_checker.accept(lvalue.expr)
            inferred = lvalue.def_var
    elif isinstance(lvalue, IndexExpr):
        index_lvalue = lvalue
    elif isinstance(lvalue, MemberExpr):
        lvalue_type = self.expr_checker.analyze_ordinary_member_access(lvalue, True)
        self.store_type(lvalue, lvalue_type)
    elif isinstance(lvalue, NameExpr):
        lvalue_type = self.expr_checker.analyze_ref_expr(lvalue, lvalue=True)
        self.store_type(lvalue, lvalue_type)
    elif isinstance(lvalue, TupleExpr) or isinstance(lvalue, ListExpr):
        types = [
            self.check_lvalue(sub_expr)[0] or
            # This type will be used as a context for further inference of rvalue,
            # we put Uninhabited if there is no information available from lvalue.
            UninhabitedType()
            for sub_expr in lvalue.items
        ]
        lvalue_type = TupleType(types, self.named_type("builtins.tuple"))
    elif isinstance(lvalue, StarExpr):
        typ, _, _ = self.check_lvalue(lvalue.expr)
        lvalue_type = StarType(typ) if typ else None
    else:
        lvalue_type = self.expr_checker.accept(lvalue)

    return lvalue_type, index_lvalue, inferred

</t>
<t tx="ekr.20221004064034.43">def make_cache(input_dir: str, sqlite: bool) -&gt; MetadataStore:
    if sqlite:
        return SqliteMetadataStore(input_dir)
    else:
        return FilesystemMetadataStore(input_dir)


</t>
<t tx="ekr.20221004064034.430">def is_definition(self, s: Lvalue) -&gt; bool:
    if isinstance(s, NameExpr):
        if s.is_inferred_def:
            return True
        # If the node type is not defined, this must the first assignment
        # that we process =&gt; this is a definition, even though the semantic
        # analyzer did not recognize this as such. This can arise in code
        # that uses isinstance checks, if type checking of the primary
        # definition is skipped due to an always False type check.
        node = s.node
        if isinstance(node, Var):
            return node.type is None
    elif isinstance(s, MemberExpr):
        return s.is_inferred_def
    return False

</t>
<t tx="ekr.20221004064034.431">def infer_variable_type(
    self, name: Var, lvalue: Lvalue, init_type: Type, context: Context
) -&gt; None:
    """Infer the type of initialized variables from initializer type."""
    if isinstance(init_type, DeletedType):
        self.msg.deleted_as_rvalue(init_type, context)
    elif not is_valid_inferred_type(init_type) and not self.no_partial_types:
        # We cannot use the type of the initialization expression for full type
        # inference (it's not specific enough), but we might be able to give
        # partial type which will be made more specific later. A partial type
        # gets generated in assignment like 'x = []' where item type is not known.
        if not self.infer_partial_type(name, lvalue, init_type):
            self.msg.need_annotation_for_var(name, context, self.options.python_version)
            self.set_inference_error_fallback_type(name, lvalue, init_type)
    elif (
        isinstance(lvalue, MemberExpr)
        and self.inferred_attribute_types is not None
        and lvalue.def_var
        and lvalue.def_var in self.inferred_attribute_types
        and not is_same_type(self.inferred_attribute_types[lvalue.def_var], init_type)
    ):
        # Multiple, inconsistent types inferred for an attribute.
        self.msg.need_annotation_for_var(name, context, self.options.python_version)
        name.type = AnyType(TypeOfAny.from_error)
    else:
        # Infer type of the target.

        # Make the type more general (strip away function names etc.).
        init_type = strip_type(init_type)

        self.set_inferred_type(name, lvalue, init_type)

</t>
<t tx="ekr.20221004064034.432">def infer_partial_type(self, name: Var, lvalue: Lvalue, init_type: Type) -&gt; bool:
    init_type = get_proper_type(init_type)
    if isinstance(init_type, NoneType):
        partial_type = PartialType(None, name)
    elif isinstance(init_type, Instance):
        fullname = init_type.type.fullname
        is_ref = isinstance(lvalue, RefExpr)
        if (
            is_ref
            and (
                fullname == "builtins.list"
                or fullname == "builtins.set"
                or fullname == "builtins.dict"
                or fullname == "collections.OrderedDict"
            )
            and all(
                isinstance(t, (NoneType, UninhabitedType))
                for t in get_proper_types(init_type.args)
            )
        ):
            partial_type = PartialType(init_type.type, name)
        elif is_ref and fullname == "collections.defaultdict":
            arg0 = get_proper_type(init_type.args[0])
            arg1 = get_proper_type(init_type.args[1])
            if isinstance(
                arg0, (NoneType, UninhabitedType)
            ) and self.is_valid_defaultdict_partial_value_type(arg1):
                arg1 = erase_type(arg1)
                assert isinstance(arg1, Instance)
                partial_type = PartialType(init_type.type, name, arg1)
            else:
                return False
        else:
            return False
    else:
        return False
    self.set_inferred_type(name, lvalue, partial_type)
    self.partial_types[-1].map[name] = lvalue
    return True

</t>
<t tx="ekr.20221004064034.433">def is_valid_defaultdict_partial_value_type(self, t: ProperType) -&gt; bool:
    """Check if t can be used as the basis for a partial defaultdict value type.

    Examples:

      * t is 'int' --&gt; True
      * t is 'list[&lt;nothing&gt;]' --&gt; True
      * t is 'dict[...]' --&gt; False (only generic types with a single type
        argument supported)
    """
    if not isinstance(t, Instance):
        return False
    if len(t.args) == 0:
        return True
    if len(t.args) == 1:
        arg = get_proper_type(t.args[0])
        # TODO: This is too permissive -- we only allow TypeVarType since
        #       they leak in cases like defaultdict(list) due to a bug.
        #       This can result in incorrect types being inferred, but only
        #       in rare cases.
        if isinstance(arg, (TypeVarType, UninhabitedType, NoneType)):
            return True
    return False

</t>
<t tx="ekr.20221004064034.434">def set_inferred_type(self, var: Var, lvalue: Lvalue, type: Type) -&gt; None:
    """Store inferred variable type.

    Store the type to both the variable node and the expression node that
    refers to the variable (lvalue). If var is None, do nothing.
    """
    if var and not self.current_node_deferred:
        var.type = type
        var.is_inferred = True
        if var not in self.var_decl_frames:
            # Used for the hack to improve optional type inference in conditionals
            self.var_decl_frames[var] = {frame.id for frame in self.binder.frames}
        if isinstance(lvalue, MemberExpr) and self.inferred_attribute_types is not None:
            # Store inferred attribute type so that we can check consistency afterwards.
            if lvalue.def_var is not None:
                self.inferred_attribute_types[lvalue.def_var] = type
        self.store_type(lvalue, type)

</t>
<t tx="ekr.20221004064034.435">def set_inference_error_fallback_type(self, var: Var, lvalue: Lvalue, type: Type) -&gt; None:
    """Store best known type for variable if type inference failed.

    If a program ignores error on type inference error, the variable should get some
    inferred type so that if can used later on in the program. Example:

      x = []  # type: ignore
      x.append(1)   # Should be ok!

    We implement this here by giving x a valid type (replacing inferred &lt;nothing&gt; with Any).
    """
    fallback = self.inference_error_fallback_type(type)
    self.set_inferred_type(var, lvalue, fallback)

</t>
<t tx="ekr.20221004064034.436">def inference_error_fallback_type(self, type: Type) -&gt; Type:
    fallback = type.accept(SetNothingToAny())
    # Type variables may leak from inference, see https://github.com/python/mypy/issues/5738,
    # we therefore need to erase them.
    return erase_typevars(fallback)

</t>
<t tx="ekr.20221004064034.437">def check_simple_assignment(
    self,
    lvalue_type: Type | None,
    rvalue: Expression,
    context: Context,
    msg: ErrorMessage = message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT,
    lvalue_name: str = "variable",
    rvalue_name: str = "expression",
    *,
    notes: list[str] | None = None,
) -&gt; Type:
    if self.is_stub and isinstance(rvalue, EllipsisExpr):
        # '...' is always a valid initializer in a stub.
        return AnyType(TypeOfAny.special_form)
    else:
        always_allow_any = lvalue_type is not None and not isinstance(
            get_proper_type(lvalue_type), AnyType
        )
        rvalue_type = self.expr_checker.accept(
            rvalue, lvalue_type, always_allow_any=always_allow_any
        )
        if isinstance(rvalue_type, DeletedType):
            self.msg.deleted_as_rvalue(rvalue_type, context)
        if isinstance(lvalue_type, DeletedType):
            self.msg.deleted_as_lvalue(lvalue_type, context)
        elif lvalue_type:
            self.check_subtype(
                # Preserve original aliases for error messages when possible.
                rvalue_type,
                lvalue_type,
                context,
                msg,
                f"{rvalue_name} has type",
                f"{lvalue_name} has type",
                notes=notes,
            )
        return rvalue_type

</t>
<t tx="ekr.20221004064034.438">def check_member_assignment(
    self, instance_type: Type, attribute_type: Type, rvalue: Expression, context: Context
) -&gt; tuple[Type, Type, bool]:
    """Type member assignment.

    This defers to check_simple_assignment, unless the member expression
    is a descriptor, in which case this checks descriptor semantics as well.

    Return the inferred rvalue_type, inferred lvalue_type, and whether to use the binder
    for this assignment.

    Note: this method exists here and not in checkmember.py, because we need to take
    care about interaction between binder and __set__().
    """
    instance_type = get_proper_type(instance_type)
    attribute_type = get_proper_type(attribute_type)
    # Descriptors don't participate in class-attribute access
    if (isinstance(instance_type, FunctionLike) and instance_type.is_type_obj()) or isinstance(
        instance_type, TypeType
    ):
        rvalue_type = self.check_simple_assignment(attribute_type, rvalue, context)
        return rvalue_type, attribute_type, True

    if not isinstance(attribute_type, Instance):
        # TODO: support __set__() for union types.
        rvalue_type = self.check_simple_assignment(attribute_type, rvalue, context)
        return rvalue_type, attribute_type, True

    mx = MemberContext(
        is_lvalue=False,
        is_super=False,
        is_operator=False,
        original_type=instance_type,
        context=context,
        self_type=None,
        msg=self.msg,
        chk=self,
    )
    get_type = analyze_descriptor_access(attribute_type, mx)
    if not attribute_type.type.has_readable_member("__set__"):
        # If there is no __set__, we type-check that the assigned value matches
        # the return type of __get__. This doesn't match the python semantics,
        # (which allow you to override the descriptor with any value), but preserves
        # the type of accessing the attribute (even after the override).
        rvalue_type = self.check_simple_assignment(get_type, rvalue, context)
        return rvalue_type, get_type, True

    dunder_set = attribute_type.type.get_method("__set__")
    if dunder_set is None:
        self.fail(message_registry.DESCRIPTOR_SET_NOT_CALLABLE.format(attribute_type), context)
        return AnyType(TypeOfAny.from_error), get_type, False

    bound_method = analyze_decorator_or_funcbase_access(
        defn=dunder_set,
        itype=attribute_type,
        info=attribute_type.type,
        self_type=attribute_type,
        name="__set__",
        mx=mx,
    )
    typ = map_instance_to_supertype(attribute_type, dunder_set.info)
    dunder_set_type = expand_type_by_instance(bound_method, typ)

    callable_name = self.expr_checker.method_fullname(attribute_type, "__set__")
    dunder_set_type = self.expr_checker.transform_callee_type(
        callable_name,
        dunder_set_type,
        [TempNode(instance_type, context=context), rvalue],
        [nodes.ARG_POS, nodes.ARG_POS],
        context,
        object_type=attribute_type,
    )

    # For non-overloaded setters, the result should be type-checked like a regular assignment.
    # Hence, we first only try to infer the type by using the rvalue as type context.
    type_context = rvalue
    with self.msg.filter_errors():
        _, inferred_dunder_set_type = self.expr_checker.check_call(
            dunder_set_type,
            [TempNode(instance_type, context=context), type_context],
            [nodes.ARG_POS, nodes.ARG_POS],
            context,
            object_type=attribute_type,
            callable_name=callable_name,
        )

    # And now we in fact type check the call, to show errors related to wrong arguments
    # count, etc., replacing the type context for non-overloaded setters only.
    inferred_dunder_set_type = get_proper_type(inferred_dunder_set_type)
    if isinstance(inferred_dunder_set_type, CallableType):
        type_context = TempNode(AnyType(TypeOfAny.special_form), context=context)
    self.expr_checker.check_call(
        dunder_set_type,
        [TempNode(instance_type, context=context), type_context],
        [nodes.ARG_POS, nodes.ARG_POS],
        context,
        object_type=attribute_type,
        callable_name=callable_name,
    )

    # In the following cases, a message already will have been recorded in check_call.
    if (not isinstance(inferred_dunder_set_type, CallableType)) or (
        len(inferred_dunder_set_type.arg_types) &lt; 2
    ):
        return AnyType(TypeOfAny.from_error), get_type, False

    set_type = inferred_dunder_set_type.arg_types[1]
    # Special case: if the rvalue_type is a subtype of both '__get__' and '__set__' types,
    # and '__get__' type is narrower than '__set__', then we invoke the binder to narrow type
    # by this assignment. Technically, this is not safe, but in practice this is
    # what a user expects.
    rvalue_type = self.check_simple_assignment(set_type, rvalue, context)
    infer = is_subtype(rvalue_type, get_type) and is_subtype(get_type, set_type)
    return rvalue_type if infer else set_type, get_type, infer

</t>
<t tx="ekr.20221004064034.439">def check_indexed_assignment(
    self, lvalue: IndexExpr, rvalue: Expression, context: Context
) -&gt; None:
    """Type check indexed assignment base[index] = rvalue.

    The lvalue argument is the base[index] expression.
    """
    self.try_infer_partial_type_from_indexed_assignment(lvalue, rvalue)
    basetype = get_proper_type(self.expr_checker.accept(lvalue.base))
    method_type = self.expr_checker.analyze_external_member_access(
        "__setitem__", basetype, lvalue
    )

    lvalue.method_type = method_type
    self.expr_checker.check_method_call(
        "__setitem__",
        basetype,
        method_type,
        [lvalue.index, rvalue],
        [nodes.ARG_POS, nodes.ARG_POS],
        context,
    )

</t>
<t tx="ekr.20221004064034.44">def apply_diff(cache_dir: str, diff_file: str, sqlite: bool = False) -&gt; None:
    cache = make_cache(cache_dir, sqlite)
    with open(diff_file) as f:
        diff = json.load(f)

    old_deps = json.loads(cache.read("@deps.meta.json"))

    for file, data in diff.items():
        if data is None:
            cache.remove(file)
        else:
            cache.write(file, data)
            if file.endswith(".meta.json") and "@deps" not in file:
                meta = json.loads(data)
                old_deps["snapshot"][meta["id"]] = meta["hash"]

    cache.write("@deps.meta.json", json.dumps(old_deps))

    cache.commit()


</t>
<t tx="ekr.20221004064034.440">def try_infer_partial_type_from_indexed_assignment(
    self, lvalue: IndexExpr, rvalue: Expression
) -&gt; None:
    # TODO: Should we share some of this with try_infer_partial_type?
    var = None
    if isinstance(lvalue.base, RefExpr) and isinstance(lvalue.base.node, Var):
        var = lvalue.base.node
    elif isinstance(lvalue.base, MemberExpr):
        var = self.expr_checker.get_partial_self_var(lvalue.base)
    if isinstance(var, Var):
        if isinstance(var.type, PartialType):
            type_type = var.type.type
            if type_type is None:
                return  # The partial type is None.
            partial_types = self.find_partial_types(var)
            if partial_types is None:
                return
            typename = type_type.fullname
            if (
                typename == "builtins.dict"
                or typename == "collections.OrderedDict"
                or typename == "collections.defaultdict"
            ):
                # TODO: Don't infer things twice.
                key_type = self.expr_checker.accept(lvalue.index)
                value_type = self.expr_checker.accept(rvalue)
                if (
                    is_valid_inferred_type(key_type)
                    and is_valid_inferred_type(value_type)
                    and not self.current_node_deferred
                    and not (
                        typename == "collections.defaultdict"
                        and var.type.value_type is not None
                        and not is_equivalent(value_type, var.type.value_type)
                    )
                ):
                    var.type = self.named_generic_type(typename, [key_type, value_type])
                    del partial_types[var]

</t>
<t tx="ekr.20221004064034.441">def type_requires_usage(self, typ: Type) -&gt; tuple[str, ErrorCode] | None:
    """Some types require usage in all cases. The classic example is
    an unused coroutine.

    In the case that it does require usage, returns a note to attach
    to the error message.
    """
    proper_type = get_proper_type(typ)
    if isinstance(proper_type, Instance):
        # We use different error codes for generic awaitable vs coroutine.
        # Coroutines are on by default, whereas generic awaitables are not.
        if proper_type.type.fullname == "typing.Coroutine":
            return ("Are you missing an await?", UNUSED_COROUTINE)
        if proper_type.type.get("__await__") is not None:
            return ("Are you missing an await?", UNUSED_AWAITABLE)
    return None

</t>
<t tx="ekr.20221004064034.442">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    expr_type = self.expr_checker.accept(s.expr, allow_none_return=True, always_allow_any=True)
    error_note_and_code = self.type_requires_usage(expr_type)
    if error_note_and_code:
        error_note, code = error_note_and_code
        self.fail(
            message_registry.TYPE_MUST_BE_USED.format(format_type(expr_type)), s, code=code
        )
        self.note(error_note, s, code=code)

</t>
<t tx="ekr.20221004064034.443">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    """Type check a return statement."""
    self.check_return_stmt(s)
    self.binder.unreachable()

</t>
<t tx="ekr.20221004064034.444">def check_return_stmt(self, s: ReturnStmt) -&gt; None:
    defn = self.scope.top_function()
    if defn is not None:
        if defn.is_generator:
            return_type = self.get_generator_return_type(
                self.return_types[-1], defn.is_coroutine
            )
        elif defn.is_coroutine:
            return_type = self.get_coroutine_return_type(self.return_types[-1])
        else:
            return_type = self.return_types[-1]
        return_type = get_proper_type(return_type)

        if isinstance(return_type, UninhabitedType):
            self.fail(message_registry.NO_RETURN_EXPECTED, s)
            return

        if s.expr:
            is_lambda = isinstance(self.scope.top_function(), LambdaExpr)
            declared_none_return = isinstance(return_type, NoneType)
            declared_any_return = isinstance(return_type, AnyType)

            # This controls whether or not we allow a function call that
            # returns None as the expression of this return statement.
            # E.g. `return f()` for some `f` that returns None.  We allow
            # this only if we're in a lambda or in a function that returns
            # `None` or `Any`.
            allow_none_func_call = is_lambda or declared_none_return or declared_any_return

            # Return with a value.
            typ = get_proper_type(
                self.expr_checker.accept(
                    s.expr, return_type, allow_none_return=allow_none_func_call
                )
            )

            if defn.is_async_generator:
                self.fail(message_registry.RETURN_IN_ASYNC_GENERATOR, s)
                return
            # Returning a value of type Any is always fine.
            if isinstance(typ, AnyType):
                # (Unless you asked to be warned in that case, and the
                # function is not declared to return Any)
                if (
                    self.options.warn_return_any
                    and not self.current_node_deferred
                    and not is_proper_subtype(AnyType(TypeOfAny.special_form), return_type)
                    and not (
                        defn.name in BINARY_MAGIC_METHODS
                        and is_literal_not_implemented(s.expr)
                    )
                    and not (
                        isinstance(return_type, Instance)
                        and return_type.type.fullname == "builtins.object"
                    )
                ):
                    self.msg.incorrectly_returning_any(return_type, s)
                return

            # Disallow return expressions in functions declared to return
            # None, subject to two exceptions below.
            if declared_none_return:
                # Lambdas are allowed to have None returns.
                # Functions returning a value of type None are allowed to have a None return.
                if is_lambda or isinstance(typ, NoneType):
                    return
                self.fail(message_registry.NO_RETURN_VALUE_EXPECTED, s)
            else:
                self.check_subtype(
                    subtype_label="got",
                    subtype=typ,
                    supertype_label="expected",
                    supertype=return_type,
                    context=s.expr,
                    outer_context=s,
                    msg=message_registry.INCOMPATIBLE_RETURN_VALUE_TYPE,
                )
        else:
            # Empty returns are valid in Generators with Any typed returns, but not in
            # coroutines.
            if (
                defn.is_generator
                and not defn.is_coroutine
                and isinstance(return_type, AnyType)
            ):
                return

            if isinstance(return_type, (NoneType, AnyType)):
                return

            if self.in_checked_function():
                self.fail(message_registry.RETURN_VALUE_EXPECTED, s)

</t>
<t tx="ekr.20221004064034.445">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    """Type check an if statement."""
    # This frame records the knowledge from previous if/elif clauses not being taken.
    # Fall-through to the original frame is handled explicitly in each block.
    with self.binder.frame_context(can_skip=False, conditional_frame=True, fall_through=0):
        for e, b in zip(s.expr, s.body):
            t = get_proper_type(self.expr_checker.accept(e))

            if isinstance(t, DeletedType):
                self.msg.deleted_as_rvalue(t, s)

            if_map, else_map = self.find_isinstance_check(e)

            # XXX Issue a warning if condition is always False?
            with self.binder.frame_context(can_skip=True, fall_through=2):
                self.push_type_map(if_map)
                self.accept(b)

            # XXX Issue a warning if condition is always True?
            self.push_type_map(else_map)

        with self.binder.frame_context(can_skip=False, fall_through=2):
            if s.else_body:
                self.accept(s.else_body)

</t>
<t tx="ekr.20221004064034.446">def visit_while_stmt(self, s: WhileStmt) -&gt; None:
    """Type check a while statement."""
    if_stmt = IfStmt([s.expr], [s.body], None)
    if_stmt.set_line(s)
    self.accept_loop(if_stmt, s.else_body, exit_condition=s.expr)

</t>
<t tx="ekr.20221004064034.447">def visit_operator_assignment_stmt(self, s: OperatorAssignmentStmt) -&gt; None:
    """Type check an operator assignment statement, e.g. x += 1."""
    self.try_infer_partial_generic_type_from_assignment(s.lvalue, s.rvalue, s.op)
    if isinstance(s.lvalue, MemberExpr):
        # Special case, some additional errors may be given for
        # assignments to read-only or final attributes.
        lvalue_type = self.expr_checker.visit_member_expr(s.lvalue, True)
    else:
        lvalue_type = self.expr_checker.accept(s.lvalue)
    inplace, method = infer_operator_assignment_method(lvalue_type, s.op)
    if inplace:
        # There is __ifoo__, treat as x = x.__ifoo__(y)
        rvalue_type, method_type = self.expr_checker.check_op(method, lvalue_type, s.rvalue, s)
        if not is_subtype(rvalue_type, lvalue_type):
            self.msg.incompatible_operator_assignment(s.op, s)
    else:
        # There is no __ifoo__, treat as x = x &lt;foo&gt; y
        expr = OpExpr(s.op, s.lvalue, s.rvalue)
        expr.set_line(s)
        self.check_assignment(
            lvalue=s.lvalue, rvalue=expr, infer_lvalue_type=True, new_syntax=False
        )
    self.check_final(s)

</t>
<t tx="ekr.20221004064034.448">def visit_assert_stmt(self, s: AssertStmt) -&gt; None:
    self.expr_checker.accept(s.expr)

    if isinstance(s.expr, TupleExpr) and len(s.expr.items) &gt; 0:
        self.fail(message_registry.MALFORMED_ASSERT, s)

    # If this is asserting some isinstance check, bind that type in the following code
    true_map, else_map = self.find_isinstance_check(s.expr)
    if s.msg is not None:
        self.expr_checker.analyze_cond_branch(else_map, s.msg, None)
    self.push_type_map(true_map)

</t>
<t tx="ekr.20221004064034.449">def visit_raise_stmt(self, s: RaiseStmt) -&gt; None:
    """Type check a raise statement."""
    if s.expr:
        self.type_check_raise(s.expr, s)
    if s.from_expr:
        self.type_check_raise(s.from_expr, s, optional=True)
    self.binder.unreachable()

</t>
<t tx="ekr.20221004064034.45">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--sqlite", action="store_true", default=False, help="Use a sqlite cache")
    parser.add_argument("cache_dir", help="Directory for the cache")
    parser.add_argument("diff", help="Cache diff file")
    args = parser.parse_args()

    apply_diff(args.cache_dir, args.diff, args.sqlite)


</t>
<t tx="ekr.20221004064034.450">def type_check_raise(self, e: Expression, s: RaiseStmt, optional: bool = False) -&gt; None:
    typ = get_proper_type(self.expr_checker.accept(e))
    if isinstance(typ, DeletedType):
        self.msg.deleted_as_rvalue(typ, e)
        return

    exc_type = self.named_type("builtins.BaseException")
    expected_type_items = [exc_type, TypeType(exc_type)]
    if optional:
        # This is used for `x` part in a case like `raise e from x`,
        # where we allow `raise e from None`.
        expected_type_items.append(NoneType())

    self.check_subtype(
        typ, UnionType.make_union(expected_type_items), s, message_registry.INVALID_EXCEPTION
    )

    if isinstance(typ, FunctionLike):
        # https://github.com/python/mypy/issues/11089
        self.expr_checker.check_call(typ, [], [], e)

</t>
<t tx="ekr.20221004064034.451">def visit_try_stmt(self, s: TryStmt) -&gt; None:
    """Type check a try statement."""
    # Our enclosing frame will get the result if the try/except falls through.
    # This one gets all possible states after the try block exited abnormally
    # (by exception, return, break, etc.)
    with self.binder.frame_context(can_skip=False, fall_through=0):
        # Not only might the body of the try statement exit
        # abnormally, but so might an exception handler or else
        # clause. The finally clause runs in *all* cases, so we
        # need an outer try frame to catch all intermediate states
        # in case an exception is raised during an except or else
        # clause. As an optimization, only create the outer try
        # frame when there actually is a finally clause.
        self.visit_try_without_finally(s, try_frame=bool(s.finally_body))
        if s.finally_body:
            # First we check finally_body is type safe on all abnormal exit paths
            self.accept(s.finally_body)

    if s.finally_body:
        # Then we try again for the more restricted set of options
        # that can fall through. (Why do we need to check the
        # finally clause twice? Depending on whether the finally
        # clause was reached by the try clause falling off the end
        # or exiting abnormally, after completing the finally clause
        # either flow will continue to after the entire try statement
        # or the exception/return/etc. will be processed and control
        # flow will escape. We need to check that the finally clause
        # type checks in both contexts, but only the resulting types
        # from the latter context affect the type state in the code
        # that follows the try statement.)
        if not self.binder.is_unreachable():
            self.accept(s.finally_body)

</t>
<t tx="ekr.20221004064034.452">def visit_try_without_finally(self, s: TryStmt, try_frame: bool) -&gt; None:
    """Type check a try statement, ignoring the finally block.

    On entry, the top frame should receive all flow that exits the
    try block abnormally (i.e., such that the else block does not
    execute), and its parent should receive all flow that exits
    the try block normally.
    """
    # This frame will run the else block if the try fell through.
    # In that case, control flow continues to the parent of what
    # was the top frame on entry.
    with self.binder.frame_context(can_skip=False, fall_through=2, try_frame=try_frame):
        # This frame receives exit via exception, and runs exception handlers
        with self.binder.frame_context(can_skip=False, conditional_frame=True, fall_through=2):
            # Finally, the body of the try statement
            with self.binder.frame_context(can_skip=False, fall_through=2, try_frame=True):
                self.accept(s.body)
            for i in range(len(s.handlers)):
                with self.binder.frame_context(can_skip=True, fall_through=4):
                    typ = s.types[i]
                    if typ:
                        t = self.check_except_handler_test(typ)
                        var = s.vars[i]
                        if var:
                            # To support local variables, we make this a definition line,
                            # causing assignment to set the variable's type.
                            var.is_inferred_def = True
                            self.check_assignment(var, self.temp_node(t, var))
                    self.accept(s.handlers[i])
                    var = s.vars[i]
                    if var:
                        # Exception variables are deleted.
                        # Unfortunately, this doesn't let us detect usage before the
                        # try/except block.
                        source = var.name
                        if isinstance(var.node, Var):
                            var.node.type = DeletedType(source=source)
                        self.binder.cleanse(var)
        if s.else_body:
            self.accept(s.else_body)

</t>
<t tx="ekr.20221004064034.453">def check_except_handler_test(self, n: Expression) -&gt; Type:
    """Type check an exception handler test clause."""
    typ = self.expr_checker.accept(n)

    all_types: list[Type] = []
    test_types = self.get_types_from_except_handler(typ, n)

    for ttype in get_proper_types(test_types):
        if isinstance(ttype, AnyType):
            all_types.append(ttype)
            continue

        if isinstance(ttype, FunctionLike):
            item = ttype.items[0]
            if not item.is_type_obj():
                self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
                return AnyType(TypeOfAny.from_error)
            exc_type = item.ret_type
        elif isinstance(ttype, TypeType):
            exc_type = ttype.item
        else:
            self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
            return AnyType(TypeOfAny.from_error)

        if not is_subtype(exc_type, self.named_type("builtins.BaseException")):
            self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
            return AnyType(TypeOfAny.from_error)

        all_types.append(exc_type)

    return make_simplified_union(all_types)

</t>
<t tx="ekr.20221004064034.454">def get_types_from_except_handler(self, typ: Type, n: Expression) -&gt; list[Type]:
    """Helper for check_except_handler_test to retrieve handler types."""
    typ = get_proper_type(typ)
    if isinstance(typ, TupleType):
        return typ.items
    elif isinstance(typ, UnionType):
        return [
            union_typ
            for item in typ.relevant_items()
            for union_typ in self.get_types_from_except_handler(item, n)
        ]
    elif is_named_instance(typ, "builtins.tuple"):
        # variadic tuple
        return [typ.args[0]]
    else:
        return [typ]

</t>
<t tx="ekr.20221004064034.455">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    """Type check a for statement."""
    if s.is_async:
        iterator_type, item_type = self.analyze_async_iterable_item_type(s.expr)
    else:
        iterator_type, item_type = self.analyze_iterable_item_type(s.expr)
    s.inferred_item_type = item_type
    s.inferred_iterator_type = iterator_type
    self.analyze_index_variables(s.index, item_type, s.index_type is None, s)
    self.accept_loop(s.body, s.else_body)

</t>
<t tx="ekr.20221004064034.456">def analyze_async_iterable_item_type(self, expr: Expression) -&gt; tuple[Type, Type]:
    """Analyse async iterable expression and return iterator and iterator item types."""
    echk = self.expr_checker
    iterable = echk.accept(expr)
    iterator = echk.check_method_call_by_name("__aiter__", iterable, [], [], expr)[0]
    awaitable = echk.check_method_call_by_name("__anext__", iterator, [], [], expr)[0]
    item_type = echk.check_awaitable_expr(
        awaitable, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_FOR
    )
    return iterator, item_type

</t>
<t tx="ekr.20221004064034.457">def analyze_iterable_item_type(self, expr: Expression) -&gt; tuple[Type, Type]:
    """Analyse iterable expression and return iterator and iterator item types."""
    echk = self.expr_checker
    iterable = get_proper_type(echk.accept(expr))
    iterator = echk.check_method_call_by_name("__iter__", iterable, [], [], expr)[0]

    int_type = self.analyze_range_native_int_type(expr)
    if int_type:
        return iterator, int_type

    if isinstance(iterable, TupleType):
        joined: Type = UninhabitedType()
        for item in iterable.items:
            joined = join_types(joined, item)
        return iterator, joined
    else:
        # Non-tuple iterable.
        return iterator, echk.check_method_call_by_name("__next__", iterator, [], [], expr)[0]

</t>
<t tx="ekr.20221004064034.458">def analyze_range_native_int_type(self, expr: Expression) -&gt; Type | None:
    """Try to infer native int item type from arguments to range(...).

    For example, return i64 if the expression is "range(0, i64(n))".

    Return None if unsuccessful.
    """
    if (
        isinstance(expr, CallExpr)
        and isinstance(expr.callee, RefExpr)
        and expr.callee.fullname == "builtins.range"
        and 1 &lt;= len(expr.args) &lt;= 3
        and all(kind == ARG_POS for kind in expr.arg_kinds)
    ):
        native_int: Type | None = None
        ok = True
        for arg in expr.args:
            argt = get_proper_type(self.lookup_type(arg))
            if isinstance(argt, Instance) and argt.type.fullname in (
                "mypy_extensions.i64",
                "mypy_extensions.i32",
            ):
                if native_int is None:
                    native_int = argt
                elif argt != native_int:
                    ok = False
        if ok and native_int:
            return native_int
    return None

</t>
<t tx="ekr.20221004064034.459">def analyze_container_item_type(self, typ: Type) -&gt; Type | None:
    """Check if a type is a nominal container of a union of such.

    Return the corresponding container item type.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        types: list[Type] = []
        for item in typ.items:
            c_type = self.analyze_container_item_type(item)
            if c_type:
                types.append(c_type)
        return UnionType.make_union(types)
    if isinstance(typ, Instance) and typ.type.has_base("typing.Container"):
        supertype = self.named_type("typing.Container").type
        super_instance = map_instance_to_supertype(typ, supertype)
        assert len(super_instance.args) == 1
        return super_instance.args[0]
    if isinstance(typ, TupleType):
        return self.analyze_container_item_type(tuple_fallback(typ))
    return None

</t>
<t tx="ekr.20221004064034.46">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
"""Test various combinations of generators/coroutines.

This was used to cross-check the errors in the test case
testFullCoroutineMatrix in test-data/unit/check-async-await.test.
"""

from __future__ import annotations

import sys
from types import coroutine
from typing import Any, Awaitable, Generator, Iterator

# The various things you might try to use in `await` or `yield from`.


@others
# Run main().

if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.460">def analyze_index_variables(
    self, index: Expression, item_type: Type, infer_lvalue_type: bool, context: Context
) -&gt; None:
    """Type check or infer for loop or list comprehension index vars."""
    self.check_assignment(index, self.temp_node(item_type, context), infer_lvalue_type)

</t>
<t tx="ekr.20221004064034.461">def visit_del_stmt(self, s: DelStmt) -&gt; None:
    if isinstance(s.expr, IndexExpr):
        e = s.expr
        m = MemberExpr(e.base, "__delitem__")
        m.line = s.line
        m.column = s.column
        c = CallExpr(m, [e.index], [nodes.ARG_POS], [None])
        c.line = s.line
        c.column = s.column
        self.expr_checker.accept(c, allow_none_return=True)
    else:
        s.expr.accept(self.expr_checker)
        for elt in flatten(s.expr):
            if isinstance(elt, NameExpr):
                self.binder.assign_type(
                    elt, DeletedType(source=elt.name), get_declaration(elt), False
                )

</t>
<t tx="ekr.20221004064034.462">def visit_decorator(self, e: Decorator) -&gt; None:
    for d in e.decorators:
        if isinstance(d, RefExpr):
            if d.fullname == "typing.no_type_check":
                e.var.type = AnyType(TypeOfAny.special_form)
                e.var.is_ready = True
                return

    if self.recurse_into_functions:
        with self.tscope.function_scope(e.func):
            self.check_func_item(e.func, name=e.func.name)

    # Process decorators from the inside out to determine decorated signature, which
    # may be different from the declared signature.
    sig: Type = self.function_type(e.func)
    for d in reversed(e.decorators):
        if refers_to_fullname(d, OVERLOAD_NAMES):
            self.fail(message_registry.MULTIPLE_OVERLOADS_REQUIRED, e)
            continue
        dec = self.expr_checker.accept(d)
        temp = self.temp_node(sig, context=e)
        fullname = None
        if isinstance(d, RefExpr):
            fullname = d.fullname
        # if this is a expression like @b.a where b is an object, get the type of b
        # so we can pass it the method hook in the plugins
        object_type: Type | None = None
        if fullname is None and isinstance(d, MemberExpr) and self.has_type(d.expr):
            object_type = self.lookup_type(d.expr)
            fullname = self.expr_checker.method_fullname(object_type, d.name)
        self.check_for_untyped_decorator(e.func, dec, d)
        sig, t2 = self.expr_checker.check_call(
            dec, [temp], [nodes.ARG_POS], e, callable_name=fullname, object_type=object_type
        )
    self.check_untyped_after_decorator(sig, e.func)
    sig = set_callable_name(sig, e.func)
    e.var.type = sig
    e.var.is_ready = True
    if e.func.is_property:
        if isinstance(sig, CallableType):
            if len([k for k in sig.arg_kinds if k.is_required()]) &gt; 1:
                self.msg.fail("Too many arguments for property", e)
        self.check_incompatible_property_override(e)
    # For overloaded functions we already checked override for overload as a whole.
    if e.func.info and not e.func.is_dynamic() and not e.is_overload:
        self.check_method_override(e)

    if e.func.info and e.func.name in ("__init__", "__new__"):
        if e.type and not isinstance(get_proper_type(e.type), (FunctionLike, AnyType)):
            self.fail(message_registry.BAD_CONSTRUCTOR_TYPE, e)

</t>
<t tx="ekr.20221004064034.463">def check_for_untyped_decorator(
    self, func: FuncDef, dec_type: Type, dec_expr: Expression
) -&gt; None:
    if (
        self.options.disallow_untyped_decorators
        and is_typed_callable(func.type)
        and is_untyped_decorator(dec_type)
    ):
        self.msg.typed_function_untyped_decorator(func.name, dec_expr)

</t>
<t tx="ekr.20221004064034.464">def check_incompatible_property_override(self, e: Decorator) -&gt; None:
    if not e.var.is_settable_property and e.func.info:
        name = e.func.name
        for base in e.func.info.mro[1:]:
            base_attr = base.names.get(name)
            if not base_attr:
                continue
            if (
                isinstance(base_attr.node, OverloadedFuncDef)
                and base_attr.node.is_property
                and cast(Decorator, base_attr.node.items[0]).var.is_settable_property
            ):
                self.fail(message_registry.READ_ONLY_PROPERTY_OVERRIDES_READ_WRITE, e)

</t>
<t tx="ekr.20221004064034.465">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    exceptions_maybe_suppressed = False
    for expr, target in zip(s.expr, s.target):
        if s.is_async:
            exit_ret_type = self.check_async_with_item(expr, target, s.unanalyzed_type is None)
        else:
            exit_ret_type = self.check_with_item(expr, target, s.unanalyzed_type is None)

        # Based on the return type, determine if this context manager 'swallows'
        # exceptions or not. We determine this using a heuristic based on the
        # return type of the __exit__ method -- see the discussion in
        # https://github.com/python/mypy/issues/7214 and the section about context managers
        # in https://github.com/python/typeshed/blob/master/CONTRIBUTING.md#conventions
        # for more details.

        exit_ret_type = get_proper_type(exit_ret_type)
        if is_literal_type(exit_ret_type, "builtins.bool", False):
            continue

        if is_literal_type(exit_ret_type, "builtins.bool", True) or (
            isinstance(exit_ret_type, Instance)
            and exit_ret_type.type.fullname == "builtins.bool"
            and state.strict_optional
        ):
            # Note: if strict-optional is disabled, this bool instance
            # could actually be an Optional[bool].
            exceptions_maybe_suppressed = True

    if exceptions_maybe_suppressed:
        # Treat this 'with' block in the same way we'd treat a 'try: BODY; except: pass'
        # block. This means control flow can continue after the 'with' even if the 'with'
        # block immediately returns.
        with self.binder.frame_context(can_skip=True, try_frame=True):
            self.accept(s.body)
    else:
        self.accept(s.body)

</t>
<t tx="ekr.20221004064034.466">def check_untyped_after_decorator(self, typ: Type, func: FuncDef) -&gt; None:
    if not self.options.disallow_any_decorated or self.is_stub:
        return

    if mypy.checkexpr.has_any_type(typ):
        self.msg.untyped_decorated_function(typ, func)

</t>
<t tx="ekr.20221004064034.467">def check_async_with_item(
    self, expr: Expression, target: Expression | None, infer_lvalue_type: bool
) -&gt; Type:
    echk = self.expr_checker
    ctx = echk.accept(expr)
    obj = echk.check_method_call_by_name("__aenter__", ctx, [], [], expr)[0]
    obj = echk.check_awaitable_expr(
        obj, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AENTER
    )
    if target:
        self.check_assignment(target, self.temp_node(obj, expr), infer_lvalue_type)
    arg = self.temp_node(AnyType(TypeOfAny.special_form), expr)
    res, _ = echk.check_method_call_by_name(
        "__aexit__", ctx, [arg] * 3, [nodes.ARG_POS] * 3, expr
    )
    return echk.check_awaitable_expr(
        res, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AEXIT
    )

</t>
<t tx="ekr.20221004064034.468">def check_with_item(
    self, expr: Expression, target: Expression | None, infer_lvalue_type: bool
) -&gt; Type:
    echk = self.expr_checker
    ctx = echk.accept(expr)
    obj = echk.check_method_call_by_name("__enter__", ctx, [], [], expr)[0]
    if target:
        self.check_assignment(target, self.temp_node(obj, expr), infer_lvalue_type)
    arg = self.temp_node(AnyType(TypeOfAny.special_form), expr)
    res, _ = echk.check_method_call_by_name(
        "__exit__", ctx, [arg] * 3, [nodes.ARG_POS] * 3, expr
    )
    return res

</t>
<t tx="ekr.20221004064034.469">def visit_break_stmt(self, s: BreakStmt) -&gt; None:
    self.binder.handle_break()

</t>
<t tx="ekr.20221004064034.47">def plain_generator() -&gt; Generator[str, None, int]:
    yield "a"
    return 1


</t>
<t tx="ekr.20221004064034.470">def visit_continue_stmt(self, s: ContinueStmt) -&gt; None:
    self.binder.handle_continue()
    return None

</t>
<t tx="ekr.20221004064034.471">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    with self.binder.frame_context(can_skip=False, fall_through=0):
        subject_type = get_proper_type(self.expr_checker.accept(s.subject))

        if isinstance(subject_type, DeletedType):
            self.msg.deleted_as_rvalue(subject_type, s)

        # We infer types of patterns twice. The first pass is used
        # to infer the types of capture variables. The type of a
        # capture variable may depend on multiple patterns (it
        # will be a union of all capture types). This pass ignores
        # guard expressions.
        pattern_types = [self.pattern_checker.accept(p, subject_type) for p in s.patterns]
        type_maps: list[TypeMap] = [t.captures for t in pattern_types]
        inferred_types = self.infer_variable_types_from_type_maps(type_maps)

        # The second pass narrows down the types and type checks bodies.
        for p, g, b in zip(s.patterns, s.guards, s.bodies):
            current_subject_type = self.expr_checker.narrow_type_from_binder(
                s.subject, subject_type
            )
            pattern_type = self.pattern_checker.accept(p, current_subject_type)
            with self.binder.frame_context(can_skip=True, fall_through=2):
                if b.is_unreachable or isinstance(
                    get_proper_type(pattern_type.type), UninhabitedType
                ):
                    self.push_type_map(None)
                    else_map: TypeMap = {}
                else:
                    pattern_map, else_map = conditional_types_to_typemaps(
                        s.subject, pattern_type.type, pattern_type.rest_type
                    )
                    self.remove_capture_conflicts(pattern_type.captures, inferred_types)
                    self.push_type_map(pattern_map)
                    self.push_type_map(pattern_type.captures)
                if g is not None:
                    with self.binder.frame_context(can_skip=True, fall_through=3):
                        gt = get_proper_type(self.expr_checker.accept(g))

                        if isinstance(gt, DeletedType):
                            self.msg.deleted_as_rvalue(gt, s)

                        guard_map, guard_else_map = self.find_isinstance_check(g)
                        else_map = or_conditional_maps(else_map, guard_else_map)

                        self.push_type_map(guard_map)
                        self.accept(b)
                else:
                    self.accept(b)
            self.push_type_map(else_map)

        # This is needed due to a quirk in frame_context. Without it types will stay narrowed
        # after the match.
        with self.binder.frame_context(can_skip=False, fall_through=2):
            pass

</t>
<t tx="ekr.20221004064034.472">def infer_variable_types_from_type_maps(self, type_maps: list[TypeMap]) -&gt; dict[Var, Type]:
    all_captures: dict[Var, list[tuple[NameExpr, Type]]] = defaultdict(list)
    for tm in type_maps:
        if tm is not None:
            for expr, typ in tm.items():
                if isinstance(expr, NameExpr):
                    node = expr.node
                    assert isinstance(node, Var)
                    all_captures[node].append((expr, typ))

    inferred_types: dict[Var, Type] = {}
    for var, captures in all_captures.items():
        already_exists = False
        types: list[Type] = []
        for expr, typ in captures:
            types.append(typ)

            previous_type, _, _ = self.check_lvalue(expr)
            if previous_type is not None:
                already_exists = True
                if self.check_subtype(
                    typ,
                    previous_type,
                    expr,
                    msg=message_registry.INCOMPATIBLE_TYPES_IN_CAPTURE,
                    subtype_label="pattern captures type",
                    supertype_label="variable has type",
                ):
                    inferred_types[var] = previous_type

        if not already_exists:
            new_type = UnionType.make_union(types)
            # Infer the union type at the first occurrence
            first_occurrence, _ = captures[0]
            inferred_types[var] = new_type
            self.infer_variable_type(var, first_occurrence, new_type, first_occurrence)
    return inferred_types

</t>
<t tx="ekr.20221004064034.473">def remove_capture_conflicts(self, type_map: TypeMap, inferred_types: dict[Var, Type]) -&gt; None:
    if type_map:
        for expr, typ in list(type_map.items()):
            if isinstance(expr, NameExpr):
                node = expr.node
                assert isinstance(node, Var)
                if node not in inferred_types or not is_subtype(typ, inferred_types[node]):
                    del type_map[expr]

</t>
<t tx="ekr.20221004064034.474">def make_fake_typeinfo(
    self,
    curr_module_fullname: str,
    class_gen_name: str,
    class_short_name: str,
    bases: list[Instance],
) -&gt; tuple[ClassDef, TypeInfo]:
    # Build the fake ClassDef and TypeInfo together.
    # The ClassDef is full of lies and doesn't actually contain a body.
    # Use format_bare to generate a nice name for error messages.
    # We skip fully filling out a handful of TypeInfo fields because they
    # should be irrelevant for a generated type like this:
    # is_protocol, protocol_members, is_abstract
    cdef = ClassDef(class_short_name, Block([]))
    cdef.fullname = curr_module_fullname + "." + class_gen_name
    info = TypeInfo(SymbolTable(), cdef, curr_module_fullname)
    cdef.info = info
    info.bases = bases
    calculate_mro(info)
    info.metaclass_type = info.calculate_metaclass_type()
    return cdef, info

</t>
<t tx="ekr.20221004064034.475">def intersect_instances(
    self, instances: tuple[Instance, Instance], ctx: Context
) -&gt; Instance | None:
    """Try creating an ad-hoc intersection of the given instances.

    Note that this function does *not* try and create a full-fledged
    intersection type. Instead, it returns an instance of a new ad-hoc
    subclass of the given instances.

    This is mainly useful when you need a way of representing some
    theoretical subclass of the instances the user may be trying to use
    the generated intersection can serve as a placeholder.

    This function will create a fresh subclass every time you call it,
    even if you pass in the exact same arguments. So this means calling
    `self.intersect_intersection([inst_1, inst_2], ctx)` twice will result
    in instances of two distinct subclasses of inst_1 and inst_2.

    This is by design: we want each ad-hoc intersection to be unique since
    they're supposed represent some other unknown subclass.

    Returns None if creating the subclass is impossible (e.g. due to
    MRO errors or incompatible signatures). If we do successfully create
    a subclass, its TypeInfo will automatically be added to the global scope.
    """
    curr_module = self.scope.stack[0]
    assert isinstance(curr_module, MypyFile)

    def _get_base_classes(instances_: tuple[Instance, Instance]) -&gt; list[Instance]:
        base_classes_ = []
        for inst in instances_:
            if inst.type.is_intersection:
                expanded = inst.type.bases
            else:
                expanded = [inst]

            for expanded_inst in expanded:
                base_classes_.append(expanded_inst)
        return base_classes_

    def _make_fake_typeinfo_and_full_name(
        base_classes_: list[Instance], curr_module_: MypyFile
    ) -&gt; tuple[TypeInfo, str]:
        names_list = pretty_seq([x.type.name for x in base_classes_], "and")
        short_name = f"&lt;subclass of {names_list}&gt;"
        full_name_ = gen_unique_name(short_name, curr_module_.names)
        cdef, info_ = self.make_fake_typeinfo(
            curr_module_.fullname, full_name_, short_name, base_classes_
        )
        return info_, full_name_

    base_classes = _get_base_classes(instances)
    # We use the pretty_names_list for error messages but can't
    # use it for the real name that goes into the symbol table
    # because it can have dots in it.
    pretty_names_list = pretty_seq(format_type_distinctly(*base_classes, bare=True), "and")
    try:
        info, full_name = _make_fake_typeinfo_and_full_name(base_classes, curr_module)
        with self.msg.filter_errors() as local_errors:
            self.check_multiple_inheritance(info)
        if local_errors.has_new_errors():
            # "class A(B, C)" unsafe, now check "class A(C, B)":
            base_classes = _get_base_classes(instances[::-1])
            info, full_name = _make_fake_typeinfo_and_full_name(base_classes, curr_module)
            with self.msg.filter_errors() as local_errors:
                self.check_multiple_inheritance(info)
        info.is_intersection = True
    except MroError:
        if self.should_report_unreachable_issues():
            self.msg.impossible_intersection(
                pretty_names_list, "inconsistent method resolution order", ctx
            )
        return None

    if local_errors.has_new_errors():
        if self.should_report_unreachable_issues():
            self.msg.impossible_intersection(
                pretty_names_list, "incompatible method signatures", ctx
            )
        return None

    curr_module.names[full_name] = SymbolTableNode(GDEF, info)
    return Instance(info, [], extra_attrs=instances[0].extra_attrs or instances[1].extra_attrs)

</t>
<t tx="ekr.20221004064034.476">def intersect_instance_callable(self, typ: Instance, callable_type: CallableType) -&gt; Instance:
    """Creates a fake type that represents the intersection of an Instance and a CallableType.

    It operates by creating a bare-minimum dummy TypeInfo that
    subclasses type and adds a __call__ method matching callable_type.
    """

    # In order for this to work in incremental mode, the type we generate needs to
    # have a valid fullname and a corresponding entry in a symbol table. We generate
    # a unique name inside the symbol table of the current module.
    cur_module = cast(MypyFile, self.scope.stack[0])
    gen_name = gen_unique_name(f"&lt;callable subtype of {typ.type.name}&gt;", cur_module.names)

    # Synthesize a fake TypeInfo
    short_name = format_type_bare(typ)
    cdef, info = self.make_fake_typeinfo(cur_module.fullname, gen_name, short_name, [typ])

    # Build up a fake FuncDef so we can populate the symbol table.
    func_def = FuncDef("__call__", [], Block([]), callable_type)
    func_def._fullname = cdef.fullname + ".__call__"
    func_def.info = info
    info.names["__call__"] = SymbolTableNode(MDEF, func_def)

    cur_module.names[gen_name] = SymbolTableNode(GDEF, info)

    return Instance(info, [], extra_attrs=typ.extra_attrs)

</t>
<t tx="ekr.20221004064034.477">def make_fake_callable(self, typ: Instance) -&gt; Instance:
    """Produce a new type that makes type Callable with a generic callable type."""

    fallback = self.named_type("builtins.function")
    callable_type = CallableType(
        [AnyType(TypeOfAny.explicit), AnyType(TypeOfAny.explicit)],
        [nodes.ARG_STAR, nodes.ARG_STAR2],
        [None, None],
        ret_type=AnyType(TypeOfAny.explicit),
        fallback=fallback,
        is_ellipsis_args=True,
    )

    return self.intersect_instance_callable(typ, callable_type)

</t>
<t tx="ekr.20221004064034.478">def partition_by_callable(
    self, typ: Type, unsound_partition: bool
) -&gt; tuple[list[Type], list[Type]]:
    """Partitions a type into callable subtypes and uncallable subtypes.

    Thus, given:
    `callables, uncallables = partition_by_callable(type)`

    If we assert `callable(type)` then `type` has type Union[*callables], and
    If we assert `not callable(type)` then `type` has type Union[*uncallables]

    If unsound_partition is set, assume that anything that is not
    clearly callable is in fact not callable. Otherwise we generate a
    new subtype that *is* callable.

    Guaranteed to not return [], [].
    """
    typ = get_proper_type(typ)

    if isinstance(typ, FunctionLike) or isinstance(typ, TypeType):
        return [typ], []

    if isinstance(typ, AnyType):
        return [typ], [typ]

    if isinstance(typ, NoneType):
        return [], [typ]

    if isinstance(typ, UnionType):
        callables = []
        uncallables = []
        for subtype in typ.items:
            # Use unsound_partition when handling unions in order to
            # allow the expected type discrimination.
            subcallables, subuncallables = self.partition_by_callable(
                subtype, unsound_partition=True
            )
            callables.extend(subcallables)
            uncallables.extend(subuncallables)
        return callables, uncallables

    if isinstance(typ, TypeVarType):
        # We could do better probably?
        # Refine the the type variable's bound as our type in the case that
        # callable() is true. This unfortunately loses the information that
        # the type is a type variable in that branch.
        # This matches what is done for isinstance, but it may be possible to
        # do better.
        # If it is possible for the false branch to execute, return the original
        # type to avoid losing type information.
        callables, uncallables = self.partition_by_callable(
            erase_to_union_or_bound(typ), unsound_partition
        )
        uncallables = [typ] if len(uncallables) else []
        return callables, uncallables

    # A TupleType is callable if its fallback is, but needs special handling
    # when we dummy up a new type.
    ityp = typ
    if isinstance(typ, TupleType):
        ityp = tuple_fallback(typ)

    if isinstance(ityp, Instance):
        method = ityp.type.get_method("__call__")
        if method and method.type:
            callables, uncallables = self.partition_by_callable(
                method.type, unsound_partition=False
            )
            if len(callables) and not len(uncallables):
                # Only consider the type callable if its __call__ method is
                # definitely callable.
                return [typ], []

        if not unsound_partition:
            fake = self.make_fake_callable(ityp)
            if isinstance(typ, TupleType):
                fake.type.tuple_type = TupleType(typ.items, fake)
                return [fake.type.tuple_type], [typ]
            return [fake], [typ]

    if unsound_partition:
        return [], [typ]
    else:
        # We don't know how properly make the type callable.
        return [typ], [typ]

</t>
<t tx="ekr.20221004064034.479">def conditional_callable_type_map(
    self, expr: Expression, current_type: Type | None
) -&gt; tuple[TypeMap, TypeMap]:
    """Takes in an expression and the current type of the expression.

    Returns a 2-tuple: The first element is a map from the expression to
    the restricted type if it were callable. The second element is a
    map from the expression to the type it would hold if it weren't
    callable.
    """
    if not current_type:
        return {}, {}

    if isinstance(get_proper_type(current_type), AnyType):
        return {}, {}

    callables, uncallables = self.partition_by_callable(current_type, unsound_partition=False)

    if len(callables) and len(uncallables):
        callable_map = {expr: UnionType.make_union(callables)} if len(callables) else None
        uncallable_map = (
            {expr: UnionType.make_union(uncallables)} if len(uncallables) else None
        )
        return callable_map, uncallable_map

    elif len(callables):
        return {}, None

    return None, {}

</t>
<t tx="ekr.20221004064034.48">async def plain_coroutine() -&gt; int:
    return 1


</t>
<t tx="ekr.20221004064034.480">def _is_truthy_type(self, t: ProperType) -&gt; bool:
    return (
        (
            isinstance(t, Instance)
            and bool(t.type)
            and not t.type.has_readable_member("__bool__")
            and not t.type.has_readable_member("__len__")
        )
        or isinstance(t, FunctionLike)
        or (
            isinstance(t, UnionType)
            and all(self._is_truthy_type(t) for t in get_proper_types(t.items))
        )
    )

</t>
<t tx="ekr.20221004064034.481">def _check_for_truthy_type(self, t: Type, expr: Expression) -&gt; None:
    if not state.strict_optional:
        return  # if everything can be None, all bets are off

    t = get_proper_type(t)
    if not self._is_truthy_type(t):
        return

    def format_expr_type() -&gt; str:
        typ = format_type(t)
        if isinstance(expr, MemberExpr):
            return f'Member "{expr.name}" has type {typ}'
        elif isinstance(expr, RefExpr) and expr.fullname:
            return f'"{expr.fullname}" has type {typ}'
        elif isinstance(expr, CallExpr):
            if isinstance(expr.callee, MemberExpr):
                return f'"{expr.callee.name}" returns {typ}'
            elif isinstance(expr.callee, RefExpr) and expr.callee.fullname:
                return f'"{expr.callee.fullname}" returns {typ}'
            return f"Call returns {typ}"
        else:
            return f"Expression has type {typ}"

    if isinstance(t, FunctionLike):
        self.fail(message_registry.FUNCTION_ALWAYS_TRUE.format(format_type(t)), expr)
    elif isinstance(t, UnionType):
        self.fail(message_registry.TYPE_ALWAYS_TRUE_UNIONTYPE.format(format_expr_type()), expr)
    else:
        self.fail(message_registry.TYPE_ALWAYS_TRUE.format(format_expr_type()), expr)

</t>
<t tx="ekr.20221004064034.482">def find_type_equals_check(
    self, node: ComparisonExpr, expr_indices: list[int]
) -&gt; tuple[TypeMap, TypeMap]:
    """Narrow types based on any checks of the type ``type(x) == T``

    Args:
        node: The node that might contain the comparison
        expr_indices: The list of indices of expressions in ``node`` that are being
            compared
    """

    def is_type_call(expr: CallExpr) -&gt; bool:
        """Is expr a call to type with one argument?"""
        return refers_to_fullname(expr.callee, "builtins.type") and len(expr.args) == 1

    # exprs that are being passed into type
    exprs_in_type_calls: list[Expression] = []
    # type that is being compared to type(expr)
    type_being_compared: list[TypeRange] | None = None
    # whether the type being compared to is final
    is_final = False

    for index in expr_indices:
        expr = node.operands[index]

        if isinstance(expr, CallExpr) and is_type_call(expr):
            exprs_in_type_calls.append(expr.args[0])
        else:
            current_type = self.get_isinstance_type(expr)
            if current_type is None:
                continue
            if type_being_compared is not None:
                # It doesn't really make sense to have several types being
                # compared to the output of type (like type(x) == int == str)
                # because whether that's true is solely dependent on what the
                # types being compared are, so we don't try to narrow types any
                # further because we can't really get any information about the
                # type of x from that check
                return {}, {}
            else:
                if isinstance(expr, RefExpr) and isinstance(expr.node, TypeInfo):
                    is_final = expr.node.is_final
                type_being_compared = current_type

    if not exprs_in_type_calls:
        return {}, {}

    if_maps: list[TypeMap] = []
    else_maps: list[TypeMap] = []
    for expr in exprs_in_type_calls:
        current_if_type, current_else_type = self.conditional_types_with_intersection(
            self.lookup_type(expr), type_being_compared, expr
        )
        current_if_map, current_else_map = conditional_types_to_typemaps(
            expr, current_if_type, current_else_type
        )
        if_maps.append(current_if_map)
        else_maps.append(current_else_map)

    def combine_maps(list_maps: list[TypeMap]) -&gt; TypeMap:
        """Combine all typemaps in list_maps into one typemap"""
        result_map = {}
        for d in list_maps:
            if d is not None:
                result_map.update(d)
        return result_map

    if_map = combine_maps(if_maps)
    # type(x) == T is only true when x has the same type as T, meaning
    # that it can be false if x is an instance of a subclass of T. That means
    # we can't do any narrowing in the else case unless T is final, in which
    # case T can't be subclassed
    if is_final:
        else_map = combine_maps(else_maps)
    else:
        else_map = {}
    return if_map, else_map

</t>
<t tx="ekr.20221004064034.483">def find_isinstance_check(self, node: Expression) -&gt; tuple[TypeMap, TypeMap]:
    """Find any isinstance checks (within a chain of ands).  Includes
    implicit and explicit checks for None and calls to callable.
    Also includes TypeGuard functions.

    Return value is a map of variables to their types if the condition
    is true and a map of variables to their types if the condition is false.

    If either of the values in the tuple is None, then that particular
    branch can never occur.

    May return {}, {}.
    Can return None, None in situations involving NoReturn.
    """
    if_map, else_map = self.find_isinstance_check_helper(node)
    new_if_map = self.propagate_up_typemap_info(if_map)
    new_else_map = self.propagate_up_typemap_info(else_map)
    return new_if_map, new_else_map

</t>
<t tx="ekr.20221004064034.484">def find_isinstance_check_helper(self, node: Expression) -&gt; tuple[TypeMap, TypeMap]:
    if is_true_literal(node):
        return {}, None
    if is_false_literal(node):
        return None, {}

    if isinstance(node, CallExpr) and len(node.args) != 0:
        expr = collapse_walrus(node.args[0])
        if refers_to_fullname(node.callee, "builtins.isinstance"):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                return conditional_types_to_typemaps(
                    expr,
                    *self.conditional_types_with_intersection(
                        self.lookup_type(expr), self.get_isinstance_type(node.args[1]), expr
                    ),
                )
        elif refers_to_fullname(node.callee, "builtins.issubclass"):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                return self.infer_issubclass_maps(node, expr)
        elif refers_to_fullname(node.callee, "builtins.callable"):
            if len(node.args) != 1:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                vartype = self.lookup_type(expr)
                return self.conditional_callable_type_map(expr, vartype)
        elif refers_to_fullname(node.callee, "builtins.hasattr"):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            attr = try_getting_str_literals(node.args[1], self.lookup_type(node.args[1]))
            if literal(expr) == LITERAL_TYPE and attr and len(attr) == 1:
                return self.hasattr_type_maps(expr, self.lookup_type(expr), attr[0])
        elif isinstance(node.callee, RefExpr):
            if node.callee.type_guard is not None:
                # TODO: Follow keyword args or *args, **kwargs
                if node.arg_kinds[0] != nodes.ARG_POS:
                    self.fail(message_registry.TYPE_GUARD_POS_ARG_REQUIRED, node)
                    return {}, {}
                if literal(expr) == LITERAL_TYPE:
                    # Note: we wrap the target type, so that we can special case later.
                    # Namely, for isinstance() we use a normal meet, while TypeGuard is
                    # considered "always right" (i.e. even if the types are not overlapping).
                    # Also note that a care must be taken to unwrap this back at read places
                    # where we use this to narrow down declared type.
                    return {expr: TypeGuardedType(node.callee.type_guard)}, {}
    elif isinstance(node, ComparisonExpr):
        # Step 1: Obtain the types of each operand and whether or not we can
        # narrow their types. (For example, we shouldn't try narrowing the
        # types of literal string or enum expressions).

        operands = [collapse_walrus(x) for x in node.operands]
        operand_types = []
        narrowable_operand_index_to_hash = {}
        for i, expr in enumerate(operands):
            if not self.has_type(expr):
                return {}, {}
            expr_type = self.lookup_type(expr)
            operand_types.append(expr_type)

            if (
                literal(expr) == LITERAL_TYPE
                and not is_literal_none(expr)
                and not self.is_literal_enum(expr)
            ):
                h = literal_hash(expr)
                if h is not None:
                    narrowable_operand_index_to_hash[i] = h

        # Step 2: Group operands chained by either the 'is' or '==' operands
        # together. For all other operands, we keep them in groups of size 2.
        # So the expression:
        #
        #   x0 == x1 == x2 &lt; x3 &lt; x4 is x5 is x6 is not x7 is not x8
        #
        # ...is converted into the simplified operator list:
        #
        #  [("==", [0, 1, 2]), ("&lt;", [2, 3]), ("&lt;", [3, 4]),
        #   ("is", [4, 5, 6]), ("is not", [6, 7]), ("is not", [7, 8])]
        #
        # We group identity/equality expressions so we can propagate information
        # we discover about one operand across the entire chain. We don't bother
        # handling 'is not' and '!=' chains in a special way: those are very rare
        # in practice.

        simplified_operator_list = group_comparison_operands(
            node.pairwise(), narrowable_operand_index_to_hash, {"==", "is"}
        )

        # Step 3: Analyze each group and infer more precise type maps for each
        # assignable operand, if possible. We combine these type maps together
        # in the final step.

        partial_type_maps = []
        for operator, expr_indices in simplified_operator_list:
            if operator in {"is", "is not", "==", "!="}:
                # is_valid_target:
                #   Controls which types we're allowed to narrow exprs to. Note that
                #   we cannot use 'is_literal_type_like' in both cases since doing
                #   'x = 10000 + 1; x is 10001' is not always True in all Python
                #   implementations.
                #
                # coerce_only_in_literal_context:
                #   If true, coerce types into literal types only if one or more of
                #   the provided exprs contains an explicit Literal type. This could
                #   technically be set to any arbitrary value, but it seems being liberal
                #   with narrowing when using 'is' and conservative when using '==' seems
                #   to break the least amount of real-world code.
                #
                # should_narrow_by_identity:
                #   Set to 'false' only if the user defines custom __eq__ or __ne__ methods
                #   that could cause identity-based narrowing to produce invalid results.
                if operator in {"is", "is not"}:
                    is_valid_target: Callable[[Type], bool] = is_singleton_type
                    coerce_only_in_literal_context = False
                    should_narrow_by_identity = True
                else:

                    def is_exactly_literal_type(t: Type) -&gt; bool:
                        return isinstance(get_proper_type(t), LiteralType)

                    def has_no_custom_eq_checks(t: Type) -&gt; bool:
                        return not custom_special_method(
                            t, "__eq__", check_all=False
                        ) and not custom_special_method(t, "__ne__", check_all=False)

                    is_valid_target = is_exactly_literal_type
                    coerce_only_in_literal_context = True

                    expr_types = [operand_types[i] for i in expr_indices]
                    should_narrow_by_identity = all(map(has_no_custom_eq_checks, expr_types))

                if_map: TypeMap = {}
                else_map: TypeMap = {}
                if should_narrow_by_identity:
                    if_map, else_map = self.refine_identity_comparison_expression(
                        operands,
                        operand_types,
                        expr_indices,
                        narrowable_operand_index_to_hash.keys(),
                        is_valid_target,
                        coerce_only_in_literal_context,
                    )

                # Strictly speaking, we should also skip this check if the objects in the expr
                # chain have custom __eq__ or __ne__ methods. But we (maybe optimistically)
                # assume nobody would actually create a custom objects that considers itself
                # equal to None.
                if if_map == {} and else_map == {}:
                    if_map, else_map = self.refine_away_none_in_comparison(
                        operands,
                        operand_types,
                        expr_indices,
                        narrowable_operand_index_to_hash.keys(),
                    )

                # If we haven't been able to narrow types yet, we might be dealing with a
                # explicit type(x) == some_type check
                if if_map == {} and else_map == {}:
                    if_map, else_map = self.find_type_equals_check(node, expr_indices)
            elif operator in {"in", "not in"}:
                assert len(expr_indices) == 2
                left_index, right_index = expr_indices
                if left_index not in narrowable_operand_index_to_hash:
                    continue

                item_type = operand_types[left_index]
                collection_type = operand_types[right_index]

                # We only try and narrow away 'None' for now
                if not is_optional(item_type):
                    continue

                collection_item_type = get_proper_type(builtin_item_type(collection_type))
                if collection_item_type is None or is_optional(collection_item_type):
                    continue
                if (
                    isinstance(collection_item_type, Instance)
                    and collection_item_type.type.fullname == "builtins.object"
                ):
                    continue
                if is_overlapping_erased_types(item_type, collection_item_type):
                    if_map, else_map = {operands[left_index]: remove_optional(item_type)}, {}
                else:
                    continue
            else:
                if_map = {}
                else_map = {}

            if operator in {"is not", "!=", "not in"}:
                if_map, else_map = else_map, if_map

            partial_type_maps.append((if_map, else_map))

        return reduce_conditional_maps(partial_type_maps)
    elif isinstance(node, AssignmentExpr):
        if_map = {}
        else_map = {}

        if_assignment_map, else_assignment_map = self.find_isinstance_check(node.target)

        if if_assignment_map is not None:
            if_map.update(if_assignment_map)
        if else_assignment_map is not None:
            else_map.update(else_assignment_map)

        if_condition_map, else_condition_map = self.find_isinstance_check(node.value)

        if if_condition_map is not None:
            if_map.update(if_condition_map)
        if else_condition_map is not None:
            else_map.update(else_condition_map)

        return (
            (None if if_assignment_map is None or if_condition_map is None else if_map),
            (None if else_assignment_map is None or else_condition_map is None else else_map),
        )
    elif isinstance(node, OpExpr) and node.op == "and":
        left_if_vars, left_else_vars = self.find_isinstance_check(node.left)
        right_if_vars, right_else_vars = self.find_isinstance_check(node.right)

        # (e1 and e2) is true if both e1 and e2 are true,
        # and false if at least one of e1 and e2 is false.
        return (
            and_conditional_maps(left_if_vars, right_if_vars),
            or_conditional_maps(left_else_vars, right_else_vars),
        )
    elif isinstance(node, OpExpr) and node.op == "or":
        left_if_vars, left_else_vars = self.find_isinstance_check(node.left)
        right_if_vars, right_else_vars = self.find_isinstance_check(node.right)

        # (e1 or e2) is true if at least one of e1 or e2 is true,
        # and false if both e1 and e2 are false.
        return (
            or_conditional_maps(left_if_vars, right_if_vars),
            and_conditional_maps(left_else_vars, right_else_vars),
        )
    elif isinstance(node, UnaryExpr) and node.op == "not":
        left, right = self.find_isinstance_check(node.expr)
        return right, left

    # Restrict the type of the variable to True-ish/False-ish in the if and else branches
    # respectively
    original_vartype = self.lookup_type(node)
    self._check_for_truthy_type(original_vartype, node)
    vartype = try_expanding_sum_type_to_union(original_vartype, "builtins.bool")

    if_type = true_only(vartype)
    else_type = false_only(vartype)
    if_map = {node: if_type} if not isinstance(if_type, UninhabitedType) else None
    else_map = {node: else_type} if not isinstance(else_type, UninhabitedType) else None
    return if_map, else_map

</t>
<t tx="ekr.20221004064034.485">def propagate_up_typemap_info(self, new_types: TypeMap) -&gt; TypeMap:
    """Attempts refining parent expressions of any MemberExpr or IndexExprs in new_types.

    Specifically, this function accepts two mappings of expression to original types:
    the original mapping (existing_types), and a new mapping (new_types) intended to
    update the original.

    This function iterates through new_types and attempts to use the information to try
    refining any parent types that happen to be unions.

    For example, suppose there are two types "A = Tuple[int, int]" and "B = Tuple[str, str]".
    Next, suppose that 'new_types' specifies the expression 'foo[0]' has a refined type
    of 'int' and that 'foo' was previously deduced to be of type Union[A, B].

    Then, this function will observe that since A[0] is an int and B[0] is not, the type of
    'foo' can be further refined from Union[A, B] into just B.

    We perform this kind of "parent narrowing" for member lookup expressions and indexing
    expressions into tuples, namedtuples, and typeddicts. We repeat this narrowing
    recursively if the parent is also a "lookup expression". So for example, if we have
    the expression "foo['bar'].baz[0]", we'd potentially end up refining types for the
    expressions "foo", "foo['bar']", and "foo['bar'].baz".

    We return the newly refined map. This map is guaranteed to be a superset of 'new_types'.
    """
    if new_types is None:
        return None
    output_map = {}
    for expr, expr_type in new_types.items():
        # The original inferred type should always be present in the output map, of course
        output_map[expr] = expr_type

        # Next, try using this information to refine the parent types, if applicable.
        new_mapping = self.refine_parent_types(expr, expr_type)
        for parent_expr, proposed_parent_type in new_mapping.items():
            # We don't try inferring anything if we've already inferred something for
            # the parent expression.
            # TODO: Consider picking the narrower type instead of always discarding this?
            if parent_expr in new_types:
                continue
            output_map[parent_expr] = proposed_parent_type
    return output_map

</t>
<t tx="ekr.20221004064034.486">def refine_parent_types(self, expr: Expression, expr_type: Type) -&gt; Mapping[Expression, Type]:
    """Checks if the given expr is a 'lookup operation' into a union and iteratively refines
    the parent types based on the 'expr_type'.

    For example, if 'expr' is an expression like 'a.b.c.d', we'll potentially return refined
    types for expressions 'a', 'a.b', and 'a.b.c'.

    For more details about what a 'lookup operation' is and how we use the expr_type to refine
    the parent types of lookup_expr, see the docstring in 'propagate_up_typemap_info'.
    """
    output: dict[Expression, Type] = {}

    # Note: parent_expr and parent_type are progressively refined as we crawl up the
    # parent lookup chain.
    while True:
        # First, check if this expression is one that's attempting to
        # "lookup" some key in the parent type. If so, save the parent type
        # and create function that will try replaying the same lookup
        # operation against arbitrary types.
        if isinstance(expr, MemberExpr):
            parent_expr = collapse_walrus(expr.expr)
            parent_type = self.lookup_type_or_none(parent_expr)
            member_name = expr.name

            def replay_lookup(new_parent_type: ProperType) -&gt; Type | None:
                with self.msg.filter_errors() as w:
                    member_type = analyze_member_access(
                        name=member_name,
                        typ=new_parent_type,
                        context=parent_expr,
                        is_lvalue=False,
                        is_super=False,
                        is_operator=False,
                        msg=self.msg,
                        original_type=new_parent_type,
                        chk=self,
                        in_literal_context=False,
                    )
                if w.has_new_errors():
                    return None
                else:
                    return member_type

        elif isinstance(expr, IndexExpr):
            parent_expr = collapse_walrus(expr.base)
            parent_type = self.lookup_type_or_none(parent_expr)

            index_type = self.lookup_type_or_none(expr.index)
            if index_type is None:
                return output

            str_literals = try_getting_str_literals_from_type(index_type)
            if str_literals is not None:
                # Refactoring these two indexing replay functions is surprisingly
                # tricky -- see https://github.com/python/mypy/pull/7917, which
                # was blocked by https://github.com/mypyc/mypyc/issues/586
                def replay_lookup(new_parent_type: ProperType) -&gt; Type | None:
                    if not isinstance(new_parent_type, TypedDictType):
                        return None
                    try:
                        assert str_literals is not None
                        member_types = [new_parent_type.items[key] for key in str_literals]
                    except KeyError:
                        return None
                    return make_simplified_union(member_types)

            else:
                int_literals = try_getting_int_literals_from_type(index_type)
                if int_literals is not None:

                    def replay_lookup(new_parent_type: ProperType) -&gt; Type | None:
                        if not isinstance(new_parent_type, TupleType):
                            return None
                        try:
                            assert int_literals is not None
                            member_types = [new_parent_type.items[key] for key in int_literals]
                        except IndexError:
                            return None
                        return make_simplified_union(member_types)

                else:
                    return output
        else:
            return output

        # If we somehow didn't previously derive the parent type, abort completely
        # with what we have so far: something went wrong at an earlier stage.
        if parent_type is None:
            return output

        # We currently only try refining the parent type if it's a Union.
        # If not, there's no point in trying to refine any further parents
        # since we have no further information we can use to refine the lookup
        # chain, so we end early as an optimization.
        parent_type = get_proper_type(parent_type)
        if not isinstance(parent_type, UnionType):
            return output

        # Take each element in the parent union and replay the original lookup procedure
        # to figure out which parents are compatible.
        new_parent_types = []
        for item in flatten_nested_unions(parent_type.items):
            member_type = replay_lookup(get_proper_type(item))
            if member_type is None:
                # We were unable to obtain the member type. So, we give up on refining this
                # parent type entirely and abort.
                return output

            if is_overlapping_types(member_type, expr_type):
                new_parent_types.append(item)

        # If none of the parent types overlap (if we derived an empty union), something
        # went wrong. We should never hit this case, but deriving the uninhabited type or
        # reporting an error both seem unhelpful. So we abort.
        if not new_parent_types:
            return output

        expr = parent_expr
        expr_type = output[parent_expr] = make_simplified_union(new_parent_types)

</t>
<t tx="ekr.20221004064034.487">def refine_identity_comparison_expression(
    self,
    operands: list[Expression],
    operand_types: list[Type],
    chain_indices: list[int],
    narrowable_operand_indices: AbstractSet[int],
    is_valid_target: Callable[[ProperType], bool],
    coerce_only_in_literal_context: bool,
) -&gt; tuple[TypeMap, TypeMap]:
    """Produce conditional type maps refining expressions by an identity/equality comparison.

    The 'operands' and 'operand_types' lists should be the full list of operands used
    in the overall comparison expression. The 'chain_indices' list is the list of indices
    actually used within this identity comparison chain.

    So if we have the expression:

        a &lt;= b is c is d &lt;= e

    ...then 'operands' and 'operand_types' would be lists of length 5 and 'chain_indices'
    would be the list [1, 2, 3].

    The 'narrowable_operand_indices' parameter is the set of all indices we are allowed
    to refine the types of: that is, all operands that will potentially be a part of
    the output TypeMaps.

    Although this function could theoretically try setting the types of the operands
    in the chains to the meet, doing that causes too many issues in real-world code.
    Instead, we use 'is_valid_target' to identify which of the given chain types
    we could plausibly use as the refined type for the expressions in the chain.

    Similarly, 'coerce_only_in_literal_context' controls whether we should try coercing
    expressions in the chain to a Literal type. Performing this coercion is sometimes
    too aggressive of a narrowing, depending on context.
    """
    should_coerce = True
    if coerce_only_in_literal_context:
        should_coerce = any(is_literal_type_like(operand_types[i]) for i in chain_indices)

    target: Type | None = None
    possible_target_indices = []
    for i in chain_indices:
        expr_type = operand_types[i]
        if should_coerce:
            expr_type = coerce_to_literal(expr_type)
        if not is_valid_target(get_proper_type(expr_type)):
            continue
        if target and not is_same_type(target, expr_type):
            # We have multiple disjoint target types. So the 'if' branch
            # must be unreachable.
            return None, {}
        target = expr_type
        possible_target_indices.append(i)

    # There's nothing we can currently infer if none of the operands are valid targets,
    # so we end early and infer nothing.
    if target is None:
        return {}, {}

    # If possible, use an unassignable expression as the target.
    # We skip refining the type of the target below, so ideally we'd
    # want to pick an expression we were going to skip anyways.
    singleton_index = -1
    for i in possible_target_indices:
        if i not in narrowable_operand_indices:
            singleton_index = i

    # But if none of the possible singletons are unassignable ones, we give up
    # and arbitrarily pick the last item, mostly because other parts of the
    # type narrowing logic bias towards picking the rightmost item and it'd be
    # nice to stay consistent.
    #
    # That said, it shouldn't matter which index we pick. For example, suppose we
    # have this if statement, where 'x' and 'y' both have singleton types:
    #
    #     if x is y:
    #         reveal_type(x)
    #         reveal_type(y)
    #     else:
    #         reveal_type(x)
    #         reveal_type(y)
    #
    # At this point, 'x' and 'y' *must* have the same singleton type: we would have
    # ended early in the first for-loop in this function if they weren't.
    #
    # So, we should always get the same result in the 'if' case no matter which
    # index we pick. And while we do end up getting different results in the 'else'
    # case depending on the index (e.g. if we pick 'y', then its type stays the same
    # while 'x' is narrowed to '&lt;uninhabited&gt;'), this distinction is also moot: mypy
    # currently will just mark the whole branch as unreachable if either operand is
    # narrowed to &lt;uninhabited&gt;.
    if singleton_index == -1:
        singleton_index = possible_target_indices[-1]

    sum_type_name = None
    target = get_proper_type(target)
    if isinstance(target, LiteralType) and (
        target.is_enum_literal() or isinstance(target.value, bool)
    ):
        sum_type_name = target.fallback.type.fullname

    target_type = [TypeRange(target, is_upper_bound=False)]

    partial_type_maps = []
    for i in chain_indices:
        # If we try refining a type against itself, conditional_type_map
        # will end up assuming that the 'else' branch is unreachable. This is
        # typically not what we want: generally the user will intend for the
        # target type to be some fixed 'sentinel' value and will want to refine
        # the other exprs against this one instead.
        if i == singleton_index:
            continue

        # Naturally, we can't refine operands which are not permitted to be refined.
        if i not in narrowable_operand_indices:
            continue

        expr = operands[i]
        expr_type = coerce_to_literal(operand_types[i])

        if sum_type_name is not None:
            expr_type = try_expanding_sum_type_to_union(expr_type, sum_type_name)

        # We intentionally use 'conditional_types' directly here instead of
        # 'self.conditional_types_with_intersection': we only compute ad-hoc
        # intersections when working with pure instances.
        types = conditional_types(expr_type, target_type)
        partial_type_maps.append(conditional_types_to_typemaps(expr, *types))

    return reduce_conditional_maps(partial_type_maps)

</t>
<t tx="ekr.20221004064034.488">def refine_away_none_in_comparison(
    self,
    operands: list[Expression],
    operand_types: list[Type],
    chain_indices: list[int],
    narrowable_operand_indices: AbstractSet[int],
) -&gt; tuple[TypeMap, TypeMap]:
    """Produces conditional type maps refining away None in an identity/equality chain.

    For more details about what the different arguments mean, see the
    docstring of 'refine_identity_comparison_expression' up above.
    """
    non_optional_types = []
    for i in chain_indices:
        typ = operand_types[i]
        if not is_optional(typ):
            non_optional_types.append(typ)

    # Make sure we have a mixture of optional and non-optional types.
    if len(non_optional_types) == 0 or len(non_optional_types) == len(chain_indices):
        return {}, {}

    if_map = {}
    for i in narrowable_operand_indices:
        expr_type = operand_types[i]
        if not is_optional(expr_type):
            continue
        if any(is_overlapping_erased_types(expr_type, t) for t in non_optional_types):
            if_map[operands[i]] = remove_optional(expr_type)

    return if_map, {}

</t>
<t tx="ekr.20221004064034.489">#
# Helpers
#
@overload
def check_subtype(
    self,
    subtype: Type,
    supertype: Type,
    context: Context,
    msg: str,
    subtype_label: str | None = None,
    supertype_label: str | None = None,
    *,
    notes: list[str] | None = None,
    code: ErrorCode | None = None,
    outer_context: Context | None = None,
) -&gt; bool:
    ...

</t>
<t tx="ekr.20221004064034.49">@coroutine
def decorated_generator() -&gt; Generator[str, None, int]:
    yield "a"
    return 1


</t>
<t tx="ekr.20221004064034.490">@overload
def check_subtype(
    self,
    subtype: Type,
    supertype: Type,
    context: Context,
    msg: ErrorMessage,
    subtype_label: str | None = None,
    supertype_label: str | None = None,
    *,
    notes: list[str] | None = None,
    outer_context: Context | None = None,
) -&gt; bool:
    ...

</t>
<t tx="ekr.20221004064034.491">def check_subtype(
    self,
    subtype: Type,
    supertype: Type,
    context: Context,
    msg: str | ErrorMessage,
    subtype_label: str | None = None,
    supertype_label: str | None = None,
    *,
    notes: list[str] | None = None,
    code: ErrorCode | None = None,
    outer_context: Context | None = None,
) -&gt; bool:
    """Generate an error if the subtype is not compatible with supertype."""
    if is_subtype(subtype, supertype, options=self.options):
        return True

    if isinstance(msg, str):
        msg = ErrorMessage(msg, code=code)

    orig_subtype = subtype
    subtype = get_proper_type(subtype)
    orig_supertype = supertype
    supertype = get_proper_type(supertype)
    if self.msg.try_report_long_tuple_assignment_error(
        subtype, supertype, context, msg, subtype_label, supertype_label
    ):
        return False
    extra_info: list[str] = []
    note_msg = ""
    notes = notes or []
    if subtype_label is not None or supertype_label is not None:
        subtype_str, supertype_str = format_type_distinctly(orig_subtype, orig_supertype)
        if subtype_label is not None:
            extra_info.append(subtype_label + " " + subtype_str)
        if supertype_label is not None:
            extra_info.append(supertype_label + " " + supertype_str)
        note_msg = make_inferred_type_note(
            outer_context or context, subtype, supertype, supertype_str
        )
        if isinstance(subtype, Instance) and isinstance(supertype, Instance):
            notes = append_invariance_notes(notes, subtype, supertype)
    if extra_info:
        msg = msg.with_additional_msg(" (" + ", ".join(extra_info) + ")")

    self.fail(msg, context)
    for note in notes:
        self.msg.note(note, context, code=msg.code)
    if note_msg:
        self.note(note_msg, context, code=msg.code)
    self.msg.maybe_note_concatenate_pos_args(subtype, supertype, context, code=msg.code)
    if (
        isinstance(supertype, Instance)
        and supertype.type.is_protocol
        and isinstance(subtype, (Instance, TupleType, TypedDictType))
    ):
        self.msg.report_protocol_problems(subtype, supertype, context, code=msg.code)
    if isinstance(supertype, CallableType) and isinstance(subtype, Instance):
        call = find_member("__call__", subtype, subtype, is_operator=True)
        if call:
            self.msg.note_call(subtype, call, context, code=msg.code)
    if isinstance(subtype, (CallableType, Overloaded)) and isinstance(supertype, Instance):
        if supertype.type.is_protocol and supertype.type.protocol_members == ["__call__"]:
            call = find_member("__call__", supertype, subtype, is_operator=True)
            assert call is not None
            self.msg.note_call(supertype, call, context, code=msg.code)
    self.check_possible_missing_await(subtype, supertype, context)
    return False

</t>
<t tx="ekr.20221004064034.492">def get_precise_awaitable_type(self, typ: Type, local_errors: ErrorWatcher) -&gt; Type | None:
    """If type implements Awaitable[X] with non-Any X, return X.

    In all other cases return None. This method must be called in context
    of local_errors.
    """
    if isinstance(get_proper_type(typ), PartialType):
        # Partial types are special, ignore them here.
        return None
    try:
        aw_type = self.expr_checker.check_awaitable_expr(
            typ, Context(), "", ignore_binder=True
        )
    except KeyError:
        # This is a hack to speed up tests by not including Awaitable in all typing stubs.
        return None
    if local_errors.has_new_errors():
        return None
    if isinstance(get_proper_type(aw_type), (AnyType, UnboundType)):
        return None
    return aw_type

</t>
<t tx="ekr.20221004064034.493">@contextmanager
def checking_await_set(self) -&gt; Iterator[None]:
    self.checking_missing_await = True
    try:
        yield
    finally:
        self.checking_missing_await = False

</t>
<t tx="ekr.20221004064034.494">def check_possible_missing_await(
    self, subtype: Type, supertype: Type, context: Context
) -&gt; None:
    """Check if the given type becomes a subtype when awaited."""
    if self.checking_missing_await:
        # Avoid infinite recursion.
        return
    with self.checking_await_set(), self.msg.filter_errors() as local_errors:
        aw_type = self.get_precise_awaitable_type(subtype, local_errors)
        if aw_type is None:
            return
        if not self.check_subtype(
            aw_type, supertype, context, msg=message_registry.INCOMPATIBLE_TYPES
        ):
            return
    self.msg.possible_missing_await(context)

</t>
<t tx="ekr.20221004064034.495">def contains_none(self, t: Type) -&gt; bool:
    t = get_proper_type(t)
    return (
        isinstance(t, NoneType)
        or (isinstance(t, UnionType) and any(self.contains_none(ut) for ut in t.items))
        or (isinstance(t, TupleType) and any(self.contains_none(tt) for tt in t.items))
        or (
            isinstance(t, Instance)
            and bool(t.args)
            and any(self.contains_none(it) for it in t.args)
        )
    )

</t>
<t tx="ekr.20221004064034.496">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with given name and implicit Any type args.

    For example, named_type('builtins.object') produces the 'object' type.
    """
    # Assume that the name refers to a type.
    sym = self.lookup_qualified(name)
    node = sym.node
    if isinstance(node, TypeAlias):
        assert isinstance(node.target, Instance)  # type: ignore[misc]
        node = node.target.type
    assert isinstance(node, TypeInfo)
    any_type = AnyType(TypeOfAny.from_omitted_generics)
    return Instance(node, [any_type] * len(node.defn.type_vars))

</t>
<t tx="ekr.20221004064034.497">def named_generic_type(self, name: str, args: list[Type]) -&gt; Instance:
    """Return an instance with the given name and type arguments.

    Assume that the number of arguments is correct.  Assume that
    the name refers to a compatible generic type.
    """
    info = self.lookup_typeinfo(name)
    args = [remove_instance_last_known_values(arg) for arg in args]
    # TODO: assert len(args) == len(info.defn.type_vars)
    return Instance(info, args)

</t>
<t tx="ekr.20221004064034.498">def lookup_typeinfo(self, fullname: str) -&gt; TypeInfo:
    # Assume that the name refers to a class.
    sym = self.lookup_qualified(fullname)
    node = sym.node
    assert isinstance(node, TypeInfo)
    return node

</t>
<t tx="ekr.20221004064034.499">def type_type(self) -&gt; Instance:
    """Return instance type 'type'."""
    return self.named_type("builtins.type")

</t>
<t tx="ekr.20221004064034.5"># This function name is special to pytest.  See
# http://doc.pytest.org/en/latest/writing_plugins.html#initialization-command-line-and-configuration-hooks
def pytest_addoption(parser) -&gt; None:
    parser.addoption(
        "--bench", action="store_true", default=False, help="Enable the benchmark test runs"
    )
</t>
<t tx="ekr.20221004064034.50">@coroutine
async def decorated_coroutine() -&gt; int:
    return 1


</t>
<t tx="ekr.20221004064034.500">def str_type(self) -&gt; Instance:
    """Return instance type 'str'."""
    return self.named_type("builtins.str")

</t>
<t tx="ekr.20221004064034.501">def store_type(self, node: Expression, typ: Type) -&gt; None:
    """Store the type of a node in the type map."""
    self._type_maps[-1][node] = typ

</t>
<t tx="ekr.20221004064034.502">def has_type(self, node: Expression) -&gt; bool:
    for m in reversed(self._type_maps):
        if node in m:
            return True
    return False

</t>
<t tx="ekr.20221004064034.503">def lookup_type_or_none(self, node: Expression) -&gt; Type | None:
    for m in reversed(self._type_maps):
        if node in m:
            return m[node]
    return None

</t>
<t tx="ekr.20221004064034.504">def lookup_type(self, node: Expression) -&gt; Type:
    for m in reversed(self._type_maps):
        t = m.get(node)
        if t is not None:
            return t
    raise KeyError(node)

</t>
<t tx="ekr.20221004064034.505">def store_types(self, d: dict[Expression, Type]) -&gt; None:
    self._type_maps[-1].update(d)

</t>
<t tx="ekr.20221004064034.506">@contextmanager
def local_type_map(self) -&gt; Iterator[dict[Expression, Type]]:
    """Store inferred types into a temporary type map (returned).

    This can be used to perform type checking "experiments" without
    affecting exported types (which are used by mypyc).
    """
    temp_type_map: dict[Expression, Type] = {}
    self._type_maps.append(temp_type_map)
    yield temp_type_map
    self._type_maps.pop()

</t>
<t tx="ekr.20221004064034.507">def in_checked_function(self) -&gt; bool:
    """Should we type-check the current function?

    - Yes if --check-untyped-defs is set.
    - Yes outside functions.
    - Yes in annotated functions.
    - No otherwise.
    """
    return (
        self.options.check_untyped_defs or not self.dynamic_funcs or not self.dynamic_funcs[-1]
    )

</t>
<t tx="ekr.20221004064034.508">def lookup(self, name: str) -&gt; SymbolTableNode:
    """Look up a definition from the symbol table with the given name."""
    if name in self.globals:
        return self.globals[name]
    else:
        b = self.globals.get("__builtins__", None)
        if b:
            table = cast(MypyFile, b.node).names
            if name in table:
                return table[name]
        raise KeyError(f"Failed lookup: {name}")

</t>
<t tx="ekr.20221004064034.509">def lookup_qualified(self, name: str) -&gt; SymbolTableNode:
    if "." not in name:
        return self.lookup(name)
    else:
        parts = name.split(".")
        n = self.modules[parts[0]]
        for i in range(1, len(parts) - 1):
            sym = n.names.get(parts[i])
            assert sym is not None, "Internal error: attempted lookup of unknown name"
            n = cast(MypyFile, sym.node)
        last = parts[-1]
        if last in n.names:
            return n.names[last]
        elif len(parts) == 2 and parts[0] == "builtins":
            fullname = "builtins." + last
            if fullname in SUGGESTED_TEST_FIXTURES:
                suggestion = ", e.g. add '[builtins fixtures/{}]' to your test".format(
                    SUGGESTED_TEST_FIXTURES[fullname]
                )
            else:
                suggestion = ""
            raise KeyError(
                "Could not find builtin symbol '{}' (If you are running a "
                "test case, use a fixture that "
                "defines this symbol{})".format(last, suggestion)
            )
        else:
            msg = "Failed qualified lookup: '{}' (fullname = '{}')."
            raise KeyError(msg.format(last, name))

</t>
<t tx="ekr.20221004064034.51">class It(Iterator[str]):
    stop = False

    @others
</t>
<t tx="ekr.20221004064034.510">@contextmanager
def enter_partial_types(
    self, *, is_function: bool = False, is_class: bool = False
) -&gt; Iterator[None]:
    """Enter a new scope for collecting partial types.

    Also report errors for (some) variables which still have partial
    types, i.e. we couldn't infer a complete type.
    """
    is_local = (self.partial_types and self.partial_types[-1].is_local) or is_function
    self.partial_types.append(PartialTypeScope({}, is_function, is_local))
    yield

    # Don't complain about not being able to infer partials if it is
    # at the toplevel (with allow_untyped_globals) or if it is in an
    # untyped function being checked with check_untyped_defs.
    permissive = (self.options.allow_untyped_globals and not is_local) or (
        self.options.check_untyped_defs and self.dynamic_funcs and self.dynamic_funcs[-1]
    )

    partial_types, _, _ = self.partial_types.pop()
    if not self.current_node_deferred:
        for var, context in partial_types.items():
            # If we require local partial types, there are a few exceptions where
            # we fall back to inferring just "None" as the type from a None initializer:
            #
            # 1. If all happens within a single function this is acceptable, since only
            #    the topmost function is a separate target in fine-grained incremental mode.
            #    We primarily want to avoid "splitting" partial types across targets.
            #
            # 2. A None initializer in the class body if the attribute is defined in a base
            #    class is fine, since the attribute is already defined and it's currently okay
            #    to vary the type of an attribute covariantly. The None type will still be
            #    checked for compatibility with base classes elsewhere. Without this exception
            #    mypy could require an annotation for an attribute that already has been
            #    declared in a base class, which would be bad.
            allow_none = (
                not self.options.local_partial_types
                or is_function
                or (is_class and self.is_defined_in_base_class(var))
            )
            if (
                allow_none
                and isinstance(var.type, PartialType)
                and var.type.type is None
                and not permissive
            ):
                var.type = NoneType()
            else:
                if var not in self.partial_reported and not permissive:
                    self.msg.need_annotation_for_var(var, context, self.options.python_version)
                    self.partial_reported.add(var)
                if var.type:
                    fixed = self.fixup_partial_type(var.type)
                    var.invalid_partial_type = fixed != var.type
                    var.type = fixed

</t>
<t tx="ekr.20221004064034.511">def handle_partial_var_type(
    self, typ: PartialType, is_lvalue: bool, node: Var, context: Context
) -&gt; Type:
    """Handle a reference to a partial type through a var.

    (Used by checkexpr and checkmember.)
    """
    in_scope, is_local, partial_types = self.find_partial_types_in_all_scopes(node)
    if typ.type is None and in_scope:
        # 'None' partial type. It has a well-defined type. In an lvalue context
        # we want to preserve the knowledge of it being a partial type.
        if not is_lvalue:
            return NoneType()
        else:
            return typ
    else:
        if partial_types is not None and not self.current_node_deferred:
            if in_scope:
                context = partial_types[node]
                if is_local or not self.options.allow_untyped_globals:
                    self.msg.need_annotation_for_var(
                        node, context, self.options.python_version
                    )
                    self.partial_reported.add(node)
            else:
                # Defer the node -- we might get a better type in the outer scope
                self.handle_cannot_determine_type(node.name, context)
        return self.fixup_partial_type(typ)

</t>
<t tx="ekr.20221004064034.512">def fixup_partial_type(self, typ: Type) -&gt; Type:
    """Convert a partial type that we couldn't resolve into something concrete.

    This means, for None we make it Optional[Any], and for anything else we
    fill in all of the type arguments with Any.
    """
    if not isinstance(typ, PartialType):
        return typ
    if typ.type is None:
        return UnionType.make_union([AnyType(TypeOfAny.unannotated), NoneType()])
    else:
        return Instance(typ.type, [AnyType(TypeOfAny.unannotated)] * len(typ.type.type_vars))

</t>
<t tx="ekr.20221004064034.513">def is_defined_in_base_class(self, var: Var) -&gt; bool:
    if var.info:
        for base in var.info.mro[1:]:
            if base.get(var.name) is not None:
                return True
        if var.info.fallback_to_any:
            return True
    return False

</t>
<t tx="ekr.20221004064034.514">def find_partial_types(self, var: Var) -&gt; dict[Var, Context] | None:
    """Look for an active partial type scope containing variable.

    A scope is active if assignments in the current context can refine a partial
    type originally defined in the scope. This is affected by the local_partial_types
    configuration option.
    """
    in_scope, _, partial_types = self.find_partial_types_in_all_scopes(var)
    if in_scope:
        return partial_types
    return None

</t>
<t tx="ekr.20221004064034.515">def find_partial_types_in_all_scopes(
    self, var: Var
) -&gt; tuple[bool, bool, dict[Var, Context] | None]:
    """Look for partial type scope containing variable.

    Return tuple (is the scope active, is the scope a local scope, scope).
    """
    for scope in reversed(self.partial_types):
        if var in scope.map:
            # All scopes within the outermost function are active. Scopes out of
            # the outermost function are inactive to allow local reasoning (important
            # for fine-grained incremental mode).
            disallow_other_scopes = self.options.local_partial_types

            if isinstance(var.type, PartialType) and var.type.type is not None and var.info:
                # This is an ugly hack to make partial generic self attributes behave
                # as if --local-partial-types is always on (because it used to be like this).
                disallow_other_scopes = True

            scope_active = (
                not disallow_other_scopes or scope.is_local == self.partial_types[-1].is_local
            )
            return scope_active, scope.is_local, scope.map
    return False, False, None

</t>
<t tx="ekr.20221004064034.516">def temp_node(self, t: Type, context: Context | None = None) -&gt; TempNode:
    """Create a temporary node with the given, fixed type."""
    return TempNode(t, context=context)

</t>
<t tx="ekr.20221004064034.517">def fail(
    self, msg: str | ErrorMessage, context: Context, *, code: ErrorCode | None = None
) -&gt; None:
    """Produce an error message."""
    if isinstance(msg, ErrorMessage):
        self.msg.fail(msg.value, context, code=msg.code)
        return
    self.msg.fail(msg, context, code=code)

</t>
<t tx="ekr.20221004064034.518">def note(
    self,
    msg: str | ErrorMessage,
    context: Context,
    offset: int = 0,
    *,
    code: ErrorCode | None = None,
) -&gt; None:
    """Produce a note."""
    if isinstance(msg, ErrorMessage):
        self.msg.note(msg.value, context, code=msg.code)
        return
    self.msg.note(msg, context, offset=offset, code=code)

</t>
<t tx="ekr.20221004064034.519">def iterable_item_type(self, instance: Instance) -&gt; Type:
    iterable = map_instance_to_supertype(instance, self.lookup_typeinfo("typing.Iterable"))
    item_type = iterable.args[0]
    if not isinstance(get_proper_type(item_type), AnyType):
        # This relies on 'map_instance_to_supertype' returning 'Iterable[Any]'
        # in case there is no explicit base class.
        return item_type
    # Try also structural typing.
    iter_type = get_proper_type(find_member("__iter__", instance, instance, is_operator=True))
    if iter_type and isinstance(iter_type, CallableType):
        ret_type = get_proper_type(iter_type.ret_type)
        if isinstance(ret_type, Instance):
            iterator = map_instance_to_supertype(
                ret_type, self.lookup_typeinfo("typing.Iterator")
            )
            item_type = iterator.args[0]
    return item_type

</t>
<t tx="ekr.20221004064034.52">def __iter__(self) -&gt; It:
    return self

</t>
<t tx="ekr.20221004064034.520">def function_type(self, func: FuncBase) -&gt; FunctionLike:
    return function_type(func, self.named_type("builtins.function"))

</t>
<t tx="ekr.20221004064034.521">def push_type_map(self, type_map: TypeMap) -&gt; None:
    if type_map is None:
        self.binder.unreachable()
    else:
        for expr, type in type_map.items():
            self.binder.put(expr, type)

</t>
<t tx="ekr.20221004064034.522">def infer_issubclass_maps(self, node: CallExpr, expr: Expression) -&gt; tuple[TypeMap, TypeMap]:
    """Infer type restrictions for an expression in issubclass call."""
    vartype = self.lookup_type(expr)
    type = self.get_isinstance_type(node.args[1])
    if isinstance(vartype, TypeVarType):
        vartype = vartype.upper_bound
    vartype = get_proper_type(vartype)
    if isinstance(vartype, UnionType):
        union_list = []
        for t in get_proper_types(vartype.items):
            if isinstance(t, TypeType):
                union_list.append(t.item)
            else:
                # This is an error that should be reported earlier
                # if we reach here, we refuse to do any type inference.
                return {}, {}
        vartype = UnionType(union_list)
    elif isinstance(vartype, TypeType):
        vartype = vartype.item
    elif isinstance(vartype, Instance) and vartype.type.is_metaclass():
        vartype = self.named_type("builtins.object")
    else:
        # Any other object whose type we don't know precisely
        # for example, Any or a custom metaclass.
        return {}, {}  # unknown type
    yes_type, no_type = self.conditional_types_with_intersection(vartype, type, expr)
    yes_map, no_map = conditional_types_to_typemaps(expr, yes_type, no_type)
    yes_map, no_map = map(convert_to_typetype, (yes_map, no_map))
    return yes_map, no_map

</t>
<t tx="ekr.20221004064034.523">@overload
def conditional_types_with_intersection(
    self,
    expr_type: Type,
    type_ranges: list[TypeRange] | None,
    ctx: Context,
    default: None = None,
) -&gt; tuple[Type | None, Type | None]:
    ...

</t>
<t tx="ekr.20221004064034.524">@overload
def conditional_types_with_intersection(
    self, expr_type: Type, type_ranges: list[TypeRange] | None, ctx: Context, default: Type
) -&gt; tuple[Type, Type]:
    ...

</t>
<t tx="ekr.20221004064034.525">def conditional_types_with_intersection(
    self,
    expr_type: Type,
    type_ranges: list[TypeRange] | None,
    ctx: Context,
    default: Type | None = None,
) -&gt; tuple[Type | None, Type | None]:
    initial_types = conditional_types(expr_type, type_ranges, default)
    # For some reason, doing "yes_map, no_map = conditional_types_to_typemaps(...)"
    # doesn't work: mypyc will decide that 'yes_map' is of type None if we try.
    yes_type: Type | None = initial_types[0]
    no_type: Type | None = initial_types[1]

    if not isinstance(get_proper_type(yes_type), UninhabitedType) or type_ranges is None:
        return yes_type, no_type

    # If conditional_types was unable to successfully narrow the expr_type
    # using the type_ranges and concluded if-branch is unreachable, we try
    # computing it again using a different algorithm that tries to generate
    # an ad-hoc intersection between the expr_type and the type_ranges.
    proper_type = get_proper_type(expr_type)
    if isinstance(proper_type, UnionType):
        possible_expr_types = get_proper_types(proper_type.relevant_items())
    else:
        possible_expr_types = [proper_type]

    possible_target_types = []
    for tr in type_ranges:
        item = get_proper_type(tr.item)
        if not isinstance(item, Instance) or tr.is_upper_bound:
            return yes_type, no_type
        possible_target_types.append(item)

    out = []
    for v in possible_expr_types:
        if not isinstance(v, Instance):
            return yes_type, no_type
        for t in possible_target_types:
            intersection = self.intersect_instances((v, t), ctx)
            if intersection is None:
                continue
            out.append(intersection)
    if len(out) == 0:
        return UninhabitedType(), expr_type
    new_yes_type = make_simplified_union(out)
    return new_yes_type, expr_type

</t>
<t tx="ekr.20221004064034.526">def is_writable_attribute(self, node: Node) -&gt; bool:
    """Check if an attribute is writable"""
    if isinstance(node, Var):
        if node.is_property and not node.is_settable_property:
            return False
        return True
    elif isinstance(node, OverloadedFuncDef) and node.is_property:
        first_item = cast(Decorator, node.items[0])
        return first_item.var.is_settable_property
    else:
        return False

</t>
<t tx="ekr.20221004064034.527">def get_isinstance_type(self, expr: Expression) -&gt; list[TypeRange] | None:
    if isinstance(expr, OpExpr) and expr.op == "|":
        left = self.get_isinstance_type(expr.left)
        right = self.get_isinstance_type(expr.right)
        if left is None or right is None:
            return None
        return left + right
    all_types = get_proper_types(flatten_types(self.lookup_type(expr)))
    types: list[TypeRange] = []
    for typ in all_types:
        if isinstance(typ, FunctionLike) and typ.is_type_obj():
            # Type variables may be present -- erase them, which is the best
            # we can do (outside disallowing them here).
            erased_type = erase_typevars(typ.items[0].ret_type)
            types.append(TypeRange(erased_type, is_upper_bound=False))
        elif isinstance(typ, TypeType):
            # Type[A] means "any type that is a subtype of A" rather than "precisely type A"
            # we indicate this by setting is_upper_bound flag
            types.append(TypeRange(typ.item, is_upper_bound=True))
        elif isinstance(typ, Instance) and typ.type.fullname == "builtins.type":
            object_type = Instance(typ.type.mro[-1], [])
            types.append(TypeRange(object_type, is_upper_bound=True))
        elif isinstance(typ, AnyType):
            types.append(TypeRange(typ, is_upper_bound=False))
        else:  # we didn't see an actual type, but rather a variable with unknown value
            return None
    if not types:
        # this can happen if someone has empty tuple as 2nd argument to isinstance
        # strictly speaking, we should return UninhabitedType but for simplicity we will simply
        # refuse to do any type inference for now
        return None
    return types

</t>
<t tx="ekr.20221004064034.528">def is_literal_enum(self, n: Expression) -&gt; bool:
    """Returns true if this expression (with the given type context) is an Enum literal.

    For example, if we had an enum:

        class Foo(Enum):
            A = 1
            B = 2

    ...and if the expression 'Foo' referred to that enum within the current type context,
    then the expression 'Foo.A' would be a literal enum. However, if we did 'a = Foo.A',
    then the variable 'a' would *not* be a literal enum.

    We occasionally special-case expressions like 'Foo.A' and treat them as a single primitive
    unit for the same reasons we sometimes treat 'True', 'False', or 'None' as a single
    primitive unit.
    """
    if not isinstance(n, MemberExpr) or not isinstance(n.expr, NameExpr):
        return False

    parent_type = self.lookup_type_or_none(n.expr)
    member_type = self.lookup_type_or_none(n)
    if member_type is None or parent_type is None:
        return False

    parent_type = get_proper_type(parent_type)
    member_type = get_proper_type(coerce_to_literal(member_type))
    if not isinstance(parent_type, FunctionLike) or not isinstance(member_type, LiteralType):
        return False

    if not parent_type.is_type_obj():
        return False

    return (
        member_type.is_enum_literal()
        and member_type.fallback.type == parent_type.type_object()
    )

</t>
<t tx="ekr.20221004064034.529">def add_any_attribute_to_type(self, typ: Type, name: str) -&gt; Type:
    """Inject an extra attribute with Any type using fallbacks."""
    orig_typ = typ
    typ = get_proper_type(typ)
    any_type = AnyType(TypeOfAny.unannotated)
    if isinstance(typ, Instance):
        result = typ.copy_with_extra_attr(name, any_type)
        # For instances, we erase the possible module name, so that restrictions
        # become anonymous types.ModuleType instances, allowing hasattr() to
        # have effect on modules.
        assert result.extra_attrs is not None
        result.extra_attrs.mod_name = None
        return result
    if isinstance(typ, TupleType):
        fallback = typ.partial_fallback.copy_with_extra_attr(name, any_type)
        return typ.copy_modified(fallback=fallback)
    if isinstance(typ, CallableType):
        fallback = typ.fallback.copy_with_extra_attr(name, any_type)
        return typ.copy_modified(fallback=fallback)
    if isinstance(typ, TypeType) and isinstance(typ.item, Instance):
        return TypeType.make_normalized(self.add_any_attribute_to_type(typ.item, name))
    if isinstance(typ, TypeVarType):
        return typ.copy_modified(
            upper_bound=self.add_any_attribute_to_type(typ.upper_bound, name),
            values=[self.add_any_attribute_to_type(v, name) for v in typ.values],
        )
    if isinstance(typ, UnionType):
        with_attr, without_attr = self.partition_union_by_attr(typ, name)
        return make_simplified_union(
            with_attr + [self.add_any_attribute_to_type(typ, name) for typ in without_attr]
        )
    return orig_typ

</t>
<t tx="ekr.20221004064034.53">def __next__(self) -&gt; str:
    if self.stop:
        raise StopIteration("end")
    else:
        self.stop = True
        return "a"


</t>
<t tx="ekr.20221004064034.530">def hasattr_type_maps(
    self, expr: Expression, source_type: Type, name: str
) -&gt; tuple[TypeMap, TypeMap]:
    """Simple support for hasattr() checks.

    Essentially the logic is following:
        * In the if branch, keep types that already has a valid attribute as is,
          for other inject an attribute with `Any` type.
        * In the else branch, remove types that already have a valid attribute,
          while keeping the rest.
    """
    if self.has_valid_attribute(source_type, name):
        return {expr: source_type}, {}

    source_type = get_proper_type(source_type)
    if isinstance(source_type, UnionType):
        _, without_attr = self.partition_union_by_attr(source_type, name)
        yes_map = {expr: self.add_any_attribute_to_type(source_type, name)}
        return yes_map, {expr: make_simplified_union(without_attr)}

    type_with_attr = self.add_any_attribute_to_type(source_type, name)
    if type_with_attr != source_type:
        return {expr: type_with_attr}, {}
    return {}, {}

</t>
<t tx="ekr.20221004064034.531">def partition_union_by_attr(
    self, source_type: UnionType, name: str
) -&gt; tuple[list[Type], list[Type]]:
    with_attr = []
    without_attr = []
    for item in source_type.items:
        if self.has_valid_attribute(item, name):
            with_attr.append(item)
        else:
            without_attr.append(item)
    return with_attr, without_attr

</t>
<t tx="ekr.20221004064034.532">def has_valid_attribute(self, typ: Type, name: str) -&gt; bool:
    p_typ = get_proper_type(typ)
    if isinstance(p_typ, AnyType):
        return False
    if isinstance(p_typ, Instance) and p_typ.extra_attrs and p_typ.extra_attrs.mod_name:
        # Presence of module_symbol_table means this check will skip ModuleType.__getattr__
        module_symbol_table = p_typ.type.names
    else:
        module_symbol_table = None
    with self.msg.filter_errors() as watcher:
        analyze_member_access(
            name,
            typ,
            TempNode(AnyType(TypeOfAny.special_form)),
            False,
            False,
            False,
            self.msg,
            original_type=typ,
            chk=self,
            # This is not a real attribute lookup so don't mess with deferring nodes.
            no_deferral=True,
            module_symbol_table=module_symbol_table,
        )
    return not watcher.has_new_errors()


</t>
<t tx="ekr.20221004064034.533">class CollectArgTypeVarTypes(TypeTraverserVisitor):
    """Collects the non-nested argument types in a set."""

    def __init__(self) -&gt; None:
        self.arg_types: set[TypeVarType] = set()

    def visit_type_var(self, t: TypeVarType) -&gt; None:
        self.arg_types.add(t)


</t>
<t tx="ekr.20221004064034.537">@overload
def conditional_types(
    current_type: Type, proposed_type_ranges: list[TypeRange] | None, default: None = None
) -&gt; tuple[Type | None, Type | None]:
    ...


</t>
<t tx="ekr.20221004064034.538">@overload
def conditional_types(
    current_type: Type, proposed_type_ranges: list[TypeRange] | None, default: Type
) -&gt; tuple[Type, Type]:
    ...


</t>
<t tx="ekr.20221004064034.539">def conditional_types(
    current_type: Type, proposed_type_ranges: list[TypeRange] | None, default: Type | None = None
) -&gt; tuple[Type | None, Type | None]:
    """Takes in the current type and a proposed type of an expression.

    Returns a 2-tuple: The first element is the proposed type, if the expression
    can be the proposed type. The second element is the type it would hold
    if it was not the proposed type, if any. UninhabitedType means unreachable.
    None means no new information can be inferred. If default is set it is returned
    instead."""
    if proposed_type_ranges:
        if len(proposed_type_ranges) == 1:
            target = proposed_type_ranges[0].item
            target = get_proper_type(target)
            if isinstance(target, LiteralType) and (
                target.is_enum_literal() or isinstance(target.value, bool)
            ):
                enum_name = target.fallback.type.fullname
                current_type = try_expanding_sum_type_to_union(current_type, enum_name)
        proposed_items = [type_range.item for type_range in proposed_type_ranges]
        proposed_type = make_simplified_union(proposed_items)
        if isinstance(proposed_type, AnyType):
            # We don't really know much about the proposed type, so we shouldn't
            # attempt to narrow anything. Instead, we broaden the expr to Any to
            # avoid false positives
            return proposed_type, default
        elif not any(
            type_range.is_upper_bound for type_range in proposed_type_ranges
        ) and is_proper_subtype(current_type, proposed_type, ignore_promotions=True):
            # Expression is always of one of the types in proposed_type_ranges
            return default, UninhabitedType()
        elif not is_overlapping_types(
            current_type, proposed_type, prohibit_none_typevar_overlap=True, ignore_promotions=True
        ):
            # Expression is never of any type in proposed_type_ranges
            return UninhabitedType(), default
        else:
            # we can only restrict when the type is precise, not bounded
            proposed_precise_type = UnionType.make_union(
                [
                    type_range.item
                    for type_range in proposed_type_ranges
                    if not type_range.is_upper_bound
                ]
            )
            remaining_type = restrict_subtype_away(current_type, proposed_precise_type)
            return proposed_type, remaining_type
    else:
        # An isinstance check, but we don't understand the type
        return current_type, default


</t>
<t tx="ekr.20221004064034.54">def other_iterator() -&gt; It:
    return It()


</t>
<t tx="ekr.20221004064034.540">def conditional_types_to_typemaps(
    expr: Expression, yes_type: Type | None, no_type: Type | None
) -&gt; tuple[TypeMap, TypeMap]:
    maps: list[TypeMap] = []
    for typ in (yes_type, no_type):
        proper_type = get_proper_type(typ)
        if isinstance(proper_type, UninhabitedType):
            maps.append(None)
        elif proper_type is None:
            maps.append({})
        else:
            assert typ is not None
            maps.append({expr: typ})

    return cast(Tuple[TypeMap, TypeMap], tuple(maps))


</t>
<t tx="ekr.20221004064034.541">def gen_unique_name(base: str, table: SymbolTable) -&gt; str:
    """Generate a name that does not appear in table by appending numbers to base."""
    if base not in table:
        return base
    i = 1
    while base + str(i) in table:
        i += 1
    return base + str(i)


</t>
<t tx="ekr.20221004064034.542">def is_true_literal(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'True' literal/keyword."""
    return refers_to_fullname(n, "builtins.True") or isinstance(n, IntExpr) and n.value != 0


</t>
<t tx="ekr.20221004064034.543">def is_false_literal(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'False' literal/keyword."""
    return refers_to_fullname(n, "builtins.False") or isinstance(n, IntExpr) and n.value == 0


</t>
<t tx="ekr.20221004064034.544">def is_literal_none(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'None' literal/keyword."""
    return isinstance(n, NameExpr) and n.fullname == "builtins.None"


</t>
<t tx="ekr.20221004064034.545">def is_literal_not_implemented(n: Expression) -&gt; bool:
    return isinstance(n, NameExpr) and n.fullname == "builtins.NotImplemented"


</t>
<t tx="ekr.20221004064034.546">def builtin_item_type(tp: Type) -&gt; Type | None:
    """Get the item type of a builtin container.

    If 'tp' is not one of the built containers (these includes NamedTuple and TypedDict)
    or if the container is not parameterized (like List or List[Any])
    return None. This function is used to narrow optional types in situations like this:

        x: Optional[int]
        if x in (1, 2, 3):
            x + 42  # OK

    Note: this is only OK for built-in containers, where we know the behavior
    of __contains__.
    """
    tp = get_proper_type(tp)

    if isinstance(tp, Instance):
        if tp.type.fullname in [
            "builtins.list",
            "builtins.tuple",
            "builtins.dict",
            "builtins.set",
            "builtins.frozenset",
        ]:
            if not tp.args:
                # TODO: fix tuple in lib-stub/builtins.pyi (it should be generic).
                return None
            if not isinstance(get_proper_type(tp.args[0]), AnyType):
                return tp.args[0]
    elif isinstance(tp, TupleType) and all(
        not isinstance(it, AnyType) for it in get_proper_types(tp.items)
    ):
        return make_simplified_union(tp.items)  # this type is not externally visible
    elif isinstance(tp, TypedDictType):
        # TypedDict always has non-optional string keys. Find the key type from the Mapping
        # base class.
        for base in tp.fallback.type.mro:
            if base.fullname == "typing.Mapping":
                return map_instance_to_supertype(tp.fallback, base).args[0]
        assert False, "No Mapping base class found for TypedDict fallback"
    return None


</t>
<t tx="ekr.20221004064034.547">def and_conditional_maps(m1: TypeMap, m2: TypeMap) -&gt; TypeMap:
    """Calculate what information we can learn from the truth of (e1 and e2)
    in terms of the information that we can learn from the truth of e1 and
    the truth of e2.
    """

    if m1 is None or m2 is None:
        # One of the conditions can never be true.
        return None
    # Both conditions can be true; combine the information. Anything
    # we learn from either conditions's truth is valid. If the same
    # expression's type is refined by both conditions, we somewhat
    # arbitrarily give precedence to m2. (In the future, we could use
    # an intersection type.)
    result = m2.copy()
    m2_keys = {literal_hash(n2) for n2 in m2}
    for n1 in m1:
        if literal_hash(n1) not in m2_keys:
            result[n1] = m1[n1]
    return result


</t>
<t tx="ekr.20221004064034.548">def or_conditional_maps(m1: TypeMap, m2: TypeMap) -&gt; TypeMap:
    """Calculate what information we can learn from the truth of (e1 or e2)
    in terms of the information that we can learn from the truth of e1 and
    the truth of e2.
    """

    if m1 is None:
        return m2
    if m2 is None:
        return m1
    # Both conditions can be true. Combine information about
    # expressions whose type is refined by both conditions. (We do not
    # learn anything about expressions whose type is refined by only
    # one condition.)
    result: dict[Expression, Type] = {}
    for n1 in m1:
        for n2 in m2:
            if literal_hash(n1) == literal_hash(n2):
                result[n1] = make_simplified_union([m1[n1], m2[n2]])
    return result


</t>
<t tx="ekr.20221004064034.549">def reduce_conditional_maps(type_maps: list[tuple[TypeMap, TypeMap]]) -&gt; tuple[TypeMap, TypeMap]:
    """Reduces a list containing pairs of if/else TypeMaps into a single pair.

    We "and" together all of the if TypeMaps and "or" together the else TypeMaps. So
    for example, if we had the input:

        [
            ({x: TypeIfX, shared: TypeIfShared1}, {x: TypeElseX, shared: TypeElseShared1}),
            ({y: TypeIfY, shared: TypeIfShared2}, {y: TypeElseY, shared: TypeElseShared2}),
        ]

    ...we'd return the output:

        (
            {x: TypeIfX,   y: TypeIfY,   shared: PseudoIntersection[TypeIfShared1, TypeIfShared2]},
            {shared: Union[TypeElseShared1, TypeElseShared2]},
        )

    ...where "PseudoIntersection[X, Y] == Y" because mypy actually doesn't understand intersections
    yet, so we settle for just arbitrarily picking the right expr's type.

    We only retain the shared expression in the 'else' case because we don't actually know
    whether x was refined or y was refined -- only just that one of the two was refined.
    """
    if len(type_maps) == 0:
        return {}, {}
    elif len(type_maps) == 1:
        return type_maps[0]
    else:
        final_if_map, final_else_map = type_maps[0]
        for if_map, else_map in type_maps[1:]:
            final_if_map = and_conditional_maps(final_if_map, if_map)
            final_else_map = or_conditional_maps(final_else_map, else_map)

        return final_if_map, final_else_map


</t>
<t tx="ekr.20221004064034.55">class Aw(Awaitable[int]):
    def __await__(self) -&gt; Generator[str, Any, int]:
        yield "a"
        return 1


</t>
<t tx="ekr.20221004064034.550">def convert_to_typetype(type_map: TypeMap) -&gt; TypeMap:
    converted_type_map: dict[Expression, Type] = {}
    if type_map is None:
        return None
    for expr, typ in type_map.items():
        t = typ
        if isinstance(t, TypeVarType):
            t = t.upper_bound
        # TODO: should we only allow unions of instances as per PEP 484?
        if not isinstance(get_proper_type(t), (UnionType, Instance)):
            # unknown type; error was likely reported earlier
            return {}
        converted_type_map[expr] = TypeType.make_normalized(typ)
    return converted_type_map


</t>
<t tx="ekr.20221004064034.551">def flatten(t: Expression) -&gt; list[Expression]:
    """Flatten a nested sequence of tuples/lists into one list of nodes."""
    if isinstance(t, TupleExpr) or isinstance(t, ListExpr):
        return [b for a in t.items for b in flatten(a)]
    elif isinstance(t, StarExpr):
        return flatten(t.expr)
    else:
        return [t]


</t>
<t tx="ekr.20221004064034.552">def flatten_types(t: Type) -&gt; list[Type]:
    """Flatten a nested sequence of tuples into one list of nodes."""
    t = get_proper_type(t)
    if isinstance(t, TupleType):
        return [b for a in t.items for b in flatten_types(a)]
    else:
        return [t]


</t>
<t tx="ekr.20221004064034.553">def expand_func(defn: FuncItem, map: dict[TypeVarId, Type]) -&gt; FuncItem:
    visitor = TypeTransformVisitor(map)
    ret = visitor.node(defn)
    assert isinstance(ret, FuncItem)
    return ret


</t>
<t tx="ekr.20221004064034.554">class TypeTransformVisitor(TransformVisitor):
    def __init__(self, map: dict[TypeVarId, Type]) -&gt; None:
        super().__init__()
        self.map = map

    def type(self, type: Type) -&gt; Type:
        return expand_type(type, self.map)


</t>
<t tx="ekr.20221004064034.555">def are_argument_counts_overlapping(t: CallableType, s: CallableType) -&gt; bool:
    """Can a single call match both t and s, based just on positional argument counts?"""
    min_args = max(t.min_args, s.min_args)
    max_args = min(t.max_possible_positional_args(), s.max_possible_positional_args())
    return min_args &lt;= max_args


</t>
<t tx="ekr.20221004064034.556">def is_unsafe_overlapping_overload_signatures(
    signature: CallableType, other: CallableType
) -&gt; bool:
    """Check if two overloaded signatures are unsafely overlapping or partially overlapping.

    We consider two functions 's' and 't' to be unsafely overlapping if both
    of the following are true:

    1.  s's parameters are all more precise or partially overlapping with t's
    2.  s's return type is NOT a subtype of t's.

    Assumes that 'signature' appears earlier in the list of overload
    alternatives then 'other' and that their argument counts are overlapping.
    """
    # Try detaching callables from the containing class so that all TypeVars
    # are treated as being free.
    #
    # This lets us identify cases where the two signatures use completely
    # incompatible types -- e.g. see the testOverloadingInferUnionReturnWithMixedTypevars
    # test case.
    signature = detach_callable(signature)
    other = detach_callable(other)

    # Note: We repeat this check twice in both directions due to a slight
    # asymmetry in 'is_callable_compatible'. When checking for partial overlaps,
    # we attempt to unify 'signature' and 'other' both against each other.
    #
    # If 'signature' cannot be unified with 'other', we end early. However,
    # if 'other' cannot be modified with 'signature', the function continues
    # using the older version of 'other'.
    #
    # This discrepancy is unfortunately difficult to get rid of, so we repeat the
    # checks twice in both directions for now.
    return is_callable_compatible(
        signature,
        other,
        is_compat=is_overlapping_types_no_promote_no_uninhabited,
        is_compat_return=lambda l, r: not is_subtype_no_promote(l, r),
        ignore_return=False,
        check_args_covariantly=True,
        allow_partial_overlap=True,
    ) or is_callable_compatible(
        other,
        signature,
        is_compat=is_overlapping_types_no_promote_no_uninhabited,
        is_compat_return=lambda l, r: not is_subtype_no_promote(r, l),
        ignore_return=False,
        check_args_covariantly=False,
        allow_partial_overlap=True,
    )


</t>
<t tx="ekr.20221004064034.557">def detach_callable(typ: CallableType) -&gt; CallableType:
    """Ensures that the callable's type variables are 'detached' and independent of the context.

    A callable normally keeps track of the type variables it uses within its 'variables' field.
    However, if the callable is from a method and that method is using a class type variable,
    the callable will not keep track of that type variable since it belongs to the class.

    This function will traverse the callable and find all used type vars and add them to the
    variables field if it isn't already present.

    The caller can then unify on all type variables whether or not the callable is originally
    from a class or not."""
    type_list = typ.arg_types + [typ.ret_type]

    appear_map: dict[str, list[int]] = {}
    for i, inner_type in enumerate(type_list):
        typevars_available = get_type_vars(inner_type)
        for var in typevars_available:
            if var.fullname not in appear_map:
                appear_map[var.fullname] = []
            appear_map[var.fullname].append(i)

    used_type_var_names = set()
    for var_name, appearances in appear_map.items():
        used_type_var_names.add(var_name)

    all_type_vars = get_type_vars(typ)
    new_variables = []
    for var in set(all_type_vars):
        if var.fullname not in used_type_var_names:
            continue
        new_variables.append(
            TypeVarType(
                name=var.name,
                fullname=var.fullname,
                id=var.id,
                values=var.values,
                upper_bound=var.upper_bound,
                variance=var.variance,
            )
        )
    out = typ.copy_modified(
        variables=new_variables, arg_types=type_list[:-1], ret_type=type_list[-1]
    )
    return out


</t>
<t tx="ekr.20221004064034.558">def overload_can_never_match(signature: CallableType, other: CallableType) -&gt; bool:
    """Check if the 'other' method can never be matched due to 'signature'.

    This can happen if signature's parameters are all strictly broader then
    other's parameters.

    Assumes that both signatures have overlapping argument counts.
    """
    # The extra erasure is needed to prevent spurious errors
    # in situations where an `Any` overload is used as a fallback
    # for an overload with type variables. The spurious error appears
    # because the type variables turn into `Any` during unification in
    # the below subtype check and (surprisingly?) `is_proper_subtype(Any, Any)`
    # returns `True`.
    # TODO: find a cleaner solution instead of this ad-hoc erasure.
    exp_signature = expand_type(
        signature, {tvar.id: erase_def_to_union_or_bound(tvar) for tvar in signature.variables}
    )
    assert isinstance(exp_signature, CallableType)
    return is_callable_compatible(
        exp_signature, other, is_compat=is_more_precise, ignore_return=True
    )


</t>
<t tx="ekr.20221004064034.559">def is_more_general_arg_prefix(t: FunctionLike, s: FunctionLike) -&gt; bool:
    """Does t have wider arguments than s?"""
    # TODO should an overload with additional items be allowed to be more
    #      general than one with fewer items (or just one item)?
    if isinstance(t, CallableType):
        if isinstance(s, CallableType):
            return is_callable_compatible(t, s, is_compat=is_proper_subtype, ignore_return=True)
    elif isinstance(t, FunctionLike):
        if isinstance(s, FunctionLike):
            if len(t.items) == len(s.items):
                return all(
                    is_same_arg_prefix(items, itemt) for items, itemt in zip(t.items, s.items)
                )
    return False


</t>
<t tx="ekr.20221004064034.56">def other_coroutine() -&gt; Aw:
    return Aw()


</t>
<t tx="ekr.20221004064034.560">def is_same_arg_prefix(t: CallableType, s: CallableType) -&gt; bool:
    return is_callable_compatible(
        t,
        s,
        is_compat=is_same_type,
        ignore_return=True,
        check_args_covariantly=True,
        ignore_pos_arg_names=True,
    )


</t>
<t tx="ekr.20221004064034.561">def infer_operator_assignment_method(typ: Type, operator: str) -&gt; tuple[bool, str]:
    """Determine if operator assignment on given value type is in-place, and the method name.

    For example, if operator is '+', return (True, '__iadd__') or (False, '__add__')
    depending on which method is supported by the type.
    """
    typ = get_proper_type(typ)
    method = operators.op_methods[operator]
    if isinstance(typ, Instance):
        if operator in operators.ops_with_inplace_method:
            inplace_method = "__i" + method[2:]
            if typ.type.has_readable_member(inplace_method):
                return True, inplace_method
    return False, method


</t>
<t tx="ekr.20221004064034.562">def is_valid_inferred_type(typ: Type) -&gt; bool:
    """Is an inferred type valid?

    Examples of invalid types include the None type or List[&lt;uninhabited&gt;].

    When not doing strict Optional checking, all types containing None are
    invalid.  When doing strict Optional checking, only None and types that are
    incompletely defined (i.e. contain UninhabitedType) are invalid.
    """
    if isinstance(get_proper_type(typ), (NoneType, UninhabitedType)):
        # With strict Optional checking, we *may* eventually infer NoneType when
        # the initializer is None, but we only do that if we can't infer a
        # specific Optional type.  This resolution happens in
        # leave_partial_types when we pop a partial types scope.
        return False
    return not typ.accept(NothingSeeker())


</t>
<t tx="ekr.20221004064034.563">class NothingSeeker(TypeQuery[bool]):
    """Find any &lt;nothing&gt; types resulting from failed (ambiguous) type inference."""

    @others
</t>
<t tx="ekr.20221004064034.564">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20221004064034.565">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return t.ambiguous


</t>
<t tx="ekr.20221004064034.566">class SetNothingToAny(TypeTranslator):
    """Replace all ambiguous &lt;nothing&gt; types with Any (to avoid spurious extra errors)."""

    @others
</t>
<t tx="ekr.20221004064034.567">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    if t.ambiguous:
        return AnyType(TypeOfAny.from_error)
    return t

</t>
<t tx="ekr.20221004064034.568">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Target of the alias cannot by an ambiguous &lt;nothing&gt;, so we just
    # replace the arguments.
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20221004064034.569">def is_node_static(node: Node | None) -&gt; bool | None:
    """Find out if a node describes a static function method."""

    if isinstance(node, FuncDef):
        return node.is_static

    if isinstance(node, Var):
        return node.is_staticmethod

    return None


</t>
<t tx="ekr.20221004064034.57"># The various contexts in which `await` or `yield from` might occur.


</t>
<t tx="ekr.20221004064034.570">class CheckerScope:
    # We keep two stacks combined, to maintain the relative order
    stack: list[TypeInfo | FuncItem | MypyFile]

    @others
</t>
<t tx="ekr.20221004064034.571">def __init__(self, module: MypyFile) -&gt; None:
    self.stack = [module]

</t>
<t tx="ekr.20221004064034.572">def top_function(self) -&gt; FuncItem | None:
    for e in reversed(self.stack):
        if isinstance(e, FuncItem):
            return e
    return None

</t>
<t tx="ekr.20221004064034.573">def top_non_lambda_function(self) -&gt; FuncItem | None:
    for e in reversed(self.stack):
        if isinstance(e, FuncItem) and not isinstance(e, LambdaExpr):
            return e
    return None

</t>
<t tx="ekr.20221004064034.574">def active_class(self) -&gt; TypeInfo | None:
    if isinstance(self.stack[-1], TypeInfo):
        return self.stack[-1]
    return None

</t>
<t tx="ekr.20221004064034.575">def enclosing_class(self) -&gt; TypeInfo | None:
    """Is there a class *directly* enclosing this function?"""
    top = self.top_function()
    assert top, "This method must be called from inside a function"
    index = self.stack.index(top)
    assert index, "CheckerScope stack must always start with a module"
    enclosing = self.stack[index - 1]
    if isinstance(enclosing, TypeInfo):
        return enclosing
    return None

</t>
<t tx="ekr.20221004064034.576">def active_self_type(self) -&gt; Instance | TupleType | None:
    """An instance or tuple type representing the current class.

    This returns None unless we are in class body or in a method.
    In particular, inside a function nested in method this returns None.
    """
    info = self.active_class()
    if not info and self.top_function():
        info = self.enclosing_class()
    if info:
        return fill_typevars(info)
    return None

</t>
<t tx="ekr.20221004064034.577">@contextmanager
def push_function(self, item: FuncItem) -&gt; Iterator[None]:
    self.stack.append(item)
    yield
    self.stack.pop()

</t>
<t tx="ekr.20221004064034.578">@contextmanager
def push_class(self, info: TypeInfo) -&gt; Iterator[None]:
    self.stack.append(info)
    yield
    self.stack.pop()


</t>
<t tx="ekr.20221004064034.579">TKey = TypeVar("TKey")
TValue = TypeVar("TValue")


</t>
<t tx="ekr.20221004064034.58">def plain_host_generator(func) -&gt; Generator[str, None, None]:
    yield "a"
    x = 0
    f = func()
    try:
        x = yield from f  # noqa: F841
    finally:
        try:
            f.close()
        except AttributeError:
            pass


</t>
<t tx="ekr.20221004064034.580">class DisjointDict(Generic[TKey, TValue]):
    """An variation of the union-find algorithm/data structure where instead of keeping
    track of just disjoint sets, we keep track of disjoint dicts -- keep track of multiple
    Set[Key] -&gt; Set[Value] mappings, where each mapping's keys are guaranteed to be disjoint.

    This data structure is currently used exclusively by 'group_comparison_operands' below
    to merge chains of '==' and 'is' comparisons when two or more chains use the same expression
    in best-case O(n), where n is the number of operands.

    Specifically, the `add_mapping()` function and `items()` functions will take on average
    O(k + v) and O(n) respectively, where k and v are the number of keys and values we're adding
    for a given chain. Note that k &lt;= n and v &lt;= n.

    We hit these average/best-case scenarios for most user code: e.g. when the user has just
    a single chain like 'a == b == c == d == ...' or multiple disjoint chains like
    'a==b &lt; c==d &lt; e==f &lt; ...'. (Note that a naive iterative merging would be O(n^2) for
    the latter case).

    In comparison, this data structure will make 'group_comparison_operands' have a worst-case
    runtime of O(n*log(n)): 'add_mapping()' and 'items()' are worst-case O(k*log(n) + v) and
    O(k*log(n)) respectively. This happens only in the rare case where the user keeps repeatedly
    making disjoint mappings before merging them in a way that persistently dodges the path
    compression optimization in '_lookup_root_id', which would end up constructing a single
    tree of height log_2(n). This makes root lookups no longer amoritized constant time when we
    finally call 'items()'.
    """

    @others
</t>
<t tx="ekr.20221004064034.581">def __init__(self) -&gt; None:
    # Each key maps to a unique ID
    self._key_to_id: dict[TKey, int] = {}

    # Each id points to the parent id, forming a forest of upwards-pointing trees. If the
    # current id already is the root, it points to itself. We gradually flatten these trees
    # as we perform root lookups: eventually all nodes point directly to its root.
    self._id_to_parent_id: dict[int, int] = {}

    # Each root id in turn maps to the set of values.
    self._root_id_to_values: dict[int, set[TValue]] = {}

</t>
<t tx="ekr.20221004064034.582">def add_mapping(self, keys: set[TKey], values: set[TValue]) -&gt; None:
    """Adds a 'Set[TKey] -&gt; Set[TValue]' mapping. If there already exists a mapping
    containing one or more of the given keys, we merge the input mapping with the old one.

    Note that the given set of keys must be non-empty -- otherwise, nothing happens.
    """
    if len(keys) == 0:
        return

    subtree_roots = [self._lookup_or_make_root_id(key) for key in keys]
    new_root = subtree_roots[0]

    root_values = self._root_id_to_values[new_root]
    root_values.update(values)
    for subtree_root in subtree_roots[1:]:
        if subtree_root == new_root or subtree_root not in self._root_id_to_values:
            continue
        self._id_to_parent_id[subtree_root] = new_root
        root_values.update(self._root_id_to_values.pop(subtree_root))

</t>
<t tx="ekr.20221004064034.583">def items(self) -&gt; list[tuple[set[TKey], set[TValue]]]:
    """Returns all disjoint mappings in key-value pairs."""
    root_id_to_keys: dict[int, set[TKey]] = {}
    for key in self._key_to_id:
        root_id = self._lookup_root_id(key)
        if root_id not in root_id_to_keys:
            root_id_to_keys[root_id] = set()
        root_id_to_keys[root_id].add(key)

    output = []
    for root_id, keys in root_id_to_keys.items():
        output.append((keys, self._root_id_to_values[root_id]))

    return output

</t>
<t tx="ekr.20221004064034.584">def _lookup_or_make_root_id(self, key: TKey) -&gt; int:
    if key in self._key_to_id:
        return self._lookup_root_id(key)
    else:
        new_id = len(self._key_to_id)
        self._key_to_id[key] = new_id
        self._id_to_parent_id[new_id] = new_id
        self._root_id_to_values[new_id] = set()
        return new_id

</t>
<t tx="ekr.20221004064034.585">def _lookup_root_id(self, key: TKey) -&gt; int:
    i = self._key_to_id[key]
    while i != self._id_to_parent_id[i]:
        # Optimization: make keys directly point to their grandparents to speed up
        # future traversals. This prevents degenerate trees of height n from forming.
        new_parent = self._id_to_parent_id[self._id_to_parent_id[i]]
        self._id_to_parent_id[i] = new_parent
        i = new_parent
    return i


</t>
<t tx="ekr.20221004064034.586">def group_comparison_operands(
    pairwise_comparisons: Iterable[tuple[str, Expression, Expression]],
    operand_to_literal_hash: Mapping[int, Key],
    operators_to_group: set[str],
) -&gt; list[tuple[str, list[int]]]:
    """Group a series of comparison operands together chained by any operand
    in the 'operators_to_group' set. All other pairwise operands are kept in
    groups of size 2.

    For example, suppose we have the input comparison expression:

        x0 == x1 == x2 &lt; x3 &lt; x4 is x5 is x6 is not x7 is not x8

    If we get these expressions in a pairwise way (e.g. by calling ComparisionExpr's
    'pairwise()' method), we get the following as input:

        [('==', x0, x1), ('==', x1, x2), ('&lt;', x2, x3), ('&lt;', x3, x4),
         ('is', x4, x5), ('is', x5, x6), ('is not', x6, x7), ('is not', x7, x8)]

    If `operators_to_group` is the set {'==', 'is'}, this function will produce
    the following "simplified operator list":

       [("==", [0, 1, 2]), ("&lt;", [2, 3]), ("&lt;", [3, 4]),
        ("is", [4, 5, 6]), ("is not", [6, 7]), ("is not", [7, 8])]

    Note that (a) we yield *indices* to the operands rather then the operand
    expressions themselves and that (b) operands used in a consecutive chain
    of '==' or 'is' are grouped together.

    If two of these chains happen to contain operands with the same underlying
    literal hash (e.g. are assignable and correspond to the same expression),
    we combine those chains together. For example, if we had:

        same == x &lt; y == same

    ...and if 'operand_to_literal_hash' contained the same values for the indices
    0 and 3, we'd produce the following output:

        [("==", [0, 1, 2, 3]), ("&lt;", [1, 2])]

    But if the 'operand_to_literal_hash' did *not* contain an entry, we'd instead
    default to returning:

        [("==", [0, 1]), ("&lt;", [1, 2]), ("==", [2, 3])]

    This function is currently only used to assist with type-narrowing refinements
    and is extracted out to a helper function so we can unit test it.
    """
    groups: dict[str, DisjointDict[Key, int]] = {op: DisjointDict() for op in operators_to_group}

    simplified_operator_list: list[tuple[str, list[int]]] = []
    last_operator: str | None = None
    current_indices: set[int] = set()
    current_hashes: set[Key] = set()
    for i, (operator, left_expr, right_expr) in enumerate(pairwise_comparisons):
        if last_operator is None:
            last_operator = operator

        if current_indices and (operator != last_operator or operator not in operators_to_group):
            # If some of the operands in the chain are assignable, defer adding it: we might
            # end up needing to merge it with other chains that appear later.
            if len(current_hashes) == 0:
                simplified_operator_list.append((last_operator, sorted(current_indices)))
            else:
                groups[last_operator].add_mapping(current_hashes, current_indices)
            last_operator = operator
            current_indices = set()
            current_hashes = set()

        # Note: 'i' corresponds to the left operand index, so 'i + 1' is the
        # right operand.
        current_indices.add(i)
        current_indices.add(i + 1)

        # We only ever want to combine operands/combine chains for these operators
        if operator in operators_to_group:
            left_hash = operand_to_literal_hash.get(i)
            if left_hash is not None:
                current_hashes.add(left_hash)
            right_hash = operand_to_literal_hash.get(i + 1)
            if right_hash is not None:
                current_hashes.add(right_hash)

    if last_operator is not None:
        if len(current_hashes) == 0:
            simplified_operator_list.append((last_operator, sorted(current_indices)))
        else:
            groups[last_operator].add_mapping(current_hashes, current_indices)

    # Now that we know which chains happen to contain the same underlying expressions
    # and can be merged together, add in this info back to the output.
    for operator, disjoint_dict in groups.items():
        for keys, indices in disjoint_dict.items():
            simplified_operator_list.append((operator, sorted(indices)))

    # For stability, reorder list by the first operand index to appear
    simplified_operator_list.sort(key=lambda item: item[1][0])
    return simplified_operator_list


</t>
<t tx="ekr.20221004064034.587">def is_typed_callable(c: Type | None) -&gt; bool:
    c = get_proper_type(c)
    if not c or not isinstance(c, CallableType):
        return False
    return not all(
        isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated
        for t in get_proper_types(c.arg_types + [c.ret_type])
    )


</t>
<t tx="ekr.20221004064034.588">def is_untyped_decorator(typ: Type | None) -&gt; bool:
    typ = get_proper_type(typ)
    if not typ:
        return True
    elif isinstance(typ, CallableType):
        return not is_typed_callable(typ)
    elif isinstance(typ, Instance):
        method = typ.type.get_method("__call__")
        if method:
            if isinstance(method, Decorator):
                return is_untyped_decorator(method.func.type) or is_untyped_decorator(
                    method.var.type
                )

            if isinstance(method.type, Overloaded):
                return any(is_untyped_decorator(item) for item in method.type.items)
            else:
                return not is_typed_callable(method.type)
        else:
            return False
    elif isinstance(typ, Overloaded):
        return any(is_untyped_decorator(item) for item in typ.items)
    return True


</t>
<t tx="ekr.20221004064034.589">def is_static(func: FuncBase | Decorator) -&gt; bool:
    if isinstance(func, Decorator):
        return is_static(func.func)
    elif isinstance(func, FuncBase):
        return func.is_static
    assert False, f"Unexpected func type: {type(func)}"


</t>
<t tx="ekr.20221004064034.59">async def plain_host_coroutine(func) -&gt; None:
    x = 0
    x = await func()  # noqa: F841


</t>
<t tx="ekr.20221004064034.590">def is_property(defn: SymbolNode) -&gt; bool:
    if isinstance(defn, Decorator):
        return defn.func.is_property
    if isinstance(defn, OverloadedFuncDef):
        if defn.items and isinstance(defn.items[0], Decorator):
            return defn.items[0].func.is_property
    return False


</t>
<t tx="ekr.20221004064034.591">def get_property_type(t: ProperType) -&gt; ProperType:
    if isinstance(t, CallableType):
        return get_proper_type(t.ret_type)
    if isinstance(t, Overloaded):
        return get_proper_type(t.items[0].ret_type)
    return t


</t>
<t tx="ekr.20221004064034.592">def is_subtype_no_promote(left: Type, right: Type) -&gt; bool:
    return is_subtype(left, right, ignore_promotions=True)


</t>
<t tx="ekr.20221004064034.593">def is_overlapping_types_no_promote_no_uninhabited(left: Type, right: Type) -&gt; bool:
    # For the purpose of unsafe overload checks we consider list[&lt;nothing&gt;] and list[int]
    # non-overlapping. This is consistent with how we treat list[int] and list[str] as
    # non-overlapping, despite [] belongs to both. Also this will prevent false positives
    # for failed type inference during unification.
    return is_overlapping_types(left, right, ignore_promotions=True, ignore_uninhabited=True)


</t>
<t tx="ekr.20221004064034.594">def is_private(node_name: str) -&gt; bool:
    """Check if node is private to class definition."""
    return node_name.startswith("__") and not node_name.endswith("__")


</t>
<t tx="ekr.20221004064034.595">def is_string_literal(typ: Type) -&gt; bool:
    strs = try_getting_str_literals_from_type(typ)
    return strs is not None and len(strs) == 1


</t>
<t tx="ekr.20221004064034.596">def has_bool_item(typ: ProperType) -&gt; bool:
    """Return True if type is 'bool' or a union with a 'bool' item."""
    if is_named_instance(typ, "builtins.bool"):
        return True
    if isinstance(typ, UnionType):
        return any(is_named_instance(item, "builtins.bool") for item in typ.items)
    return False


</t>
<t tx="ekr.20221004064034.597">def collapse_walrus(e: Expression) -&gt; Expression:
    """If an expression is an AssignmentExpr, pull out the assignment target.

    We don't make any attempt to pull out all the targets in code like `x := (y := z)`.
    We could support narrowing those if that sort of code turns out to be common.
    """
    if isinstance(e, AssignmentExpr):
        return e.target
    return e
</t>
<t tx="ekr.20221004064034.598">@path C:/Repos/ekr-mypy2/mypy/
"""Expression type checker. This file is conceptually part of TypeChecker."""

from __future__ import annotations

import itertools
from contextlib import contextmanager
from typing import Callable, ClassVar, Iterator, List, Optional, Sequence, cast
from typing_extensions import Final, TypeAlias as _TypeAlias, overload

import mypy.checker
import mypy.errorcodes as codes
from mypy import applytype, erasetype, join, message_registry, nodes, operators, types
from mypy.argmap import ArgTypeExpander, map_actuals_to_formals, map_formals_to_actuals
from mypy.checkmember import analyze_member_access, type_object_type
from mypy.checkstrformat import StringFormatterChecker
from mypy.erasetype import erase_type, remove_instance_last_known_values, replace_meta_vars
from mypy.errors import ErrorWatcher, report_internal_error
from mypy.expandtype import expand_type, expand_type_by_instance, freshen_function_type_vars
from mypy.infer import ArgumentInferContext, infer_function_type_arguments, infer_type_arguments
from mypy.literals import literal
from mypy.maptype import map_instance_to_supertype
from mypy.meet import is_overlapping_types, narrow_declared_type
from mypy.message_registry import ErrorMessage
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    IMPLICITLY_ABSTRACT,
    LITERAL_TYPE,
    REVEAL_TYPE,
    ArgKind,
    AssertTypeExpr,
    AssignmentExpr,
    AwaitExpr,
    BytesExpr,
    CallExpr,
    CastExpr,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    Context,
    Decorator,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    FloatExpr,
    FuncDef,
    GeneratorExpr,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    OpExpr,
    OverloadedFuncDef,
    ParamSpecExpr,
    PlaceholderNode,
    PromoteExpr,
    RefExpr,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    StrExpr,
    SuperExpr,
    SymbolNode,
    TempNode,
    TupleExpr,
    TypeAlias,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeInfo,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    YieldExpr,
    YieldFromExpr,
)
from mypy.plugin import (
    FunctionContext,
    FunctionSigContext,
    MethodContext,
    MethodSigContext,
    Plugin,
)
from mypy.semanal_enum import ENUM_BASES
from mypy.state import state
from mypy.subtypes import is_equivalent, is_same_type, is_subtype, non_method_protocol_members
from mypy.traverser import has_await_expression
from mypy.typeanal import (
    check_for_explicit_any,
    expand_type_alias,
    has_any_from_unimported_type,
    make_optional_type,
    set_any_tvars,
)
from mypy.typeops import (
    callable_type,
    custom_special_method,
    erase_to_union_or_bound,
    false_only,
    function_type,
    is_literal_type_like,
    make_simplified_union,
    simple_literal_type,
    true_only,
    try_expanding_sum_type_to_union,
    try_getting_str_literals,
    tuple_fallback,
)
from mypy.types import (
    LITERAL_TYPE_NAMES,
    TUPLE_LIKE_INSTANCE_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    ExtraAttrs,
    FunctionLike,
    Instance,
    LiteralType,
    LiteralValue,
    NoneType,
    Overloaded,
    ParamSpecFlavor,
    ParamSpecType,
    PartialType,
    ProperType,
    StarType,
    TupleType,
    Type,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarType,
    UninhabitedType,
    UnionType,
    flatten_nested_unions,
    get_proper_type,
    get_proper_types,
    has_recursive_types,
    is_generic_instance,
    is_named_instance,
    is_optional,
    is_self_type_like,
    remove_optional,
)
from mypy.typestate import TypeState
from mypy.typevars import fill_typevars
from mypy.util import split_module_names
from mypy.visitor import ExpressionVisitor

# Type of callback user for checking individual function arguments. See
# check_args() below for details.
ArgChecker: _TypeAlias = Callable[
    [Type, Type, ArgKind, Type, int, int, CallableType, Optional[Type], Context, Context], None,
]

# Maximum nesting level for math union in overloads, setting this to large values
# may cause performance issues. The reason is that although union math algorithm we use
# nicely captures most corner cases, its worst case complexity is exponential,
# see https://github.com/python/mypy/pull/5255#discussion_r196896335 for discussion.
MAX_UNIONS: Final = 5


# Types considered safe for comparisons with --strict-equality due to known behaviour of __eq__.
# NOTE: All these types are subtypes of AbstractSet.
OVERLAPPING_TYPES_ALLOWLIST: Final = [
    "builtins.set",
    "builtins.frozenset",
    "typing.KeysView",
    "typing.ItemsView",
    "builtins._dict_keys",
    "builtins._dict_items",
    "_collections_abc.dict_keys",
    "_collections_abc.dict_items",
]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.599">class TooManyUnions(Exception):
    """Indicates that we need to stop splitting unions in an attempt
    to match an overload in order to save performance.
    """


</t>
<t tx="ekr.20221004064034.6">@path C:/Repos/ekr-mypy2/
"""Stand-alone test file for issue #12352"""

@others
# a: str="abc"
# b="xyz"
# c: str
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.60">@coroutine
def decorated_host_generator(func) -&gt; Generator[str, None, None]:
    yield "a"
    x = 0
    f = func()
    try:
        x = yield from f  # noqa: F841
    finally:
        try:
            f.close()
        except AttributeError:
            pass


</t>
<t tx="ekr.20221004064034.600">def allow_fast_container_literal(t: ProperType) -&gt; bool:
    return isinstance(t, Instance) or (
        isinstance(t, TupleType)
        and all(allow_fast_container_literal(get_proper_type(it)) for it in t.items)
    )


</t>
<t tx="ekr.20221004064034.601">def extract_refexpr_names(expr: RefExpr) -&gt; set[str]:
    """Recursively extracts all module references from a reference expression.

    Note that currently, the only two subclasses of RefExpr are NameExpr and
    MemberExpr."""
    output: set[str] = set()
    while isinstance(expr.node, MypyFile) or expr.fullname is not None:
        if isinstance(expr.node, MypyFile) and expr.fullname is not None:
            # If it's None, something's wrong (perhaps due to an
            # import cycle or a suppressed error).  For now we just
            # skip it.
            output.add(expr.fullname)

        if isinstance(expr, NameExpr):
            is_suppressed_import = isinstance(expr.node, Var) and expr.node.is_suppressed_import
            if isinstance(expr.node, TypeInfo):
                # Reference to a class or a nested class
                output.update(split_module_names(expr.node.module_name))
            elif expr.fullname is not None and "." in expr.fullname and not is_suppressed_import:
                # Everything else (that is not a silenced import within a class)
                output.add(expr.fullname.rsplit(".", 1)[0])
            break
        elif isinstance(expr, MemberExpr):
            if isinstance(expr.expr, RefExpr):
                expr = expr.expr
            else:
                break
        else:
            raise AssertionError(f"Unknown RefExpr subclass: {type(expr)}")
    return output


</t>
<t tx="ekr.20221004064034.602">class Finished(Exception):
    """Raised if we can terminate overload argument check early (no match)."""


</t>
<t tx="ekr.20221004064034.603">class ExpressionChecker(ExpressionVisitor[Type]):
    """Expression type checker.

    This class works closely together with checker.TypeChecker.
    """

    # Some services are provided by a TypeChecker instance.
    chk: mypy.checker.TypeChecker
    # This is shared with TypeChecker, but stored also here for convenience.
    msg: MessageBuilder
    # Type context for type inference
    type_context: list[Type | None]

    # cache resolved types in some cases
    resolved_type: dict[Expression, ProperType]

    strfrm_checker: StringFormatterChecker
    plugin: Plugin

    @others
</t>
<t tx="ekr.20221004064034.604">def __init__(self, chk: mypy.checker.TypeChecker, msg: MessageBuilder, plugin: Plugin) -&gt; None:
    """Construct an expression type checker."""
    self.chk = chk
    self.msg = msg
    self.plugin = plugin
    self.type_context = [None]

    # Temporary overrides for expression types. This is currently
    # used by the union math in overloads.
    # TODO: refactor this to use a pattern similar to one in
    # multiassign_from_union, or maybe even combine the two?
    self.type_overrides: dict[Expression, Type] = {}
    self.strfrm_checker = StringFormatterChecker(self, self.chk, self.msg)

    self.resolved_type = {}

    # Callee in a call expression is in some sense both runtime context and
    # type context, because we support things like C[int](...). Store information
    # on whether current expression is a callee, to give better error messages
    # related to type context.
    self.is_callee = False

</t>
<t tx="ekr.20221004064034.605">def reset(self) -&gt; None:
    self.resolved_type = {}

</t>
<t tx="ekr.20221004064034.606">def visit_name_expr(self, e: NameExpr) -&gt; Type:
    """Type check a name expression.

    It can be of any kind: local, member or global.
    """
    self.chk.module_refs.update(extract_refexpr_names(e))
    result = self.analyze_ref_expr(e)
    return self.narrow_type_from_binder(e, result)

</t>
<t tx="ekr.20221004064034.607">def analyze_ref_expr(self, e: RefExpr, lvalue: bool = False) -&gt; Type:
    result: Type | None = None
    node = e.node

    if isinstance(e, NameExpr) and e.is_special_form:
        # A special form definition, nothing to check here.
        return AnyType(TypeOfAny.special_form)

    if isinstance(node, Var):
        # Variable reference.
        result = self.analyze_var_ref(node, e)
        if isinstance(result, PartialType):
            result = self.chk.handle_partial_var_type(result, lvalue, node, e)
    elif isinstance(node, FuncDef):
        # Reference to a global function.
        result = function_type(node, self.named_type("builtins.function"))
    elif isinstance(node, OverloadedFuncDef) and node.type is not None:
        # node.type is None when there are multiple definitions of a function
        # and it's decorated by something that is not typing.overload
        # TODO: use a dummy Overloaded instead of AnyType in this case
        # like we do in mypy.types.function_type()?
        result = node.type
    elif isinstance(node, TypeInfo):
        # Reference to a type object.
        if node.typeddict_type:
            # We special-case TypedDict, because they don't define any constructor.
            result = self.typeddict_callable(node)
        else:
            result = type_object_type(node, self.named_type)
        if isinstance(result, CallableType) and isinstance(  # type: ignore[misc]
            result.ret_type, Instance
        ):
            # We need to set correct line and column
            # TODO: always do this in type_object_type by passing the original context
            result.ret_type.line = e.line
            result.ret_type.column = e.column
        if isinstance(get_proper_type(self.type_context[-1]), TypeType):
            # This is the type in a Type[] expression, so substitute type
            # variables with Any.
            result = erasetype.erase_typevars(result)
    elif isinstance(node, MypyFile):
        # Reference to a module object.
        result = self.module_type(node)
    elif isinstance(node, Decorator):
        result = self.analyze_var_ref(node.var, e)
    elif isinstance(node, TypeAlias):
        # Something that refers to a type alias appears in runtime context.
        # Note that we suppress bogus errors for alias redefinitions,
        # they are already reported in semanal.py.
        result = self.alias_type_in_runtime_context(
            node, ctx=e, alias_definition=e.is_alias_rvalue or lvalue
        )
    elif isinstance(node, (TypeVarExpr, ParamSpecExpr)):
        result = self.object_type()
    else:
        if isinstance(node, PlaceholderNode):
            assert False, f"PlaceholderNode {node.fullname!r} leaked to checker"
        # Unknown reference; use any type implicitly to avoid
        # generating extra type errors.
        result = AnyType(TypeOfAny.from_error)
    assert result is not None
    return result

</t>
<t tx="ekr.20221004064034.608">def analyze_var_ref(self, var: Var, context: Context) -&gt; Type:
    if var.type:
        var_type = get_proper_type(var.type)
        if isinstance(var_type, Instance):
            if self.is_literal_context() and var_type.last_known_value is not None:
                return var_type.last_known_value
            if var.name in {"True", "False"}:
                return self.infer_literal_expr_type(var.name == "True", "builtins.bool")
        return var.type
    else:
        if not var.is_ready and self.chk.in_checked_function():
            self.chk.handle_cannot_determine_type(var.name, context)
        # Implicit 'Any' type.
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.609">def module_type(self, node: MypyFile) -&gt; Instance:
    try:
        result = self.named_type("types.ModuleType")
    except KeyError:
        # In test cases might 'types' may not be available.
        # Fall back to a dummy 'object' type instead to
        # avoid a crash.
        result = self.named_type("builtins.object")
    module_attrs = {}
    immutable = set()
    for name, n in node.names.items():
        if not n.module_public:
            continue
        if isinstance(n.node, Var) and n.node.is_final:
            immutable.add(name)
        typ = self.chk.determine_type_of_member(n)
        if typ:
            module_attrs[name] = typ
        else:
            # TODO: what to do about nested module references?
            # They are non-trivial because there may be import cycles.
            module_attrs[name] = AnyType(TypeOfAny.special_form)
    result.extra_attrs = ExtraAttrs(module_attrs, immutable, node.fullname)
    return result

</t>
<t tx="ekr.20221004064034.61">@coroutine
async def decorated_host_coroutine(func) -&gt; None:
    x = 0
    x = await func()  # noqa: F841


</t>
<t tx="ekr.20221004064034.610">def visit_call_expr(self, e: CallExpr, allow_none_return: bool = False) -&gt; Type:
    """Type check a call expression."""
    if e.analyzed:
        if isinstance(e.analyzed, NamedTupleExpr) and not e.analyzed.is_typed:
            # Type check the arguments, but ignore the results. This relies
            # on the typeshed stubs to type check the arguments.
            self.visit_call_expr_inner(e)
        # It's really a special form that only looks like a call.
        return self.accept(e.analyzed, self.type_context[-1])
    return self.visit_call_expr_inner(e, allow_none_return=allow_none_return)

</t>
<t tx="ekr.20221004064034.611">def refers_to_typeddict(self, base: Expression) -&gt; bool:
    if not isinstance(base, RefExpr):
        return False
    if isinstance(base.node, TypeInfo) and base.node.typeddict_type is not None:
        # Direct reference.
        return True
    return isinstance(base.node, TypeAlias) and isinstance(
        get_proper_type(base.node.target), TypedDictType
    )

</t>
<t tx="ekr.20221004064034.612">def visit_call_expr_inner(self, e: CallExpr, allow_none_return: bool = False) -&gt; Type:
    if (
        self.refers_to_typeddict(e.callee)
        or isinstance(e.callee, IndexExpr)
        and self.refers_to_typeddict(e.callee.base)
    ):
        typeddict_callable = get_proper_type(self.accept(e.callee, is_callee=True))
        if isinstance(typeddict_callable, CallableType):
            typeddict_type = get_proper_type(typeddict_callable.ret_type)
            assert isinstance(typeddict_type, TypedDictType)
            return self.check_typeddict_call(
                typeddict_type, e.arg_kinds, e.arg_names, e.args, e, typeddict_callable
            )
    if (
        isinstance(e.callee, NameExpr)
        and e.callee.name in ("isinstance", "issubclass")
        and len(e.args) == 2
    ):
        for typ in mypy.checker.flatten(e.args[1]):
            node = None
            if isinstance(typ, NameExpr):
                try:
                    node = self.chk.lookup_qualified(typ.name)
                except KeyError:
                    # Undefined names should already be reported in semantic analysis.
                    pass
            if is_expr_literal_type(typ):
                self.msg.cannot_use_function_with_type(e.callee.name, "Literal", e)
                continue
            if (
                node
                and isinstance(node.node, TypeAlias)
                and isinstance(get_proper_type(node.node.target), AnyType)
            ):
                self.msg.cannot_use_function_with_type(e.callee.name, "Any", e)
                continue
            if (
                isinstance(typ, IndexExpr)
                and isinstance(typ.analyzed, (TypeApplication, TypeAliasExpr))
            ) or (
                isinstance(typ, NameExpr)
                and node
                and isinstance(node.node, TypeAlias)
                and not node.node.no_args
            ):
                self.msg.type_arguments_not_allowed(e)
            if isinstance(typ, RefExpr) and isinstance(typ.node, TypeInfo):
                if typ.node.typeddict_type:
                    self.msg.cannot_use_function_with_type(e.callee.name, "TypedDict", e)
                elif typ.node.is_newtype:
                    self.msg.cannot_use_function_with_type(e.callee.name, "NewType", e)
    self.try_infer_partial_type(e)
    type_context = None
    if isinstance(e.callee, LambdaExpr):
        formal_to_actual = map_actuals_to_formals(
            e.arg_kinds,
            e.arg_names,
            e.callee.arg_kinds,
            e.callee.arg_names,
            lambda i: self.accept(e.args[i]),
        )

        arg_types = [
            join.join_type_list([self.accept(e.args[j]) for j in formal_to_actual[i]])
            for i in range(len(e.callee.arg_kinds))
        ]
        type_context = CallableType(
            arg_types,
            e.callee.arg_kinds,
            e.callee.arg_names,
            ret_type=self.object_type(),
            fallback=self.named_type("builtins.function"),
        )
    callee_type = get_proper_type(
        self.accept(e.callee, type_context, always_allow_any=True, is_callee=True)
    )
    if (
        self.chk.options.disallow_untyped_calls
        and self.chk.in_checked_function()
        and isinstance(callee_type, CallableType)
        and callee_type.implicit
    ):
        self.msg.untyped_function_call(callee_type, e)

    # Figure out the full name of the callee for plugin lookup.
    object_type = None
    member = None
    fullname = None
    if isinstance(e.callee, RefExpr):
        # There are two special cases where plugins might act:
        # * A "static" reference/alias to a class or function;
        #   get_function_hook() will be invoked for these.
        fullname = e.callee.fullname
        if isinstance(e.callee.node, TypeAlias):
            target = get_proper_type(e.callee.node.target)
            if isinstance(target, Instance):
                fullname = target.type.fullname
        # * Call to a method on object that has a full name (see
        #   method_fullname() for details on supported objects);
        #   get_method_hook() and get_method_signature_hook() will
        #   be invoked for these.
        if (
            fullname is None
            and isinstance(e.callee, MemberExpr)
            and self.chk.has_type(e.callee.expr)
        ):
            member = e.callee.name
            object_type = self.chk.lookup_type(e.callee.expr)
    ret_type = self.check_call_expr_with_callee_type(
        callee_type, e, fullname, object_type, member
    )
    if isinstance(e.callee, RefExpr) and len(e.args) == 2:
        if e.callee.fullname in ("builtins.isinstance", "builtins.issubclass"):
            self.check_runtime_protocol_test(e)
        if e.callee.fullname == "builtins.issubclass":
            self.check_protocol_issubclass(e)
    if isinstance(e.callee, MemberExpr) and e.callee.name == "format":
        self.check_str_format_call(e)
    ret_type = get_proper_type(ret_type)
    if isinstance(ret_type, UnionType):
        ret_type = make_simplified_union(ret_type.items)
    if isinstance(ret_type, UninhabitedType) and not ret_type.ambiguous:
        self.chk.binder.unreachable()
    # Warn on calls to functions that always return None. The check
    # of ret_type is both a common-case optimization and prevents reporting
    # the error in dynamic functions (where it will be Any).
    if (
        not allow_none_return
        and isinstance(ret_type, NoneType)
        and self.always_returns_none(e.callee)
    ):
        self.chk.msg.does_not_return_value(callee_type, e)
        return AnyType(TypeOfAny.from_error)
    return ret_type

</t>
<t tx="ekr.20221004064034.613">def check_str_format_call(self, e: CallExpr) -&gt; None:
    """More precise type checking for str.format() calls on literals."""
    assert isinstance(e.callee, MemberExpr)
    format_value = None
    if isinstance(e.callee.expr, StrExpr):
        format_value = e.callee.expr.value
    elif self.chk.has_type(e.callee.expr):
        base_typ = try_getting_literal(self.chk.lookup_type(e.callee.expr))
        if isinstance(base_typ, LiteralType) and isinstance(base_typ.value, str):
            format_value = base_typ.value
    if format_value is not None:
        self.strfrm_checker.check_str_format_call(e, format_value)

</t>
<t tx="ekr.20221004064034.614">def method_fullname(self, object_type: Type, method_name: str) -&gt; str | None:
    """Convert a method name to a fully qualified name, based on the type of the object that
    it is invoked on. Return `None` if the name of `object_type` cannot be determined.
    """
    object_type = get_proper_type(object_type)

    if isinstance(object_type, CallableType) and object_type.is_type_obj():
        # For class method calls, object_type is a callable representing the class object.
        # We "unwrap" it to a regular type, as the class/instance method difference doesn't
        # affect the fully qualified name.
        object_type = get_proper_type(object_type.ret_type)
    elif isinstance(object_type, TypeType):
        object_type = object_type.item

    type_name = None
    if isinstance(object_type, Instance):
        type_name = object_type.type.fullname
    elif isinstance(object_type, (TypedDictType, LiteralType)):
        info = object_type.fallback.type.get_containing_type_info(method_name)
        type_name = info.fullname if info is not None else None
    elif isinstance(object_type, TupleType):
        type_name = tuple_fallback(object_type).type.fullname

    if type_name is not None:
        return f"{type_name}.{method_name}"
    else:
        return None

</t>
<t tx="ekr.20221004064034.615">def always_returns_none(self, node: Expression) -&gt; bool:
    """Check if `node` refers to something explicitly annotated as only returning None."""
    if isinstance(node, RefExpr):
        if self.defn_returns_none(node.node):
            return True
    if isinstance(node, MemberExpr) and node.node is None:  # instance or class attribute
        typ = get_proper_type(self.chk.lookup_type(node.expr))
        if isinstance(typ, Instance):
            info = typ.type
        elif isinstance(typ, CallableType) and typ.is_type_obj():
            ret_type = get_proper_type(typ.ret_type)
            if isinstance(ret_type, Instance):
                info = ret_type.type
            else:
                return False
        else:
            return False
        sym = info.get(node.name)
        if sym and self.defn_returns_none(sym.node):
            return True
    return False

</t>
<t tx="ekr.20221004064034.616">def defn_returns_none(self, defn: SymbolNode | None) -&gt; bool:
    """Check if `defn` can _only_ return None."""
    if isinstance(defn, FuncDef):
        return isinstance(defn.type, CallableType) and isinstance(
            get_proper_type(defn.type.ret_type), NoneType
        )
    if isinstance(defn, OverloadedFuncDef):
        return all(self.defn_returns_none(item) for item in defn.items)
    if isinstance(defn, Var):
        typ = get_proper_type(defn.type)
        if (
            not defn.is_inferred
            and isinstance(typ, CallableType)
            and isinstance(get_proper_type(typ.ret_type), NoneType)
        ):
            return True
        if isinstance(typ, Instance):
            sym = typ.type.get("__call__")
            if sym and self.defn_returns_none(sym.node):
                return True
    return False

</t>
<t tx="ekr.20221004064034.617">def check_runtime_protocol_test(self, e: CallExpr) -&gt; None:
    for expr in mypy.checker.flatten(e.args[1]):
        tp = get_proper_type(self.chk.lookup_type(expr))
        if (
            isinstance(tp, CallableType)
            and tp.is_type_obj()
            and tp.type_object().is_protocol
            and not tp.type_object().runtime_protocol
        ):
            self.chk.fail(message_registry.RUNTIME_PROTOCOL_EXPECTED, e)

</t>
<t tx="ekr.20221004064034.618">def check_protocol_issubclass(self, e: CallExpr) -&gt; None:
    for expr in mypy.checker.flatten(e.args[1]):
        tp = get_proper_type(self.chk.lookup_type(expr))
        if isinstance(tp, CallableType) and tp.is_type_obj() and tp.type_object().is_protocol:
            attr_members = non_method_protocol_members(tp.type_object())
            if attr_members:
                self.chk.msg.report_non_method_protocol(tp.type_object(), attr_members, e)

</t>
<t tx="ekr.20221004064034.619">def check_typeddict_call(
    self,
    callee: TypedDictType,
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None],
    args: list[Expression],
    context: Context,
    orig_callee: Type | None,
) -&gt; Type:
    if len(args) &gt;= 1 and all([ak == ARG_NAMED for ak in arg_kinds]):
        # ex: Point(x=42, y=1337)
        assert all(arg_name is not None for arg_name in arg_names)
        item_names = cast(List[str], arg_names)
        item_args = args
        return self.check_typeddict_call_with_kwargs(
            callee, dict(zip(item_names, item_args)), context, orig_callee
        )

    if len(args) == 1 and arg_kinds[0] == ARG_POS:
        unique_arg = args[0]
        if isinstance(unique_arg, DictExpr):
            # ex: Point({'x': 42, 'y': 1337})
            return self.check_typeddict_call_with_dict(
                callee, unique_arg, context, orig_callee
            )
        if isinstance(unique_arg, CallExpr) and isinstance(unique_arg.analyzed, DictExpr):
            # ex: Point(dict(x=42, y=1337))
            return self.check_typeddict_call_with_dict(
                callee, unique_arg.analyzed, context, orig_callee
            )

    if len(args) == 0:
        # ex: EmptyDict()
        return self.check_typeddict_call_with_kwargs(callee, {}, context, orig_callee)

    self.chk.fail(message_registry.INVALID_TYPEDDICT_ARGS, context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064034.62"># Main driver.


</t>
<t tx="ekr.20221004064034.620">def validate_typeddict_kwargs(self, kwargs: DictExpr) -&gt; dict[str, Expression] | None:
    item_args = [item[1] for item in kwargs.items]

    item_names = []  # List[str]
    for item_name_expr, item_arg in kwargs.items:
        literal_value = None
        if item_name_expr:
            key_type = self.accept(item_name_expr)
            values = try_getting_str_literals(item_name_expr, key_type)
            if values and len(values) == 1:
                literal_value = values[0]
        if literal_value is None:
            key_context = item_name_expr or item_arg
            self.chk.fail(message_registry.TYPEDDICT_KEY_MUST_BE_STRING_LITERAL, key_context)
            return None
        else:
            item_names.append(literal_value)
    return dict(zip(item_names, item_args))

</t>
<t tx="ekr.20221004064034.621">def match_typeddict_call_with_dict(
    self, callee: TypedDictType, kwargs: DictExpr, context: Context
) -&gt; bool:
    validated_kwargs = self.validate_typeddict_kwargs(kwargs=kwargs)
    if validated_kwargs is not None:
        return callee.required_keys &lt;= set(validated_kwargs.keys()) &lt;= set(callee.items.keys())
    else:
        return False

</t>
<t tx="ekr.20221004064034.622">def check_typeddict_call_with_dict(
    self, callee: TypedDictType, kwargs: DictExpr, context: Context, orig_callee: Type | None
) -&gt; Type:
    validated_kwargs = self.validate_typeddict_kwargs(kwargs=kwargs)
    if validated_kwargs is not None:
        return self.check_typeddict_call_with_kwargs(
            callee, kwargs=validated_kwargs, context=context, orig_callee=orig_callee
        )
    else:
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064034.623">def typeddict_callable(self, info: TypeInfo) -&gt; CallableType:
    """Construct a reasonable type for a TypedDict type in runtime context.

    If it appears as a callee, it will be special-cased anyway, e.g. it is
    also allowed to accept a single positional argument if it is a dict literal.

    Note it is not safe to move this to type_object_type() since it will crash
    on plugin-generated TypedDicts, that may not have the special_alias.
    """
    assert info.special_alias is not None
    target = info.special_alias.target
    assert isinstance(target, ProperType) and isinstance(target, TypedDictType)
    expected_types = list(target.items.values())
    kinds = [ArgKind.ARG_NAMED] * len(expected_types)
    names = list(target.items.keys())
    return CallableType(
        expected_types,
        kinds,
        names,
        target,
        self.named_type("builtins.type"),
        variables=info.defn.type_vars,
    )

</t>
<t tx="ekr.20221004064034.624">def typeddict_callable_from_context(self, callee: TypedDictType) -&gt; CallableType:
    return CallableType(
        list(callee.items.values()),
        [ArgKind.ARG_NAMED] * len(callee.items),
        list(callee.items.keys()),
        callee,
        self.named_type("builtins.type"),
    )

</t>
<t tx="ekr.20221004064034.625">def check_typeddict_call_with_kwargs(
    self,
    callee: TypedDictType,
    kwargs: dict[str, Expression],
    context: Context,
    orig_callee: Type | None,
) -&gt; Type:
    if not (callee.required_keys &lt;= set(kwargs.keys()) &lt;= set(callee.items.keys())):
        expected_keys = [
            key
            for key in callee.items.keys()
            if key in callee.required_keys or key in kwargs.keys()
        ]
        actual_keys = kwargs.keys()
        self.msg.unexpected_typeddict_keys(
            callee, expected_keys=expected_keys, actual_keys=list(actual_keys), context=context
        )
        return AnyType(TypeOfAny.from_error)

    orig_callee = get_proper_type(orig_callee)
    if isinstance(orig_callee, CallableType):
        infer_callee = orig_callee
    else:
        # Try reconstructing from type context.
        if callee.fallback.type.special_alias is not None:
            infer_callee = self.typeddict_callable(callee.fallback.type)
        else:
            # Likely a TypedDict type generated by a plugin.
            infer_callee = self.typeddict_callable_from_context(callee)

    # We don't show any errors, just infer types in a generic TypedDict type,
    # a custom error message will be given below, if there are errors.
    with self.msg.filter_errors(), self.chk.local_type_map():
        orig_ret_type, _ = self.check_callable_call(
            infer_callee,
            list(kwargs.values()),
            [ArgKind.ARG_NAMED] * len(kwargs),
            context,
            list(kwargs.keys()),
            None,
            None,
            None,
        )

    ret_type = get_proper_type(orig_ret_type)
    if not isinstance(ret_type, TypedDictType):
        # If something went really wrong, type-check call with original type,
        # this may give a better error message.
        ret_type = callee

    for (item_name, item_expected_type) in ret_type.items.items():
        if item_name in kwargs:
            item_value = kwargs[item_name]
            self.chk.check_simple_assignment(
                lvalue_type=item_expected_type,
                rvalue=item_value,
                context=item_value,
                msg=ErrorMessage(
                    message_registry.INCOMPATIBLE_TYPES.value, code=codes.TYPEDDICT_ITEM
                ),
                lvalue_name=f'TypedDict item "{item_name}"',
                rvalue_name="expression",
            )

    return orig_ret_type

</t>
<t tx="ekr.20221004064034.626">def get_partial_self_var(self, expr: MemberExpr) -&gt; Var | None:
    """Get variable node for a partial self attribute.

    If the expression is not a self attribute, or attribute is not variable,
    or variable is not partial, return None.
    """
    if not (
        isinstance(expr.expr, NameExpr)
        and isinstance(expr.expr.node, Var)
        and expr.expr.node.is_self
    ):
        # Not a self.attr expression.
        return None
    info = self.chk.scope.enclosing_class()
    if not info or expr.name not in info.names:
        # Don't mess with partial types in superclasses.
        return None
    sym = info.names[expr.name]
    if isinstance(sym.node, Var) and isinstance(sym.node.type, PartialType):
        return sym.node
    return None

</t>
<t tx="ekr.20221004064034.627"># Types and methods that can be used to infer partial types.
item_args: ClassVar[dict[str, list[str]]] = {
    "builtins.list": ["append"],
    "builtins.set": ["add", "discard"],
}
container_args: ClassVar[dict[str, dict[str, list[str]]]] = {
    "builtins.list": {"extend": ["builtins.list"]},
    "builtins.dict": {"update": ["builtins.dict"]},
    "collections.OrderedDict": {"update": ["builtins.dict"]},
    "builtins.set": {"update": ["builtins.set", "builtins.list"]},
}

</t>
<t tx="ekr.20221004064034.628">def try_infer_partial_type(self, e: CallExpr) -&gt; None:
    """Try to make partial type precise from a call."""
    if not isinstance(e.callee, MemberExpr):
        return
    callee = e.callee
    if isinstance(callee.expr, RefExpr):
        # Call a method with a RefExpr callee, such as 'x.method(...)'.
        ret = self.get_partial_var(callee.expr)
        if ret is None:
            return
        var, partial_types = ret
        typ = self.try_infer_partial_value_type_from_call(e, callee.name, var)
        if typ is not None:
            var.type = typ
            del partial_types[var]
    elif isinstance(callee.expr, IndexExpr) and isinstance(callee.expr.base, RefExpr):
        # Call 'x[y].method(...)'; may infer type of 'x' if it's a partial defaultdict.
        if callee.expr.analyzed is not None:
            return  # A special form
        base = callee.expr.base
        index = callee.expr.index
        ret = self.get_partial_var(base)
        if ret is None:
            return
        var, partial_types = ret
        partial_type = get_partial_instance_type(var.type)
        if partial_type is None or partial_type.value_type is None:
            return
        value_type = self.try_infer_partial_value_type_from_call(e, callee.name, var)
        if value_type is not None:
            # Infer key type.
            key_type = self.accept(index)
            if mypy.checker.is_valid_inferred_type(key_type):
                # Store inferred partial type.
                assert partial_type.type is not None
                typename = partial_type.type.fullname
                var.type = self.chk.named_generic_type(typename, [key_type, value_type])
                del partial_types[var]

</t>
<t tx="ekr.20221004064034.629">def get_partial_var(self, ref: RefExpr) -&gt; tuple[Var, dict[Var, Context]] | None:
    var = ref.node
    if var is None and isinstance(ref, MemberExpr):
        var = self.get_partial_self_var(ref)
    if not isinstance(var, Var):
        return None
    partial_types = self.chk.find_partial_types(var)
    if partial_types is None:
        return None
    return var, partial_types

</t>
<t tx="ekr.20221004064034.63">def main() -&gt; None:
    verbose = "-v" in sys.argv
    for host in [
        plain_host_generator,
        plain_host_coroutine,
        decorated_host_generator,
        decorated_host_coroutine,
    ]:
        print()
        print("==== Host:", host.__name__)
        for func in [
            plain_generator,
            plain_coroutine,
            decorated_generator,
            decorated_coroutine,
            other_iterator,
            other_coroutine,
        ]:
            print("  ---- Func:", func.__name__)
            try:
                f = host(func)
                for i in range(10):
                    try:
                        x = f.send(None)
                        if verbose:
                            print("    yield:", x)
                    except StopIteration as e:
                        if verbose:
                            print("    stop:", e.value)
                        break
                else:
                    if verbose:
                        print("    ???? still going")
            except Exception as e:
                print("    error:", repr(e))


</t>
<t tx="ekr.20221004064034.630">def try_infer_partial_value_type_from_call(
    self, e: CallExpr, methodname: str, var: Var
) -&gt; Instance | None:
    """Try to make partial type precise from a call such as 'x.append(y)'."""
    if self.chk.current_node_deferred:
        return None
    partial_type = get_partial_instance_type(var.type)
    if partial_type is None:
        return None
    if partial_type.value_type:
        typename = partial_type.value_type.type.fullname
    else:
        assert partial_type.type is not None
        typename = partial_type.type.fullname
    # Sometimes we can infer a full type for a partial List, Dict or Set type.
    # TODO: Don't infer argument expression twice.
    if (
        typename in self.item_args
        and methodname in self.item_args[typename]
        and e.arg_kinds == [ARG_POS]
    ):
        item_type = self.accept(e.args[0])
        if mypy.checker.is_valid_inferred_type(item_type):
            return self.chk.named_generic_type(typename, [item_type])
    elif (
        typename in self.container_args
        and methodname in self.container_args[typename]
        and e.arg_kinds == [ARG_POS]
    ):
        arg_type = get_proper_type(self.accept(e.args[0]))
        if isinstance(arg_type, Instance):
            arg_typename = arg_type.type.fullname
            if arg_typename in self.container_args[typename][methodname]:
                if all(
                    mypy.checker.is_valid_inferred_type(item_type)
                    for item_type in arg_type.args
                ):
                    return self.chk.named_generic_type(typename, list(arg_type.args))
        elif isinstance(arg_type, AnyType):
            return self.chk.named_type(typename)

    return None

</t>
<t tx="ekr.20221004064034.631">def apply_function_plugin(
    self,
    callee: CallableType,
    arg_kinds: list[ArgKind],
    arg_types: list[Type],
    arg_names: Sequence[str | None] | None,
    formal_to_actual: list[list[int]],
    args: list[Expression],
    fullname: str,
    object_type: Type | None,
    context: Context,
) -&gt; Type:
    """Use special case logic to infer the return type of a specific named function/method.

    Caller must ensure that a plugin hook exists. There are two different cases:

    - If object_type is None, the caller must ensure that a function hook exists
      for fullname.
    - If object_type is not None, the caller must ensure that a method hook exists
      for fullname.

    Return the inferred return type.
    """
    num_formals = len(callee.arg_types)
    formal_arg_types: list[list[Type]] = [[] for _ in range(num_formals)]
    formal_arg_exprs: list[list[Expression]] = [[] for _ in range(num_formals)]
    formal_arg_names: list[list[str | None]] = [[] for _ in range(num_formals)]
    formal_arg_kinds: list[list[ArgKind]] = [[] for _ in range(num_formals)]
    for formal, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            formal_arg_types[formal].append(arg_types[actual])
            formal_arg_exprs[formal].append(args[actual])
            if arg_names:
                formal_arg_names[formal].append(arg_names[actual])
            formal_arg_kinds[formal].append(arg_kinds[actual])

    if object_type is None:
        # Apply function plugin
        callback = self.plugin.get_function_hook(fullname)
        assert callback is not None  # Assume that caller ensures this
        return callback(
            FunctionContext(
                formal_arg_types,
                formal_arg_kinds,
                callee.arg_names,
                formal_arg_names,
                callee.ret_type,
                formal_arg_exprs,
                context,
                self.chk,
            )
        )
    else:
        # Apply method plugin
        method_callback = self.plugin.get_method_hook(fullname)
        assert method_callback is not None  # Assume that caller ensures this
        object_type = get_proper_type(object_type)
        return method_callback(
            MethodContext(
                object_type,
                formal_arg_types,
                formal_arg_kinds,
                callee.arg_names,
                formal_arg_names,
                callee.ret_type,
                formal_arg_exprs,
                context,
                self.chk,
            )
        )

</t>
<t tx="ekr.20221004064034.632">def apply_signature_hook(
    self,
    callee: FunctionLike,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    hook: Callable[[list[list[Expression]], CallableType], FunctionLike],
) -&gt; FunctionLike:
    """Helper to apply a signature hook for either a function or method"""
    if isinstance(callee, CallableType):
        num_formals = len(callee.arg_kinds)
        formal_to_actual = map_actuals_to_formals(
            arg_kinds,
            arg_names,
            callee.arg_kinds,
            callee.arg_names,
            lambda i: self.accept(args[i]),
        )
        formal_arg_exprs: list[list[Expression]] = [[] for _ in range(num_formals)]
        for formal, actuals in enumerate(formal_to_actual):
            for actual in actuals:
                formal_arg_exprs[formal].append(args[actual])
        return hook(formal_arg_exprs, callee)
    else:
        assert isinstance(callee, Overloaded)
        items = []
        for item in callee.items:
            adjusted = self.apply_signature_hook(item, args, arg_kinds, arg_names, hook)
            assert isinstance(adjusted, CallableType)
            items.append(adjusted)
        return Overloaded(items)

</t>
<t tx="ekr.20221004064034.633">def apply_function_signature_hook(
    self,
    callee: FunctionLike,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None,
    signature_hook: Callable[[FunctionSigContext], FunctionLike],
) -&gt; FunctionLike:
    """Apply a plugin hook that may infer a more precise signature for a function."""
    return self.apply_signature_hook(
        callee,
        args,
        arg_kinds,
        arg_names,
        (lambda args, sig: signature_hook(FunctionSigContext(args, sig, context, self.chk))),
    )

</t>
<t tx="ekr.20221004064034.634">def apply_method_signature_hook(
    self,
    callee: FunctionLike,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None,
    object_type: Type,
    signature_hook: Callable[[MethodSigContext], FunctionLike],
) -&gt; FunctionLike:
    """Apply a plugin hook that may infer a more precise signature for a method."""
    pobject_type = get_proper_type(object_type)
    return self.apply_signature_hook(
        callee,
        args,
        arg_kinds,
        arg_names,
        (
            lambda args, sig: signature_hook(
                MethodSigContext(pobject_type, args, sig, context, self.chk)
            )
        ),
    )

</t>
<t tx="ekr.20221004064034.635">def transform_callee_type(
    self,
    callable_name: str | None,
    callee: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None = None,
    object_type: Type | None = None,
) -&gt; Type:
    """Attempt to determine a more accurate signature for a method call.

    This is done by looking up and applying a method signature hook (if one exists for the
    given method name).

    If no matching method signature hook is found, callee is returned unmodified. The same
    happens if the arguments refer to a non-method callable (this is allowed so that the code
    calling transform_callee_type needs to perform fewer boilerplate checks).

    Note: this method is *not* called automatically as part of check_call, because in some
    cases check_call is called multiple times while checking a single call (for example when
    dealing with overloads). Instead, this method needs to be called explicitly
    (if appropriate) before the signature is passed to check_call.
    """
    callee = get_proper_type(callee)
    if callable_name is not None and isinstance(callee, FunctionLike):
        if object_type is not None:
            method_sig_hook = self.plugin.get_method_signature_hook(callable_name)
            if method_sig_hook:
                return self.apply_method_signature_hook(
                    callee, args, arg_kinds, context, arg_names, object_type, method_sig_hook
                )
        else:
            function_sig_hook = self.plugin.get_function_signature_hook(callable_name)
            if function_sig_hook:
                return self.apply_function_signature_hook(
                    callee, args, arg_kinds, context, arg_names, function_sig_hook
                )

    return callee

</t>
<t tx="ekr.20221004064034.636">def check_call_expr_with_callee_type(
    self,
    callee_type: Type,
    e: CallExpr,
    callable_name: str | None,
    object_type: Type | None,
    member: str | None = None,
) -&gt; Type:
    """Type check call expression.

    The callee_type should be used as the type of callee expression. In particular,
    in case of a union type this can be a particular item of the union, so that we can
    apply plugin hooks to each item.

    The 'member', 'callable_name' and 'object_type' are only used to call plugin hooks.
    If 'callable_name' is None but 'member' is not None (member call), try constructing
    'callable_name' using 'object_type' (the base type on which the method is called),
    for example 'typing.Mapping.get'.
    """
    if callable_name is None and member is not None:
        assert object_type is not None
        callable_name = self.method_fullname(object_type, member)
    object_type = get_proper_type(object_type)
    if callable_name:
        # Try to refine the call signature using plugin hooks before checking the call.
        callee_type = self.transform_callee_type(
            callable_name, callee_type, e.args, e.arg_kinds, e, e.arg_names, object_type
        )
    # Unions are special-cased to allow plugins to act on each item in the union.
    elif member is not None and isinstance(object_type, UnionType):
        return self.check_union_call_expr(e, object_type, member)
    ret_type, callee_type = self.check_call(
        callee_type,
        e.args,
        e.arg_kinds,
        e,
        e.arg_names,
        callable_node=e.callee,
        callable_name=callable_name,
        object_type=object_type,
    )
    proper_callee = get_proper_type(callee_type)
    if (
        isinstance(e.callee, RefExpr)
        and isinstance(proper_callee, CallableType)
        and proper_callee.type_guard is not None
    ):
        # Cache it for find_isinstance_check()
        e.callee.type_guard = proper_callee.type_guard
    return ret_type

</t>
<t tx="ekr.20221004064034.637">def check_union_call_expr(self, e: CallExpr, object_type: UnionType, member: str) -&gt; Type:
    """Type check calling a member expression where the base type is a union."""
    res: list[Type] = []
    for typ in object_type.relevant_items():
        # Member access errors are already reported when visiting the member expression.
        with self.msg.filter_errors():
            item = analyze_member_access(
                member,
                typ,
                e,
                False,
                False,
                False,
                self.msg,
                original_type=object_type,
                chk=self.chk,
                in_literal_context=self.is_literal_context(),
                self_type=typ,
            )
        narrowed = self.narrow_type_from_binder(e.callee, item, skip_non_overlapping=True)
        if narrowed is None:
            continue
        callable_name = self.method_fullname(typ, member)
        item_object_type = typ if callable_name else None
        res.append(
            self.check_call_expr_with_callee_type(narrowed, e, callable_name, item_object_type)
        )
    return make_simplified_union(res)

</t>
<t tx="ekr.20221004064034.638">def check_call(
    self,
    callee: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None = None,
    callable_node: Expression | None = None,
    callable_name: str | None = None,
    object_type: Type | None = None,
) -&gt; tuple[Type, Type]:
    """Type check a call.

    Also infer type arguments if the callee is a generic function.

    Return (result type, inferred callee type).

    Arguments:
        callee: type of the called value
        args: actual argument expressions
        arg_kinds: contains nodes.ARG_* constant for each argument in args
             describing whether the argument is positional, *arg, etc.
        context: current expression context, used for inference.
        arg_names: names of arguments (optional)
        callable_node: associate the inferred callable type to this node,
            if specified
        callable_name: Fully-qualified name of the function/method to call,
            or None if unavailable (examples: 'builtins.open', 'typing.Mapping.get')
        object_type: If callable_name refers to a method, the type of the object
            on which the method is being called
    """
    callee = get_proper_type(callee)

    if isinstance(callee, CallableType):
        return self.check_callable_call(
            callee,
            args,
            arg_kinds,
            context,
            arg_names,
            callable_node,
            callable_name,
            object_type,
        )
    elif isinstance(callee, Overloaded):
        return self.check_overload_call(
            callee, args, arg_kinds, arg_names, callable_name, object_type, context
        )
    elif isinstance(callee, AnyType) or not self.chk.in_checked_function():
        return self.check_any_type_call(args, callee)
    elif isinstance(callee, UnionType):
        return self.check_union_call(callee, args, arg_kinds, arg_names, context)
    elif isinstance(callee, Instance):
        call_function = analyze_member_access(
            "__call__",
            callee,
            context,
            is_lvalue=False,
            is_super=False,
            is_operator=True,
            msg=self.msg,
            original_type=callee,
            chk=self.chk,
            in_literal_context=self.is_literal_context(),
        )
        callable_name = callee.type.fullname + ".__call__"
        # Apply method signature hook, if one exists
        call_function = self.transform_callee_type(
            callable_name, call_function, args, arg_kinds, context, arg_names, callee
        )
        result = self.check_call(
            call_function,
            args,
            arg_kinds,
            context,
            arg_names,
            callable_node,
            callable_name,
            callee,
        )
        if callable_node:
            # check_call() stored "call_function" as the type, which is incorrect.
            # Override the type.
            self.chk.store_type(callable_node, callee)
        return result
    elif isinstance(callee, TypeVarType):
        return self.check_call(
            callee.upper_bound, args, arg_kinds, context, arg_names, callable_node
        )
    elif isinstance(callee, TypeType):
        item = self.analyze_type_type_callee(callee.item, context)
        return self.check_call(item, args, arg_kinds, context, arg_names, callable_node)
    elif isinstance(callee, TupleType):
        return self.check_call(
            tuple_fallback(callee),
            args,
            arg_kinds,
            context,
            arg_names,
            callable_node,
            callable_name,
            object_type,
        )
    else:
        return self.msg.not_callable(callee, context), AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064034.639">def check_callable_call(
    self,
    callee: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    arg_names: Sequence[str | None] | None,
    callable_node: Expression | None,
    callable_name: str | None,
    object_type: Type | None,
) -&gt; tuple[Type, Type]:
    """Type check a call that targets a callable value.

    See the docstring of check_call for more information.
    """
    # Always unpack **kwargs before checking a call.
    callee = callee.with_unpacked_kwargs()
    if callable_name is None and callee.name:
        callable_name = callee.name
    ret_type = get_proper_type(callee.ret_type)
    if callee.is_type_obj() and isinstance(ret_type, Instance):
        callable_name = ret_type.type.fullname
    if isinstance(callable_node, RefExpr) and callable_node.fullname in ENUM_BASES:
        # An Enum() call that failed SemanticAnalyzerPass2.check_enum_call().
        return callee.ret_type, callee

    if (
        callee.is_type_obj()
        and callee.type_object().is_protocol
        # Exception for Type[...]
        and not callee.from_type_type
    ):
        self.chk.fail(
            message_registry.CANNOT_INSTANTIATE_PROTOCOL.format(callee.type_object().name),
            context,
        )
    elif (
        callee.is_type_obj()
        and callee.type_object().is_abstract
        # Exception for Type[...]
        and not callee.from_type_type
        and not callee.type_object().fallback_to_any
    ):
        type = callee.type_object()
        # Determine whether the implicitly abstract attributes are functions with
        # None-compatible return types.
        abstract_attributes: dict[str, bool] = {}
        for attr_name, abstract_status in type.abstract_attributes:
            if abstract_status == IMPLICITLY_ABSTRACT:
                abstract_attributes[attr_name] = self.can_return_none(type, attr_name)
            else:
                abstract_attributes[attr_name] = False
        self.msg.cannot_instantiate_abstract_class(
            callee.type_object().name, abstract_attributes, context
        )

    formal_to_actual = map_actuals_to_formals(
        arg_kinds,
        arg_names,
        callee.arg_kinds,
        callee.arg_names,
        lambda i: self.accept(args[i]),
    )

    if callee.is_generic():
        need_refresh = any(isinstance(v, ParamSpecType) for v in callee.variables)
        callee = freshen_function_type_vars(callee)
        callee = self.infer_function_type_arguments_using_context(callee, context)
        callee = self.infer_function_type_arguments(
            callee, args, arg_kinds, formal_to_actual, context
        )
        if need_refresh:
            # Argument kinds etc. may have changed due to
            # ParamSpec variables being replaced with an arbitrary
            # number of arguments; recalculate actual-to-formal map
            formal_to_actual = map_actuals_to_formals(
                arg_kinds,
                arg_names,
                callee.arg_kinds,
                callee.arg_names,
                lambda i: self.accept(args[i]),
            )

    param_spec = callee.param_spec()
    if param_spec is not None and arg_kinds == [ARG_STAR, ARG_STAR2]:
        arg1 = self.accept(args[0])
        arg2 = self.accept(args[1])
        if (
            isinstance(arg1, ParamSpecType)
            and isinstance(arg2, ParamSpecType)
            and arg1.flavor == ParamSpecFlavor.ARGS
            and arg2.flavor == ParamSpecFlavor.KWARGS
            and arg1.id == arg2.id == param_spec.id
        ):
            return callee.ret_type, callee

    arg_types = self.infer_arg_types_in_context(callee, args, arg_kinds, formal_to_actual)

    self.check_argument_count(
        callee,
        arg_types,
        arg_kinds,
        arg_names,
        formal_to_actual,
        context,
        object_type,
        callable_name,
    )

    self.check_argument_types(
        arg_types, arg_kinds, args, callee, formal_to_actual, context, object_type=object_type
    )

    if (
        callee.is_type_obj()
        and (len(arg_types) == 1)
        and is_equivalent(callee.ret_type, self.named_type("builtins.type"))
    ):
        callee = callee.copy_modified(ret_type=TypeType.make_normalized(arg_types[0]))

    if callable_node:
        # Store the inferred callable type.
        self.chk.store_type(callable_node, callee)

    if callable_name and (
        (object_type is None and self.plugin.get_function_hook(callable_name))
        or (object_type is not None and self.plugin.get_method_hook(callable_name))
    ):
        new_ret_type = self.apply_function_plugin(
            callee,
            arg_kinds,
            arg_types,
            arg_names,
            formal_to_actual,
            args,
            callable_name,
            object_type,
            context,
        )
        callee = callee.copy_modified(ret_type=new_ret_type)
    return callee.ret_type, callee

</t>
<t tx="ekr.20221004064034.64">@path C:/Repos/ekr-mypy2/misc/
"""
The main GitHub workflow where wheels are built:
https://github.com/mypyc/mypy_mypyc-wheels/blob/master/.github/workflows/build.yml

The script that builds wheels:
https://github.com/mypyc/mypy_mypyc-wheels/blob/master/build_wheel.py

That script is a light wrapper around cibuildwheel. Now that cibuildwheel has native configuration
and better support for local builds, we could probably replace the script.
"""

raise ImportError("This script has been moved back to https://github.com/mypyc/mypy_mypyc-wheels")
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.640">def can_return_none(self, type: TypeInfo, attr_name: str) -&gt; bool:
    """Is the given attribute a method with a None-compatible return type?

    Overloads are only checked if there is an implementation.
    """
    if not state.strict_optional:
        # If strict-optional is not set, is_subtype(NoneType(), T) is always True.
        # So, we cannot do anything useful here in that case.
        return False
    for base in type.mro:
        symnode = base.names.get(attr_name)
        if symnode is None:
            continue
        node = symnode.node
        if isinstance(node, OverloadedFuncDef):
            node = node.impl
        if isinstance(node, Decorator):
            node = node.func
        if isinstance(node, FuncDef):
            if node.type is not None:
                assert isinstance(node.type, CallableType)
                return is_subtype(NoneType(), node.type.ret_type)
    return False

</t>
<t tx="ekr.20221004064034.641">def analyze_type_type_callee(self, item: ProperType, context: Context) -&gt; Type:
    """Analyze the callee X in X(...) where X is Type[item].

    Return a Y that we can pass to check_call(Y, ...).
    """
    if isinstance(item, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=item)
    if isinstance(item, Instance):
        res = type_object_type(item.type, self.named_type)
        if isinstance(res, CallableType):
            res = res.copy_modified(from_type_type=True)
        expanded = expand_type_by_instance(res, item)
        if isinstance(expanded, CallableType):
            # Callee of the form Type[...] should never be generic, only
            # proper class objects can be.
            expanded = expanded.copy_modified(variables=[])
        return expanded
    if isinstance(item, UnionType):
        return UnionType(
            [
                self.analyze_type_type_callee(get_proper_type(tp), context)
                for tp in item.relevant_items()
            ],
            item.line,
        )
    if isinstance(item, TypeVarType):
        # Pretend we're calling the typevar's upper bound,
        # i.e. its constructor (a poor approximation for reality,
        # but better than AnyType...), but replace the return type
        # with typevar.
        callee = self.analyze_type_type_callee(get_proper_type(item.upper_bound), context)
        callee = get_proper_type(callee)
        if isinstance(callee, CallableType):
            callee = callee.copy_modified(ret_type=item)
        elif isinstance(callee, Overloaded):
            callee = Overloaded([c.copy_modified(ret_type=item) for c in callee.items])
        return callee
    # We support Type of namedtuples but not of tuples in general
    if isinstance(item, TupleType) and tuple_fallback(item).type.fullname != "builtins.tuple":
        return self.analyze_type_type_callee(tuple_fallback(item), context)

    self.msg.unsupported_type_type(item, context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064034.642">def infer_arg_types_in_empty_context(self, args: list[Expression]) -&gt; list[Type]:
    """Infer argument expression types in an empty context.

    In short, we basically recurse on each argument without considering
    in what context the argument was called.
    """
    res: list[Type] = []

    for arg in args:
        arg_type = self.accept(arg)
        if has_erased_component(arg_type):
            res.append(NoneType())
        else:
            res.append(arg_type)
    return res

</t>
<t tx="ekr.20221004064034.643">@contextmanager
def allow_unions(self, type_context: Type) -&gt; Iterator[None]:
    # This is a hack to better support inference for recursive types.
    # When the outer context for a function call is known to be recursive,
    # we solve type constraints inferred from arguments using unions instead
    # of joins. This is a bit arbitrary, but in practice it works for most
    # cases. A cleaner alternative would be to switch to single bin type
    # inference, but this is a lot of work.
    old = TypeState.infer_unions
    if has_recursive_types(type_context):
        TypeState.infer_unions = True
    try:
        yield
    finally:
        TypeState.infer_unions = old

</t>
<t tx="ekr.20221004064034.644">def infer_arg_types_in_context(
    self,
    callee: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
) -&gt; list[Type]:
    """Infer argument expression types using a callable type as context.

    For example, if callee argument 2 has type List[int], infer the
    argument expression with List[int] type context.

    Returns the inferred types of *actual arguments*.
    """
    res: list[Type | None] = [None] * len(args)

    for i, actuals in enumerate(formal_to_actual):
        for ai in actuals:
            if not arg_kinds[ai].is_star():
                with self.allow_unions(callee.arg_types[i]):
                    res[ai] = self.accept(args[ai], callee.arg_types[i])

    # Fill in the rest of the argument types.
    for i, t in enumerate(res):
        if not t:
            res[i] = self.accept(args[i])
    assert all(tp is not None for tp in res)
    return cast(List[Type], res)

</t>
<t tx="ekr.20221004064034.645">def infer_function_type_arguments_using_context(
    self, callable: CallableType, error_context: Context
) -&gt; CallableType:
    """Unify callable return type to type context to infer type vars.

    For example, if the return type is set[t] where 't' is a type variable
    of callable, and if the context is set[int], return callable modified
    by substituting 't' with 'int'.
    """
    ctx = self.type_context[-1]
    if not ctx:
        return callable
    # The return type may have references to type metavariables that
    # we are inferring right now. We must consider them as indeterminate
    # and they are not potential results; thus we replace them with the
    # special ErasedType type. On the other hand, class type variables are
    # valid results.
    erased_ctx = replace_meta_vars(ctx, ErasedType())
    ret_type = callable.ret_type
    if is_optional(ret_type) and is_optional(ctx):
        # If both the context and the return type are optional, unwrap the optional,
        # since in 99% cases this is what a user expects. In other words, we replace
        #     Optional[T] &lt;: Optional[int]
        # with
        #     T &lt;: int
        # while the former would infer T &lt;: Optional[int].
        ret_type = remove_optional(ret_type)
        erased_ctx = remove_optional(erased_ctx)
        #
        # TODO: Instead of this hack and the one below, we need to use outer and
        # inner contexts at the same time. This is however not easy because of two
        # reasons:
        #   * We need to support constraints like [1 &lt;: 2, 2 &lt;: X], i.e. with variables
        #     on both sides. (This is not too hard.)
        #   * We need to update all the inference "infrastructure", so that all
        #     variables in an expression are inferred at the same time.
        #     (And this is hard, also we need to be careful with lambdas that require
        #     two passes.)
    if isinstance(ret_type, TypeVarType):
        # Another special case: the return type is a type variable. If it's unrestricted,
        # we could infer a too general type for the type variable if we use context,
        # and this could result in confusing and spurious type errors elsewhere.
        #
        # So we give up and just use function arguments for type inference, with just two
        # exceptions:
        #
        # 1. If the context is a generic instance type, actually use it as context, as
        #    this *seems* to usually be the reasonable thing to do.
        #
        #    See also github issues #462 and #360.
        #
        # 2. If the context is some literal type, we want to "propagate" that information
        #    down so that we infer a more precise type for literal expressions. For example,
        #    the expression `3` normally has an inferred type of `builtins.int`: but if it's
        #    in a literal context like below, we want it to infer `Literal[3]` instead.
        #
        #        def expects_literal(x: Literal[3]) -&gt; None: pass
        #        def identity(x: T) -&gt; T: return x
        #
        #        expects_literal(identity(3))  # Should type-check
        if not is_generic_instance(ctx) and not is_literal_type_like(ctx):
            return callable.copy_modified()
    args = infer_type_arguments(callable.type_var_ids(), ret_type, erased_ctx)
    # Only substitute non-Uninhabited and non-erased types.
    new_args: list[Type | None] = []
    for arg in args:
        if has_uninhabited_component(arg) or has_erased_component(arg):
            new_args.append(None)
        else:
            new_args.append(arg)
    # Don't show errors after we have only used the outer context for inference.
    # We will use argument context to infer more variables.
    return self.apply_generic_arguments(
        callable, new_args, error_context, skip_unsatisfied=True
    )

</t>
<t tx="ekr.20221004064034.646">def infer_function_type_arguments(
    self,
    callee_type: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
    context: Context,
) -&gt; CallableType:
    """Infer the type arguments for a generic callee type.

    Infer based on the types of arguments.

    Return a derived callable type that has the arguments applied.
    """
    if self.chk.in_checked_function():
        # Disable type errors during type inference. There may be errors
        # due to partial available context information at this time, but
        # these errors can be safely ignored as the arguments will be
        # inferred again later.
        with self.msg.filter_errors():
            arg_types = self.infer_arg_types_in_context(
                callee_type, args, arg_kinds, formal_to_actual
            )

        arg_pass_nums = self.get_arg_infer_passes(
            callee_type.arg_types, formal_to_actual, len(args)
        )

        pass1_args: list[Type | None] = []
        for i, arg in enumerate(arg_types):
            if arg_pass_nums[i] &gt; 1:
                pass1_args.append(None)
            else:
                pass1_args.append(arg)

        inferred_args = infer_function_type_arguments(
            callee_type,
            pass1_args,
            arg_kinds,
            formal_to_actual,
            context=self.argument_infer_context(),
            strict=self.chk.in_checked_function(),
        )

        if 2 in arg_pass_nums:
            # Second pass of type inference.
            (callee_type, inferred_args) = self.infer_function_type_arguments_pass2(
                callee_type, args, arg_kinds, formal_to_actual, inferred_args, context
            )

        if (
            callee_type.special_sig == "dict"
            and len(inferred_args) == 2
            and (ARG_NAMED in arg_kinds or ARG_STAR2 in arg_kinds)
        ):
            # HACK: Infer str key type for dict(...) with keyword args. The type system
            #       can't represent this so we special case it, as this is a pretty common
            #       thing. This doesn't quite work with all possible subclasses of dict
            #       if they shuffle type variables around, as we assume that there is a 1-1
            #       correspondence with dict type variables. This is a marginal issue and
            #       a little tricky to fix so it's left unfixed for now.
            first_arg = get_proper_type(inferred_args[0])
            if isinstance(first_arg, (NoneType, UninhabitedType)):
                inferred_args[0] = self.named_type("builtins.str")
            elif not first_arg or not is_subtype(self.named_type("builtins.str"), first_arg):
                self.chk.fail(message_registry.KEYWORD_ARGUMENT_REQUIRES_STR_KEY_TYPE, context)
    else:
        # In dynamically typed functions use implicit 'Any' types for
        # type variables.
        inferred_args = [AnyType(TypeOfAny.unannotated)] * len(callee_type.variables)
    return self.apply_inferred_arguments(callee_type, inferred_args, context)

</t>
<t tx="ekr.20221004064034.647">def infer_function_type_arguments_pass2(
    self,
    callee_type: CallableType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
    old_inferred_args: Sequence[Type | None],
    context: Context,
) -&gt; tuple[CallableType, list[Type | None]]:
    """Perform second pass of generic function type argument inference.

    The second pass is needed for arguments with types such as Callable[[T], S],
    where both T and S are type variables, when the actual argument is a
    lambda with inferred types.  The idea is to infer the type variable T
    in the first pass (based on the types of other arguments).  This lets
    us infer the argument and return type of the lambda expression and
    thus also the type variable S in this second pass.

    Return (the callee with type vars applied, inferred actual arg types).
    """
    # None or erased types in inferred types mean that there was not enough
    # information to infer the argument. Replace them with None values so
    # that they are not applied yet below.
    inferred_args = list(old_inferred_args)
    for i, arg in enumerate(get_proper_types(inferred_args)):
        if isinstance(arg, (NoneType, UninhabitedType)) or has_erased_component(arg):
            inferred_args[i] = None
    callee_type = self.apply_generic_arguments(callee_type, inferred_args, context)

    arg_types = self.infer_arg_types_in_context(callee_type, args, arg_kinds, formal_to_actual)

    inferred_args = infer_function_type_arguments(
        callee_type,
        arg_types,
        arg_kinds,
        formal_to_actual,
        context=self.argument_infer_context(),
    )

    return callee_type, inferred_args

</t>
<t tx="ekr.20221004064034.648">def argument_infer_context(self) -&gt; ArgumentInferContext:
    return ArgumentInferContext(
        self.chk.named_type("typing.Mapping"), self.chk.named_type("typing.Iterable")
    )

</t>
<t tx="ekr.20221004064034.649">def get_arg_infer_passes(
    self, arg_types: list[Type], formal_to_actual: list[list[int]], num_actuals: int
) -&gt; list[int]:
    """Return pass numbers for args for two-pass argument type inference.

    For each actual, the pass number is either 1 (first pass) or 2 (second
    pass).

    Two-pass argument type inference primarily lets us infer types of
    lambdas more effectively.
    """
    res = [1] * num_actuals
    for i, arg in enumerate(arg_types):
        if arg.accept(ArgInferSecondPassQuery()):
            for j in formal_to_actual[i]:
                res[j] = 2
    return res

</t>
<t tx="ekr.20221004064034.65">@path C:/Repos/ekr-mypy2/misc/
"""Cherry-pick a commit from typeshed.

Usage:

  python3 misc/cherry-pick-typeshed.py --typeshed-dir dir hash
"""

from __future__ import annotations

import argparse
import os.path
import re
import subprocess
import sys
import tempfile


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.650">def apply_inferred_arguments(
    self, callee_type: CallableType, inferred_args: Sequence[Type | None], context: Context
) -&gt; CallableType:
    """Apply inferred values of type arguments to a generic function.

    Inferred_args contains the values of function type arguments.
    """
    # Report error if some of the variables could not be solved. In that
    # case assume that all variables have type Any to avoid extra
    # bogus error messages.
    for i, inferred_type in enumerate(inferred_args):
        if not inferred_type or has_erased_component(inferred_type):
            # Could not infer a non-trivial type for a type variable.
            self.msg.could_not_infer_type_arguments(callee_type, i + 1, context)
            inferred_args = [AnyType(TypeOfAny.from_error)] * len(inferred_args)
    # Apply the inferred types to the function type. In this case the
    # return type must be CallableType, since we give the right number of type
    # arguments.
    return self.apply_generic_arguments(callee_type, inferred_args, context)

</t>
<t tx="ekr.20221004064034.651">def check_argument_count(
    self,
    callee: CallableType,
    actual_types: list[Type],
    actual_kinds: list[ArgKind],
    actual_names: Sequence[str | None] | None,
    formal_to_actual: list[list[int]],
    context: Context | None,
    object_type: Type | None = None,
    callable_name: str | None = None,
) -&gt; bool:
    """Check that there is a value for all required arguments to a function.

    Also check that there are no duplicate values for arguments. Report found errors
    using 'messages' if it's not None. If 'messages' is given, 'context' must also be given.

    Return False if there were any errors. Otherwise return True
    """
    if context is None:
        # Avoid "is None" checks
        context = TempNode(AnyType(TypeOfAny.special_form))

    # TODO(jukka): We could return as soon as we find an error if messages is None.

    # Collect dict of all actual arguments matched to formal arguments, with occurrence count
    all_actuals: dict[int, int] = {}
    for actuals in formal_to_actual:
        for a in actuals:
            all_actuals[a] = all_actuals.get(a, 0) + 1

    ok, is_unexpected_arg_error = self.check_for_extra_actual_arguments(
        callee, actual_types, actual_kinds, actual_names, all_actuals, context
    )

    # Check for too many or few values for formals.
    for i, kind in enumerate(callee.arg_kinds):
        if kind.is_required() and not formal_to_actual[i] and not is_unexpected_arg_error:
            # No actual for a mandatory formal
            if kind.is_positional():
                self.msg.too_few_arguments(callee, context, actual_names)
                if object_type and callable_name and "." in callable_name:
                    self.missing_classvar_callable_note(object_type, callable_name, context)
            else:
                argname = callee.arg_names[i] or "?"
                self.msg.missing_named_argument(callee, context, argname)
            ok = False
        elif not kind.is_star() and is_duplicate_mapping(
            formal_to_actual[i], actual_types, actual_kinds
        ):
            if self.chk.in_checked_function() or isinstance(
                get_proper_type(actual_types[formal_to_actual[i][0]]), TupleType
            ):
                self.msg.duplicate_argument_value(callee, i, context)
                ok = False
        elif (
            kind.is_named()
            and formal_to_actual[i]
            and actual_kinds[formal_to_actual[i][0]] not in [nodes.ARG_NAMED, nodes.ARG_STAR2]
        ):
            # Positional argument when expecting a keyword argument.
            self.msg.too_many_positional_arguments(callee, context)
            ok = False
    return ok

</t>
<t tx="ekr.20221004064034.652">def check_for_extra_actual_arguments(
    self,
    callee: CallableType,
    actual_types: list[Type],
    actual_kinds: list[ArgKind],
    actual_names: Sequence[str | None] | None,
    all_actuals: dict[int, int],
    context: Context,
) -&gt; tuple[bool, bool]:
    """Check for extra actual arguments.

    Return tuple (was everything ok,
                  was there an extra keyword argument error [used to avoid duplicate errors]).
    """

    is_unexpected_arg_error = False  # Keep track of errors to avoid duplicate errors
    ok = True  # False if we've found any error

    for i, kind in enumerate(actual_kinds):
        if (
            i not in all_actuals
            and
            # We accept the other iterables than tuple (including Any)
            # as star arguments because they could be empty, resulting no arguments.
            (kind != nodes.ARG_STAR or is_non_empty_tuple(actual_types[i]))
            and
            # Accept all types for double-starred arguments, because they could be empty
            # dictionaries and we can't tell it from their types
            kind != nodes.ARG_STAR2
        ):
            # Extra actual: not matched by a formal argument.
            ok = False
            if kind != nodes.ARG_NAMED:
                self.msg.too_many_arguments(callee, context)
            else:
                assert actual_names, "Internal error: named kinds without names given"
                act_name = actual_names[i]
                assert act_name is not None
                act_type = actual_types[i]
                self.msg.unexpected_keyword_argument(callee, act_name, act_type, context)
                is_unexpected_arg_error = True
        elif (
            kind == nodes.ARG_STAR and nodes.ARG_STAR not in callee.arg_kinds
        ) or kind == nodes.ARG_STAR2:
            actual_type = get_proper_type(actual_types[i])
            if isinstance(actual_type, (TupleType, TypedDictType)):
                if all_actuals.get(i, 0) &lt; len(actual_type.items):
                    # Too many tuple/dict items as some did not match.
                    if kind != nodes.ARG_STAR2 or not isinstance(actual_type, TypedDictType):
                        self.msg.too_many_arguments(callee, context)
                    else:
                        self.msg.too_many_arguments_from_typed_dict(
                            callee, actual_type, context
                        )
                        is_unexpected_arg_error = True
                    ok = False
            # *args/**kwargs can be applied even if the function takes a fixed
            # number of positional arguments. This may succeed at runtime.

    return ok, is_unexpected_arg_error

</t>
<t tx="ekr.20221004064034.653">def missing_classvar_callable_note(
    self, object_type: Type, callable_name: str, context: Context
) -&gt; None:
    if isinstance(object_type, ProperType) and isinstance(object_type, Instance):
        _, var_name = callable_name.rsplit(".", maxsplit=1)
        node = object_type.type.get(var_name)
        if node is not None and isinstance(node.node, Var):
            if not node.node.is_inferred and not node.node.is_classvar:
                self.msg.note(
                    f'"{var_name}" is considered instance variable,'
                    " to make it class variable use ClassVar[...]",
                    context,
                )

</t>
<t tx="ekr.20221004064034.654">def check_argument_types(
    self,
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    args: list[Expression],
    callee: CallableType,
    formal_to_actual: list[list[int]],
    context: Context,
    check_arg: ArgChecker | None = None,
    object_type: Type | None = None,
) -&gt; None:
    """Check argument types against a callable type.

    Report errors if the argument types are not compatible.

    The check_call docstring describes some of the arguments.
    """
    check_arg = check_arg or self.check_arg
    # Keep track of consumed tuple *arg items.
    mapper = ArgTypeExpander(self.argument_infer_context())
    for i, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            actual_type = arg_types[actual]
            if actual_type is None:
                continue  # Some kind of error was already reported.
            actual_kind = arg_kinds[actual]
            # Check that a *arg is valid as varargs.
            if actual_kind == nodes.ARG_STAR and not self.is_valid_var_arg(actual_type):
                self.msg.invalid_var_arg(actual_type, context)
            if actual_kind == nodes.ARG_STAR2 and not self.is_valid_keyword_var_arg(
                actual_type
            ):
                is_mapping = is_subtype(actual_type, self.chk.named_type("typing.Mapping"))
                self.msg.invalid_keyword_var_arg(actual_type, is_mapping, context)
            expanded_actual = mapper.expand_actual_type(
                actual_type, actual_kind, callee.arg_names[i], callee.arg_kinds[i]
            )
            check_arg(
                expanded_actual,
                actual_type,
                arg_kinds[actual],
                callee.arg_types[i],
                actual + 1,
                i + 1,
                callee,
                object_type,
                args[actual],
                context,
            )

</t>
<t tx="ekr.20221004064034.655">def check_arg(
    self,
    caller_type: Type,
    original_caller_type: Type,
    caller_kind: ArgKind,
    callee_type: Type,
    n: int,
    m: int,
    callee: CallableType,
    object_type: Type | None,
    context: Context,
    outer_context: Context,
) -&gt; None:
    """Check the type of a single argument in a call."""
    caller_type = get_proper_type(caller_type)
    original_caller_type = get_proper_type(original_caller_type)
    callee_type = get_proper_type(callee_type)

    if isinstance(caller_type, DeletedType):
        self.msg.deleted_as_rvalue(caller_type, context)
    # Only non-abstract non-protocol class can be given where Type[...] is expected...
    elif (
        isinstance(caller_type, CallableType)
        and isinstance(callee_type, TypeType)
        and caller_type.is_type_obj()
        and (caller_type.type_object().is_abstract or caller_type.type_object().is_protocol)
        and isinstance(callee_type.item, Instance)
        and (callee_type.item.type.is_abstract or callee_type.item.type.is_protocol)
        and not self.chk.allow_abstract_call
    ):
        self.msg.concrete_only_call(callee_type, context)
    elif not is_subtype(caller_type, callee_type, options=self.chk.options):
        code = self.msg.incompatible_argument(
            n,
            m,
            callee,
            original_caller_type,
            caller_kind,
            object_type=object_type,
            context=context,
            outer_context=outer_context,
        )
        self.msg.incompatible_argument_note(
            original_caller_type, callee_type, context, code=code
        )
        self.chk.check_possible_missing_await(caller_type, callee_type, context)

</t>
<t tx="ekr.20221004064034.656">def check_overload_call(
    self,
    callee: Overloaded,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    callable_name: str | None,
    object_type: Type | None,
    context: Context,
) -&gt; tuple[Type, Type]:
    """Checks a call to an overloaded function."""
    # Normalize unpacked kwargs before checking the call.
    callee = callee.with_unpacked_kwargs()
    arg_types = self.infer_arg_types_in_empty_context(args)
    # Step 1: Filter call targets to remove ones where the argument counts don't match
    plausible_targets = self.plausible_overload_call_targets(
        arg_types, arg_kinds, arg_names, callee
    )

    # Step 2: If the arguments contain a union, we try performing union math first,
    #         instead of picking the first matching overload.
    #         This is because picking the first overload often ends up being too greedy:
    #         for example, when we have a fallback alternative that accepts an unrestricted
    #         typevar. See https://github.com/python/mypy/issues/4063 for related discussion.
    erased_targets: list[CallableType] | None = None
    unioned_result: tuple[Type, Type] | None = None
    union_interrupted = False  # did we try all union combinations?
    if any(self.real_union(arg) for arg in arg_types):
        try:
            with self.msg.filter_errors():
                unioned_return = self.union_overload_result(
                    plausible_targets,
                    args,
                    arg_types,
                    arg_kinds,
                    arg_names,
                    callable_name,
                    object_type,
                    context,
                )
        except TooManyUnions:
            union_interrupted = True
        else:
            # Record if we succeeded. Next we need to see if maybe normal procedure
            # gives a narrower type.
            if unioned_return:
                returns, inferred_types = zip(*unioned_return)
                # Note that we use `combine_function_signatures` instead of just returning
                # a union of inferred callables because for example a call
                # Union[int -&gt; int, str -&gt; str](Union[int, str]) is invalid and
                # we don't want to introduce internal inconsistencies.
                unioned_result = (
                    make_simplified_union(list(returns), context.line, context.column),
                    self.combine_function_signatures(inferred_types),
                )

    # Step 3: We try checking each branch one-by-one.
    inferred_result = self.infer_overload_return_type(
        plausible_targets,
        args,
        arg_types,
        arg_kinds,
        arg_names,
        callable_name,
        object_type,
        context,
    )
    # If any of checks succeed, stop early.
    if inferred_result is not None and unioned_result is not None:
        # Both unioned and direct checks succeeded, choose the more precise type.
        if is_subtype(inferred_result[0], unioned_result[0]) and not isinstance(
            get_proper_type(inferred_result[0]), AnyType
        ):
            return inferred_result
        return unioned_result
    elif unioned_result is not None:
        return unioned_result
    elif inferred_result is not None:
        return inferred_result

    # Step 4: Failure. At this point, we know there is no match. We fall back to trying
    #         to find a somewhat plausible overload target using the erased types
    #         so we can produce a nice error message.
    #
    #         For example, suppose the user passes a value of type 'List[str]' into an
    #         overload with signatures f(x: int) -&gt; int and f(x: List[int]) -&gt; List[int].
    #
    #         Neither alternative matches, but we can guess the user probably wants the
    #         second one.
    erased_targets = self.overload_erased_call_targets(
        plausible_targets, arg_types, arg_kinds, arg_names, args, context
    )

    # Step 5: We try and infer a second-best alternative if possible. If not, fall back
    #         to using 'Any'.
    if len(erased_targets) &gt; 0:
        # Pick the first plausible erased target as the fallback
        # TODO: Adjust the error message here to make it clear there was no match.
        #       In order to do this, we need to find a clean way of associating
        #       a note with whatever error message 'self.check_call' will generate.
        #       In particular, the note's line and column numbers need to be the same
        #       as the error's.
        target: Type = erased_targets[0]
    else:
        # There was no plausible match: give up
        target = AnyType(TypeOfAny.from_error)
        if not is_operator_method(callable_name):
            code = None
        else:
            code = codes.OPERATOR
        self.msg.no_variant_matches_arguments(callee, arg_types, context, code=code)

    result = self.check_call(
        target,
        args,
        arg_kinds,
        context,
        arg_names,
        callable_name=callable_name,
        object_type=object_type,
    )
    if union_interrupted:
        self.chk.fail(message_registry.TOO_MANY_UNION_COMBINATIONS, context)
    return result

</t>
<t tx="ekr.20221004064034.657">def plausible_overload_call_targets(
    self,
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    overload: Overloaded,
) -&gt; list[CallableType]:
    """Returns all overload call targets that having matching argument counts.

    If the given args contains a star-arg (*arg or **kwarg argument), this method
    will ensure all star-arg overloads appear at the start of the list, instead
    of their usual location.

    The only exception is if the starred argument is something like a Tuple or a
    NamedTuple, which has a definitive "shape". If so, we don't move the corresponding
    alternative to the front since we can infer a more precise match using the original
    order."""

    def has_shape(typ: Type) -&gt; bool:
        typ = get_proper_type(typ)
        return (
            isinstance(typ, TupleType)
            or isinstance(typ, TypedDictType)
            or (isinstance(typ, Instance) and typ.type.is_named_tuple)
        )

    matches: list[CallableType] = []
    star_matches: list[CallableType] = []

    args_have_var_arg = False
    args_have_kw_arg = False
    for kind, typ in zip(arg_kinds, arg_types):
        if kind == ARG_STAR and not has_shape(typ):
            args_have_var_arg = True
        if kind == ARG_STAR2 and not has_shape(typ):
            args_have_kw_arg = True

    for typ in overload.items:
        formal_to_actual = map_actuals_to_formals(
            arg_kinds, arg_names, typ.arg_kinds, typ.arg_names, lambda i: arg_types[i]
        )

        with self.msg.filter_errors():
            if self.check_argument_count(
                typ, arg_types, arg_kinds, arg_names, formal_to_actual, None
            ):
                if args_have_var_arg and typ.is_var_arg:
                    star_matches.append(typ)
                elif args_have_kw_arg and typ.is_kw_arg:
                    star_matches.append(typ)
                else:
                    matches.append(typ)

    return star_matches + matches

</t>
<t tx="ekr.20221004064034.658">def infer_overload_return_type(
    self,
    plausible_targets: list[CallableType],
    args: list[Expression],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    callable_name: str | None,
    object_type: Type | None,
    context: Context,
) -&gt; tuple[Type, Type] | None:
    """Attempts to find the first matching callable from the given list.

    If a match is found, returns a tuple containing the result type and the inferred
    callee type. (This tuple is meant to be eventually returned by check_call.)
    If multiple targets match due to ambiguous Any parameters, returns (AnyType, AnyType).
    If no targets match, returns None.

    Assumes all of the given targets have argument counts compatible with the caller.
    """

    matches: list[CallableType] = []
    return_types: list[Type] = []
    inferred_types: list[Type] = []
    args_contain_any = any(map(has_any_type, arg_types))
    type_maps: list[dict[Expression, Type]] = []

    for typ in plausible_targets:
        assert self.msg is self.chk.msg
        with self.msg.filter_errors() as w:
            with self.chk.local_type_map() as m:
                ret_type, infer_type = self.check_call(
                    callee=typ,
                    args=args,
                    arg_kinds=arg_kinds,
                    arg_names=arg_names,
                    context=context,
                    callable_name=callable_name,
                    object_type=object_type,
                )
        is_match = not w.has_new_errors()
        if is_match:
            # Return early if possible; otherwise record info so we can
            # check for ambiguity due to 'Any' below.
            if not args_contain_any:
                return ret_type, infer_type
            matches.append(typ)
            return_types.append(ret_type)
            inferred_types.append(infer_type)
            type_maps.append(m)

    if len(matches) == 0:
        # No match was found
        return None
    elif any_causes_overload_ambiguity(matches, return_types, arg_types, arg_kinds, arg_names):
        # An argument of type or containing the type 'Any' caused ambiguity.
        # We try returning a precise type if we can. If not, we give up and just return 'Any'.
        if all_same_types(return_types):
            self.chk.store_types(type_maps[0])
            return return_types[0], inferred_types[0]
        elif all_same_types([erase_type(typ) for typ in return_types]):
            self.chk.store_types(type_maps[0])
            return erase_type(return_types[0]), erase_type(inferred_types[0])
        else:
            return self.check_call(
                callee=AnyType(TypeOfAny.special_form),
                args=args,
                arg_kinds=arg_kinds,
                arg_names=arg_names,
                context=context,
                callable_name=callable_name,
                object_type=object_type,
            )
    else:
        # Success! No ambiguity; return the first match.
        self.chk.store_types(type_maps[0])
        return return_types[0], inferred_types[0]

</t>
<t tx="ekr.20221004064034.659">def overload_erased_call_targets(
    self,
    plausible_targets: list[CallableType],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    args: list[Expression],
    context: Context,
) -&gt; list[CallableType]:
    """Returns a list of all targets that match the caller after erasing types.

    Assumes all of the given targets have argument counts compatible with the caller.
    """
    matches: list[CallableType] = []
    for typ in plausible_targets:
        if self.erased_signature_similarity(
            arg_types, arg_kinds, arg_names, args, typ, context
        ):
            matches.append(typ)
    return matches

</t>
<t tx="ekr.20221004064034.66">def parse_commit_title(diff: str) -&gt; str:
    m = re.search("\n    ([^ ].*)", diff)
    assert m is not None, "Could not parse diff"
    return m.group(1)


</t>
<t tx="ekr.20221004064034.660">def union_overload_result(
    self,
    plausible_targets: list[CallableType],
    args: list[Expression],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    callable_name: str | None,
    object_type: Type | None,
    context: Context,
    level: int = 0,
) -&gt; list[tuple[Type, Type]] | None:
    """Accepts a list of overload signatures and attempts to match calls by destructuring
    the first union.

    Return a list of (&lt;return type&gt;, &lt;inferred variant type&gt;) if call succeeds for every
    item of the desctructured union. Returns None if there is no match.
    """
    # Step 1: If we are already too deep, then stop immediately. Otherwise mypy might
    # hang for long time because of a weird overload call. The caller will get
    # the exception and generate an appropriate note message, if needed.
    if level &gt;= MAX_UNIONS:
        raise TooManyUnions

    # Step 2: Find position of the first union in arguments. Return the normal inferred
    # type if no more unions left.
    for idx, typ in enumerate(arg_types):
        if self.real_union(typ):
            break
    else:
        # No unions in args, just fall back to normal inference
        with self.type_overrides_set(args, arg_types):
            res = self.infer_overload_return_type(
                plausible_targets,
                args,
                arg_types,
                arg_kinds,
                arg_names,
                callable_name,
                object_type,
                context,
            )
        if res is not None:
            return [res]
        return None

    # Step 3: Try a direct match before splitting to avoid unnecessary union splits
    # and save performance.
    with self.type_overrides_set(args, arg_types):
        direct = self.infer_overload_return_type(
            plausible_targets,
            args,
            arg_types,
            arg_kinds,
            arg_names,
            callable_name,
            object_type,
            context,
        )
    if direct is not None and not isinstance(get_proper_type(direct[0]), (UnionType, AnyType)):
        # We only return non-unions soon, to avoid greedy match.
        return [direct]

    # Step 4: Split the first remaining union type in arguments into items and
    # try to match each item individually (recursive).
    first_union = get_proper_type(arg_types[idx])
    assert isinstance(first_union, UnionType)
    res_items = []
    for item in first_union.relevant_items():
        new_arg_types = arg_types.copy()
        new_arg_types[idx] = item
        sub_result = self.union_overload_result(
            plausible_targets,
            args,
            new_arg_types,
            arg_kinds,
            arg_names,
            callable_name,
            object_type,
            context,
            level + 1,
        )
        if sub_result is not None:
            res_items.extend(sub_result)
        else:
            # Some item doesn't match, return soon.
            return None

    # Step 5: If splitting succeeded, then filter out duplicate items before returning.
    seen: set[tuple[Type, Type]] = set()
    result = []
    for pair in res_items:
        if pair not in seen:
            seen.add(pair)
            result.append(pair)
    return result

</t>
<t tx="ekr.20221004064034.661">def real_union(self, typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, UnionType) and len(typ.relevant_items()) &gt; 1

</t>
<t tx="ekr.20221004064034.662">@contextmanager
def type_overrides_set(
    self, exprs: Sequence[Expression], overrides: Sequence[Type]
) -&gt; Iterator[None]:
    """Set _temporary_ type overrides for given expressions."""
    assert len(exprs) == len(overrides)
    for expr, typ in zip(exprs, overrides):
        self.type_overrides[expr] = typ
    try:
        yield
    finally:
        for expr in exprs:
            del self.type_overrides[expr]

</t>
<t tx="ekr.20221004064034.663">def combine_function_signatures(self, types: Sequence[Type]) -&gt; AnyType | CallableType:
    """Accepts a list of function signatures and attempts to combine them together into a
    new CallableType consisting of the union of all of the given arguments and return types.

    If there is at least one non-callable type, return Any (this can happen if there is
    an ambiguity because of Any in arguments).
    """
    assert types, "Trying to merge no callables"
    types = get_proper_types(types)
    if not all(isinstance(c, CallableType) for c in types):
        return AnyType(TypeOfAny.special_form)
    callables = cast(Sequence[CallableType], types)
    if len(callables) == 1:
        return callables[0]

    # Note: we are assuming here that if a user uses some TypeVar 'T' in
    # two different functions, they meant for that TypeVar to mean the
    # same thing.
    #
    # This function will make sure that all instances of that TypeVar 'T'
    # refer to the same underlying TypeVarType objects to simplify the union-ing
    # logic below.
    #
    # (If the user did *not* mean for 'T' to be consistently bound to the
    # same type in their overloads, well, their code is probably too
    # confusing and ought to be re-written anyways.)
    callables, variables = merge_typevars_in_callables_by_name(callables)

    new_args: list[list[Type]] = [[] for _ in range(len(callables[0].arg_types))]
    new_kinds = list(callables[0].arg_kinds)
    new_returns: list[Type] = []

    too_complex = False
    for target in callables:
        # We fall back to Callable[..., Union[&lt;returns&gt;]] if the functions do not have
        # the exact same signature. The only exception is if one arg is optional and
        # the other is positional: in that case, we continue unioning (and expect a
        # positional arg).
        # TODO: Enhance the merging logic to handle a wider variety of signatures.
        if len(new_kinds) != len(target.arg_kinds):
            too_complex = True
            break
        for i, (new_kind, target_kind) in enumerate(zip(new_kinds, target.arg_kinds)):
            if new_kind == target_kind:
                continue
            elif new_kind.is_positional() and target_kind.is_positional():
                new_kinds[i] = ARG_POS
            else:
                too_complex = True
                break

        if too_complex:
            break  # outer loop

        for i, arg in enumerate(target.arg_types):
            new_args[i].append(arg)
        new_returns.append(target.ret_type)

    union_return = make_simplified_union(new_returns)
    if too_complex:
        any = AnyType(TypeOfAny.special_form)
        return callables[0].copy_modified(
            arg_types=[any, any],
            arg_kinds=[ARG_STAR, ARG_STAR2],
            arg_names=[None, None],
            ret_type=union_return,
            variables=variables,
            implicit=True,
        )

    final_args = []
    for args_list in new_args:
        new_type = make_simplified_union(args_list)
        final_args.append(new_type)

    return callables[0].copy_modified(
        arg_types=final_args,
        arg_kinds=new_kinds,
        ret_type=union_return,
        variables=variables,
        implicit=True,
    )

</t>
<t tx="ekr.20221004064034.664">def erased_signature_similarity(
    self,
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    args: list[Expression],
    callee: CallableType,
    context: Context,
) -&gt; bool:
    """Determine whether arguments could match the signature at runtime, after
    erasing types."""
    formal_to_actual = map_actuals_to_formals(
        arg_kinds, arg_names, callee.arg_kinds, callee.arg_names, lambda i: arg_types[i]
    )

    with self.msg.filter_errors():
        if not self.check_argument_count(
            callee, arg_types, arg_kinds, arg_names, formal_to_actual, None
        ):
            # Too few or many arguments -&gt; no match.
            return False

    def check_arg(
        caller_type: Type,
        original_ccaller_type: Type,
        caller_kind: ArgKind,
        callee_type: Type,
        n: int,
        m: int,
        callee: CallableType,
        object_type: Type | None,
        context: Context,
        outer_context: Context,
    ) -&gt; None:
        if not arg_approximate_similarity(caller_type, callee_type):
            # No match -- exit early since none of the remaining work can change
            # the result.
            raise Finished

    try:
        self.check_argument_types(
            arg_types,
            arg_kinds,
            args,
            callee,
            formal_to_actual,
            context=context,
            check_arg=check_arg,
        )
        return True
    except Finished:
        return False

</t>
<t tx="ekr.20221004064034.665">def apply_generic_arguments(
    self,
    callable: CallableType,
    types: Sequence[Type | None],
    context: Context,
    skip_unsatisfied: bool = False,
) -&gt; CallableType:
    """Simple wrapper around mypy.applytype.apply_generic_arguments."""
    return applytype.apply_generic_arguments(
        callable,
        types,
        self.msg.incompatible_typevar_value,
        context,
        skip_unsatisfied=skip_unsatisfied,
    )

</t>
<t tx="ekr.20221004064034.666">def check_any_type_call(self, args: list[Expression], callee: Type) -&gt; tuple[Type, Type]:
    self.infer_arg_types_in_empty_context(args)
    callee = get_proper_type(callee)
    if isinstance(callee, AnyType):
        return (
            AnyType(TypeOfAny.from_another_any, source_any=callee),
            AnyType(TypeOfAny.from_another_any, source_any=callee),
        )
    else:
        return AnyType(TypeOfAny.special_form), AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.667">def check_union_call(
    self,
    callee: UnionType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
    context: Context,
) -&gt; tuple[Type, Type]:
    with self.msg.disable_type_names():
        results = [
            self.check_call(subtype, args, arg_kinds, context, arg_names)
            for subtype in callee.relevant_items()
        ]

    return (make_simplified_union([res[0] for res in results]), callee)

</t>
<t tx="ekr.20221004064034.668">def visit_member_expr(self, e: MemberExpr, is_lvalue: bool = False) -&gt; Type:
    """Visit member expression (of form e.id)."""
    self.chk.module_refs.update(extract_refexpr_names(e))
    result = self.analyze_ordinary_member_access(e, is_lvalue)
    return self.narrow_type_from_binder(e, result)

</t>
<t tx="ekr.20221004064034.669">def analyze_ordinary_member_access(self, e: MemberExpr, is_lvalue: bool) -&gt; Type:
    """Analyse member expression or member lvalue."""
    if e.kind is not None:
        # This is a reference to a module attribute.
        return self.analyze_ref_expr(e)
    else:
        # This is a reference to a non-module attribute.
        original_type = self.accept(e.expr, is_callee=self.is_callee)
        base = e.expr
        module_symbol_table = None

        if isinstance(base, RefExpr) and isinstance(base.node, MypyFile):
            module_symbol_table = base.node.names

        member_type = analyze_member_access(
            e.name,
            original_type,
            e,
            is_lvalue,
            False,
            False,
            self.msg,
            original_type=original_type,
            chk=self.chk,
            in_literal_context=self.is_literal_context(),
            module_symbol_table=module_symbol_table,
        )

        return member_type

</t>
<t tx="ekr.20221004064034.67">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--typeshed-dir", help="location of typeshed", metavar="dir", required=True
    )
    parser.add_argument("commit", help="typeshed commit hash to cherry-pick")
    args = parser.parse_args()
    typeshed_dir = args.typeshed_dir
    commit = args.commit

    if not os.path.isdir(typeshed_dir):
        sys.exit(f"error: {typeshed_dir} does not exist")
    if not re.match("[0-9a-fA-F]+$", commit):
        sys.exit(f"error: Invalid commit {commit!r}")

    if not os.path.exists("mypy") or not os.path.exists("mypyc"):
        sys.exit("error: This script must be run at the mypy repository root directory")

    with tempfile.TemporaryDirectory() as d:
        diff_file = os.path.join(d, "diff")
        out = subprocess.run(
            ["git", "show", commit], capture_output=True, text=True, check=True, cwd=typeshed_dir
        )
        with open(diff_file, "w") as f:
            f.write(out.stdout)
        subprocess.run(
            [
                "git",
                "apply",
                "--index",
                "--directory=mypy/typeshed",
                "--exclude=**/tests/**",
                diff_file,
            ],
            check=True,
        )

        title = parse_commit_title(out.stdout)
        subprocess.run(["git", "commit", "-m", f"Typeshed cherry-pick: {title}"], check=True)

    print()
    print(f"Cherry-picked commit {commit} from {typeshed_dir}")


</t>
<t tx="ekr.20221004064034.670">def analyze_external_member_access(
    self, member: str, base_type: Type, context: Context
) -&gt; Type:
    """Analyse member access that is external, i.e. it cannot
    refer to private definitions. Return the result type.
    """
    # TODO remove; no private definitions in mypy
    return analyze_member_access(
        member,
        base_type,
        context,
        False,
        False,
        False,
        self.msg,
        original_type=base_type,
        chk=self.chk,
        in_literal_context=self.is_literal_context(),
    )

</t>
<t tx="ekr.20221004064034.671">def is_literal_context(self) -&gt; bool:
    return is_literal_type_like(self.type_context[-1])

</t>
<t tx="ekr.20221004064034.672">def infer_literal_expr_type(self, value: LiteralValue, fallback_name: str) -&gt; Type:
    """Analyzes the given literal expression and determines if we should be
    inferring an Instance type, a Literal[...] type, or an Instance that
    remembers the original literal. We...

    1. ...Infer a normal Instance in most circumstances.

    2. ...Infer a Literal[...] if we're in a literal context. For example, if we
       were analyzing the "3" in "foo(3)" where "foo" has a signature of
       "def foo(Literal[3]) -&gt; None", we'd want to infer that the "3" has a
       type of Literal[3] instead of Instance.

    3. ...Infer an Instance that remembers the original Literal if we're declaring
       a Final variable with an inferred type -- for example, "bar" in "bar: Final = 3"
       would be assigned an Instance that remembers it originated from a '3'. See
       the comments in Instance's constructor for more details.
    """
    typ = self.named_type(fallback_name)
    if self.is_literal_context():
        return LiteralType(value=value, fallback=typ)
    else:
        return typ.copy_modified(
            last_known_value=LiteralType(
                value=value, fallback=typ, line=typ.line, column=typ.column
            )
        )

</t>
<t tx="ekr.20221004064034.673">def concat_tuples(self, left: TupleType, right: TupleType) -&gt; TupleType:
    """Concatenate two fixed length tuples."""
    return TupleType(
        items=left.items + right.items, fallback=self.named_type("builtins.tuple")
    )

</t>
<t tx="ekr.20221004064034.674">def visit_int_expr(self, e: IntExpr) -&gt; Type:
    """Type check an integer literal (trivial)."""
    return self.infer_literal_expr_type(e.value, "builtins.int")

</t>
<t tx="ekr.20221004064034.675">def visit_str_expr(self, e: StrExpr) -&gt; Type:
    """Type check a string literal (trivial)."""
    return self.infer_literal_expr_type(e.value, "builtins.str")

</t>
<t tx="ekr.20221004064034.676">def visit_bytes_expr(self, e: BytesExpr) -&gt; Type:
    """Type check a bytes literal (trivial)."""
    return self.infer_literal_expr_type(e.value, "builtins.bytes")

</t>
<t tx="ekr.20221004064034.677">def visit_float_expr(self, e: FloatExpr) -&gt; Type:
    """Type check a float literal (trivial)."""
    return self.named_type("builtins.float")

</t>
<t tx="ekr.20221004064034.678">def visit_complex_expr(self, e: ComplexExpr) -&gt; Type:
    """Type check a complex literal."""
    return self.named_type("builtins.complex")

</t>
<t tx="ekr.20221004064034.679">def visit_ellipsis(self, e: EllipsisExpr) -&gt; Type:
    """Type check '...'."""
    return self.named_type("builtins.ellipsis")

</t>
<t tx="ekr.20221004064034.68">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
"""Script for converting between cache formats.

We support a filesystem tree based cache and a sqlite based cache.
See mypy/metastore.py for details.
"""

from __future__ import annotations

import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse

from mypy.metastore import FilesystemMetadataStore, MetadataStore, SqliteMetadataStore


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.680">def visit_op_expr(self, e: OpExpr) -&gt; Type:
    """Type check a binary operator expression."""
    if e.op == "and" or e.op == "or":
        return self.check_boolean_op(e, e)
    if e.op == "*" and isinstance(e.left, ListExpr):
        # Expressions of form [...] * e get special type inference.
        return self.check_list_multiply(e)
    if e.op == "%":
        if isinstance(e.left, BytesExpr) and self.chk.options.python_version &gt;= (3, 5):
            return self.strfrm_checker.check_str_interpolation(e.left, e.right)
        if isinstance(e.left, StrExpr):
            return self.strfrm_checker.check_str_interpolation(e.left, e.right)
    left_type = self.accept(e.left)

    proper_left_type = get_proper_type(left_type)
    if isinstance(proper_left_type, TupleType) and e.op == "+":
        left_add_method = proper_left_type.partial_fallback.type.get("__add__")
        if left_add_method and left_add_method.fullname == "builtins.tuple.__add__":
            proper_right_type = get_proper_type(self.accept(e.right))
            if isinstance(proper_right_type, TupleType):
                right_radd_method = proper_right_type.partial_fallback.type.get("__radd__")
                if right_radd_method is None:
                    return self.concat_tuples(proper_left_type, proper_right_type)

    if e.op in operators.op_methods:
        method = operators.op_methods[e.op]
        result, method_type = self.check_op(method, left_type, e.right, e, allow_reverse=True)
        e.method_type = method_type
        return result
    else:
        raise RuntimeError(f"Unknown operator {e.op}")

</t>
<t tx="ekr.20221004064034.681">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; Type:
    """Type check a comparison expression.

    Comparison expressions are type checked consecutive-pair-wise
    That is, 'a &lt; b &gt; c == d' is check as 'a &lt; b and b &gt; c and c == d'
    """
    result: Type | None = None
    sub_result: Type | None = None

    # Check each consecutive operand pair and their operator
    for left, right, operator in zip(e.operands, e.operands[1:], e.operators):
        left_type = self.accept(left)

        method_type: mypy.types.Type | None = None

        if operator == "in" or operator == "not in":
            # If the right operand has partial type, look it up without triggering
            # a "Need type annotation ..." message, as it would be noise.
            right_type = self.find_partial_type_ref_fast_path(right)
            if right_type is None:
                right_type = self.accept(right)  # Validate the right operand

            # Keep track of whether we get type check errors (these won't be reported, they
            # are just to verify whether something is valid typing wise).
            with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
                _, method_type = self.check_method_call_by_name(
                    method="__contains__",
                    base_type=right_type,
                    args=[left],
                    arg_kinds=[ARG_POS],
                    context=e,
                )

            sub_result = self.bool_type()
            # Container item type for strict type overlap checks. Note: we need to only
            # check for nominal type, because a usual "Unsupported operands for in"
            # will be reported for types incompatible with __contains__().
            # See testCustomContainsCheckStrictEquality for an example.
            cont_type = self.chk.analyze_container_item_type(right_type)
            if isinstance(right_type, PartialType):
                # We don't really know if this is an error or not, so just shut up.
                pass
            elif (
                local_errors.has_new_errors()
                and
                # is_valid_var_arg is True for any Iterable
                self.is_valid_var_arg(right_type)
            ):
                _, itertype = self.chk.analyze_iterable_item_type(right)
                method_type = CallableType(
                    [left_type],
                    [nodes.ARG_POS],
                    [None],
                    self.bool_type(),
                    self.named_type("builtins.function"),
                )
                if not is_subtype(left_type, itertype):
                    self.msg.unsupported_operand_types("in", left_type, right_type, e)
            # Only show dangerous overlap if there are no other errors.
            elif (
                not local_errors.has_new_errors()
                and cont_type
                and self.dangerous_comparison(
                    left_type, cont_type, original_container=right_type
                )
            ):
                self.msg.dangerous_comparison(left_type, cont_type, "container", e)
            else:
                self.msg.add_errors(local_errors.filtered_errors())
        elif operator in operators.op_methods:
            method = operators.op_methods[operator]

            with ErrorWatcher(self.msg.errors) as w:
                sub_result, method_type = self.check_op(
                    method, left_type, right, e, allow_reverse=True
                )

            # Only show dangerous overlap if there are no other errors. See
            # testCustomEqCheckStrictEquality for an example.
            if not w.has_new_errors() and operator in ("==", "!="):
                right_type = self.accept(right)
                # We suppress the error if there is a custom __eq__() method on either
                # side. User defined (or even standard library) classes can define this
                # to return True for comparisons between non-overlapping types.
                if not custom_special_method(
                    left_type, "__eq__"
                ) and not custom_special_method(right_type, "__eq__"):
                    # Also flag non-overlapping literals in situations like:
                    #    x: Literal['a', 'b']
                    #    if x == 'c':
                    #        ...
                    left_type = try_getting_literal(left_type)
                    right_type = try_getting_literal(right_type)
                    if self.dangerous_comparison(left_type, right_type):
                        self.msg.dangerous_comparison(left_type, right_type, "equality", e)

        elif operator == "is" or operator == "is not":
            right_type = self.accept(right)  # validate the right operand
            sub_result = self.bool_type()
            left_type = try_getting_literal(left_type)
            right_type = try_getting_literal(right_type)
            if self.dangerous_comparison(left_type, right_type):
                self.msg.dangerous_comparison(left_type, right_type, "identity", e)
            method_type = None
        else:
            raise RuntimeError(f"Unknown comparison operator {operator}")

        e.method_types.append(method_type)

        #  Determine type of boolean-and of result and sub_result
        if result is None:
            result = sub_result
        else:
            result = join.join_types(result, sub_result)

    assert result is not None
    return result

</t>
<t tx="ekr.20221004064034.682">def find_partial_type_ref_fast_path(self, expr: Expression) -&gt; Type | None:
    """If expression has a partial generic type, return it without additional checks.

    In particular, this does not generate an error about a missing annotation.

    Otherwise, return None.
    """
    if not isinstance(expr, RefExpr):
        return None
    if isinstance(expr.node, Var):
        result = self.analyze_var_ref(expr.node, expr)
        if isinstance(result, PartialType) and result.type is not None:
            self.chk.store_type(expr, self.chk.fixup_partial_type(result))
            return result
    return None

</t>
<t tx="ekr.20221004064034.683">def dangerous_comparison(
    self, left: Type, right: Type, original_container: Type | None = None
) -&gt; bool:
    """Check for dangerous non-overlapping comparisons like 42 == 'no'.

    The original_container is the original container type for 'in' checks
    (and None for equality checks).

    Rules:
        * X and None are overlapping even in strict-optional mode. This is to allow
        'assert x is not None' for x defined as 'x = None  # type: str' in class body
        (otherwise mypy itself would have couple dozen errors because of this).
        * Optional[X] and Optional[Y] are non-overlapping if X and Y are
        non-overlapping, although technically None is overlap, it is most
        likely an error.
        * Any overlaps with everything, i.e. always safe.
        * Special case: b'abc' in b'cde' is safe.
    """
    if not self.chk.options.strict_equality:
        return False

    left, right = get_proper_types((left, right))

    if self.chk.binder.is_unreachable_warning_suppressed():
        # We are inside a function that contains type variables with value restrictions in
        # its signature. In this case we just suppress all strict-equality checks to avoid
        # false positives for code like:
        #
        #     T = TypeVar('T', str, int)
        #     def f(x: T) -&gt; T:
        #         if x == 0:
        #             ...
        #         return x
        #
        # TODO: find a way of disabling the check only for types resulted from the expansion.
        return False
    if isinstance(left, NoneType) or isinstance(right, NoneType):
        return False
    if isinstance(left, UnionType) and isinstance(right, UnionType):
        left = remove_optional(left)
        right = remove_optional(right)
        left, right = get_proper_types((left, right))
    if (
        original_container
        and has_bytes_component(original_container)
        and has_bytes_component(left)
    ):
        # We need to special case bytes and bytearray, because 97 in b'abc', b'a' in b'abc',
        # b'a' in bytearray(b'abc') etc. all return True (and we want to show the error only
        # if the check can _never_ be True).
        return False
    if isinstance(left, Instance) and isinstance(right, Instance):
        # Special case some builtin implementations of AbstractSet.
        if (
            left.type.fullname in OVERLAPPING_TYPES_ALLOWLIST
            and right.type.fullname in OVERLAPPING_TYPES_ALLOWLIST
        ):
            abstract_set = self.chk.lookup_typeinfo("typing.AbstractSet")
            left = map_instance_to_supertype(left, abstract_set)
            right = map_instance_to_supertype(right, abstract_set)
            return not is_overlapping_types(left.args[0], right.args[0])
    if isinstance(left, LiteralType) and isinstance(right, LiteralType):
        if isinstance(left.value, bool) and isinstance(right.value, bool):
            # Comparing different booleans is not dangerous.
            return False
    return not is_overlapping_types(left, right, ignore_promotions=False)

</t>
<t tx="ekr.20221004064034.684">def check_method_call_by_name(
    self,
    method: str,
    base_type: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    original_type: Type | None = None,
) -&gt; tuple[Type, Type]:
    """Type check a call to a named method on an object.

    Return tuple (result type, inferred method type). The 'original_type'
    is used for error messages.
    """
    original_type = original_type or base_type
    # Unions are special-cased to allow plugins to act on each element of the union.
    base_type = get_proper_type(base_type)
    if isinstance(base_type, UnionType):
        return self.check_union_method_call_by_name(
            method, base_type, args, arg_kinds, context, original_type
        )

    method_type = analyze_member_access(
        method,
        base_type,
        context,
        False,
        False,
        True,
        self.msg,
        original_type=original_type,
        chk=self.chk,
        in_literal_context=self.is_literal_context(),
    )
    return self.check_method_call(method, base_type, method_type, args, arg_kinds, context)

</t>
<t tx="ekr.20221004064034.685">def check_union_method_call_by_name(
    self,
    method: str,
    base_type: UnionType,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
    original_type: Type | None = None,
) -&gt; tuple[Type, Type]:
    """Type check a call to a named method on an object with union type.

    This essentially checks the call using check_method_call_by_name() for each
    union item and unions the result. We do this to allow plugins to act on
    individual union items.
    """
    res: list[Type] = []
    meth_res: list[Type] = []
    for typ in base_type.relevant_items():
        # Format error messages consistently with
        # mypy.checkmember.analyze_union_member_access().
        with self.msg.disable_type_names():
            item, meth_item = self.check_method_call_by_name(
                method, typ, args, arg_kinds, context, original_type
            )
        res.append(item)
        meth_res.append(meth_item)
    return make_simplified_union(res), make_simplified_union(meth_res)

</t>
<t tx="ekr.20221004064034.686">def check_method_call(
    self,
    method_name: str,
    base_type: Type,
    method_type: Type,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    context: Context,
) -&gt; tuple[Type, Type]:
    """Type check a call to a method with the given name and type on an object.

    Return tuple (result type, inferred method type).
    """
    callable_name = self.method_fullname(base_type, method_name)
    object_type = base_type if callable_name is not None else None

    # Try to refine the method signature using plugin hooks before checking the call.
    method_type = self.transform_callee_type(
        callable_name, method_type, args, arg_kinds, context, object_type=object_type
    )

    return self.check_call(
        method_type,
        args,
        arg_kinds,
        context,
        callable_name=callable_name,
        object_type=base_type,
    )

</t>
<t tx="ekr.20221004064034.687">def check_op_reversible(
    self,
    op_name: str,
    left_type: Type,
    left_expr: Expression,
    right_type: Type,
    right_expr: Expression,
    context: Context,
) -&gt; tuple[Type, Type]:
    def lookup_operator(op_name: str, base_type: Type) -&gt; Type | None:
        """Looks up the given operator and returns the corresponding type,
        if it exists."""

        # This check is an important performance optimization,
        # even though it is mostly a subset of
        # analyze_member_access.
        # TODO: Find a way to remove this call without performance implications.
        if not self.has_member(base_type, op_name):
            return None

        with self.msg.filter_errors() as w:
            member = analyze_member_access(
                name=op_name,
                typ=base_type,
                is_lvalue=False,
                is_super=False,
                is_operator=True,
                original_type=base_type,
                context=context,
                msg=self.msg,
                chk=self.chk,
                in_literal_context=self.is_literal_context(),
            )
            return None if w.has_new_errors() else member

    def lookup_definer(typ: Instance, attr_name: str) -&gt; str | None:
        """Returns the name of the class that contains the actual definition of attr_name.

        So if class A defines foo and class B subclasses A, running
        'get_class_defined_in(B, "foo")` would return the full name of A.

        However, if B were to override and redefine foo, that method call would
        return the full name of B instead.

        If the attr name is not present in the given class or its MRO, returns None.
        """
        for cls in typ.type.mro:
            if cls.names.get(attr_name):
                return cls.fullname
        return None

    left_type = get_proper_type(left_type)
    right_type = get_proper_type(right_type)

    # If either the LHS or the RHS are Any, we can't really concluding anything
    # about the operation since the Any type may or may not define an
    # __op__ or __rop__ method. So, we punt and return Any instead.

    if isinstance(left_type, AnyType):
        any_type = AnyType(TypeOfAny.from_another_any, source_any=left_type)
        return any_type, any_type
    if isinstance(right_type, AnyType):
        any_type = AnyType(TypeOfAny.from_another_any, source_any=right_type)
        return any_type, any_type

    # STEP 1:
    # We start by getting the __op__ and __rop__ methods, if they exist.

    rev_op_name = operators.reverse_op_methods[op_name]

    left_op = lookup_operator(op_name, left_type)
    right_op = lookup_operator(rev_op_name, right_type)

    # STEP 2a:
    # We figure out in which order Python will call the operator methods. As it
    # turns out, it's not as simple as just trying to call __op__ first and
    # __rop__ second.
    #
    # We store the determined order inside the 'variants_raw' variable,
    # which records tuples containing the method, base type, and the argument.

    if op_name in operators.op_methods_that_shortcut and is_same_type(left_type, right_type):
        # When we do "A() + A()", for example, Python will only call the __add__ method,
        # never the __radd__ method.
        #
        # This is the case even if the __add__ method is completely missing and the __radd__
        # method is defined.

        variants_raw = [(left_op, left_type, right_expr)]
    elif (
        is_subtype(right_type, left_type)
        and isinstance(left_type, Instance)
        and isinstance(right_type, Instance)
        and left_type.type.alt_promote is not right_type.type
        and lookup_definer(left_type, op_name) != lookup_definer(right_type, rev_op_name)
    ):
        # When we do "A() + B()" where B is a subclass of A, we'll actually try calling
        # B's __radd__ method first, but ONLY if B explicitly defines or overrides the
        # __radd__ method.
        #
        # This mechanism lets subclasses "refine" the expected outcome of the operation, even
        # if they're located on the RHS.
        #
        # As a special case, the alt_promote check makes sure that we don't use the
        # __radd__ method of int if the LHS is a native int type.

        variants_raw = [(right_op, right_type, left_expr), (left_op, left_type, right_expr)]
    else:
        # In all other cases, we do the usual thing and call __add__ first and
        # __radd__ second when doing "A() + B()".

        variants_raw = [(left_op, left_type, right_expr), (right_op, right_type, left_expr)]

    # STEP 3:
    # We now filter out all non-existent operators. The 'variants' list contains
    # all operator methods that are actually present, in the order that Python
    # attempts to invoke them.

    variants = [(op, obj, arg) for (op, obj, arg) in variants_raw if op is not None]

    # STEP 4:
    # We now try invoking each one. If an operation succeeds, end early and return
    # the corresponding result. Otherwise, return the result and errors associated
    # with the first entry.

    errors = []
    results = []
    for method, obj, arg in variants:
        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            result = self.check_method_call(op_name, obj, method, [arg], [ARG_POS], context)
        if local_errors.has_new_errors():
            errors.append(local_errors.filtered_errors())
            results.append(result)
        else:
            return result

    # We finish invoking above operators and no early return happens. Therefore,
    # we check if either the LHS or the RHS is Instance and fallbacks to Any,
    # if so, we also return Any
    if (isinstance(left_type, Instance) and left_type.type.fallback_to_any) or (
        isinstance(right_type, Instance) and right_type.type.fallback_to_any
    ):
        any_type = AnyType(TypeOfAny.special_form)
        return any_type, any_type

    # STEP 4b:
    # Sometimes, the variants list is empty. In that case, we fall-back to attempting to
    # call the __op__ method (even though it's missing).

    if not variants:
        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            result = self.check_method_call_by_name(
                op_name, left_type, [right_expr], [ARG_POS], context
            )

        if local_errors.has_new_errors():
            errors.append(local_errors.filtered_errors())
            results.append(result)
        else:
            # In theory, we should never enter this case, but it seems
            # we sometimes do, when dealing with Type[...]? E.g. see
            # check-classes.testTypeTypeComparisonWorks.
            #
            # This is probably related to the TODO in lookup_operator(...)
            # up above.
            #
            # TODO: Remove this extra case
            return result

    self.msg.add_errors(errors[0])
    if len(results) == 1:
        return results[0]
    else:
        error_any = AnyType(TypeOfAny.from_error)
        result = error_any, error_any
        return result

</t>
<t tx="ekr.20221004064034.688">def check_op(
    self,
    method: str,
    base_type: Type,
    arg: Expression,
    context: Context,
    allow_reverse: bool = False,
) -&gt; tuple[Type, Type]:
    """Type check a binary operation which maps to a method call.

    Return tuple (result type, inferred operator method type).
    """

    if allow_reverse:
        left_variants = [base_type]
        base_type = get_proper_type(base_type)
        if isinstance(base_type, UnionType):
            left_variants = [
                item for item in flatten_nested_unions(base_type.relevant_items())
            ]
        right_type = self.accept(arg)

        # Step 1: We first try leaving the right arguments alone and destructure
        # just the left ones. (Mypy can sometimes perform some more precise inference
        # if we leave the right operands a union -- see testOperatorWithEmptyListAndSum.)
        all_results = []
        all_inferred = []

        with self.msg.filter_errors() as local_errors:
            for left_possible_type in left_variants:
                result, inferred = self.check_op_reversible(
                    op_name=method,
                    left_type=left_possible_type,
                    left_expr=TempNode(left_possible_type, context=context),
                    right_type=right_type,
                    right_expr=arg,
                    context=context,
                )
                all_results.append(result)
                all_inferred.append(inferred)

        if not local_errors.has_new_errors():
            results_final = make_simplified_union(all_results)
            inferred_final = make_simplified_union(all_inferred)
            return results_final, inferred_final

        # Step 2: If that fails, we try again but also destructure the right argument.
        # This is also necessary to make certain edge cases work -- see
        # testOperatorDoubleUnionInterwovenUnionAdd, for example.

        # Note: We want to pass in the original 'arg' for 'left_expr' and 'right_expr'
        # whenever possible so that plugins and similar things can introspect on the original
        # node if possible.
        #
        # We don't do the same for the base expression because it could lead to weird
        # type inference errors -- e.g. see 'testOperatorDoubleUnionSum'.
        # TODO: Can we use `type_overrides_set()` here?
        right_variants = [(right_type, arg)]
        right_type = get_proper_type(right_type)
        if isinstance(right_type, UnionType):
            right_variants = [
                (item, TempNode(item, context=context))
                for item in flatten_nested_unions(right_type.relevant_items())
            ]

        all_results = []
        all_inferred = []

        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            for left_possible_type in left_variants:
                for right_possible_type, right_expr in right_variants:
                    result, inferred = self.check_op_reversible(
                        op_name=method,
                        left_type=left_possible_type,
                        left_expr=TempNode(left_possible_type, context=context),
                        right_type=right_possible_type,
                        right_expr=right_expr,
                        context=context,
                    )
                    all_results.append(result)
                    all_inferred.append(inferred)

        if local_errors.has_new_errors():
            self.msg.add_errors(local_errors.filtered_errors())
            # Point any notes to the same location as an existing message.
            err = local_errors.filtered_errors()[-1]
            recent_context = TempNode(NoneType())
            recent_context.line = err.line
            recent_context.column = err.column
            if len(left_variants) &gt;= 2 and len(right_variants) &gt;= 2:
                self.msg.warn_both_operands_are_from_unions(recent_context)
            elif len(left_variants) &gt;= 2:
                self.msg.warn_operand_was_from_union("Left", base_type, context=recent_context)
            elif len(right_variants) &gt;= 2:
                self.msg.warn_operand_was_from_union(
                    "Right", right_type, context=recent_context
                )

        # See the comment in 'check_overload_call' for more details on why
        # we call 'combine_function_signature' instead of just unioning the inferred
        # callable types.
        results_final = make_simplified_union(all_results)
        inferred_final = self.combine_function_signatures(all_inferred)
        return results_final, inferred_final
    else:
        return self.check_method_call_by_name(
            method=method,
            base_type=base_type,
            args=[arg],
            arg_kinds=[ARG_POS],
            context=context,
        )

</t>
<t tx="ekr.20221004064034.689">def check_boolean_op(self, e: OpExpr, context: Context) -&gt; Type:
    """Type check a boolean operation ('and' or 'or')."""

    # A boolean operation can evaluate to either of the operands.

    # We use the current type context to guide the type inference of of
    # the left operand. We also use the left operand type to guide the type
    # inference of the right operand so that expressions such as
    # '[1] or []' are inferred correctly.
    ctx = self.type_context[-1]
    left_type = self.accept(e.left, ctx)
    expanded_left_type = try_expanding_sum_type_to_union(
        self.accept(e.left, ctx), "builtins.bool"
    )

    assert e.op in ("and", "or")  # Checked by visit_op_expr

    if e.right_always:
        left_map: mypy.checker.TypeMap = None
        right_map: mypy.checker.TypeMap = {}
    elif e.right_unreachable:
        left_map, right_map = {}, None
    elif e.op == "and":
        right_map, left_map = self.chk.find_isinstance_check(e.left)
    elif e.op == "or":
        left_map, right_map = self.chk.find_isinstance_check(e.left)

    # If left_map is None then we know mypy considers the left expression
    # to be redundant.
    if (
        codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes
        and left_map is None
        # don't report an error if it's intentional
        and not e.right_always
    ):
        self.msg.redundant_left_operand(e.op, e.left)

    if (
        self.chk.should_report_unreachable_issues()
        and right_map is None
        # don't report an error if it's intentional
        and not e.right_unreachable
    ):
        self.msg.unreachable_right_operand(e.op, e.right)

    # If right_map is None then we know mypy considers the right branch
    # to be unreachable and therefore any errors found in the right branch
    # should be suppressed.
    with self.msg.filter_errors(filter_errors=right_map is None):
        right_type = self.analyze_cond_branch(right_map, e.right, expanded_left_type)

    if left_map is None and right_map is None:
        return UninhabitedType()

    if right_map is None:
        # The boolean expression is statically known to be the left value
        assert left_map is not None
        return left_type
    if left_map is None:
        # The boolean expression is statically known to be the right value
        assert right_map is not None
        return right_type

    if e.op == "and":
        restricted_left_type = false_only(expanded_left_type)
        result_is_left = not expanded_left_type.can_be_true
    elif e.op == "or":
        restricted_left_type = true_only(expanded_left_type)
        result_is_left = not expanded_left_type.can_be_false

    if isinstance(restricted_left_type, UninhabitedType):
        # The left operand can never be the result
        return right_type
    elif result_is_left:
        # The left operand is always the result
        return left_type
    else:
        return make_simplified_union([restricted_left_type, right_type])

</t>
<t tx="ekr.20221004064034.69">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--to-sqlite",
        action="store_true",
        default=False,
        help="Convert to a sqlite cache (default: convert from)",
    )
    parser.add_argument(
        "--output_dir",
        action="store",
        default=None,
        help="Output cache location (default: same as input)",
    )
    parser.add_argument("input_dir", help="Input directory for the cache")
    args = parser.parse_args()

    input_dir = args.input_dir
    output_dir = args.output_dir or input_dir
    if args.to_sqlite:
        input: MetadataStore = FilesystemMetadataStore(input_dir)
        output: MetadataStore = SqliteMetadataStore(output_dir)
    else:
        input, output = SqliteMetadataStore(input_dir), FilesystemMetadataStore(output_dir)

    for s in input.list_all():
        if s.endswith(".json"):
            assert output.write(s, input.read(s), input.getmtime(s)), "Failed to write cache file!"
    output.commit()


</t>
<t tx="ekr.20221004064034.690">def check_list_multiply(self, e: OpExpr) -&gt; Type:
    """Type check an expression of form '[...] * e'.

    Type inference is special-cased for this common construct.
    """
    right_type = self.accept(e.right)
    if is_subtype(right_type, self.named_type("builtins.int")):
        # Special case: [...] * &lt;int value&gt;. Use the type context of the
        # OpExpr, since the multiplication does not affect the type.
        left_type = self.accept(e.left, type_context=self.type_context[-1])
    else:
        left_type = self.accept(e.left)
    result, method_type = self.check_op("__mul__", left_type, e.right, e)
    e.method_type = method_type
    return result

</t>
<t tx="ekr.20221004064034.691">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; Type:
    value = self.accept(e.value)
    self.chk.check_assignment(e.target, e.value)
    self.chk.check_final(e)
    self.chk.store_type(e.target, value)
    self.find_partial_type_ref_fast_path(e.target)
    return value

</t>
<t tx="ekr.20221004064034.692">def visit_unary_expr(self, e: UnaryExpr) -&gt; Type:
    """Type check an unary operation ('not', '-', '+' or '~')."""
    operand_type = self.accept(e.expr)
    op = e.op
    if op == "not":
        result: Type = self.bool_type()
    else:
        method = operators.unary_op_methods[op]
        result, method_type = self.check_method_call_by_name(method, operand_type, [], [], e)
        e.method_type = method_type
    return result

</t>
<t tx="ekr.20221004064034.693">def visit_index_expr(self, e: IndexExpr) -&gt; Type:
    """Type check an index expression (base[index]).

    It may also represent type application.
    """
    result = self.visit_index_expr_helper(e)
    result = self.narrow_type_from_binder(e, result)
    p_result = get_proper_type(result)
    if (
        self.is_literal_context()
        and isinstance(p_result, Instance)
        and p_result.last_known_value is not None
    ):
        result = p_result.last_known_value
    return result

</t>
<t tx="ekr.20221004064034.694">def visit_index_expr_helper(self, e: IndexExpr) -&gt; Type:
    if e.analyzed:
        # It's actually a type application.
        return self.accept(e.analyzed)
    left_type = self.accept(e.base)
    return self.visit_index_with_type(left_type, e)

</t>
<t tx="ekr.20221004064034.695">def visit_index_with_type(
    self, left_type: Type, e: IndexExpr, original_type: ProperType | None = None
) -&gt; Type:
    """Analyze type of an index expression for a given type of base expression.

    The 'original_type' is used for error messages (currently used for union types).
    """
    index = e.index
    left_type = get_proper_type(left_type)

    # Visit the index, just to make sure we have a type for it available
    self.accept(index)

    if isinstance(left_type, UnionType):
        original_type = original_type or left_type
        # Don't combine literal types, since we may need them for type narrowing.
        return make_simplified_union(
            [
                self.visit_index_with_type(typ, e, original_type)
                for typ in left_type.relevant_items()
            ],
            contract_literals=False,
        )
    elif isinstance(left_type, TupleType) and self.chk.in_checked_function():
        # Special case for tuples. They return a more specific type when
        # indexed by an integer literal.
        if isinstance(index, SliceExpr):
            return self.visit_tuple_slice_helper(left_type, index)

        ns = self.try_getting_int_literals(index)
        if ns is not None:
            out = []
            for n in ns:
                if n &lt; 0:
                    n += len(left_type.items)
                if 0 &lt;= n &lt; len(left_type.items):
                    out.append(left_type.items[n])
                else:
                    self.chk.fail(message_registry.TUPLE_INDEX_OUT_OF_RANGE, e)
                    return AnyType(TypeOfAny.from_error)
            return make_simplified_union(out)
        else:
            return self.nonliteral_tuple_index_helper(left_type, index)
    elif isinstance(left_type, TypedDictType):
        return self.visit_typeddict_index_expr(left_type, e.index)
    elif (
        isinstance(left_type, CallableType)
        and left_type.is_type_obj()
        and left_type.type_object().is_enum
    ):
        return self.visit_enum_index_expr(left_type.type_object(), e.index, e)
    elif isinstance(left_type, TypeVarType) and not self.has_member(
        left_type.upper_bound, "__getitem__"
    ):
        return self.visit_index_with_type(left_type.upper_bound, e, original_type)
    else:
        result, method_type = self.check_method_call_by_name(
            "__getitem__", left_type, [e.index], [ARG_POS], e, original_type=original_type
        )
        e.method_type = method_type
        return result

</t>
<t tx="ekr.20221004064034.696">def visit_tuple_slice_helper(self, left_type: TupleType, slic: SliceExpr) -&gt; Type:
    begin: Sequence[int | None] = [None]
    end: Sequence[int | None] = [None]
    stride: Sequence[int | None] = [None]

    if slic.begin_index:
        begin_raw = self.try_getting_int_literals(slic.begin_index)
        if begin_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        begin = begin_raw

    if slic.end_index:
        end_raw = self.try_getting_int_literals(slic.end_index)
        if end_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        end = end_raw

    if slic.stride:
        stride_raw = self.try_getting_int_literals(slic.stride)
        if stride_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        stride = stride_raw

    items: list[Type] = []
    for b, e, s in itertools.product(begin, end, stride):
        items.append(left_type.slice(b, e, s))
    return make_simplified_union(items)

</t>
<t tx="ekr.20221004064034.697">def try_getting_int_literals(self, index: Expression) -&gt; list[int] | None:
    """If the given expression or type corresponds to an int literal
    or a union of int literals, returns a list of the underlying ints.
    Otherwise, returns None.

    Specifically, this function is guaranteed to return a list with
    one or more ints if one one the following is true:

    1. 'expr' is a IntExpr or a UnaryExpr backed by an IntExpr
    2. 'typ' is a LiteralType containing an int
    3. 'typ' is a UnionType containing only LiteralType of ints
    """
    if isinstance(index, IntExpr):
        return [index.value]
    elif isinstance(index, UnaryExpr):
        if index.op == "-":
            operand = index.expr
            if isinstance(operand, IntExpr):
                return [-1 * operand.value]
    typ = get_proper_type(self.accept(index))
    if isinstance(typ, Instance) and typ.last_known_value is not None:
        typ = typ.last_known_value
    if isinstance(typ, LiteralType) and isinstance(typ.value, int):
        return [typ.value]
    if isinstance(typ, UnionType):
        out = []
        for item in get_proper_types(typ.items):
            if isinstance(item, LiteralType) and isinstance(item.value, int):
                out.append(item.value)
            else:
                return None
        return out
    return None

</t>
<t tx="ekr.20221004064034.698">def nonliteral_tuple_index_helper(self, left_type: TupleType, index: Expression) -&gt; Type:
    self.check_method_call_by_name("__getitem__", left_type, [index], [ARG_POS], context=index)
    # We could return the return type from above, but unions are often better than the join
    union = make_simplified_union(left_type.items)
    if isinstance(index, SliceExpr):
        return self.chk.named_generic_type("builtins.tuple", [union])
    return union

</t>
<t tx="ekr.20221004064034.699">def visit_typeddict_index_expr(self, td_type: TypedDictType, index: Expression) -&gt; Type:
    if isinstance(index, StrExpr):
        key_names = [index.value]
    else:
        typ = get_proper_type(self.accept(index))
        if isinstance(typ, UnionType):
            key_types: list[Type] = list(typ.items)
        else:
            key_types = [typ]

        key_names = []
        for key_type in get_proper_types(key_types):
            if isinstance(key_type, Instance) and key_type.last_known_value is not None:
                key_type = key_type.last_known_value

            if (
                isinstance(key_type, LiteralType)
                and isinstance(key_type.value, str)
                and key_type.fallback.type.fullname != "builtins.bytes"
            ):
                key_names.append(key_type.value)
            else:
                self.msg.typeddict_key_must_be_string_literal(td_type, index)
                return AnyType(TypeOfAny.from_error)

    value_types = []
    for key_name in key_names:
        value_type = td_type.items.get(key_name)
        if value_type is None:
            self.msg.typeddict_key_not_found(td_type, key_name, index)
            return AnyType(TypeOfAny.from_error)
        else:
            value_types.append(value_type)
    return make_simplified_union(value_types)

</t>
<t tx="ekr.20221004064034.7">def ekr_f_annotated(ekr_a: str) -&gt; None:
    pass
    
</t>
<t tx="ekr.20221004064034.70">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
"""Produce a diff between mypy caches.

With some infrastructure, this can allow for distributing small cache diffs to users in
many cases instead of full cache artifacts.
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from collections import defaultdict
from typing import Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mypy.metastore import FilesystemMetadataStore, MetadataStore, SqliteMetadataStore


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.700">def visit_enum_index_expr(
    self, enum_type: TypeInfo, index: Expression, context: Context
) -&gt; Type:
    string_type: Type = self.named_type("builtins.str")
    self.chk.check_subtype(
        self.accept(index),
        string_type,
        context,
        "Enum index should be a string",
        "actual index type",
    )
    return Instance(enum_type, [])

</t>
<t tx="ekr.20221004064034.701">def visit_cast_expr(self, expr: CastExpr) -&gt; Type:
    """Type check a cast expression."""
    source_type = self.accept(
        expr.expr,
        type_context=AnyType(TypeOfAny.special_form),
        allow_none_return=True,
        always_allow_any=True,
    )
    target_type = expr.type
    options = self.chk.options
    if (
        options.warn_redundant_casts
        and not isinstance(get_proper_type(target_type), AnyType)
        and source_type == target_type
    ):
        self.msg.redundant_cast(target_type, expr)
    if options.disallow_any_unimported and has_any_from_unimported_type(target_type):
        self.msg.unimported_type_becomes_any("Target type of cast", target_type, expr)
    check_for_explicit_any(
        target_type, self.chk.options, self.chk.is_typeshed_stub, self.msg, context=expr
    )
    return target_type

</t>
<t tx="ekr.20221004064034.702">def visit_assert_type_expr(self, expr: AssertTypeExpr) -&gt; Type:
    source_type = self.accept(
        expr.expr,
        type_context=self.type_context[-1],
        allow_none_return=True,
        always_allow_any=True,
    )
    target_type = expr.type
    if not is_same_type(source_type, target_type):
        if not self.chk.in_checked_function():
            self.msg.note(
                '"assert_type" expects everything to be "Any" in unchecked functions',
                expr.expr,
            )
        self.msg.assert_type_fail(source_type, target_type, expr)
    return source_type

</t>
<t tx="ekr.20221004064034.703">def visit_reveal_expr(self, expr: RevealExpr) -&gt; Type:
    """Type check a reveal_type expression."""
    if expr.kind == REVEAL_TYPE:
        assert expr.expr is not None
        revealed_type = self.accept(
            expr.expr, type_context=self.type_context[-1], allow_none_return=True
        )
        if not self.chk.current_node_deferred:
            self.msg.reveal_type(revealed_type, expr.expr)
            if not self.chk.in_checked_function():
                self.msg.note(
                    "'reveal_type' always outputs 'Any' in unchecked functions", expr.expr
                )
        return revealed_type
    else:
        # REVEAL_LOCALS
        if not self.chk.current_node_deferred:
            # the RevealExpr contains a local_nodes attribute,
            # calculated at semantic analysis time. Use it to pull out the
            # corresponding subset of variables in self.chk.type_map
            names_to_types = (
                {var_node.name: var_node.type for var_node in expr.local_nodes}
                if expr.local_nodes is not None
                else {}
            )

            self.msg.reveal_locals(names_to_types, expr)
        return NoneType()

</t>
<t tx="ekr.20221004064034.704">def visit_type_application(self, tapp: TypeApplication) -&gt; Type:
    """Type check a type application (expr[type, ...]).

    There are two different options here, depending on whether expr refers
    to a type alias or directly to a generic class. In the first case we need
    to use a dedicated function typeanal.expand_type_aliases. This
    is due to the fact that currently type aliases machinery uses
    unbound type variables, while normal generics use bound ones;
    see TypeAlias docstring for more details.
    """
    if isinstance(tapp.expr, RefExpr) and isinstance(tapp.expr.node, TypeAlias):
        # Subscription of a (generic) alias in runtime context, expand the alias.
        item = expand_type_alias(
            tapp.expr.node, tapp.types, self.chk.fail, tapp.expr.node.no_args, tapp
        )
        item = get_proper_type(item)
        if isinstance(item, Instance):
            tp = type_object_type(item.type, self.named_type)
            return self.apply_type_arguments_to_callable(tp, item.args, tapp)
        elif isinstance(item, TupleType) and item.partial_fallback.type.is_named_tuple:
            tp = type_object_type(item.partial_fallback.type, self.named_type)
            return self.apply_type_arguments_to_callable(tp, item.partial_fallback.args, tapp)
        elif isinstance(item, TypedDictType):
            return self.typeddict_callable_from_context(item)
        else:
            self.chk.fail(message_registry.ONLY_CLASS_APPLICATION, tapp)
            return AnyType(TypeOfAny.from_error)
    # Type application of a normal generic class in runtime context.
    # This is typically used as `x = G[int]()`.
    tp = get_proper_type(self.accept(tapp.expr))
    if isinstance(tp, (CallableType, Overloaded)):
        if not tp.is_type_obj():
            self.chk.fail(message_registry.ONLY_CLASS_APPLICATION, tapp)
        return self.apply_type_arguments_to_callable(tp, tapp.types, tapp)
    if isinstance(tp, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=tp)
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.705">def visit_type_alias_expr(self, alias: TypeAliasExpr) -&gt; Type:
    """Right hand side of a type alias definition.

    It has the same type as if the alias itself was used in a runtime context.
    For example, here:

        A = reveal_type(List[T])
        reveal_type(A)

    both `reveal_type` instances will reveal the same type `def (...) -&gt; builtins.list[Any]`.
    Note that type variables are implicitly substituted with `Any`.
    """
    return self.alias_type_in_runtime_context(alias.node, ctx=alias, alias_definition=True)

</t>
<t tx="ekr.20221004064034.706">def alias_type_in_runtime_context(
    self, alias: TypeAlias, *, ctx: Context, alias_definition: bool = False
) -&gt; Type:
    """Get type of a type alias (could be generic) in a runtime expression.

    Note that this function can be called only if the alias appears _not_
    as a target of type application, which is treated separately in the
    visit_type_application method. Some examples where this method is called are
    casts and instantiation:

        class LongName(Generic[T]): ...
        A = LongName[int]

        x = A()
        y = cast(A, ...)
    """
    if isinstance(alias.target, Instance) and alias.target.invalid:  # type: ignore[misc]
        # An invalid alias, error already has been reported
        return AnyType(TypeOfAny.from_error)
    # If this is a generic alias, we set all variables to `Any`.
    # For example:
    #     A = List[Tuple[T, T]]
    #     x = A() &lt;- same as List[Tuple[Any, Any]], see PEP 484.
    disallow_any = self.chk.options.disallow_any_generics and self.is_callee
    item = get_proper_type(
        set_any_tvars(
            alias, ctx.line, ctx.column, disallow_any=disallow_any, fail=self.msg.fail
        )
    )
    if isinstance(item, Instance):
        # Normally we get a callable type (or overloaded) with .is_type_obj() true
        # representing the class's constructor
        tp = type_object_type(item.type, self.named_type)
        if alias.no_args:
            return tp
        return self.apply_type_arguments_to_callable(tp, item.args, ctx)
    elif (
        isinstance(item, TupleType)
        and
        # Tuple[str, int]() fails at runtime, only named tuples and subclasses work.
        tuple_fallback(item).type.fullname != "builtins.tuple"
    ):
        return type_object_type(tuple_fallback(item).type, self.named_type)
    elif isinstance(item, TypedDictType):
        return self.typeddict_callable_from_context(item)
    elif isinstance(item, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=item)
    else:
        if alias_definition:
            return AnyType(TypeOfAny.special_form)
        # This type is invalid in most runtime contexts, give it an 'object' type.
        # TODO: Use typing._SpecialForm instead?
        return self.named_type("builtins.object")

</t>
<t tx="ekr.20221004064034.707">def apply_type_arguments_to_callable(
    self, tp: Type, args: Sequence[Type], ctx: Context
) -&gt; Type:
    """Apply type arguments to a generic callable type coming from a type object.

    This will first perform type arguments count checks, report the
    error as needed, and return the correct kind of Any. As a special
    case this returns Any for non-callable types, because if type object type
    is not callable, then an error should be already reported.
    """
    tp = get_proper_type(tp)

    if isinstance(tp, CallableType):
        if len(tp.variables) != len(args):
            self.msg.incompatible_type_application(len(tp.variables), len(args), ctx)
            return AnyType(TypeOfAny.from_error)
        return self.apply_generic_arguments(tp, args, ctx)
    if isinstance(tp, Overloaded):
        for it in tp.items:
            if len(it.variables) != len(args):
                self.msg.incompatible_type_application(len(it.variables), len(args), ctx)
                return AnyType(TypeOfAny.from_error)
        return Overloaded([self.apply_generic_arguments(it, args, ctx) for it in tp.items])
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.708">def visit_list_expr(self, e: ListExpr) -&gt; Type:
    """Type check a list expression [...]."""
    return self.check_lst_expr(e, "builtins.list", "&lt;list&gt;")

</t>
<t tx="ekr.20221004064034.709">def visit_set_expr(self, e: SetExpr) -&gt; Type:
    return self.check_lst_expr(e, "builtins.set", "&lt;set&gt;")

</t>
<t tx="ekr.20221004064034.71">def make_cache(input_dir: str, sqlite: bool) -&gt; MetadataStore:
    if sqlite:
        return SqliteMetadataStore(input_dir)
    else:
        return FilesystemMetadataStore(input_dir)


</t>
<t tx="ekr.20221004064034.710">def fast_container_type(
    self, e: ListExpr | SetExpr | TupleExpr, container_fullname: str
) -&gt; Type | None:
    """
    Fast path to determine the type of a list or set literal,
    based on the list of entries. This mostly impacts large
    module-level constant definitions.

    Limitations:
     - no active type context
     - no star expressions
     - the joined type of all entries must be an Instance or Tuple type
    """
    ctx = self.type_context[-1]
    if ctx:
        return None
    rt = self.resolved_type.get(e, None)
    if rt is not None:
        return rt if isinstance(rt, Instance) else None
    values: list[Type] = []
    for item in e.items:
        if isinstance(item, StarExpr):
            # fallback to slow path
            self.resolved_type[e] = NoneType()
            return None
        values.append(self.accept(item))
    vt = join.join_type_list(values)
    if not allow_fast_container_literal(vt):
        self.resolved_type[e] = NoneType()
        return None
    ct = self.chk.named_generic_type(container_fullname, [vt])
    self.resolved_type[e] = ct
    return ct

</t>
<t tx="ekr.20221004064034.711">def check_lst_expr(self, e: ListExpr | SetExpr | TupleExpr, fullname: str, tag: str) -&gt; Type:
    # fast path
    t = self.fast_container_type(e, fullname)
    if t:
        return t

    # Translate into type checking a generic function call.
    # Used for list and set expressions, as well as for tuples
    # containing star expressions that don't refer to a
    # Tuple. (Note: "lst" stands for list-set-tuple. :-)
    tv = TypeVarType("T", "T", -1, [], self.object_type())
    constructor = CallableType(
        [tv],
        [nodes.ARG_STAR],
        [None],
        self.chk.named_generic_type(fullname, [tv]),
        self.named_type("builtins.function"),
        name=tag,
        variables=[tv],
    )
    out = self.check_call(
        constructor,
        [(i.expr if isinstance(i, StarExpr) else i) for i in e.items],
        [(nodes.ARG_STAR if isinstance(i, StarExpr) else nodes.ARG_POS) for i in e.items],
        e,
    )[0]
    return remove_instance_last_known_values(out)

</t>
<t tx="ekr.20221004064034.712">def visit_tuple_expr(self, e: TupleExpr) -&gt; Type:
    """Type check a tuple expression."""
    # Try to determine type context for type inference.
    type_context = get_proper_type(self.type_context[-1])
    type_context_items = None
    if isinstance(type_context, UnionType):
        tuples_in_context = [
            t
            for t in get_proper_types(type_context.items)
            if (isinstance(t, TupleType) and len(t.items) == len(e.items))
            or is_named_instance(t, TUPLE_LIKE_INSTANCE_NAMES)
        ]
        if len(tuples_in_context) == 1:
            type_context = tuples_in_context[0]
        else:
            # There are either no relevant tuples in the Union, or there is
            # more than one.  Either way, we can't decide on a context.
            pass

    if isinstance(type_context, TupleType):
        type_context_items = type_context.items
    elif type_context and is_named_instance(type_context, TUPLE_LIKE_INSTANCE_NAMES):
        assert isinstance(type_context, Instance)
        if type_context.args:
            type_context_items = [type_context.args[0]] * len(e.items)
    # NOTE: it's possible for the context to have a different
    # number of items than e.  In that case we use those context
    # items that match a position in e, and we'll worry about type
    # mismatches later.

    # Infer item types.  Give up if there's a star expression
    # that's not a Tuple.
    items: list[Type] = []
    j = 0  # Index into type_context_items; irrelevant if type_context_items is none
    for i in range(len(e.items)):
        item = e.items[i]
        if isinstance(item, StarExpr):
            # Special handling for star expressions.
            # TODO: If there's a context, and item.expr is a
            # TupleExpr, flatten it, so we can benefit from the
            # context?  Counterargument: Why would anyone write
            # (1, *(2, 3)) instead of (1, 2, 3) except in a test?
            tt = self.accept(item.expr)
            tt = get_proper_type(tt)
            if isinstance(tt, TupleType):
                items.extend(tt.items)
                j += len(tt.items)
            else:
                # A star expression that's not a Tuple.
                # Treat the whole thing as a variable-length tuple.
                return self.check_lst_expr(e, "builtins.tuple", "&lt;tuple&gt;")
        else:
            if not type_context_items or j &gt;= len(type_context_items):
                tt = self.accept(item)
            else:
                tt = self.accept(item, type_context_items[j])
                j += 1
            items.append(tt)
    # This is a partial fallback item type. A precise type will be calculated on demand.
    fallback_item = AnyType(TypeOfAny.special_form)
    return TupleType(items, self.chk.named_generic_type("builtins.tuple", [fallback_item]))

</t>
<t tx="ekr.20221004064034.713">def fast_dict_type(self, e: DictExpr) -&gt; Type | None:
    """
    Fast path to determine the type of a dict literal,
    based on the list of entries. This mostly impacts large
    module-level constant definitions.

    Limitations:
     - no active type context
     - only supported star expressions are other dict instances
     - the joined types of all keys and values must be Instance or Tuple types
    """
    ctx = self.type_context[-1]
    if ctx:
        return None
    rt = self.resolved_type.get(e, None)
    if rt is not None:
        return rt if isinstance(rt, Instance) else None
    keys: list[Type] = []
    values: list[Type] = []
    stargs: tuple[Type, Type] | None = None
    for key, value in e.items:
        if key is None:
            st = get_proper_type(self.accept(value))
            if (
                isinstance(st, Instance)
                and st.type.fullname == "builtins.dict"
                and len(st.args) == 2
            ):
                stargs = (st.args[0], st.args[1])
            else:
                self.resolved_type[e] = NoneType()
                return None
        else:
            keys.append(self.accept(key))
            values.append(self.accept(value))
    kt = join.join_type_list(keys)
    vt = join.join_type_list(values)
    if not (allow_fast_container_literal(kt) and allow_fast_container_literal(vt)):
        self.resolved_type[e] = NoneType()
        return None
    if stargs and (stargs[0] != kt or stargs[1] != vt):
        self.resolved_type[e] = NoneType()
        return None
    dt = self.chk.named_generic_type("builtins.dict", [kt, vt])
    self.resolved_type[e] = dt
    return dt

</t>
<t tx="ekr.20221004064034.714">def visit_dict_expr(self, e: DictExpr) -&gt; Type:
    """Type check a dict expression.

    Translate it into a call to dict(), with provisions for **expr.
    """
    # if the dict literal doesn't match TypedDict, check_typeddict_call_with_dict reports
    # an error, but returns the TypedDict type that matches the literal it found
    # that would cause a second error when that TypedDict type is returned upstream
    # to avoid the second error, we always return TypedDict type that was requested
    typeddict_context = self.find_typeddict_context(self.type_context[-1], e)
    if typeddict_context:
        orig_ret_type = self.check_typeddict_call_with_dict(
            callee=typeddict_context, kwargs=e, context=e, orig_callee=None
        )
        ret_type = get_proper_type(orig_ret_type)
        if isinstance(ret_type, TypedDictType):
            return ret_type.copy_modified()
        return typeddict_context.copy_modified()

    # fast path attempt
    dt = self.fast_dict_type(e)
    if dt:
        return dt

    # Collect function arguments, watching out for **expr.
    args: list[Expression] = []  # Regular "key: value"
    stargs: list[Expression] = []  # For "**expr"
    for key, value in e.items:
        if key is None:
            stargs.append(value)
        else:
            tup = TupleExpr([key, value])
            if key.line &gt;= 0:
                tup.line = key.line
                tup.column = key.column
            else:
                tup.line = value.line
                tup.column = value.column
            args.append(tup)
    # Define type variables (used in constructors below).
    kt = TypeVarType("KT", "KT", -1, [], self.object_type())
    vt = TypeVarType("VT", "VT", -2, [], self.object_type())
    rv = None
    # Call dict(*args), unless it's empty and stargs is not.
    if args or not stargs:
        # The callable type represents a function like this:
        #
        #   def &lt;unnamed&gt;(*v: Tuple[kt, vt]) -&gt; Dict[kt, vt]: ...
        constructor = CallableType(
            [TupleType([kt, vt], self.named_type("builtins.tuple"))],
            [nodes.ARG_STAR],
            [None],
            self.chk.named_generic_type("builtins.dict", [kt, vt]),
            self.named_type("builtins.function"),
            name="&lt;dict&gt;",
            variables=[kt, vt],
        )
        rv = self.check_call(constructor, args, [nodes.ARG_POS] * len(args), e)[0]
    else:
        # dict(...) will be called below.
        pass
    # Call rv.update(arg) for each arg in **stargs,
    # except if rv isn't set yet, then set rv = dict(arg).
    if stargs:
        for arg in stargs:
            if rv is None:
                constructor = CallableType(
                    [self.chk.named_generic_type("typing.Mapping", [kt, vt])],
                    [nodes.ARG_POS],
                    [None],
                    self.chk.named_generic_type("builtins.dict", [kt, vt]),
                    self.named_type("builtins.function"),
                    name="&lt;list&gt;",
                    variables=[kt, vt],
                )
                rv = self.check_call(constructor, [arg], [nodes.ARG_POS], arg)[0]
            else:
                self.check_method_call_by_name("update", rv, [arg], [nodes.ARG_POS], arg)
    assert rv is not None
    return rv

</t>
<t tx="ekr.20221004064034.715">def find_typeddict_context(
    self, context: Type | None, dict_expr: DictExpr
) -&gt; TypedDictType | None:
    context = get_proper_type(context)
    if isinstance(context, TypedDictType):
        return context
    elif isinstance(context, UnionType):
        items = []
        for item in context.items:
            item_context = self.find_typeddict_context(item, dict_expr)
            if item_context is not None and self.match_typeddict_call_with_dict(
                item_context, dict_expr, dict_expr
            ):
                items.append(item_context)
        if len(items) == 1:
            # Only one union item is valid TypedDict for the given dict_expr, so use the
            # context as it's unambiguous.
            return items[0]
        if len(items) &gt; 1:
            self.msg.typeddict_context_ambiguous(items, dict_expr)
    # No TypedDict type in context.
    return None

</t>
<t tx="ekr.20221004064034.716">def visit_lambda_expr(self, e: LambdaExpr) -&gt; Type:
    """Type check lambda expression."""
    self.chk.check_default_args(e, body_is_trivial=False)
    inferred_type, type_override = self.infer_lambda_type_using_context(e)
    if not inferred_type:
        self.chk.return_types.append(AnyType(TypeOfAny.special_form))
        # Type check everything in the body except for the final return
        # statement (it can contain tuple unpacking before return).
        with self.chk.scope.push_function(e):
            # Lambdas can have more than one element in body,
            # when we add "fictional" AssigmentStatement nodes, like in:
            # `lambda (a, b): a`
            for stmt in e.body.body[:-1]:
                stmt.accept(self.chk)
            # Only type check the return expression, not the return statement.
            # This is important as otherwise the following statements would be
            # considered unreachable. There's no useful type context.
            ret_type = self.accept(e.expr(), allow_none_return=True)
        fallback = self.named_type("builtins.function")
        self.chk.return_types.pop()
        return callable_type(e, fallback, ret_type)
    else:
        # Type context available.
        self.chk.return_types.append(inferred_type.ret_type)
        self.chk.check_func_item(e, type_override=type_override)
        if not self.chk.has_type(e.expr()):
            # TODO: return expression must be accepted before exiting function scope.
            self.accept(e.expr(), allow_none_return=True)
        ret_type = self.chk.lookup_type(e.expr())
        self.chk.return_types.pop()
        return replace_callable_return_type(inferred_type, ret_type)

</t>
<t tx="ekr.20221004064034.717">def infer_lambda_type_using_context(
    self, e: LambdaExpr
) -&gt; tuple[CallableType | None, CallableType | None]:
    """Try to infer lambda expression type using context.

    Return None if could not infer type.
    The second item in the return type is the type_override parameter for check_func_item.
    """
    # TODO also accept 'Any' context
    ctx = get_proper_type(self.type_context[-1])

    if isinstance(ctx, UnionType):
        callables = [
            t for t in get_proper_types(ctx.relevant_items()) if isinstance(t, CallableType)
        ]
        if len(callables) == 1:
            ctx = callables[0]

    if not ctx or not isinstance(ctx, CallableType):
        return None, None

    # The context may have function type variables in it. We replace them
    # since these are the type variables we are ultimately trying to infer;
    # they must be considered as indeterminate. We use ErasedType since it
    # does not affect type inference results (it is for purposes like this
    # only).
    callable_ctx = get_proper_type(replace_meta_vars(ctx, ErasedType()))
    assert isinstance(callable_ctx, CallableType)

    # The callable_ctx may have a fallback of builtins.type if the context
    # is a constructor -- but this fallback doesn't make sense for lambdas.
    callable_ctx = callable_ctx.copy_modified(fallback=self.named_type("builtins.function"))

    if callable_ctx.type_guard is not None:
        # Lambda's return type cannot be treated as a `TypeGuard`,
        # because it is implicit. And `TypeGuard`s must be explicit.
        # See https://github.com/python/mypy/issues/9927
        return None, None

    arg_kinds = [arg.kind for arg in e.arguments]

    if callable_ctx.is_ellipsis_args or ctx.param_spec() is not None:
        # Fill in Any arguments to match the arguments of the lambda.
        callable_ctx = callable_ctx.copy_modified(
            is_ellipsis_args=False,
            arg_types=[AnyType(TypeOfAny.special_form)] * len(arg_kinds),
            arg_kinds=arg_kinds,
            arg_names=e.arg_names[:],
        )

    if ARG_STAR in arg_kinds or ARG_STAR2 in arg_kinds:
        # TODO treat this case appropriately
        return callable_ctx, None

    if callable_ctx.arg_kinds != arg_kinds:
        # Incompatible context; cannot use it to infer types.
        self.chk.fail(message_registry.CANNOT_INFER_LAMBDA_TYPE, e)
        return None, None

    return callable_ctx, callable_ctx

</t>
<t tx="ekr.20221004064034.718">def visit_super_expr(self, e: SuperExpr) -&gt; Type:
    """Type check a super expression (non-lvalue)."""

    # We have an expression like super(T, var).member

    # First compute the types of T and var
    types = self._super_arg_types(e)
    if isinstance(types, tuple):
        type_type, instance_type = types
    else:
        return types

    # Now get the MRO
    type_info = type_info_from_type(type_type)
    if type_info is None:
        self.chk.fail(message_registry.UNSUPPORTED_ARG_1_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    instance_info = type_info_from_type(instance_type)
    if instance_info is None:
        self.chk.fail(message_registry.UNSUPPORTED_ARG_2_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    mro = instance_info.mro

    # The base is the first MRO entry *after* type_info that has a member
    # with the right name
    index = None
    if type_info in mro:
        index = mro.index(type_info)
    else:
        method = self.chk.scope.top_function()
        assert method is not None
        # Mypy explicitly allows supertype upper bounds (and no upper bound at all)
        # for annotating self-types. However, if such an annotation is used for
        # checking super() we will still get an error. So to be consistent, we also
        # allow such imprecise annotations for use with super(), where we fall back
        # to the current class MRO instead.
        if is_self_type_like(instance_type, is_classmethod=method.is_class):
            if e.info and type_info in e.info.mro:
                mro = e.info.mro
                index = mro.index(type_info)
    if index is None:
        self.chk.fail(message_registry.SUPER_ARG_2_NOT_INSTANCE_OF_ARG_1, e)
        return AnyType(TypeOfAny.from_error)

    if len(mro) == index + 1:
        self.chk.fail(message_registry.TARGET_CLASS_HAS_NO_BASE_CLASS, e)
        return AnyType(TypeOfAny.from_error)

    for base in mro[index + 1 :]:
        if e.name in base.names or base == mro[-1]:
            if e.info and e.info.fallback_to_any and base == mro[-1]:
                # There's an undefined base class, and we're at the end of the
                # chain.  That's not an error.
                return AnyType(TypeOfAny.special_form)

            return analyze_member_access(
                name=e.name,
                typ=instance_type,
                is_lvalue=False,
                is_super=True,
                is_operator=False,
                original_type=instance_type,
                override_info=base,
                context=e,
                msg=self.msg,
                chk=self.chk,
                in_literal_context=self.is_literal_context(),
            )

    assert False, "unreachable"

</t>
<t tx="ekr.20221004064034.719">def _super_arg_types(self, e: SuperExpr) -&gt; Type | tuple[Type, Type]:
    """
    Computes the types of the type and instance expressions in super(T, instance), or the
    implicit ones for zero-argument super() expressions.  Returns a single type for the whole
    super expression when possible (for errors, anys), otherwise the pair of computed types.
    """

    if not self.chk.in_checked_function():
        return AnyType(TypeOfAny.unannotated)
    elif len(e.call.args) == 0:
        if not e.info:
            # This has already been reported by the semantic analyzer.
            return AnyType(TypeOfAny.from_error)
        elif self.chk.scope.active_class():
            self.chk.fail(message_registry.SUPER_OUTSIDE_OF_METHOD_NOT_SUPPORTED, e)
            return AnyType(TypeOfAny.from_error)

        # Zero-argument super() is like super(&lt;current class&gt;, &lt;self&gt;)
        current_type = fill_typevars(e.info)
        type_type: ProperType = TypeType(current_type)

        # Use the type of the self argument, in case it was annotated
        method = self.chk.scope.top_function()
        assert method is not None
        if method.arguments:
            instance_type: Type = method.arguments[0].variable.type or current_type
        else:
            self.chk.fail(message_registry.SUPER_ENCLOSING_POSITIONAL_ARGS_REQUIRED, e)
            return AnyType(TypeOfAny.from_error)
    elif ARG_STAR in e.call.arg_kinds:
        self.chk.fail(message_registry.SUPER_VARARGS_NOT_SUPPORTED, e)
        return AnyType(TypeOfAny.from_error)
    elif set(e.call.arg_kinds) != {ARG_POS}:
        self.chk.fail(message_registry.SUPER_POSITIONAL_ARGS_REQUIRED, e)
        return AnyType(TypeOfAny.from_error)
    elif len(e.call.args) == 1:
        self.chk.fail(message_registry.SUPER_WITH_SINGLE_ARG_NOT_SUPPORTED, e)
        return AnyType(TypeOfAny.from_error)
    elif len(e.call.args) == 2:
        type_type = get_proper_type(self.accept(e.call.args[0]))
        instance_type = self.accept(e.call.args[1])
    else:
        self.chk.fail(message_registry.TOO_MANY_ARGS_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    # Imprecisely assume that the type is the current class
    if isinstance(type_type, AnyType):
        if e.info:
            type_type = TypeType(fill_typevars(e.info))
        else:
            return AnyType(TypeOfAny.from_another_any, source_any=type_type)
    elif isinstance(type_type, TypeType):
        type_item = type_type.item
        if isinstance(type_item, AnyType):
            if e.info:
                type_type = TypeType(fill_typevars(e.info))
            else:
                return AnyType(TypeOfAny.from_another_any, source_any=type_item)

    if not isinstance(type_type, TypeType) and not (
        isinstance(type_type, FunctionLike) and type_type.is_type_obj()
    ):
        self.msg.first_argument_for_super_must_be_type(type_type, e)
        return AnyType(TypeOfAny.from_error)

    # Imprecisely assume that the instance is of the current class
    instance_type = get_proper_type(instance_type)
    if isinstance(instance_type, AnyType):
        if e.info:
            instance_type = fill_typevars(e.info)
        else:
            return AnyType(TypeOfAny.from_another_any, source_any=instance_type)
    elif isinstance(instance_type, TypeType):
        instance_item = instance_type.item
        if isinstance(instance_item, AnyType):
            if e.info:
                instance_type = TypeType(fill_typevars(e.info))
            else:
                return AnyType(TypeOfAny.from_another_any, source_any=instance_item)

    return type_type, instance_type

</t>
<t tx="ekr.20221004064034.72">def merge_deps(all: dict[str, set[str]], new: dict[str, set[str]]) -&gt; None:
    for k, v in new.items():
        all.setdefault(k, set()).update(v)


</t>
<t tx="ekr.20221004064034.720">def visit_slice_expr(self, e: SliceExpr) -&gt; Type:
    expected = make_optional_type(self.named_type("builtins.int"))
    for index in [e.begin_index, e.end_index, e.stride]:
        if index:
            t = self.accept(index)
            self.chk.check_subtype(t, expected, index, message_registry.INVALID_SLICE_INDEX)
    return self.named_type("builtins.slice")

</t>
<t tx="ekr.20221004064034.721">def visit_list_comprehension(self, e: ListComprehension) -&gt; Type:
    return self.check_generator_or_comprehension(
        e.generator, "builtins.list", "&lt;list-comprehension&gt;"
    )

</t>
<t tx="ekr.20221004064034.722">def visit_set_comprehension(self, e: SetComprehension) -&gt; Type:
    return self.check_generator_or_comprehension(
        e.generator, "builtins.set", "&lt;set-comprehension&gt;"
    )

</t>
<t tx="ekr.20221004064034.723">def visit_generator_expr(self, e: GeneratorExpr) -&gt; Type:
    # If any of the comprehensions use async for, the expression will return an async generator
    # object, or if the left-side expression uses await.
    if any(e.is_async) or has_await_expression(e.left_expr):
        typ = "typing.AsyncGenerator"
        # received type is always None in async generator expressions
        additional_args: list[Type] = [NoneType()]
    else:
        typ = "typing.Generator"
        # received type and returned type are None
        additional_args = [NoneType(), NoneType()]
    return self.check_generator_or_comprehension(
        e, typ, "&lt;generator&gt;", additional_args=additional_args
    )

</t>
<t tx="ekr.20221004064034.724">def check_generator_or_comprehension(
    self,
    gen: GeneratorExpr,
    type_name: str,
    id_for_messages: str,
    additional_args: list[Type] | None = None,
) -&gt; Type:
    """Type check a generator expression or a list comprehension."""
    additional_args = additional_args or []
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        self.check_for_comp(gen)

        # Infer the type of the list comprehension by using a synthetic generic
        # callable type.
        tv = TypeVarType("T", "T", -1, [], self.object_type())
        tv_list: list[Type] = [tv]
        constructor = CallableType(
            tv_list,
            [nodes.ARG_POS],
            [None],
            self.chk.named_generic_type(type_name, tv_list + additional_args),
            self.chk.named_type("builtins.function"),
            name=id_for_messages,
            variables=[tv],
        )
        return self.check_call(constructor, [gen.left_expr], [nodes.ARG_POS], gen)[0]

</t>
<t tx="ekr.20221004064034.725">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; Type:
    """Type check a dictionary comprehension."""
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        self.check_for_comp(e)

        # Infer the type of the list comprehension by using a synthetic generic
        # callable type.
        ktdef = TypeVarType("KT", "KT", -1, [], self.object_type())
        vtdef = TypeVarType("VT", "VT", -2, [], self.object_type())
        constructor = CallableType(
            [ktdef, vtdef],
            [nodes.ARG_POS, nodes.ARG_POS],
            [None, None],
            self.chk.named_generic_type("builtins.dict", [ktdef, vtdef]),
            self.chk.named_type("builtins.function"),
            name="&lt;dictionary-comprehension&gt;",
            variables=[ktdef, vtdef],
        )
        return self.check_call(
            constructor, [e.key, e.value], [nodes.ARG_POS, nodes.ARG_POS], e
        )[0]

</t>
<t tx="ekr.20221004064034.726">def check_for_comp(self, e: GeneratorExpr | DictionaryComprehension) -&gt; None:
    """Check the for_comp part of comprehensions. That is the part from 'for':
    ... for x in y if z

    Note: This adds the type information derived from the condlists to the current binder.
    """
    for index, sequence, conditions, is_async in zip(
        e.indices, e.sequences, e.condlists, e.is_async
    ):
        if is_async:
            _, sequence_type = self.chk.analyze_async_iterable_item_type(sequence)
        else:
            _, sequence_type = self.chk.analyze_iterable_item_type(sequence)
        self.chk.analyze_index_variables(index, sequence_type, True, e)
        for condition in conditions:
            self.accept(condition)

            # values are only part of the comprehension when all conditions are true
            true_map, false_map = self.chk.find_isinstance_check(condition)

            if true_map:
                self.chk.push_type_map(true_map)

            if codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes:
                if true_map is None:
                    self.msg.redundant_condition_in_comprehension(False, condition)
                elif false_map is None:
                    self.msg.redundant_condition_in_comprehension(True, condition)

</t>
<t tx="ekr.20221004064034.727">def visit_conditional_expr(self, e: ConditionalExpr, allow_none_return: bool = False) -&gt; Type:
    self.accept(e.cond)
    ctx = self.type_context[-1]

    # Gain type information from isinstance if it is there
    # but only for the current expression
    if_map, else_map = self.chk.find_isinstance_check(e.cond)
    if codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes:
        if if_map is None:
            self.msg.redundant_condition_in_if(False, e.cond)
        elif else_map is None:
            self.msg.redundant_condition_in_if(True, e.cond)

    if_type = self.analyze_cond_branch(
        if_map, e.if_expr, context=ctx, allow_none_return=allow_none_return
    )

    # we want to keep the narrowest value of if_type for union'ing the branches
    # however, it would be silly to pass a literal as a type context. Pass the
    # underlying fallback type instead.
    if_type_fallback = simple_literal_type(get_proper_type(if_type)) or if_type

    # Analyze the right branch using full type context and store the type
    full_context_else_type = self.analyze_cond_branch(
        else_map, e.else_expr, context=ctx, allow_none_return=allow_none_return
    )

    if not mypy.checker.is_valid_inferred_type(if_type):
        # Analyze the right branch disregarding the left branch.
        else_type = full_context_else_type
        # we want to keep the narrowest value of else_type for union'ing the branches
        # however, it would be silly to pass a literal as a type context. Pass the
        # underlying fallback type instead.
        else_type_fallback = simple_literal_type(get_proper_type(else_type)) or else_type

        # If it would make a difference, re-analyze the left
        # branch using the right branch's type as context.
        if ctx is None or not is_equivalent(else_type_fallback, ctx):
            # TODO: If it's possible that the previous analysis of
            # the left branch produced errors that are avoided
            # using this context, suppress those errors.
            if_type = self.analyze_cond_branch(
                if_map,
                e.if_expr,
                context=else_type_fallback,
                allow_none_return=allow_none_return,
            )

    elif if_type_fallback == ctx:
        # There is no point re-running the analysis if if_type is equal to ctx.
        # That would  be an exact duplicate of the work we just did.
        # This optimization is particularly important to avoid exponential blowup with nested
        # if/else expressions: https://github.com/python/mypy/issues/9591
        # TODO: would checking for is_proper_subtype also work and cover more cases?
        else_type = full_context_else_type
    else:
        # Analyze the right branch in the context of the left
        # branch's type.
        else_type = self.analyze_cond_branch(
            else_map,
            e.else_expr,
            context=if_type_fallback,
            allow_none_return=allow_none_return,
        )

    # Only create a union type if the type context is a union, to be mostly
    # compatible with older mypy versions where we always did a join.
    #
    # TODO: Always create a union or at least in more cases?
    if isinstance(get_proper_type(self.type_context[-1]), UnionType):
        res = make_simplified_union([if_type, full_context_else_type])
    else:
        res = join.join_types(if_type, else_type)

    return res

</t>
<t tx="ekr.20221004064034.728">def analyze_cond_branch(
    self,
    map: dict[Expression, Type] | None,
    node: Expression,
    context: Type | None,
    allow_none_return: bool = False,
) -&gt; Type:
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        if map is None:
            # We still need to type check node, in case we want to
            # process it for isinstance checks later
            self.accept(node, type_context=context, allow_none_return=allow_none_return)
            return UninhabitedType()
        self.chk.push_type_map(map)
        return self.accept(node, type_context=context, allow_none_return=allow_none_return)

</t>
<t tx="ekr.20221004064034.729">#
# Helpers
#

</t>
<t tx="ekr.20221004064034.73">def load(cache: MetadataStore, s: str) -&gt; Any:
    data = cache.read(s)
    obj = json.loads(data)
    if s.endswith(".meta.json"):
        # For meta files, zero out the mtimes and sort the
        # dependencies to avoid spurious conflicts
        obj["mtime"] = 0
        obj["data_mtime"] = 0
        if "dependencies" in obj:
            all_deps = obj["dependencies"] + obj["suppressed"]
            num_deps = len(obj["dependencies"])
            thing = list(zip(all_deps, obj["dep_prios"], obj["dep_lines"]))

            def unzip(x: Any) -&gt; Any:
                return zip(*x) if x else ((), (), ())

            obj["dependencies"], prios1, lines1 = unzip(sorted(thing[:num_deps]))
            obj["suppressed"], prios2, lines2 = unzip(sorted(thing[num_deps:]))
            obj["dep_prios"] = prios1 + prios2
            obj["dep_lines"] = lines1 + lines2
    if s.endswith(".deps.json"):
        # For deps files, sort the deps to avoid spurious mismatches
        for v in obj.values():
            v.sort()
    return obj


</t>
<t tx="ekr.20221004064034.730">def accept(
    self,
    node: Expression,
    type_context: Type | None = None,
    allow_none_return: bool = False,
    always_allow_any: bool = False,
    is_callee: bool = False,
) -&gt; Type:
    """Type check a node in the given type context.  If allow_none_return
    is True and this expression is a call, allow it to return None.  This
    applies only to this expression and not any subexpressions.
    """
    if node in self.type_overrides:
        return self.type_overrides[node]
    self.type_context.append(type_context)
    old_is_callee = self.is_callee
    self.is_callee = is_callee
    try:
        if allow_none_return and isinstance(node, CallExpr):
            typ = self.visit_call_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, YieldFromExpr):
            typ = self.visit_yield_from_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, ConditionalExpr):
            typ = self.visit_conditional_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, AwaitExpr):
            typ = self.visit_await_expr(node, allow_none_return=True)
        else:
            typ = node.accept(self)
    except Exception as err:
        report_internal_error(
            err, self.chk.errors.file, node.line, self.chk.errors, self.chk.options
        )
    self.is_callee = old_is_callee
    self.type_context.pop()
    assert typ is not None
    self.chk.store_type(node, typ)

    if (
        self.chk.options.disallow_any_expr
        and not always_allow_any
        and not self.chk.is_stub
        and self.chk.in_checked_function()
        and has_any_type(typ)
        and not self.chk.current_node_deferred
    ):
        self.msg.disallowed_any_type(typ, node)

    if not self.chk.in_checked_function() or self.chk.current_node_deferred:
        return AnyType(TypeOfAny.unannotated)
    else:
        return typ

</t>
<t tx="ekr.20221004064034.731">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with type given by the name and no type
    arguments. Alias for TypeChecker.named_type.
    """
    return self.chk.named_type(name)

</t>
<t tx="ekr.20221004064034.732">def is_valid_var_arg(self, typ: Type) -&gt; bool:
    """Is a type valid as a *args argument?"""
    typ = get_proper_type(typ)
    return (
        isinstance(typ, TupleType)
        or is_subtype(
            typ,
            self.chk.named_generic_type("typing.Iterable", [AnyType(TypeOfAny.special_form)]),
        )
        or isinstance(typ, AnyType)
        or isinstance(typ, ParamSpecType)
    )

</t>
<t tx="ekr.20221004064034.733">def is_valid_keyword_var_arg(self, typ: Type) -&gt; bool:
    """Is a type valid as a **kwargs argument?"""
    return (
        is_subtype(
            typ,
            self.chk.named_generic_type(
                "typing.Mapping",
                [self.named_type("builtins.str"), AnyType(TypeOfAny.special_form)],
            ),
        )
        or is_subtype(
            typ,
            self.chk.named_generic_type(
                "typing.Mapping", [UninhabitedType(), UninhabitedType()]
            ),
        )
        or isinstance(typ, ParamSpecType)
    )

</t>
<t tx="ekr.20221004064034.734">def has_member(self, typ: Type, member: str) -&gt; bool:
    """Does type have member with the given name?"""
    # TODO: refactor this to use checkmember.analyze_member_access, otherwise
    # these two should be carefully kept in sync.
    # This is much faster than analyze_member_access, though, and so using
    # it first as a filter is important for performance.
    typ = get_proper_type(typ)

    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = tuple_fallback(typ)
    if isinstance(typ, LiteralType):
        typ = typ.fallback
    if isinstance(typ, Instance):
        return typ.type.has_readable_member(member)
    if isinstance(typ, CallableType) and typ.is_type_obj():
        return typ.fallback.type.has_readable_member(member)
    elif isinstance(typ, AnyType):
        return True
    elif isinstance(typ, UnionType):
        result = all(self.has_member(x, member) for x in typ.relevant_items())
        return result
    elif isinstance(typ, TypeType):
        # Type[Union[X, ...]] is always normalized to Union[Type[X], ...],
        # so we don't need to care about unions here.
        item = typ.item
        if isinstance(item, TypeVarType):
            item = get_proper_type(item.upper_bound)
        if isinstance(item, TupleType):
            item = tuple_fallback(item)
        if isinstance(item, Instance) and item.type.metaclass_type is not None:
            return self.has_member(item.type.metaclass_type, member)
        if isinstance(item, AnyType):
            return True
        return False
    else:
        return False

</t>
<t tx="ekr.20221004064034.735">def not_ready_callback(self, name: str, context: Context) -&gt; None:
    """Called when we can't infer the type of a variable because it's not ready yet.

    Either defer type checking of the enclosing function to the next
    pass or report an error.
    """
    self.chk.handle_cannot_determine_type(name, context)

</t>
<t tx="ekr.20221004064034.736">def visit_yield_expr(self, e: YieldExpr) -&gt; Type:
    return_type = self.chk.return_types[-1]
    expected_item_type = self.chk.get_generator_yield_type(return_type, False)
    if e.expr is None:
        if (
            not isinstance(get_proper_type(expected_item_type), (NoneType, AnyType))
            and self.chk.in_checked_function()
        ):
            self.chk.fail(message_registry.YIELD_VALUE_EXPECTED, e)
    else:
        actual_item_type = self.accept(e.expr, expected_item_type)
        self.chk.check_subtype(
            actual_item_type,
            expected_item_type,
            e,
            message_registry.INCOMPATIBLE_TYPES_IN_YIELD,
            "actual type",
            "expected type",
        )
    return self.chk.get_generator_receive_type(return_type, False)

</t>
<t tx="ekr.20221004064034.737">def visit_await_expr(self, e: AwaitExpr, allow_none_return: bool = False) -&gt; Type:
    expected_type = self.type_context[-1]
    if expected_type is not None:
        expected_type = self.chk.named_generic_type("typing.Awaitable", [expected_type])
    actual_type = get_proper_type(self.accept(e.expr, expected_type))
    if isinstance(actual_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=actual_type)
    ret = self.check_awaitable_expr(
        actual_type, e, message_registry.INCOMPATIBLE_TYPES_IN_AWAIT
    )
    if not allow_none_return and isinstance(get_proper_type(ret), NoneType):
        self.chk.msg.does_not_return_value(None, e)
    return ret

</t>
<t tx="ekr.20221004064034.738">def check_awaitable_expr(
    self, t: Type, ctx: Context, msg: str | ErrorMessage, ignore_binder: bool = False
) -&gt; Type:
    """Check the argument to `await` and extract the type of value.

    Also used by `async for` and `async with`.
    """
    if not self.chk.check_subtype(
        t, self.named_type("typing.Awaitable"), ctx, msg, "actual type", "expected type"
    ):
        return AnyType(TypeOfAny.special_form)
    else:
        generator = self.check_method_call_by_name("__await__", t, [], [], ctx)[0]
        ret_type = self.chk.get_generator_return_type(generator, False)
        ret_type = get_proper_type(ret_type)
        if (
            not ignore_binder
            and isinstance(ret_type, UninhabitedType)
            and not ret_type.ambiguous
        ):
            self.chk.binder.unreachable()
        return ret_type

</t>
<t tx="ekr.20221004064034.739">def visit_yield_from_expr(self, e: YieldFromExpr, allow_none_return: bool = False) -&gt; Type:
    # NOTE: Whether `yield from` accepts an `async def` decorated
    # with `@types.coroutine` (or `@asyncio.coroutine`) depends on
    # whether the generator containing the `yield from` is itself
    # thus decorated.  But it accepts a generator regardless of
    # how it's decorated.
    return_type = self.chk.return_types[-1]
    # TODO: What should the context for the sub-expression be?
    # If the containing function has type Generator[X, Y, ...],
    # the context should be Generator[X, Y, T], where T is the
    # context of the 'yield from' itself (but it isn't known).
    subexpr_type = get_proper_type(self.accept(e.expr))

    # Check that the expr is an instance of Iterable and get the type of the iterator produced
    # by __iter__.
    if isinstance(subexpr_type, AnyType):
        iter_type: Type = AnyType(TypeOfAny.from_another_any, source_any=subexpr_type)
    elif self.chk.type_is_iterable(subexpr_type):
        if is_async_def(subexpr_type) and not has_coroutine_decorator(return_type):
            self.chk.msg.yield_from_invalid_operand_type(subexpr_type, e)

        any_type = AnyType(TypeOfAny.special_form)
        generic_generator_type = self.chk.named_generic_type(
            "typing.Generator", [any_type, any_type, any_type]
        )
        iter_type, _ = self.check_method_call_by_name(
            "__iter__", subexpr_type, [], [], context=generic_generator_type
        )
    else:
        if not (is_async_def(subexpr_type) and has_coroutine_decorator(return_type)):
            self.chk.msg.yield_from_invalid_operand_type(subexpr_type, e)
            iter_type = AnyType(TypeOfAny.from_error)
        else:
            iter_type = self.check_awaitable_expr(
                subexpr_type, e, message_registry.INCOMPATIBLE_TYPES_IN_YIELD_FROM
            )

    # Check that the iterator's item type matches the type yielded by the Generator function
    # containing this `yield from` expression.
    expected_item_type = self.chk.get_generator_yield_type(return_type, False)
    actual_item_type = self.chk.get_generator_yield_type(iter_type, False)

    self.chk.check_subtype(
        actual_item_type,
        expected_item_type,
        e,
        message_registry.INCOMPATIBLE_TYPES_IN_YIELD_FROM,
        "actual type",
        "expected type",
    )

    # Determine the type of the entire yield from expression.
    iter_type = get_proper_type(iter_type)
    if isinstance(iter_type, Instance) and iter_type.type.fullname == "typing.Generator":
        expr_type = self.chk.get_generator_return_type(iter_type, False)
    else:
        # Non-Generators don't return anything from `yield from` expressions.
        # However special-case Any (which might be produced by an error).
        actual_item_type = get_proper_type(actual_item_type)
        if isinstance(actual_item_type, AnyType):
            expr_type = AnyType(TypeOfAny.from_another_any, source_any=actual_item_type)
        else:
            # Treat `Iterator[X]` as a shorthand for `Generator[X, None, Any]`.
            expr_type = NoneType()

    if not allow_none_return and isinstance(get_proper_type(expr_type), NoneType):
        self.chk.msg.does_not_return_value(None, e)
    return expr_type

</t>
<t tx="ekr.20221004064034.74">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--verbose", action="store_true", default=False, help="Increase verbosity")
    parser.add_argument("--sqlite", action="store_true", default=False, help="Use a sqlite cache")
    parser.add_argument("input_dir1", help="Input directory for the cache")
    parser.add_argument("input_dir2", help="Input directory for the cache")
    parser.add_argument("output", help="Output file")
    args = parser.parse_args()

    cache1 = make_cache(args.input_dir1, args.sqlite)
    cache2 = make_cache(args.input_dir2, args.sqlite)

    type_misses: dict[str, int] = defaultdict(int)
    type_hits: dict[str, int] = defaultdict(int)

    updates: dict[str, str | None] = {}

    deps1: dict[str, set[str]] = {}
    deps2: dict[str, set[str]] = {}

    misses = hits = 0
    cache1_all = list(cache1.list_all())
    for s in cache1_all:
        obj1 = load(cache1, s)
        try:
            obj2 = load(cache2, s)
        except FileNotFoundError:
            obj2 = None

        typ = s.split(".")[-2]
        if obj1 != obj2:
            misses += 1
            type_misses[typ] += 1

            # Collect the dependencies instead of including them directly in the diff
            # so we can produce a much smaller direct diff of them.
            if ".deps." not in s:
                if obj2 is not None:
                    updates[s] = json.dumps(obj2)
                else:
                    updates[s] = None
            elif obj2:
                merge_deps(deps1, obj1)
                merge_deps(deps2, obj2)
        else:
            hits += 1
            type_hits[typ] += 1

    cache1_all_set = set(cache1_all)
    for s in cache2.list_all():
        if s not in cache1_all_set:
            updates[s] = cache2.read(s)

    # Compute what deps have been added and merge them all into the
    # @root deps file.
    new_deps = {k: deps1.get(k, set()) - deps2.get(k, set()) for k in deps2}
    new_deps = {k: v for k, v in new_deps.items() if v}
    try:
        root_deps = load(cache1, "@root.deps.json")
    except FileNotFoundError:
        root_deps = {}
    merge_deps(new_deps, root_deps)

    new_deps_json = {k: list(v) for k, v in new_deps.items() if v}
    updates["@root.deps.json"] = json.dumps(new_deps_json)

    # Drop updates to deps.meta.json for size reasons. The diff
    # applier will manually fix it up.
    updates.pop("./@deps.meta.json", None)
    updates.pop("@deps.meta.json", None)

    ###

    print("Generated incremental cache:", hits, "hits,", misses, "misses")
    if args.verbose:
        print("hits", type_hits)
        print("misses", type_misses)

    with open(args.output, "w") as f:
        json.dump(updates, f)


</t>
<t tx="ekr.20221004064034.740">def visit_temp_node(self, e: TempNode) -&gt; Type:
    return e.type

</t>
<t tx="ekr.20221004064034.741">def visit_type_var_expr(self, e: TypeVarExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.742">def visit_paramspec_expr(self, e: ParamSpecExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.743">def visit_type_var_tuple_expr(self, e: TypeVarTupleExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.744">def visit_newtype_expr(self, e: NewTypeExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.745">def visit_namedtuple_expr(self, e: NamedTupleExpr) -&gt; Type:
    tuple_type = e.info.tuple_type
    if tuple_type:
        if self.chk.options.disallow_any_unimported and has_any_from_unimported_type(
            tuple_type
        ):
            self.msg.unimported_type_becomes_any("NamedTuple type", tuple_type, e)
        check_for_explicit_any(
            tuple_type, self.chk.options, self.chk.is_typeshed_stub, self.msg, context=e
        )
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.746">def visit_enum_call_expr(self, e: EnumCallExpr) -&gt; Type:
    for name, value in zip(e.items, e.values):
        if value is not None:
            typ = self.accept(value)
            if not isinstance(get_proper_type(typ), AnyType):
                var = e.info.names[name].node
                if isinstance(var, Var):
                    # Inline TypeChecker.set_inferred_type(),
                    # without the lvalue.  (This doesn't really do
                    # much, since the value attribute is defined
                    # to have type Any in the typeshed stub.)
                    var.type = typ
                    var.is_inferred = True
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.747">def visit_typeddict_expr(self, e: TypedDictExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064034.748">def visit__promote_expr(self, e: PromoteExpr) -&gt; Type:
    return e.type

</t>
<t tx="ekr.20221004064034.749">def visit_star_expr(self, e: StarExpr) -&gt; StarType:
    return StarType(self.accept(e.expr))

</t>
<t tx="ekr.20221004064034.75">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
"""
Parse source files and print the abstract syntax trees.
"""

from __future__ import annotations

import argparse
import sys

from mypy import defaults
from mypy.errors import CompileError
from mypy.options import Options
from mypy.parse import parse


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.750">def object_type(self) -&gt; Instance:
    """Return instance type 'object'."""
    return self.named_type("builtins.object")

</t>
<t tx="ekr.20221004064034.751">def bool_type(self) -&gt; Instance:
    """Return instance type 'bool'."""
    return self.named_type("builtins.bool")

</t>
<t tx="ekr.20221004064034.752">@overload
def narrow_type_from_binder(self, expr: Expression, known_type: Type) -&gt; Type:
    ...

</t>
<t tx="ekr.20221004064034.753">@overload
def narrow_type_from_binder(
    self, expr: Expression, known_type: Type, skip_non_overlapping: bool
) -&gt; Type | None:
    ...

</t>
<t tx="ekr.20221004064034.754">def narrow_type_from_binder(
    self, expr: Expression, known_type: Type, skip_non_overlapping: bool = False
) -&gt; Type | None:
    """Narrow down a known type of expression using information in conditional type binder.

    If 'skip_non_overlapping' is True, return None if the type and restriction are
    non-overlapping.
    """
    if literal(expr) &gt;= LITERAL_TYPE:
        restriction = self.chk.binder.get(expr)
        # If the current node is deferred, some variables may get Any types that they
        # otherwise wouldn't have. We don't want to narrow down these since it may
        # produce invalid inferred Optional[Any] types, at least.
        if restriction and not (
            isinstance(get_proper_type(known_type), AnyType) and self.chk.current_node_deferred
        ):
            # Note: this call should match the one in narrow_declared_type().
            if skip_non_overlapping and not is_overlapping_types(
                known_type, restriction, prohibit_none_typevar_overlap=True
            ):
                return None
            return narrow_declared_type(known_type, restriction)
    return known_type


</t>
<t tx="ekr.20221004064034.755">def has_any_type(t: Type, ignore_in_type_obj: bool = False) -&gt; bool:
    """Whether t contains an Any type"""
    return t.accept(HasAnyType(ignore_in_type_obj))


</t>
<t tx="ekr.20221004064034.756">class HasAnyType(types.TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20221004064034.757">def __init__(self, ignore_in_type_obj: bool) -&gt; None:
    super().__init__(any)
    self.ignore_in_type_obj = ignore_in_type_obj

</t>
<t tx="ekr.20221004064034.758">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any != TypeOfAny.special_form  # special forms are not real Any types

</t>
<t tx="ekr.20221004064034.759">def visit_callable_type(self, t: CallableType) -&gt; bool:
    if self.ignore_in_type_obj and t.is_type_obj():
        return False
    return super().visit_callable_type(t)


</t>
<t tx="ekr.20221004064034.76">def dump(fname: str, python_version: tuple[int, int], quiet: bool = False) -&gt; None:
    options = Options()
    options.python_version = python_version
    with open(fname, "rb") as f:
        s = f.read()
        tree = parse(s, fname, None, errors=None, options=options)
        if not quiet:
            print(tree)


</t>
<t tx="ekr.20221004064034.760">def has_coroutine_decorator(t: Type) -&gt; bool:
    """Whether t came from a function decorated with `@coroutine`."""
    t = get_proper_type(t)
    return isinstance(t, Instance) and t.type.fullname == "typing.AwaitableGenerator"


</t>
<t tx="ekr.20221004064034.761">def is_async_def(t: Type) -&gt; bool:
    """Whether t came from a function defined using `async def`."""
    # In check_func_def(), when we see a function decorated with
    # `@typing.coroutine` or `@async.coroutine`, we change the
    # return type to typing.AwaitableGenerator[...], so that its
    # type is compatible with either Generator or Awaitable.
    # But for the check here we need to know whether the original
    # function (before decoration) was an `async def`.  The
    # AwaitableGenerator type conveniently preserves the original
    # type as its 4th parameter (3rd when using 0-origin indexing
    # :-), so that we can recover that information here.
    # (We really need to see whether the original, undecorated
    # function was an `async def`, which is orthogonal to its
    # decorations.)
    t = get_proper_type(t)
    if (
        isinstance(t, Instance)
        and t.type.fullname == "typing.AwaitableGenerator"
        and len(t.args) &gt;= 4
    ):
        t = get_proper_type(t.args[3])
    return isinstance(t, Instance) and t.type.fullname == "typing.Coroutine"


</t>
<t tx="ekr.20221004064034.762">def is_non_empty_tuple(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, TupleType) and bool(t.items)


</t>
<t tx="ekr.20221004064034.763">def is_duplicate_mapping(
    mapping: list[int], actual_types: list[Type], actual_kinds: list[ArgKind]
) -&gt; bool:
    return (
        len(mapping) &gt; 1
        # Multiple actuals can map to the same formal if they both come from
        # varargs (*args and **kwargs); in this case at runtime it is possible
        # that here are no duplicates. We need to allow this, as the convention
        # f(..., *args, **kwargs) is common enough.
        and not (
            len(mapping) == 2
            and actual_kinds[mapping[0]] == nodes.ARG_STAR
            and actual_kinds[mapping[1]] == nodes.ARG_STAR2
        )
        # Multiple actuals can map to the same formal if there are multiple
        # **kwargs which cannot be mapped with certainty (non-TypedDict
        # **kwargs).
        and not all(
            actual_kinds[m] == nodes.ARG_STAR2
            and not isinstance(get_proper_type(actual_types[m]), TypedDictType)
            for m in mapping
        )
    )


</t>
<t tx="ekr.20221004064034.764">def replace_callable_return_type(c: CallableType, new_ret_type: Type) -&gt; CallableType:
    """Return a copy of a callable type with a different return type."""
    return c.copy_modified(ret_type=new_ret_type)


</t>
<t tx="ekr.20221004064034.765">class ArgInferSecondPassQuery(types.TypeQuery[bool]):
    """Query whether an argument type should be inferred in the second pass.

    The result is True if the type has a type variable in a callable return
    type anywhere. For example, the result for Callable[[], T] is True if t is
    a type variable.
    """

    @others
</t>
<t tx="ekr.20221004064034.766">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20221004064034.767">def visit_callable_type(self, t: CallableType) -&gt; bool:
    return self.query_types(t.arg_types) or t.accept(HasTypeVarQuery())


</t>
<t tx="ekr.20221004064034.768">class HasTypeVarQuery(types.TypeQuery[bool]):
    """Visitor for querying whether a type has a type variable component."""

    @others
</t>
<t tx="ekr.20221004064034.769">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20221004064034.77">def main() -&gt; None:
    # Parse a file and dump the AST (or display errors).
    parser = argparse.ArgumentParser(
        description="Parse source files and print the abstract syntax tree (AST)."
    )
    parser.add_argument("--quiet", action="store_true", help="do not print AST")
    parser.add_argument("FILE", nargs="*", help="files to parse")
    args = parser.parse_args()

    status = 0
    for fname in args.FILE:
        try:
            dump(fname, defaults.PYTHON3_VERSION, args.quiet)
        except CompileError as e:
            for msg in e.messages:
                sys.stderr.write("%s\n" % msg)
            status = 1
    sys.exit(status)


</t>
<t tx="ekr.20221004064034.770">def visit_type_var(self, t: TypeVarType) -&gt; bool:
    return True


</t>
<t tx="ekr.20221004064034.771">def has_erased_component(t: Type | None) -&gt; bool:
    return t is not None and t.accept(HasErasedComponentsQuery())


</t>
<t tx="ekr.20221004064034.772">class HasErasedComponentsQuery(types.TypeQuery[bool]):
    """Visitor for querying whether a type has an erased component."""

    @others
</t>
<t tx="ekr.20221004064034.773">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20221004064034.774">def visit_erased_type(self, t: ErasedType) -&gt; bool:
    return True


</t>
<t tx="ekr.20221004064034.775">def has_uninhabited_component(t: Type | None) -&gt; bool:
    return t is not None and t.accept(HasUninhabitedComponentsQuery())


</t>
<t tx="ekr.20221004064034.776">class HasUninhabitedComponentsQuery(types.TypeQuery[bool]):
    """Visitor for querying whether a type has an UninhabitedType component."""

    @others
</t>
<t tx="ekr.20221004064034.777">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20221004064034.778">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return True


</t>
<t tx="ekr.20221004064034.779">def arg_approximate_similarity(actual: Type, formal: Type) -&gt; bool:
    """Return if caller argument (actual) is roughly compatible with signature arg (formal).

    This function is deliberately loose and will report two types are similar
    as long as their "shapes" are plausibly the same.

    This is useful when we're doing error reporting: for example, if we're trying
    to select an overload alternative and there's no exact match, we can use
    this function to help us identify which alternative the user might have
    *meant* to match.
    """
    actual = get_proper_type(actual)
    formal = get_proper_type(formal)

    # Erase typevars: we'll consider them all to have the same "shape".
    if isinstance(actual, TypeVarType):
        actual = erase_to_union_or_bound(actual)
    if isinstance(formal, TypeVarType):
        formal = erase_to_union_or_bound(formal)

    @others
    if isinstance(formal, CallableType):
        if isinstance(actual, (CallableType, Overloaded, TypeType)):
            return True
    if is_typetype_like(actual) and is_typetype_like(formal):
        return True

    # Unions
    if isinstance(actual, UnionType):
        return any(arg_approximate_similarity(item, formal) for item in actual.relevant_items())
    if isinstance(formal, UnionType):
        return any(arg_approximate_similarity(actual, item) for item in formal.relevant_items())

    # TypedDicts
    if isinstance(actual, TypedDictType):
        if isinstance(formal, TypedDictType):
            return True
        return arg_approximate_similarity(actual.fallback, formal)

    # Instances
    # For instances, we mostly defer to the existing is_subtype check.
    if isinstance(formal, Instance):
        if isinstance(actual, CallableType):
            actual = actual.fallback
        if isinstance(actual, Overloaded):
            actual = actual.items[0].fallback
        if isinstance(actual, TupleType):
            actual = tuple_fallback(actual)
        if isinstance(actual, Instance) and formal.type in actual.type.mro:
            # Try performing a quick check as an optimization
            return True

    # Fall back to a standard subtype check for the remaining kinds of type.
    return is_subtype(erasetype.erase_type(actual), erasetype.erase_type(formal))


</t>
<t tx="ekr.20221004064034.78">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
# Usage: find_type.py FILENAME START_LINE START_COL END_LINE END_COL MYPY_AND_ARGS
# Prints out the type of the expression in the given location if the mypy run
# succeeds cleanly.  Otherwise, prints out the errors encountered.
# Note: this only works on expressions, and not assignment targets.
# Note: MYPY_AND_ARGS is should be the remainder of argv, not a single
# spaces-included argument.
# NOTE: Line numbers are 1-based; column numbers are 0-based.
#
#
# Example vim usage:
# function RevealType()
#   " Set this to the command you use to run mypy on your project.  Include the mypy invocation.
#   let mypycmd = 'python3 -m mypy mypy --incremental'
#   let [startline, startcol] = getpos("'&lt;")[1:2]
#   let [endline, endcol] = getpos("'&gt;")[1:2]
#   " Convert to 0-based column offsets
#   let startcol = startcol - 1
#   " Change this line to point to the find_type.py script.
#   execute '!python3 /path/to/mypy/misc/find_type.py % ' . startline . ' ' . startcol . ' ' . endline . ' ' . endcol . ' ' . mypycmd
# endfunction
# vnoremap &lt;Leader&gt;t :call RevealType()&lt;CR&gt;
#
# For an Emacs example, see misc/macs.el.

from __future__ import annotations

import os.path
import re
import subprocess
import sys
import tempfile

REVEAL_TYPE_START = "reveal_type("
REVEAL_TYPE_END = ")"


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.780"># Callable or Type[...]-ish types
def is_typetype_like(typ: ProperType) -&gt; bool:
    return (
        isinstance(typ, TypeType)
        or (isinstance(typ, FunctionLike) and typ.is_type_obj())
        or (isinstance(typ, Instance) and typ.type.fullname == "builtins.type")
    )

</t>
<t tx="ekr.20221004064034.781">def any_causes_overload_ambiguity(
    items: list[CallableType],
    return_types: list[Type],
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None] | None,
) -&gt; bool:
    """May an argument containing 'Any' cause ambiguous result type on call to overloaded function?

    Note that this sometimes returns True even if there is no ambiguity, since a correct
    implementation would be complex (and the call would be imprecisely typed due to Any
    types anyway).

    Args:
        items: Overload items matching the actual arguments
        arg_types: Actual argument types
        arg_kinds: Actual argument kinds
        arg_names: Actual argument names
    """
    if all_same_types(return_types):
        return False

    actual_to_formal = [
        map_formals_to_actuals(
            arg_kinds, arg_names, item.arg_kinds, item.arg_names, lambda i: arg_types[i]
        )
        for item in items
    ]

    for arg_idx, arg_type in enumerate(arg_types):
        # We ignore Anys in type object callables as ambiguity
        # creators, since that can lead to falsely claiming ambiguity
        # for overloads between Type and Callable.
        if has_any_type(arg_type, ignore_in_type_obj=True):
            matching_formals_unfiltered = [
                (item_idx, lookup[arg_idx])
                for item_idx, lookup in enumerate(actual_to_formal)
                if lookup[arg_idx]
            ]

            matching_returns = []
            matching_formals = []
            for item_idx, formals in matching_formals_unfiltered:
                matched_callable = items[item_idx]
                matching_returns.append(matched_callable.ret_type)

                # Note: if an actual maps to multiple formals of differing types within
                # a single callable, then we know at least one of those formals must be
                # a different type then the formal(s) in some other callable.
                # So it's safe to just append everything to the same list.
                for formal in formals:
                    matching_formals.append(matched_callable.arg_types[formal])
            if not all_same_types(matching_formals) and not all_same_types(matching_returns):
                # Any maps to multiple different types, and the return types of these items differ.
                return True
    return False


</t>
<t tx="ekr.20221004064034.782">def all_same_types(types: list[Type]) -&gt; bool:
    if len(types) == 0:
        return True
    return all(is_same_type(t, types[0]) for t in types[1:])


</t>
<t tx="ekr.20221004064034.783">def merge_typevars_in_callables_by_name(
    callables: Sequence[CallableType],
) -&gt; tuple[list[CallableType], list[TypeVarType]]:
    """Takes all the typevars present in the callables and 'combines' the ones with the same name.

    For example, suppose we have two callables with signatures "f(x: T, y: S) -&gt; T" and
    "f(x: List[Tuple[T, S]]) -&gt; Tuple[T, S]". Both callables use typevars named "T" and
    "S", but we treat them as distinct, unrelated typevars. (E.g. they could both have
    distinct ids.)

    If we pass in both callables into this function, it returns a list containing two
    new callables that are identical in signature, but use the same underlying TypeVarType
    for T and S.

    This is useful if we want to take the output lists and "merge" them into one callable
    in some way -- for example, when unioning together overloads.

    Returns both the new list of callables and a list of all distinct TypeVarType objects used.
    """
    output: list[CallableType] = []
    unique_typevars: dict[str, TypeVarType] = {}
    variables: list[TypeVarType] = []

    for target in callables:
        if target.is_generic():
            target = freshen_function_type_vars(target)

            rename = {}  # Dict[TypeVarId, TypeVar]
            for tv in target.variables:
                name = tv.fullname
                if name not in unique_typevars:
                    # TODO(PEP612): fix for ParamSpecType
                    if isinstance(tv, ParamSpecType):
                        continue
                    assert isinstance(tv, TypeVarType)
                    unique_typevars[name] = tv
                    variables.append(tv)
                rename[tv.id] = unique_typevars[name]

            target = cast(CallableType, expand_type(target, rename))
        output.append(target)

    return output, variables


</t>
<t tx="ekr.20221004064034.784">def try_getting_literal(typ: Type) -&gt; ProperType:
    """If possible, get a more precise literal type for a given type."""
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and typ.last_known_value is not None:
        return typ.last_known_value
    return typ


</t>
<t tx="ekr.20221004064034.785">def is_expr_literal_type(node: Expression) -&gt; bool:
    """Returns 'true' if the given node is a Literal"""
    if isinstance(node, IndexExpr):
        base = node.base
        return isinstance(base, RefExpr) and base.fullname in LITERAL_TYPE_NAMES
    if isinstance(node, NameExpr):
        underlying = node.node
        return isinstance(underlying, TypeAlias) and isinstance(
            get_proper_type(underlying.target), LiteralType
        )
    return False


</t>
<t tx="ekr.20221004064034.786">def has_bytes_component(typ: Type) -&gt; bool:
    """Is this one of builtin byte types, or a union that contains it?"""
    typ = get_proper_type(typ)
    byte_types = {"builtins.bytes", "builtins.bytearray"}
    if isinstance(typ, UnionType):
        return any(has_bytes_component(t) for t in typ.items)
    if isinstance(typ, Instance) and typ.type.fullname in byte_types:
        return True
    return False


</t>
<t tx="ekr.20221004064034.787">def type_info_from_type(typ: Type) -&gt; TypeInfo | None:
    """Gets the TypeInfo for a type, indirecting through things like type variables and tuples."""
    typ = get_proper_type(typ)
    if isinstance(typ, FunctionLike) and typ.is_type_obj():
        return typ.type_object()
    if isinstance(typ, TypeType):
        typ = typ.item
    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = tuple_fallback(typ)
    if isinstance(typ, Instance):
        return typ.type

    # A complicated type. Too tricky, give up.
    # TODO: Do something more clever here.
    return None


</t>
<t tx="ekr.20221004064034.788">def is_operator_method(fullname: str | None) -&gt; bool:
    if fullname is None:
        return False
    short_name = fullname.split(".")[-1]
    return (
        short_name in operators.op_methods.values()
        or short_name in operators.reverse_op_methods.values()
        or short_name in operators.unary_op_methods.values()
    )


</t>
<t tx="ekr.20221004064034.789">def get_partial_instance_type(t: Type | None) -&gt; PartialType | None:
    if t is None or not isinstance(t, PartialType) or t.type is None:
        return None
    return t
</t>
<t tx="ekr.20221004064034.79">def update_line(line: str, s: str, pos: int) -&gt; str:
    return line[:pos] + s + line[pos:]


</t>
<t tx="ekr.20221004064034.790">@path C:/Repos/ekr-mypy2/mypy/
"""Type checking of attribute access"""

from __future__ import annotations

from typing import TYPE_CHECKING, Callable, Sequence, cast

from mypy import meet, message_registry, subtypes
from mypy.erasetype import erase_typevars
from mypy.expandtype import expand_type_by_instance, freshen_function_type_vars
from mypy.maptype import map_instance_to_supertype
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    SYMBOL_FUNCBASE_TYPES,
    Context,
    Decorator,
    FuncBase,
    FuncDef,
    IndexExpr,
    MypyFile,
    OverloadedFuncDef,
    SymbolNode,
    SymbolTable,
    TempNode,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    Var,
    is_final_node,
)
from mypy.plugin import AttributeContext
from mypy.typeops import (
    bind_self,
    class_callable,
    erase_to_bound,
    function_type,
    make_simplified_union,
    tuple_fallback,
    type_object_type_from_function,
)
from mypy.types import (
    ENUM_REMOVED_PROPS,
    AnyType,
    CallableType,
    DeletedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnionType,
    get_proper_type,
    has_type_vars,
)

if TYPE_CHECKING:  # import for forward declaration only
    import mypy.checker

from mypy import state


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.791">class MemberContext:
    """Information and objects needed to type check attribute access.

    Look at the docstring of analyze_member_access for more information.
    """

    @others
</t>
<t tx="ekr.20221004064034.792">def __init__(
    self,
    is_lvalue: bool,
    is_super: bool,
    is_operator: bool,
    original_type: Type,
    context: Context,
    msg: MessageBuilder,
    chk: mypy.checker.TypeChecker,
    self_type: Type | None,
    module_symbol_table: SymbolTable | None = None,
    no_deferral: bool = False,
) -&gt; None:
    self.is_lvalue = is_lvalue
    self.is_super = is_super
    self.is_operator = is_operator
    self.original_type = original_type
    self.self_type = self_type or original_type
    self.context = context  # Error context
    self.msg = msg
    self.chk = chk
    self.module_symbol_table = module_symbol_table
    self.no_deferral = no_deferral

</t>
<t tx="ekr.20221004064034.793">def named_type(self, name: str) -&gt; Instance:
    return self.chk.named_type(name)

</t>
<t tx="ekr.20221004064034.794">def not_ready_callback(self, name: str, context: Context) -&gt; None:
    self.chk.handle_cannot_determine_type(name, context)

</t>
<t tx="ekr.20221004064034.795">def copy_modified(
    self,
    *,
    messages: MessageBuilder | None = None,
    self_type: Type | None = None,
    is_lvalue: bool | None = None,
) -&gt; MemberContext:
    mx = MemberContext(
        self.is_lvalue,
        self.is_super,
        self.is_operator,
        self.original_type,
        self.context,
        self.msg,
        self.chk,
        self.self_type,
        self.module_symbol_table,
        self.no_deferral,
    )
    if messages is not None:
        mx.msg = messages
    if self_type is not None:
        mx.self_type = self_type
    if is_lvalue is not None:
        mx.is_lvalue = is_lvalue
    return mx


</t>
<t tx="ekr.20221004064034.796">def analyze_member_access(
    name: str,
    typ: Type,
    context: Context,
    is_lvalue: bool,
    is_super: bool,
    is_operator: bool,
    msg: MessageBuilder,
    *,
    original_type: Type,
    chk: mypy.checker.TypeChecker,
    override_info: TypeInfo | None = None,
    in_literal_context: bool = False,
    self_type: Type | None = None,
    module_symbol_table: SymbolTable | None = None,
    no_deferral: bool = False,
) -&gt; Type:
    """Return the type of attribute 'name' of 'typ'.

    The actual implementation is in '_analyze_member_access' and this docstring
    also applies to it.

    This is a general operation that supports various different variations:

      1. lvalue or non-lvalue access (setter or getter access)
      2. supertype access when using super() (is_super == True and
         'override_info' should refer to the supertype)

    'original_type' is the most precise inferred or declared type of the base object
    that we have available. When looking for an attribute of 'typ', we may perform
    recursive calls targeting the fallback type, and 'typ' may become some supertype
    of 'original_type'. 'original_type' is always preserved as the 'typ' type used in
    the initial, non-recursive call. The 'self_type' is a component of 'original_type'
    to which generic self should be bound (a narrower type that has a fallback to instance).
    Currently this is used only for union types.

    'module_symbol_table' is passed to this function if 'typ' is actually a module
    and we want to keep track of the available attributes of the module (since they
    are not available via the type object directly)
    """
    mx = MemberContext(
        is_lvalue,
        is_super,
        is_operator,
        original_type,
        context,
        msg,
        chk=chk,
        self_type=self_type,
        module_symbol_table=module_symbol_table,
        no_deferral=no_deferral,
    )
    result = _analyze_member_access(name, typ, mx, override_info)
    possible_literal = get_proper_type(result)
    if (
        in_literal_context
        and isinstance(possible_literal, Instance)
        and possible_literal.last_known_value is not None
    ):
        return possible_literal.last_known_value
    else:
        return result


</t>
<t tx="ekr.20221004064034.797">def _analyze_member_access(
    name: str, typ: Type, mx: MemberContext, override_info: TypeInfo | None = None
) -&gt; Type:
    # TODO: This and following functions share some logic with subtypes.find_member;
    #       consider refactoring.
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return analyze_instance_member_access(name, typ, mx, override_info)
    elif isinstance(typ, AnyType):
        # The base object has dynamic type.
        return AnyType(TypeOfAny.from_another_any, source_any=typ)
    elif isinstance(typ, UnionType):
        return analyze_union_member_access(name, typ, mx)
    elif isinstance(typ, FunctionLike) and typ.is_type_obj():
        return analyze_type_callable_member_access(name, typ, mx)
    elif isinstance(typ, TypeType):
        return analyze_type_type_member_access(name, typ, mx, override_info)
    elif isinstance(typ, TupleType):
        # Actually look up from the fallback instance type.
        return _analyze_member_access(name, tuple_fallback(typ), mx, override_info)
    elif isinstance(typ, (LiteralType, FunctionLike)):
        # Actually look up from the fallback instance type.
        return _analyze_member_access(name, typ.fallback, mx, override_info)
    elif isinstance(typ, TypedDictType):
        return analyze_typeddict_access(name, typ, mx, override_info)
    elif isinstance(typ, NoneType):
        return analyze_none_member_access(name, typ, mx)
    elif isinstance(typ, TypeVarLikeType):
        if isinstance(typ, TypeVarType) and typ.values:
            return _analyze_member_access(
                name, make_simplified_union(typ.values), mx, override_info
            )
        return _analyze_member_access(name, typ.upper_bound, mx, override_info)
    elif isinstance(typ, DeletedType):
        mx.msg.deleted_as_rvalue(typ, mx.context)
        return AnyType(TypeOfAny.from_error)
    return report_missing_attribute(mx.original_type, typ, name, mx)


</t>
<t tx="ekr.20221004064034.798">def may_be_awaitable_attribute(
    name: str, typ: Type, mx: MemberContext, override_info: TypeInfo | None = None
) -&gt; bool:
    """Check if the given type has the attribute when awaited."""
    if mx.chk.checking_missing_await:
        # Avoid infinite recursion.
        return False
    with mx.chk.checking_await_set(), mx.msg.filter_errors() as local_errors:
        aw_type = mx.chk.get_precise_awaitable_type(typ, local_errors)
        if aw_type is None:
            return False
        _ = _analyze_member_access(name, aw_type, mx, override_info)
        return not local_errors.has_new_errors()


</t>
<t tx="ekr.20221004064034.799">def report_missing_attribute(
    original_type: Type,
    typ: Type,
    name: str,
    mx: MemberContext,
    override_info: TypeInfo | None = None,
) -&gt; Type:
    res_type = mx.msg.has_no_attr(original_type, typ, name, mx.context, mx.module_symbol_table)
    if may_be_awaitable_attribute(name, typ, mx, override_info):
        mx.msg.possible_missing_await(mx.context)
    return res_type


</t>
<t tx="ekr.20221004064034.8"># Passes with legacy mypy.
def ekr_f_annotated_initialized(ekr_a: str="abc") -&gt; None:
    pass

</t>
<t tx="ekr.20221004064034.80">def run_mypy(mypy_and_args: list[str], filename: str, tmp_name: str) -&gt; str:
    proc = subprocess.run(
        mypy_and_args + ["--shadow-file", filename, tmp_name], stdout=subprocess.PIPE
    )
    assert isinstance(
        proc.stdout, bytes
    )  # Guaranteed to be true because we called run with universal_newlines=False
    return proc.stdout.decode(encoding="utf-8")


</t>
<t tx="ekr.20221004064034.800"># The several functions that follow implement analyze_member_access for various
# types and aren't documented individually.


</t>
<t tx="ekr.20221004064034.801">def analyze_instance_member_access(
    name: str, typ: Instance, mx: MemberContext, override_info: TypeInfo | None
) -&gt; Type:
    if name == "__init__" and not mx.is_super:
        # Accessing __init__ in statically typed code would compromise
        # type safety unless used via super().
        mx.msg.fail(message_registry.CANNOT_ACCESS_INIT, mx.context)
        return AnyType(TypeOfAny.from_error)

    # The base object has an instance type.

    info = typ.type
    if override_info:
        info = override_info

    if (
        state.find_occurrences
        and info.name == state.find_occurrences[0]
        and name == state.find_occurrences[1]
    ):
        mx.msg.note("Occurrence of '{}.{}'".format(*state.find_occurrences), mx.context)

    # Look up the member. First look up the method dictionary.
    method = info.get_method(name)
    if method and not isinstance(method, Decorator):
        if mx.is_super:
            validate_super_call(method, mx)

        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            first_item = cast(Decorator, method.items[0])
            return analyze_var(name, first_item.var, typ, info, mx)
        if mx.is_lvalue:
            mx.msg.cant_assign_to_method(mx.context)
        signature = function_type(method, mx.named_type("builtins.function"))
        signature = freshen_function_type_vars(signature)
        if name == "__new__" or method.is_static:
            # __new__ is special and behaves like a static method -- don't strip
            # the first argument.
            pass
        else:
            if name != "__call__":
                # TODO: use proper treatment of special methods on unions instead
                #       of this hack here and below (i.e. mx.self_type).
                dispatched_type = meet.meet_types(mx.original_type, typ)
                signature = check_self_arg(
                    signature, dispatched_type, method.is_class, mx.context, name, mx.msg
                )
            signature = bind_self(signature, mx.self_type, is_classmethod=method.is_class)
        # TODO: should we skip these steps for static methods as well?
        # Since generic static methods should not be allowed.
        typ = map_instance_to_supertype(typ, method.info)
        member_type = expand_type_by_instance(signature, typ)
        freeze_type_vars(member_type)
        return member_type
    else:
        # Not a method.
        return analyze_member_var_access(name, typ, info, mx)


</t>
<t tx="ekr.20221004064034.802">def validate_super_call(node: FuncBase, mx: MemberContext) -&gt; None:
    unsafe_super = False
    if isinstance(node, FuncDef) and node.is_trivial_body:
        unsafe_super = True
        impl = node
    elif isinstance(node, OverloadedFuncDef):
        if node.impl:
            impl = node.impl if isinstance(node.impl, FuncDef) else node.impl.func
            unsafe_super = impl.is_trivial_body
    if unsafe_super:
        ret_type = (
            impl.type.ret_type
            if isinstance(impl.type, CallableType)
            else AnyType(TypeOfAny.unannotated)
        )
        if not subtypes.is_subtype(NoneType(), ret_type):
            mx.msg.unsafe_super(node.name, node.info.name, mx.context)


</t>
<t tx="ekr.20221004064034.803">def analyze_type_callable_member_access(name: str, typ: FunctionLike, mx: MemberContext) -&gt; Type:
    # Class attribute.
    # TODO super?
    ret_type = typ.items[0].ret_type
    assert isinstance(ret_type, ProperType)
    if isinstance(ret_type, TupleType):
        ret_type = tuple_fallback(ret_type)
    if isinstance(ret_type, TypedDictType):
        ret_type = ret_type.fallback
    if isinstance(ret_type, Instance):
        if not mx.is_operator:
            # When Python sees an operator (eg `3 == 4`), it automatically translates that
            # into something like `int.__eq__(3, 4)` instead of `(3).__eq__(4)` as an
            # optimization.
            #
            # While it normally it doesn't matter which of the two versions are used, it
            # does cause inconsistencies when working with classes. For example, translating
            # `int == int` to `int.__eq__(int)` would not work since `int.__eq__` is meant to
            # compare two int _instances_. What we really want is `type(int).__eq__`, which
            # is meant to compare two types or classes.
            #
            # This check makes sure that when we encounter an operator, we skip looking up
            # the corresponding method in the current instance to avoid this edge case.
            # See https://github.com/python/mypy/pull/1787 for more info.
            # TODO: do not rely on same type variables being present in all constructor overloads.
            result = analyze_class_attribute_access(
                ret_type, name, mx, original_vars=typ.items[0].variables
            )
            if result:
                return result
        # Look up from the 'type' type.
        return _analyze_member_access(name, typ.fallback, mx)
    else:
        assert False, f"Unexpected type {ret_type!r}"


</t>
<t tx="ekr.20221004064034.804">def analyze_type_type_member_access(
    name: str, typ: TypeType, mx: MemberContext, override_info: TypeInfo | None
) -&gt; Type:
    # Similar to analyze_type_callable_attribute_access.
    item = None
    fallback = mx.named_type("builtins.type")
    if isinstance(typ.item, Instance):
        item = typ.item
    elif isinstance(typ.item, AnyType):
        with mx.msg.filter_errors():
            return _analyze_member_access(name, fallback, mx, override_info)
    elif isinstance(typ.item, TypeVarType):
        upper_bound = get_proper_type(typ.item.upper_bound)
        if isinstance(upper_bound, Instance):
            item = upper_bound
        elif isinstance(upper_bound, TupleType):
            item = tuple_fallback(upper_bound)
        elif isinstance(upper_bound, AnyType):
            with mx.msg.filter_errors():
                return _analyze_member_access(name, fallback, mx, override_info)
    elif isinstance(typ.item, TupleType):
        item = tuple_fallback(typ.item)
    elif isinstance(typ.item, FunctionLike) and typ.item.is_type_obj():
        item = typ.item.fallback
    elif isinstance(typ.item, TypeType):
        # Access member on metaclass object via Type[Type[C]]
        if isinstance(typ.item.item, Instance):
            item = typ.item.item.type.metaclass_type
    ignore_messages = False
    if item and not mx.is_operator:
        # See comment above for why operators are skipped
        result = analyze_class_attribute_access(item, name, mx, override_info)
        if result:
            if not (isinstance(get_proper_type(result), AnyType) and item.type.fallback_to_any):
                return result
            else:
                # We don't want errors on metaclass lookup for classes with Any fallback
                ignore_messages = True
    if item is not None:
        fallback = item.type.metaclass_type or fallback

    with mx.msg.filter_errors(filter_errors=ignore_messages):
        return _analyze_member_access(name, fallback, mx, override_info)


</t>
<t tx="ekr.20221004064034.805">def analyze_union_member_access(name: str, typ: UnionType, mx: MemberContext) -&gt; Type:
    with mx.msg.disable_type_names():
        results = []
        for subtype in typ.relevant_items():
            # Self types should be bound to every individual item of a union.
            item_mx = mx.copy_modified(self_type=subtype)
            results.append(_analyze_member_access(name, subtype, item_mx))
    return make_simplified_union(results)


</t>
<t tx="ekr.20221004064034.806">def analyze_none_member_access(name: str, typ: NoneType, mx: MemberContext) -&gt; Type:
    if name == "__bool__":
        literal_false = LiteralType(False, fallback=mx.named_type("builtins.bool"))
        return CallableType(
            arg_types=[],
            arg_kinds=[],
            arg_names=[],
            ret_type=literal_false,
            fallback=mx.named_type("builtins.function"),
        )
    else:
        return _analyze_member_access(name, mx.named_type("builtins.object"), mx)


</t>
<t tx="ekr.20221004064034.807">def analyze_member_var_access(
    name: str, itype: Instance, info: TypeInfo, mx: MemberContext
) -&gt; Type:
    """Analyse attribute access that does not target a method.

    This is logically part of analyze_member_access and the arguments are similar.

    original_type is the type of E in the expression E.var
    """
    # It was not a method. Try looking up a variable.
    v = lookup_member_var_or_accessor(info, name, mx.is_lvalue)

    vv = v
    if isinstance(vv, Decorator):
        # The associated Var node of a decorator contains the type.
        v = vv.var
        if mx.is_super:
            validate_super_call(vv.func, mx)

    if isinstance(vv, TypeInfo):
        # If the associated variable is a TypeInfo synthesize a Var node for
        # the purposes of type checking.  This enables us to type check things
        # like accessing class attributes on an inner class.
        v = Var(name, type=type_object_type(vv, mx.named_type))
        v.info = info

    if isinstance(vv, TypeAlias):
        # Similar to the above TypeInfo case, we allow using
        # qualified type aliases in runtime context if it refers to an
        # instance type. For example:
        #     class C:
        #         A = List[int]
        #     x = C.A() &lt;- this is OK
        typ = mx.chk.expr_checker.alias_type_in_runtime_context(
            vv, ctx=mx.context, alias_definition=mx.is_lvalue
        )
        v = Var(name, type=typ)
        v.info = info

    if isinstance(v, Var):
        implicit = info[name].implicit

        # An assignment to final attribute is always an error,
        # independently of types.
        if mx.is_lvalue and not mx.chk.get_final_context():
            check_final_member(name, info, mx.msg, mx.context)

        return analyze_var(name, v, itype, info, mx, implicit=implicit)
    elif isinstance(v, FuncDef):
        assert False, "Did not expect a function"
    elif isinstance(v, MypyFile):
        mx.chk.module_refs.add(v.fullname)
        return mx.chk.expr_checker.module_type(v)
    elif (
        not v
        and name not in ["__getattr__", "__setattr__", "__getattribute__"]
        and not mx.is_operator
        and mx.module_symbol_table is None
    ):
        # Above we skip ModuleType.__getattr__ etc. if we have a
        # module symbol table, since the symbol table allows precise
        # checking.
        if not mx.is_lvalue:
            for method_name in ("__getattribute__", "__getattr__"):
                method = info.get_method(method_name)

                # __getattribute__ is defined on builtins.object and returns Any, so without
                # the guard this search will always find object.__getattribute__ and conclude
                # that the attribute exists
                if method and method.info.fullname != "builtins.object":
                    bound_method = analyze_decorator_or_funcbase_access(
                        defn=method,
                        itype=itype,
                        info=info,
                        self_type=mx.self_type,
                        name=method_name,
                        mx=mx,
                    )
                    typ = map_instance_to_supertype(itype, method.info)
                    getattr_type = get_proper_type(expand_type_by_instance(bound_method, typ))
                    if isinstance(getattr_type, CallableType):
                        result = getattr_type.ret_type
                    else:
                        result = getattr_type

                    # Call the attribute hook before returning.
                    fullname = f"{method.info.fullname}.{name}"
                    hook = mx.chk.plugin.get_attribute_hook(fullname)
                    if hook:
                        result = hook(
                            AttributeContext(
                                get_proper_type(mx.original_type), result, mx.context, mx.chk
                            )
                        )
                    return result
        else:
            setattr_meth = info.get_method("__setattr__")
            if setattr_meth and setattr_meth.info.fullname != "builtins.object":
                bound_type = analyze_decorator_or_funcbase_access(
                    defn=setattr_meth,
                    itype=itype,
                    info=info,
                    self_type=mx.self_type,
                    name=name,
                    mx=mx.copy_modified(is_lvalue=False),
                )
                typ = map_instance_to_supertype(itype, setattr_meth.info)
                setattr_type = get_proper_type(expand_type_by_instance(bound_type, typ))
                if isinstance(setattr_type, CallableType) and len(setattr_type.arg_types) &gt; 0:
                    return setattr_type.arg_types[-1]

    if itype.type.fallback_to_any:
        return AnyType(TypeOfAny.special_form)

    # Could not find the member.
    if itype.extra_attrs and name in itype.extra_attrs.attrs:
        # For modules use direct symbol table lookup.
        if not itype.extra_attrs.mod_name:
            return itype.extra_attrs.attrs[name]

    if mx.is_super:
        mx.msg.undefined_in_superclass(name, mx.context)
        return AnyType(TypeOfAny.from_error)
    else:
        return report_missing_attribute(mx.original_type, itype, name, mx)


</t>
<t tx="ekr.20221004064034.808">def check_final_member(name: str, info: TypeInfo, msg: MessageBuilder, ctx: Context) -&gt; None:
    """Give an error if the name being assigned was declared as final."""
    for base in info.mro:
        sym = base.names.get(name)
        if sym and is_final_node(sym.node):
            msg.cant_assign_to_final(name, attr_assign=True, ctx=ctx)


</t>
<t tx="ekr.20221004064034.809">def analyze_descriptor_access(descriptor_type: Type, mx: MemberContext) -&gt; Type:
    """Type check descriptor access.

    Arguments:
        descriptor_type: The type of the descriptor attribute being accessed
            (the type of ``f`` in ``a.f`` when ``f`` is a descriptor).
        mx: The current member access context.
    Return:
        The return type of the appropriate ``__get__`` overload for the descriptor.
    """
    instance_type = get_proper_type(mx.original_type)
    orig_descriptor_type = descriptor_type
    descriptor_type = get_proper_type(descriptor_type)

    if isinstance(descriptor_type, UnionType):
        # Map the access over union types
        return make_simplified_union(
            [analyze_descriptor_access(typ, mx) for typ in descriptor_type.items]
        )
    elif not isinstance(descriptor_type, Instance):
        return orig_descriptor_type

    if not descriptor_type.type.has_readable_member("__get__"):
        return orig_descriptor_type

    dunder_get = descriptor_type.type.get_method("__get__")
    if dunder_get is None:
        mx.msg.fail(
            message_registry.DESCRIPTOR_GET_NOT_CALLABLE.format(descriptor_type), mx.context
        )
        return AnyType(TypeOfAny.from_error)

    bound_method = analyze_decorator_or_funcbase_access(
        defn=dunder_get,
        itype=descriptor_type,
        info=descriptor_type.type,
        self_type=descriptor_type,
        name="__set__",
        mx=mx,
    )

    typ = map_instance_to_supertype(descriptor_type, dunder_get.info)
    dunder_get_type = expand_type_by_instance(bound_method, typ)

    if isinstance(instance_type, FunctionLike) and instance_type.is_type_obj():
        owner_type = instance_type.items[0].ret_type
        instance_type = NoneType()
    elif isinstance(instance_type, TypeType):
        owner_type = instance_type.item
        instance_type = NoneType()
    else:
        owner_type = instance_type

    callable_name = mx.chk.expr_checker.method_fullname(descriptor_type, "__get__")
    dunder_get_type = mx.chk.expr_checker.transform_callee_type(
        callable_name,
        dunder_get_type,
        [
            TempNode(instance_type, context=mx.context),
            TempNode(TypeType.make_normalized(owner_type), context=mx.context),
        ],
        [ARG_POS, ARG_POS],
        mx.context,
        object_type=descriptor_type,
    )

    _, inferred_dunder_get_type = mx.chk.expr_checker.check_call(
        dunder_get_type,
        [
            TempNode(instance_type, context=mx.context),
            TempNode(TypeType.make_normalized(owner_type), context=mx.context),
        ],
        [ARG_POS, ARG_POS],
        mx.context,
        object_type=descriptor_type,
        callable_name=callable_name,
    )

    inferred_dunder_get_type = get_proper_type(inferred_dunder_get_type)
    if isinstance(inferred_dunder_get_type, AnyType):
        # check_call failed, and will have reported an error
        return inferred_dunder_get_type

    if not isinstance(inferred_dunder_get_type, CallableType):
        mx.msg.fail(
            message_registry.DESCRIPTOR_GET_NOT_CALLABLE.format(descriptor_type), mx.context
        )
        return AnyType(TypeOfAny.from_error)

    return inferred_dunder_get_type.ret_type


</t>
<t tx="ekr.20221004064034.81">def get_revealed_type(line: str, relevant_file: str, relevant_line: int) -&gt; str | None:
    m = re.match(r'(.+?):(\d+): note: Revealed type is "(.*)"$', line)
    if m and int(m.group(2)) == relevant_line and os.path.samefile(relevant_file, m.group(1)):
        return m.group(3)
    else:
        return None


</t>
<t tx="ekr.20221004064034.810">def is_instance_var(var: Var, info: TypeInfo) -&gt; bool:
    """Return if var is an instance variable according to PEP 526."""
    return (
        # check the type_info node is the var (not a decorated function, etc.)
        var.name in info.names
        and info.names[var.name].node is var
        and not var.is_classvar
        # variables without annotations are treated as classvar
        and not var.is_inferred
    )


</t>
<t tx="ekr.20221004064034.811">def analyze_var(
    name: str,
    var: Var,
    itype: Instance,
    info: TypeInfo,
    mx: MemberContext,
    *,
    implicit: bool = False,
) -&gt; Type:
    """Analyze access to an attribute via a Var node.

    This is conceptually part of analyze_member_access and the arguments are similar.

    itype is the class object in which var is defined
    original_type is the type of E in the expression E.var
    if implicit is True, the original Var was created as an assignment to self
    """
    # Found a member variable.
    itype = map_instance_to_supertype(itype, var.info)
    typ = var.type
    if typ:
        if isinstance(typ, PartialType):
            return mx.chk.handle_partial_var_type(typ, mx.is_lvalue, var, mx.context)
        if mx.is_lvalue and var.is_property and not var.is_settable_property:
            # TODO allow setting attributes in subclass (although it is probably an error)
            mx.msg.read_only_property(name, itype.type, mx.context)
        if mx.is_lvalue and var.is_classvar:
            mx.msg.cant_assign_to_classvar(name, mx.context)
        t = get_proper_type(expand_type_by_instance(typ, itype))
        result: Type = t
        typ = get_proper_type(typ)
        if (
            var.is_initialized_in_class
            and (not is_instance_var(var, info) or mx.is_operator)
            and isinstance(typ, FunctionLike)
            and not typ.is_type_obj()
        ):
            if mx.is_lvalue:
                if var.is_property:
                    if not var.is_settable_property:
                        mx.msg.read_only_property(name, itype.type, mx.context)
                else:
                    mx.msg.cant_assign_to_method(mx.context)

            if not var.is_staticmethod:
                # Class-level function objects and classmethods become bound methods:
                # the former to the instance, the latter to the class.
                functype = typ
                # Use meet to narrow original_type to the dispatched type.
                # For example, assume
                # * A.f: Callable[[A1], None] where A1 &lt;: A (maybe A1 == A)
                # * B.f: Callable[[B1], None] where B1 &lt;: B (maybe B1 == B)
                # * x: Union[A1, B1]
                # In `x.f`, when checking `x` against A1 we assume x is compatible with A
                # and similarly for B1 when checking against B
                dispatched_type = meet.meet_types(mx.original_type, itype)
                signature = freshen_function_type_vars(functype)
                signature = check_self_arg(
                    signature, dispatched_type, var.is_classmethod, mx.context, name, mx.msg
                )
                signature = bind_self(signature, mx.self_type, var.is_classmethod)
                expanded_signature = expand_type_by_instance(signature, itype)
                freeze_type_vars(expanded_signature)
                if var.is_property:
                    # A property cannot have an overloaded type =&gt; the cast is fine.
                    assert isinstance(expanded_signature, CallableType)
                    result = expanded_signature.ret_type
                else:
                    result = expanded_signature
    else:
        if not var.is_ready and not mx.no_deferral:
            mx.not_ready_callback(var.name, mx.context)
        # Implicit 'Any' type.
        result = AnyType(TypeOfAny.special_form)
    fullname = f"{var.info.fullname}.{name}"
    hook = mx.chk.plugin.get_attribute_hook(fullname)
    if result and not mx.is_lvalue and not implicit:
        result = analyze_descriptor_access(result, mx)
    if hook:
        result = hook(
            AttributeContext(get_proper_type(mx.original_type), result, mx.context, mx.chk)
        )
    return result


</t>
<t tx="ekr.20221004064034.812">def freeze_type_vars(member_type: Type) -&gt; None:
    if not isinstance(member_type, ProperType):
        return
    if isinstance(member_type, CallableType):
        for v in member_type.variables:
            v.id.meta_level = 0
    if isinstance(member_type, Overloaded):
        for it in member_type.items:
            for v in it.variables:
                v.id.meta_level = 0


</t>
<t tx="ekr.20221004064034.813">def lookup_member_var_or_accessor(info: TypeInfo, name: str, is_lvalue: bool) -&gt; SymbolNode | None:
    """Find the attribute/accessor node that refers to a member of a type."""
    # TODO handle lvalues
    node = info.get(name)
    if node:
        return node.node
    else:
        return None


</t>
<t tx="ekr.20221004064034.814">def check_self_arg(
    functype: FunctionLike,
    dispatched_arg_type: Type,
    is_classmethod: bool,
    context: Context,
    name: str,
    msg: MessageBuilder,
) -&gt; FunctionLike:
    """Check that an instance has a valid type for a method with annotated 'self'.

    For example if the method is defined as:
        class A:
            def f(self: S) -&gt; T: ...
    then for 'x.f' we check that meet(type(x), A) &lt;: S. If the method is overloaded, we
    select only overloads items that satisfy this requirement. If there are no matching
    overloads, an error is generated.

    Note: dispatched_arg_type uses a meet to select a relevant item in case if the
    original type of 'x' is a union. This is done because several special methods
    treat union types in ad-hoc manner, so we can't use MemberContext.self_type yet.
    """
    items = functype.items
    if not items:
        return functype
    new_items = []
    if is_classmethod:
        dispatched_arg_type = TypeType.make_normalized(dispatched_arg_type)

    for item in items:
        if not item.arg_types or item.arg_kinds[0] not in (ARG_POS, ARG_STAR):
            # No positional first (self) argument (*args is okay).
            msg.no_formal_self(name, item, context)
            # This is pretty bad, so just return the original signature if
            # there is at least one such error.
            return functype
        else:
            selfarg = get_proper_type(item.arg_types[0])
            if subtypes.is_subtype(dispatched_arg_type, erase_typevars(erase_to_bound(selfarg))):
                new_items.append(item)
            elif isinstance(selfarg, ParamSpecType):
                # TODO: This is not always right. What's the most reasonable thing to do here?
                new_items.append(item)
            elif isinstance(selfarg, TypeVarTupleType):
                raise NotImplementedError
    if not new_items:
        # Choose first item for the message (it may be not very helpful for overloads).
        msg.incompatible_self_argument(
            name, dispatched_arg_type, items[0], is_classmethod, context
        )
        return functype
    if len(new_items) == 1:
        return new_items[0]
    return Overloaded(new_items)


</t>
<t tx="ekr.20221004064034.815">def analyze_class_attribute_access(
    itype: Instance,
    name: str,
    mx: MemberContext,
    override_info: TypeInfo | None = None,
    original_vars: Sequence[TypeVarLikeType] | None = None,
) -&gt; Type | None:
    """Analyze access to an attribute on a class object.

    itype is the return type of the class object callable, original_type is the type
    of E in the expression E.var, original_vars are type variables of the class callable
    (for generic classes).
    """
    info = itype.type
    if override_info:
        info = override_info

    fullname = f"{info.fullname}.{name}"
    hook = mx.chk.plugin.get_class_attribute_hook(fullname)

    node = info.get(name)
    if not node:
        if itype.extra_attrs and name in itype.extra_attrs.attrs:
            # For modules use direct symbol table lookup.
            if not itype.extra_attrs.mod_name:
                return itype.extra_attrs.attrs[name]
        if info.fallback_to_any:
            return apply_class_attr_hook(mx, hook, AnyType(TypeOfAny.special_form))
        return None

    is_decorated = isinstance(node.node, Decorator)
    is_method = is_decorated or isinstance(node.node, FuncBase)
    if mx.is_lvalue:
        if is_method:
            mx.msg.cant_assign_to_method(mx.context)
        if isinstance(node.node, TypeInfo):
            mx.msg.fail(message_registry.CANNOT_ASSIGN_TO_TYPE, mx.context)

    # If a final attribute was declared on `self` in `__init__`, then it
    # can't be accessed on the class object.
    if node.implicit and isinstance(node.node, Var) and node.node.is_final:
        mx.msg.fail(
            message_registry.CANNOT_ACCESS_FINAL_INSTANCE_ATTR.format(node.node.name), mx.context
        )

    # An assignment to final attribute on class object is also always an error,
    # independently of types.
    if mx.is_lvalue and not mx.chk.get_final_context():
        check_final_member(name, info, mx.msg, mx.context)

    if info.is_enum and not (mx.is_lvalue or is_decorated or is_method):
        enum_class_attribute_type = analyze_enum_class_attribute_access(itype, name, mx)
        if enum_class_attribute_type:
            return apply_class_attr_hook(mx, hook, enum_class_attribute_type)

    t = node.type
    if t:
        if isinstance(t, PartialType):
            symnode = node.node
            assert isinstance(symnode, Var)
            return apply_class_attr_hook(
                mx, hook, mx.chk.handle_partial_var_type(t, mx.is_lvalue, symnode, mx.context)
            )

        # Find the class where method/variable was defined.
        if isinstance(node.node, Decorator):
            super_info: TypeInfo | None = node.node.var.info
        elif isinstance(node.node, (Var, SYMBOL_FUNCBASE_TYPES)):
            super_info = node.node.info
        else:
            super_info = None

        # Map the type to how it would look as a defining class. For example:
        #     class C(Generic[T]): ...
        #     class D(C[Tuple[T, S]]): ...
        #     D[int, str].method()
        # Here itype is D[int, str], isuper is C[Tuple[int, str]].
        if not super_info:
            isuper = None
        else:
            isuper = map_instance_to_supertype(itype, super_info)

        if isinstance(node.node, Var):
            assert isuper is not None
            # Check if original variable type has type variables. For example:
            #     class C(Generic[T]):
            #         x: T
            #     C.x  # Error, ambiguous access
            #     C[int].x  # Also an error, since C[int] is same as C at runtime
            if isinstance(t, TypeVarType) or has_type_vars(t):
                # Exception: access on Type[...], including first argument of class methods is OK.
                if not isinstance(get_proper_type(mx.original_type), TypeType) or node.implicit:
                    if node.node.is_classvar:
                        message = message_registry.GENERIC_CLASS_VAR_ACCESS
                    else:
                        message = message_registry.GENERIC_INSTANCE_VAR_CLASS_ACCESS
                    mx.msg.fail(message, mx.context)

            # Erase non-mapped variables, but keep mapped ones, even if there is an error.
            # In the above example this means that we infer following types:
            #     C.x -&gt; Any
            #     C[int].x -&gt; int
            t = erase_typevars(expand_type_by_instance(t, isuper))

        is_classmethod = (is_decorated and cast(Decorator, node.node).func.is_class) or (
            isinstance(node.node, FuncBase) and node.node.is_class
        )
        t = get_proper_type(t)
        if isinstance(t, FunctionLike) and is_classmethod:
            t = check_self_arg(t, mx.self_type, False, mx.context, name, mx.msg)
        result = add_class_tvars(
            t, isuper, is_classmethod, mx.self_type, original_vars=original_vars
        )
        if not mx.is_lvalue:
            result = analyze_descriptor_access(result, mx)

        return apply_class_attr_hook(mx, hook, result)
    elif isinstance(node.node, Var):
        mx.not_ready_callback(name, mx.context)
        return AnyType(TypeOfAny.special_form)

    if isinstance(node.node, TypeVarExpr):
        mx.msg.fail(
            message_registry.CANNOT_USE_TYPEVAR_AS_EXPRESSION.format(info.name, name), mx.context
        )
        return AnyType(TypeOfAny.from_error)

    if isinstance(node.node, TypeInfo):
        return type_object_type(node.node, mx.named_type)

    if isinstance(node.node, MypyFile):
        # Reference to a module object.
        return mx.named_type("types.ModuleType")

    if isinstance(node.node, TypeAlias):
        return mx.chk.expr_checker.alias_type_in_runtime_context(
            node.node, ctx=mx.context, alias_definition=mx.is_lvalue
        )

    if is_decorated:
        assert isinstance(node.node, Decorator)
        if node.node.type:
            return apply_class_attr_hook(mx, hook, node.node.type)
        else:
            mx.not_ready_callback(name, mx.context)
            return AnyType(TypeOfAny.from_error)
    else:
        assert isinstance(node.node, FuncBase)
        typ = function_type(node.node, mx.named_type("builtins.function"))
        # Note: if we are accessing class method on class object, the cls argument is bound.
        # Annotated and/or explicit class methods go through other code paths above, for
        # unannotated implicit class methods we do this here.
        if node.node.is_class:
            typ = bind_self(typ, is_classmethod=True)
        return apply_class_attr_hook(mx, hook, typ)


</t>
<t tx="ekr.20221004064034.816">def apply_class_attr_hook(
    mx: MemberContext, hook: Callable[[AttributeContext], Type] | None, result: Type
) -&gt; Type | None:
    if hook:
        result = hook(
            AttributeContext(get_proper_type(mx.original_type), result, mx.context, mx.chk)
        )
    return result


</t>
<t tx="ekr.20221004064034.817">def analyze_enum_class_attribute_access(
    itype: Instance, name: str, mx: MemberContext
) -&gt; Type | None:
    # Skip these since Enum will remove it
    if name in ENUM_REMOVED_PROPS:
        return report_missing_attribute(mx.original_type, itype, name, mx)
    # For other names surrendered by underscores, we don't make them Enum members
    if name.startswith("__") and name.endswith("__") and name.replace("_", "") != "":
        return None

    enum_literal = LiteralType(name, fallback=itype)
    return itype.copy_modified(last_known_value=enum_literal)


</t>
<t tx="ekr.20221004064034.818">def analyze_typeddict_access(
    name: str, typ: TypedDictType, mx: MemberContext, override_info: TypeInfo | None
) -&gt; Type:
    if name == "__setitem__":
        if isinstance(mx.context, IndexExpr):
            # Since we can get this during `a['key'] = ...`
            # it is safe to assume that the context is `IndexExpr`.
            item_type = mx.chk.expr_checker.visit_typeddict_index_expr(typ, mx.context.index)
        else:
            # It can also be `a.__setitem__(...)` direct call.
            # In this case `item_type` can be `Any`,
            # because we don't have args available yet.
            # TODO: check in `default` plugin that `__setitem__` is correct.
            item_type = AnyType(TypeOfAny.implementation_artifact)
        return CallableType(
            arg_types=[mx.chk.named_type("builtins.str"), item_type],
            arg_kinds=[ARG_POS, ARG_POS],
            arg_names=[None, None],
            ret_type=NoneType(),
            fallback=mx.chk.named_type("builtins.function"),
            name=name,
        )
    elif name == "__delitem__":
        return CallableType(
            arg_types=[mx.chk.named_type("builtins.str")],
            arg_kinds=[ARG_POS],
            arg_names=[None],
            ret_type=NoneType(),
            fallback=mx.chk.named_type("builtins.function"),
            name=name,
        )
    return _analyze_member_access(name, typ.fallback, mx, override_info)


</t>
<t tx="ekr.20221004064034.819">def add_class_tvars(
    t: ProperType,
    isuper: Instance | None,
    is_classmethod: bool,
    original_type: Type,
    original_vars: Sequence[TypeVarLikeType] | None = None,
) -&gt; Type:
    """Instantiate type variables during analyze_class_attribute_access,
    e.g T and Q in the following:

    class A(Generic[T]):
        @classmethod
        def foo(cls: Type[Q]) -&gt; Tuple[T, Q]: ...

    class B(A[str]): pass
    B.foo()

    Args:
        t: Declared type of the method (or property)
        isuper: Current instance mapped to the superclass where method was defined, this
            is usually done by map_instance_to_supertype()
        is_classmethod: True if this method is decorated with @classmethod
        original_type: The value of the type B in the expression B.foo() or the corresponding
            component in case of a union (this is used to bind the self-types)
        original_vars: Type variables of the class callable on which the method was accessed
    Returns:
        Expanded method type with added type variables (when needed).
    """
    # TODO: verify consistency between Q and T

    # We add class type variables if the class method is accessed on class object
    # without applied type arguments, this matches the behavior of __init__().
    # For example (continuing the example in docstring):
    #     A       # The type of callable is def [T] () -&gt; A[T], _not_ def () -&gt; A[Any]
    #     A[int]  # The type of callable is def () -&gt; A[int]
    # and
    #     A.foo       # The type is generic def [T] () -&gt; Tuple[T, A[T]]
    #     A[int].foo  # The type is non-generic def () -&gt; Tuple[int, A[int]]
    #
    # This behaviour is useful for defining alternative constructors for generic classes.
    # To achieve such behaviour, we add the class type variables that are still free
    # (i.e. appear in the return type of the class object on which the method was accessed).
    if isinstance(t, CallableType):
        tvars = original_vars if original_vars is not None else []
        if is_classmethod:
            t = freshen_function_type_vars(t)
            t = bind_self(t, original_type, is_classmethod=True)
            assert isuper is not None
            t = cast(CallableType, expand_type_by_instance(t, isuper))
            freeze_type_vars(t)
        return t.copy_modified(variables=list(tvars) + list(t.variables))
    elif isinstance(t, Overloaded):
        return Overloaded(
            [
                cast(
                    CallableType,
                    add_class_tvars(
                        item, isuper, is_classmethod, original_type, original_vars=original_vars
                    ),
                )
                for item in t.items
            ]
        )
    if isuper is not None:
        t = expand_type_by_instance(t, isuper)
    return t


</t>
<t tx="ekr.20221004064034.82">def process_output(output: str, filename: str, start_line: int) -&gt; tuple[str | None, bool]:
    error_found = False
    for line in output.splitlines():
        t = get_revealed_type(line, filename, start_line)
        if t:
            return t, error_found
        elif "error:" in line:
            error_found = True
    return None, True  # finding no reveal_type is an error


</t>
<t tx="ekr.20221004064034.820">def type_object_type(info: TypeInfo, named_type: Callable[[str], Instance]) -&gt; ProperType:
    """Return the type of a type object.

    For a generic type G with type variables T and S the type is generally of form

      Callable[..., G[T, S]]

    where ... are argument types for the __init__/__new__ method (without the self
    argument). Also, the fallback type will be 'type' instead of 'function'.
    """

    # We take the type from whichever of __init__ and __new__ is first
    # in the MRO, preferring __init__ if there is a tie.
    init_method = info.get("__init__")
    new_method = info.get("__new__")
    if not init_method or not is_valid_constructor(init_method.node):
        # Must be an invalid class definition.
        return AnyType(TypeOfAny.from_error)
    # There *should* always be a __new__ method except the test stubs
    # lack it, so just copy init_method in that situation
    new_method = new_method or init_method
    if not is_valid_constructor(new_method.node):
        # Must be an invalid class definition.
        return AnyType(TypeOfAny.from_error)

    # The two is_valid_constructor() checks ensure this.
    assert isinstance(new_method.node, (SYMBOL_FUNCBASE_TYPES, Decorator))
    assert isinstance(init_method.node, (SYMBOL_FUNCBASE_TYPES, Decorator))

    init_index = info.mro.index(init_method.node.info)
    new_index = info.mro.index(new_method.node.info)

    fallback = info.metaclass_type or named_type("builtins.type")
    if init_index &lt; new_index:
        method: FuncBase | Decorator = init_method.node
        is_new = False
    elif init_index &gt; new_index:
        method = new_method.node
        is_new = True
    else:
        if init_method.node.info.fullname == "builtins.object":
            # Both are defined by object.  But if we've got a bogus
            # base class, we can't know for sure, so check for that.
            if info.fallback_to_any:
                # Construct a universal callable as the prototype.
                any_type = AnyType(TypeOfAny.special_form)
                sig = CallableType(
                    arg_types=[any_type, any_type],
                    arg_kinds=[ARG_STAR, ARG_STAR2],
                    arg_names=["_args", "_kwds"],
                    ret_type=any_type,
                    fallback=named_type("builtins.function"),
                )
                return class_callable(sig, info, fallback, None, is_new=False)

        # Otherwise prefer __init__ in a tie. It isn't clear that this
        # is the right thing, but __new__ caused problems with
        # typeshed (#5647).
        method = init_method.node
        is_new = False
    # Construct callable type based on signature of __init__. Adjust
    # return type and insert type arguments.
    if isinstance(method, FuncBase):
        t = function_type(method, fallback)
    else:
        assert isinstance(method.type, ProperType)
        assert isinstance(method.type, FunctionLike)  # is_valid_constructor() ensures this
        t = method.type
    return type_object_type_from_function(t, info, method.info, fallback, is_new)


</t>
<t tx="ekr.20221004064034.821">def analyze_decorator_or_funcbase_access(
    defn: Decorator | FuncBase,
    itype: Instance,
    info: TypeInfo,
    self_type: Type | None,
    name: str,
    mx: MemberContext,
) -&gt; Type:
    """Analyzes the type behind method access.

    The function itself can possibly be decorated.
    See: https://github.com/python/mypy/issues/10409
    """
    if isinstance(defn, Decorator):
        return analyze_var(name, defn.var, itype, info, mx)
    return bind_self(
        function_type(defn, mx.chk.named_type("builtins.function")), original_type=self_type
    )


</t>
<t tx="ekr.20221004064034.822">def is_valid_constructor(n: SymbolNode | None) -&gt; bool:
    """Does this node represents a valid constructor method?

    This includes normal functions, overloaded functions, and decorators
    that return a callable type.
    """
    if isinstance(n, FuncBase):
        return True
    if isinstance(n, Decorator):
        return isinstance(get_proper_type(n.type), FunctionLike)
    return False
</t>
<t tx="ekr.20221004064034.823">@path C:/Repos/ekr-mypy2/mypy/
"""Pattern checker. This file is conceptually part of TypeChecker."""

from __future__ import annotations

from collections import defaultdict
from typing import NamedTuple
from typing_extensions import Final

import mypy.checker
from mypy import message_registry
from mypy.checkmember import analyze_member_access
from mypy.expandtype import expand_type_by_instance
from mypy.join import join_types
from mypy.literals import literal_hash
from mypy.maptype import map_instance_to_supertype
from mypy.meet import narrow_declared_type
from mypy.messages import MessageBuilder
from mypy.nodes import ARG_POS, Expression, NameExpr, TypeAlias, TypeInfo, Var
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    Pattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.plugin import Plugin
from mypy.subtypes import is_subtype
from mypy.typeops import (
    coerce_to_literal,
    make_simplified_union,
    try_getting_str_literals_from_type,
    tuple_fallback,
)
from mypy.types import (
    AnyType,
    Instance,
    LiteralType,
    NoneType,
    ProperType,
    TupleType,
    Type,
    TypedDictType,
    TypeOfAny,
    UninhabitedType,
    UnionType,
    get_proper_type,
)
from mypy.typevars import fill_typevars
from mypy.visitor import PatternVisitor

self_match_type_names: Final = [
    "builtins.bool",
    "builtins.bytearray",
    "builtins.bytes",
    "builtins.dict",
    "builtins.float",
    "builtins.frozenset",
    "builtins.int",
    "builtins.list",
    "builtins.set",
    "builtins.str",
    "builtins.tuple",
]

non_sequence_match_type_names: Final = ["builtins.str", "builtins.bytes", "builtins.bytearray"]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.824"># For every Pattern a PatternType can be calculated. This requires recursively calculating
# the PatternTypes of the sub-patterns first.
# Using the data in the PatternType the match subject and captured names can be narrowed/inferred.
class PatternType(NamedTuple):
    type: Type  # The type the match subject can be narrowed to
    rest_type: Type  # The remaining type if the pattern didn't match
    captures: dict[Expression, Type]  # The variables captured by the pattern


</t>
<t tx="ekr.20221004064034.825">class PatternChecker(PatternVisitor[PatternType]):
    """Pattern checker.

    This class checks if a pattern can match a type, what the type can be narrowed to, and what
    type capture patterns should be inferred as.
    """

    # Some services are provided by a TypeChecker instance.
    chk: mypy.checker.TypeChecker
    # This is shared with TypeChecker, but stored also here for convenience.
    msg: MessageBuilder
    # Currently unused
    plugin: Plugin
    # The expression being matched against the pattern
    subject: Expression

    subject_type: Type
    # Type of the subject to check the (sub)pattern against
    type_context: list[Type]
    # Types that match against self instead of their __match_args__ if used as a class pattern
    # Filled in from self_match_type_names
    self_match_types: list[Type]
    # Types that are sequences, but don't match sequence patterns. Filled in from
    # non_sequence_match_type_names
    non_sequence_match_types: list[Type]

    @others
</t>
<t tx="ekr.20221004064034.826">def __init__(self, chk: mypy.checker.TypeChecker, msg: MessageBuilder, plugin: Plugin) -&gt; None:
    self.chk = chk
    self.msg = msg
    self.plugin = plugin

    self.type_context = []
    self.self_match_types = self.generate_types_from_names(self_match_type_names)
    self.non_sequence_match_types = self.generate_types_from_names(
        non_sequence_match_type_names
    )

</t>
<t tx="ekr.20221004064034.827">def accept(self, o: Pattern, type_context: Type) -&gt; PatternType:
    self.type_context.append(type_context)
    result = o.accept(self)
    self.type_context.pop()

    return result

</t>
<t tx="ekr.20221004064034.828">def visit_as_pattern(self, o: AsPattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    if o.pattern is not None:
        pattern_type = self.accept(o.pattern, current_type)
        typ, rest_type, type_map = pattern_type
    else:
        typ, rest_type, type_map = current_type, UninhabitedType(), {}

    if not is_uninhabited(typ) and o.name is not None:
        typ, _ = self.chk.conditional_types_with_intersection(
            current_type, [get_type_range(typ)], o, default=current_type
        )
        if not is_uninhabited(typ):
            type_map[o.name] = typ

    return PatternType(typ, rest_type, type_map)

</t>
<t tx="ekr.20221004064034.829">def visit_or_pattern(self, o: OrPattern) -&gt; PatternType:
    current_type = self.type_context[-1]

    #
    # Check all the subpatterns
    #
    pattern_types = []
    for pattern in o.patterns:
        pattern_type = self.accept(pattern, current_type)
        pattern_types.append(pattern_type)
        current_type = pattern_type.rest_type

    #
    # Collect the final type
    #
    types = []
    for pattern_type in pattern_types:
        if not is_uninhabited(pattern_type.type):
            types.append(pattern_type.type)

    #
    # Check the capture types
    #
    capture_types: dict[Var, list[tuple[Expression, Type]]] = defaultdict(list)
    # Collect captures from the first subpattern
    for expr, typ in pattern_types[0].captures.items():
        node = get_var(expr)
        capture_types[node].append((expr, typ))

    # Check if other subpatterns capture the same names
    for i, pattern_type in enumerate(pattern_types[1:]):
        vars = {get_var(expr) for expr, _ in pattern_type.captures.items()}
        if capture_types.keys() != vars:
            self.msg.fail(message_registry.OR_PATTERN_ALTERNATIVE_NAMES, o.patterns[i])
        for expr, typ in pattern_type.captures.items():
            node = get_var(expr)
            capture_types[node].append((expr, typ))

    captures: dict[Expression, Type] = {}
    for var, capture_list in capture_types.items():
        typ = UninhabitedType()
        for _, other in capture_list:
            typ = join_types(typ, other)

        captures[capture_list[0][0]] = typ

    union_type = make_simplified_union(types)
    return PatternType(union_type, current_type, captures)

</t>
<t tx="ekr.20221004064034.83">def main() -&gt; None:
    filename, start_line_str, start_col_str, end_line_str, end_col_str, *mypy_and_args = sys.argv[
        1:
    ]
    start_line = int(start_line_str)
    start_col = int(start_col_str)
    end_line = int(end_line_str)
    end_col = int(end_col_str)
    with open(filename) as f:
        lines = f.readlines()
        lines[end_line - 1] = update_line(
            lines[end_line - 1], REVEAL_TYPE_END, end_col
        )  # insert after end_col
        lines[start_line - 1] = update_line(lines[start_line - 1], REVEAL_TYPE_START, start_col)
        with tempfile.NamedTemporaryFile(mode="w", prefix="mypy") as tmp_f:
            tmp_f.writelines(lines)
            tmp_f.flush()

            output = run_mypy(mypy_and_args, filename, tmp_f.name)
            revealed_type, error = process_output(output, filename, start_line)
            if revealed_type:
                print(revealed_type)
            if error:
                print(output)
            exit(int(error))


</t>
<t tx="ekr.20221004064034.830">def visit_value_pattern(self, o: ValuePattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    typ = self.chk.expr_checker.accept(o.expr)
    typ = coerce_to_literal(typ)
    narrowed_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type, [get_type_range(typ)], o, default=current_type
    )
    if not isinstance(get_proper_type(narrowed_type), (LiteralType, UninhabitedType)):
        return PatternType(narrowed_type, UnionType.make_union([narrowed_type, rest_type]), {})
    return PatternType(narrowed_type, rest_type, {})

</t>
<t tx="ekr.20221004064034.831">def visit_singleton_pattern(self, o: SingletonPattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    value: bool | None = o.value
    if isinstance(value, bool):
        typ = self.chk.expr_checker.infer_literal_expr_type(value, "builtins.bool")
    elif value is None:
        typ = NoneType()
    else:
        assert False

    narrowed_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type, [get_type_range(typ)], o, default=current_type
    )
    return PatternType(narrowed_type, rest_type, {})

</t>
<t tx="ekr.20221004064034.832">def visit_sequence_pattern(self, o: SequencePattern) -&gt; PatternType:
    #
    # check for existence of a starred pattern
    #
    current_type = get_proper_type(self.type_context[-1])
    if not self.can_match_sequence(current_type):
        return self.early_non_match()
    star_positions = [i for i, p in enumerate(o.patterns) if isinstance(p, StarredPattern)]
    star_position: int | None = None
    if len(star_positions) == 1:
        star_position = star_positions[0]
    elif len(star_positions) &gt;= 2:
        assert False, "Parser should prevent multiple starred patterns"
    required_patterns = len(o.patterns)
    if star_position is not None:
        required_patterns -= 1

    #
    # get inner types of original type
    #
    if isinstance(current_type, TupleType):
        inner_types = current_type.items
        size_diff = len(inner_types) - required_patterns
        if size_diff &lt; 0:
            return self.early_non_match()
        elif size_diff &gt; 0 and star_position is None:
            return self.early_non_match()
    else:
        inner_type = self.get_sequence_type(current_type)
        if inner_type is None:
            inner_type = self.chk.named_type("builtins.object")
        inner_types = [inner_type] * len(o.patterns)

    #
    # match inner patterns
    #
    contracted_new_inner_types: list[Type] = []
    contracted_rest_inner_types: list[Type] = []
    captures: dict[Expression, Type] = {}

    contracted_inner_types = self.contract_starred_pattern_types(
        inner_types, star_position, required_patterns
    )
    for p, t in zip(o.patterns, contracted_inner_types):
        pattern_type = self.accept(p, t)
        typ, rest, type_map = pattern_type
        contracted_new_inner_types.append(typ)
        contracted_rest_inner_types.append(rest)
        self.update_type_map(captures, type_map)

    new_inner_types = self.expand_starred_pattern_types(
        contracted_new_inner_types, star_position, len(inner_types)
    )
    rest_inner_types = self.expand_starred_pattern_types(
        contracted_rest_inner_types, star_position, len(inner_types)
    )

    #
    # Calculate new type
    #
    new_type: Type
    rest_type: Type = current_type
    if isinstance(current_type, TupleType):
        narrowed_inner_types = []
        inner_rest_types = []
        for inner_type, new_inner_type in zip(inner_types, new_inner_types):
            (
                narrowed_inner_type,
                inner_rest_type,
            ) = self.chk.conditional_types_with_intersection(
                new_inner_type, [get_type_range(inner_type)], o, default=new_inner_type
            )
            narrowed_inner_types.append(narrowed_inner_type)
            inner_rest_types.append(inner_rest_type)
        if all(not is_uninhabited(typ) for typ in narrowed_inner_types):
            new_type = TupleType(narrowed_inner_types, current_type.partial_fallback)
        else:
            new_type = UninhabitedType()

        if all(is_uninhabited(typ) for typ in inner_rest_types):
            # All subpatterns always match, so we can apply negative narrowing
            rest_type = TupleType(rest_inner_types, current_type.partial_fallback)
    else:
        new_inner_type = UninhabitedType()
        for typ in new_inner_types:
            new_inner_type = join_types(new_inner_type, typ)
        new_type = self.construct_sequence_child(current_type, new_inner_type)
        if is_subtype(new_type, current_type):
            new_type, _ = self.chk.conditional_types_with_intersection(
                current_type, [get_type_range(new_type)], o, default=current_type
            )
        else:
            new_type = current_type
    return PatternType(new_type, rest_type, captures)

</t>
<t tx="ekr.20221004064034.833">def get_sequence_type(self, t: Type) -&gt; Type | None:
    t = get_proper_type(t)
    if isinstance(t, AnyType):
        return AnyType(TypeOfAny.from_another_any, t)
    if isinstance(t, UnionType):
        items = [self.get_sequence_type(item) for item in t.items]
        not_none_items = [item for item in items if item is not None]
        if len(not_none_items) &gt; 0:
            return make_simplified_union(not_none_items)
        else:
            return None

    if self.chk.type_is_iterable(t) and isinstance(t, (Instance, TupleType)):
        if isinstance(t, TupleType):
            t = tuple_fallback(t)
        return self.chk.iterable_item_type(t)
    else:
        return None

</t>
<t tx="ekr.20221004064034.834">def contract_starred_pattern_types(
    self, types: list[Type], star_pos: int | None, num_patterns: int
) -&gt; list[Type]:
    """
    Contracts a list of types in a sequence pattern depending on the position of a starred
    capture pattern.

    For example if the sequence pattern [a, *b, c] is matched against types [bool, int, str,
    bytes] the contracted types are [bool, Union[int, str], bytes].

    If star_pos in None the types are returned unchanged.
    """
    if star_pos is None:
        return types
    new_types = types[:star_pos]
    star_length = len(types) - num_patterns
    new_types.append(make_simplified_union(types[star_pos : star_pos + star_length]))
    new_types += types[star_pos + star_length :]

    return new_types

</t>
<t tx="ekr.20221004064034.835">def expand_starred_pattern_types(
    self, types: list[Type], star_pos: int | None, num_types: int
) -&gt; list[Type]:
    """Undoes the contraction done by contract_starred_pattern_types.

    For example if the sequence pattern is [a, *b, c] and types [bool, int, str] are extended
    to length 4 the result is [bool, int, int, str].
    """
    if star_pos is None:
        return types
    new_types = types[:star_pos]
    star_length = num_types - len(types) + 1
    new_types += [types[star_pos]] * star_length
    new_types += types[star_pos + 1 :]

    return new_types

</t>
<t tx="ekr.20221004064034.836">def visit_starred_pattern(self, o: StarredPattern) -&gt; PatternType:
    captures: dict[Expression, Type] = {}
    if o.capture is not None:
        list_type = self.chk.named_generic_type("builtins.list", [self.type_context[-1]])
        captures[o.capture] = list_type
    return PatternType(self.type_context[-1], UninhabitedType(), captures)

</t>
<t tx="ekr.20221004064034.837">def visit_mapping_pattern(self, o: MappingPattern) -&gt; PatternType:
    current_type = get_proper_type(self.type_context[-1])
    can_match = True
    captures: dict[Expression, Type] = {}
    for key, value in zip(o.keys, o.values):
        inner_type = self.get_mapping_item_type(o, current_type, key)
        if inner_type is None:
            can_match = False
            inner_type = self.chk.named_type("builtins.object")
        pattern_type = self.accept(value, inner_type)
        if is_uninhabited(pattern_type.type):
            can_match = False
        else:
            self.update_type_map(captures, pattern_type.captures)

    if o.rest is not None:
        mapping = self.chk.named_type("typing.Mapping")
        if is_subtype(current_type, mapping) and isinstance(current_type, Instance):
            mapping_inst = map_instance_to_supertype(current_type, mapping.type)
            dict_typeinfo = self.chk.lookup_typeinfo("builtins.dict")
            rest_type = Instance(dict_typeinfo, mapping_inst.args)
        else:
            object_type = self.chk.named_type("builtins.object")
            rest_type = self.chk.named_generic_type(
                "builtins.dict", [object_type, object_type]
            )

        captures[o.rest] = rest_type

    if can_match:
        # We can't narrow the type here, as Mapping key is invariant.
        new_type = self.type_context[-1]
    else:
        new_type = UninhabitedType()
    return PatternType(new_type, current_type, captures)

</t>
<t tx="ekr.20221004064034.838">def get_mapping_item_type(
    self, pattern: MappingPattern, mapping_type: Type, key: Expression
) -&gt; Type | None:
    mapping_type = get_proper_type(mapping_type)
    if isinstance(mapping_type, TypedDictType):
        with self.msg.filter_errors() as local_errors:
            result: Type | None = self.chk.expr_checker.visit_typeddict_index_expr(
                mapping_type, key
            )
            has_local_errors = local_errors.has_new_errors()
        # If we can't determine the type statically fall back to treating it as a normal
        # mapping
        if has_local_errors:
            with self.msg.filter_errors() as local_errors:
                result = self.get_simple_mapping_item_type(pattern, mapping_type, key)

                if local_errors.has_new_errors():
                    result = None
    else:
        with self.msg.filter_errors():
            result = self.get_simple_mapping_item_type(pattern, mapping_type, key)
    return result

</t>
<t tx="ekr.20221004064034.839">def get_simple_mapping_item_type(
    self, pattern: MappingPattern, mapping_type: Type, key: Expression
) -&gt; Type:
    result, _ = self.chk.expr_checker.check_method_call_by_name(
        "__getitem__", mapping_type, [key], [ARG_POS], pattern
    )
    return result

</t>
<t tx="ekr.20221004064034.84">@path C:/Repos/ekr-mypy2/misc/
"""Fixer for lib2to3 that inserts mypy annotations into all methods.

The simplest way to run this is to copy it into lib2to3's "fixes"
subdirectory and then run "2to3 -f annotate" over your files.

The fixer transforms e.g.

  def foo(self, bar, baz=12):
      return bar + baz

into

  def foo(self, bar, baz=12):
      # type: (Any, int) -&gt; Any
      return bar + baz

It does not do type inference but it recognizes some basic default
argument values such as numbers and strings (and assumes their type
implies the argument type).

It also uses some basic heuristics to decide whether to ignore the
first argument:

  - always if it's named 'self'
  - if there's a @classmethod decorator

Finally, it knows that __init__() is supposed to return None.
"""

from __future__ import annotations

import os
import re
from lib2to3.fixer_base import BaseFix
from lib2to3.fixer_util import syms, token, touch_import
from lib2to3.patcomp import compile_pattern
from lib2to3.pytree import Leaf, Node


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.840">def visit_class_pattern(self, o: ClassPattern) -&gt; PatternType:
    current_type = get_proper_type(self.type_context[-1])

    #
    # Check class type
    #
    type_info = o.class_ref.node
    if type_info is None:
        return PatternType(AnyType(TypeOfAny.from_error), AnyType(TypeOfAny.from_error), {})
    if isinstance(type_info, TypeAlias) and not type_info.no_args:
        self.msg.fail(message_registry.CLASS_PATTERN_GENERIC_TYPE_ALIAS, o)
        return self.early_non_match()
    if isinstance(type_info, TypeInfo):
        any_type = AnyType(TypeOfAny.implementation_artifact)
        typ: Type = Instance(type_info, [any_type] * len(type_info.defn.type_vars))
    elif isinstance(type_info, TypeAlias):
        typ = type_info.target
    else:
        if isinstance(type_info, Var):
            name = str(type_info.type)
        else:
            name = type_info.name
        self.msg.fail(message_registry.CLASS_PATTERN_TYPE_REQUIRED.format(name), o.class_ref)
        return self.early_non_match()

    new_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type, [get_type_range(typ)], o, default=current_type
    )
    if is_uninhabited(new_type):
        return self.early_non_match()
    # TODO: Do I need this?
    narrowed_type = narrow_declared_type(current_type, new_type)

    #
    # Convert positional to keyword patterns
    #
    keyword_pairs: list[tuple[str | None, Pattern]] = []
    match_arg_set: set[str] = set()

    captures: dict[Expression, Type] = {}

    if len(o.positionals) != 0:
        if self.should_self_match(typ):
            if len(o.positionals) &gt; 1:
                self.msg.fail(message_registry.CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS, o)
            pattern_type = self.accept(o.positionals[0], narrowed_type)
            if not is_uninhabited(pattern_type.type):
                return PatternType(
                    pattern_type.type,
                    join_types(rest_type, pattern_type.rest_type),
                    pattern_type.captures,
                )
            captures = pattern_type.captures
        else:
            with self.msg.filter_errors() as local_errors:
                match_args_type = analyze_member_access(
                    "__match_args__",
                    typ,
                    o,
                    False,
                    False,
                    False,
                    self.msg,
                    original_type=typ,
                    chk=self.chk,
                )
                has_local_errors = local_errors.has_new_errors()
            if has_local_errors:
                self.msg.fail(message_registry.MISSING_MATCH_ARGS.format(typ), o)
                return self.early_non_match()

            proper_match_args_type = get_proper_type(match_args_type)
            if isinstance(proper_match_args_type, TupleType):
                match_arg_names = get_match_arg_names(proper_match_args_type)

                if len(o.positionals) &gt; len(match_arg_names):
                    self.msg.fail(message_registry.CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS, o)
                    return self.early_non_match()
            else:
                match_arg_names = [None] * len(o.positionals)

            for arg_name, pos in zip(match_arg_names, o.positionals):
                keyword_pairs.append((arg_name, pos))
                if arg_name is not None:
                    match_arg_set.add(arg_name)

    #
    # Check for duplicate patterns
    #
    keyword_arg_set = set()
    has_duplicates = False
    for key, value in zip(o.keyword_keys, o.keyword_values):
        keyword_pairs.append((key, value))
        if key in match_arg_set:
            self.msg.fail(
                message_registry.CLASS_PATTERN_KEYWORD_MATCHES_POSITIONAL.format(key), value
            )
            has_duplicates = True
        elif key in keyword_arg_set:
            self.msg.fail(
                message_registry.CLASS_PATTERN_DUPLICATE_KEYWORD_PATTERN.format(key), value
            )
            has_duplicates = True
        keyword_arg_set.add(key)

    if has_duplicates:
        return self.early_non_match()

    #
    # Check keyword patterns
    #
    can_match = True
    for keyword, pattern in keyword_pairs:
        key_type: Type | None = None
        with self.msg.filter_errors() as local_errors:
            if keyword is not None:
                key_type = analyze_member_access(
                    keyword,
                    narrowed_type,
                    pattern,
                    False,
                    False,
                    False,
                    self.msg,
                    original_type=new_type,
                    chk=self.chk,
                )
            else:
                key_type = AnyType(TypeOfAny.from_error)
            has_local_errors = local_errors.has_new_errors()
        if has_local_errors or key_type is None:
            key_type = AnyType(TypeOfAny.from_error)
            self.msg.fail(
                message_registry.CLASS_PATTERN_UNKNOWN_KEYWORD.format(typ, keyword), pattern
            )

        inner_type, inner_rest_type, inner_captures = self.accept(pattern, key_type)
        if is_uninhabited(inner_type):
            can_match = False
        else:
            self.update_type_map(captures, inner_captures)
            if not is_uninhabited(inner_rest_type):
                rest_type = current_type

    if not can_match:
        new_type = UninhabitedType()
    return PatternType(new_type, rest_type, captures)

</t>
<t tx="ekr.20221004064034.841">def should_self_match(self, typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and typ.type.is_named_tuple:
        return False
    for other in self.self_match_types:
        if is_subtype(typ, other):
            return True
    return False

</t>
<t tx="ekr.20221004064034.842">def can_match_sequence(self, typ: ProperType) -&gt; bool:
    if isinstance(typ, UnionType):
        return any(self.can_match_sequence(get_proper_type(item)) for item in typ.items)
    for other in self.non_sequence_match_types:
        # We have to ignore promotions, as memoryview should match, but bytes,
        # which it can be promoted to, shouldn't
        if is_subtype(typ, other, ignore_promotions=True):
            return False
    sequence = self.chk.named_type("typing.Sequence")
    # If the static type is more general than sequence the actual type could still match
    return is_subtype(typ, sequence) or is_subtype(sequence, typ)

</t>
<t tx="ekr.20221004064034.843">def generate_types_from_names(self, type_names: list[str]) -&gt; list[Type]:
    types: list[Type] = []
    for name in type_names:
        try:
            types.append(self.chk.named_type(name))
        except KeyError as e:
            # Some built in types are not defined in all test cases
            if not name.startswith("builtins."):
                raise e
    return types

</t>
<t tx="ekr.20221004064034.844">def update_type_map(
    self, original_type_map: dict[Expression, Type], extra_type_map: dict[Expression, Type]
) -&gt; None:
    # Calculating this would not be needed if TypeMap directly used literal hashes instead of
    # expressions, as suggested in the TODO above it's definition
    already_captured = {literal_hash(expr) for expr in original_type_map}
    for expr, typ in extra_type_map.items():
        if literal_hash(expr) in already_captured:
            node = get_var(expr)
            self.msg.fail(
                message_registry.MULTIPLE_ASSIGNMENTS_IN_PATTERN.format(node.name), expr
            )
        else:
            original_type_map[expr] = typ

</t>
<t tx="ekr.20221004064034.845">def construct_sequence_child(self, outer_type: Type, inner_type: Type) -&gt; Type:
    """
    If outer_type is a child class of typing.Sequence returns a new instance of
    outer_type, that is a Sequence of inner_type. If outer_type is not a child class of
    typing.Sequence just returns a Sequence of inner_type

    For example:
    construct_sequence_child(List[int], str) = List[str]

    TODO: this doesn't make sense. For example if one has class S(Sequence[int], Generic[T])
    or class T(Sequence[Tuple[T, T]]), there is no way any of those can map to Sequence[str].
    """
    proper_type = get_proper_type(outer_type)
    if isinstance(proper_type, UnionType):
        types = [
            self.construct_sequence_child(item, inner_type)
            for item in proper_type.items
            if self.can_match_sequence(get_proper_type(item))
        ]
        return make_simplified_union(types)
    sequence = self.chk.named_generic_type("typing.Sequence", [inner_type])
    if is_subtype(outer_type, self.chk.named_type("typing.Sequence")):
        proper_type = get_proper_type(outer_type)
        if isinstance(proper_type, TupleType):
            proper_type = tuple_fallback(proper_type)
        assert isinstance(proper_type, Instance)
        empty_type = fill_typevars(proper_type.type)
        partial_type = expand_type_by_instance(empty_type, sequence)
        return expand_type_by_instance(partial_type, proper_type)
    else:
        return sequence

</t>
<t tx="ekr.20221004064034.846">def early_non_match(self) -&gt; PatternType:
    return PatternType(UninhabitedType(), self.type_context[-1], {})


</t>
<t tx="ekr.20221004064034.847">def get_match_arg_names(typ: TupleType) -&gt; list[str | None]:
    args: list[str | None] = []
    for item in typ.items:
        values = try_getting_str_literals_from_type(item)
        if values is None or len(values) != 1:
            args.append(None)
        else:
            args.append(values[0])
    return args


</t>
<t tx="ekr.20221004064034.848">def get_var(expr: Expression) -&gt; Var:
    """
    Warning: this in only true for expressions captured by a match statement.
    Don't call it from anywhere else
    """
    assert isinstance(expr, NameExpr)
    node = expr.node
    assert isinstance(node, Var)
    return node


</t>
<t tx="ekr.20221004064034.849">def get_type_range(typ: Type) -&gt; mypy.checker.TypeRange:
    typ = get_proper_type(typ)
    if (
        isinstance(typ, Instance)
        and typ.last_known_value
        and isinstance(typ.last_known_value.value, bool)
    ):
        typ = typ.last_known_value
    return mypy.checker.TypeRange(typ, is_upper_bound=False)


</t>
<t tx="ekr.20221004064034.85">class FixAnnotate(BaseFix):

    # This fixer is compatible with the bottom matcher.
    BM_compatible = True

    # This fixer shouldn't run by default.
    explicit = True

    # The pattern to match.
    PATTERN = """
              funcdef&lt; 'def' name=any parameters&lt; '(' [args=any] ')' &gt; ':' suite=any+ &gt;
              """

    counter = None if not os.getenv("MAXFIXES") else int(os.getenv("MAXFIXES"))

    @others
</t>
<t tx="ekr.20221004064034.850">def is_uninhabited(typ: Type) -&gt; bool:
    return isinstance(get_proper_type(typ), UninhabitedType)
</t>
<t tx="ekr.20221004064034.851">@path C:/Repos/ekr-mypy2/mypy/
"""
Format expression type checker.

This file is conceptually part of ExpressionChecker and TypeChecker. Main functionality
is located in StringFormatterChecker.check_str_format_call() for '{}'.format(), and in
StringFormatterChecker.check_str_interpolation() for printf-style % interpolation.

Note that although at runtime format strings are parsed using custom parsers,
here we use a regexp-based approach. This way we 99% match runtime behaviour while keeping
implementation simple.
"""

from __future__ import annotations

import re
from typing import TYPE_CHECKING, Callable, Dict, Match, Pattern, Tuple, Union, cast
from typing_extensions import Final, TypeAlias as _TypeAlias

import mypy.errorcodes as codes
from mypy.errors import Errors
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    BytesExpr,
    CallExpr,
    Context,
    DictExpr,
    Expression,
    ExpressionStmt,
    IndexExpr,
    IntExpr,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    StarExpr,
    StrExpr,
    TempNode,
    TupleExpr,
)
from mypy.types import (
    AnyType,
    Instance,
    LiteralType,
    TupleType,
    Type,
    TypeOfAny,
    TypeVarType,
    UnionType,
    get_proper_type,
    get_proper_types,
)

if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.checker
    import mypy.checkexpr

from mypy import message_registry
from mypy.maptype import map_instance_to_supertype
from mypy.messages import MessageBuilder
from mypy.parse import parse
from mypy.subtypes import is_subtype
from mypy.typeops import custom_special_method

FormatStringExpr: _TypeAlias = Union[StrExpr, BytesExpr]
Checkers: _TypeAlias = Tuple[Callable[[Expression], None], Callable[[Type], bool]]
MatchMap: _TypeAlias = Dict[Tuple[int, int], Match[str]]  # span -&gt; match


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.852">def compile_format_re() -&gt; Pattern[str]:
    """Construct regexp to match format conversion specifiers in % interpolation.

    See https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting
    The regexp is intentionally a bit wider to report better errors.
    """
    key_re = r"(\((?P&lt;key&gt;[^)]*)\))?"  # (optional) parenthesised sequence of characters.
    flags_re = r"(?P&lt;flags&gt;[#0\-+ ]*)"  # (optional) sequence of flags.
    width_re = r"(?P&lt;width&gt;[1-9][0-9]*|\*)?"  # (optional) minimum field width (* or numbers).
    precision_re = r"(?:\.(?P&lt;precision&gt;\*|[0-9]+)?)?"  # (optional) . followed by * of numbers.
    length_mod_re = r"[hlL]?"  # (optional) length modifier (unused).
    type_re = r"(?P&lt;type&gt;.)?"  # conversion type.
    format_re = "%" + key_re + flags_re + width_re + precision_re + length_mod_re + type_re
    return re.compile(format_re)


</t>
<t tx="ekr.20221004064034.853">def compile_new_format_re(custom_spec: bool) -&gt; Pattern[str]:
    """Construct regexps to match format conversion specifiers in str.format() calls.

    See After https://docs.python.org/3/library/string.html#formatspec for
    specifications. The regexps are intentionally wider, to report better errors,
    instead of just not matching.
    """

    # Field (optional) is an integer/identifier possibly followed by several .attr and [index].
    field = r"(?P&lt;field&gt;(?P&lt;key&gt;[^.[!:]*)([^:!]+)?)"

    # Conversion (optional) is ! followed by one of letters for forced repr(), str(), or ascii().
    conversion = r"(?P&lt;conversion&gt;![^:])?"

    # Format specification (optional) follows its own mini-language:
    if not custom_spec:
        # Fill and align is valid for all builtin types.
        fill_align = r"(?P&lt;fill_align&gt;.?[&lt;&gt;=^])?"
        # Number formatting options are only valid for int, float, complex, and Decimal,
        # except if only width is given (it is valid for all types).
        # This contains sign, flags (sign, # and/or 0), width, grouping (_ or ,) and precision.
        num_spec = r"(?P&lt;flags&gt;[+\- ]?#?0?)(?P&lt;width&gt;\d+)?[_,]?(?P&lt;precision&gt;\.\d+)?"
        # The last element is type.
        conv_type = r"(?P&lt;type&gt;.)?"  # only some are supported, but we want to give a better error
        format_spec = r"(?P&lt;format_spec&gt;:" + fill_align + num_spec + conv_type + r")?"
    else:
        # Custom types can define their own form_spec using __format__().
        format_spec = r"(?P&lt;format_spec&gt;:.*)?"

    return re.compile(field + conversion + format_spec)


</t>
<t tx="ekr.20221004064034.854">FORMAT_RE: Final = compile_format_re()
FORMAT_RE_NEW: Final = compile_new_format_re(False)
FORMAT_RE_NEW_CUSTOM: Final = compile_new_format_re(True)
DUMMY_FIELD_NAME: Final = "__dummy_name__"

# Types that require either int or float.
NUMERIC_TYPES_OLD: Final = {"d", "i", "o", "u", "x", "X", "e", "E", "f", "F", "g", "G"}
NUMERIC_TYPES_NEW: Final = {"b", "d", "o", "e", "E", "f", "F", "g", "G", "n", "x", "X", "%"}

# These types accept _only_ int.
REQUIRE_INT_OLD: Final = {"o", "x", "X"}
REQUIRE_INT_NEW: Final = {"b", "d", "o", "x", "X"}

# These types fall back to SupportsFloat with % (other fall back to SupportsInt)
FLOAT_TYPES: Final = {"e", "E", "f", "F", "g", "G"}


</t>
<t tx="ekr.20221004064034.855">class ConversionSpecifier:
    @others
</t>
<t tx="ekr.20221004064034.856">def __init__(
    self, match: Match[str], start_pos: int = -1, non_standard_format_spec: bool = False
) -&gt; None:

    self.whole_seq = match.group()
    self.start_pos = start_pos

    m_dict = match.groupdict()
    self.key = m_dict.get("key")

    # Replace unmatched optional groups with empty matches (for convenience).
    self.conv_type = m_dict.get("type", "")
    self.flags = m_dict.get("flags", "")
    self.width = m_dict.get("width", "")
    self.precision = m_dict.get("precision", "")

    # Used only for str.format() calls (it may be custom for types with __format__()).
    self.format_spec = m_dict.get("format_spec")
    self.non_standard_format_spec = non_standard_format_spec
    # Used only for str.format() calls.
    self.conversion = m_dict.get("conversion")
    # Full formatted expression (i.e. key plus following attributes and/or indexes).
    # Used only for str.format() calls.
    self.field = m_dict.get("field")

</t>
<t tx="ekr.20221004064034.857">def has_key(self) -&gt; bool:
    return self.key is not None

</t>
<t tx="ekr.20221004064034.858">def has_star(self) -&gt; bool:
    return self.width == "*" or self.precision == "*"


</t>
<t tx="ekr.20221004064034.859">def parse_conversion_specifiers(format_str: str) -&gt; list[ConversionSpecifier]:
    """Parse c-printf-style format string into list of conversion specifiers."""
    specifiers: list[ConversionSpecifier] = []
    for m in re.finditer(FORMAT_RE, format_str):
        specifiers.append(ConversionSpecifier(m, start_pos=m.start()))
    return specifiers


</t>
<t tx="ekr.20221004064034.86">def transform(self, node, results):
    if FixAnnotate.counter is not None:
        if FixAnnotate.counter &lt;= 0:
            return
    suite = results["suite"]
    children = suite[0].children

    # NOTE: I've reverse-engineered the structure of the parse tree.
    # It's always a list of nodes, the first of which contains the
    # entire suite.  Its children seem to be:
    #
    #   [0] NEWLINE
    #   [1] INDENT
    #   [2...n-2] statements (the first may be a docstring)
    #   [n-1] DEDENT
    #
    # Comments before the suite are part of the INDENT's prefix.
    #
    # "Compact" functions (e.g. "def foo(x, y): return max(x, y)")
    # have a different structure that isn't matched by PATTERN.
    #
    #   print('-'*60)
    #   print(node)
    #   for i, ch in enumerate(children):
    #       print(i, repr(ch.prefix), repr(ch))
    #
    # Check if there's already an annotation.
    for ch in children:
        if ch.prefix.lstrip().startswith("# type:"):
            return  # There's already a # type: comment here; don't change anything.

    # Compute the annotation
    annot = self.make_annotation(node, results)

    # Insert '# type: {annot}' comment.
    # For reference, see lib2to3/fixes/fix_tuple_params.py in stdlib.
    if len(children) &gt;= 2 and children[1].type == token.INDENT:
        children[1].prefix = "{}# type: {}\n{}".format(
            children[1].value, annot, children[1].prefix
        )
        children[1].changed()
        if FixAnnotate.counter is not None:
            FixAnnotate.counter -= 1

    # Also add 'from typing import Any' at the top.
    if "Any" in annot:
        touch_import("typing", "Any", node)

</t>
<t tx="ekr.20221004064034.860">def parse_format_value(
    format_value: str, ctx: Context, msg: MessageBuilder, nested: bool = False
) -&gt; list[ConversionSpecifier] | None:
    """Parse format string into list of conversion specifiers.

    The specifiers may be nested (two levels maximum), in this case they are ordered as
    '{0:{1}}, {2:{3}{4}}'. Return None in case of an error.
    """
    top_targets = find_non_escaped_targets(format_value, ctx, msg)
    if top_targets is None:
        return None

    result: list[ConversionSpecifier] = []
    for target, start_pos in top_targets:
        match = FORMAT_RE_NEW.fullmatch(target)
        if match:
            conv_spec = ConversionSpecifier(match, start_pos=start_pos)
        else:
            custom_match = FORMAT_RE_NEW_CUSTOM.fullmatch(target)
            if custom_match:
                conv_spec = ConversionSpecifier(
                    custom_match, start_pos=start_pos, non_standard_format_spec=True
                )
            else:
                msg.fail(
                    "Invalid conversion specifier in format string",
                    ctx,
                    code=codes.STRING_FORMATTING,
                )
                return None

        if conv_spec.key and ("{" in conv_spec.key or "}" in conv_spec.key):
            msg.fail("Conversion value must not contain { or }", ctx, code=codes.STRING_FORMATTING)
            return None
        result.append(conv_spec)

        # Parse nested conversions that are allowed in format specifier.
        if (
            conv_spec.format_spec
            and conv_spec.non_standard_format_spec
            and ("{" in conv_spec.format_spec or "}" in conv_spec.format_spec)
        ):
            if nested:
                msg.fail(
                    "Formatting nesting must be at most two levels deep",
                    ctx,
                    code=codes.STRING_FORMATTING,
                )
                return None
            sub_conv_specs = parse_format_value(conv_spec.format_spec, ctx, msg, nested=True)
            if sub_conv_specs is None:
                return None
            result.extend(sub_conv_specs)
    return result


</t>
<t tx="ekr.20221004064034.861">def find_non_escaped_targets(
    format_value: str, ctx: Context, msg: MessageBuilder
) -&gt; list[tuple[str, int]] | None:
    """Return list of raw (un-parsed) format specifiers in format string.

    Format specifiers don't include enclosing braces. We don't use regexp for
    this because they don't work well with nested/repeated patterns
    (both greedy and non-greedy), and these are heavily used internally for
    representation of f-strings.

    Return None in case of an error.
    """
    result = []
    next_spec = ""
    pos = 0
    nesting = 0
    while pos &lt; len(format_value):
        c = format_value[pos]
        if not nesting:
            # Skip any paired '{{' and '}}', enter nesting on '{', report error on '}'.
            if c == "{":
                if pos &lt; len(format_value) - 1 and format_value[pos + 1] == "{":
                    pos += 1
                else:
                    nesting = 1
            if c == "}":
                if pos &lt; len(format_value) - 1 and format_value[pos + 1] == "}":
                    pos += 1
                else:
                    msg.fail(
                        "Invalid conversion specifier in format string: unexpected }",
                        ctx,
                        code=codes.STRING_FORMATTING,
                    )
                    return None
        else:
            # Adjust nesting level, then either continue adding chars or move on.
            if c == "{":
                nesting += 1
            if c == "}":
                nesting -= 1
            if nesting:
                next_spec += c
            else:
                result.append((next_spec, pos - len(next_spec)))
                next_spec = ""
        pos += 1
    if nesting:
        msg.fail(
            "Invalid conversion specifier in format string: unmatched {",
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return None
    return result


</t>
<t tx="ekr.20221004064034.862">class StringFormatterChecker:
    """String interpolation/formatter type checker.

    This class works closely together with checker.ExpressionChecker.
    """

    # Some services are provided by a TypeChecker instance.
    chk: mypy.checker.TypeChecker
    # This is shared with TypeChecker, but stored also here for convenience.
    msg: MessageBuilder
    # Some services are provided by a ExpressionChecker instance.
    exprchk: mypy.checkexpr.ExpressionChecker

    @others
</t>
<t tx="ekr.20221004064034.863">def __init__(
    self,
    exprchk: mypy.checkexpr.ExpressionChecker,
    chk: mypy.checker.TypeChecker,
    msg: MessageBuilder,
) -&gt; None:
    """Construct an expression type checker."""
    self.chk = chk
    self.exprchk = exprchk
    self.msg = msg

</t>
<t tx="ekr.20221004064034.864">def check_str_format_call(self, call: CallExpr, format_value: str) -&gt; None:
    """Perform more precise checks for str.format() calls when possible.

    Currently the checks are performed for:
      * Actual string literals
      * Literal types with string values
      * Final names with string values

    The checks that we currently perform:
      * Check generic validity (e.g. unmatched { or }, and {} in invalid positions)
      * Check consistency of specifiers' auto-numbering
      * Verify that replacements can be found for all conversion specifiers,
        and all arguments were used
      * Non-standard format specs are only allowed for types with custom __format__
      * Type check replacements with accessors applied (if any).
      * Verify that specifier type is known and matches replacement type
      * Perform special checks for some specifier types:
        - 'c' requires a single character string
        - 's' must not accept bytes
        - non-empty flags are only allowed for numeric types
    """
    conv_specs = parse_format_value(format_value, call, self.msg)
    if conv_specs is None:
        return
    if not self.auto_generate_keys(conv_specs, call):
        return
    self.check_specs_in_format_call(call, conv_specs, format_value)

</t>
<t tx="ekr.20221004064034.865">def check_specs_in_format_call(
    self, call: CallExpr, specs: list[ConversionSpecifier], format_value: str
) -&gt; None:
    """Perform pairwise checks for conversion specifiers vs their replacements.

    The core logic for format checking is implemented in this method.
    """
    assert all(s.key for s in specs), "Keys must be auto-generated first!"
    replacements = self.find_replacements_in_call(call, [cast(str, s.key) for s in specs])
    assert len(replacements) == len(specs)
    for spec, repl in zip(specs, replacements):
        repl = self.apply_field_accessors(spec, repl, ctx=call)
        actual_type = repl.type if isinstance(repl, TempNode) else self.chk.lookup_type(repl)
        assert actual_type is not None

        # Special case custom formatting.
        if (
            spec.format_spec
            and spec.non_standard_format_spec
            and
            # Exclude "dynamic" specifiers (i.e. containing nested formatting).
            not ("{" in spec.format_spec or "}" in spec.format_spec)
        ):
            if (
                not custom_special_method(actual_type, "__format__", check_all=True)
                or spec.conversion
            ):
                # TODO: add support for some custom specs like datetime?
                self.msg.fail(
                    "Unrecognized format" ' specification "{}"'.format(spec.format_spec[1:]),
                    call,
                    code=codes.STRING_FORMATTING,
                )
                continue
        # Adjust expected and actual types.
        if not spec.conv_type:
            expected_type: Type | None = AnyType(TypeOfAny.special_form)
        else:
            assert isinstance(call.callee, MemberExpr)
            if isinstance(call.callee.expr, StrExpr):
                format_str = call.callee.expr
            else:
                format_str = StrExpr(format_value)
            expected_type = self.conversion_type(
                spec.conv_type, call, format_str, format_call=True
            )
        if spec.conversion is not None:
            # If the explicit conversion is given, then explicit conversion is called _first_.
            if spec.conversion[1] not in "rsa":
                self.msg.fail(
                    'Invalid conversion type "{}",'
                    ' must be one of "r", "s" or "a"'.format(spec.conversion[1]),
                    call,
                    code=codes.STRING_FORMATTING,
                )
            actual_type = self.named_type("builtins.str")

        # Perform the checks for given types.
        if expected_type is None:
            continue

        a_type = get_proper_type(actual_type)
        actual_items = (
            get_proper_types(a_type.items) if isinstance(a_type, UnionType) else [a_type]
        )
        for a_type in actual_items:
            if custom_special_method(a_type, "__format__"):
                continue
            self.check_placeholder_type(a_type, expected_type, call)
            self.perform_special_format_checks(spec, call, repl, a_type, expected_type)

</t>
<t tx="ekr.20221004064034.866">def perform_special_format_checks(
    self,
    spec: ConversionSpecifier,
    call: CallExpr,
    repl: Expression,
    actual_type: Type,
    expected_type: Type,
) -&gt; None:
    # TODO: try refactoring to combine this logic with % formatting.
    if spec.conv_type == "c":
        if isinstance(repl, (StrExpr, BytesExpr)) and len(repl.value) != 1:
            self.msg.requires_int_or_char(call, format_call=True)
        c_typ = get_proper_type(self.chk.lookup_type(repl))
        if isinstance(c_typ, Instance) and c_typ.last_known_value:
            c_typ = c_typ.last_known_value
        if isinstance(c_typ, LiteralType) and isinstance(c_typ.value, str):
            if len(c_typ.value) != 1:
                self.msg.requires_int_or_char(call, format_call=True)
    if (not spec.conv_type or spec.conv_type == "s") and not spec.conversion:
        if has_type_component(actual_type, "builtins.bytes") and not custom_special_method(
            actual_type, "__str__"
        ):
            self.msg.fail(
                'On Python 3 formatting "b\'abc\'" with "{}" '
                'produces "b\'abc\'", not "abc"; '
                'use "{!r}" if this is desired behavior',
                call,
                code=codes.STR_BYTES_PY3,
            )
    if spec.flags:
        numeric_types = UnionType(
            [self.named_type("builtins.int"), self.named_type("builtins.float")]
        )
        if (
            spec.conv_type
            and spec.conv_type not in NUMERIC_TYPES_NEW
            or not spec.conv_type
            and not is_subtype(actual_type, numeric_types)
            and not custom_special_method(actual_type, "__format__")
        ):
            self.msg.fail(
                "Numeric flags are only allowed for numeric types",
                call,
                code=codes.STRING_FORMATTING,
            )

</t>
<t tx="ekr.20221004064034.867">def find_replacements_in_call(self, call: CallExpr, keys: list[str]) -&gt; list[Expression]:
    """Find replacement expression for every specifier in str.format() call.

    In case of an error use TempNode(AnyType).
    """
    result: list[Expression] = []
    used: set[Expression] = set()
    for key in keys:
        if key.isdecimal():
            expr = self.get_expr_by_position(int(key), call)
            if not expr:
                self.msg.fail(
                    "Cannot find replacement for positional"
                    " format specifier {}".format(key),
                    call,
                    code=codes.STRING_FORMATTING,
                )
                expr = TempNode(AnyType(TypeOfAny.from_error))
        else:
            expr = self.get_expr_by_name(key, call)
            if not expr:
                self.msg.fail(
                    "Cannot find replacement for named" ' format specifier "{}"'.format(key),
                    call,
                    code=codes.STRING_FORMATTING,
                )
                expr = TempNode(AnyType(TypeOfAny.from_error))
        result.append(expr)
        if not isinstance(expr, TempNode):
            used.add(expr)
    # Strictly speaking not using all replacements is not a type error, but most likely
    # a typo in user code, so we show an error like we do for % formatting.
    total_explicit = len([kind for kind in call.arg_kinds if kind in (ARG_POS, ARG_NAMED)])
    if len(used) &lt; total_explicit:
        self.msg.too_many_string_formatting_arguments(call)
    return result

</t>
<t tx="ekr.20221004064034.868">def get_expr_by_position(self, pos: int, call: CallExpr) -&gt; Expression | None:
    """Get positional replacement expression from '{0}, {1}'.format(x, y, ...) call.

    If the type is from *args, return TempNode(&lt;item type&gt;). Return None in case of
    an error.
    """
    pos_args = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_POS]
    if pos &lt; len(pos_args):
        return pos_args[pos]
    star_args = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_STAR]
    if not star_args:
        return None

    # Fall back to *args when present in call.
    star_arg = star_args[0]
    varargs_type = get_proper_type(self.chk.lookup_type(star_arg))
    if not isinstance(varargs_type, Instance) or not varargs_type.type.has_base(
        "typing.Sequence"
    ):
        # Error should be already reported.
        return TempNode(AnyType(TypeOfAny.special_form))
    iter_info = self.chk.named_generic_type(
        "typing.Sequence", [AnyType(TypeOfAny.special_form)]
    ).type
    return TempNode(map_instance_to_supertype(varargs_type, iter_info).args[0])

</t>
<t tx="ekr.20221004064034.869">def get_expr_by_name(self, key: str, call: CallExpr) -&gt; Expression | None:
    """Get named replacement expression from '{name}'.format(name=...) call.

    If the type is from **kwargs, return TempNode(&lt;item type&gt;). Return None in case of
    an error.
    """
    named_args = [
        arg
        for arg, kind, name in zip(call.args, call.arg_kinds, call.arg_names)
        if kind == ARG_NAMED and name == key
    ]
    if named_args:
        return named_args[0]
    star_args_2 = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_STAR2]
    if not star_args_2:
        return None
    star_arg_2 = star_args_2[0]
    kwargs_type = get_proper_type(self.chk.lookup_type(star_arg_2))
    if not isinstance(kwargs_type, Instance) or not kwargs_type.type.has_base(
        "typing.Mapping"
    ):
        # Error should be already reported.
        return TempNode(AnyType(TypeOfAny.special_form))
    any_type = AnyType(TypeOfAny.special_form)
    mapping_info = self.chk.named_generic_type("typing.Mapping", [any_type, any_type]).type
    return TempNode(map_instance_to_supertype(kwargs_type, mapping_info).args[1])

</t>
<t tx="ekr.20221004064034.87">def make_annotation(self, node, results):
    name = results["name"]
    assert isinstance(name, Leaf), repr(name)
    assert name.type == token.NAME, repr(name)
    decorators = self.get_decorators(node)
    is_method = self.is_method(node)
    if name.value == "__init__" or not self.has_return_exprs(node):
        restype = "None"
    else:
        restype = "Any"
    args = results.get("args")
    argtypes = []
    if isinstance(args, Node):
        children = args.children
    elif isinstance(args, Leaf):
        children = [args]
    else:
        children = []
    # Interpret children according to the following grammar:
    # (('*'|'**')? NAME ['=' expr] ','?)*
    stars = inferred_type = ""
    in_default = False
    at_start = True
    for child in children:
        if isinstance(child, Leaf):
            if child.value in ("*", "**"):
                stars += child.value
            elif child.type == token.NAME and not in_default:
                if not is_method or not at_start or "staticmethod" in decorators:
                    inferred_type = "Any"
                else:
                    # Always skip the first argument if it's named 'self'.
                    # Always skip the first argument of a class method.
                    if child.value == "self" or "classmethod" in decorators:
                        pass
                    else:
                        inferred_type = "Any"
            elif child.value == "=":
                in_default = True
            elif in_default and child.value != ",":
                if child.type == token.NUMBER:
                    if re.match(r"\d+[lL]?$", child.value):
                        inferred_type = "int"
                    else:
                        inferred_type = "float"  # TODO: complex?
                elif child.type == token.STRING:
                    if child.value.startswith(("u", "U")):
                        inferred_type = "unicode"
                    else:
                        inferred_type = "str"
                elif child.type == token.NAME and child.value in ("True", "False"):
                    inferred_type = "bool"
            elif child.value == ",":
                if inferred_type:
                    argtypes.append(stars + inferred_type)
                # Reset
                stars = inferred_type = ""
                in_default = False
                at_start = False
    if inferred_type:
        argtypes.append(stars + inferred_type)
    return "(" + ", ".join(argtypes) + ") -&gt; " + restype

</t>
<t tx="ekr.20221004064034.870">def auto_generate_keys(self, all_specs: list[ConversionSpecifier], ctx: Context) -&gt; bool:
    """Translate '{} {name} {}' to '{0} {name} {1}'.

    Return True if generation was successful, otherwise report an error and return false.
    """
    some_defined = any(s.key and s.key.isdecimal() for s in all_specs)
    all_defined = all(bool(s.key) for s in all_specs)
    if some_defined and not all_defined:
        self.msg.fail(
            "Cannot combine automatic field numbering and manual field specification",
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return False
    if all_defined:
        return True
    next_index = 0
    for spec in all_specs:
        if not spec.key:
            str_index = str(next_index)
            spec.key = str_index
            # Update also the full field (i.e. turn {.x} into {0.x}).
            if not spec.field:
                spec.field = str_index
            else:
                spec.field = str_index + spec.field
            next_index += 1
    return True

</t>
<t tx="ekr.20221004064034.871">def apply_field_accessors(
    self, spec: ConversionSpecifier, repl: Expression, ctx: Context
) -&gt; Expression:
    """Transform and validate expr in '{.attr[item]}'.format(expr) into expr.attr['item'].

    If validation fails, return TempNode(AnyType).
    """
    assert spec.key, "Keys must be auto-generated first!"
    if spec.field == spec.key:
        return repl
    assert spec.field

    temp_errors = Errors()
    dummy = DUMMY_FIELD_NAME + spec.field[len(spec.key) :]
    temp_ast: Node = parse(
        dummy, fnam="&lt;format&gt;", module=None, options=self.chk.options, errors=temp_errors
    )
    if temp_errors.is_errors():
        self.msg.fail(
            f'Syntax error in format specifier "{spec.field}"',
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return TempNode(AnyType(TypeOfAny.from_error))

    # These asserts are guaranteed by the original regexp.
    assert isinstance(temp_ast, MypyFile)
    temp_ast = temp_ast.defs[0]
    assert isinstance(temp_ast, ExpressionStmt)
    temp_ast = temp_ast.expr
    if not self.validate_and_transform_accessors(temp_ast, repl, spec, ctx=ctx):
        return TempNode(AnyType(TypeOfAny.from_error))

    # Check if there are any other errors (like missing members).
    # TODO: fix column to point to actual start of the format specifier _within_ string.
    temp_ast.line = ctx.line
    temp_ast.column = ctx.column
    self.exprchk.accept(temp_ast)
    return temp_ast

</t>
<t tx="ekr.20221004064034.872">def validate_and_transform_accessors(
    self,
    temp_ast: Expression,
    original_repl: Expression,
    spec: ConversionSpecifier,
    ctx: Context,
) -&gt; bool:
    """Validate and transform (in-place) format field accessors.

    On error, report it and return False. The transformations include replacing the dummy
    variable with actual replacement expression and translating any name expressions in an
    index into strings, so that this will work:

        class User(TypedDict):
            name: str
            id: int
        u: User
        '{[id]:d} -&gt; {[name]}'.format(u)
    """
    if not isinstance(temp_ast, (MemberExpr, IndexExpr)):
        self.msg.fail(
            "Only index and member expressions are allowed in"
            ' format field accessors; got "{}"'.format(spec.field),
            ctx,
            code=codes.STRING_FORMATTING,
        )
        return False
    if isinstance(temp_ast, MemberExpr):
        node = temp_ast.expr
    else:
        node = temp_ast.base
        if not isinstance(temp_ast.index, (NameExpr, IntExpr)):
            assert spec.key, "Call this method only after auto-generating keys!"
            assert spec.field
            self.msg.fail(
                "Invalid index expression in format field"
                ' accessor "{}"'.format(spec.field[len(spec.key) :]),
                ctx,
                code=codes.STRING_FORMATTING,
            )
            return False
        if isinstance(temp_ast.index, NameExpr):
            temp_ast.index = StrExpr(temp_ast.index.name)
    if isinstance(node, NameExpr) and node.name == DUMMY_FIELD_NAME:
        # Replace it with the actual replacement expression.
        assert isinstance(temp_ast, (IndexExpr, MemberExpr))  # XXX: this is redundant
        if isinstance(temp_ast, IndexExpr):
            temp_ast.base = original_repl
        else:
            temp_ast.expr = original_repl
        return True
    node.line = ctx.line
    node.column = ctx.column
    return self.validate_and_transform_accessors(
        node, original_repl=original_repl, spec=spec, ctx=ctx
    )

</t>
<t tx="ekr.20221004064034.873"># TODO: In Python 3, the bytes formatting has a more restricted set of options
#       compared to string formatting.
def check_str_interpolation(self, expr: FormatStringExpr, replacements: Expression) -&gt; Type:
    """Check the types of the 'replacements' in a string interpolation
    expression: str % replacements.
    """
    self.exprchk.accept(expr)
    specifiers = parse_conversion_specifiers(expr.value)
    has_mapping_keys = self.analyze_conversion_specifiers(specifiers, expr)
    if isinstance(expr, BytesExpr) and self.chk.options.python_version &lt; (3, 5):
        self.msg.fail(
            "Bytes formatting is only supported in Python 3.5 and later",
            replacements,
            code=codes.STRING_FORMATTING,
        )
        return AnyType(TypeOfAny.from_error)

    if has_mapping_keys is None:
        pass  # Error was reported
    elif has_mapping_keys:
        self.check_mapping_str_interpolation(specifiers, replacements, expr)
    else:
        self.check_simple_str_interpolation(specifiers, replacements, expr)

    if isinstance(expr, BytesExpr):
        return self.named_type("builtins.bytes")
    elif isinstance(expr, StrExpr):
        return self.named_type("builtins.str")
    else:
        assert False

</t>
<t tx="ekr.20221004064034.874">def analyze_conversion_specifiers(
    self, specifiers: list[ConversionSpecifier], context: Context
) -&gt; bool | None:
    has_star = any(specifier.has_star() for specifier in specifiers)
    has_key = any(specifier.has_key() for specifier in specifiers)
    all_have_keys = all(
        specifier.has_key() or specifier.conv_type == "%" for specifier in specifiers
    )

    if has_key and has_star:
        self.msg.string_interpolation_with_star_and_key(context)
        return None
    if has_key and not all_have_keys:
        self.msg.string_interpolation_mixing_key_and_non_keys(context)
        return None
    return has_key

</t>
<t tx="ekr.20221004064034.875">def check_simple_str_interpolation(
    self,
    specifiers: list[ConversionSpecifier],
    replacements: Expression,
    expr: FormatStringExpr,
) -&gt; None:
    """Check % string interpolation with positional specifiers '%s, %d' % ('yes, 42')."""
    checkers = self.build_replacement_checkers(specifiers, replacements, expr)
    if checkers is None:
        return

    rhs_type = get_proper_type(self.accept(replacements))
    rep_types: list[Type] = []
    if isinstance(rhs_type, TupleType):
        rep_types = rhs_type.items
    elif isinstance(rhs_type, AnyType):
        return
    elif isinstance(rhs_type, Instance) and rhs_type.type.fullname == "builtins.tuple":
        # Assume that an arbitrary-length tuple has the right number of items.
        rep_types = [rhs_type.args[0]] * len(checkers)
    elif isinstance(rhs_type, UnionType):
        for typ in rhs_type.relevant_items():
            temp_node = TempNode(typ)
            temp_node.line = replacements.line
            self.check_simple_str_interpolation(specifiers, temp_node, expr)
        return
    else:
        rep_types = [rhs_type]

    if len(checkers) &gt; len(rep_types):
        # Only check the fix-length Tuple type. Other Iterable types would skip.
        if is_subtype(rhs_type, self.chk.named_type("typing.Iterable")) and not isinstance(
            rhs_type, TupleType
        ):
            return
        else:
            self.msg.too_few_string_formatting_arguments(replacements)
    elif len(checkers) &lt; len(rep_types):
        self.msg.too_many_string_formatting_arguments(replacements)
    else:
        if len(checkers) == 1:
            check_node, check_type = checkers[0]
            if isinstance(rhs_type, TupleType) and len(rhs_type.items) == 1:
                check_type(rhs_type.items[0])
            else:
                check_node(replacements)
        elif isinstance(replacements, TupleExpr) and not any(
            isinstance(item, StarExpr) for item in replacements.items
        ):
            for checks, rep_node in zip(checkers, replacements.items):
                check_node, check_type = checks
                check_node(rep_node)
        else:
            for checks, rep_type in zip(checkers, rep_types):
                check_node, check_type = checks
                check_type(rep_type)

</t>
<t tx="ekr.20221004064034.876">def check_mapping_str_interpolation(
    self,
    specifiers: list[ConversionSpecifier],
    replacements: Expression,
    expr: FormatStringExpr,
) -&gt; None:
    """Check % string interpolation with names specifiers '%(name)s' % {'name': 'John'}."""
    if isinstance(replacements, DictExpr) and all(
        isinstance(k, (StrExpr, BytesExpr)) for k, v in replacements.items
    ):
        mapping: dict[str, Type] = {}
        for k, v in replacements.items:
            if isinstance(expr, BytesExpr):
                # Special case: for bytes formatting keys must be bytes.
                if not isinstance(k, BytesExpr):
                    self.msg.fail(
                        "Dictionary keys in bytes formatting must be bytes, not strings",
                        expr,
                        code=codes.STRING_FORMATTING,
                    )
            key_str = cast(FormatStringExpr, k).value
            mapping[key_str] = self.accept(v)

        for specifier in specifiers:
            if specifier.conv_type == "%":
                # %% is allowed in mappings, no checking is required
                continue
            assert specifier.key is not None
            if specifier.key not in mapping:
                self.msg.key_not_in_mapping(specifier.key, replacements)
                return
            rep_type = mapping[specifier.key]
            assert specifier.conv_type is not None
            expected_type = self.conversion_type(specifier.conv_type, replacements, expr)
            if expected_type is None:
                return
            self.chk.check_subtype(
                rep_type,
                expected_type,
                replacements,
                message_registry.INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION,
                "expression has type",
                f"placeholder with key '{specifier.key}' has type",
                code=codes.STRING_FORMATTING,
            )
            if specifier.conv_type == "s":
                self.check_s_special_cases(expr, rep_type, expr)
    else:
        rep_type = self.accept(replacements)
        dict_type = self.build_dict_type(expr)
        self.chk.check_subtype(
            rep_type,
            dict_type,
            replacements,
            message_registry.FORMAT_REQUIRES_MAPPING,
            "expression has type",
            "expected type for mapping is",
            code=codes.STRING_FORMATTING,
        )

</t>
<t tx="ekr.20221004064034.877">def build_dict_type(self, expr: FormatStringExpr) -&gt; Type:
    """Build expected mapping type for right operand in % formatting."""
    any_type = AnyType(TypeOfAny.special_form)
    if isinstance(expr, BytesExpr):
        bytes_type = self.chk.named_generic_type("builtins.bytes", [])
        return self.chk.named_generic_type("typing.Mapping", [bytes_type, any_type])
    elif isinstance(expr, StrExpr):
        str_type = self.chk.named_generic_type("builtins.str", [])
        return self.chk.named_generic_type("typing.Mapping", [str_type, any_type])
    else:
        assert False, "Unreachable"

</t>
<t tx="ekr.20221004064034.878">def build_replacement_checkers(
    self, specifiers: list[ConversionSpecifier], context: Context, expr: FormatStringExpr
) -&gt; list[Checkers] | None:
    checkers: list[Checkers] = []
    for specifier in specifiers:
        checker = self.replacement_checkers(specifier, context, expr)
        if checker is None:
            return None
        checkers.extend(checker)
    return checkers

</t>
<t tx="ekr.20221004064034.879">def replacement_checkers(
    self, specifier: ConversionSpecifier, context: Context, expr: FormatStringExpr
) -&gt; list[Checkers] | None:
    """Returns a list of tuples of two functions that check whether a replacement is
    of the right type for the specifier. The first function takes a node and checks
    its type in the right type context. The second function just checks a type.
    """
    checkers: list[Checkers] = []

    if specifier.width == "*":
        checkers.append(self.checkers_for_star(context))
    if specifier.precision == "*":
        checkers.append(self.checkers_for_star(context))

    if specifier.conv_type == "c":
        c = self.checkers_for_c_type(specifier.conv_type, context, expr)
        if c is None:
            return None
        checkers.append(c)
    elif specifier.conv_type is not None and specifier.conv_type != "%":
        c = self.checkers_for_regular_type(specifier.conv_type, context, expr)
        if c is None:
            return None
        checkers.append(c)
    return checkers

</t>
<t tx="ekr.20221004064034.88"># The parse tree has a different shape when there is a single
# decorator vs. when there are multiple decorators.
DECORATED = "decorated&lt; (d=decorator | decorators&lt; dd=decorator+ &gt;) funcdef &gt;"
decorated = compile_pattern(DECORATED)

</t>
<t tx="ekr.20221004064034.880">def checkers_for_star(self, context: Context) -&gt; Checkers:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with a star in a conversion specifier.
    """
    expected = self.named_type("builtins.int")

    def check_type(type: Type) -&gt; bool:
        expected = self.named_type("builtins.int")
        return self.chk.check_subtype(
            type, expected, context, "* wants int", code=codes.STRING_FORMATTING
        )

    def check_expr(expr: Expression) -&gt; None:
        type = self.accept(expr, expected)
        check_type(type)

    return check_expr, check_type

</t>
<t tx="ekr.20221004064034.881">def check_placeholder_type(self, typ: Type, expected_type: Type, context: Context) -&gt; bool:
    return self.chk.check_subtype(
        typ,
        expected_type,
        context,
        message_registry.INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION,
        "expression has type",
        "placeholder has type",
        code=codes.STRING_FORMATTING,
    )

</t>
<t tx="ekr.20221004064034.882">def checkers_for_regular_type(
    self, conv_type: str, context: Context, expr: FormatStringExpr
) -&gt; Checkers | None:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with 'type'. Return None in case of an error.
    """
    expected_type = self.conversion_type(conv_type, context, expr)
    if expected_type is None:
        return None

    def check_type(typ: Type) -&gt; bool:
        assert expected_type is not None
        ret = self.check_placeholder_type(typ, expected_type, context)
        if ret and conv_type == "s":
            ret = self.check_s_special_cases(expr, typ, context)
        return ret

    def check_expr(expr: Expression) -&gt; None:
        type = self.accept(expr, expected_type)
        check_type(type)

    return check_expr, check_type

</t>
<t tx="ekr.20221004064034.883">def check_s_special_cases(self, expr: FormatStringExpr, typ: Type, context: Context) -&gt; bool:
    """Additional special cases for %s in bytes vs string context."""
    if isinstance(expr, StrExpr):
        # Couple special cases for string formatting.
        if has_type_component(typ, "builtins.bytes"):
            self.msg.fail(
                'On Python 3 formatting "b\'abc\'" with "%s" '
                'produces "b\'abc\'", not "abc"; '
                'use "%r" if this is desired behavior',
                context,
                code=codes.STR_BYTES_PY3,
            )
            return False
    if isinstance(expr, BytesExpr):
        # A special case for bytes formatting: b'%s' actually requires bytes on Python 3.
        if has_type_component(typ, "builtins.str"):
            self.msg.fail(
                "On Python 3 b'%s' requires bytes, not string",
                context,
                code=codes.STRING_FORMATTING,
            )
            return False
    return True

</t>
<t tx="ekr.20221004064034.884">def checkers_for_c_type(
    self, type: str, context: Context, format_expr: FormatStringExpr
) -&gt; Checkers | None:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with 'type' that is a character type.
    """
    expected_type = self.conversion_type(type, context, format_expr)
    if expected_type is None:
        return None

    def check_type(type: Type) -&gt; bool:
        assert expected_type is not None
        if isinstance(format_expr, BytesExpr):
            err_msg = '"%c" requires an integer in range(256) or a single byte'
        else:
            err_msg = '"%c" requires int or char'
        return self.chk.check_subtype(
            type,
            expected_type,
            context,
            err_msg,
            "expression has type",
            code=codes.STRING_FORMATTING,
        )

    def check_expr(expr: Expression) -&gt; None:
        """int, or str with length 1"""
        type = self.accept(expr, expected_type)
        # We need further check with expr to make sure that
        # it has exact one char or one single byte.
        if check_type(type):
            # Python 3 doesn't support b'%c' % str
            if (
                isinstance(format_expr, BytesExpr)
                and isinstance(expr, BytesExpr)
                and len(expr.value) != 1
            ):
                self.msg.requires_int_or_single_byte(context)
            elif isinstance(expr, (StrExpr, BytesExpr)) and len(expr.value) != 1:
                self.msg.requires_int_or_char(context)

    return check_expr, check_type

</t>
<t tx="ekr.20221004064034.885">def conversion_type(
    self, p: str, context: Context, expr: FormatStringExpr, format_call: bool = False
) -&gt; Type | None:
    """Return the type that is accepted for a string interpolation conversion specifier type.

    Note that both Python's float (e.g. %f) and integer (e.g. %d)
    specifier types accept both float and integers.

    The 'format_call' argument indicates whether this type came from % interpolation or from
    a str.format() call, the meaning of few formatting types are different.
    """
    NUMERIC_TYPES = NUMERIC_TYPES_NEW if format_call else NUMERIC_TYPES_OLD
    INT_TYPES = REQUIRE_INT_NEW if format_call else REQUIRE_INT_OLD
    if p == "b" and not format_call:
        if self.chk.options.python_version &lt; (3, 5):
            self.msg.fail(
                'Format character "b" is only supported in Python 3.5 and later',
                context,
                code=codes.STRING_FORMATTING,
            )
            return None
        if not isinstance(expr, BytesExpr):
            self.msg.fail(
                'Format character "b" is only supported on bytes patterns',
                context,
                code=codes.STRING_FORMATTING,
            )
            return None
        return self.named_type("builtins.bytes")
    elif p == "a":
        # TODO: return type object?
        return AnyType(TypeOfAny.special_form)
    elif p in ["s", "r"]:
        return AnyType(TypeOfAny.special_form)
    elif p in NUMERIC_TYPES:
        if p in INT_TYPES:
            numeric_types = [self.named_type("builtins.int")]
        else:
            numeric_types = [
                self.named_type("builtins.int"),
                self.named_type("builtins.float"),
            ]
            if not format_call:
                if p in FLOAT_TYPES:
                    numeric_types.append(self.named_type("typing.SupportsFloat"))
                else:
                    numeric_types.append(self.named_type("typing.SupportsInt"))
        return UnionType.make_union(numeric_types)
    elif p in ["c"]:
        if isinstance(expr, BytesExpr):
            return UnionType(
                [self.named_type("builtins.int"), self.named_type("builtins.bytes")]
            )
        else:
            return UnionType(
                [self.named_type("builtins.int"), self.named_type("builtins.str")]
            )
    else:
        self.msg.unsupported_placeholder(p, context)
        return None

</t>
<t tx="ekr.20221004064034.886">#
# Helpers
#

</t>
<t tx="ekr.20221004064034.887">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with type given by the name and no type
    arguments. Alias for TypeChecker.named_type.
    """
    return self.chk.named_type(name)

</t>
<t tx="ekr.20221004064034.888">def accept(self, expr: Expression, context: Type | None = None) -&gt; Type:
    """Type check a node. Alias for TypeChecker.accept."""
    return self.chk.expr_checker.accept(expr, context)


</t>
<t tx="ekr.20221004064034.889">def has_type_component(typ: Type, fullname: str) -&gt; bool:
    """Is this a specific instance type, or a union that contains it?

    We use this ad-hoc function instead of a proper visitor or subtype check
    because some str vs bytes errors are strictly speaking not runtime errors,
    but rather highly counter-intuitive behavior. This is similar to what is used for
    --strict-equality.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return typ.type.has_base(fullname)
    elif isinstance(typ, TypeVarType):
        return has_type_component(typ.upper_bound, fullname) or any(
            has_type_component(v, fullname) for v in typ.values
        )
    elif isinstance(typ, UnionType):
        return any(has_type_component(t, fullname) for t in typ.relevant_items())
    return False
</t>
<t tx="ekr.20221004064034.89">def get_decorators(self, node):
    """Return a list of decorators found on a function definition.

    This is a list of strings; only simple decorators
    (e.g. @staticmethod) are returned.

    If the function is undecorated or only non-simple decorators
    are found, return [].
    """
    if node.parent is None:
        return []
    results = {}
    if not self.decorated.match(node.parent, results):
        return []
    decorators = results.get("dd") or [results["d"]]
    decs = []
    for d in decorators:
        for child in d.children:
            if isinstance(child, Leaf) and child.type == token.NAME:
                decs.append(child.value)
    return decs

</t>
<t tx="ekr.20221004064034.890">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import argparse
import configparser
import glob as fileglob
import os
import re
import sys
from io import StringIO

from mypy.errorcodes import error_codes

if sys.version_info &gt;= (3, 11):
    import tomllib
else:
    import tomli as tomllib

from typing import (
    Any,
    Callable,
    Dict,
    Iterable,
    List,
    Mapping,
    MutableMapping,
    Sequence,
    TextIO,
    Tuple,
    Union,
)
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy import defaults
from mypy.options import PER_MODULE_OPTIONS, Options

_CONFIG_VALUE_TYPES: _TypeAlias = Union[
    str, bool, int, float, Dict[str, str], List[str], Tuple[int, int],
]
_INI_PARSER_CALLABLE: _TypeAlias = Callable[[Any], _CONFIG_VALUE_TYPES]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.891">def parse_version(v: str | float) -&gt; tuple[int, int]:
    m = re.match(r"\A(\d)\.(\d+)\Z", str(v))
    if not m:
        raise argparse.ArgumentTypeError(f"Invalid python version '{v}' (expected format: 'x.y')")
    major, minor = int(m.group(1)), int(m.group(2))
    if major == 2 and minor == 7:
        pass  # Error raised elsewhere
    elif major == 3:
        if minor &lt; defaults.PYTHON3_VERSION_MIN[1]:
            msg = "Python 3.{} is not supported (must be {}.{} or higher)".format(
                minor, *defaults.PYTHON3_VERSION_MIN
            )

            if isinstance(v, float):
                msg += ". You may need to put quotes around your Python version"

            raise argparse.ArgumentTypeError(msg)
    else:
        raise argparse.ArgumentTypeError(
            f"Python major version '{major}' out of range (must be 3)"
        )
    return major, minor


</t>
<t tx="ekr.20221004064034.892">def try_split(v: str | Sequence[str], split_regex: str = "[,]") -&gt; list[str]:
    """Split and trim a str or list of str into a list of str"""
    if isinstance(v, str):
        return [p.strip() for p in re.split(split_regex, v)]

    return [p.strip() for p in v]


</t>
<t tx="ekr.20221004064034.893">def validate_codes(codes: list[str]) -&gt; list[str]:
    invalid_codes = set(codes) - set(error_codes.keys())
    if invalid_codes:
        raise argparse.ArgumentTypeError(
            f"Invalid error code(s): {', '.join(sorted(invalid_codes))}"
        )
    return codes


</t>
<t tx="ekr.20221004064034.894">def expand_path(path: str) -&gt; str:
    """Expand the user home directory and any environment variables contained within
    the provided path.
    """

    return os.path.expandvars(os.path.expanduser(path))


</t>
<t tx="ekr.20221004064034.895">def str_or_array_as_list(v: str | Sequence[str]) -&gt; list[str]:
    if isinstance(v, str):
        return [v.strip()] if v.strip() else []
    return [p.strip() for p in v if p.strip()]


</t>
<t tx="ekr.20221004064034.896">def split_and_match_files_list(paths: Sequence[str]) -&gt; list[str]:
    """Take a list of files/directories (with support for globbing through the glob library).

    Where a path/glob matches no file, we still include the raw path in the resulting list.

    Returns a list of file paths
    """
    expanded_paths = []

    for path in paths:
        path = expand_path(path.strip())
        globbed_files = fileglob.glob(path, recursive=True)
        if globbed_files:
            expanded_paths.extend(globbed_files)
        else:
            expanded_paths.append(path)

    return expanded_paths


</t>
<t tx="ekr.20221004064034.897">def split_and_match_files(paths: str) -&gt; list[str]:
    """Take a string representing a list of files/directories (with support for globbing
    through the glob library).

    Where a path/glob matches no file, we still include the raw path in the resulting list.

    Returns a list of file paths
    """

    return split_and_match_files_list(paths.split(","))


</t>
<t tx="ekr.20221004064034.898">def check_follow_imports(choice: str) -&gt; str:
    choices = ["normal", "silent", "skip", "error"]
    if choice not in choices:
        raise argparse.ArgumentTypeError(
            "invalid choice '{}' (choose from {})".format(
                choice, ", ".join(f"'{x}'" for x in choices)
            )
        )
    return choice


</t>
<t tx="ekr.20221004064034.899"># For most options, the type of the default value set in options.py is
# sufficient, and we don't have to do anything here.  This table
# exists to specify types for values initialized to None or container
# types.
ini_config_types: Final[dict[str, _INI_PARSER_CALLABLE]] = {
    "python_version": parse_version,
    "custom_typing_module": str,
    "custom_typeshed_dir": expand_path,
    "mypy_path": lambda s: [expand_path(p.strip()) for p in re.split("[,:]", s)],
    "files": split_and_match_files,
    "quickstart_file": expand_path,
    "junit_xml": expand_path,
    "follow_imports": check_follow_imports,
    "no_site_packages": bool,
    "plugins": lambda s: [p.strip() for p in s.split(",")],
    "always_true": lambda s: [p.strip() for p in s.split(",")],
    "always_false": lambda s: [p.strip() for p in s.split(",")],
    "disable_error_code": lambda s: validate_codes([p.strip() for p in s.split(",")]),
    "enable_error_code": lambda s: validate_codes([p.strip() for p in s.split(",")]),
    "package_root": lambda s: [p.strip() for p in s.split(",")],
    "cache_dir": expand_path,
    "python_executable": expand_path,
    "strict": bool,
    "exclude": lambda s: [s.strip()],
}

# Reuse the ini_config_types and overwrite the diff
toml_config_types: Final[dict[str, _INI_PARSER_CALLABLE]] = ini_config_types.copy()
toml_config_types.update(
    {
        "python_version": parse_version,
        "mypy_path": lambda s: [expand_path(p) for p in try_split(s, "[,:]")],
        "files": lambda s: split_and_match_files_list(try_split(s)),
        "follow_imports": lambda s: check_follow_imports(str(s)),
        "plugins": try_split,
        "always_true": try_split,
        "always_false": try_split,
        "disable_error_code": lambda s: validate_codes(try_split(s)),
        "enable_error_code": lambda s: validate_codes(try_split(s)),
        "package_root": try_split,
        "exclude": str_or_array_as_list,
    }
)


</t>
<t tx="ekr.20221004064034.9"># Fails with legacy mypy.
# Change this case!
def ekr_f_not_annotated(ekr_a="abc", i=1, f=0.1, aBool=True, ekr_b=b's' ) -&gt; None:
    pass
    
</t>
<t tx="ekr.20221004064034.90">def is_method(self, node):
    """Return whether the node occurs (directly) inside a class."""
    node = node.parent
    while node is not None:
        if node.type == syms.classdef:
            return True
        if node.type == syms.funcdef:
            return False
        node = node.parent
    return False

</t>
<t tx="ekr.20221004064034.900">def parse_config_file(
    options: Options,
    set_strict_flags: Callable[[], None],
    filename: str | None,
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
) -&gt; None:
    """Parse a config file into an Options object.

    Errors are written to stderr but are not fatal.

    If filename is None, fall back to default config files.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr

    if filename is not None:
        config_files: tuple[str, ...] = (filename,)
    else:
        config_files_iter: Iterable[str] = map(os.path.expanduser, defaults.CONFIG_FILES)
        config_files = tuple(config_files_iter)

    config_parser = configparser.RawConfigParser()

    for config_file in config_files:
        if not os.path.exists(config_file):
            continue
        try:
            if is_toml(config_file):
                with open(config_file, "rb") as f:
                    toml_data = tomllib.load(f)
                # Filter down to just mypy relevant toml keys
                toml_data = toml_data.get("tool", {})
                if "mypy" not in toml_data:
                    continue
                toml_data = {"mypy": toml_data["mypy"]}
                parser: MutableMapping[str, Any] = destructure_overrides(toml_data)
                config_types = toml_config_types
            else:
                config_parser.read(config_file)
                parser = config_parser
                config_types = ini_config_types
        except (tomllib.TOMLDecodeError, configparser.Error, ConfigTOMLValueError) as err:
            print(f"{config_file}: {err}", file=stderr)
        else:
            if config_file in defaults.SHARED_CONFIG_FILES and "mypy" not in parser:
                continue
            file_read = config_file
            options.config_file = file_read
            break
    else:
        return

    os.environ["MYPY_CONFIG_FILE_DIR"] = os.path.dirname(os.path.abspath(config_file))

    if "mypy" not in parser:
        if filename or file_read not in defaults.SHARED_CONFIG_FILES:
            print(f"{file_read}: No [mypy] section in config file", file=stderr)
    else:
        section = parser["mypy"]
        prefix = f"{file_read}: [mypy]: "
        updates, report_dirs = parse_section(
            prefix, options, set_strict_flags, section, config_types, stderr
        )
        for k, v in updates.items():
            setattr(options, k, v)
        options.report_dirs.update(report_dirs)

    for name, section in parser.items():
        if name.startswith("mypy-"):
            prefix = get_prefix(file_read, name)
            updates, report_dirs = parse_section(
                prefix, options, set_strict_flags, section, config_types, stderr
            )
            if report_dirs:
                print(
                    "%sPer-module sections should not specify reports (%s)"
                    % (prefix, ", ".join(s + "_report" for s in sorted(report_dirs))),
                    file=stderr,
                )
            if set(updates) - PER_MODULE_OPTIONS:
                print(
                    "%sPer-module sections should only specify per-module flags (%s)"
                    % (prefix, ", ".join(sorted(set(updates) - PER_MODULE_OPTIONS))),
                    file=stderr,
                )
                updates = {k: v for k, v in updates.items() if k in PER_MODULE_OPTIONS}

            globs = name[5:]
            for glob in globs.split(","):
                # For backwards compatibility, replace (back)slashes with dots.
                glob = glob.replace(os.sep, ".")
                if os.altsep:
                    glob = glob.replace(os.altsep, ".")

                if any(c in glob for c in "?[]!") or any(
                    "*" in x and x != "*" for x in glob.split(".")
                ):
                    print(
                        "%sPatterns must be fully-qualified module names, optionally "
                        "with '*' in some components (e.g spam.*.eggs.*)" % prefix,
                        file=stderr,
                    )
                else:
                    options.per_module_options[glob] = updates


</t>
<t tx="ekr.20221004064034.901">def get_prefix(file_read: str, name: str) -&gt; str:
    if is_toml(file_read):
        module_name_str = 'module = "%s"' % "-".join(name.split("-")[1:])
    else:
        module_name_str = name

    return f"{file_read}: [{module_name_str}]: "


</t>
<t tx="ekr.20221004064034.902">def is_toml(filename: str) -&gt; bool:
    return filename.lower().endswith(".toml")


</t>
<t tx="ekr.20221004064034.903">def destructure_overrides(toml_data: dict[str, Any]) -&gt; dict[str, Any]:
    """Take the new [[tool.mypy.overrides]] section array in the pyproject.toml file,
    and convert it back to a flatter structure that the existing config_parser can handle.

    E.g. the following pyproject.toml file:

        [[tool.mypy.overrides]]
        module = [
            "a.b",
            "b.*"
        ]
        disallow_untyped_defs = true

        [[tool.mypy.overrides]]
        module = 'c'
        disallow_untyped_defs = false

    Would map to the following config dict that it would have gotten from parsing an equivalent
    ini file:

        {
            "mypy-a.b": {
                disallow_untyped_defs = true,
            },
            "mypy-b.*": {
                disallow_untyped_defs = true,
            },
            "mypy-c": {
                disallow_untyped_defs: false,
            },
        }
    """
    if "overrides" not in toml_data["mypy"]:
        return toml_data

    if not isinstance(toml_data["mypy"]["overrides"], list):
        raise ConfigTOMLValueError(
            "tool.mypy.overrides sections must be an array. Please make "
            "sure you are using double brackets like so: [[tool.mypy.overrides]]"
        )

    result = toml_data.copy()
    for override in result["mypy"]["overrides"]:
        if "module" not in override:
            raise ConfigTOMLValueError(
                "toml config file contains a [[tool.mypy.overrides]] "
                "section, but no module to override was specified."
            )

        if isinstance(override["module"], str):
            modules = [override["module"]]
        elif isinstance(override["module"], list):
            modules = override["module"]
        else:
            raise ConfigTOMLValueError(
                "toml config file contains a [[tool.mypy.overrides]] "
                "section with a module value that is not a string or a list of "
                "strings"
            )

        for module in modules:
            module_overrides = override.copy()
            del module_overrides["module"]
            old_config_name = f"mypy-{module}"
            if old_config_name not in result:
                result[old_config_name] = module_overrides
            else:
                for new_key, new_value in module_overrides.items():
                    if (
                        new_key in result[old_config_name]
                        and result[old_config_name][new_key] != new_value
                    ):
                        raise ConfigTOMLValueError(
                            "toml config file contains "
                            "[[tool.mypy.overrides]] sections with conflicting "
                            "values. Module '%s' has two different values for '%s'"
                            % (module, new_key)
                        )
                    result[old_config_name][new_key] = new_value

    del result["mypy"]["overrides"]
    return result


</t>
<t tx="ekr.20221004064034.904">def parse_section(
    prefix: str,
    template: Options,
    set_strict_flags: Callable[[], None],
    section: Mapping[str, Any],
    config_types: dict[str, Any],
    stderr: TextIO = sys.stderr,
) -&gt; tuple[dict[str, object], dict[str, str]]:
    """Parse one section of a config file.

    Returns a dict of option values encountered, and a dict of report directories.
    """
    results: dict[str, object] = {}
    report_dirs: dict[str, str] = {}
    for key in section:
        invert = False
        options_key = key
        if key in config_types:
            ct = config_types[key]
        else:
            dv = None
            # We have to keep new_semantic_analyzer in Options
            # for plugin compatibility but it is not a valid option anymore.
            assert hasattr(template, "new_semantic_analyzer")
            if key != "new_semantic_analyzer":
                dv = getattr(template, key, None)
            if dv is None:
                if key.endswith("_report"):
                    report_type = key[:-7].replace("_", "-")
                    if report_type in defaults.REPORTER_NAMES:
                        report_dirs[report_type] = str(section[key])
                    else:
                        print(f"{prefix}Unrecognized report type: {key}", file=stderr)
                    continue
                if key.startswith("x_"):
                    pass  # Don't complain about `x_blah` flags
                elif key.startswith("no_") and hasattr(template, key[3:]):
                    options_key = key[3:]
                    invert = True
                elif key.startswith("allow") and hasattr(template, "dis" + key):
                    options_key = "dis" + key
                    invert = True
                elif key.startswith("disallow") and hasattr(template, key[3:]):
                    options_key = key[3:]
                    invert = True
                elif key.startswith("show_") and hasattr(template, "hide_" + key[5:]):
                    options_key = "hide_" + key[5:]
                    invert = True
                elif key == "strict":
                    pass  # Special handling below
                else:
                    print(f"{prefix}Unrecognized option: {key} = {section[key]}", file=stderr)
                if invert:
                    dv = getattr(template, options_key, None)
                else:
                    continue
            ct = type(dv)
        v: Any = None
        try:
            if ct is bool:
                if isinstance(section, dict):
                    v = convert_to_boolean(section.get(key))
                else:
                    v = section.getboolean(key)  # type: ignore[attr-defined]  # Until better stub
                if invert:
                    v = not v
            elif callable(ct):
                if invert:
                    print(f"{prefix}Can not invert non-boolean key {options_key}", file=stderr)
                    continue
                try:
                    v = ct(section.get(key))
                except argparse.ArgumentTypeError as err:
                    print(f"{prefix}{key}: {err}", file=stderr)
                    continue
            else:
                print(f"{prefix}Don't know what type {key} should have", file=stderr)
                continue
        except ValueError as err:
            print(f"{prefix}{key}: {err}", file=stderr)
            continue
        if key == "strict":
            if v:
                set_strict_flags()
            continue
        results[options_key] = v

    # These two flags act as per-module overrides, so store the empty defaults.
    if "disable_error_code" not in results:
        results["disable_error_code"] = []
    if "enable_error_code" not in results:
        results["enable_error_code"] = []

    return results, report_dirs


</t>
<t tx="ekr.20221004064034.905">def convert_to_boolean(value: Any | None) -&gt; bool:
    """Return a boolean value translating from other types if necessary."""
    if isinstance(value, bool):
        return value
    if not isinstance(value, str):
        value = str(value)
    if value.lower() not in configparser.RawConfigParser.BOOLEAN_STATES:
        raise ValueError(f"Not a boolean: {value}")
    return configparser.RawConfigParser.BOOLEAN_STATES[value.lower()]


</t>
<t tx="ekr.20221004064034.906">def split_directive(s: str) -&gt; tuple[list[str], list[str]]:
    """Split s on commas, except during quoted sections.

    Returns the parts and a list of error messages."""
    parts = []
    cur: list[str] = []
    errors = []
    i = 0
    while i &lt; len(s):
        if s[i] == ",":
            parts.append("".join(cur).strip())
            cur = []
        elif s[i] == '"':
            i += 1
            while i &lt; len(s) and s[i] != '"':
                cur.append(s[i])
                i += 1
            if i == len(s):
                errors.append("Unterminated quote in configuration comment")
                cur.clear()
        else:
            cur.append(s[i])
        i += 1
    if cur:
        parts.append("".join(cur).strip())

    return parts, errors


</t>
<t tx="ekr.20221004064034.907">def mypy_comments_to_config_map(line: str, template: Options) -&gt; tuple[dict[str, str], list[str]]:
    """Rewrite the mypy comment syntax into ini file syntax.

    Returns
    """
    options = {}
    entries, errors = split_directive(line)
    for entry in entries:
        if "=" not in entry:
            name = entry
            value = None
        else:
            name, value = (x.strip() for x in entry.split("=", 1))

        name = name.replace("-", "_")
        if value is None:
            value = "True"
        options[name] = value

    return options, errors


</t>
<t tx="ekr.20221004064034.908">def parse_mypy_comments(
    args: list[tuple[int, str]], template: Options
) -&gt; tuple[dict[str, object], list[tuple[int, str]]]:
    """Parse a collection of inline mypy: configuration comments.

    Returns a dictionary of options to be applied and a list of error messages
    generated.
    """

    errors: list[tuple[int, str]] = []
    sections = {}

    for lineno, line in args:
        # In order to easily match the behavior for bools, we abuse configparser.
        # Oddly, the only way to get the SectionProxy object with the getboolean
        # method is to create a config parser.
        parser = configparser.RawConfigParser()
        options, parse_errors = mypy_comments_to_config_map(line, template)
        parser["dummy"] = options
        errors.extend((lineno, x) for x in parse_errors)

        stderr = StringIO()
        strict_found = False

        def set_strict_flags() -&gt; None:
            nonlocal strict_found
            strict_found = True

        new_sections, reports = parse_section(
            "", template, set_strict_flags, parser["dummy"], ini_config_types, stderr=stderr
        )
        errors.extend((lineno, x) for x in stderr.getvalue().strip().split("\n") if x)
        if reports:
            errors.append((lineno, "Reports not supported in inline configuration"))
        if strict_found:
            errors.append(
                (
                    lineno,
                    'Setting "strict" not supported in inline configuration: specify it in '
                    "a configuration file instead, or set individual inline flags "
                    '(see "mypy -h" for the list of flags enabled in strict mode)',
                )
            )

        sections.update(new_sections)

    return sections, errors


</t>
<t tx="ekr.20221004064034.909">def get_config_module_names(filename: str | None, modules: list[str]) -&gt; str:
    if not filename or not modules:
        return ""

    if not is_toml(filename):
        return ", ".join(f"[mypy-{module}]" for module in modules)

    return "module = ['%s']" % ("', '".join(sorted(modules)))


</t>
<t tx="ekr.20221004064034.91">RETURN_EXPR = "return_stmt&lt; 'return' any &gt;"
return_expr = compile_pattern(RETURN_EXPR)

</t>
<t tx="ekr.20221004064034.910">class ConfigTOMLValueError(ValueError):
    pass
</t>
<t tx="ekr.20221004064034.911">@path C:/Repos/ekr-mypy2/mypy/
"""Type inference constraints."""

from __future__ import annotations

from typing import TYPE_CHECKING, Iterable, List, Sequence
from typing_extensions import Final

import mypy.subtypes
import mypy.typeops
from mypy.argmap import ArgTypeExpander
from mypy.erasetype import erase_typevars
from mypy.maptype import map_instance_to_supertype
from mypy.nodes import ARG_OPT, ARG_POS, CONTRAVARIANT, COVARIANT, ArgKind
from mypy.types import (
    TUPLE_LIKE_INSTANCE_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeOfAny,
    TypeQuery,
    TypeType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    callable_with_ellipsis,
    get_proper_type,
    has_recursive_types,
    has_type_vars,
    is_named_instance,
    is_union_with_any,
)
from mypy.typestate import TypeState
from mypy.typevartuples import (
    extract_unpack,
    find_unpack_in_list,
    split_with_instance,
    split_with_mapped_and_template,
    split_with_prefix_and_suffix,
)

if TYPE_CHECKING:
    from mypy.infer import ArgumentInferContext

SUBTYPE_OF: Final = 0
SUPERTYPE_OF: Final = 1


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.912">class Constraint:
    """A representation of a type constraint.

    It can be either T &lt;: type or T :&gt; type (T is a type variable).
    """

    type_var: TypeVarId
    op = 0  # SUBTYPE_OF or SUPERTYPE_OF
    target: Type

    @others
</t>
<t tx="ekr.20221004064034.913">def __init__(self, type_var: TypeVarLikeType, op: int, target: Type) -&gt; None:
    self.type_var = type_var.id
    self.op = op
    self.target = target
    self.origin_type_var = type_var

</t>
<t tx="ekr.20221004064034.914">def __repr__(self) -&gt; str:
    op_str = "&lt;:"
    if self.op == SUPERTYPE_OF:
        op_str = ":&gt;"
    return f"{self.type_var} {op_str} {self.target}"

</t>
<t tx="ekr.20221004064034.915">def __hash__(self) -&gt; int:
    return hash((self.type_var, self.op, self.target))

</t>
<t tx="ekr.20221004064034.916">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Constraint):
        return False
    return (self.type_var, self.op, self.target) == (other.type_var, other.op, other.target)


</t>
<t tx="ekr.20221004064034.917">def infer_constraints_for_callable(
    callee: CallableType,
    arg_types: Sequence[Type | None],
    arg_kinds: list[ArgKind],
    formal_to_actual: list[list[int]],
    context: ArgumentInferContext,
) -&gt; list[Constraint]:
    """Infer type variable constraints for a callable and actual arguments.

    Return a list of constraints.
    """
    constraints: list[Constraint] = []
    mapper = ArgTypeExpander(context)

    for i, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            actual_arg_type = arg_types[actual]
            if actual_arg_type is None:
                continue

            actual_type = mapper.expand_actual_type(
                actual_arg_type, arg_kinds[actual], callee.arg_names[i], callee.arg_kinds[i]
            )
            c = infer_constraints(callee.arg_types[i], actual_type, SUPERTYPE_OF)
            constraints.extend(c)

    return constraints


</t>
<t tx="ekr.20221004064034.918">def infer_constraints(template: Type, actual: Type, direction: int) -&gt; list[Constraint]:
    """Infer type constraints.

    Match a template type, which may contain type variable references,
    recursively against a type which does not contain (the same) type
    variable references. The result is a list of type constrains of
    form 'T is a supertype/subtype of x', where T is a type variable
    present in the template and x is a type without reference to type
    variables present in the template.

    Assume T and S are type variables. Now the following results can be
    calculated (read as '(template, actual) --&gt; result'):

      (T, X)            --&gt;  T :&gt; X
      (X[T], X[Y])      --&gt;  T &lt;: Y and T :&gt; Y
      ((T, T), (X, Y))  --&gt;  T :&gt; X and T :&gt; Y
      ((T, S), (X, Y))  --&gt;  T :&gt; X and S :&gt; Y
      (X[T], Any)       --&gt;  T &lt;: Any and T :&gt; Any

    The constraints are represented as Constraint objects.
    """
    if any(
        get_proper_type(template) == get_proper_type(t)
        and get_proper_type(actual) == get_proper_type(a)
        for (t, a) in reversed(TypeState.inferring)
    ):
        return []
    if has_recursive_types(template):
        # This case requires special care because it may cause infinite recursion.
        if not has_type_vars(template):
            # Return early on an empty branch.
            return []
        TypeState.inferring.append((template, actual))
        res = _infer_constraints(template, actual, direction)
        TypeState.inferring.pop()
        return res
    return _infer_constraints(template, actual, direction)


</t>
<t tx="ekr.20221004064034.919">def _infer_constraints(template: Type, actual: Type, direction: int) -&gt; list[Constraint]:

    orig_template = template
    template = get_proper_type(template)
    actual = get_proper_type(actual)

    # Type inference shouldn't be affected by whether union types have been simplified.
    # We however keep any ErasedType items, so that the caller will see it when using
    # checkexpr.has_erased_component().
    if isinstance(template, UnionType):
        template = mypy.typeops.make_simplified_union(template.items, keep_erased=True)
    if isinstance(actual, UnionType):
        actual = mypy.typeops.make_simplified_union(actual.items, keep_erased=True)

    # Ignore Any types from the type suggestion engine to avoid them
    # causing us to infer Any in situations where a better job could
    # be done otherwise. (This can produce false positives but that
    # doesn't really matter because it is all heuristic anyway.)
    if isinstance(actual, AnyType) and actual.type_of_any == TypeOfAny.suggestion_engine:
        return []

    # If the template is simply a type variable, emit a Constraint directly.
    # We need to handle this case before handling Unions for two reasons:
    #  1. "T &lt;: Union[U1, U2]" is not equivalent to "T &lt;: U1 or T &lt;: U2",
    #     because T can itself be a union (notably, Union[U1, U2] itself).
    #  2. "T :&gt; Union[U1, U2]" is logically equivalent to "T :&gt; U1 and
    #     T :&gt; U2", but they are not equivalent to the constraint solver,
    #     which never introduces new Union types (it uses join() instead).
    if isinstance(template, TypeVarType):
        return [Constraint(template, direction, actual)]

    # Now handle the case of either template or actual being a Union.
    # For a Union to be a subtype of another type, every item of the Union
    # must be a subtype of that type, so concatenate the constraints.
    if direction == SUBTYPE_OF and isinstance(template, UnionType):
        res = []
        for t_item in template.items:
            res.extend(infer_constraints(t_item, actual, direction))
        return res
    if direction == SUPERTYPE_OF and isinstance(actual, UnionType):
        res = []
        for a_item in actual.items:
            res.extend(infer_constraints(orig_template, a_item, direction))
        return res

    # Now the potential subtype is known not to be a Union or a type
    # variable that we are solving for. In that case, for a Union to
    # be a supertype of the potential subtype, some item of the Union
    # must be a supertype of it.
    if direction == SUBTYPE_OF and isinstance(actual, UnionType):
        # If some of items is not a complete type, disregard that.
        items = simplify_away_incomplete_types(actual.items)
        # We infer constraints eagerly -- try to find constraints for a type
        # variable if possible. This seems to help with some real-world
        # use cases.
        return any_constraints(
            [infer_constraints_if_possible(template, a_item, direction) for a_item in items],
            eager=True,
        )
    if direction == SUPERTYPE_OF and isinstance(template, UnionType):
        # When the template is a union, we are okay with leaving some
        # type variables indeterminate. This helps with some special
        # cases, though this isn't very principled.
        result = any_constraints(
            [
                infer_constraints_if_possible(t_item, actual, direction)
                for t_item in template.items
            ],
            eager=False,
        )
        if result:
            return result
        elif has_recursive_types(template) and not has_recursive_types(actual):
            return handle_recursive_union(template, actual, direction)
        return []

    # Remaining cases are handled by ConstraintBuilderVisitor.
    return template.accept(ConstraintBuilderVisitor(actual, direction))


</t>
<t tx="ekr.20221004064034.92">def has_return_exprs(self, node):
    """Traverse the tree below node looking for 'return expr'.

    Return True if at least 'return expr' is found, False if not.
    (If both 'return' and 'return expr' are found, return True.)
    """
    results = {}
    if self.return_expr.match(node, results):
        return True
    for child in node.children:
        if child.type not in (syms.funcdef, syms.classdef):
            if self.has_return_exprs(child):
                return True
    return False
</t>
<t tx="ekr.20221004064034.920">def infer_constraints_if_possible(
    template: Type, actual: Type, direction: int
) -&gt; list[Constraint] | None:
    """Like infer_constraints, but return None if the input relation is
    known to be unsatisfiable, for example if template=List[T] and actual=int.
    (In this case infer_constraints would return [], just like it would for
    an automatically satisfied relation like template=List[T] and actual=object.)
    """
    if direction == SUBTYPE_OF and not mypy.subtypes.is_subtype(erase_typevars(template), actual):
        return None
    if direction == SUPERTYPE_OF and not mypy.subtypes.is_subtype(
        actual, erase_typevars(template)
    ):
        return None
    if (
        direction == SUPERTYPE_OF
        and isinstance(template, TypeVarType)
        and not mypy.subtypes.is_subtype(actual, erase_typevars(template.upper_bound))
    ):
        # This is not caught by the above branch because of the erase_typevars() call,
        # that would return 'Any' for a type variable.
        return None
    return infer_constraints(template, actual, direction)


</t>
<t tx="ekr.20221004064034.921">def select_trivial(options: Sequence[list[Constraint] | None]) -&gt; list[list[Constraint]]:
    """Select only those lists where each item is a constraint against Any."""
    res = []
    for option in options:
        if option is None:
            continue
        if all(isinstance(get_proper_type(c.target), AnyType) for c in option):
            res.append(option)
    return res


</t>
<t tx="ekr.20221004064034.922">def merge_with_any(constraint: Constraint) -&gt; Constraint:
    """Transform a constraint target into a union with given Any type."""
    target = constraint.target
    if is_union_with_any(target):
        # Do not produce redundant unions.
        return constraint
    # TODO: if we will support multiple sources Any, use this here instead.
    any_type = AnyType(TypeOfAny.implementation_artifact)
    return Constraint(
        constraint.origin_type_var,
        constraint.op,
        UnionType.make_union([target, any_type], target.line, target.column),
    )


</t>
<t tx="ekr.20221004064034.923">def handle_recursive_union(template: UnionType, actual: Type, direction: int) -&gt; list[Constraint]:
    # This is a hack to special-case things like Union[T, Inst[T]] in recursive types. Although
    # it is quite arbitrary, it is a relatively common pattern, so we should handle it well.
    # This function may be called when inferring against such union resulted in different
    # constraints for each item. Normally we give up in such case, but here we instead split
    # the union in two parts, and try inferring sequentially.
    non_type_var_items = [t for t in template.items if not isinstance(t, TypeVarType)]
    type_var_items = [t for t in template.items if isinstance(t, TypeVarType)]
    return infer_constraints(
        UnionType.make_union(non_type_var_items), actual, direction
    ) or infer_constraints(UnionType.make_union(type_var_items), actual, direction)


</t>
<t tx="ekr.20221004064034.924">def any_constraints(options: list[list[Constraint] | None], eager: bool) -&gt; list[Constraint]:
    """Deduce what we can from a collection of constraint lists.

    It's a given that at least one of the lists must be satisfied. A
    None element in the list of options represents an unsatisfiable
    constraint and is ignored.  Ignore empty constraint lists if eager
    is true -- they are always trivially satisfiable.
    """
    if eager:
        valid_options = [option for option in options if option]
    else:
        valid_options = [option for option in options if option is not None]

    if not valid_options:
        return []

    if len(valid_options) == 1:
        return valid_options[0]

    if all(is_same_constraints(valid_options[0], c) for c in valid_options[1:]):
        # Multiple sets of constraints that are all the same. Just pick any one of them.
        return valid_options[0]

    if all(is_similar_constraints(valid_options[0], c) for c in valid_options[1:]):
        # All options have same structure. In this case we can merge-in trivial
        # options (i.e. those that only have Any) and try again.
        # TODO: More generally, if a given (variable, direction) pair appears in
        # every option, combine the bounds with meet/join always, not just for Any.
        trivial_options = select_trivial(valid_options)
        if trivial_options and len(trivial_options) &lt; len(valid_options):
            merged_options = []
            for option in valid_options:
                if option in trivial_options:
                    continue
                if option is not None:
                    merged_option: list[Constraint] | None = [merge_with_any(c) for c in option]
                else:
                    merged_option = None
                merged_options.append(merged_option)
            return any_constraints(list(merged_options), eager)

    # If normal logic didn't work, try excluding trivially unsatisfiable constraint (due to
    # upper bounds) from each option, and comparing them again.
    filtered_options = [filter_satisfiable(o) for o in options]
    if filtered_options != options:
        return any_constraints(filtered_options, eager=eager)

    # Otherwise, there are either no valid options or multiple, inconsistent valid
    # options. Give up and deduce nothing.
    return []


</t>
<t tx="ekr.20221004064034.925">def filter_satisfiable(option: list[Constraint] | None) -&gt; list[Constraint] | None:
    """Keep only constraints that can possibly be satisfied.

    Currently, we filter out constraints where target is not a subtype of the upper bound.
    Since those can be never satisfied. We may add more cases in future if it improves type
    inference.
    """
    if not option:
        return option
    satisfiable = []
    for c in option:
        # TODO: add similar logic for TypeVar values (also in various other places)?
        if mypy.subtypes.is_subtype(c.target, c.origin_type_var.upper_bound):
            satisfiable.append(c)
    if not satisfiable:
        return None
    return satisfiable


</t>
<t tx="ekr.20221004064034.926">def is_same_constraints(x: list[Constraint], y: list[Constraint]) -&gt; bool:
    for c1 in x:
        if not any(is_same_constraint(c1, c2) for c2 in y):
            return False
    for c1 in y:
        if not any(is_same_constraint(c1, c2) for c2 in x):
            return False
    return True


</t>
<t tx="ekr.20221004064034.927">def is_same_constraint(c1: Constraint, c2: Constraint) -&gt; bool:
    # Ignore direction when comparing constraints against Any.
    skip_op_check = isinstance(get_proper_type(c1.target), AnyType) and isinstance(
        get_proper_type(c2.target), AnyType
    )
    return (
        c1.type_var == c2.type_var
        and (c1.op == c2.op or skip_op_check)
        and mypy.subtypes.is_same_type(c1.target, c2.target)
    )


</t>
<t tx="ekr.20221004064034.928">def is_similar_constraints(x: list[Constraint], y: list[Constraint]) -&gt; bool:
    """Check that two lists of constraints have similar structure.

    This means that each list has same type variable plus direction pairs (i.e we
    ignore the target). Except for constraints where target is Any type, there
    we ignore direction as well.
    """
    return _is_similar_constraints(x, y) and _is_similar_constraints(y, x)


</t>
<t tx="ekr.20221004064034.929">def _is_similar_constraints(x: list[Constraint], y: list[Constraint]) -&gt; bool:
    """Check that every constraint in the first list has a similar one in the second.

    See docstring above for definition of similarity.
    """
    for c1 in x:
        has_similar = False
        for c2 in y:
            # Ignore direction when either constraint is against Any.
            skip_op_check = isinstance(get_proper_type(c1.target), AnyType) or isinstance(
                get_proper_type(c2.target), AnyType
            )
            if c1.type_var == c2.type_var and (c1.op == c2.op or skip_op_check):
                has_similar = True
                break
        if not has_similar:
            return False
    return True


</t>
<t tx="ekr.20221004064034.93">@path C:/Repos/ekr-mypy2/misc/
#!/usr/bin/env python3
"""
This file compares the output and runtime of running normal vs incremental mode
on the history of any arbitrary git repo as a way of performing a sanity check
to make sure incremental mode is working correctly and efficiently.

It does so by first running mypy without incremental mode on the specified range
of commits to find the expected result, then rewinds back to the first commit and
re-runs mypy on the commits with incremental mode enabled to make sure it returns
the same results.

This script will download and test the official mypy repo by default. Running:

    python3 misc/incremental_checker.py last 30

is equivalent to running

    python3 misc/incremental_checker.py last 30 \\
            --repo_url https://github.com/python/mypy.git \\
            --file-path mypy

You can chose to run this script against a specific commit id or against the
last n commits.

To run this script against the last 30 commits:

    python3 misc/incremental_checker.py last 30

To run this script starting from the commit id 2a432b:

    python3 misc/incremental_checker.py commit 2a432b
"""

from __future__ import annotations

import base64
import json
import os
import random
import re
import shutil
import subprocess
import sys
import textwrap
import time
from argparse import ArgumentParser, Namespace, RawDescriptionHelpFormatter
from typing import Any, Dict
from typing_extensions import Final, TypeAlias as _TypeAlias

CACHE_PATH: Final = ".incremental_checker_cache.json"
MYPY_REPO_URL: Final = "https://github.com/python/mypy.git"
MYPY_TARGET_FILE: Final = "mypy"
DAEMON_CMD: Final = ["python3", "-m", "mypy.dmypy"]

JsonDict: _TypeAlias = Dict[str, Any]


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.930">def simplify_away_incomplete_types(types: Iterable[Type]) -&gt; list[Type]:
    complete = [typ for typ in types if is_complete_type(typ)]
    if complete:
        return complete
    else:
        return list(types)


</t>
<t tx="ekr.20221004064034.931">def is_complete_type(typ: Type) -&gt; bool:
    """Is a type complete?

    A complete doesn't have uninhabited type components or (when not in strict
    optional mode) None components.
    """
    return typ.accept(CompleteTypeVisitor())


</t>
<t tx="ekr.20221004064034.932">class CompleteTypeVisitor(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(all)

    def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
        return False


</t>
<t tx="ekr.20221004064034.933">class ConstraintBuilderVisitor(TypeVisitor[List[Constraint]]):
    """Visitor class for inferring type constraints."""

    # The type that is compared against a template
    # TODO: The value may be None. Is that actually correct?
    actual: ProperType

    @others
</t>
<t tx="ekr.20221004064034.934">def __init__(self, actual: ProperType, direction: int) -&gt; None:
    # Direction must be SUBTYPE_OF or SUPERTYPE_OF.
    self.actual = actual
    self.direction = direction

</t>
<t tx="ekr.20221004064034.935"># Trivial leaf types

</t>
<t tx="ekr.20221004064034.936">def visit_unbound_type(self, template: UnboundType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20221004064034.937">def visit_any(self, template: AnyType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20221004064034.938">def visit_none_type(self, template: NoneType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20221004064034.939">def visit_uninhabited_type(self, template: UninhabitedType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20221004064034.94">def print_offset(text: str, indent_length: int = 4) -&gt; None:
    print()
    print(textwrap.indent(text, " " * indent_length))
    print()


</t>
<t tx="ekr.20221004064034.940">def visit_erased_type(self, template: ErasedType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20221004064034.941">def visit_deleted_type(self, template: DeletedType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20221004064034.942">def visit_literal_type(self, template: LiteralType) -&gt; list[Constraint]:
    return []

</t>
<t tx="ekr.20221004064034.943"># Errors

</t>
<t tx="ekr.20221004064034.944">def visit_partial_type(self, template: PartialType) -&gt; list[Constraint]:
    # We can't do anything useful with a partial type here.
    assert False, "Internal error"

</t>
<t tx="ekr.20221004064034.945"># Non-trivial leaf type

</t>
<t tx="ekr.20221004064034.946">def visit_type_var(self, template: TypeVarType) -&gt; list[Constraint]:
    assert False, (
        "Unexpected TypeVarType in ConstraintBuilderVisitor"
        " (should have been handled in infer_constraints)"
    )

</t>
<t tx="ekr.20221004064034.947">def visit_param_spec(self, template: ParamSpecType) -&gt; list[Constraint]:
    # Can't infer ParamSpecs from component values (only via Callable[P, T]).
    return []

</t>
<t tx="ekr.20221004064034.948">def visit_type_var_tuple(self, template: TypeVarTupleType) -&gt; list[Constraint]:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064034.949">def visit_unpack_type(self, template: UnpackType) -&gt; list[Constraint]:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064034.95">def delete_folder(folder_path: str) -&gt; None:
    if os.path.exists(folder_path):
        shutil.rmtree(folder_path)


</t>
<t tx="ekr.20221004064034.950">def visit_parameters(self, template: Parameters) -&gt; list[Constraint]:
    # constraining Any against C[P] turns into infer_against_any([P], Any)
    # ... which seems like the only case this can happen. Better to fail loudly.
    if isinstance(self.actual, AnyType):
        return self.infer_against_any(template.arg_types, self.actual)
    raise RuntimeError("Parameters cannot be constrained to")

</t>
<t tx="ekr.20221004064034.951"># Non-leaf types

</t>
<t tx="ekr.20221004064034.952">def visit_instance(self, template: Instance) -&gt; list[Constraint]:
    original_actual = actual = self.actual
    res: list[Constraint] = []
    if isinstance(actual, (CallableType, Overloaded)) and template.type.is_protocol:
        if template.type.protocol_members == ["__call__"]:
            # Special case: a generic callback protocol
            if not any(template == t for t in template.type.inferring):
                template.type.inferring.append(template)
                call = mypy.subtypes.find_member(
                    "__call__", template, actual, is_operator=True
                )
                assert call is not None
                if mypy.subtypes.is_subtype(actual, erase_typevars(call)):
                    subres = infer_constraints(call, actual, self.direction)
                    res.extend(subres)
                template.type.inferring.pop()
                return res
    if isinstance(actual, CallableType) and actual.fallback is not None:
        if actual.is_type_obj() and template.type.is_protocol:
            ret_type = get_proper_type(actual.ret_type)
            if isinstance(ret_type, TupleType):
                ret_type = mypy.typeops.tuple_fallback(ret_type)
            if isinstance(ret_type, Instance):
                if self.direction == SUBTYPE_OF:
                    subtype = template
                else:
                    subtype = ret_type
                res.extend(
                    self.infer_constraints_from_protocol_members(
                        ret_type, template, subtype, template, class_obj=True
                    )
                )
        actual = actual.fallback
    if isinstance(actual, TypeType) and template.type.is_protocol:
        if isinstance(actual.item, Instance):
            if self.direction == SUBTYPE_OF:
                subtype = template
            else:
                subtype = actual.item
            res.extend(
                self.infer_constraints_from_protocol_members(
                    actual.item, template, subtype, template, class_obj=True
                )
            )

    if isinstance(actual, Overloaded) and actual.fallback is not None:
        actual = actual.fallback
    if isinstance(actual, TypedDictType):
        actual = actual.as_anonymous().fallback
    if isinstance(actual, LiteralType):
        actual = actual.fallback
    if isinstance(actual, Instance):
        instance = actual
        erased = erase_typevars(template)
        assert isinstance(erased, Instance)  # type: ignore[misc]
        # We always try nominal inference if possible,
        # it is much faster than the structural one.
        if self.direction == SUBTYPE_OF and template.type.has_base(instance.type.fullname):
            mapped = map_instance_to_supertype(template, instance.type)
            tvars = mapped.type.defn.type_vars

            if instance.type.has_type_var_tuple_type:
                mapped_prefix, mapped_middle, mapped_suffix = split_with_instance(mapped)
                instance_prefix, instance_middle, instance_suffix = split_with_instance(
                    instance
                )

                # Add a constraint for the type var tuple, and then
                # remove it for the case below.
                instance_unpack = extract_unpack(instance_middle)
                if instance_unpack is not None:
                    if isinstance(instance_unpack, TypeVarTupleType):
                        res.append(
                            Constraint(
                                instance_unpack, SUBTYPE_OF, TypeList(list(mapped_middle))
                            )
                        )
                    elif (
                        isinstance(instance_unpack, Instance)
                        and instance_unpack.type.fullname == "builtins.tuple"
                    ):
                        for item in mapped_middle:
                            res.extend(
                                infer_constraints(
                                    instance_unpack.args[0], item, self.direction
                                )
                            )
                    elif isinstance(instance_unpack, TupleType):
                        if len(instance_unpack.items) == len(mapped_middle):
                            for instance_arg, item in zip(
                                instance_unpack.items, mapped_middle
                            ):
                                res.extend(
                                    infer_constraints(instance_arg, item, self.direction)
                                )

                mapped_args = mapped_prefix + mapped_suffix
                instance_args = instance_prefix + instance_suffix

                assert instance.type.type_var_tuple_prefix is not None
                assert instance.type.type_var_tuple_suffix is not None
                tvars_prefix, _, tvars_suffix = split_with_prefix_and_suffix(
                    tuple(tvars),
                    instance.type.type_var_tuple_prefix,
                    instance.type.type_var_tuple_suffix,
                )
                tvars = list(tvars_prefix + tvars_suffix)
            else:
                mapped_args = mapped.args
                instance_args = instance.args

            # N.B: We use zip instead of indexing because the lengths might have
            # mismatches during daemon reprocessing.
            for tvar, mapped_arg, instance_arg in zip(tvars, mapped_args, instance_args):
                # TODO(PEP612): More ParamSpec work (or is Parameters the only thing accepted)
                if isinstance(tvar, TypeVarType):
                    # The constraints for generic type parameters depend on variance.
                    # Include constraints from both directions if invariant.
                    if tvar.variance != CONTRAVARIANT:
                        res.extend(infer_constraints(mapped_arg, instance_arg, self.direction))
                    if tvar.variance != COVARIANT:
                        res.extend(
                            infer_constraints(mapped_arg, instance_arg, neg_op(self.direction))
                        )
                elif isinstance(tvar, ParamSpecType) and isinstance(mapped_arg, ParamSpecType):
                    suffix = get_proper_type(instance_arg)

                    if isinstance(suffix, CallableType):
                        prefix = mapped_arg.prefix
                        from_concat = bool(prefix.arg_types) or suffix.from_concatenate
                        suffix = suffix.copy_modified(from_concatenate=from_concat)

                    if isinstance(suffix, Parameters) or isinstance(suffix, CallableType):
                        # no such thing as variance for ParamSpecs
                        # TODO: is there a case I am missing?
                        # TODO: constraints between prefixes
                        prefix = mapped_arg.prefix
                        suffix = suffix.copy_modified(
                            suffix.arg_types[len(prefix.arg_types) :],
                            suffix.arg_kinds[len(prefix.arg_kinds) :],
                            suffix.arg_names[len(prefix.arg_names) :],
                        )
                        res.append(Constraint(mapped_arg, SUPERTYPE_OF, suffix))
                    elif isinstance(suffix, ParamSpecType):
                        res.append(Constraint(mapped_arg, SUPERTYPE_OF, suffix))
                else:
                    # This case should have been handled above.
                    assert not isinstance(tvar, TypeVarTupleType)

            return res
        elif self.direction == SUPERTYPE_OF and instance.type.has_base(template.type.fullname):
            mapped = map_instance_to_supertype(instance, template.type)
            tvars = template.type.defn.type_vars
            if template.type.has_type_var_tuple_type:
                (
                    mapped_prefix,
                    mapped_middle,
                    mapped_suffix,
                    template_prefix,
                    template_middle,
                    template_suffix,
                ) = split_with_mapped_and_template(mapped, template)

                # Add a constraint for the type var tuple, and then
                # remove it for the case below.
                template_unpack = extract_unpack(template_middle)
                if template_unpack is not None:
                    if isinstance(template_unpack, TypeVarTupleType):
                        res.append(
                            Constraint(
                                template_unpack, SUPERTYPE_OF, TypeList(list(mapped_middle))
                            )
                        )
                    elif (
                        isinstance(template_unpack, Instance)
                        and template_unpack.type.fullname == "builtins.tuple"
                    ):
                        for item in mapped_middle:
                            res.extend(
                                infer_constraints(
                                    template_unpack.args[0], item, self.direction
                                )
                            )
                    elif isinstance(template_unpack, TupleType):
                        if len(template_unpack.items) == len(mapped_middle):
                            for template_arg, item in zip(
                                template_unpack.items, mapped_middle
                            ):
                                res.extend(
                                    infer_constraints(template_arg, item, self.direction)
                                )

                mapped_args = mapped_prefix + mapped_suffix
                template_args = template_prefix + template_suffix

                assert template.type.type_var_tuple_prefix is not None
                assert template.type.type_var_tuple_suffix is not None
                tvars_prefix, _, tvars_suffix = split_with_prefix_and_suffix(
                    tuple(tvars),
                    template.type.type_var_tuple_prefix,
                    template.type.type_var_tuple_suffix,
                )
                tvars = list(tvars_prefix + tvars_suffix)
            else:
                mapped_args = mapped.args
                template_args = template.args
            # N.B: We use zip instead of indexing because the lengths might have
            # mismatches during daemon reprocessing.
            for tvar, mapped_arg, template_arg in zip(tvars, mapped_args, template_args):
                assert not isinstance(tvar, TypeVarTupleType)
                if isinstance(tvar, TypeVarType):
                    # The constraints for generic type parameters depend on variance.
                    # Include constraints from both directions if invariant.
                    if tvar.variance != CONTRAVARIANT:
                        res.extend(infer_constraints(template_arg, mapped_arg, self.direction))
                    if tvar.variance != COVARIANT:
                        res.extend(
                            infer_constraints(template_arg, mapped_arg, neg_op(self.direction))
                        )
                elif isinstance(tvar, ParamSpecType) and isinstance(
                    template_arg, ParamSpecType
                ):
                    suffix = get_proper_type(mapped_arg)

                    if isinstance(suffix, CallableType):
                        prefix = template_arg.prefix
                        from_concat = bool(prefix.arg_types) or suffix.from_concatenate
                        suffix = suffix.copy_modified(from_concatenate=from_concat)

                    if isinstance(suffix, Parameters) or isinstance(suffix, CallableType):
                        # no such thing as variance for ParamSpecs
                        # TODO: is there a case I am missing?
                        # TODO: constraints between prefixes
                        prefix = template_arg.prefix

                        suffix = suffix.copy_modified(
                            suffix.arg_types[len(prefix.arg_types) :],
                            suffix.arg_kinds[len(prefix.arg_kinds) :],
                            suffix.arg_names[len(prefix.arg_names) :],
                        )
                        res.append(Constraint(template_arg, SUPERTYPE_OF, suffix))
                    elif isinstance(suffix, ParamSpecType):
                        res.append(Constraint(template_arg, SUPERTYPE_OF, suffix))
                else:
                    # This case should have been handled above.
                    assert not isinstance(tvar, TypeVarTupleType)
            return res
        if (
            template.type.is_protocol
            and self.direction == SUPERTYPE_OF
            and
            # We avoid infinite recursion for structural subtypes by checking
            # whether this type already appeared in the inference chain.
            # This is a conservative way to break the inference cycles.
            # It never produces any "false" constraints but gives up soon
            # on purely structural inference cycles, see #3829.
            # Note that we use is_protocol_implementation instead of is_subtype
            # because some type may be considered a subtype of a protocol
            # due to _promote, but still not implement the protocol.
            not any(template == t for t in reversed(template.type.inferring))
            and mypy.subtypes.is_protocol_implementation(instance, erased)
        ):
            template.type.inferring.append(template)
            res.extend(
                self.infer_constraints_from_protocol_members(
                    instance, template, original_actual, template
                )
            )
            template.type.inferring.pop()
            return res
        elif (
            instance.type.is_protocol
            and self.direction == SUBTYPE_OF
            and
            # We avoid infinite recursion for structural subtypes also here.
            not any(instance == i for i in reversed(instance.type.inferring))
            and mypy.subtypes.is_protocol_implementation(erased, instance)
        ):
            instance.type.inferring.append(instance)
            res.extend(
                self.infer_constraints_from_protocol_members(
                    instance, template, template, instance
                )
            )
            instance.type.inferring.pop()
            return res
    if res:
        return res

    if isinstance(actual, AnyType):
        return self.infer_against_any(template.args, actual)
    if (
        isinstance(actual, TupleType)
        and is_named_instance(template, TUPLE_LIKE_INSTANCE_NAMES)
        and self.direction == SUPERTYPE_OF
    ):
        for item in actual.items:
            cb = infer_constraints(template.args[0], item, SUPERTYPE_OF)
            res.extend(cb)
        return res
    elif isinstance(actual, TupleType) and self.direction == SUPERTYPE_OF:
        return infer_constraints(template, mypy.typeops.tuple_fallback(actual), self.direction)
    elif isinstance(actual, TypeVarType):
        if not actual.values:
            return infer_constraints(template, actual.upper_bound, self.direction)
        return []
    elif isinstance(actual, ParamSpecType):
        return infer_constraints(template, actual.upper_bound, self.direction)
    elif isinstance(actual, TypeVarTupleType):
        raise NotImplementedError
    else:
        return []

</t>
<t tx="ekr.20221004064034.953">def infer_constraints_from_protocol_members(
    self,
    instance: Instance,
    template: Instance,
    subtype: Type,
    protocol: Instance,
    class_obj: bool = False,
) -&gt; list[Constraint]:
    """Infer constraints for situations where either 'template' or 'instance' is a protocol.

    The 'protocol' is the one of two that is an instance of protocol type, 'subtype'
    is the type used to bind self during inference. Currently, we just infer constrains for
    every protocol member type (both ways for settable members).
    """
    res = []
    for member in protocol.type.protocol_members:
        inst = mypy.subtypes.find_member(member, instance, subtype, class_obj=class_obj)
        temp = mypy.subtypes.find_member(member, template, subtype)
        if inst is None or temp is None:
            return []  # See #11020
        # The above is safe since at this point we know that 'instance' is a subtype
        # of (erased) 'template', therefore it defines all protocol members
        res.extend(infer_constraints(temp, inst, self.direction))
        if mypy.subtypes.IS_SETTABLE in mypy.subtypes.get_member_flags(member, protocol):
            # Settable members are invariant, add opposite constraints
            res.extend(infer_constraints(temp, inst, neg_op(self.direction)))
    return res

</t>
<t tx="ekr.20221004064034.954">def visit_callable_type(self, template: CallableType) -&gt; list[Constraint]:
    # Normalize callables before matching against each other.
    # Note that non-normalized callables can be created in annotations
    # using e.g. callback protocols.
    template = template.with_unpacked_kwargs()
    if isinstance(self.actual, CallableType):
        res: list[Constraint] = []
        cactual = self.actual.with_unpacked_kwargs()
        param_spec = template.param_spec()
        if param_spec is None:
            # FIX verify argument counts
            # FIX what if one of the functions is generic

            # We can't infer constraints from arguments if the template is Callable[..., T]
            # (with literal '...').
            if not template.is_ellipsis_args:
                # The lengths should match, but don't crash (it will error elsewhere).
                for t, a in zip(template.arg_types, cactual.arg_types):
                    # Negate direction due to function argument type contravariance.
                    res.extend(infer_constraints(t, a, neg_op(self.direction)))
        else:
            # sometimes, it appears we try to get constraints between two paramspec callables?
            # TODO: Direction
            # TODO: check the prefixes match
            prefix = param_spec.prefix
            prefix_len = len(prefix.arg_types)
            cactual_ps = cactual.param_spec()

            if not cactual_ps:
                max_prefix_len = len([k for k in cactual.arg_kinds if k in (ARG_POS, ARG_OPT)])
                prefix_len = min(prefix_len, max_prefix_len)
                res.append(
                    Constraint(
                        param_spec,
                        SUBTYPE_OF,
                        cactual.copy_modified(
                            arg_types=cactual.arg_types[prefix_len:],
                            arg_kinds=cactual.arg_kinds[prefix_len:],
                            arg_names=cactual.arg_names[prefix_len:],
                            ret_type=NoneType(),
                        ),
                    )
                )
            else:
                res.append(Constraint(param_spec, SUBTYPE_OF, cactual_ps))

            # compare prefixes
            cactual_prefix = cactual.copy_modified(
                arg_types=cactual.arg_types[:prefix_len],
                arg_kinds=cactual.arg_kinds[:prefix_len],
                arg_names=cactual.arg_names[:prefix_len],
            )

            # TODO: see above "FIX" comments for param_spec is None case
            # TODO: this assume positional arguments
            for t, a in zip(prefix.arg_types, cactual_prefix.arg_types):
                res.extend(infer_constraints(t, a, neg_op(self.direction)))

        template_ret_type, cactual_ret_type = template.ret_type, cactual.ret_type
        if template.type_guard is not None:
            template_ret_type = template.type_guard
        if cactual.type_guard is not None:
            cactual_ret_type = cactual.type_guard

        res.extend(infer_constraints(template_ret_type, cactual_ret_type, self.direction))
        return res
    elif isinstance(self.actual, AnyType):
        param_spec = template.param_spec()
        any_type = AnyType(TypeOfAny.from_another_any, source_any=self.actual)
        if param_spec is None:
            # FIX what if generic
            res = self.infer_against_any(template.arg_types, self.actual)
        else:
            res = [
                Constraint(
                    param_spec,
                    SUBTYPE_OF,
                    callable_with_ellipsis(any_type, any_type, template.fallback),
                )
            ]
        res.extend(infer_constraints(template.ret_type, any_type, self.direction))
        return res
    elif isinstance(self.actual, Overloaded):
        return self.infer_against_overloaded(self.actual, template)
    elif isinstance(self.actual, TypeType):
        return infer_constraints(template.ret_type, self.actual.item, self.direction)
    elif isinstance(self.actual, Instance):
        # Instances with __call__ method defined are considered structural
        # subtypes of Callable with a compatible signature.
        call = mypy.subtypes.find_member(
            "__call__", self.actual, self.actual, is_operator=True
        )
        if call:
            return infer_constraints(template, call, self.direction)
        else:
            return []
    else:
        return []

</t>
<t tx="ekr.20221004064034.955">def infer_against_overloaded(
    self, overloaded: Overloaded, template: CallableType
) -&gt; list[Constraint]:
    # Create constraints by matching an overloaded type against a template.
    # This is tricky to do in general. We cheat by only matching against
    # the first overload item that is callable compatible. This
    # seems to work somewhat well, but we should really use a more
    # reliable technique.
    item = find_matching_overload_item(overloaded, template)
    return infer_constraints(template, item, self.direction)

</t>
<t tx="ekr.20221004064034.956">def visit_tuple_type(self, template: TupleType) -&gt; list[Constraint]:
    actual = self.actual
    # TODO: Support subclasses of Tuple
    is_varlength_tuple = (
        isinstance(actual, Instance) and actual.type.fullname == "builtins.tuple"
    )
    unpack_index = find_unpack_in_list(template.items)

    if unpack_index is not None:
        unpack_item = get_proper_type(template.items[unpack_index])
        assert isinstance(unpack_item, UnpackType)

        unpacked_type = get_proper_type(unpack_item.type)
        if isinstance(unpacked_type, TypeVarTupleType):
            if is_varlength_tuple:
                # This case is only valid when the unpack is the only
                # item in the tuple.
                #
                # TODO: We should support this in the case that all the items
                # in the tuple besides the unpack have the same type as the
                # varlength tuple's type. E.g. Tuple[int, ...] should be valid
                # where we expect Tuple[int, Unpack[Ts]], but not for Tuple[str, Unpack[Ts]].
                assert len(template.items) == 1

            if isinstance(actual, (TupleType, AnyType)) or is_varlength_tuple:
                modified_actual = actual
                if isinstance(actual, TupleType):
                    # Exclude the items from before and after the unpack index.
                    # TODO: Support including constraints from the prefix/suffix.
                    _, actual_items, _ = split_with_prefix_and_suffix(
                        tuple(actual.items),
                        unpack_index,
                        len(template.items) - unpack_index - 1,
                    )
                    modified_actual = actual.copy_modified(items=list(actual_items))
                return [
                    Constraint(
                        type_var=unpacked_type, op=self.direction, target=modified_actual
                    )
                ]

    if isinstance(actual, TupleType) and len(actual.items) == len(template.items):
        if (
            actual.partial_fallback.type.is_named_tuple
            and template.partial_fallback.type.is_named_tuple
        ):
            # For named tuples using just the fallbacks usually gives better results.
            return infer_constraints(
                template.partial_fallback, actual.partial_fallback, self.direction
            )
        res: list[Constraint] = []
        for i in range(len(template.items)):
            res.extend(infer_constraints(template.items[i], actual.items[i], self.direction))
        return res
    elif isinstance(actual, AnyType):
        return self.infer_against_any(template.items, actual)
    else:
        return []

</t>
<t tx="ekr.20221004064034.957">def visit_typeddict_type(self, template: TypedDictType) -&gt; list[Constraint]:
    actual = self.actual
    if isinstance(actual, TypedDictType):
        res: list[Constraint] = []
        # NOTE: Non-matching keys are ignored. Compatibility is checked
        #       elsewhere so this shouldn't be unsafe.
        for (item_name, template_item_type, actual_item_type) in template.zip(actual):
            res.extend(infer_constraints(template_item_type, actual_item_type, self.direction))
        return res
    elif isinstance(actual, AnyType):
        return self.infer_against_any(template.items.values(), actual)
    else:
        return []

</t>
<t tx="ekr.20221004064034.958">def visit_union_type(self, template: UnionType) -&gt; list[Constraint]:
    assert False, (
        "Unexpected UnionType in ConstraintBuilderVisitor"
        " (should have been handled in infer_constraints)"
    )

</t>
<t tx="ekr.20221004064034.959">def visit_type_alias_type(self, template: TypeAliasType) -&gt; list[Constraint]:
    assert False, f"This should be never called, got {template}"

</t>
<t tx="ekr.20221004064034.96">def execute(command: list[str], fail_on_error: bool = True) -&gt; tuple[str, str, int]:
    proc = subprocess.Popen(
        " ".join(command), stderr=subprocess.PIPE, stdout=subprocess.PIPE, shell=True
    )
    stdout_bytes, stderr_bytes = proc.communicate()
    stdout, stderr = stdout_bytes.decode("utf-8"), stderr_bytes.decode("utf-8")
    if fail_on_error and proc.returncode != 0:
        print("EXECUTED COMMAND:", repr(command))
        print("RETURN CODE:", proc.returncode)
        print()
        print("STDOUT:")
        print_offset(stdout)
        print("STDERR:")
        print_offset(stderr)
        raise RuntimeError("Unexpected error from external tool.")
    return stdout, stderr, proc.returncode


</t>
<t tx="ekr.20221004064034.960">def infer_against_any(self, types: Iterable[Type], any_type: AnyType) -&gt; list[Constraint]:
    res: list[Constraint] = []
    for t in types:
        # Note that we ignore variance and simply always use the
        # original direction. This is because for Any targets direction is
        # irrelevant in most cases, see e.g. is_same_constraint().
        res.extend(infer_constraints(t, any_type, self.direction))
    return res

</t>
<t tx="ekr.20221004064034.961">def visit_overloaded(self, template: Overloaded) -&gt; list[Constraint]:
    if isinstance(self.actual, CallableType):
        items = find_matching_overload_items(template, self.actual)
    else:
        items = template.items
    res: list[Constraint] = []
    for t in items:
        res.extend(infer_constraints(t, self.actual, self.direction))
    return res

</t>
<t tx="ekr.20221004064034.962">def visit_type_type(self, template: TypeType) -&gt; list[Constraint]:
    if isinstance(self.actual, CallableType):
        return infer_constraints(template.item, self.actual.ret_type, self.direction)
    elif isinstance(self.actual, Overloaded):
        return infer_constraints(template.item, self.actual.items[0].ret_type, self.direction)
    elif isinstance(self.actual, TypeType):
        return infer_constraints(template.item, self.actual.item, self.direction)
    elif isinstance(self.actual, AnyType):
        return infer_constraints(template.item, self.actual, self.direction)
    else:
        return []


</t>
<t tx="ekr.20221004064034.963">def neg_op(op: int) -&gt; int:
    """Map SubtypeOf to SupertypeOf and vice versa."""

    if op == SUBTYPE_OF:
        return SUPERTYPE_OF
    elif op == SUPERTYPE_OF:
        return SUBTYPE_OF
    else:
        raise ValueError(f"Invalid operator {op}")


</t>
<t tx="ekr.20221004064034.964">def find_matching_overload_item(overloaded: Overloaded, template: CallableType) -&gt; CallableType:
    """Disambiguate overload item against a template."""
    items = overloaded.items
    for item in items:
        # Return type may be indeterminate in the template, so ignore it when performing a
        # subtype check.
        if mypy.subtypes.is_callable_compatible(
            item, template, is_compat=mypy.subtypes.is_subtype, ignore_return=True
        ):
            return item
    # Fall back to the first item if we can't find a match. This is totally arbitrary --
    # maybe we should just bail out at this point.
    return items[0]


</t>
<t tx="ekr.20221004064034.965">def find_matching_overload_items(
    overloaded: Overloaded, template: CallableType
) -&gt; list[CallableType]:
    """Like find_matching_overload_item, but return all matches, not just the first."""
    items = overloaded.items
    res = []
    for item in items:
        # Return type may be indeterminate in the template, so ignore it when performing a
        # subtype check.
        if mypy.subtypes.is_callable_compatible(
            item, template, is_compat=mypy.subtypes.is_subtype, ignore_return=True
        ):
            res.append(item)
    if not res:
        # Falling back to all items if we can't find a match is pretty arbitrary, but
        # it maintains backward compatibility.
        res = items[:]
    return res
</t>
<t tx="ekr.20221004064034.966">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Any, cast

from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    TypeAliasType,
    TypedDictType,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
)

# type_visitor needs to be imported after types
from mypy.type_visitor import TypeVisitor  # isort: skip


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.967">def copy_type(t: ProperType) -&gt; ProperType:
    """Create a shallow copy of a type.

    This can be used to mutate the copy with truthiness information.

    Classes compiled with mypyc don't support copy.copy(), so we need
    a custom implementation.
    """
    return t.accept(TypeShallowCopier())


</t>
<t tx="ekr.20221004064034.968">class TypeShallowCopier(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20221004064034.969">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064034.97">def ensure_environment_is_ready(mypy_path: str, temp_repo_path: str, mypy_cache_path: str) -&gt; None:
    os.chdir(mypy_path)
    delete_folder(temp_repo_path)
    delete_folder(mypy_cache_path)


</t>
<t tx="ekr.20221004064034.970">def visit_any(self, t: AnyType) -&gt; ProperType:
    return self.copy_common(t, AnyType(t.type_of_any, t.source_any, t.missing_import_name))

</t>
<t tx="ekr.20221004064034.971">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    return self.copy_common(t, NoneType())

</t>
<t tx="ekr.20221004064034.972">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    dup = UninhabitedType(t.is_noreturn)
    dup.ambiguous = t.ambiguous
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20221004064034.973">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.copy_common(t, ErasedType())

</t>
<t tx="ekr.20221004064034.974">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return self.copy_common(t, DeletedType(t.source))

</t>
<t tx="ekr.20221004064034.975">def visit_instance(self, t: Instance) -&gt; ProperType:
    dup = Instance(t.type, t.args, last_known_value=t.last_known_value)
    dup.invalid = t.invalid
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20221004064034.976">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    dup = TypeVarType(
        t.name,
        t.fullname,
        t.id,
        values=t.values,
        upper_bound=t.upper_bound,
        variance=t.variance,
    )
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20221004064034.977">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    dup = ParamSpecType(t.name, t.fullname, t.id, t.flavor, t.upper_bound, prefix=t.prefix)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20221004064034.978">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    dup = Parameters(
        t.arg_types,
        t.arg_kinds,
        t.arg_names,
        variables=t.variables,
        is_ellipsis_args=t.is_ellipsis_args,
    )
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20221004064034.979">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    dup = TypeVarTupleType(t.name, t.fullname, t.id, t.upper_bound)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20221004064034.98">def initialize_repo(repo_url: str, temp_repo_path: str, branch: str) -&gt; None:
    print(f"Cloning repo {repo_url} to {temp_repo_path}")
    execute(["git", "clone", repo_url, temp_repo_path])
    if branch is not None:
        print(f"Checking out branch {branch}")
        execute(["git", "-C", temp_repo_path, "checkout", branch])


</t>
<t tx="ekr.20221004064034.980">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    dup = UnpackType(t.type)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20221004064034.981">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    return self.copy_common(t, PartialType(t.type, t.var, t.value_type))

</t>
<t tx="ekr.20221004064034.982">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    return self.copy_common(t, t.copy_modified())

</t>
<t tx="ekr.20221004064034.983">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    return self.copy_common(t, TupleType(t.items, t.partial_fallback, implicit=t.implicit))

</t>
<t tx="ekr.20221004064034.984">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    return self.copy_common(t, TypedDictType(t.items, t.required_keys, t.fallback))

</t>
<t tx="ekr.20221004064034.985">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    return self.copy_common(t, LiteralType(value=t.value, fallback=t.fallback))

</t>
<t tx="ekr.20221004064034.986">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    return self.copy_common(t, UnionType(t.items))

</t>
<t tx="ekr.20221004064034.987">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    return self.copy_common(t, Overloaded(items=t.items))

</t>
<t tx="ekr.20221004064034.988">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    # Use cast since the type annotations in TypeType are imprecise.
    return self.copy_common(t, TypeType(cast(Any, t.item)))

</t>
<t tx="ekr.20221004064034.989">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, "only ProperTypes supported"

</t>
<t tx="ekr.20221004064034.99">def get_commits(repo_folder_path: str, commit_range: str) -&gt; list[tuple[str, str]]:
    raw_data, _stderr, _errcode = execute(
        ["git", "-C", repo_folder_path, "log", "--reverse", "--oneline", commit_range]
    )
    output = []
    for line in raw_data.strip().split("\n"):
        commit_id, _, message = line.partition(" ")
        output.append((commit_id, message))
    return output


</t>
<t tx="ekr.20221004064034.990">def copy_common(self, t: ProperType, t2: ProperType) -&gt; ProperType:
    t2.line = t.line
    t2.column = t.column
    t2.can_be_false = t.can_be_false
    t2.can_be_true = t.can_be_true
    return t2
</t>
<t tx="ekr.20221004064034.991">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import os
from typing_extensions import Final

PYTHON2_VERSION: Final = (2, 7)

# Earliest fully supported Python 3.x version. Used as the default Python
# version in tests. Mypy wheels should be built starting with this version,
# and CI tests should be run on this version (and later versions).
PYTHON3_VERSION: Final = (3, 7)

# Earliest Python 3.x version supported via --python-version 3.x. To run
# mypy, at least version PYTHON3_VERSION is needed.
PYTHON3_VERSION_MIN: Final = (3, 4)

CACHE_DIR: Final = ".mypy_cache"
CONFIG_FILE: Final = ["mypy.ini", ".mypy.ini"]
PYPROJECT_CONFIG_FILES: Final = ["pyproject.toml"]
SHARED_CONFIG_FILES: Final = ["setup.cfg"]
USER_CONFIG_FILES: Final = ["~/.config/mypy/config", "~/.mypy.ini"]
if os.environ.get("XDG_CONFIG_HOME"):
    USER_CONFIG_FILES.insert(0, os.path.join(os.environ["XDG_CONFIG_HOME"], "mypy/config"))

CONFIG_FILES: Final = (
    CONFIG_FILE + PYPROJECT_CONFIG_FILES + SHARED_CONFIG_FILES + USER_CONFIG_FILES
)

# This must include all reporters defined in mypy.report. This is defined here
# to make reporter names available without importing mypy.report -- this speeds
# up startup.
REPORTER_NAMES: Final = [
    "linecount",
    "any-exprs",
    "linecoverage",
    "memory-xml",
    "cobertura-xml",
    "xml",
    "xslt-html",
    "xslt-txt",
    "html",
    "txt",
    "lineprecision",
]

# Threshold after which we sometimes filter out most errors to avoid very
# verbose output
MANY_ERRORS_THRESHOLD: Final = 200
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.992">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import sys
from typing import Any, Callable

if sys.platform == "win32":
    import ctypes
    import subprocess
    from ctypes.wintypes import DWORD, HANDLE

    PROCESS_QUERY_LIMITED_INFORMATION = ctypes.c_ulong(0x1000)

    kernel32 = ctypes.windll.kernel32
    OpenProcess: Callable[[DWORD, int, int], HANDLE] = kernel32.OpenProcess
    GetExitCodeProcess: Callable[[HANDLE, Any], int] = kernel32.GetExitCodeProcess
else:
    import os
    import signal


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.993">def alive(pid: int) -&gt; bool:
    """Is the process alive?"""
    if sys.platform == "win32":
        # why can't anything be easy...
        status = DWORD()
        handle = OpenProcess(PROCESS_QUERY_LIMITED_INFORMATION, 0, pid)
        GetExitCodeProcess(handle, ctypes.byref(status))
        return status.value == 259  # STILL_ACTIVE
    else:
        try:
            os.kill(pid, 0)
        except OSError:
            return False
        return True


</t>
<t tx="ekr.20221004064034.994">def kill(pid: int) -&gt; None:
    """Kill the process."""
    if sys.platform == "win32":
        subprocess.check_output(f"taskkill /pid {pid} /f /t")
    else:
        os.kill(pid, signal.SIGKILL)
</t>
<t tx="ekr.20221004064034.995">@path C:/Repos/ekr-mypy2/mypy/
"""Server for mypy daemon mode.

This implements a daemon process which keeps useful state in memory
to enable fine-grained incremental reprocessing of changes.
"""

from __future__ import annotations

import argparse
import base64
import io
import json
import os
import pickle
import subprocess
import sys
import time
import traceback
from contextlib import redirect_stderr, redirect_stdout
from typing import AbstractSet, Any, Callable, List, Sequence, Tuple
from typing_extensions import Final, TypeAlias as _TypeAlias

import mypy.build
import mypy.errors
import mypy.main
from mypy.dmypy_util import receive
from mypy.find_sources import InvalidSourceList, create_source_list
from mypy.fscache import FileSystemCache
from mypy.fswatcher import FileData, FileSystemWatcher
from mypy.inspections import InspectionEngine
from mypy.ipc import IPCServer
from mypy.modulefinder import BuildSource, FindModuleCache, SearchPaths, compute_search_paths
from mypy.options import Options
from mypy.server.update import FineGrainedBuildManager, refresh_suppressed_submodules
from mypy.suggestions import SuggestionEngine, SuggestionFailure
from mypy.typestate import reset_global_state
from mypy.util import FancyFormatter, count_stats
from mypy.version import __version__

MEM_PROFILE: Final = False  # If True, dump memory profile after initialization

if sys.platform == "win32":
    from subprocess import STARTUPINFO

    def daemonize(
        options: Options, status_file: str, timeout: int | None = None, log_file: str | None = None
    ) -&gt; int:
        """Create the daemon process via "dmypy daemon" and pass options via command line

        When creating the daemon grandchild, we create it in a new console, which is
        started hidden. We cannot use DETACHED_PROCESS since it will cause console windows
        to pop up when starting. See
        https://github.com/python/cpython/pull/4150#issuecomment-340215696
        for more on why we can't have nice things.

        It also pickles the options to be unpickled by mypy.
        """
        command = [sys.executable, "-m", "mypy.dmypy", "--status-file", status_file, "daemon"]
        pickled_options = pickle.dumps((options.snapshot(), timeout, log_file))
        command.append(f'--options-data="{base64.b64encode(pickled_options).decode()}"')
        info = STARTUPINFO()
        info.dwFlags = 0x1  # STARTF_USESHOWWINDOW aka use wShowWindow's value
        info.wShowWindow = 0  # SW_HIDE aka make the window invisible
        try:
            subprocess.Popen(command, creationflags=0x10, startupinfo=info)  # CREATE_NEW_CONSOLE
            return 0
        except subprocess.CalledProcessError as e:
            return e.returncode

else:

    def _daemonize_cb(func: Callable[[], None], log_file: str | None = None) -&gt; int:
        """Arrange to call func() in a grandchild of the current process.

        Return 0 for success, exit status for failure, negative if
        subprocess killed by signal.
        """
        # See https://stackoverflow.com/questions/473620/how-do-you-create-a-daemon-in-python
        sys.stdout.flush()
        sys.stderr.flush()
        pid = os.fork()
        if pid:
            # Parent process: wait for child in case things go bad there.
            npid, sts = os.waitpid(pid, 0)
            sig = sts &amp; 0xFF
            if sig:
                print("Child killed by signal", sig)
                return -sig
            sts = sts &gt;&gt; 8
            if sts:
                print("Child exit status", sts)
            return sts
        # Child process: do a bunch of UNIX stuff and then fork a grandchild.
        try:
            os.setsid()  # Detach controlling terminal
            os.umask(0o27)
            devnull = os.open("/dev/null", os.O_RDWR)
            os.dup2(devnull, 0)
            os.dup2(devnull, 1)
            os.dup2(devnull, 2)
            os.close(devnull)
            pid = os.fork()
            if pid:
                # Child is done, exit to parent.
                os._exit(0)
            # Grandchild: run the server.
            if log_file:
                sys.stdout = sys.stderr = open(log_file, "a", buffering=1)
                fd = sys.stdout.fileno()
                os.dup2(fd, 2)
                os.dup2(fd, 1)
            func()
        finally:
            # Make sure we never get back into the caller.
            os._exit(1)

    def daemonize(
        options: Options, status_file: str, timeout: int | None = None, log_file: str | None = None
    ) -&gt; int:
        """Run the mypy daemon in a grandchild of the current process

        Return 0 for success, exit status for failure, negative if
        subprocess killed by signal.
        """
        return _daemonize_cb(Server(options, status_file, timeout).serve, log_file)


# Server code.

CONNECTION_NAME: Final = "dmypy"


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064034.996">def process_start_options(flags: list[str], allow_sources: bool) -&gt; Options:
    _, options = mypy.main.process_options(
        ["-i"] + flags, require_targets=False, server_options=True
    )
    if options.report_dirs:
        print("dmypy: Ignoring report generation settings. Start/restart cannot generate reports.")
    if options.junit_xml:
        print(
            "dmypy: Ignoring report generation settings. "
            "Start/restart does not support --junit-xml. Pass it to check/recheck instead"
        )
        options.junit_xml = None
    if not options.incremental:
        sys.exit("dmypy: start/restart should not disable incremental mode")
    if options.follow_imports not in ("skip", "error", "normal"):
        sys.exit("dmypy: follow-imports=silent not supported")
    return options


</t>
<t tx="ekr.20221004064034.997">def ignore_suppressed_imports(module: str) -&gt; bool:
    """Can we skip looking for newly unsuppressed imports to module?"""
    # Various submodules of 'encodings' can be suppressed, since it
    # uses module-level '__getattr__'. Skip them since there are many
    # of them, and following imports to them is kind of pointless.
    return module.startswith("encodings.")


</t>
<t tx="ekr.20221004064034.998">ModulePathPair: _TypeAlias = Tuple[str, str]
ModulePathPairs: _TypeAlias = List[ModulePathPair]
ChangesAndRemovals: _TypeAlias = Tuple[ModulePathPairs, ModulePathPairs]


</t>
<t tx="ekr.20221004064034.999">class Server:

    # NOTE: the instance is constructed in the parent process but
    # serve() is called in the grandchild (by daemonize()).

    @others
</t>
<t tx="ekr.20221004064035.1">class IPCException(Exception):
    """Exception for IPC issues."""


</t>
<t tx="ekr.20221004064035.10">def __exit__(
    self,
    exc_ty: type[BaseException] | None = None,
    exc_val: BaseException | None = None,
    exc_tb: TracebackType | None = None,
) -&gt; None:
    self.close()


</t>
<t tx="ekr.20221004064035.100">def visit_type_var_expr(self, e: TypeVarExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1000">def get_class_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    """
    Adjust type of a class attribute.

    This method is called with attribute full name using the class where the attribute was
    defined (or Var.info.fullname for generated attributes).

    For example:

        class Cls:
            x: Any

        Cls.x

    get_class_attribute_hook is called with '__main__.Cls.x' as fullname.
    """
    return None

</t>
<t tx="ekr.20221004064035.1001">def get_class_decorator_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    """Update class definition for given class decorators.

    The plugin can modify a TypeInfo _in place_ (for example add some generated
    methods to the symbol table). This hook is called after the class body was
    semantically analyzed, but *there may still be placeholders* (typically
    caused by forward references).

    NOTE: Usually get_class_decorator_hook_2 is the better option, since it
          guarantees that there are no placeholders.

    The hook is called with full names of all class decorators.

    The hook can be called multiple times per class, so it must be
    idempotent.
    """
    return None

</t>
<t tx="ekr.20221004064035.1002">def get_class_decorator_hook_2(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], bool] | None:
    """Update class definition for given class decorators.

    Similar to get_class_decorator_hook, but this runs in a later pass when
    placeholders have been resolved.

    The hook can return False if some base class hasn't been
    processed yet using class hooks. It causes all class hooks
    (that are run in this same pass) to be invoked another time for
    the file(s) currently being processed.

    The hook can be called multiple times per class, so it must be
    idempotent.
    """
    return None

</t>
<t tx="ekr.20221004064035.1003">def get_metaclass_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    """Update class definition for given declared metaclasses.

    Same as get_class_decorator_hook() but for metaclasses. Note:
    this hook will be only called for explicit metaclasses, not for
    inherited ones.

    TODO: probably it should also be called on inherited metaclasses.
    """
    return None

</t>
<t tx="ekr.20221004064035.1004">def get_base_class_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    """Update class definition for given base classes.

    Same as get_class_decorator_hook() but for base classes. Base classes
    don't need to refer to TypeInfos, if a base class refers to a variable with
    Any type, this hook will still be called.
    """
    return None

</t>
<t tx="ekr.20221004064035.1005">def get_customize_class_mro_hook(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], None] | None:
    """Customize MRO for given classes.

    The plugin can modify the class MRO _in place_. This method is called
    with the class full name before its body was semantically analyzed.
    """
    return None

</t>
<t tx="ekr.20221004064035.1006">def get_dynamic_class_hook(
    self, fullname: str
) -&gt; Callable[[DynamicClassDefContext], None] | None:
    """Semantically analyze a dynamic class definition.

    This plugin hook allows one to semantically analyze dynamic class definitions like:

        from lib import dynamic_class

        X = dynamic_class('X', [])

    For such definition, this hook will be called with 'lib.dynamic_class'.
    The plugin should create the corresponding TypeInfo, and place it into a relevant
    symbol table, e.g. using ctx.api.add_symbol_table_node().
    """
    return None


</t>
<t tx="ekr.20221004064035.1007">T = TypeVar("T")


</t>
<t tx="ekr.20221004064035.1008">class ChainedPlugin(Plugin):
    """A plugin that represents a sequence of chained plugins.

    Each lookup method returns the hook for the first plugin that
    reports a match.

    This class should not be subclassed -- use Plugin as the base class
    for all plugins.
    """

    # TODO: Support caching of lookup results (through a LRU cache, for example).

    @others
</t>
<t tx="ekr.20221004064035.1009">def __init__(self, options: Options, plugins: list[Plugin]) -&gt; None:
    """Initialize chained plugin.

    Assume that the child plugins aren't mutated (results may be cached).
    """
    super().__init__(options)
    self._plugins = plugins

</t>
<t tx="ekr.20221004064035.101">def visit_paramspec_expr(self, e: ParamSpecExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1010">def set_modules(self, modules: dict[str, MypyFile]) -&gt; None:
    for plugin in self._plugins:
        plugin.set_modules(modules)

</t>
<t tx="ekr.20221004064035.1011">def report_config_data(self, ctx: ReportConfigContext) -&gt; Any:
    config_data = [plugin.report_config_data(ctx) for plugin in self._plugins]
    return config_data if any(x is not None for x in config_data) else None

</t>
<t tx="ekr.20221004064035.1012">def get_additional_deps(self, file: MypyFile) -&gt; list[tuple[int, str, int]]:
    deps = []
    for plugin in self._plugins:
        deps.extend(plugin.get_additional_deps(file))
    return deps

</t>
<t tx="ekr.20221004064035.1013">def get_type_analyze_hook(self, fullname: str) -&gt; Callable[[AnalyzeTypeContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_type_analyze_hook(fullname))

</t>
<t tx="ekr.20221004064035.1014">def get_function_signature_hook(
    self, fullname: str
) -&gt; Callable[[FunctionSigContext], FunctionLike] | None:
    return self._find_hook(lambda plugin: plugin.get_function_signature_hook(fullname))

</t>
<t tx="ekr.20221004064035.1015">def get_function_hook(self, fullname: str) -&gt; Callable[[FunctionContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_function_hook(fullname))

</t>
<t tx="ekr.20221004064035.1016">def get_method_signature_hook(
    self, fullname: str
) -&gt; Callable[[MethodSigContext], FunctionLike] | None:
    return self._find_hook(lambda plugin: plugin.get_method_signature_hook(fullname))

</t>
<t tx="ekr.20221004064035.1017">def get_method_hook(self, fullname: str) -&gt; Callable[[MethodContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_method_hook(fullname))

</t>
<t tx="ekr.20221004064035.1018">def get_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_attribute_hook(fullname))

</t>
<t tx="ekr.20221004064035.1019">def get_class_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    return self._find_hook(lambda plugin: plugin.get_class_attribute_hook(fullname))

</t>
<t tx="ekr.20221004064035.102">def visit_type_var_tuple_expr(self, e: TypeVarTupleExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1020">def get_class_decorator_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_class_decorator_hook(fullname))

</t>
<t tx="ekr.20221004064035.1021">def get_class_decorator_hook_2(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], bool] | None:
    return self._find_hook(lambda plugin: plugin.get_class_decorator_hook_2(fullname))

</t>
<t tx="ekr.20221004064035.1022">def get_metaclass_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_metaclass_hook(fullname))

</t>
<t tx="ekr.20221004064035.1023">def get_base_class_hook(self, fullname: str) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_base_class_hook(fullname))

</t>
<t tx="ekr.20221004064035.1024">def get_customize_class_mro_hook(
    self, fullname: str
) -&gt; Callable[[ClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_customize_class_mro_hook(fullname))

</t>
<t tx="ekr.20221004064035.1025">def get_dynamic_class_hook(
    self, fullname: str
) -&gt; Callable[[DynamicClassDefContext], None] | None:
    return self._find_hook(lambda plugin: plugin.get_dynamic_class_hook(fullname))

</t>
<t tx="ekr.20221004064035.1026">def _find_hook(self, lookup: Callable[[Plugin], T]) -&gt; T | None:
    for plugin in self._plugins:
        hook = lookup(plugin)
        if hook:
            return hook
    return None
</t>
<t tx="ekr.20221004064035.1027">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

"""Utilities to find the site and prefix information of a Python executable.

This file MUST remain compatible with all Python 3.7+ versions. Since we cannot make any assumptions about the
Python being executed, this module should not use *any* dependencies outside of the standard
library found in Python 3.7. This file is run each mypy run, so it should be kept as fast as
possible.
"""
import os
import site
import sys
import sysconfig

if __name__ == "__main__":
    # HACK: We don't want to pick up mypy.types as the top-level types
    #       module. This could happen if this file is run as a script.
    #       This workaround fixes it.
    old_sys_path = sys.path
    sys.path = sys.path[1:]
    import types  # noqa: F401

    sys.path = old_sys_path


@others
if __name__ == "__main__":
    if sys.argv[-1] == "getsearchdirs":
        print(repr(getsearchdirs()))
    else:
        print("ERROR: incorrect argument to pyinfo.py.", file=sys.stderr)
        sys.exit(1)
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1028">def getsitepackages() -&gt; list[str]:
    res = []
    if hasattr(site, "getsitepackages"):
        res.extend(site.getsitepackages())

        if hasattr(site, "getusersitepackages") and site.ENABLE_USER_SITE:
            res.insert(0, site.getusersitepackages())
    else:
        from distutils.sysconfig import get_python_lib

        res = [get_python_lib()]
    return res


</t>
<t tx="ekr.20221004064035.1029">def getsyspath() -&gt; list[str]:
    # Do not include things from the standard library
    # because those should come from typeshed.
    stdlib_zip = os.path.join(
        sys.base_exec_prefix,
        getattr(sys, "platlibdir", "lib"),
        f"python{sys.version_info.major}{sys.version_info.minor}.zip",
    )
    stdlib = sysconfig.get_path("stdlib")
    stdlib_ext = os.path.join(stdlib, "lib-dynload")
    excludes = {stdlib_zip, stdlib, stdlib_ext}

    # Drop the first entry of sys.path
    # - If pyinfo.py is executed as a script (in a subprocess), this is the directory
    #   containing pyinfo.py
    # - Otherwise, if mypy launched via console script, this is the directory of the script
    # - Otherwise, if mypy launched via python -m mypy, this is the current directory
    # In all these cases, it is desirable to drop the first entry
    # Note that mypy adds the cwd to SearchPaths.python_path, so we still find things on the
    # cwd consistently (the return value here sets SearchPaths.package_path)

    # Python 3.11 adds a "safe_path" flag wherein Python won't automatically prepend
    # anything to sys.path. In this case, the first entry of sys.path is no longer special.
    offset = 0 if sys.version_info &gt;= (3, 11) and sys.flags.safe_path else 1

    abs_sys_path = (os.path.abspath(p) for p in sys.path[offset:])
    return [p for p in abs_sys_path if p not in excludes]


</t>
<t tx="ekr.20221004064035.103">def visit_type_alias_expr(self, e: TypeAliasExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1030">def getsearchdirs() -&gt; tuple[list[str], list[str]]:
    return (getsyspath(), getsitepackages())


</t>
<t tx="ekr.20221004064035.1031">@path C:/Repos/ekr-mypy2/mypy/
"""Utilities related to determining the reachability of code (in semantic analysis)."""

from __future__ import annotations

from typing import Tuple, TypeVar
from typing_extensions import Final

from mypy.literals import literal
from mypy.nodes import (
    LITERAL_YES,
    AssertStmt,
    Block,
    CallExpr,
    ComparisonExpr,
    Expression,
    FuncDef,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    MatchStmt,
    MemberExpr,
    NameExpr,
    OpExpr,
    SliceExpr,
    StrExpr,
    TupleExpr,
    UnaryExpr,
)
from mypy.options import Options
from mypy.patterns import AsPattern, OrPattern, Pattern
from mypy.traverser import TraverserVisitor

# Inferred truth value of an expression.
ALWAYS_TRUE: Final = 1
MYPY_TRUE: Final = 2  # True in mypy, False at runtime
ALWAYS_FALSE: Final = 3
MYPY_FALSE: Final = 4  # False in mypy, True at runtime
TRUTH_VALUE_UNKNOWN: Final = 5

inverted_truth_mapping: Final = {
    ALWAYS_TRUE: ALWAYS_FALSE,
    ALWAYS_FALSE: ALWAYS_TRUE,
    TRUTH_VALUE_UNKNOWN: TRUTH_VALUE_UNKNOWN,
    MYPY_TRUE: MYPY_FALSE,
    MYPY_FALSE: MYPY_TRUE,
}

reverse_op: Final = {"==": "==", "!=": "!=", "&lt;": "&gt;", "&gt;": "&lt;", "&lt;=": "&gt;=", "&gt;=": "&lt;="}


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1032">def infer_reachability_of_if_statement(s: IfStmt, options: Options) -&gt; None:
    for i in range(len(s.expr)):
        result = infer_condition_value(s.expr[i], options)
        if result in (ALWAYS_FALSE, MYPY_FALSE):
            # The condition is considered always false, so we skip the if/elif body.
            mark_block_unreachable(s.body[i])
        elif result in (ALWAYS_TRUE, MYPY_TRUE):
            # This condition is considered always true, so all of the remaining
            # elif/else bodies should not be checked.
            if result == MYPY_TRUE:
                # This condition is false at runtime; this will affect
                # import priorities.
                mark_block_mypy_only(s.body[i])
            for body in s.body[i + 1 :]:
                mark_block_unreachable(body)

            # Make sure else body always exists and is marked as
            # unreachable so the type checker always knows that
            # all control flow paths will flow through the if
            # statement body.
            if not s.else_body:
                s.else_body = Block([])
            mark_block_unreachable(s.else_body)
            break


</t>
<t tx="ekr.20221004064035.1033">def infer_reachability_of_match_statement(s: MatchStmt, options: Options) -&gt; None:
    for i, guard in enumerate(s.guards):
        pattern_value = infer_pattern_value(s.patterns[i])

        if guard is not None:
            guard_value = infer_condition_value(guard, options)
        else:
            guard_value = ALWAYS_TRUE

        if pattern_value in (ALWAYS_FALSE, MYPY_FALSE) or guard_value in (
            ALWAYS_FALSE,
            MYPY_FALSE,
        ):
            # The case is considered always false, so we skip the case body.
            mark_block_unreachable(s.bodies[i])
        elif pattern_value in (ALWAYS_FALSE, MYPY_TRUE) and guard_value in (
            ALWAYS_TRUE,
            MYPY_TRUE,
        ):
            for body in s.bodies[i + 1 :]:
                mark_block_unreachable(body)

        if guard_value == MYPY_TRUE:
            # This condition is false at runtime; this will affect
            # import priorities.
            mark_block_mypy_only(s.bodies[i])


</t>
<t tx="ekr.20221004064035.1034">def assert_will_always_fail(s: AssertStmt, options: Options) -&gt; bool:
    return infer_condition_value(s.expr, options) in (ALWAYS_FALSE, MYPY_FALSE)


</t>
<t tx="ekr.20221004064035.1035">def infer_condition_value(expr: Expression, options: Options) -&gt; int:
    """Infer whether the given condition is always true/false.

    Return ALWAYS_TRUE if always true, ALWAYS_FALSE if always false,
    MYPY_TRUE if true under mypy and false at runtime, MYPY_FALSE if
    false under mypy and true at runtime, else TRUTH_VALUE_UNKNOWN.
    """
    pyversion = options.python_version
    name = ""
    negated = False
    alias = expr
    if isinstance(alias, UnaryExpr):
        if alias.op == "not":
            expr = alias.expr
            negated = True
    result = TRUTH_VALUE_UNKNOWN
    if isinstance(expr, NameExpr):
        name = expr.name
    elif isinstance(expr, MemberExpr):
        name = expr.name
    elif isinstance(expr, OpExpr) and expr.op in ("and", "or"):
        left = infer_condition_value(expr.left, options)
        if (left in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "and") or (
            left in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "or"
        ):
            # Either `True and &lt;other&gt;` or `False or &lt;other&gt;`: the result will
            # always be the right-hand-side.
            return infer_condition_value(expr.right, options)
        else:
            # The result will always be the left-hand-side (e.g. ALWAYS_* or
            # TRUTH_VALUE_UNKNOWN).
            return left
    else:
        result = consider_sys_version_info(expr, pyversion)
        if result == TRUTH_VALUE_UNKNOWN:
            result = consider_sys_platform(expr, options.platform)
    if result == TRUTH_VALUE_UNKNOWN:
        if name == "PY2":
            result = ALWAYS_FALSE
        elif name == "PY3":
            result = ALWAYS_TRUE
        elif name == "MYPY" or name == "TYPE_CHECKING":
            result = MYPY_TRUE
        elif name in options.always_true:
            result = ALWAYS_TRUE
        elif name in options.always_false:
            result = ALWAYS_FALSE
    if negated:
        result = inverted_truth_mapping[result]
    return result


</t>
<t tx="ekr.20221004064035.1036">def infer_pattern_value(pattern: Pattern) -&gt; int:
    if isinstance(pattern, AsPattern) and pattern.pattern is None:
        return ALWAYS_TRUE
    elif isinstance(pattern, OrPattern) and any(
        infer_pattern_value(p) == ALWAYS_TRUE for p in pattern.patterns
    ):
        return ALWAYS_TRUE
    else:
        return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20221004064035.1037">def consider_sys_version_info(expr: Expression, pyversion: tuple[int, ...]) -&gt; int:
    """Consider whether expr is a comparison involving sys.version_info.

    Return ALWAYS_TRUE, ALWAYS_FALSE, or TRUTH_VALUE_UNKNOWN.
    """
    # Cases supported:
    # - sys.version_info[&lt;int&gt;] &lt;compare_op&gt; &lt;int&gt;
    # - sys.version_info[:&lt;int&gt;] &lt;compare_op&gt; &lt;tuple_of_n_ints&gt;
    # - sys.version_info &lt;compare_op&gt; &lt;tuple_of_1_or_2_ints&gt;
    #   (in this case &lt;compare_op&gt; must be &gt;, &gt;=, &lt;, &lt;=, but cannot be ==, !=)
    if not isinstance(expr, ComparisonExpr):
        return TRUTH_VALUE_UNKNOWN
    # Let's not yet support chained comparisons.
    if len(expr.operators) &gt; 1:
        return TRUTH_VALUE_UNKNOWN
    op = expr.operators[0]
    if op not in ("==", "!=", "&lt;=", "&gt;=", "&lt;", "&gt;"):
        return TRUTH_VALUE_UNKNOWN

    index = contains_sys_version_info(expr.operands[0])
    thing = contains_int_or_tuple_of_ints(expr.operands[1])
    if index is None or thing is None:
        index = contains_sys_version_info(expr.operands[1])
        thing = contains_int_or_tuple_of_ints(expr.operands[0])
        op = reverse_op[op]
    if isinstance(index, int) and isinstance(thing, int):
        # sys.version_info[i] &lt;compare_op&gt; k
        if 0 &lt;= index &lt;= 1:
            return fixed_comparison(pyversion[index], op, thing)
        else:
            return TRUTH_VALUE_UNKNOWN
    elif isinstance(index, tuple) and isinstance(thing, tuple):
        lo, hi = index
        if lo is None:
            lo = 0
        if hi is None:
            hi = 2
        if 0 &lt;= lo &lt; hi &lt;= 2:
            val = pyversion[lo:hi]
            if len(val) == len(thing) or len(val) &gt; len(thing) and op not in ("==", "!="):
                return fixed_comparison(val, op, thing)
    return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20221004064035.1038">def consider_sys_platform(expr: Expression, platform: str) -&gt; int:
    """Consider whether expr is a comparison involving sys.platform.

    Return ALWAYS_TRUE, ALWAYS_FALSE, or TRUTH_VALUE_UNKNOWN.
    """
    # Cases supported:
    # - sys.platform == 'posix'
    # - sys.platform != 'win32'
    # - sys.platform.startswith('win')
    if isinstance(expr, ComparisonExpr):
        # Let's not yet support chained comparisons.
        if len(expr.operators) &gt; 1:
            return TRUTH_VALUE_UNKNOWN
        op = expr.operators[0]
        if op not in ("==", "!="):
            return TRUTH_VALUE_UNKNOWN
        if not is_sys_attr(expr.operands[0], "platform"):
            return TRUTH_VALUE_UNKNOWN
        right = expr.operands[1]
        if not isinstance(right, StrExpr):
            return TRUTH_VALUE_UNKNOWN
        return fixed_comparison(platform, op, right.value)
    elif isinstance(expr, CallExpr):
        if not isinstance(expr.callee, MemberExpr):
            return TRUTH_VALUE_UNKNOWN
        if len(expr.args) != 1 or not isinstance(expr.args[0], StrExpr):
            return TRUTH_VALUE_UNKNOWN
        if not is_sys_attr(expr.callee.expr, "platform"):
            return TRUTH_VALUE_UNKNOWN
        if expr.callee.name != "startswith":
            return TRUTH_VALUE_UNKNOWN
        if platform.startswith(expr.args[0].value):
            return ALWAYS_TRUE
        else:
            return ALWAYS_FALSE
    else:
        return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20221004064035.1039">Targ = TypeVar("Targ", int, str, Tuple[int, ...])


</t>
<t tx="ekr.20221004064035.104">def visit_namedtuple_expr(self, e: NamedTupleExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1040">def fixed_comparison(left: Targ, op: str, right: Targ) -&gt; int:
    rmap = {False: ALWAYS_FALSE, True: ALWAYS_TRUE}
    if op == "==":
        return rmap[left == right]
    if op == "!=":
        return rmap[left != right]
    if op == "&lt;=":
        return rmap[left &lt;= right]
    if op == "&gt;=":
        return rmap[left &gt;= right]
    if op == "&lt;":
        return rmap[left &lt; right]
    if op == "&gt;":
        return rmap[left &gt; right]
    return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20221004064035.1041">def contains_int_or_tuple_of_ints(expr: Expression) -&gt; None | int | tuple[int, ...]:
    if isinstance(expr, IntExpr):
        return expr.value
    if isinstance(expr, TupleExpr):
        if literal(expr) == LITERAL_YES:
            thing = []
            for x in expr.items:
                if not isinstance(x, IntExpr):
                    return None
                thing.append(x.value)
            return tuple(thing)
    return None


</t>
<t tx="ekr.20221004064035.1042">def contains_sys_version_info(expr: Expression) -&gt; None | int | tuple[int | None, int | None]:
    if is_sys_attr(expr, "version_info"):
        return (None, None)  # Same as sys.version_info[:]
    if isinstance(expr, IndexExpr) and is_sys_attr(expr.base, "version_info"):
        index = expr.index
        if isinstance(index, IntExpr):
            return index.value
        if isinstance(index, SliceExpr):
            if index.stride is not None:
                if not isinstance(index.stride, IntExpr) or index.stride.value != 1:
                    return None
            begin = end = None
            if index.begin_index is not None:
                if not isinstance(index.begin_index, IntExpr):
                    return None
                begin = index.begin_index.value
            if index.end_index is not None:
                if not isinstance(index.end_index, IntExpr):
                    return None
                end = index.end_index.value
            return (begin, end)
    return None


</t>
<t tx="ekr.20221004064035.1043">def is_sys_attr(expr: Expression, name: str) -&gt; bool:
    # TODO: This currently doesn't work with code like this:
    # - import sys as _sys
    # - from sys import version_info
    if isinstance(expr, MemberExpr) and expr.name == name:
        if isinstance(expr.expr, NameExpr) and expr.expr.name == "sys":
            # TODO: Guard against a local named sys, etc.
            # (Though later passes will still do most checking.)
            return True
    return False


</t>
<t tx="ekr.20221004064035.1044">def mark_block_unreachable(block: Block) -&gt; None:
    block.is_unreachable = True
    block.accept(MarkImportsUnreachableVisitor())


</t>
<t tx="ekr.20221004064035.1045">class MarkImportsUnreachableVisitor(TraverserVisitor):
    """Visitor that flags all imports nested within a node as unreachable."""

    @others
</t>
<t tx="ekr.20221004064035.1046">def visit_import(self, node: Import) -&gt; None:
    node.is_unreachable = True

</t>
<t tx="ekr.20221004064035.1047">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_unreachable = True

</t>
<t tx="ekr.20221004064035.1048">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_unreachable = True


</t>
<t tx="ekr.20221004064035.1049">def mark_block_mypy_only(block: Block) -&gt; None:
    block.accept(MarkImportsMypyOnlyVisitor())


</t>
<t tx="ekr.20221004064035.105">def visit_enum_call_expr(self, e: EnumCallExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1050">class MarkImportsMypyOnlyVisitor(TraverserVisitor):
    """Visitor that sets is_mypy_only (which affects priority)."""

    @others
</t>
<t tx="ekr.20221004064035.1051">def visit_import(self, node: Import) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20221004064035.1052">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20221004064035.1053">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20221004064035.1054">def visit_func_def(self, node: FuncDef) -&gt; None:
    node.is_mypy_only = True
</t>
<t tx="ekr.20221004064035.1055">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from contextlib import contextmanager
from typing import Iterator
from typing_extensions import Final

from mypy.nodes import (
    AssignmentStmt,
    Block,
    BreakStmt,
    ClassDef,
    ContinueStmt,
    ForStmt,
    FuncDef,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    ListExpr,
    Lvalue,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NameExpr,
    StarExpr,
    TryStmt,
    TupleExpr,
    WhileStmt,
    WithStmt,
)
from mypy.patterns import AsPattern
from mypy.traverser import TraverserVisitor

# Scope kinds
FILE: Final = 0
FUNCTION: Final = 1
CLASS: Final = 2


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1056">class VariableRenameVisitor(TraverserVisitor):
    """Rename variables to allow redefinition of variables.

    For example, consider this code:

      x = 0
      f(x)

      x = "a"
      g(x)

    It will be transformed like this:

      x' = 0
      f(x')

      x = "a"
      g(x)

    There will be two independent variables (x' and x) that will have separate
    inferred types. The publicly exposed variant will get the non-suffixed name.
    This is the last definition at module top level and the first definition
    (argument) within a function.

    Renaming only happens for assignments within the same block. Renaming is
    performed before semantic analysis, immediately after parsing.

    The implementation performs a rudimentary static analysis. The analysis is
    overly conservative to keep things simple.
    """

    @others
</t>
<t tx="ekr.20221004064035.1057">def __init__(self) -&gt; None:
    # Counter for labeling new blocks
    self.block_id = 0
    # Number of surrounding try statements that disallow variable redefinition
    self.disallow_redef_depth = 0
    # Number of surrounding loop statements
    self.loop_depth = 0
    # Map block id to loop depth.
    self.block_loop_depth: dict[int, int] = {}
    # Stack of block ids being processed.
    self.blocks: list[int] = []
    # List of scopes; each scope maps short (unqualified) name to block id.
    self.var_blocks: list[dict[str, int]] = []

    # References to variables that we may need to rename. List of
    # scopes; each scope is a mapping from name to list of collections
    # of names that refer to the same logical variable.
    self.refs: list[dict[str, list[list[NameExpr]]]] = []
    # Number of reads of the most recent definition of a variable (per scope)
    self.num_reads: list[dict[str, int]] = []
    # Kinds of nested scopes (FILE, FUNCTION or CLASS)
    self.scope_kinds: list[int] = []

</t>
<t tx="ekr.20221004064035.1058">def visit_mypy_file(self, file_node: MypyFile) -&gt; None:
    """Rename variables within a file.

    This is the main entry point to this class.
    """
    self.clear()
    with self.enter_scope(FILE), self.enter_block():
        for d in file_node.defs:
            d.accept(self)

</t>
<t tx="ekr.20221004064035.1059">def visit_func_def(self, fdef: FuncDef) -&gt; None:
    # Conservatively do not allow variable defined before a function to
    # be redefined later, since function could refer to either definition.
    self.reject_redefinition_of_vars_in_scope()

    with self.enter_scope(FUNCTION), self.enter_block():
        for arg in fdef.arguments:
            name = arg.variable.name
            # 'self' can't be redefined since it's special as it allows definition of
            # attributes. 'cls' can't be used to define attributes so we can ignore it.
            can_be_redefined = name != "self"  # TODO: Proper check
            self.record_assignment(arg.variable.name, can_be_redefined)
            self.handle_arg(name)

        for stmt in fdef.body.body:
            stmt.accept(self)

</t>
<t tx="ekr.20221004064035.106">def visit_typeddict_expr(self, e: TypedDictExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1060">def visit_class_def(self, cdef: ClassDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope(CLASS):
        super().visit_class_def(cdef)

</t>
<t tx="ekr.20221004064035.1061">def visit_block(self, block: Block) -&gt; None:
    with self.enter_block():
        super().visit_block(block)

</t>
<t tx="ekr.20221004064035.1062">def visit_while_stmt(self, stmt: WhileStmt) -&gt; None:
    with self.enter_loop():
        super().visit_while_stmt(stmt)

</t>
<t tx="ekr.20221004064035.1063">def visit_for_stmt(self, stmt: ForStmt) -&gt; None:
    stmt.expr.accept(self)
    self.analyze_lvalue(stmt.index, True)
    # Also analyze as non-lvalue so that every for loop index variable is assumed to be read.
    stmt.index.accept(self)
    with self.enter_loop():
        stmt.body.accept(self)
    if stmt.else_body:
        stmt.else_body.accept(self)

</t>
<t tx="ekr.20221004064035.1064">def visit_break_stmt(self, stmt: BreakStmt) -&gt; None:
    self.reject_redefinition_of_vars_in_loop()

</t>
<t tx="ekr.20221004064035.1065">def visit_continue_stmt(self, stmt: ContinueStmt) -&gt; None:
    self.reject_redefinition_of_vars_in_loop()

</t>
<t tx="ekr.20221004064035.1066">def visit_try_stmt(self, stmt: TryStmt) -&gt; None:
    # Variables defined by a try statement get special treatment in the
    # type checker which allows them to be always redefined, so no need to
    # do renaming here.
    with self.enter_try():
        super().visit_try_stmt(stmt)

</t>
<t tx="ekr.20221004064035.1067">def visit_with_stmt(self, stmt: WithStmt) -&gt; None:
    for expr in stmt.expr:
        expr.accept(self)
    for target in stmt.target:
        if target is not None:
            self.analyze_lvalue(target)
    # We allow redefinitions in the body of a with statement for
    # convenience.  This is unsafe since with statements can affect control
    # flow by catching exceptions, but this is rare except for
    # assertRaises() and other similar functions, where the exception is
    # raised by the last statement in the body, which usually isn't a
    # problem.
    stmt.body.accept(self)

</t>
<t tx="ekr.20221004064035.1068">def visit_import(self, imp: Import) -&gt; None:
    for id, as_id in imp.ids:
        self.record_assignment(as_id or id, False)

</t>
<t tx="ekr.20221004064035.1069">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    for id, as_id in imp.names:
        self.record_assignment(as_id or id, False)

</t>
<t tx="ekr.20221004064035.107">def visit_newtype_expr(self, e: NewTypeExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1070">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    s.rvalue.accept(self)
    for lvalue in s.lvalues:
        self.analyze_lvalue(lvalue)

</t>
<t tx="ekr.20221004064035.1071">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    for i in range(len(s.patterns)):
        with self.enter_block():
            s.patterns[i].accept(self)
            guard = s.guards[i]
            if guard is not None:
                guard.accept(self)
            # We already entered a block, so visit this block's statements directly
            for stmt in s.bodies[i].body:
                stmt.accept(self)

</t>
<t tx="ekr.20221004064035.1072">def visit_capture_pattern(self, p: AsPattern) -&gt; None:
    if p.name is not None:
        self.analyze_lvalue(p.name)

</t>
<t tx="ekr.20221004064035.1073">def analyze_lvalue(self, lvalue: Lvalue, is_nested: bool = False) -&gt; None:
    """Process assignment; in particular, keep track of (re)defined names.

    Args:
        is_nested: True for non-outermost Lvalue in a multiple assignment such as
            "x, y = ..."
    """
    if isinstance(lvalue, NameExpr):
        name = lvalue.name
        is_new = self.record_assignment(name, True)
        if is_new:
            self.handle_def(lvalue)
        else:
            self.handle_refine(lvalue)
        if is_nested:
            # This allows these to be redefined freely even if never read. Multiple
            # assignment like "x, _ _ = y" defines dummy variables that are never read.
            self.handle_ref(lvalue)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.analyze_lvalue(item, is_nested=True)
    elif isinstance(lvalue, MemberExpr):
        lvalue.expr.accept(self)
    elif isinstance(lvalue, IndexExpr):
        lvalue.base.accept(self)
        lvalue.index.accept(self)
    elif isinstance(lvalue, StarExpr):
        # Propagate is_nested since in a typical use case like "x, *rest = ..." 'rest' may
        # be freely reused.
        self.analyze_lvalue(lvalue.expr, is_nested=is_nested)

</t>
<t tx="ekr.20221004064035.1074">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    self.handle_ref(expr)

</t>
<t tx="ekr.20221004064035.1075"># Helpers for renaming references

</t>
<t tx="ekr.20221004064035.1076">def handle_arg(self, name: str) -&gt; None:
    """Store function argument."""
    self.refs[-1][name] = [[]]
    self.num_reads[-1][name] = 0

</t>
<t tx="ekr.20221004064035.1077">def handle_def(self, expr: NameExpr) -&gt; None:
    """Store new name definition."""
    name = expr.name
    names = self.refs[-1].setdefault(name, [])
    names.append([expr])
    self.num_reads[-1][name] = 0

</t>
<t tx="ekr.20221004064035.1078">def handle_refine(self, expr: NameExpr) -&gt; None:
    """Store assignment to an existing name (that replaces previous value, if any)."""
    name = expr.name
    if name in self.refs[-1]:
        names = self.refs[-1][name]
        if not names:
            names.append([])
        names[-1].append(expr)

</t>
<t tx="ekr.20221004064035.1079">def handle_ref(self, expr: NameExpr) -&gt; None:
    """Store reference to defined name."""
    name = expr.name
    if name in self.refs[-1]:
        names = self.refs[-1][name]
        if not names:
            names.append([])
        names[-1].append(expr)
    num_reads = self.num_reads[-1]
    num_reads[name] = num_reads.get(name, 0) + 1

</t>
<t tx="ekr.20221004064035.108">def visit__promote_expr(self, e: PromoteExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1080">def flush_refs(self) -&gt; None:
    """Rename all references within the current scope.

    This will be called at the end of a scope.
    """
    is_func = self.scope_kinds[-1] == FUNCTION
    for name, refs in self.refs[-1].items():
        if len(refs) == 1:
            # Only one definition -- no renaming needed.
            continue
        if is_func:
            # In a function, don't rename the first definition, as it
            # may be an argument that must preserve the name.
            to_rename = refs[1:]
        else:
            # At module top level, don't rename the final definition,
            # as it will be publicly visible outside the module.
            to_rename = refs[:-1]
        for i, item in enumerate(to_rename):
            rename_refs(item, i)
    self.refs.pop()

</t>
<t tx="ekr.20221004064035.1081"># Helpers for determining which assignments define new variables

</t>
<t tx="ekr.20221004064035.1082">def clear(self) -&gt; None:
    self.blocks = []
    self.var_blocks = []

</t>
<t tx="ekr.20221004064035.1083">@contextmanager
def enter_block(self) -&gt; Iterator[None]:
    self.block_id += 1
    self.blocks.append(self.block_id)
    self.block_loop_depth[self.block_id] = self.loop_depth
    try:
        yield
    finally:
        self.blocks.pop()

</t>
<t tx="ekr.20221004064035.1084">@contextmanager
def enter_try(self) -&gt; Iterator[None]:
    self.disallow_redef_depth += 1
    try:
        yield
    finally:
        self.disallow_redef_depth -= 1

</t>
<t tx="ekr.20221004064035.1085">@contextmanager
def enter_loop(self) -&gt; Iterator[None]:
    self.loop_depth += 1
    try:
        yield
    finally:
        self.loop_depth -= 1

</t>
<t tx="ekr.20221004064035.1086">def current_block(self) -&gt; int:
    return self.blocks[-1]

</t>
<t tx="ekr.20221004064035.1087">@contextmanager
def enter_scope(self, kind: int) -&gt; Iterator[None]:
    self.var_blocks.append({})
    self.refs.append({})
    self.num_reads.append({})
    self.scope_kinds.append(kind)
    try:
        yield
    finally:
        self.flush_refs()
        self.var_blocks.pop()
        self.num_reads.pop()
        self.scope_kinds.pop()

</t>
<t tx="ekr.20221004064035.1088">def is_nested(self) -&gt; int:
    return len(self.var_blocks) &gt; 1

</t>
<t tx="ekr.20221004064035.1089">def reject_redefinition_of_vars_in_scope(self) -&gt; None:
    """Make it impossible to redefine defined variables in the current scope.

    This is used if we encounter a function definition that
    can make it ambiguous which definition is live. Example:

      x = 0

      def f() -&gt; int:
          return x

      x = ''  # Error -- cannot redefine x across function definition
    """
    var_blocks = self.var_blocks[-1]
    for key in var_blocks:
        var_blocks[key] = -1

</t>
<t tx="ekr.20221004064035.109">def visit_await_expr(self, e: AwaitExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.1090">def reject_redefinition_of_vars_in_loop(self) -&gt; None:
    """Reject redefinition of variables in the innermost loop.

    If there is an early exit from a loop, there may be ambiguity about which
    value may escape the loop. Example where this matters:

      while f():
          x = 0
          if g():
              break
          x = ''  # Error -- not a redefinition
      reveal_type(x)  # int

    This method ensures that the second assignment to 'x' doesn't introduce a new
    variable.
    """
    var_blocks = self.var_blocks[-1]
    for key, block in var_blocks.items():
        if self.block_loop_depth.get(block) == self.loop_depth:
            var_blocks[key] = -1

</t>
<t tx="ekr.20221004064035.1091">def record_assignment(self, name: str, can_be_redefined: bool) -&gt; bool:
    """Record assignment to given name and return True if it defines a new variable.

    Args:
        can_be_redefined: If True, allows assignment in the same block to redefine
            this name (if this is a new definition)
    """
    if self.num_reads[-1].get(name, -1) == 0:
        # Only set, not read, so no reason to redefine
        return False
    if self.disallow_redef_depth &gt; 0:
        # Can't redefine within try/with a block.
        can_be_redefined = False
    block = self.current_block()
    var_blocks = self.var_blocks[-1]
    if name not in var_blocks:
        # New definition in this scope.
        if can_be_redefined:
            # Store the block where this was defined to allow redefinition in
            # the same block only.
            var_blocks[name] = block
        else:
            # This doesn't support arbitrary redefinition.
            var_blocks[name] = -1
        return True
    elif var_blocks[name] == block:
        # Redefinition -- defines a new variable with the same name.
        return True
    else:
        # Assigns to an existing variable.
        return False


</t>
<t tx="ekr.20221004064035.1092">class LimitedVariableRenameVisitor(TraverserVisitor):
    """Perform some limited variable renaming in with statements.

    This allows reusing a variable in multiple with statements with
    different types. For example, the two instances of 'x' can have
    incompatible types:

       with C() as x:
           f(x)
       with D() as x:
           g(x)

    The above code gets renamed conceptually into this (not valid Python!):

       with C() as x':
           f(x')
       with D() as x:
           g(x)

    If there's a reference to a variable defined in 'with' outside the
    statement, or if there's any trickiness around variable visibility
    (e.g. function definitions), we give up and won't perform renaming.

    The main use case is to allow binding both readable and writable
    binary files into the same variable. These have different types:

        with open(fnam, 'rb') as f: ...
        with open(fnam, 'wb') as f: ...
    """

    @others
</t>
<t tx="ekr.20221004064035.1093">def __init__(self) -&gt; None:
    # Short names of variables bound in with statements using "as"
    # in a surrounding scope
    self.bound_vars: list[str] = []
    # Stack of names that can't be safely renamed, per scope ('*' means that
    # no names can be renamed)
    self.skipped: list[set[str]] = []
    # References to variables that we may need to rename. Stack of
    # scopes; each scope is a mapping from name to list of collections
    # of names that refer to the same logical variable.
    self.refs: list[dict[str, list[list[NameExpr]]]] = []

</t>
<t tx="ekr.20221004064035.1094">def visit_mypy_file(self, file_node: MypyFile) -&gt; None:
    """Rename variables within a file.

    This is the main entry point to this class.
    """
    with self.enter_scope():
        for d in file_node.defs:
            d.accept(self)

</t>
<t tx="ekr.20221004064035.1095">def visit_func_def(self, fdef: FuncDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope():
        for arg in fdef.arguments:
            self.record_skipped(arg.variable.name)
        super().visit_func_def(fdef)

</t>
<t tx="ekr.20221004064035.1096">def visit_class_def(self, cdef: ClassDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope():
        super().visit_class_def(cdef)

</t>
<t tx="ekr.20221004064035.1097">def visit_with_stmt(self, stmt: WithStmt) -&gt; None:
    for expr in stmt.expr:
        expr.accept(self)
    old_len = len(self.bound_vars)
    for target in stmt.target:
        if target is not None:
            self.analyze_lvalue(target)
    for target in stmt.target:
        if target:
            target.accept(self)
    stmt.body.accept(self)

    while len(self.bound_vars) &gt; old_len:
        self.bound_vars.pop()

</t>
<t tx="ekr.20221004064035.1098">def analyze_lvalue(self, lvalue: Lvalue) -&gt; None:
    if isinstance(lvalue, NameExpr):
        name = lvalue.name
        if name in self.bound_vars:
            # Name bound in a surrounding with statement, so it can be renamed
            self.visit_name_expr(lvalue)
        else:
            var_info = self.refs[-1]
            if name not in var_info:
                var_info[name] = []
            var_info[name].append([])
            self.bound_vars.append(name)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.analyze_lvalue(item)
    elif isinstance(lvalue, MemberExpr):
        lvalue.expr.accept(self)
    elif isinstance(lvalue, IndexExpr):
        lvalue.base.accept(self)
        lvalue.index.accept(self)
    elif isinstance(lvalue, StarExpr):
        self.analyze_lvalue(lvalue.expr)

</t>
<t tx="ekr.20221004064035.1099">def visit_import(self, imp: Import) -&gt; None:
    # We don't support renaming imports
    for id, as_id in imp.ids:
        self.record_skipped(as_id or id)

</t>
<t tx="ekr.20221004064035.11">class IPCServer(IPCBase):

    BUFFER_SIZE: Final = 2**16

    @others
</t>
<t tx="ekr.20221004064035.110">def visit_temp_node(self, e: TempNode) -&gt; None:
    return None


</t>
<t tx="ekr.20221004064035.1100">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    # We don't support renaming imports
    for id, as_id in imp.names:
        self.record_skipped(as_id or id)

</t>
<t tx="ekr.20221004064035.1101">def visit_import_all(self, imp: ImportAll) -&gt; None:
    # Give up, since we don't know all imported names yet
    self.reject_redefinition_of_vars_in_scope()

</t>
<t tx="ekr.20221004064035.1102">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    name = expr.name
    if name in self.bound_vars:
        # Record reference so that it can be renamed later
        for scope in reversed(self.refs):
            if name in scope:
                scope[name][-1].append(expr)
    else:
        self.record_skipped(name)

</t>
<t tx="ekr.20221004064035.1103">@contextmanager
def enter_scope(self) -&gt; Iterator[None]:
    self.skipped.append(set())
    self.refs.append({})
    yield None
    self.flush_refs()

</t>
<t tx="ekr.20221004064035.1104">def reject_redefinition_of_vars_in_scope(self) -&gt; None:
    self.record_skipped("*")

</t>
<t tx="ekr.20221004064035.1105">def record_skipped(self, name: str) -&gt; None:
    self.skipped[-1].add(name)

</t>
<t tx="ekr.20221004064035.1106">def flush_refs(self) -&gt; None:
    ref_dict = self.refs.pop()
    skipped = self.skipped.pop()
    if "*" not in skipped:
        for name, refs in ref_dict.items():
            if len(refs) &lt;= 1 or name in skipped:
                continue
            # At module top level we must not rename the final definition,
            # as it may be publicly visible
            to_rename = refs[:-1]
            for i, item in enumerate(to_rename):
                rename_refs(item, i)


</t>
<t tx="ekr.20221004064035.1107">def rename_refs(names: list[NameExpr], index: int) -&gt; None:
    name = names[0].name
    new_name = name + "'" * (index + 1)
    for expr in names:
        expr.name = new_name
</t>
<t tx="ekr.20221004064035.1108">@path C:/Repos/ekr-mypy2/mypy/
"""Classes for producing HTML reports about imprecision."""

from __future__ import annotations

import collections
import itertools
import json
import os
import shutil
import sys
import time
import tokenize
from abc import ABCMeta, abstractmethod
from operator import attrgetter
from typing import Any, Callable, Dict, Iterator, Tuple, cast
from typing_extensions import Final, TypeAlias as _TypeAlias
from urllib.request import pathname2url

from mypy import stats
from mypy.defaults import REPORTER_NAMES
from mypy.nodes import Expression, FuncDef, MypyFile
from mypy.options import Options
from mypy.traverser import TraverserVisitor
from mypy.types import Type, TypeOfAny
from mypy.version import __version__

try:
    from lxml import etree  # type: ignore[import]

    LXML_INSTALLED = True
except ImportError:
    LXML_INSTALLED = False

type_of_any_name_map: Final[collections.OrderedDict[int, str]] = collections.OrderedDict(
    [
        (TypeOfAny.unannotated, "Unannotated"),
        (TypeOfAny.explicit, "Explicit"),
        (TypeOfAny.from_unimported_type, "Unimported"),
        (TypeOfAny.from_omitted_generics, "Omitted Generics"),
        (TypeOfAny.from_error, "Error"),
        (TypeOfAny.special_form, "Special Form"),
        (TypeOfAny.implementation_artifact, "Implementation Artifact"),
    ]
)

ReporterClasses: _TypeAlias = Dict[
    str, Tuple[Callable[["Reports", str], "AbstractReporter"], bool],
]

reporter_classes: Final[ReporterClasses] = {}


@others
register_reporter("lineprecision", LinePrecisionReporter)


# Reporter class names are defined twice to speed up mypy startup, as this
# module is slow to import. Ensure that the two definitions match.
assert set(reporter_classes) == set(REPORTER_NAMES)
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1109">class Reports:
    @others
</t>
<t tx="ekr.20221004064035.111">@path C:/Repos/ekr-mypy2/mypy/
"""
This is a module for various lookup functions:
functions that will find a semantic node by its name.
"""

from __future__ import annotations

from mypy.nodes import MypyFile, SymbolTableNode, TypeInfo

# TODO: gradually move existing lookup functions to this module.


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1110">def __init__(self, data_dir: str, report_dirs: dict[str, str]) -&gt; None:
    self.data_dir = data_dir
    self.reporters: list[AbstractReporter] = []
    self.named_reporters: dict[str, AbstractReporter] = {}

    for report_type, report_dir in sorted(report_dirs.items()):
        self.add_report(report_type, report_dir)

</t>
<t tx="ekr.20221004064035.1111">def add_report(self, report_type: str, report_dir: str) -&gt; AbstractReporter:
    try:
        return self.named_reporters[report_type]
    except KeyError:
        pass
    reporter_cls, needs_lxml = reporter_classes[report_type]
    if needs_lxml and not LXML_INSTALLED:
        print(
            (
                "You must install the lxml package before you can run mypy"
                " with `--{}-report`.\n"
                "You can do this with `python3 -m pip install lxml`."
            ).format(report_type),
            file=sys.stderr,
        )
        raise ImportError
    reporter = reporter_cls(self, report_dir)
    self.reporters.append(reporter)
    self.named_reporters[report_type] = reporter
    return reporter

</t>
<t tx="ekr.20221004064035.1112">def file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    for reporter in self.reporters:
        reporter.on_file(tree, modules, type_map, options)

</t>
<t tx="ekr.20221004064035.1113">def finish(self) -&gt; None:
    for reporter in self.reporters:
        reporter.on_finish()


</t>
<t tx="ekr.20221004064035.1114">class AbstractReporter(metaclass=ABCMeta):
    @others
</t>
<t tx="ekr.20221004064035.1115">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    self.output_dir = output_dir
    if output_dir != "&lt;memory&gt;":
        stats.ensure_dir_exists(output_dir)

</t>
<t tx="ekr.20221004064035.1116">@abstractmethod
def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.1117">@abstractmethod
def on_finish(self) -&gt; None:
    pass


</t>
<t tx="ekr.20221004064035.1118">def register_reporter(
    report_name: str,
    reporter: Callable[[Reports, str], AbstractReporter],
    needs_lxml: bool = False,
) -&gt; None:
    reporter_classes[report_name] = (reporter, needs_lxml)


</t>
<t tx="ekr.20221004064035.1119">def alias_reporter(source_reporter: str, target_reporter: str) -&gt; None:
    reporter_classes[target_reporter] = reporter_classes[source_reporter]


</t>
<t tx="ekr.20221004064035.112">def lookup_fully_qualified(
    name: str, modules: dict[str, MypyFile], *, raise_on_missing: bool = False
) -&gt; SymbolTableNode | None:
    """Find a symbol using it fully qualified name.

    The algorithm has two steps: first we try splitting the name on '.' to find
    the module, then iteratively look for each next chunk after a '.' (e.g. for
    nested classes).

    This function should *not* be used to find a module. Those should be looked
    in the modules dictionary.
    """
    head = name
    rest = []
    # 1. Find a module tree in modules dictionary.
    while True:
        if "." not in head:
            if raise_on_missing:
                assert "." in head, f"Cannot find module for {name}"
            return None
        head, tail = head.rsplit(".", maxsplit=1)
        rest.append(tail)
        mod = modules.get(head)
        if mod is not None:
            break
    names = mod.names
    # 2. Find the symbol in the module tree.
    if not rest:
        # Looks like a module, don't use this to avoid confusions.
        if raise_on_missing:
            assert rest, f"Cannot find {name}, got a module symbol"
        return None
    while True:
        key = rest.pop()
        if key not in names:
            if raise_on_missing:
                assert key in names, f"Cannot find component {key!r} for {name!r}"
            return None
        stnode = names[key]
        if not rest:
            return stnode
        node = stnode.node
        # In fine-grained mode, could be a cross-reference to a deleted module
        # or a Var made up for a missing module.
        if not isinstance(node, TypeInfo):
            if raise_on_missing:
                assert node, f"Cannot find {name}"
            return None
        names = node.names
</t>
<t tx="ekr.20221004064035.1120">def should_skip_path(path: str) -&gt; bool:
    if stats.is_special_module(path):
        return True
    if path.startswith(".."):
        return True
    if "stubs" in path.split("/") or "stubs" in path.split(os.sep):
        return True
    return False


</t>
<t tx="ekr.20221004064035.1121">def iterate_python_lines(path: str) -&gt; Iterator[tuple[int, str]]:
    """Return an iterator over (line number, line text) from a Python file."""
    try:
        with tokenize.open(path) as input_file:
            yield from enumerate(input_file, 1)
    except IsADirectoryError:
        # can happen with namespace packages
        pass


</t>
<t tx="ekr.20221004064035.1122">class FuncCounterVisitor(TraverserVisitor):
    def __init__(self) -&gt; None:
        super().__init__()
        self.counts = [0, 0]

    def visit_func_def(self, defn: FuncDef) -&gt; None:
        self.counts[defn.type is not None] += 1


</t>
<t tx="ekr.20221004064035.1123">class LineCountReporter(AbstractReporter):
    @others
</t>
<t tx="ekr.20221004064035.1124">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.counts: dict[str, tuple[int, int, int, int]] = {}

</t>
<t tx="ekr.20221004064035.1125">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    # Count physical lines.  This assumes the file's encoding is a
    # superset of ASCII (or at least uses \n in its line endings).
    with open(tree.path, "rb") as f:
        physical_lines = len(f.readlines())

    func_counter = FuncCounterVisitor()
    tree.accept(func_counter)
    unannotated_funcs, annotated_funcs = func_counter.counts
    total_funcs = annotated_funcs + unannotated_funcs

    # Don't count lines or functions as annotated if they have their errors ignored.
    if options.ignore_errors:
        annotated_funcs = 0

    imputed_annotated_lines = (
        physical_lines * annotated_funcs // total_funcs if total_funcs else physical_lines
    )

    self.counts[tree._fullname] = (
        imputed_annotated_lines,
        physical_lines,
        annotated_funcs,
        total_funcs,
    )

</t>
<t tx="ekr.20221004064035.1126">def on_finish(self) -&gt; None:
    counts: list[tuple[tuple[int, int, int, int], str]] = sorted(
        ((c, p) for p, c in self.counts.items()), reverse=True
    )
    total_counts = tuple(sum(c[i] for c, p in counts) for i in range(4))
    with open(os.path.join(self.output_dir, "linecount.txt"), "w") as f:
        f.write("{:7} {:7} {:6} {:6} total\n".format(*total_counts))
        for c, p in counts:
            f.write(f"{c[0]:7} {c[1]:7} {c[2]:6} {c[3]:6} {p}\n")


</t>
<t tx="ekr.20221004064035.1127">register_reporter("linecount", LineCountReporter)


</t>
<t tx="ekr.20221004064035.1128">class AnyExpressionsReporter(AbstractReporter):
    """Report frequencies of different kinds of Any types."""

    @others
</t>
<t tx="ekr.20221004064035.1129">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.counts: dict[str, tuple[int, int]] = {}
    self.any_types_counter: dict[str, collections.Counter[int]] = {}

</t>
<t tx="ekr.20221004064035.113">@path C:/Repos/ekr-mypy2/mypy/
"""Mypy type checker command line tool."""

from __future__ import annotations

import argparse
import os
import subprocess
import sys
import time
from gettext import gettext
from typing import IO, Any, NoReturn, Sequence, TextIO
from typing_extensions import Final

from mypy import build, defaults, state, util
from mypy.config_parser import get_config_module_names, parse_config_file, parse_version
from mypy.errorcodes import error_codes
from mypy.errors import CompileError
from mypy.find_sources import InvalidSourceList, create_source_list
from mypy.fscache import FileSystemCache
from mypy.modulefinder import BuildSource, FindModuleCache, SearchPaths, get_search_dirs, mypy_path
from mypy.options import BuildType, Options
from mypy.split_namespace import SplitNamespace
from mypy.version import __version__

orig_stat: Final = os.stat
MEM_PROFILE: Final = False  # If True, dump memory profile


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1130">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
        visit_untyped_defs=False,
    )
    tree.accept(visitor)
    self.any_types_counter[tree.fullname] = visitor.type_of_any_counter
    num_unanalyzed_lines = list(visitor.line_map.values()).count(stats.TYPE_UNANALYZED)
    # count each line of dead code as one expression of type "Any"
    num_any = visitor.num_any_exprs + num_unanalyzed_lines
    num_total = visitor.num_imprecise_exprs + visitor.num_precise_exprs + num_any
    if num_total &gt; 0:
        self.counts[tree.fullname] = (num_any, num_total)

</t>
<t tx="ekr.20221004064035.1131">def on_finish(self) -&gt; None:
    self._report_any_exprs()
    self._report_types_of_anys()

</t>
<t tx="ekr.20221004064035.1132">def _write_out_report(
    self, filename: str, header: list[str], rows: list[list[str]], footer: list[str]
) -&gt; None:
    row_len = len(header)
    assert all(len(row) == row_len for row in rows + [header, footer])
    min_column_distance = 3  # minimum distance between numbers in two columns
    widths = [-1] * row_len
    for row in rows + [header, footer]:
        for i, value in enumerate(row):
            widths[i] = max(widths[i], len(value))
    for i, w in enumerate(widths):
        # Do not add min_column_distance to the first column.
        if i &gt; 0:
            widths[i] = w + min_column_distance
    with open(os.path.join(self.output_dir, filename), "w") as f:
        header_str = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(header, widths)))
        separator = "-" * len(header_str)
        f.write(header_str + "\n")
        f.write(separator + "\n")
        for row_values in rows:
            r = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(row_values, widths)))
            f.write(r + "\n")
        f.write(separator + "\n")
        footer_str = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(footer, widths)))
        f.write(footer_str + "\n")

</t>
<t tx="ekr.20221004064035.1133">def _report_any_exprs(self) -&gt; None:
    total_any = sum(num_any for num_any, _ in self.counts.values())
    total_expr = sum(total for _, total in self.counts.values())
    total_coverage = 100.0
    if total_expr &gt; 0:
        total_coverage = (float(total_expr - total_any) / float(total_expr)) * 100

    column_names = ["Name", "Anys", "Exprs", "Coverage"]
    rows: list[list[str]] = []
    for filename in sorted(self.counts):
        (num_any, num_total) = self.counts[filename]
        coverage = (float(num_total - num_any) / float(num_total)) * 100
        coverage_str = f"{coverage:.2f}%"
        rows.append([filename, str(num_any), str(num_total), coverage_str])
    rows.sort(key=lambda x: x[0])
    total_row = ["Total", str(total_any), str(total_expr), f"{total_coverage:.2f}%"]
    self._write_out_report("any-exprs.txt", column_names, rows, total_row)

</t>
<t tx="ekr.20221004064035.1134">def _report_types_of_anys(self) -&gt; None:
    total_counter: collections.Counter[int] = collections.Counter()
    for counter in self.any_types_counter.values():
        for any_type, value in counter.items():
            total_counter[any_type] += value
    file_column_name = "Name"
    total_row_name = "Total"
    column_names = [file_column_name] + list(type_of_any_name_map.values())
    rows: list[list[str]] = []
    for filename, counter in self.any_types_counter.items():
        rows.append([filename] + [str(counter[typ]) for typ in type_of_any_name_map])
    rows.sort(key=lambda x: x[0])
    total_row = [total_row_name] + [str(total_counter[typ]) for typ in type_of_any_name_map]
    self._write_out_report("types-of-anys.txt", column_names, rows, total_row)


</t>
<t tx="ekr.20221004064035.1135">register_reporter("any-exprs", AnyExpressionsReporter)


</t>
<t tx="ekr.20221004064035.1136">class LineCoverageVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.1137">def __init__(self, source: list[str]) -&gt; None:
    self.source = source

    # For each line of source, we maintain a pair of
    #  * the indentation level of the surrounding function
    #    (-1 if not inside a function), and
    #  * whether the surrounding function is typed.
    # Initially, everything is covered at indentation level -1.
    self.lines_covered = [(-1, True) for l in source]

</t>
<t tx="ekr.20221004064035.1138"># The Python AST has position information for the starts of
# elements, but not for their ends. Fortunately the
# indentation-based syntax makes it pretty easy to find where a
# block ends without doing any real parsing.

# TODO: Handle line continuations (explicit and implicit) and
# multi-line string literals. (But at least line continuations
# are normally more indented than their surrounding block anyways,
# by PEP 8.)

</t>
<t tx="ekr.20221004064035.1139">def indentation_level(self, line_number: int) -&gt; int | None:
    """Return the indentation of a line of the source (specified by
    zero-indexed line number). Returns None for blank lines or comments."""
    line = self.source[line_number]
    indent = 0
    for char in list(line):
        if char == " ":
            indent += 1
        elif char == "\t":
            indent = 8 * ((indent + 8) // 8)
        elif char == "#":
            # Line is a comment; ignore it
            return None
        elif char == "\n":
            # Line is entirely whitespace; ignore it
            return None
        # TODO line continuation (\)
        else:
            # Found a non-whitespace character
            return indent
    # Line is entirely whitespace, and at end of file
    # with no trailing newline; ignore it
    return None

</t>
<t tx="ekr.20221004064035.114">def stat_proxy(path: str) -&gt; os.stat_result:
    try:
        st = orig_stat(path)
    except os.error as err:
        print(f"stat({path!r}) -&gt; {err}")
        raise
    else:
        print(
            "stat(%r) -&gt; (st_mode=%o, st_mtime=%d, st_size=%d)"
            % (path, st.st_mode, st.st_mtime, st.st_size)
        )
        return st


</t>
<t tx="ekr.20221004064035.1140">def visit_func_def(self, defn: FuncDef) -&gt; None:
    start_line = defn.get_line() - 1
    start_indent = None
    # When a function is decorated, sometimes the start line will point to
    # whitespace or comments between the decorator and the function, so
    # we have to look for the start.
    while start_line &lt; len(self.source):
        start_indent = self.indentation_level(start_line)
        if start_indent is not None:
            break
        start_line += 1
    # If we can't find the function give up and don't annotate anything.
    # Our line numbers are not reliable enough to be asserting on.
    if start_indent is None:
        return

    cur_line = start_line + 1
    end_line = cur_line
    # After this loop, function body will be lines [start_line, end_line)
    while cur_line &lt; len(self.source):
        cur_indent = self.indentation_level(cur_line)
        if cur_indent is None:
            # Consume the line, but don't mark it as belonging to the function yet.
            cur_line += 1
        elif cur_indent &gt; start_indent:
            # A non-blank line that belongs to the function.
            cur_line += 1
            end_line = cur_line
        else:
            # We reached a line outside the function definition.
            break

    is_typed = defn.type is not None
    for line in range(start_line, end_line):
        old_indent, _ = self.lines_covered[line]
        # If there was an old indent level for this line, and the new
        # level isn't increasing the indentation, ignore it.
        # This is to be defensive against funniness in our line numbers,
        # which are not always reliable.
        if old_indent &lt;= start_indent:
            self.lines_covered[line] = (start_indent, is_typed)

    # Visit the body, in case there are nested functions
    super().visit_func_def(defn)


</t>
<t tx="ekr.20221004064035.1141">class LineCoverageReporter(AbstractReporter):
    """Exact line coverage reporter.

    This reporter writes a JSON dictionary with one field 'lines' to
    the file 'coverage.json' in the specified report directory. The
    value of that field is a dictionary which associates to each
    source file's absolute pathname the list of line numbers that
    belong to typed functions in that file.
    """

    @others
</t>
<t tx="ekr.20221004064035.1142">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.lines_covered: dict[str, list[int]] = {}

</t>
<t tx="ekr.20221004064035.1143">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    with open(tree.path) as f:
        tree_source = f.readlines()

    coverage_visitor = LineCoverageVisitor(tree_source)
    tree.accept(coverage_visitor)

    covered_lines = []
    for line_number, (_, typed) in enumerate(coverage_visitor.lines_covered):
        if typed:
            covered_lines.append(line_number + 1)

    self.lines_covered[os.path.abspath(tree.path)] = covered_lines

</t>
<t tx="ekr.20221004064035.1144">def on_finish(self) -&gt; None:
    with open(os.path.join(self.output_dir, "coverage.json"), "w") as f:
        json.dump({"lines": self.lines_covered}, f)


</t>
<t tx="ekr.20221004064035.1145">register_reporter("linecoverage", LineCoverageReporter)


</t>
<t tx="ekr.20221004064035.1146">class FileInfo:
    @others
</t>
<t tx="ekr.20221004064035.1147">def __init__(self, name: str, module: str) -&gt; None:
    self.name = name
    self.module = module
    self.counts = [0] * len(stats.precision_names)

</t>
<t tx="ekr.20221004064035.1148">def total(self) -&gt; int:
    return sum(self.counts)

</t>
<t tx="ekr.20221004064035.1149">def attrib(self) -&gt; dict[str, str]:
    return {name: str(val) for name, val in sorted(zip(stats.precision_names, self.counts))}


</t>
<t tx="ekr.20221004064035.115">def main(
    *,
    args: list[str] | None = None,
    stdout: TextIO = sys.stdout,
    stderr: TextIO = sys.stderr,
    clean_exit: bool = False,
) -&gt; None:
    """Main entry point to the type checker.

    Args:
        args: Custom command-line arguments.  If not given, sys.argv[1:] will
            be used.
        clean_exit: Don't hard kill the process on exit. This allows catching
            SystemExit.
    """
    util.check_python_version("mypy")
    t0 = time.time()
    # To log stat() calls: os.stat = stat_proxy
    sys.setrecursionlimit(2**14)
    if args is None:
        args = sys.argv[1:]

    fscache = FileSystemCache()
    sources, options = process_options(args, stdout=stdout, stderr=stderr, fscache=fscache)
    if clean_exit:
        options.fast_exit = False

    formatter = util.FancyFormatter(stdout, stderr, options.hide_error_codes)

    if options.install_types and (stdout is not sys.stdout or stderr is not sys.stderr):
        # Since --install-types performs user input, we want regular stdout and stderr.
        fail("error: --install-types not supported in this mode of running mypy", stderr, options)

    if options.non_interactive and not options.install_types:
        fail("error: --non-interactive is only supported with --install-types", stderr, options)

    if options.install_types and not options.incremental:
        fail(
            "error: --install-types not supported with incremental mode disabled", stderr, options
        )

    if options.install_types and options.python_executable is None:
        fail(
            "error: --install-types not supported without python executable or site packages",
            stderr,
            options,
        )

    if options.install_types and not sources:
        install_types(formatter, options, non_interactive=options.non_interactive)
        return

    res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)

    if options.non_interactive:
        missing_pkgs = read_types_packages_to_install(options.cache_dir, after_run=True)
        if missing_pkgs:
            # Install missing type packages and rerun build.
            install_types(formatter, options, after_run=True, non_interactive=True)
            fscache.flush()
            print()
            res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)
        show_messages(messages, stderr, formatter, options)

    if MEM_PROFILE:
        from mypy.memprofile import print_memory_profile

        print_memory_profile()

    code = 0
    if messages:
        code = 2 if blockers else 1
    if options.error_summary:
        n_errors, n_notes, n_files = util.count_stats(messages)
        if n_errors:
            summary = formatter.format_error(
                n_errors, n_files, len(sources), blockers=blockers, use_color=options.color_output
            )
            stdout.write(summary + "\n")
        # Only notes should also output success
        elif not messages or n_notes == len(messages):
            stdout.write(formatter.format_success(len(sources), options.color_output) + "\n")
        stdout.flush()

    if options.install_types and not options.non_interactive:
        result = install_types(formatter, options, after_run=True, non_interactive=False)
        if result:
            print()
            print("note: Run mypy again for up-to-date results with installed types")
            code = 2

    if options.fast_exit:
        # Exit without freeing objects -- it's faster.
        #
        # NOTE: We don't flush all open files on exit (or run other destructors)!
        util.hard_exit(code)
    elif code:
        sys.exit(code)

    # HACK: keep res alive so that mypyc won't free it before the hard_exit
    list([res])


</t>
<t tx="ekr.20221004064035.1150">class MemoryXmlReporter(AbstractReporter):
    """Internal reporter that generates XML in memory.

    This is used by all other XML-based reporters to avoid duplication.
    """

    @others
</t>
<t tx="ekr.20221004064035.1151">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_html_path = os.path.join(reports.data_dir, "xml", "mypy-html.xslt")
    self.xslt_txt_path = os.path.join(reports.data_dir, "xml", "mypy-txt.xslt")
    self.css_html_path = os.path.join(reports.data_dir, "xml", "mypy-html.css")
    xsd_path = os.path.join(reports.data_dir, "xml", "mypy.xsd")
    self.schema = etree.XMLSchema(etree.parse(xsd_path))
    self.last_xml: Any | None = None
    self.files: list[FileInfo] = []

</t>
<t tx="ekr.20221004064035.1152"># XML doesn't like control characters, but they are sometimes
# legal in source code (e.g. comments, string literals).
# Tabs (#x09) are allowed in XML content.
control_fixer: Final = str.maketrans("".join(chr(i) for i in range(32) if i != 9), "?" * 31)

</t>
<t tx="ekr.20221004064035.1153">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    self.last_xml = None

    try:
        path = os.path.relpath(tree.path)
    except ValueError:
        return

    if should_skip_path(path) or os.path.isdir(path):
        return  # `path` can sometimes be a directory, see #11334

    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
    )
    tree.accept(visitor)

    root = etree.Element("mypy-report-file", name=path, module=tree._fullname)
    doc = etree.ElementTree(root)
    file_info = FileInfo(path, tree._fullname)

    for lineno, line_text in iterate_python_lines(path):
        status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
        file_info.counts[status] += 1
        etree.SubElement(
            root,
            "line",
            any_info=self._get_any_info_for_line(visitor, lineno),
            content=line_text.rstrip("\n").translate(self.control_fixer),
            number=str(lineno),
            precision=stats.precision_names[status],
        )
    # Assumes a layout similar to what XmlReporter uses.
    xslt_path = os.path.relpath("mypy-html.xslt", path)
    transform_pi = etree.ProcessingInstruction(
        "xml-stylesheet", f'type="text/xsl" href="{pathname2url(xslt_path)}"'
    )
    root.addprevious(transform_pi)
    self.schema.assertValid(doc)

    self.last_xml = doc
    self.files.append(file_info)

</t>
<t tx="ekr.20221004064035.1154">@staticmethod
def _get_any_info_for_line(visitor: stats.StatisticsVisitor, lineno: int) -&gt; str:
    if lineno in visitor.any_line_map:
        result = "Any Types on this line: "
        counter: collections.Counter[int] = collections.Counter()
        for typ in visitor.any_line_map[lineno]:
            counter[typ.type_of_any] += 1
        for any_type, occurrences in counter.items():
            result += f"\n{type_of_any_name_map[any_type]} (x{occurrences})"
        return result
    else:
        return "No Anys on this line!"

</t>
<t tx="ekr.20221004064035.1155">def on_finish(self) -&gt; None:
    self.last_xml = None
    # index_path = os.path.join(self.output_dir, 'index.xml')
    output_files = sorted(self.files, key=lambda x: x.module)

    root = etree.Element("mypy-report-index", name="index")
    doc = etree.ElementTree(root)

    for file_info in output_files:
        etree.SubElement(
            root,
            "file",
            file_info.attrib(),
            module=file_info.module,
            name=pathname2url(file_info.name),
            total=str(file_info.total()),
        )
    xslt_path = os.path.relpath("mypy-html.xslt", ".")
    transform_pi = etree.ProcessingInstruction(
        "xml-stylesheet", f'type="text/xsl" href="{pathname2url(xslt_path)}"'
    )
    root.addprevious(transform_pi)
    self.schema.assertValid(doc)

    self.last_xml = doc


</t>
<t tx="ekr.20221004064035.1156">register_reporter("memory-xml", MemoryXmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20221004064035.1157">def get_line_rate(covered_lines: int, total_lines: int) -&gt; str:
    if total_lines == 0:
        return str(1.0)
    else:
        return f"{covered_lines / total_lines:.4f}"


</t>
<t tx="ekr.20221004064035.1158">class CoberturaPackage:
    """Container for XML and statistics mapping python modules to Cobertura package."""

    @others
</t>
<t tx="ekr.20221004064035.1159">def __init__(self, name: str) -&gt; None:
    self.name = name
    self.classes: dict[str, Any] = {}
    self.packages: dict[str, CoberturaPackage] = {}
    self.total_lines = 0
    self.covered_lines = 0

</t>
<t tx="ekr.20221004064035.116">def run_build(
    sources: list[BuildSource],
    options: Options,
    fscache: FileSystemCache,
    t0: float,
    stdout: TextIO,
    stderr: TextIO,
) -&gt; tuple[build.BuildResult | None, list[str], bool]:
    formatter = util.FancyFormatter(stdout, stderr, options.hide_error_codes)

    messages = []

    @others
    serious = False
    blockers = False
    res = None
    try:
        # Keep a dummy reference (res) for memory profiling afterwards, as otherwise
        # the result could be freed.
        res = build.build(sources, options, None, flush_errors, fscache, stdout, stderr)
    except CompileError as e:
        blockers = True
        if not e.use_stdout:
            serious = True
    if (
        options.warn_unused_configs
        and options.unused_configs
        and not options.incremental
        and not options.non_interactive
    ):
        print(
            "Warning: unused section(s) in %s: %s"
            % (
                options.config_file,
                get_config_module_names(
                    options.config_file,
                    [
                        glob
                        for glob in options.per_module_options.keys()
                        if glob in options.unused_configs
                    ],
                ),
            ),
            file=stderr,
        )
    maybe_write_junit_xml(time.time() - t0, serious, messages, options)
    return res, messages, blockers


</t>
<t tx="ekr.20221004064035.1160">def as_xml(self) -&gt; Any:
    package_element = etree.Element("package", complexity="1.0", name=self.name)
    package_element.attrib["branch-rate"] = "0"
    package_element.attrib["line-rate"] = get_line_rate(self.covered_lines, self.total_lines)
    classes_element = etree.SubElement(package_element, "classes")
    for class_name in sorted(self.classes):
        classes_element.append(self.classes[class_name])
    self.add_packages(package_element)
    return package_element

</t>
<t tx="ekr.20221004064035.1161">def add_packages(self, parent_element: Any) -&gt; None:
    if self.packages:
        packages_element = etree.SubElement(parent_element, "packages")
        for package in sorted(self.packages.values(), key=attrgetter("name")):
            packages_element.append(package.as_xml())


</t>
<t tx="ekr.20221004064035.1162">class CoberturaXmlReporter(AbstractReporter):
    """Reporter for generating Cobertura compliant XML."""

    @others
</t>
<t tx="ekr.20221004064035.1163">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.root = etree.Element("coverage", timestamp=str(int(time.time())), version=__version__)
    self.doc = etree.ElementTree(self.root)
    self.root_package = CoberturaPackage(".")

</t>
<t tx="ekr.20221004064035.1164">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    path = os.path.relpath(tree.path)
    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
    )
    tree.accept(visitor)

    class_name = os.path.basename(path)
    file_info = FileInfo(path, tree._fullname)
    class_element = etree.Element("class", complexity="1.0", filename=path, name=class_name)
    etree.SubElement(class_element, "methods")
    lines_element = etree.SubElement(class_element, "lines")

    with tokenize.open(path) as input_file:
        class_lines_covered = 0
        class_total_lines = 0
        for lineno, _ in enumerate(input_file, 1):
            status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
            hits = 0
            branch = False
            if status == stats.TYPE_EMPTY:
                continue
            class_total_lines += 1
            if status != stats.TYPE_ANY:
                class_lines_covered += 1
                hits = 1
            if status == stats.TYPE_IMPRECISE:
                branch = True
            file_info.counts[status] += 1
            line_element = etree.SubElement(
                lines_element,
                "line",
                branch=str(branch).lower(),
                hits=str(hits),
                number=str(lineno),
                precision=stats.precision_names[status],
            )
            if branch:
                line_element.attrib["condition-coverage"] = "50% (1/2)"
        class_element.attrib["branch-rate"] = "0"
        class_element.attrib["line-rate"] = get_line_rate(
            class_lines_covered, class_total_lines
        )
        # parent_module is set to whichever module contains this file.  For most files, we want
        # to simply strip the last element off of the module.  But for __init__.py files,
        # the module == the parent module.
        parent_module = file_info.module.rsplit(".", 1)[0]
        if file_info.name.endswith("__init__.py"):
            parent_module = file_info.module

        if parent_module not in self.root_package.packages:
            self.root_package.packages[parent_module] = CoberturaPackage(parent_module)
        current_package = self.root_package.packages[parent_module]
        packages_to_update = [self.root_package, current_package]
        for package in packages_to_update:
            package.total_lines += class_total_lines
            package.covered_lines += class_lines_covered
        current_package.classes[class_name] = class_element

</t>
<t tx="ekr.20221004064035.1165">def on_finish(self) -&gt; None:
    self.root.attrib["line-rate"] = get_line_rate(
        self.root_package.covered_lines, self.root_package.total_lines
    )
    self.root.attrib["branch-rate"] = "0"
    sources = etree.SubElement(self.root, "sources")
    source_element = etree.SubElement(sources, "source")
    source_element.text = os.getcwd()
    self.root_package.add_packages(self.root)
    out_path = os.path.join(self.output_dir, "cobertura.xml")
    self.doc.write(out_path, encoding="utf-8", pretty_print=True)
    print("Generated Cobertura report:", os.path.abspath(out_path))


</t>
<t tx="ekr.20221004064035.1166">register_reporter("cobertura-xml", CoberturaXmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20221004064035.1167">class AbstractXmlReporter(AbstractReporter):
    """Internal abstract class for reporters that work via XML."""

    @others
</t>
<t tx="ekr.20221004064035.1168">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    memory_reporter = reports.add_report("memory-xml", "&lt;memory&gt;")
    # The dependency will be called first.
    self.memory_xml = cast(MemoryXmlReporter, memory_reporter)


</t>
<t tx="ekr.20221004064035.1169">class XmlReporter(AbstractXmlReporter):
    """Public reporter that exports XML.

    The produced XML files contain a reference to the absolute path
    of the html transform, so they will be locally viewable in a browser.

    However, there is a bug in Chrome and all other WebKit-based browsers
    that makes it fail from file:// URLs but work on http:// URLs.
    """

    @others
</t>
<t tx="ekr.20221004064035.117">def flush_errors(new_messages: list[str], serious: bool) -&gt; None:
    if options.pretty:
        new_messages = formatter.fit_in_terminal(new_messages)
    messages.extend(new_messages)
    if options.non_interactive:
        # Collect messages and possibly show them later.
        return
    f = stderr if serious else stdout
    show_messages(new_messages, f, formatter, options)

</t>
<t tx="ekr.20221004064035.1170">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    last_xml = self.memory_xml.last_xml
    if last_xml is None:
        return
    path = os.path.relpath(tree.path)
    if path.startswith(".."):
        return
    out_path = os.path.join(self.output_dir, "xml", path + ".xml")
    stats.ensure_dir_exists(os.path.dirname(out_path))
    last_xml.write(out_path, encoding="utf-8")

</t>
<t tx="ekr.20221004064035.1171">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, "index.xml")
    out_xslt = os.path.join(self.output_dir, "mypy-html.xslt")
    out_css = os.path.join(self.output_dir, "mypy-html.css")
    last_xml.write(out_path, encoding="utf-8")
    shutil.copyfile(self.memory_xml.xslt_html_path, out_xslt)
    shutil.copyfile(self.memory_xml.css_html_path, out_css)
    print("Generated XML report:", os.path.abspath(out_path))


</t>
<t tx="ekr.20221004064035.1172">register_reporter("xml", XmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20221004064035.1173">class XsltHtmlReporter(AbstractXmlReporter):
    """Public reporter that exports HTML via XSLT.

    This is slightly different than running `xsltproc` on the .xml files,
    because it passes a parameter to rewrite the links.
    """

    @others
</t>
<t tx="ekr.20221004064035.1174">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_html = etree.XSLT(etree.parse(self.memory_xml.xslt_html_path))
    self.param_html = etree.XSLT.strparam("html")

</t>
<t tx="ekr.20221004064035.1175">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    last_xml = self.memory_xml.last_xml
    if last_xml is None:
        return
    path = os.path.relpath(tree.path)
    if path.startswith(".."):
        return
    out_path = os.path.join(self.output_dir, "html", path + ".html")
    stats.ensure_dir_exists(os.path.dirname(out_path))
    transformed_html = bytes(self.xslt_html(last_xml, ext=self.param_html))
    with open(out_path, "wb") as out_file:
        out_file.write(transformed_html)

</t>
<t tx="ekr.20221004064035.1176">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, "index.html")
    out_css = os.path.join(self.output_dir, "mypy-html.css")
    transformed_html = bytes(self.xslt_html(last_xml, ext=self.param_html))
    with open(out_path, "wb") as out_file:
        out_file.write(transformed_html)
    shutil.copyfile(self.memory_xml.css_html_path, out_css)
    print("Generated HTML report (via XSLT):", os.path.abspath(out_path))


</t>
<t tx="ekr.20221004064035.1177">register_reporter("xslt-html", XsltHtmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20221004064035.1178">class XsltTxtReporter(AbstractXmlReporter):
    """Public reporter that exports TXT via XSLT.

    Currently this only does the summary, not the individual reports.
    """

    @others
</t>
<t tx="ekr.20221004064035.1179">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_txt = etree.XSLT(etree.parse(self.memory_xml.xslt_txt_path))

</t>
<t tx="ekr.20221004064035.118">def show_messages(
    messages: list[str], f: TextIO, formatter: util.FancyFormatter, options: Options
) -&gt; None:
    for msg in messages:
        if options.color_output:
            msg = formatter.colorize(msg)
        f.write(msg + "\n")
    f.flush()


</t>
<t tx="ekr.20221004064035.1180">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.1181">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, "index.txt")
    transformed_txt = bytes(self.xslt_txt(last_xml))
    with open(out_path, "wb") as out_file:
        out_file.write(transformed_txt)
    print("Generated TXT report (via XSLT):", os.path.abspath(out_path))


</t>
<t tx="ekr.20221004064035.1182">register_reporter("xslt-txt", XsltTxtReporter, needs_lxml=True)

alias_reporter("xslt-html", "html")
alias_reporter("xslt-txt", "txt")


</t>
<t tx="ekr.20221004064035.1183">class LinePrecisionReporter(AbstractReporter):
    """Report per-module line counts for typing precision.

    Each line is classified into one of these categories:

    * precise (fully type checked)
    * imprecise (Any types in a type component, such as List[Any])
    * any (something with an Any type, implicit or explicit)
    * empty (empty line, comment or docstring)
    * unanalyzed (mypy considers line unreachable)

    The meaning of these categories varies slightly depending on
    context.
    """

    @others
</t>
<t tx="ekr.20221004064035.1184">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.files: list[FileInfo] = []

</t>
<t tx="ekr.20221004064035.1185">def on_file(
    self,
    tree: MypyFile,
    modules: dict[str, MypyFile],
    type_map: dict[Expression, Type],
    options: Options,
) -&gt; None:

    try:
        path = os.path.relpath(tree.path)
    except ValueError:
        return

    if should_skip_path(path):
        return

    visitor = stats.StatisticsVisitor(
        inferred=True,
        filename=tree.fullname,
        modules=modules,
        typemap=type_map,
        all_nodes=True,
    )
    tree.accept(visitor)

    file_info = FileInfo(path, tree._fullname)
    for lineno, _ in iterate_python_lines(path):
        status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
        file_info.counts[status] += 1

    self.files.append(file_info)

</t>
<t tx="ekr.20221004064035.1186">def on_finish(self) -&gt; None:
    if not self.files:
        # Nothing to do.
        return
    output_files = sorted(self.files, key=lambda x: x.module)
    report_file = os.path.join(self.output_dir, "lineprecision.txt")
    width = max(4, max(len(info.module) for info in output_files))
    titles = ("Lines", "Precise", "Imprecise", "Any", "Empty", "Unanalyzed")
    widths = (width,) + tuple(len(t) for t in titles)
    fmt = "{:%d}  {:%d}  {:%d}  {:%d}  {:%d}  {:%d}  {:%d}\n" % widths
    with open(report_file, "w") as f:
        f.write(fmt.format("Name", *titles))
        f.write("-" * (width + 51) + "\n")
        for file_info in output_files:
            counts = file_info.counts
            f.write(
                fmt.format(
                    file_info.module.ljust(width),
                    file_info.total(),
                    counts[stats.TYPE_PRECISE],
                    counts[stats.TYPE_IMPRECISE],
                    counts[stats.TYPE_ANY],
                    counts[stats.TYPE_EMPTY],
                    counts[stats.TYPE_UNANALYZED],
                )
            )


</t>
<t tx="ekr.20221004064035.1187">@path C:/Repos/ekr-mypy2/mypy/
"""Track current scope to easily calculate the corresponding fine-grained target.

TODO: Use everywhere where we track targets, including in mypy.errors.
"""

from __future__ import annotations

from contextlib import contextmanager, nullcontext
from typing import Iterator, Optional, Tuple
from typing_extensions import TypeAlias as _TypeAlias

from mypy.nodes import FuncBase, TypeInfo

SavedScope: _TypeAlias = Tuple[str, Optional[TypeInfo], Optional[FuncBase]]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1188">class Scope:
    """Track which target we are processing at any given time."""

    @others
</t>
<t tx="ekr.20221004064035.1189">def __init__(self) -&gt; None:
    self.module: str | None = None
    self.classes: list[TypeInfo] = []
    self.function: FuncBase | None = None
    # Number of nested scopes ignored (that don't get their own separate targets)
    self.ignored = 0

</t>
<t tx="ekr.20221004064035.119"># Make the help output a little less jarring.
class AugmentedHelpFormatter(argparse.RawDescriptionHelpFormatter):
    @others
</t>
<t tx="ekr.20221004064035.1190">def current_module_id(self) -&gt; str:
    assert self.module
    return self.module

</t>
<t tx="ekr.20221004064035.1191">def current_target(self) -&gt; str:
    """Return the current target (non-class; for a class return enclosing module)."""
    assert self.module
    if self.function:
        fullname = self.function.fullname
        return fullname or ""
    return self.module

</t>
<t tx="ekr.20221004064035.1192">def current_full_target(self) -&gt; str:
    """Return the current target (may be a class)."""
    assert self.module
    if self.function:
        return self.function.fullname
    if self.classes:
        return self.classes[-1].fullname
    return self.module

</t>
<t tx="ekr.20221004064035.1193">def current_type_name(self) -&gt; str | None:
    """Return the current type's short name if it exists"""
    return self.classes[-1].name if self.classes else None

</t>
<t tx="ekr.20221004064035.1194">def current_function_name(self) -&gt; str | None:
    """Return the current function's short name if it exists"""
    return self.function.name if self.function else None

</t>
<t tx="ekr.20221004064035.1195">@contextmanager
def module_scope(self, prefix: str) -&gt; Iterator[None]:
    self.module = prefix
    self.classes = []
    self.function = None
    self.ignored = 0
    yield
    assert self.module
    self.module = None

</t>
<t tx="ekr.20221004064035.1196">@contextmanager
def function_scope(self, fdef: FuncBase) -&gt; Iterator[None]:
    if not self.function:
        self.function = fdef
    else:
        # Nested functions are part of the topmost function target.
        self.ignored += 1
    yield
    if self.ignored:
        # Leave a scope that's included in the enclosing target.
        self.ignored -= 1
    else:
        assert self.function
        self.function = None

</t>
<t tx="ekr.20221004064035.1197">def enter_class(self, info: TypeInfo) -&gt; None:
    """Enter a class target scope."""
    if not self.function:
        self.classes.append(info)
    else:
        # Classes within functions are part of the enclosing function target.
        self.ignored += 1

</t>
<t tx="ekr.20221004064035.1198">def leave_class(self) -&gt; None:
    """Leave a class target scope."""
    if self.ignored:
        # Leave a scope that's included in the enclosing target.
        self.ignored -= 1
    else:
        assert self.classes
        # Leave the innermost class.
        self.classes.pop()

</t>
<t tx="ekr.20221004064035.1199">@contextmanager
def class_scope(self, info: TypeInfo) -&gt; Iterator[None]:
    self.enter_class(info)
    yield
    self.leave_class()

</t>
<t tx="ekr.20221004064035.12">def __init__(self, name: str, timeout: float | None = None) -&gt; None:
    if sys.platform == "win32":
        name = r"\\.\pipe\{}-{}.pipe".format(
            name, base64.urlsafe_b64encode(os.urandom(6)).decode()
        )
    else:
        name = f"{name}.sock"
    super().__init__(name, timeout)
    if sys.platform == "win32":
        self.connection = _winapi.CreateNamedPipe(
            self.name,
            _winapi.PIPE_ACCESS_DUPLEX
            | _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE
            | _winapi.FILE_FLAG_OVERLAPPED,
            _winapi.PIPE_READMODE_MESSAGE
            | _winapi.PIPE_TYPE_MESSAGE
            | _winapi.PIPE_WAIT
            | 0x8,  # PIPE_REJECT_REMOTE_CLIENTS
            1,  # one instance
            self.BUFFER_SIZE,
            self.BUFFER_SIZE,
            _winapi.NMPWAIT_WAIT_FOREVER,
            0,  # Use default security descriptor
        )
        if self.connection == -1:  # INVALID_HANDLE_VALUE
            err = _winapi.GetLastError()
            raise IPCException(f"Invalid handle to pipe: {err}")
    else:
        self.sock_directory = tempfile.mkdtemp()
        sockfile = os.path.join(self.sock_directory, self.name)
        self.sock = socket.socket(socket.AF_UNIX)
        self.sock.bind(sockfile)
        self.sock.listen(1)
        if timeout is not None:
            self.sock.settimeout(timeout)

</t>
<t tx="ekr.20221004064035.120">def __init__(self, prog: str) -&gt; None:
    super().__init__(prog=prog, max_help_position=28)

</t>
<t tx="ekr.20221004064035.1200">def save(self) -&gt; SavedScope:
    """Produce a saved scope that can be entered with saved_scope()"""
    assert self.module
    # We only save the innermost class, which is sufficient since
    # the rest are only needed for when classes are left.
    cls = self.classes[-1] if self.classes else None
    return self.module, cls, self.function

</t>
<t tx="ekr.20221004064035.1201">@contextmanager
def saved_scope(self, saved: SavedScope) -&gt; Iterator[None]:
    module, info, function = saved
    with self.module_scope(module):
        with self.class_scope(info) if info else nullcontext():
            with self.function_scope(function) if function else nullcontext():
                yield
</t>
<t tx="ekr.20221004064035.1202">@path C:/Repos/ekr-mypy2/mypy/
"""The semantic analyzer.

Bind names to definitions and do various other simple consistency
checks.  Populate symbol tables.  The semantic analyzer also detects
special forms which reuse generic syntax such as NamedTuple and
cast().  Multiple analysis iterations may be needed to analyze forward
references and import cycles. Each iteration "fills in" additional
bindings and references until everything has been bound.

For example, consider this program:

  x = 1
  y = x

Here semantic analysis would detect that the assignment 'x = 1'
defines a new variable, the type of which is to be inferred (in a
later pass; type inference or type checking is not part of semantic
analysis).  Also, it would bind both references to 'x' to the same
module-level variable (Var) node.  The second assignment would also
be analyzed, and the type of 'y' marked as being inferred.

Semantic analysis of types is implemented in typeanal.py.

See semanal_main.py for the top-level logic.

Some important properties:

* After semantic analysis is complete, no PlaceholderNode and
  PlaceholderType instances should remain. During semantic analysis,
  if we encounter one of these, the current target should be deferred.

* A TypeInfo is only created once we know certain basic information about
  a type, such as the MRO, existence of a Tuple base class (e.g., for named
  tuples), and whether we have a TypedDict. We use a temporary
  PlaceholderNode node in the symbol table if some such information is
  missing.

* For assignments, we only add a non-placeholder symbol table entry once
  we know the sort of thing being defined (variable, NamedTuple, type alias,
  etc.).

* Every part of the analysis step must support multiple iterations over
  the same AST nodes, and each iteration must be able to fill in arbitrary
  things that were missing or incomplete in previous iterations.

* Changes performed by the analysis need to be reversible, since mypy
  daemon strips and reuses existing ASTs (to improve performance and/or
  reduce memory use).
"""

from __future__ import annotations

from contextlib import contextmanager
from typing import Any, Callable, Iterable, Iterator, List, TypeVar, cast
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy import errorcodes as codes, message_registry
from mypy.errorcodes import ErrorCode
from mypy.errors import Errors, report_internal_error
from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.messages import (
    SUGGESTED_TEST_FIXTURES,
    TYPES_FOR_UNIMPORTED_HINTS,
    MessageBuilder,
    best_matches,
    pretty_seq,
)
from mypy.mro import MroError, calculate_mro
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    CONTRAVARIANT,
    COVARIANT,
    GDEF,
    IMPLICITLY_ABSTRACT,
    INVARIANT,
    IS_ABSTRACT,
    LDEF,
    MDEF,
    NOT_ABSTRACT,
    REVEAL_LOCALS,
    REVEAL_TYPE,
    RUNTIME_PROTOCOL_DECOS,
    ArgKind,
    AssertStmt,
    AssertTypeExpr,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    CastExpr,
    ClassDef,
    ComparisonExpr,
    ConditionalExpr,
    Context,
    ContinueStmt,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    ExpressionStmt,
    FakeExpression,
    FloatExpr,
    ForStmt,
    FuncBase,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportBase,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    Lvalue,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    OverloadPart,
    ParamSpecExpr,
    PassStmt,
    PlaceholderNode,
    PromoteExpr,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    Statement,
    StrExpr,
    SuperExpr,
    SymbolNode,
    SymbolTable,
    SymbolTableNode,
    TempNode,
    TryStmt,
    TupleExpr,
    TypeAlias,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeInfo,
    TypeVarExpr,
    TypeVarLikeExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
    get_member_expr_fullname,
    get_nongen_builtins,
    implicit_module_attrs,
    is_final_node,
    type_aliases,
    type_aliases_source_versions,
    typing_extensions_aliases,
)
from mypy.options import Options
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    SequencePattern,
    StarredPattern,
    ValuePattern,
)
from mypy.plugin import (
    ClassDefContext,
    DynamicClassDefContext,
    Plugin,
    SemanticAnalyzerPluginInterface,
)
from mypy.reachability import (
    ALWAYS_FALSE,
    ALWAYS_TRUE,
    MYPY_FALSE,
    MYPY_TRUE,
    infer_condition_value,
    infer_reachability_of_if_statement,
    infer_reachability_of_match_statement,
)
from mypy.scope import Scope
from mypy.semanal_enum import EnumCallAnalyzer
from mypy.semanal_namedtuple import NamedTupleAnalyzer
from mypy.semanal_newtype import NewTypeAnalyzer
from mypy.semanal_shared import (
    PRIORITY_FALLBACKS,
    SemanticAnalyzerInterface,
    calculate_tuple_fallback,
    has_placeholder,
    set_callable_name as set_callable_name,
)
from mypy.semanal_typeddict import TypedDictAnalyzer
from mypy.tvar_scope import TypeVarLikeScope
from mypy.typeanal import (
    TypeAnalyser,
    TypeVarLikeList,
    TypeVarLikeQuery,
    analyze_type_alias,
    check_for_explicit_any,
    detect_diverging_alias,
    fix_instance_types,
    has_any_from_unimported_type,
    no_subscript_builtin_alias,
    remove_dups,
    type_constructors,
)
from mypy.typeops import function_type, get_type_vars
from mypy.types import (
    ASSERT_TYPE_NAMES,
    FINAL_DECORATOR_NAMES,
    FINAL_TYPE_NAMES,
    NEVER_NAMES,
    OVERLOAD_NAMES,
    PROTOCOL_NAMES,
    REVEAL_TYPE_NAMES,
    TPDICT_NAMES,
    TYPE_ALIAS_NAMES,
    TYPED_NAMEDTUPLE_NAMES,
    AnyType,
    CallableType,
    FunctionLike,
    Instance,
    LiteralType,
    LiteralValue,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PlaceholderType,
    ProperType,
    StarType,
    TrivialSyntheticTypeTranslator,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarType,
    UnboundType,
    UnpackType,
    get_proper_type,
    get_proper_types,
    invalid_recursive_alias,
    is_named_instance,
)
from mypy.typevars import fill_typevars
from mypy.util import (
    correct_relative_import,
    is_dunder,
    is_typeshed_file,
    module_prefix,
    unmangle,
    unnamed_function,
)
from mypy.visitor import NodeVisitor

T = TypeVar("T")


FUTURE_IMPORTS: Final = {
    "__future__.nested_scopes": "nested_scopes",
    "__future__.generators": "generators",
    "__future__.division": "division",
    "__future__.absolute_import": "absolute_import",
    "__future__.with_statement": "with_statement",
    "__future__.print_function": "print_function",
    "__future__.unicode_literals": "unicode_literals",
    "__future__.barry_as_FLUFL": "barry_as_FLUFL",
    "__future__.generator_stop": "generator_stop",
    "__future__.annotations": "annotations",
}


# Special cased built-in classes that are needed for basic functionality and need to be
# available very early on.
CORE_BUILTIN_CLASSES: Final = ["object", "bool", "function"]

# Subclasses can override these Var attributes with incompatible types. This can also be
# set for individual attributes using 'allow_incompatible_override' of Var.
ALLOW_INCOMPATIBLE_OVERRIDE: Final = ("__slots__", "__deletable__", "__match_args__")


# Used for tracking incomplete references
Tag: _TypeAlias = int


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1203">class SemanticAnalyzer(
    NodeVisitor[None], SemanticAnalyzerInterface, SemanticAnalyzerPluginInterface
):
    """Semantically analyze parsed mypy files.

    The analyzer binds names and does various consistency checks for an
    AST. Note that type checking is performed as a separate pass.
    """

    __deletable__ = ["patches", "options", "cur_mod_node"]

    # Module name space
    modules: dict[str, MypyFile]
    # Global name space for current module
    globals: SymbolTable
    # Names declared using "global" (separate set for each scope)
    global_decls: list[set[str]]
    # Names declared using "nonlocal" (separate set for each scope)
    nonlocal_decls: list[set[str]]
    # Local names of function scopes; None for non-function scopes.
    locals: list[SymbolTable | None]
    # Whether each scope is a comprehension scope.
    is_comprehension_stack: list[bool]
    # Nested block depths of scopes
    block_depth: list[int]
    # TypeInfo of directly enclosing class (or None)
    type: TypeInfo | None = None
    # Stack of outer classes (the second tuple item contains tvars).
    type_stack: list[TypeInfo | None]
    # Type variables bound by the current scope, be it class or function
    tvar_scope: TypeVarLikeScope
    # Per-module options
    options: Options

    # Stack of functions being analyzed
    function_stack: list[FuncItem]

    # Set to True if semantic analysis defines a name, or replaces a
    # placeholder definition. If some iteration makes no progress,
    # there can be at most one additional final iteration (see below).
    progress = False
    deferred = False  # Set to true if another analysis pass is needed
    incomplete = False  # Set to true if current module namespace is missing things
    # Is this the final iteration of semantic analysis (where we report
    # unbound names due to cyclic definitions and should not defer)?
    _final_iteration = False
    # These names couldn't be added to the symbol table due to incomplete deps.
    # Note that missing names are per module, _not_ per namespace. This means that e.g.
    # a missing name at global scope will block adding same name at a class scope.
    # This should not affect correctness and is purely a performance issue,
    # since it can cause unnecessary deferrals. These are represented as
    # PlaceholderNodes in the symbol table. We use this to ensure that the first
    # definition takes precedence even if it's incomplete.
    #
    # Note that a star import adds a special name '*' to the set, this blocks
    # adding _any_ names in the current file.
    missing_names: list[set[str]]
    # Callbacks that will be called after semantic analysis to tweak things.
    patches: list[tuple[int, Callable[[], None]]]
    loop_depth = 0  # Depth of breakable loops
    cur_mod_id = ""  # Current module id (or None) (phase 2)
    _is_stub_file = False  # Are we analyzing a stub file?
    _is_typeshed_stub_file = False  # Are we analyzing a typeshed stub file?
    imports: set[str]  # Imported modules (during phase 2 analysis)
    # Note: some imports (and therefore dependencies) might
    # not be found in phase 1, for example due to * imports.
    errors: Errors  # Keeps track of generated errors
    plugin: Plugin  # Mypy plugin for special casing of library features
    statement: Statement | None = None  # Statement/definition being analyzed

    # Mapping from 'async def' function definitions to their return type wrapped as a
    # 'Coroutine[Any, Any, T]'. Used to keep track of whether a function definition's
    # return type has already been wrapped, by checking if the function definition's
    # type is stored in this mapping and that it still matches.
    wrapped_coro_return_types: dict[FuncDef, Type] = {}

    @others
</t>
<t tx="ekr.20221004064035.1204">def __init__(
    self,
    modules: dict[str, MypyFile],
    missing_modules: set[str],
    incomplete_namespaces: set[str],
    errors: Errors,
    plugin: Plugin,
) -&gt; None:
    """Construct semantic analyzer.

    We reuse the same semantic analyzer instance across multiple modules.

    Args:
        modules: Global modules dictionary
        missing_modules: Modules that could not be imported encountered so far
        incomplete_namespaces: Namespaces that are being populated during semantic analysis
            (can contain modules and classes within the current SCC; mutated by the caller)
        errors: Report analysis errors using this instance
    """
    self.locals = [None]
    self.is_comprehension_stack = [False]
    # Saved namespaces from previous iteration. Every top-level function/method body is
    # analyzed in several iterations until all names are resolved. We need to save
    # the local namespaces for the top level function and all nested functions between
    # these iterations. See also semanal_main.process_top_level_function().
    self.saved_locals: dict[
        FuncItem | GeneratorExpr | DictionaryComprehension, SymbolTable
    ] = {}
    self.imports = set()
    self.type = None
    self.type_stack = []
    # Are the namespaces of classes being processed complete?
    self.incomplete_type_stack: list[bool] = []
    self.tvar_scope = TypeVarLikeScope()
    self.function_stack = []
    self.block_depth = [0]
    self.loop_depth = 0
    self.errors = errors
    self.modules = modules
    self.msg = MessageBuilder(errors, modules)
    self.missing_modules = missing_modules
    self.missing_names = [set()]
    # These namespaces are still in process of being populated. If we encounter a
    # missing name in these namespaces, we need to defer the current analysis target,
    # since it's possible that the name will be there once the namespace is complete.
    self.incomplete_namespaces = incomplete_namespaces
    self.all_exports: list[str] = []
    # Map from module id to list of explicitly exported names (i.e. names in __all__).
    self.export_map: dict[str, list[str]] = {}
    self.plugin = plugin
    # If True, process function definitions. If False, don't. This is used
    # for processing module top levels in fine-grained incremental mode.
    self.recurse_into_functions = True
    self.scope = Scope()

    # Trace line numbers for every file where deferral happened during analysis of
    # current SCC or top-level function.
    self.deferral_debug_context: list[tuple[str, int]] = []

    # This is needed to properly support recursive type aliases. The problem is that
    # Foo[Bar] could mean three things depending on context: a target for type alias,
    # a normal index expression (including enum index), or a type application.
    # The latter is particularly problematic as it can falsely create incomplete
    # refs while analysing rvalues of type aliases. To avoid this we first analyse
    # rvalues while temporarily setting this to True.
    self.basic_type_applications = False

</t>
<t tx="ekr.20221004064035.1205"># mypyc doesn't properly handle implementing an abstractproperty
# with a regular attribute so we make them properties
@property
def is_stub_file(self) -&gt; bool:
    return self._is_stub_file

</t>
<t tx="ekr.20221004064035.1206">@property
def is_typeshed_stub_file(self) -&gt; bool:
    return self._is_typeshed_stub_file

</t>
<t tx="ekr.20221004064035.1207">@property
def final_iteration(self) -&gt; bool:
    return self._final_iteration

</t>
<t tx="ekr.20221004064035.1208">#
# Preparing module (performed before semantic analysis)
#

</t>
<t tx="ekr.20221004064035.1209">def prepare_file(self, file_node: MypyFile) -&gt; None:
    """Prepare a freshly parsed file for semantic analysis."""
    if "builtins" in self.modules:
        file_node.names["__builtins__"] = SymbolTableNode(GDEF, self.modules["builtins"])
    if file_node.fullname == "builtins":
        self.prepare_builtins_namespace(file_node)
    if file_node.fullname == "typing":
        self.prepare_typing_namespace(file_node, type_aliases)
    if file_node.fullname == "typing_extensions":
        self.prepare_typing_namespace(file_node, typing_extensions_aliases)

</t>
<t tx="ekr.20221004064035.121">def _fill_text(self, text: str, width: int, indent: str) -&gt; str:
    if "\n" in text:
        # Assume we want to manually format the text
        return super()._fill_text(text, width, indent)
    else:
        # Assume we want argparse to manage wrapping, indenting, and
        # formatting the text for us.
        return argparse.HelpFormatter._fill_text(self, text, width, indent)


</t>
<t tx="ekr.20221004064035.1210">def prepare_typing_namespace(self, file_node: MypyFile, aliases: dict[str, str]) -&gt; None:
    """Remove dummy alias definitions such as List = TypeAlias(object) from typing.

    They will be replaced with real aliases when corresponding targets are ready.
    """
    # This is all pretty unfortunate. typeshed now has a
    # sys.version_info check for OrderedDict, and we shouldn't
    # take it out, because it is correct and a typechecker should
    # use that as a source of truth. But instead we rummage
    # through IfStmts to remove the info first.  (I tried to
    # remove this whole machinery and ran into issues with the
    # builtins/typing import cycle.)
    def helper(defs: list[Statement]) -&gt; None:
        for stmt in defs.copy():
            if isinstance(stmt, IfStmt):
                for body in stmt.body:
                    helper(body.body)
                if stmt.else_body:
                    helper(stmt.else_body.body)
            if (
                isinstance(stmt, AssignmentStmt)
                and len(stmt.lvalues) == 1
                and isinstance(stmt.lvalues[0], NameExpr)
            ):
                # Assignment to a simple name, remove it if it is a dummy alias.
                if f"{file_node.fullname}.{stmt.lvalues[0].name}" in aliases:
                    defs.remove(stmt)

    helper(file_node.defs)

</t>
<t tx="ekr.20221004064035.1211">def prepare_builtins_namespace(self, file_node: MypyFile) -&gt; None:
    """Add certain special-cased definitions to the builtins module.

    Some definitions are too special or fundamental to be processed
    normally from the AST.
    """
    names = file_node.names

    # Add empty definition for core built-in classes, since they are required for basic
    # operation. These will be completed later on.
    for name in CORE_BUILTIN_CLASSES:
        cdef = ClassDef(name, Block([]))  # Dummy ClassDef, will be replaced later
        info = TypeInfo(SymbolTable(), cdef, "builtins")
        info._fullname = f"builtins.{name}"
        names[name] = SymbolTableNode(GDEF, info)

    bool_info = names["bool"].node
    assert isinstance(bool_info, TypeInfo)
    bool_type = Instance(bool_info, [])

    special_var_types: list[tuple[str, Type]] = [
        ("None", NoneType()),
        # reveal_type is a mypy-only function that gives an error with
        # the type of its arg.
        ("reveal_type", AnyType(TypeOfAny.special_form)),
        # reveal_locals is a mypy-only function that gives an error with the types of
        # locals
        ("reveal_locals", AnyType(TypeOfAny.special_form)),
        ("True", bool_type),
        ("False", bool_type),
        ("__debug__", bool_type),
    ]

    for name, typ in special_var_types:
        v = Var(name, typ)
        v._fullname = f"builtins.{name}"
        file_node.names[name] = SymbolTableNode(GDEF, v)

</t>
<t tx="ekr.20221004064035.1212">#
# Analyzing a target
#

</t>
<t tx="ekr.20221004064035.1213">def refresh_partial(
    self,
    node: MypyFile | FuncDef | OverloadedFuncDef,
    patches: list[tuple[int, Callable[[], None]]],
    final_iteration: bool,
    file_node: MypyFile,
    options: Options,
    active_type: TypeInfo | None = None,
) -&gt; None:
    """Refresh a stale target in fine-grained incremental mode."""
    self.patches = patches
    self.deferred = False
    self.incomplete = False
    self._final_iteration = final_iteration
    self.missing_names[-1] = set()

    with self.file_context(file_node, options, active_type):
        if isinstance(node, MypyFile):
            self.refresh_top_level(node)
        else:
            self.recurse_into_functions = True
            self.accept(node)
    del self.patches

</t>
<t tx="ekr.20221004064035.1214">def refresh_top_level(self, file_node: MypyFile) -&gt; None:
    """Reanalyze a stale module top-level in fine-grained incremental mode."""
    self.recurse_into_functions = False
    self.add_implicit_module_attrs(file_node)
    for d in file_node.defs:
        self.accept(d)
    if file_node.fullname == "typing":
        self.add_builtin_aliases(file_node)
    if file_node.fullname == "typing_extensions":
        self.add_typing_extension_aliases(file_node)
    self.adjust_public_exports()
    self.export_map[self.cur_mod_id] = self.all_exports
    self.all_exports = []

</t>
<t tx="ekr.20221004064035.1215">def add_implicit_module_attrs(self, file_node: MypyFile) -&gt; None:
    """Manually add implicit definitions of module '__name__' etc."""
    for name, t in implicit_module_attrs.items():
        if name == "__doc__":
            typ: Type = UnboundType("__builtins__.str")
        elif name == "__path__":
            if not file_node.is_package_init_file():
                continue
            # Need to construct the type ourselves, to avoid issues with __builtins__.list
            # not being subscriptable or typing.List not getting bound
            sym = self.lookup_qualified("__builtins__.list", Context())
            if not sym:
                continue
            node = sym.node
            if not isinstance(node, TypeInfo):
                self.defer(node)
                return
            typ = Instance(node, [self.str_type()])
        elif name == "__annotations__":
            sym = self.lookup_qualified("__builtins__.dict", Context(), suppress_errors=True)
            if not sym:
                continue
            node = sym.node
            if not isinstance(node, TypeInfo):
                self.defer(node)
                return
            typ = Instance(node, [self.str_type(), AnyType(TypeOfAny.special_form)])
        else:
            assert t is not None, f"type should be specified for {name}"
            typ = UnboundType(t)

        existing = file_node.names.get(name)
        if existing is not None and not isinstance(existing.node, PlaceholderNode):
            # Already exists.
            continue

        an_type = self.anal_type(typ)
        if an_type:
            var = Var(name, an_type)
            var._fullname = self.qualified_name(name)
            var.is_ready = True
            self.add_symbol(name, var, dummy_context())
        else:
            self.add_symbol(
                name,
                PlaceholderNode(self.qualified_name(name), file_node, -1),
                dummy_context(),
            )

</t>
<t tx="ekr.20221004064035.1216">def add_builtin_aliases(self, tree: MypyFile) -&gt; None:
    """Add builtin type aliases to typing module.

    For historical reasons, the aliases like `List = list` are not defined
    in typeshed stubs for typing module. Instead we need to manually add the
    corresponding nodes on the fly. We explicitly mark these aliases as normalized,
    so that a user can write `typing.List[int]`.
    """
    assert tree.fullname == "typing"
    for alias, target_name in type_aliases.items():
        if type_aliases_source_versions[alias] &gt; self.options.python_version:
            # This alias is not available on this Python version.
            continue
        name = alias.split(".")[-1]
        if name in tree.names and not isinstance(tree.names[name].node, PlaceholderNode):
            continue
        self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20221004064035.1217">def add_typing_extension_aliases(self, tree: MypyFile) -&gt; None:
    """Typing extensions module does contain some type aliases.

    We need to analyze them as such, because in typeshed
    they are just defined as `_Alias()` call.
    Which is not supported natively.
    """
    assert tree.fullname == "typing_extensions"

    for alias, target_name in typing_extensions_aliases.items():
        name = alias.split(".")[-1]
        if name in tree.names and isinstance(tree.names[name].node, TypeAlias):
            continue  # Do not reset TypeAliases on the second pass.

        # We need to remove any node that is there at the moment. It is invalid.
        tree.names.pop(name, None)

        # Now, create a new alias.
        self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20221004064035.1218">def create_alias(self, tree: MypyFile, target_name: str, alias: str, name: str) -&gt; None:
    tag = self.track_incomplete_refs()
    n = self.lookup_fully_qualified_or_none(target_name)
    if n:
        if isinstance(n.node, PlaceholderNode):
            self.mark_incomplete(name, tree)
        else:
            # Found built-in class target. Create alias.
            target = self.named_type_or_none(target_name, [])
            assert target is not None
            # Transform List to List[Any], etc.
            fix_instance_types(target, self.fail, self.note, self.options.python_version)
            alias_node = TypeAlias(
                target,
                alias,
                line=-1,
                column=-1,  # there is no context
                no_args=True,
                normalized=True,
            )
            self.add_symbol(name, alias_node, tree)
    elif self.found_incomplete_ref(tag):
        # Built-in class target may not ready yet -- defer.
        self.mark_incomplete(name, tree)
    else:
        # Test fixtures may be missing some builtin classes, which is okay.
        # Kill the placeholder if there is one.
        if name in tree.names:
            assert isinstance(tree.names[name].node, PlaceholderNode)
            del tree.names[name]

</t>
<t tx="ekr.20221004064035.1219">def adjust_public_exports(self) -&gt; None:
    """Adjust the module visibility of globals due to __all__."""
    if "__all__" in self.globals:
        for name, g in self.globals.items():
            # Being included in __all__ explicitly exports and makes public.
            if name in self.all_exports:
                g.module_public = True
                g.module_hidden = False
            # But when __all__ is defined, and a symbol is not included in it,
            # it cannot be public.
            else:
                g.module_public = False

</t>
<t tx="ekr.20221004064035.122"># Define pairs of flag prefixes with inverse meaning.
flag_prefix_pairs: Final = [("allow", "disallow"), ("show", "hide")]
flag_prefix_map: Final[dict[str, str]] = {}
for a, b in flag_prefix_pairs:
    flag_prefix_map[a] = b
    flag_prefix_map[b] = a


</t>
<t tx="ekr.20221004064035.1220">@contextmanager
def file_context(
    self, file_node: MypyFile, options: Options, active_type: TypeInfo | None = None
) -&gt; Iterator[None]:
    """Configure analyzer for analyzing targets within a file/class.

    Args:
        file_node: target file
        options: options specific to the file
        active_type: must be the surrounding class to analyze method targets
    """
    scope = self.scope
    self.options = options
    self.errors.set_file(file_node.path, file_node.fullname, scope=scope, options=options)
    self.cur_mod_node = file_node
    self.cur_mod_id = file_node.fullname
    with scope.module_scope(self.cur_mod_id):
        self._is_stub_file = file_node.path.lower().endswith(".pyi")
        self._is_typeshed_stub_file = is_typeshed_file(
            options.abs_custom_typeshed_dir, file_node.path
        )
        self.globals = file_node.names
        self.tvar_scope = TypeVarLikeScope()

        self.named_tuple_analyzer = NamedTupleAnalyzer(options, self)
        self.typed_dict_analyzer = TypedDictAnalyzer(options, self, self.msg)
        self.enum_call_analyzer = EnumCallAnalyzer(options, self)
        self.newtype_analyzer = NewTypeAnalyzer(options, self, self.msg)

        # Counter that keeps track of references to undefined things potentially caused by
        # incomplete namespaces.
        self.num_incomplete_refs = 0

        if active_type:
            self.incomplete_type_stack.append(False)
            scope.enter_class(active_type)
            self.enter_class(active_type.defn.info)
            for tvar in active_type.defn.type_vars:
                self.tvar_scope.bind_existing(tvar)

        yield

        if active_type:
            scope.leave_class()
            self.leave_class()
            self.type = None
            self.incomplete_type_stack.pop()
    del self.options

</t>
<t tx="ekr.20221004064035.1221">#
# Functions
#

</t>
<t tx="ekr.20221004064035.1222">def visit_func_def(self, defn: FuncDef) -&gt; None:
    self.statement = defn

    # Visit default values because they may contain assignment expressions.
    for arg in defn.arguments:
        if arg.initializer:
            arg.initializer.accept(self)

    defn.is_conditional = self.block_depth[-1] &gt; 0

    # Set full names even for those definitions that aren't added
    # to a symbol table. For example, for overload items.
    defn._fullname = self.qualified_name(defn.name)

    # We don't add module top-level functions to symbol tables
    # when we analyze their bodies in the second phase on analysis,
    # since they were added in the first phase. Nested functions
    # get always added, since they aren't separate targets.
    if not self.recurse_into_functions or len(self.function_stack) &gt; 0:
        if not defn.is_decorated and not defn.is_overload:
            self.add_function_to_symbol_table(defn)

    if not self.recurse_into_functions:
        return

    with self.scope.function_scope(defn):
        self.analyze_func_def(defn)

</t>
<t tx="ekr.20221004064035.1223">def analyze_func_def(self, defn: FuncDef) -&gt; None:
    self.function_stack.append(defn)

    if defn.type:
        assert isinstance(defn.type, CallableType)
        self.update_function_type_variables(defn.type, defn)
    self.function_stack.pop()

    if self.is_class_scope():
        # Method definition
        assert self.type is not None
        defn.info = self.type
        if defn.type is not None and defn.name in ("__init__", "__init_subclass__"):
            assert isinstance(defn.type, CallableType)
            if isinstance(get_proper_type(defn.type.ret_type), AnyType):
                defn.type = defn.type.copy_modified(ret_type=NoneType())
        self.prepare_method_signature(defn, self.type)

    # Analyze function signature
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        if defn.type:
            self.check_classvar_in_signature(defn.type)
            assert isinstance(defn.type, CallableType)
            # Signature must be analyzed in the surrounding scope so that
            # class-level imported names and type variables are in scope.
            analyzer = self.type_analyzer()
            tag = self.track_incomplete_refs()
            result = analyzer.visit_callable_type(defn.type, nested=False)
            # Don't store not ready types (including placeholders).
            if self.found_incomplete_ref(tag) or has_placeholder(result):
                self.defer(defn)
                return
            assert isinstance(result, ProperType)
            if isinstance(result, CallableType):
                result = self.remove_unpack_kwargs(defn, result)
            defn.type = result
            self.add_type_alias_deps(analyzer.aliases_used)
            self.check_function_signature(defn)
            if isinstance(defn, FuncDef):
                assert isinstance(defn.type, CallableType)
                defn.type = set_callable_name(defn.type, defn)

    self.analyze_arg_initializers(defn)
    self.analyze_function_body(defn)

    if self.is_class_scope():
        assert self.type is not None
        # Mark protocol methods with empty bodies as implicitly abstract.
        # This makes explicit protocol subclassing type-safe.
        if (
            self.type.is_protocol
            and not self.is_stub_file  # Bodies in stub files are always empty.
            and (not isinstance(self.scope.function, OverloadedFuncDef) or defn.is_property)
            and defn.abstract_status != IS_ABSTRACT
            and is_trivial_body(defn.body)
        ):
            defn.abstract_status = IMPLICITLY_ABSTRACT
        if (
            is_trivial_body(defn.body)
            and not self.is_stub_file
            and defn.abstract_status != NOT_ABSTRACT
        ):
            defn.is_trivial_body = True

    if (
        defn.is_coroutine
        and isinstance(defn.type, CallableType)
        and self.wrapped_coro_return_types.get(defn) != defn.type
    ):
        if defn.is_async_generator:
            # Async generator types are handled elsewhere
            pass
        else:
            # A coroutine defined as `async def foo(...) -&gt; T: ...`
            # has external return type `Coroutine[Any, Any, T]`.
            any_type = AnyType(TypeOfAny.special_form)
            ret_type = self.named_type_or_none(
                "typing.Coroutine", [any_type, any_type, defn.type.ret_type]
            )
            assert ret_type is not None, "Internal error: typing.Coroutine not found"
            defn.type = defn.type.copy_modified(ret_type=ret_type)
            self.wrapped_coro_return_types[defn] = defn.type

</t>
<t tx="ekr.20221004064035.1224">def remove_unpack_kwargs(self, defn: FuncDef, typ: CallableType) -&gt; CallableType:
    if not typ.arg_kinds or typ.arg_kinds[-1] is not ArgKind.ARG_STAR2:
        return typ
    last_type = get_proper_type(typ.arg_types[-1])
    if not isinstance(last_type, UnpackType):
        return typ
    last_type = get_proper_type(last_type.type)
    if not isinstance(last_type, TypedDictType):
        self.fail("Unpack item in ** argument must be a TypedDict", defn)
        new_arg_types = typ.arg_types[:-1] + [AnyType(TypeOfAny.from_error)]
        return typ.copy_modified(arg_types=new_arg_types)
    overlap = set(typ.arg_names) &amp; set(last_type.items)
    # It is OK for TypedDict to have a key named 'kwargs'.
    overlap.discard(typ.arg_names[-1])
    if overlap:
        overlapped = ", ".join([f'"{name}"' for name in overlap])
        self.fail(f"Overlap between argument names and ** TypedDict items: {overlapped}", defn)
        new_arg_types = typ.arg_types[:-1] + [AnyType(TypeOfAny.from_error)]
        return typ.copy_modified(arg_types=new_arg_types)
    # OK, everything looks right now, mark the callable type as using unpack.
    new_arg_types = typ.arg_types[:-1] + [last_type]
    return typ.copy_modified(arg_types=new_arg_types, unpack_kwargs=True)

</t>
<t tx="ekr.20221004064035.1225">def prepare_method_signature(self, func: FuncDef, info: TypeInfo) -&gt; None:
    """Check basic signature validity and tweak annotation of self/cls argument."""
    # Only non-static methods are special.
    functype = func.type
    if not func.is_static:
        if func.name in ["__init_subclass__", "__class_getitem__"]:
            func.is_class = True
        if not func.arguments:
            self.fail("Method must have at least one argument", func)
        elif isinstance(functype, CallableType):
            self_type = get_proper_type(functype.arg_types[0])
            if isinstance(self_type, AnyType):
                leading_type: Type = fill_typevars(info)
                if func.is_class or func.name == "__new__":
                    leading_type = self.class_type(leading_type)
                func.type = replace_implicit_first_type(functype, leading_type)

</t>
<t tx="ekr.20221004064035.1226">def set_original_def(self, previous: Node | None, new: FuncDef | Decorator) -&gt; bool:
    """If 'new' conditionally redefine 'previous', set 'previous' as original

    We reject straight redefinitions of functions, as they are usually
    a programming error. For example:

      def f(): ...
      def f(): ...  # Error: 'f' redefined
    """
    if isinstance(new, Decorator):
        new = new.func
    if (
        isinstance(previous, (FuncDef, Decorator))
        and unnamed_function(new.name)
        and unnamed_function(previous.name)
    ):
        return True
    if isinstance(previous, (FuncDef, Var, Decorator)) and new.is_conditional:
        new.original_def = previous
        return True
    else:
        return False

</t>
<t tx="ekr.20221004064035.1227">def update_function_type_variables(self, fun_type: CallableType, defn: FuncItem) -&gt; None:
    """Make any type variables in the signature of defn explicit.

    Update the signature of defn to contain type variable definitions
    if defn is generic.
    """
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        a = self.type_analyzer()
        fun_type.variables = a.bind_function_type_variables(fun_type, defn)

</t>
<t tx="ekr.20221004064035.1228">def visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    self.statement = defn
    self.add_function_to_symbol_table(defn)

    if not self.recurse_into_functions:
        return

    # NB: Since _visit_overloaded_func_def will call accept on the
    # underlying FuncDefs, the function might get entered twice.
    # This is fine, though, because only the outermost function is
    # used to compute targets.
    with self.scope.function_scope(defn):
        self.analyze_overloaded_func_def(defn)

</t>
<t tx="ekr.20221004064035.1229">def analyze_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    # OverloadedFuncDef refers to any legitimate situation where you have
    # more than one declaration for the same function in a row.  This occurs
    # with a @property with a setter or a deleter, and for a classic
    # @overload.

    defn._fullname = self.qualified_name(defn.name)
    # TODO: avoid modifying items.
    defn.items = defn.unanalyzed_items.copy()

    first_item = defn.items[0]
    first_item.is_overload = True
    first_item.accept(self)

    if isinstance(first_item, Decorator) and first_item.func.is_property:
        # This is a property.
        first_item.func.is_overload = True
        self.analyze_property_with_multi_part_definition(defn)
        typ = function_type(first_item.func, self.named_type("builtins.function"))
        assert isinstance(typ, CallableType)
        types = [typ]
    else:
        # This is an a normal overload. Find the item signatures, the
        # implementation (if outside a stub), and any missing @overload
        # decorators.
        types, impl, non_overload_indexes = self.analyze_overload_sigs_and_impl(defn)
        defn.impl = impl
        if non_overload_indexes:
            self.handle_missing_overload_decorators(
                defn, non_overload_indexes, some_overload_decorators=len(types) &gt; 0
            )
        # If we found an implementation, remove it from the overload item list,
        # as it's special.
        if impl is not None:
            assert impl is defn.items[-1]
            defn.items = defn.items[:-1]
        elif not non_overload_indexes:
            self.handle_missing_overload_implementation(defn)

    if types:
        defn.type = Overloaded(types)
        defn.type.line = defn.line

    if not defn.items:
        # It was not a real overload after all, but function redefinition. We've
        # visited the redefinition(s) already.
        if not defn.impl:
            # For really broken overloads with no items and no implementation we need to keep
            # at least one item to hold basic information like function name.
            defn.impl = defn.unanalyzed_items[-1]
        return

    # We know this is an overload def. Infer properties and perform some checks.
    self.process_final_in_overload(defn)
    self.process_static_or_class_method_in_overload(defn)
    self.process_overload_impl(defn)

</t>
<t tx="ekr.20221004064035.123">def invert_flag_name(flag: str) -&gt; str:
    split = flag[2:].split("-", 1)
    if len(split) == 2:
        prefix, rest = split
        if prefix in flag_prefix_map:
            return f"--{flag_prefix_map[prefix]}-{rest}"
        elif prefix == "no":
            return f"--{rest}"

    return f"--no-{flag[2:]}"


</t>
<t tx="ekr.20221004064035.1230">def process_overload_impl(self, defn: OverloadedFuncDef) -&gt; None:
    """Set flags for an overload implementation.

    Currently, this checks for a trivial body in protocols classes,
    where it makes the method implicitly abstract.
    """
    if defn.impl is None:
        return
    impl = defn.impl if isinstance(defn.impl, FuncDef) else defn.impl.func
    if is_trivial_body(impl.body) and self.is_class_scope() and not self.is_stub_file:
        assert self.type is not None
        if self.type.is_protocol:
            impl.abstract_status = IMPLICITLY_ABSTRACT
        if impl.abstract_status != NOT_ABSTRACT:
            impl.is_trivial_body = True

</t>
<t tx="ekr.20221004064035.1231">def analyze_overload_sigs_and_impl(
    self, defn: OverloadedFuncDef
) -&gt; tuple[list[CallableType], OverloadPart | None, list[int]]:
    """Find overload signatures, the implementation, and items with missing @overload.

    Assume that the first was already analyzed. As a side effect:
    analyzes remaining items and updates 'is_overload' flags.
    """
    types = []
    non_overload_indexes = []
    impl: OverloadPart | None = None
    for i, item in enumerate(defn.items):
        if i != 0:
            # Assume that the first item was already visited
            item.is_overload = True
            item.accept(self)
        # TODO: support decorated overloaded functions properly
        if isinstance(item, Decorator):
            callable = function_type(item.func, self.named_type("builtins.function"))
            assert isinstance(callable, CallableType)
            if not any(refers_to_fullname(dec, OVERLOAD_NAMES) for dec in item.decorators):
                if i == len(defn.items) - 1 and not self.is_stub_file:
                    # Last item outside a stub is impl
                    impl = item
                else:
                    # Oops it wasn't an overload after all. A clear error
                    # will vary based on where in the list it is, record
                    # that.
                    non_overload_indexes.append(i)
            else:
                item.func.is_overload = True
                types.append(callable)
                if item.var.is_property:
                    self.fail("An overload can not be a property", item)
        elif isinstance(item, FuncDef):
            if i == len(defn.items) - 1 and not self.is_stub_file:
                impl = item
            else:
                non_overload_indexes.append(i)
    return types, impl, non_overload_indexes

</t>
<t tx="ekr.20221004064035.1232">def handle_missing_overload_decorators(
    self,
    defn: OverloadedFuncDef,
    non_overload_indexes: list[int],
    some_overload_decorators: bool,
) -&gt; None:
    """Generate errors for overload items without @overload.

    Side effect: remote non-overload items.
    """
    if some_overload_decorators:
        # Some of them were overloads, but not all.
        for idx in non_overload_indexes:
            if self.is_stub_file:
                self.fail(
                    "An implementation for an overloaded function "
                    "is not allowed in a stub file",
                    defn.items[idx],
                )
            else:
                self.fail(
                    "The implementation for an overloaded function must come last",
                    defn.items[idx],
                )
    else:
        for idx in non_overload_indexes[1:]:
            self.name_already_defined(defn.name, defn.items[idx], defn.items[0])
        if defn.impl:
            self.name_already_defined(defn.name, defn.impl, defn.items[0])
    # Remove the non-overloads
    for idx in reversed(non_overload_indexes):
        del defn.items[idx]

</t>
<t tx="ekr.20221004064035.1233">def handle_missing_overload_implementation(self, defn: OverloadedFuncDef) -&gt; None:
    """Generate error about missing overload implementation (only if needed)."""
    if not self.is_stub_file:
        if self.type and self.type.is_protocol and not self.is_func_scope():
            # An overloaded protocol method doesn't need an implementation,
            # but if it doesn't have one, then it is considered abstract.
            for item in defn.items:
                if isinstance(item, Decorator):
                    item.func.abstract_status = IS_ABSTRACT
                else:
                    item.abstract_status = IS_ABSTRACT
        else:
            # TODO: also allow omitting an implementation for abstract methods in ABCs?
            self.fail(
                "An overloaded function outside a stub file must have an implementation",
                defn,
                code=codes.NO_OVERLOAD_IMPL,
            )

</t>
<t tx="ekr.20221004064035.1234">def process_final_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
    """Detect the @final status of an overloaded function (and perform checks)."""
    # If the implementation is marked as @final (or the first overload in
    # stubs), then the whole overloaded definition if @final.
    if any(item.is_final for item in defn.items):
        # We anyway mark it as final because it was probably the intention.
        defn.is_final = True
        # Only show the error once per overload
        bad_final = next(ov for ov in defn.items if ov.is_final)
        if not self.is_stub_file:
            self.fail("@final should be applied only to overload implementation", bad_final)
        elif any(item.is_final for item in defn.items[1:]):
            bad_final = next(ov for ov in defn.items[1:] if ov.is_final)
            self.fail(
                "In a stub file @final must be applied only to the first overload", bad_final
            )
    if defn.impl is not None and defn.impl.is_final:
        defn.is_final = True

</t>
<t tx="ekr.20221004064035.1235">def process_static_or_class_method_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
    class_status = []
    static_status = []
    for item in defn.items:
        if isinstance(item, Decorator):
            inner = item.func
        elif isinstance(item, FuncDef):
            inner = item
        else:
            assert False, f"The 'item' variable is an unexpected type: {type(item)}"
        class_status.append(inner.is_class)
        static_status.append(inner.is_static)

    if defn.impl is not None:
        if isinstance(defn.impl, Decorator):
            inner = defn.impl.func
        elif isinstance(defn.impl, FuncDef):
            inner = defn.impl
        else:
            assert False, f"Unexpected impl type: {type(defn.impl)}"
        class_status.append(inner.is_class)
        static_status.append(inner.is_static)

    if len(set(class_status)) != 1:
        self.msg.overload_inconsistently_applies_decorator("classmethod", defn)
    elif len(set(static_status)) != 1:
        self.msg.overload_inconsistently_applies_decorator("staticmethod", defn)
    else:
        defn.is_class = class_status[0]
        defn.is_static = static_status[0]

</t>
<t tx="ekr.20221004064035.1236">def analyze_property_with_multi_part_definition(self, defn: OverloadedFuncDef) -&gt; None:
    """Analyze a property defined using multiple methods (e.g., using @x.setter).

    Assume that the first method (@property) has already been analyzed.
    """
    defn.is_property = True
    items = defn.items
    first_item = cast(Decorator, defn.items[0])
    deleted_items = []
    for i, item in enumerate(items[1:]):
        if isinstance(item, Decorator):
            if len(item.decorators) &gt;= 1:
                node = item.decorators[0]
                if isinstance(node, MemberExpr):
                    if node.name == "setter":
                        # The first item represents the entire property.
                        first_item.var.is_settable_property = True
                        # Get abstractness from the original definition.
                        item.func.abstract_status = first_item.func.abstract_status
                else:
                    self.fail(
                        f"Only supported top decorator is @{first_item.func.name}.setter", item
                    )
            item.func.accept(self)
        else:
            self.fail(f'Unexpected definition for property "{first_item.func.name}"', item)
            deleted_items.append(i + 1)
    for i in reversed(deleted_items):
        del items[i]

</t>
<t tx="ekr.20221004064035.1237">def add_function_to_symbol_table(self, func: FuncDef | OverloadedFuncDef) -&gt; None:
    if self.is_class_scope():
        assert self.type is not None
        func.info = self.type
    func._fullname = self.qualified_name(func.name)
    self.add_symbol(func.name, func, func)

</t>
<t tx="ekr.20221004064035.1238">def analyze_arg_initializers(self, defn: FuncItem) -&gt; None:
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        # Analyze default arguments
        for arg in defn.arguments:
            if arg.initializer:
                arg.initializer.accept(self)

</t>
<t tx="ekr.20221004064035.1239">def analyze_function_body(self, defn: FuncItem) -&gt; None:
    is_method = self.is_class_scope()
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        # Bind the type variables again to visit the body.
        if defn.type:
            a = self.type_analyzer()
            a.bind_function_type_variables(cast(CallableType, defn.type), defn)
        self.function_stack.append(defn)
        with self.enter(defn):
            for arg in defn.arguments:
                self.add_local(arg.variable, defn)

            # The first argument of a non-static, non-class method is like 'self'
            # (though the name could be different), having the enclosing class's
            # instance type.
            if is_method and not defn.is_static and not defn.is_class and defn.arguments:
                defn.arguments[0].variable.is_self = True

            defn.body.accept(self)
        self.function_stack.pop()

</t>
<t tx="ekr.20221004064035.124">class PythonExecutableInferenceError(Exception):
    """Represents a failure to infer the version or executable while searching."""


</t>
<t tx="ekr.20221004064035.1240">def check_classvar_in_signature(self, typ: ProperType) -&gt; None:
    t: ProperType
    if isinstance(typ, Overloaded):
        for t in typ.items:
            self.check_classvar_in_signature(t)
        return
    if not isinstance(typ, CallableType):
        return
    for t in get_proper_types(typ.arg_types) + [get_proper_type(typ.ret_type)]:
        if self.is_classvar(t):
            self.fail_invalid_classvar(t)
            # Show only one error per signature
            break

</t>
<t tx="ekr.20221004064035.1241">def check_function_signature(self, fdef: FuncItem) -&gt; None:
    sig = fdef.type
    assert isinstance(sig, CallableType)
    if len(sig.arg_types) &lt; len(fdef.arguments):
        self.fail("Type signature has too few arguments", fdef)
        # Add dummy Any arguments to prevent crashes later.
        num_extra_anys = len(fdef.arguments) - len(sig.arg_types)
        extra_anys = [AnyType(TypeOfAny.from_error)] * num_extra_anys
        sig.arg_types.extend(extra_anys)
    elif len(sig.arg_types) &gt; len(fdef.arguments):
        self.fail("Type signature has too many arguments", fdef, blocker=True)

</t>
<t tx="ekr.20221004064035.1242">def visit_decorator(self, dec: Decorator) -&gt; None:
    self.statement = dec
    # TODO: better don't modify them at all.
    dec.decorators = dec.original_decorators.copy()
    dec.func.is_conditional = self.block_depth[-1] &gt; 0
    if not dec.is_overload:
        self.add_symbol(dec.name, dec, dec)
    dec.func._fullname = self.qualified_name(dec.name)
    dec.var._fullname = self.qualified_name(dec.name)
    for d in dec.decorators:
        d.accept(self)
    removed: list[int] = []
    no_type_check = False
    could_be_decorated_property = False
    for i, d in enumerate(dec.decorators):
        # A bunch of decorators are special cased here.
        if refers_to_fullname(d, "abc.abstractmethod"):
            removed.append(i)
            dec.func.abstract_status = IS_ABSTRACT
            self.check_decorated_function_is_method("abstractmethod", dec)
        elif refers_to_fullname(d, ("asyncio.coroutines.coroutine", "types.coroutine")):
            removed.append(i)
            dec.func.is_awaitable_coroutine = True
        elif refers_to_fullname(d, "builtins.staticmethod"):
            removed.append(i)
            dec.func.is_static = True
            dec.var.is_staticmethod = True
            self.check_decorated_function_is_method("staticmethod", dec)
        elif refers_to_fullname(d, "builtins.classmethod"):
            removed.append(i)
            dec.func.is_class = True
            dec.var.is_classmethod = True
            self.check_decorated_function_is_method("classmethod", dec)
        elif refers_to_fullname(
            d, ("builtins.property", "abc.abstractproperty", "functools.cached_property")
        ):
            removed.append(i)
            dec.func.is_property = True
            dec.var.is_property = True
            if refers_to_fullname(d, "abc.abstractproperty"):
                dec.func.abstract_status = IS_ABSTRACT
            elif refers_to_fullname(d, "functools.cached_property"):
                dec.var.is_settable_property = True
            self.check_decorated_function_is_method("property", dec)
        elif refers_to_fullname(d, "typing.no_type_check"):
            dec.var.type = AnyType(TypeOfAny.special_form)
            no_type_check = True
        elif refers_to_fullname(d, FINAL_DECORATOR_NAMES):
            if self.is_class_scope():
                assert self.type is not None, "No type set at class scope"
                if self.type.is_protocol:
                    self.msg.protocol_members_cant_be_final(d)
                else:
                    dec.func.is_final = True
                    dec.var.is_final = True
                removed.append(i)
            else:
                self.fail("@final cannot be used with non-method functions", d)
        elif not dec.var.is_property:
            # We have seen a "non-trivial" decorator before seeing @property, if
            # we will see a @property later, give an error, as we don't support this.
            could_be_decorated_property = True
    for i in reversed(removed):
        del dec.decorators[i]
    if (not dec.is_overload or dec.var.is_property) and self.type:
        dec.var.info = self.type
        dec.var.is_initialized_in_class = True
    if not no_type_check and self.recurse_into_functions:
        dec.func.accept(self)
    if could_be_decorated_property and dec.decorators and dec.var.is_property:
        self.fail("Decorators on top of @property are not supported", dec)
    if (dec.func.is_static or dec.func.is_class) and dec.var.is_property:
        self.fail("Only instance methods can be decorated with @property", dec)
    if dec.func.abstract_status == IS_ABSTRACT and dec.func.is_final:
        self.fail(f"Method {dec.func.name} is both abstract and final", dec)

</t>
<t tx="ekr.20221004064035.1243">def check_decorated_function_is_method(self, decorator: str, context: Context) -&gt; None:
    if not self.type or self.is_func_scope():
        self.fail(f'"{decorator}" used with a non-method', context)

</t>
<t tx="ekr.20221004064035.1244">#
# Classes
#

</t>
<t tx="ekr.20221004064035.1245">def visit_class_def(self, defn: ClassDef) -&gt; None:
    self.statement = defn
    self.incomplete_type_stack.append(not defn.info)
    namespace = self.qualified_name(defn.name)
    with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
        self.analyze_class(defn)
    self.incomplete_type_stack.pop()

</t>
<t tx="ekr.20221004064035.1246">def analyze_class(self, defn: ClassDef) -&gt; None:
    fullname = self.qualified_name(defn.name)
    if not defn.info and not self.is_core_builtin_class(defn):
        # Add placeholder so that self-references in base classes can be
        # resolved.  We don't want this to cause a deferral, since if there
        # are no incomplete references, we'll replace this with a TypeInfo
        # before returning.
        placeholder = PlaceholderNode(fullname, defn, defn.line, becomes_typeinfo=True)
        self.add_symbol(defn.name, placeholder, defn, can_defer=False)

    tag = self.track_incomplete_refs()

    # Restore base classes after previous iteration (things like Generic[T] might be removed).
    defn.base_type_exprs.extend(defn.removed_base_type_exprs)
    defn.removed_base_type_exprs.clear()

    self.infer_metaclass_and_bases_from_compat_helpers(defn)

    bases = defn.base_type_exprs
    bases, tvar_defs, is_protocol = self.clean_up_bases_and_infer_type_variables(
        defn, bases, context=defn
    )

    for tvd in tvar_defs:
        if isinstance(tvd, TypeVarType) and any(
            has_placeholder(t) for t in [tvd.upper_bound] + tvd.values
        ):
            # Some type variable bounds or values are not ready, we need
            # to re-analyze this class.
            self.defer()

    self.analyze_class_keywords(defn)
    bases_result = self.analyze_base_classes(bases)
    if bases_result is None or self.found_incomplete_ref(tag):
        # Something was incomplete. Defer current target.
        self.mark_incomplete(defn.name, defn)
        return

    base_types, base_error = bases_result
    if any(isinstance(base, PlaceholderType) for base, _ in base_types):
        # We need to know the TypeInfo of each base to construct the MRO. Placeholder types
        # are okay in nested positions, since they can't affect the MRO.
        self.mark_incomplete(defn.name, defn)
        return

    declared_metaclass, should_defer = self.get_declared_metaclass(defn.name, defn.metaclass)
    if should_defer or self.found_incomplete_ref(tag):
        # Metaclass was not ready. Defer current target.
        self.mark_incomplete(defn.name, defn)
        return

    if self.analyze_typeddict_classdef(defn):
        if defn.info:
            self.setup_type_vars(defn, tvar_defs)
            self.setup_alias_type_vars(defn)
        return

    if self.analyze_namedtuple_classdef(defn, tvar_defs):
        return

    # Create TypeInfo for class now that base classes and the MRO can be calculated.
    self.prepare_class_def(defn)
    self.setup_type_vars(defn, tvar_defs)
    if base_error:
        defn.info.fallback_to_any = True

    with self.scope.class_scope(defn.info):
        self.configure_base_classes(defn, base_types)
        defn.info.is_protocol = is_protocol
        self.recalculate_metaclass(defn, declared_metaclass)
        defn.info.runtime_protocol = False
        for decorator in defn.decorators:
            self.analyze_class_decorator(defn, decorator)
        self.analyze_class_body_common(defn)

</t>
<t tx="ekr.20221004064035.1247">def setup_type_vars(self, defn: ClassDef, tvar_defs: list[TypeVarLikeType]) -&gt; None:
    defn.type_vars = tvar_defs
    defn.info.type_vars = []
    # we want to make sure any additional logic in add_type_vars gets run
    defn.info.add_type_vars()

</t>
<t tx="ekr.20221004064035.1248">def setup_alias_type_vars(self, defn: ClassDef) -&gt; None:
    assert defn.info.special_alias is not None
    defn.info.special_alias.alias_tvars = list(defn.info.type_vars)
    target = defn.info.special_alias.target
    assert isinstance(target, ProperType)
    if isinstance(target, TypedDictType):
        target.fallback.args = tuple(defn.type_vars)
    elif isinstance(target, TupleType):
        target.partial_fallback.args = tuple(defn.type_vars)
    else:
        assert False, f"Unexpected special alias type: {type(target)}"

</t>
<t tx="ekr.20221004064035.1249">def is_core_builtin_class(self, defn: ClassDef) -&gt; bool:
    return self.cur_mod_id == "builtins" and defn.name in CORE_BUILTIN_CLASSES

</t>
<t tx="ekr.20221004064035.125">def python_executable_prefix(v: str) -&gt; list[str]:
    if sys.platform == "win32":
        # on Windows, all Python executables are named `python`. To handle this, there
        # is the `py` launcher, which can be passed a version e.g. `py -3.8`, and it will
        # execute an installed Python 3.8 interpreter. See also:
        # https://docs.python.org/3/using/windows.html#python-launcher-for-windows
        return ["py", f"-{v}"]
    else:
        return [f"python{v}"]


</t>
<t tx="ekr.20221004064035.1250">def analyze_class_body_common(self, defn: ClassDef) -&gt; None:
    """Parts of class body analysis that are common to all kinds of class defs."""
    self.enter_class(defn.info)
    defn.defs.accept(self)
    self.apply_class_plugin_hooks(defn)
    self.leave_class()

</t>
<t tx="ekr.20221004064035.1251">def analyze_typeddict_classdef(self, defn: ClassDef) -&gt; bool:
    if (
        defn.info
        and defn.info.typeddict_type
        and not has_placeholder(defn.info.typeddict_type)
    ):
        # This is a valid TypedDict, and it is fully analyzed.
        return True
    is_typeddict, info = self.typed_dict_analyzer.analyze_typeddict_classdef(defn)
    if is_typeddict:
        for decorator in defn.decorators:
            decorator.accept(self)
            if isinstance(decorator, RefExpr):
                if decorator.fullname in FINAL_DECORATOR_NAMES and info is not None:
                    info.is_final = True
        if info is None:
            self.mark_incomplete(defn.name, defn)
        else:
            self.prepare_class_def(defn, info)
        return True
    return False

</t>
<t tx="ekr.20221004064035.1252">def analyze_namedtuple_classdef(
    self, defn: ClassDef, tvar_defs: list[TypeVarLikeType]
) -&gt; bool:
    """Check if this class can define a named tuple."""
    if (
        defn.info
        and defn.info.is_named_tuple
        and defn.info.tuple_type
        and not has_placeholder(defn.info.tuple_type)
    ):
        # Don't reprocess everything. We just need to process methods defined
        # in the named tuple class body.
        is_named_tuple = True
        info: TypeInfo | None = defn.info
    else:
        is_named_tuple, info = self.named_tuple_analyzer.analyze_namedtuple_classdef(
            defn, self.is_stub_file, self.is_func_scope()
        )
    if is_named_tuple:
        if info is None:
            self.mark_incomplete(defn.name, defn)
        else:
            self.prepare_class_def(defn, info, custom_names=True)
            self.setup_type_vars(defn, tvar_defs)
            self.setup_alias_type_vars(defn)
            with self.scope.class_scope(defn.info):
                with self.named_tuple_analyzer.save_namedtuple_body(info):
                    self.analyze_class_body_common(defn)
        return True
    return False

</t>
<t tx="ekr.20221004064035.1253">def apply_class_plugin_hooks(self, defn: ClassDef) -&gt; None:
    """Apply a plugin hook that may infer a more precise definition for a class."""

    for decorator in defn.decorators:
        decorator_name = self.get_fullname_for_hook(decorator)
        if decorator_name:
            hook = self.plugin.get_class_decorator_hook(decorator_name)
            if hook:
                hook(ClassDefContext(defn, decorator, self))

    if defn.metaclass:
        metaclass_name = self.get_fullname_for_hook(defn.metaclass)
        if metaclass_name:
            hook = self.plugin.get_metaclass_hook(metaclass_name)
            if hook:
                hook(ClassDefContext(defn, defn.metaclass, self))

    for base_expr in defn.base_type_exprs:
        base_name = self.get_fullname_for_hook(base_expr)
        if base_name:
            hook = self.plugin.get_base_class_hook(base_name)
            if hook:
                hook(ClassDefContext(defn, base_expr, self))

</t>
<t tx="ekr.20221004064035.1254">def get_fullname_for_hook(self, expr: Expression) -&gt; str | None:
    if isinstance(expr, CallExpr):
        return self.get_fullname_for_hook(expr.callee)
    elif isinstance(expr, IndexExpr):
        return self.get_fullname_for_hook(expr.base)
    elif isinstance(expr, RefExpr):
        if expr.fullname:
            return expr.fullname
        # If we don't have a fullname look it up. This happens because base classes are
        # analyzed in a different manner (see exprtotype.py) and therefore those AST
        # nodes will not have full names.
        sym = self.lookup_type_node(expr)
        if sym:
            return sym.fullname
    return None

</t>
<t tx="ekr.20221004064035.1255">def analyze_class_keywords(self, defn: ClassDef) -&gt; None:
    for value in defn.keywords.values():
        value.accept(self)

</t>
<t tx="ekr.20221004064035.1256">def enter_class(self, info: TypeInfo) -&gt; None:
    # Remember previous active class
    self.type_stack.append(self.type)
    self.locals.append(None)  # Add class scope
    self.is_comprehension_stack.append(False)
    self.block_depth.append(-1)  # The class body increments this to 0
    self.type = info
    self.missing_names.append(set())

</t>
<t tx="ekr.20221004064035.1257">def leave_class(self) -&gt; None:
    """Restore analyzer state."""
    self.block_depth.pop()
    self.locals.pop()
    self.is_comprehension_stack.pop()
    self.type = self.type_stack.pop()
    self.missing_names.pop()

</t>
<t tx="ekr.20221004064035.1258">def analyze_class_decorator(self, defn: ClassDef, decorator: Expression) -&gt; None:
    decorator.accept(self)
    if isinstance(decorator, RefExpr):
        if decorator.fullname in RUNTIME_PROTOCOL_DECOS:
            if defn.info.is_protocol:
                defn.info.runtime_protocol = True
            else:
                self.fail("@runtime_checkable can only be used with protocol classes", defn)
        elif decorator.fullname in FINAL_DECORATOR_NAMES:
            defn.info.is_final = True

</t>
<t tx="ekr.20221004064035.1259">def clean_up_bases_and_infer_type_variables(
    self, defn: ClassDef, base_type_exprs: list[Expression], context: Context
) -&gt; tuple[list[Expression], list[TypeVarLikeType], bool]:
    """Remove extra base classes such as Generic and infer type vars.

    For example, consider this class:

      class Foo(Bar, Generic[T]): ...

    Now we will remove Generic[T] from bases of Foo and infer that the
    type variable 'T' is a type argument of Foo.

    Note that this is performed *before* semantic analysis.

    Returns (remaining base expressions, inferred type variables, is protocol).
    """
    removed: list[int] = []
    declared_tvars: TypeVarLikeList = []
    is_protocol = False
    for i, base_expr in enumerate(base_type_exprs):
        self.analyze_type_expr(base_expr)

        try:
            base = self.expr_to_unanalyzed_type(base_expr)
        except TypeTranslationError:
            # This error will be caught later.
            continue
        result = self.analyze_class_typevar_declaration(base)
        if result is not None:
            if declared_tvars:
                self.fail("Only single Generic[...] or Protocol[...] can be in bases", context)
            removed.append(i)
            tvars = result[0]
            is_protocol |= result[1]
            declared_tvars.extend(tvars)
        if isinstance(base, UnboundType):
            sym = self.lookup_qualified(base.name, base)
            if sym is not None and sym.node is not None:
                if sym.node.fullname in PROTOCOL_NAMES and i not in removed:
                    # also remove bare 'Protocol' bases
                    removed.append(i)
                    is_protocol = True

    all_tvars = self.get_all_bases_tvars(base_type_exprs, removed)
    if declared_tvars:
        if len(remove_dups(declared_tvars)) &lt; len(declared_tvars):
            self.fail("Duplicate type variables in Generic[...] or Protocol[...]", context)
        declared_tvars = remove_dups(declared_tvars)
        if not set(all_tvars).issubset(set(declared_tvars)):
            self.fail(
                "If Generic[...] or Protocol[...] is present"
                " it should list all type variables",
                context,
            )
            # In case of error, Generic tvars will go first
            declared_tvars = remove_dups(declared_tvars + all_tvars)
    else:
        declared_tvars = all_tvars
    for i in reversed(removed):
        # We need to actually remove the base class expressions like Generic[T],
        # mostly because otherwise they will create spurious dependencies in fine
        # grained incremental mode.
        defn.removed_base_type_exprs.append(defn.base_type_exprs[i])
        del base_type_exprs[i]
    tvar_defs: list[TypeVarLikeType] = []
    for name, tvar_expr in declared_tvars:
        tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
        tvar_defs.append(tvar_def)
    return base_type_exprs, tvar_defs, is_protocol

</t>
<t tx="ekr.20221004064035.126">def _python_executable_from_version(python_version: tuple[int, int]) -&gt; str:
    if sys.version_info[:2] == python_version:
        return sys.executable
    str_ver = ".".join(map(str, python_version))
    try:
        sys_exe = (
            subprocess.check_output(
                python_executable_prefix(str_ver) + ["-c", "import sys; print(sys.executable)"],
                stderr=subprocess.STDOUT,
            )
            .decode()
            .strip()
        )
        return sys_exe
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        raise PythonExecutableInferenceError(
            "failed to find a Python executable matching version {},"
            " perhaps try --python-executable, or --no-site-packages?".format(python_version)
        ) from e


</t>
<t tx="ekr.20221004064035.1260">def analyze_class_typevar_declaration(self, base: Type) -&gt; tuple[TypeVarLikeList, bool] | None:
    """Analyze type variables declared using Generic[...] or Protocol[...].

    Args:
        base: Non-analyzed base class

    Return None if the base class does not declare type variables. Otherwise,
    return the type variables.
    """
    if not isinstance(base, UnboundType):
        return None
    unbound = base
    sym = self.lookup_qualified(unbound.name, unbound)
    if sym is None or sym.node is None:
        return None
    if (
        sym.node.fullname == "typing.Generic"
        or sym.node.fullname in PROTOCOL_NAMES
        and base.args
    ):
        is_proto = sym.node.fullname != "typing.Generic"
        tvars: TypeVarLikeList = []
        have_type_var_tuple = False
        for arg in unbound.args:
            tag = self.track_incomplete_refs()
            tvar = self.analyze_unbound_tvar(arg)
            if tvar:
                if isinstance(tvar[1], TypeVarTupleExpr):
                    if have_type_var_tuple:
                        self.fail("Can only use one type var tuple in a class def", base)
                        continue
                    have_type_var_tuple = True
                tvars.append(tvar)
            elif not self.found_incomplete_ref(tag):
                self.fail("Free type variable expected in %s[...]" % sym.node.name, base)
        return tvars, is_proto
    return None

</t>
<t tx="ekr.20221004064035.1261">def analyze_unbound_tvar(self, t: Type) -&gt; tuple[str, TypeVarLikeExpr] | None:
    if not isinstance(t, UnboundType):
        return None
    unbound = t
    sym = self.lookup_qualified(unbound.name, unbound)
    if sym and isinstance(sym.node, PlaceholderNode):
        self.record_incomplete_ref()
    if sym and isinstance(sym.node, ParamSpecExpr):
        if sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
            # It's bound by our type variable scope
            return None
        return unbound.name, sym.node
    if sym and sym.fullname == "typing_extensions.Unpack":
        inner_t = unbound.args[0]
        if not isinstance(inner_t, UnboundType):
            return None
        inner_unbound = inner_t
        inner_sym = self.lookup_qualified(inner_unbound.name, inner_unbound)
        if inner_sym and isinstance(inner_sym.node, PlaceholderNode):
            self.record_incomplete_ref()
        if inner_sym and isinstance(inner_sym.node, TypeVarTupleExpr):
            if inner_sym.fullname and not self.tvar_scope.allow_binding(inner_sym.fullname):
                # It's bound by our type variable scope
                return None
            return inner_unbound.name, inner_sym.node
    if sym is None or not isinstance(sym.node, TypeVarExpr):
        return None
    elif sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
        # It's bound by our type variable scope
        return None
    else:
        assert isinstance(sym.node, TypeVarExpr)
        return unbound.name, sym.node

</t>
<t tx="ekr.20221004064035.1262">def get_all_bases_tvars(
    self, base_type_exprs: list[Expression], removed: list[int]
) -&gt; TypeVarLikeList:
    """Return all type variable references in bases."""
    tvars: TypeVarLikeList = []
    for i, base_expr in enumerate(base_type_exprs):
        if i not in removed:
            try:
                base = self.expr_to_unanalyzed_type(base_expr)
            except TypeTranslationError:
                # This error will be caught later.
                continue
            base_tvars = base.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
            tvars.extend(base_tvars)
    return remove_dups(tvars)

</t>
<t tx="ekr.20221004064035.1263">def get_and_bind_all_tvars(self, type_exprs: list[Expression]) -&gt; list[TypeVarLikeType]:
    """Return all type variable references in item type expressions.

    This is a helper for generic TypedDicts and NamedTuples. Essentially it is
    a simplified version of the logic we use for ClassDef bases. We duplicate
    some amount of code, because it is hard to refactor common pieces.
    """
    tvars = []
    for base_expr in type_exprs:
        try:
            base = self.expr_to_unanalyzed_type(base_expr)
        except TypeTranslationError:
            # This error will be caught later.
            continue
        base_tvars = base.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
        tvars.extend(base_tvars)
    tvars = remove_dups(tvars)  # Variables are defined in order of textual appearance.
    tvar_defs = []
    for name, tvar_expr in tvars:
        tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
        tvar_defs.append(tvar_def)
    return tvar_defs

</t>
<t tx="ekr.20221004064035.1264">def prepare_class_def(
    self, defn: ClassDef, info: TypeInfo | None = None, custom_names: bool = False
) -&gt; None:
    """Prepare for the analysis of a class definition.

    Create an empty TypeInfo and store it in a symbol table, or if the 'info'
    argument is provided, store it instead (used for magic type definitions).
    """
    if not defn.info:
        defn.fullname = self.qualified_name(defn.name)
        # TODO: Nested classes
        info = info or self.make_empty_type_info(defn)
        defn.info = info
        info.defn = defn
        if not custom_names:
            # Some special classes (in particular NamedTuples) use custom fullname logic.
            # Don't override it here (also see comment below, this needs cleanup).
            if not self.is_func_scope():
                info._fullname = self.qualified_name(defn.name)
            else:
                info._fullname = info.name
    local_name = defn.name
    if "@" in local_name:
        local_name = local_name.split("@")[0]
    self.add_symbol(local_name, defn.info, defn)
    if self.is_nested_within_func_scope():
        # We need to preserve local classes, let's store them
        # in globals under mangled unique names
        #
        # TODO: Putting local classes into globals breaks assumptions in fine-grained
        #       incremental mode and we should avoid it. In general, this logic is too
        #       ad-hoc and needs to be removed/refactored.
        if "@" not in defn.info._fullname:
            global_name = defn.info.name + "@" + str(defn.line)
            defn.info._fullname = self.cur_mod_id + "." + global_name
        else:
            # Preserve name from previous fine-grained incremental run.
            global_name = defn.info.name
        defn.fullname = defn.info._fullname
        if defn.info.is_named_tuple:
            # Named tuple nested within a class is stored in the class symbol table.
            self.add_symbol_skip_local(global_name, defn.info)
        else:
            self.globals[global_name] = SymbolTableNode(GDEF, defn.info)

</t>
<t tx="ekr.20221004064035.1265">def make_empty_type_info(self, defn: ClassDef) -&gt; TypeInfo:
    if (
        self.is_module_scope()
        and self.cur_mod_id == "builtins"
        and defn.name in CORE_BUILTIN_CLASSES
    ):
        # Special case core built-in classes. A TypeInfo was already
        # created for it before semantic analysis, but with a dummy
        # ClassDef. Patch the real ClassDef object.
        info = self.globals[defn.name].node
        assert isinstance(info, TypeInfo)
    else:
        info = TypeInfo(SymbolTable(), defn, self.cur_mod_id)
        info.set_line(defn)
    return info

</t>
<t tx="ekr.20221004064035.1266">def get_name_repr_of_expr(self, expr: Expression) -&gt; str | None:
    """Try finding a short simplified textual representation of a base class expression."""
    if isinstance(expr, NameExpr):
        return expr.name
    if isinstance(expr, MemberExpr):
        return get_member_expr_fullname(expr)
    if isinstance(expr, IndexExpr):
        return self.get_name_repr_of_expr(expr.base)
    if isinstance(expr, CallExpr):
        return self.get_name_repr_of_expr(expr.callee)
    return None

</t>
<t tx="ekr.20221004064035.1267">def analyze_base_classes(
    self, base_type_exprs: list[Expression]
) -&gt; tuple[list[tuple[ProperType, Expression]], bool] | None:
    """Analyze base class types.

    Return None if some definition was incomplete. Otherwise, return a tuple
    with these items:

     * List of (analyzed type, original expression) tuples
     * Boolean indicating whether one of the bases had a semantic analysis error
    """
    is_error = False
    bases = []
    for base_expr in base_type_exprs:
        if (
            isinstance(base_expr, RefExpr)
            and base_expr.fullname in TYPED_NAMEDTUPLE_NAMES + TPDICT_NAMES
        ):
            # Ignore magic bases for now.
            continue

        try:
            base = self.expr_to_analyzed_type(base_expr, allow_placeholder=True)
        except TypeTranslationError:
            name = self.get_name_repr_of_expr(base_expr)
            if isinstance(base_expr, CallExpr):
                msg = "Unsupported dynamic base class"
            else:
                msg = "Invalid base class"
            if name:
                msg += f' "{name}"'
            self.fail(msg, base_expr)
            is_error = True
            continue
        if base is None:
            return None
        base = get_proper_type(base)
        bases.append((base, base_expr))
    return bases, is_error

</t>
<t tx="ekr.20221004064035.1268">def configure_base_classes(
    self, defn: ClassDef, bases: list[tuple[ProperType, Expression]]
) -&gt; None:
    """Set up base classes.

    This computes several attributes on the corresponding TypeInfo defn.info
    related to the base classes: defn.info.bases, defn.info.mro, and
    miscellaneous others (at least tuple_type, fallback_to_any, and is_enum.)
    """
    base_types: list[Instance] = []
    info = defn.info

    for base, base_expr in bases:
        if isinstance(base, TupleType):
            actual_base = self.configure_tuple_base_class(defn, base)
            base_types.append(actual_base)
        elif isinstance(base, Instance):
            if base.type.is_newtype:
                self.fail('Cannot subclass "NewType"', defn)
            base_types.append(base)
        elif isinstance(base, AnyType):
            if self.options.disallow_subclassing_any:
                if isinstance(base_expr, (NameExpr, MemberExpr)):
                    msg = f'Class cannot subclass "{base_expr.name}" (has type "Any")'
                else:
                    msg = 'Class cannot subclass value of type "Any"'
                self.fail(msg, base_expr)
            info.fallback_to_any = True
        elif isinstance(base, TypedDictType):
            base_types.append(base.fallback)
        else:
            msg = "Invalid base class"
            name = self.get_name_repr_of_expr(base_expr)
            if name:
                msg += f' "{name}"'
            self.fail(msg, base_expr)
            info.fallback_to_any = True
        if self.options.disallow_any_unimported and has_any_from_unimported_type(base):
            if isinstance(base_expr, (NameExpr, MemberExpr)):
                prefix = f"Base type {base_expr.name}"
            else:
                prefix = "Base type"
            self.msg.unimported_type_becomes_any(prefix, base, base_expr)
        check_for_explicit_any(
            base, self.options, self.is_typeshed_stub_file, self.msg, context=base_expr
        )

    # Add 'object' as implicit base if there is no other base class.
    if not base_types and defn.fullname != "builtins.object":
        base_types.append(self.object_type())

    info.bases = base_types

    # Calculate the MRO.
    if not self.verify_base_classes(defn):
        self.set_dummy_mro(defn.info)
        return
    self.calculate_class_mro(defn, self.object_type)

</t>
<t tx="ekr.20221004064035.1269">def configure_tuple_base_class(self, defn: ClassDef, base: TupleType) -&gt; Instance:
    info = defn.info

    # There may be an existing valid tuple type from previous semanal iterations.
    # Use equality to check if it is the case.
    if info.tuple_type and info.tuple_type != base and not has_placeholder(info.tuple_type):
        self.fail("Class has two incompatible bases derived from tuple", defn)
        defn.has_incompatible_baseclass = True
    if info.special_alias and has_placeholder(info.special_alias.target):
        self.defer(force_progress=True)
    info.update_tuple_type(base)
    self.setup_alias_type_vars(defn)

    if base.partial_fallback.type.fullname == "builtins.tuple" and not has_placeholder(base):
        # Fallback can only be safely calculated after semantic analysis, since base
        # classes may be incomplete. Postpone the calculation.
        self.schedule_patch(PRIORITY_FALLBACKS, lambda: calculate_tuple_fallback(base))

    return base.partial_fallback

</t>
<t tx="ekr.20221004064035.127">def infer_python_executable(options: Options, special_opts: argparse.Namespace) -&gt; None:
    """Infer the Python executable from the given version.

    This function mutates options based on special_opts to infer the correct Python executable
    to use.
    """
    # TODO: (ethanhs) Look at folding these checks and the site packages subprocess calls into
    # one subprocess call for speed.

    # Use the command line specified executable, or fall back to one set in the
    # config file. If an executable is not specified, infer it from the version
    # (unless no_executable is set)
    python_executable = special_opts.python_executable or options.python_executable

    if python_executable is None:
        if not special_opts.no_executable and not options.no_site_packages:
            python_executable = _python_executable_from_version(options.python_version)
    options.python_executable = python_executable


</t>
<t tx="ekr.20221004064035.1270">def set_dummy_mro(self, info: TypeInfo) -&gt; None:
    # Give it an MRO consisting of just the class itself and object.
    info.mro = [info, self.object_type().type]
    info.bad_mro = True

</t>
<t tx="ekr.20221004064035.1271">def calculate_class_mro(
    self, defn: ClassDef, obj_type: Callable[[], Instance] | None = None
) -&gt; None:
    """Calculate method resolution order for a class.

    `obj_type` exists just to fill in empty base class list in case of an error.
    """
    try:
        calculate_mro(defn.info, obj_type)
    except MroError:
        self.fail(
            "Cannot determine consistent method resolution "
            'order (MRO) for "%s"' % defn.name,
            defn,
        )
        self.set_dummy_mro(defn.info)
    # Allow plugins to alter the MRO to handle the fact that `def mro()`
    # on metaclasses permits MRO rewriting.
    if defn.fullname:
        hook = self.plugin.get_customize_class_mro_hook(defn.fullname)
        if hook:
            hook(ClassDefContext(defn, FakeExpression(), self))

</t>
<t tx="ekr.20221004064035.1272">def infer_metaclass_and_bases_from_compat_helpers(self, defn: ClassDef) -&gt; None:
    """Lookup for special metaclass declarations, and update defn fields accordingly.

    * six.with_metaclass(M, B1, B2, ...)
    * @six.add_metaclass(M)
    * future.utils.with_metaclass(M, B1, B2, ...)
    * past.utils.with_metaclass(M, B1, B2, ...)
    """

    # Look for six.with_metaclass(M, B1, B2, ...)
    with_meta_expr: Expression | None = None
    if len(defn.base_type_exprs) == 1:
        base_expr = defn.base_type_exprs[0]
        if isinstance(base_expr, CallExpr) and isinstance(base_expr.callee, RefExpr):
            base_expr.accept(self)
            if (
                base_expr.callee.fullname
                in {
                    "six.with_metaclass",
                    "future.utils.with_metaclass",
                    "past.utils.with_metaclass",
                }
                and len(base_expr.args) &gt;= 1
                and all(kind == ARG_POS for kind in base_expr.arg_kinds)
            ):
                with_meta_expr = base_expr.args[0]
                defn.base_type_exprs = base_expr.args[1:]

    # Look for @six.add_metaclass(M)
    add_meta_expr: Expression | None = None
    for dec_expr in defn.decorators:
        if isinstance(dec_expr, CallExpr) and isinstance(dec_expr.callee, RefExpr):
            dec_expr.callee.accept(self)
            if (
                dec_expr.callee.fullname == "six.add_metaclass"
                and len(dec_expr.args) == 1
                and dec_expr.arg_kinds[0] == ARG_POS
            ):
                add_meta_expr = dec_expr.args[0]
                break

    metas = {defn.metaclass, with_meta_expr, add_meta_expr} - {None}
    if len(metas) == 0:
        return
    if len(metas) &gt; 1:
        self.fail("Multiple metaclass definitions", defn)
        return
    defn.metaclass = metas.pop()

</t>
<t tx="ekr.20221004064035.1273">def verify_base_classes(self, defn: ClassDef) -&gt; bool:
    info = defn.info
    cycle = False
    for base in info.bases:
        baseinfo = base.type
        if self.is_base_class(info, baseinfo):
            self.fail("Cycle in inheritance hierarchy", defn)
            cycle = True
    dup = find_duplicate(info.direct_base_classes())
    if dup:
        self.fail(f'Duplicate base class "{dup.name}"', defn, blocker=True)
        return False
    return not cycle

</t>
<t tx="ekr.20221004064035.1274">def is_base_class(self, t: TypeInfo, s: TypeInfo) -&gt; bool:
    """Determine if t is a base class of s (but do not use mro)."""
    # Search the base class graph for t, starting from s.
    worklist = [s]
    visited = {s}
    while worklist:
        nxt = worklist.pop()
        if nxt == t:
            return True
        for base in nxt.bases:
            if base.type not in visited:
                worklist.append(base.type)
                visited.add(base.type)
    return False

</t>
<t tx="ekr.20221004064035.1275">def get_declared_metaclass(
    self, name: str, metaclass_expr: Expression | None
) -&gt; tuple[Instance | None, bool]:
    """Returns either metaclass instance or boolean whether we should defer."""
    declared_metaclass = None
    if metaclass_expr:
        metaclass_name = None
        if isinstance(metaclass_expr, NameExpr):
            metaclass_name = metaclass_expr.name
        elif isinstance(metaclass_expr, MemberExpr):
            metaclass_name = get_member_expr_fullname(metaclass_expr)
        if metaclass_name is None:
            self.fail(f'Dynamic metaclass not supported for "{name}"', metaclass_expr)
            return None, False
        sym = self.lookup_qualified(metaclass_name, metaclass_expr)
        if sym is None:
            # Probably a name error - it is already handled elsewhere
            return None, False
        if isinstance(sym.node, Var) and isinstance(get_proper_type(sym.node.type), AnyType):
            # Create a fake TypeInfo that fallbacks to `Any`, basically allowing
            # all the attributes. Same thing as we do for `Any` base class.
            any_info = self.make_empty_type_info(ClassDef(sym.node.name, Block([])))
            any_info.fallback_to_any = True
            any_info._fullname = sym.node.fullname
            if self.options.disallow_subclassing_any:
                self.fail(
                    f'Class cannot use "{any_info.fullname}" as a metaclass (has type "Any")',
                    metaclass_expr,
                )
            return Instance(any_info, []), False
        if isinstance(sym.node, PlaceholderNode):
            return None, True  # defer later in the caller

        # Support type aliases, like `_Meta: TypeAlias = type`
        if (
            isinstance(sym.node, TypeAlias)
            and sym.node.no_args
            and isinstance(sym.node.target, ProperType)
            and isinstance(sym.node.target, Instance)
        ):
            metaclass_info: Node | None = sym.node.target.type
        else:
            metaclass_info = sym.node

        if not isinstance(metaclass_info, TypeInfo) or metaclass_info.tuple_type is not None:
            self.fail(f'Invalid metaclass "{metaclass_name}"', metaclass_expr)
            return None, False
        if not metaclass_info.is_metaclass():
            self.fail(
                'Metaclasses not inheriting from "type" are not supported', metaclass_expr
            )
            return None, False
        inst = fill_typevars(metaclass_info)
        assert isinstance(inst, Instance)
        declared_metaclass = inst
    return declared_metaclass, False

</t>
<t tx="ekr.20221004064035.1276">def recalculate_metaclass(self, defn: ClassDef, declared_metaclass: Instance | None) -&gt; None:
    defn.info.declared_metaclass = declared_metaclass
    defn.info.metaclass_type = defn.info.calculate_metaclass_type()
    if any(info.is_protocol for info in defn.info.mro):
        if (
            not defn.info.metaclass_type
            or defn.info.metaclass_type.type.fullname == "builtins.type"
        ):
            # All protocols and their subclasses have ABCMeta metaclass by default.
            # TODO: add a metaclass conflict check if there is another metaclass.
            abc_meta = self.named_type_or_none("abc.ABCMeta", [])
            if abc_meta is not None:  # May be None in tests with incomplete lib-stub.
                defn.info.metaclass_type = abc_meta
    if defn.info.metaclass_type and defn.info.metaclass_type.type.has_base("enum.EnumMeta"):
        defn.info.is_enum = True
        if defn.type_vars:
            self.fail("Enum class cannot be generic", defn)

</t>
<t tx="ekr.20221004064035.1277">#
# Imports
#

</t>
<t tx="ekr.20221004064035.1278">def visit_import(self, i: Import) -&gt; None:
    self.statement = i
    for id, as_id in i.ids:
        # Modules imported in a stub file without using 'import X as X' won't get exported
        # When implicit re-exporting is disabled, we have the same behavior as stubs.
        use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
        if as_id is not None:
            base_id = id
            imported_id = as_id
            module_public = use_implicit_reexport or id.split(".")[-1] == as_id
        else:
            base_id = id.split(".")[0]
            imported_id = base_id
            module_public = use_implicit_reexport
        self.add_module_symbol(
            base_id,
            imported_id,
            context=i,
            module_public=module_public,
            module_hidden=not module_public,
        )

</t>
<t tx="ekr.20221004064035.1279">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    self.statement = imp
    module_id = self.correct_relative_import(imp)
    module = self.modules.get(module_id)
    for id, as_id in imp.names:
        fullname = module_id + "." + id
        self.set_future_import_flags(fullname)
        if module is None:
            node = None
        elif module_id == self.cur_mod_id and fullname in self.modules:
            # Submodule takes precedence over definition in surround package, for
            # compatibility with runtime semantics in typical use cases. This
            # could more precisely model runtime semantics by taking into account
            # the line number beyond which the local definition should take
            # precedence, but doesn't seem to be important in most use cases.
            node = SymbolTableNode(GDEF, self.modules[fullname])
        else:
            if id == as_id == "__all__" and module_id in self.export_map:
                self.all_exports[:] = self.export_map[module_id]
            node = module.names.get(id)

        missing_submodule = False
        imported_id = as_id or id

        # Modules imported in a stub file without using 'from Y import X as X' will
        # not get exported.
        # When implicit re-exporting is disabled, we have the same behavior as stubs.
        use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
        module_public = use_implicit_reexport or (as_id is not None and id == as_id)

        # If the module does not contain a symbol with the name 'id',
        # try checking if it's a module instead.
        if not node:
            mod = self.modules.get(fullname)
            if mod is not None:
                kind = self.current_symbol_kind()
                node = SymbolTableNode(kind, mod)
            elif fullname in self.missing_modules:
                missing_submodule = True
        # If it is still not resolved, check for a module level __getattr__
        if (
            module
            and not node
            and (module.is_stub or self.options.python_version &gt;= (3, 7))
            and "__getattr__" in module.names
        ):
            # We store the fullname of the original definition so that we can
            # detect whether two imported names refer to the same thing.
            fullname = module_id + "." + id
            gvar = self.create_getattr_var(module.names["__getattr__"], imported_id, fullname)
            if gvar:
                self.add_symbol(
                    imported_id,
                    gvar,
                    imp,
                    module_public=module_public,
                    module_hidden=not module_public,
                )
                continue

        if node and not node.module_hidden:
            self.process_imported_symbol(
                node, module_id, id, imported_id, fullname, module_public, context=imp
            )
        elif module and not missing_submodule:
            # Target module exists but the imported name is missing or hidden.
            self.report_missing_module_attribute(
                module_id,
                id,
                imported_id,
                module_public=module_public,
                module_hidden=not module_public,
                context=imp,
            )
        else:
            # Import of a missing (sub)module.
            self.add_unknown_imported_symbol(
                imported_id,
                imp,
                target_name=fullname,
                module_public=module_public,
                module_hidden=not module_public,
            )

</t>
<t tx="ekr.20221004064035.128">HEADER: Final = """%(prog)s [-h] [-v] [-V] [more options; see below]
            [-m MODULE] [-p PACKAGE] [-c PROGRAM_TEXT] [files ...]"""


DESCRIPTION: Final = """
Mypy is a program that will type check your Python code.

Pass in any files or folders you want to type check. Mypy will
recursively traverse any provided folders to find .py files:

    $ mypy my_program.py my_src_folder

For more information on getting started, see:

- https://mypy.readthedocs.io/en/stable/getting_started.html

For more details on both running mypy and using the flags below, see:

- https://mypy.readthedocs.io/en/stable/running_mypy.html
- https://mypy.readthedocs.io/en/stable/command_line.html

You can also use a config file to configure mypy instead of using
command line flags. For more details, see:

- https://mypy.readthedocs.io/en/stable/config_file.html
"""

FOOTER: Final = """Environment variables:
  Define MYPYPATH for additional module search path entries.
  Define MYPY_CACHE_DIR to override configuration cache_dir path."""


</t>
<t tx="ekr.20221004064035.1280">def process_imported_symbol(
    self,
    node: SymbolTableNode,
    module_id: str,
    id: str,
    imported_id: str,
    fullname: str,
    module_public: bool,
    context: ImportBase,
) -&gt; None:
    module_hidden = not module_public and (
        # `from package import submodule` should work regardless of whether package
        # re-exports submodule, so we shouldn't hide it
        not isinstance(node.node, MypyFile)
        or fullname not in self.modules
        # but given `from somewhere import random_unrelated_module` we should hide
        # random_unrelated_module
        or not fullname.startswith(self.cur_mod_id + ".")
    )

    if isinstance(node.node, PlaceholderNode):
        if self.final_iteration:
            self.report_missing_module_attribute(
                module_id,
                id,
                imported_id,
                module_public=module_public,
                module_hidden=module_hidden,
                context=context,
            )
            return
        else:
            # This might become a type.
            self.mark_incomplete(
                imported_id,
                node.node,
                module_public=module_public,
                module_hidden=module_hidden,
                becomes_typeinfo=True,
            )
    existing_symbol = self.globals.get(imported_id)
    if (
        existing_symbol
        and not isinstance(existing_symbol.node, PlaceholderNode)
        and not isinstance(node.node, PlaceholderNode)
    ):
        # Import can redefine a variable. They get special treatment.
        if self.process_import_over_existing_name(imported_id, existing_symbol, node, context):
            return
    if existing_symbol and isinstance(node.node, PlaceholderNode):
        # Imports are special, some redefinitions are allowed, so wait until
        # we know what is the new symbol node.
        return
    # NOTE: we take the original node even for final `Var`s. This is to support
    # a common pattern when constants are re-exported (same applies to import *).
    self.add_imported_symbol(
        imported_id, node, context, module_public=module_public, module_hidden=module_hidden
    )

</t>
<t tx="ekr.20221004064035.1281">def report_missing_module_attribute(
    self,
    import_id: str,
    source_id: str,
    imported_id: str,
    module_public: bool,
    module_hidden: bool,
    context: Node,
) -&gt; None:
    # Missing attribute.
    if self.is_incomplete_namespace(import_id):
        # We don't know whether the name will be there, since the namespace
        # is incomplete. Defer the current target.
        self.mark_incomplete(
            imported_id, context, module_public=module_public, module_hidden=module_hidden
        )
        return
    message = f'Module "{import_id}" has no attribute "{source_id}"'
    # Suggest alternatives, if any match is found.
    module = self.modules.get(import_id)
    if module:
        if not self.options.implicit_reexport and source_id in module.names.keys():
            message = (
                'Module "{}" does not explicitly export attribute "{}"'
                "; implicit reexport disabled".format(import_id, source_id)
            )
        else:
            alternatives = set(module.names.keys()).difference({source_id})
            matches = best_matches(source_id, alternatives)[:3]
            if matches:
                suggestion = f"; maybe {pretty_seq(matches, 'or')}?"
                message += f"{suggestion}"
    self.fail(message, context, code=codes.ATTR_DEFINED)
    self.add_unknown_imported_symbol(
        imported_id,
        context,
        target_name=None,
        module_public=module_public,
        module_hidden=not module_public,
    )

    if import_id == "typing":
        # The user probably has a missing definition in a test fixture. Let's verify.
        fullname = f"builtins.{source_id.lower()}"
        if (
            self.lookup_fully_qualified_or_none(fullname) is None
            and fullname in SUGGESTED_TEST_FIXTURES
        ):
            # Yes. Generate a helpful note.
            self.msg.add_fixture_note(fullname, context)

</t>
<t tx="ekr.20221004064035.1282">def process_import_over_existing_name(
    self,
    imported_id: str,
    existing_symbol: SymbolTableNode,
    module_symbol: SymbolTableNode,
    import_node: ImportBase,
) -&gt; bool:
    if existing_symbol.node is module_symbol.node:
        # We added this symbol on previous iteration.
        return False
    if existing_symbol.kind in (LDEF, GDEF, MDEF) and isinstance(
        existing_symbol.node, (Var, FuncDef, TypeInfo, Decorator, TypeAlias)
    ):
        # This is a valid import over an existing definition in the file. Construct a dummy
        # assignment that we'll use to type check the import.
        lvalue = NameExpr(imported_id)
        lvalue.kind = existing_symbol.kind
        lvalue.node = existing_symbol.node
        rvalue = NameExpr(imported_id)
        rvalue.kind = module_symbol.kind
        rvalue.node = module_symbol.node
        if isinstance(rvalue.node, TypeAlias):
            # Suppress bogus errors from the dummy assignment if rvalue is an alias.
            # Otherwise mypy may complain that alias is invalid in runtime context.
            rvalue.is_alias_rvalue = True
        assignment = AssignmentStmt([lvalue], rvalue)
        for node in assignment, lvalue, rvalue:
            node.set_line(import_node)
        import_node.assignments.append(assignment)
        return True
    return False

</t>
<t tx="ekr.20221004064035.1283">def correct_relative_import(self, node: ImportFrom | ImportAll) -&gt; str:
    import_id, ok = correct_relative_import(
        self.cur_mod_id, node.relative, node.id, self.cur_mod_node.is_package_init_file()
    )
    if not ok:
        self.fail("Relative import climbs too many namespaces", node)
    return import_id

</t>
<t tx="ekr.20221004064035.1284">def visit_import_all(self, i: ImportAll) -&gt; None:
    i_id = self.correct_relative_import(i)
    if i_id in self.modules:
        m = self.modules[i_id]
        if self.is_incomplete_namespace(i_id):
            # Any names could be missing from the current namespace if the target module
            # namespace is incomplete.
            self.mark_incomplete("*", i)
        for name, node in m.names.items():
            fullname = i_id + "." + name
            self.set_future_import_flags(fullname)
            if node is None:
                continue
            # if '__all__' exists, all nodes not included have had module_public set to
            # False, and we can skip checking '_' because it's been explicitly included.
            if node.module_public and (not name.startswith("_") or "__all__" in m.names):
                if isinstance(node.node, MypyFile):
                    # Star import of submodule from a package, add it as a dependency.
                    self.imports.add(node.node.fullname)
                existing_symbol = self.lookup_current_scope(name)
                if existing_symbol and not isinstance(node.node, PlaceholderNode):
                    # Import can redefine a variable. They get special treatment.
                    if self.process_import_over_existing_name(name, existing_symbol, node, i):
                        continue
                # `from x import *` always reexports symbols
                self.add_imported_symbol(
                    name, node, i, module_public=True, module_hidden=False
                )

    else:
        # Don't add any dummy symbols for 'from x import *' if 'x' is unknown.
        pass

</t>
<t tx="ekr.20221004064035.1285">#
# Assignment
#

</t>
<t tx="ekr.20221004064035.1286">def visit_assignment_expr(self, s: AssignmentExpr) -&gt; None:
    s.value.accept(self)
    self.analyze_lvalue(s.target, escape_comprehensions=True, has_explicit_value=True)

</t>
<t tx="ekr.20221004064035.1287">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    self.statement = s

    # Special case assignment like X = X.
    if self.analyze_identity_global_assignment(s):
        return

    tag = self.track_incomplete_refs()

    # Here we have a chicken and egg problem: at this stage we can't call
    # can_be_type_alias(), because we have not enough information about rvalue.
    # But we can't use a full visit because it may emit extra incomplete refs (namely
    # when analysing any type applications there) thus preventing the further analysis.
    # To break the tie, we first analyse rvalue partially, if it can be a type alias.
    with self.basic_type_applications_set(s):
        s.rvalue.accept(self)
    if self.found_incomplete_ref(tag) or self.should_wait_rhs(s.rvalue):
        # Initializer couldn't be fully analyzed. Defer the current node and give up.
        # Make sure that if we skip the definition of some local names, they can't be
        # added later in this scope, since an earlier definition should take precedence.
        for expr in names_modified_by_assignment(s):
            self.mark_incomplete(expr.name, expr)
        return
    if self.can_possibly_be_index_alias(s):
        # Now re-visit those rvalues that were we skipped type applications above.
        # This should be safe as generally semantic analyzer is idempotent.
        s.rvalue.accept(self)

    # The r.h.s. is now ready to be classified, first check if it is a special form:
    special_form = False
    # * type alias
    if self.check_and_set_up_type_alias(s):
        s.is_alias_def = True
        special_form = True
    # * type variable definition
    elif self.process_typevar_declaration(s):
        special_form = True
    elif self.process_paramspec_declaration(s):
        special_form = True
    elif self.process_typevartuple_declaration(s):
        special_form = True
    # * type constructors
    elif self.analyze_namedtuple_assign(s):
        special_form = True
    elif self.analyze_typeddict_assign(s):
        special_form = True
    elif self.newtype_analyzer.process_newtype_declaration(s):
        special_form = True
    elif self.analyze_enum_assign(s):
        special_form = True

    if special_form:
        self.record_special_form_lvalue(s)
        return
    # Clear the alias flag if assignment turns out not a special form after all. It
    # may be set to True while there were still placeholders due to forward refs.
    s.is_alias_def = False

    # OK, this is a regular assignment, perform the necessary analysis steps.
    s.is_final_def = self.unwrap_final(s)
    self.analyze_lvalues(s)
    self.check_final_implicit_def(s)
    self.store_final_status(s)
    self.check_classvar(s)
    self.process_type_annotation(s)
    self.apply_dynamic_class_hook(s)
    if not s.type:
        self.process_module_assignment(s.lvalues, s.rvalue, s)
    self.process__all__(s)
    self.process__deletable__(s)
    self.process__slots__(s)

</t>
<t tx="ekr.20221004064035.1288">def analyze_identity_global_assignment(self, s: AssignmentStmt) -&gt; bool:
    """Special case 'X = X' in global scope.

    This allows supporting some important use cases.

    Return true if special casing was applied.
    """
    if not isinstance(s.rvalue, NameExpr) or len(s.lvalues) != 1:
        # Not of form 'X = X'
        return False
    lvalue = s.lvalues[0]
    if not isinstance(lvalue, NameExpr) or s.rvalue.name != lvalue.name:
        # Not of form 'X = X'
        return False
    if self.type is not None or self.is_func_scope():
        # Not in global scope
        return False
    # It's an assignment like 'X = X' in the global scope.
    name = lvalue.name
    sym = self.lookup(name, s)
    if sym is None:
        if self.final_iteration:
            # Fall back to normal assignment analysis.
            return False
        else:
            self.defer()
            return True
    else:
        if sym.node is None:
            # Something special -- fall back to normal assignment analysis.
            return False
        if name not in self.globals:
            # The name is from builtins. Add an alias to the current module.
            self.add_symbol(name, sym.node, s)
        if not isinstance(sym.node, PlaceholderNode):
            for node in s.rvalue, lvalue:
                node.node = sym.node
                node.kind = GDEF
                node.fullname = sym.node.fullname
        return True

</t>
<t tx="ekr.20221004064035.1289">def should_wait_rhs(self, rv: Expression) -&gt; bool:
    """Can we already classify this r.h.s. of an assignment or should we wait?

    This returns True if we don't have enough information to decide whether
    an assignment is just a normal variable definition or a special form.
    Always return False if this is a final iteration. This will typically cause
    the lvalue to be classified as a variable plus emit an error.
    """
    if self.final_iteration:
        # No chance, nothing has changed.
        return False
    if isinstance(rv, NameExpr):
        n = self.lookup(rv.name, rv)
        if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
            return True
    elif isinstance(rv, MemberExpr):
        fname = get_member_expr_fullname(rv)
        if fname:
            n = self.lookup_qualified(fname, rv, suppress_errors=True)
            if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
                return True
    elif isinstance(rv, IndexExpr) and isinstance(rv.base, RefExpr):
        return self.should_wait_rhs(rv.base)
    elif isinstance(rv, CallExpr) and isinstance(rv.callee, RefExpr):
        # This is only relevant for builtin SCC where things like 'TypeVar'
        # may be not ready.
        return self.should_wait_rhs(rv.callee)
    return False

</t>
<t tx="ekr.20221004064035.129">class CapturableArgumentParser(argparse.ArgumentParser):

    """Override ArgumentParser methods that use sys.stdout/sys.stderr directly.

    This is needed because hijacking sys.std* is not thread-safe,
    yet output must be captured to properly support mypy.api.run.
    """

    @others
</t>
<t tx="ekr.20221004064035.1290">def can_be_type_alias(self, rv: Expression, allow_none: bool = False) -&gt; bool:
    """Is this a valid r.h.s. for an alias definition?

    Note: this function should be only called for expressions where self.should_wait_rhs()
    returns False.
    """
    if isinstance(rv, RefExpr) and self.is_type_ref(rv, bare=True):
        return True
    if isinstance(rv, IndexExpr) and self.is_type_ref(rv.base, bare=False):
        return True
    if self.is_none_alias(rv):
        return True
    if allow_none and isinstance(rv, NameExpr) and rv.fullname == "builtins.None":
        return True
    if isinstance(rv, OpExpr) and rv.op == "|":
        if self.is_stub_file:
            return True
        if self.can_be_type_alias(rv.left, allow_none=True) and self.can_be_type_alias(
            rv.right, allow_none=True
        ):
            return True
    return False

</t>
<t tx="ekr.20221004064035.1291">def can_possibly_be_index_alias(self, s: AssignmentStmt) -&gt; bool:
    """Like can_be_type_alias(), but simpler and doesn't require analyzed rvalue.

    Instead, use lvalues/annotations structure to figure out whether this can
    potentially be a type alias definition. Another difference from above function
    is that we are only interested IndexExpr and OpExpr rvalues, since only those
    can be potentially recursive (things like `A = A` are never valid).
    """
    if len(s.lvalues) &gt; 1:
        return False
    if not isinstance(s.lvalues[0], NameExpr):
        return False
    if s.unanalyzed_type is not None and not self.is_pep_613(s):
        return False
    if not isinstance(s.rvalue, (IndexExpr, OpExpr)):
        return False
    # Something that looks like Foo = Bar[Baz, ...]
    return True

</t>
<t tx="ekr.20221004064035.1292">@contextmanager
def basic_type_applications_set(self, s: AssignmentStmt) -&gt; Iterator[None]:
    old = self.basic_type_applications
    # As an optimization, only use the double visit logic if this
    # can possibly be a recursive type alias.
    self.basic_type_applications = self.can_possibly_be_index_alias(s)
    try:
        yield
    finally:
        self.basic_type_applications = old

</t>
<t tx="ekr.20221004064035.1293">def is_type_ref(self, rv: Expression, bare: bool = False) -&gt; bool:
    """Does this expression refer to a type?

    This includes:
      * Special forms, like Any or Union
      * Classes (except subscripted enums)
      * Other type aliases
      * PlaceholderNodes with becomes_typeinfo=True (these can be not ready class
        definitions, and not ready aliases).

    If bare is True, this is not a base of an index expression, so some special
    forms are not valid (like a bare Union).

    Note: This method should be only used in context of a type alias definition.
    This method can only return True for RefExprs, to check if C[int] is a valid
    target for type alias call this method on expr.base (i.e. on C in C[int]).
    See also can_be_type_alias().
    """
    if not isinstance(rv, RefExpr):
        return False
    if isinstance(rv.node, TypeVarLikeExpr):
        self.fail(f'Type variable "{rv.fullname}" is invalid as target for type alias', rv)
        return False

    if bare:
        # These three are valid even if bare, for example
        # A = Tuple is just equivalent to A = Tuple[Any, ...].
        valid_refs = {"typing.Any", "typing.Tuple", "typing.Callable"}
    else:
        valid_refs = type_constructors

    if isinstance(rv.node, TypeAlias) or rv.fullname in valid_refs:
        return True
    if isinstance(rv.node, TypeInfo):
        if bare:
            return True
        # Assignment color = Color['RED'] defines a variable, not an alias.
        return not rv.node.is_enum
    if isinstance(rv.node, Var):
        return rv.node.fullname in NEVER_NAMES

    if isinstance(rv, NameExpr):
        n = self.lookup(rv.name, rv)
        if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
            return True
    elif isinstance(rv, MemberExpr):
        fname = get_member_expr_fullname(rv)
        if fname:
            # The r.h.s. for variable definitions may not be a type reference but just
            # an instance attribute, so suppress the errors.
            n = self.lookup_qualified(fname, rv, suppress_errors=True)
            if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
                return True
    return False

</t>
<t tx="ekr.20221004064035.1294">def is_none_alias(self, node: Expression) -&gt; bool:
    """Is this a r.h.s. for a None alias?

    We special case the assignments like Void = type(None), to allow using
    Void in type annotations.
    """
    if isinstance(node, CallExpr):
        if (
            isinstance(node.callee, NameExpr)
            and len(node.args) == 1
            and isinstance(node.args[0], NameExpr)
        ):
            call = self.lookup_qualified(node.callee.name, node.callee)
            arg = self.lookup_qualified(node.args[0].name, node.args[0])
            if (
                call is not None
                and call.node
                and call.node.fullname == "builtins.type"
                and arg is not None
                and arg.node
                and arg.node.fullname == "builtins.None"
            ):
                return True
    return False

</t>
<t tx="ekr.20221004064035.1295">def record_special_form_lvalue(self, s: AssignmentStmt) -&gt; None:
    """Record minimal necessary information about l.h.s. of a special form.

    This exists mostly for compatibility with the old semantic analyzer.
    """
    lvalue = s.lvalues[0]
    assert isinstance(lvalue, NameExpr)
    lvalue.is_special_form = True
    if self.current_symbol_kind() == GDEF:
        lvalue.fullname = self.qualified_name(lvalue.name)
    lvalue.kind = self.current_symbol_kind()

</t>
<t tx="ekr.20221004064035.1296">def analyze_enum_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines an Enum."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, EnumCallExpr):
        # Already analyzed enum -- nothing to do here.
        return True
    return self.enum_call_analyzer.process_enum_call(s, self.is_func_scope())

</t>
<t tx="ekr.20221004064035.1297">def analyze_namedtuple_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines a namedtuple."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, NamedTupleExpr):
        if s.rvalue.analyzed.info.tuple_type and not has_placeholder(
            s.rvalue.analyzed.info.tuple_type
        ):
            return True  # This is a valid and analyzed named tuple definition, nothing to do here.
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    namespace = self.qualified_name(name)
    with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
        internal_name, info, tvar_defs = self.named_tuple_analyzer.check_namedtuple(
            s.rvalue, name, self.is_func_scope()
        )
        if internal_name is None:
            return False
        if isinstance(lvalue, MemberExpr):
            self.fail("NamedTuple type as an attribute is not supported", lvalue)
            return False
        if internal_name != name:
            self.fail(
                'First argument to namedtuple() should be "{}", not "{}"'.format(
                    name, internal_name
                ),
                s.rvalue,
                code=codes.NAME_MATCH,
            )
            return True
        # Yes, it's a valid namedtuple, but defer if it is not ready.
        if not info:
            self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
        else:
            self.setup_type_vars(info.defn, tvar_defs)
            self.setup_alias_type_vars(info.defn)
        return True

</t>
<t tx="ekr.20221004064035.1298">def analyze_typeddict_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines a typed dict."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, TypedDictExpr):
        if s.rvalue.analyzed.info.typeddict_type and not has_placeholder(
            s.rvalue.analyzed.info.typeddict_type
        ):
            # This is a valid and analyzed typed dict definition, nothing to do here.
            return True
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    namespace = self.qualified_name(name)
    with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
        is_typed_dict, info, tvar_defs = self.typed_dict_analyzer.check_typeddict(
            s.rvalue, name, self.is_func_scope()
        )
        if not is_typed_dict:
            return False
        if isinstance(lvalue, MemberExpr):
            self.fail("TypedDict type as attribute is not supported", lvalue)
            return False
        # Yes, it's a valid typed dict, but defer if it is not ready.
        if not info:
            self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
        else:
            defn = info.defn
            self.setup_type_vars(defn, tvar_defs)
            self.setup_alias_type_vars(defn)
        return True

</t>
<t tx="ekr.20221004064035.1299">def analyze_lvalues(self, s: AssignmentStmt) -&gt; None:
    # We cannot use s.type, because analyze_simple_literal_type() will set it.
    explicit = s.unanalyzed_type is not None
    if self.is_final_type(s.unanalyzed_type):
        # We need to exclude bare Final.
        assert isinstance(s.unanalyzed_type, UnboundType)
        if not s.unanalyzed_type.args:
            explicit = False

    if s.rvalue:
        if isinstance(s.rvalue, TempNode):
            has_explicit_value = not s.rvalue.no_rhs
        else:
            has_explicit_value = True
    else:
        has_explicit_value = False

    for lval in s.lvalues:
        self.analyze_lvalue(
            lval,
            explicit_type=explicit,
            is_final=s.is_final_def,
            has_explicit_value=has_explicit_value,
        )

</t>
<t tx="ekr.20221004064035.13">def __enter__(self) -&gt; IPCServer:
    if sys.platform == "win32":
        # NOTE: It is theoretically possible that this will hang forever if the
        # client never connects, though this can be "solved" by killing the server
        try:
            ov = _winapi.ConnectNamedPipe(self.connection, overlapped=True)
            # TODO: remove once typeshed supports Literal types
            assert isinstance(ov, _winapi.Overlapped)
        except OSError as e:
            # Don't raise if the client already exists, or the client already connected
            if e.winerror not in (_winapi.ERROR_PIPE_CONNECTED, _winapi.ERROR_NO_DATA):
                raise
        else:
            try:
                timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                res = _winapi.WaitForSingleObject(ov.event, timeout)
                assert res == _winapi.WAIT_OBJECT_0
            except BaseException:
                ov.cancel()
                _winapi.CloseHandle(self.connection)
                raise
            _, err = ov.GetOverlappedResult(True)
            assert err == 0
    else:
        try:
            self.connection, _ = self.sock.accept()
        except socket.timeout as e:
            raise IPCException("The socket timed out") from e
    return self

</t>
<t tx="ekr.20221004064035.130">def __init__(self, *args: Any, **kwargs: Any):
    self.stdout = kwargs.pop("stdout", sys.stdout)
    self.stderr = kwargs.pop("stderr", sys.stderr)
    super().__init__(*args, **kwargs)

</t>
<t tx="ekr.20221004064035.1300">def apply_dynamic_class_hook(self, s: AssignmentStmt) -&gt; None:
    if not isinstance(s.rvalue, CallExpr):
        return
    fname = None
    call = s.rvalue
    while True:
        if isinstance(call.callee, RefExpr):
            fname = call.callee.fullname
        # check if method call
        if fname is None and isinstance(call.callee, MemberExpr):
            callee_expr = call.callee.expr
            if isinstance(callee_expr, RefExpr) and callee_expr.fullname:
                method_name = call.callee.name
                fname = callee_expr.fullname + "." + method_name
            elif isinstance(callee_expr, CallExpr):
                # check if chain call
                call = callee_expr
                continue
        break
    if not fname:
        return
    hook = self.plugin.get_dynamic_class_hook(fname)
    if not hook:
        return
    for lval in s.lvalues:
        if not isinstance(lval, NameExpr):
            continue
        hook(DynamicClassDefContext(call, lval.name, self))

</t>
<t tx="ekr.20221004064035.1301">def unwrap_final(self, s: AssignmentStmt) -&gt; bool:
    """Strip Final[...] if present in an assignment.

    This is done to invoke type inference during type checking phase for this
    assignment. Also, Final[...] doesn't affect type in any way -- it is rather an
    access qualifier for given `Var`.

    Also perform various consistency checks.

    Returns True if Final[...] was present.
    """
    if not s.unanalyzed_type or not self.is_final_type(s.unanalyzed_type):
        return False
    assert isinstance(s.unanalyzed_type, UnboundType)
    if len(s.unanalyzed_type.args) &gt; 1:
        self.fail("Final[...] takes at most one type argument", s.unanalyzed_type)
    invalid_bare_final = False
    if not s.unanalyzed_type.args:
        s.type = None
        if isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs:
            invalid_bare_final = True
            self.fail("Type in Final[...] can only be omitted if there is an initializer", s)
    else:
        s.type = s.unanalyzed_type.args[0]

    if s.type is not None and self.is_classvar(s.type):
        self.fail("Variable should not be annotated with both ClassVar and Final", s)
        return False

    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], RefExpr):
        self.fail("Invalid final declaration", s)
        return False
    lval = s.lvalues[0]
    assert isinstance(lval, RefExpr)

    # Reset inferred status if it was set due to simple literal rvalue on previous iteration.
    # TODO: this is a best-effort quick fix, we should avoid the need to manually sync this,
    # see https://github.com/python/mypy/issues/6458.
    if lval.is_new_def:
        lval.is_inferred_def = s.type is None

    if self.loop_depth &gt; 0:
        self.fail("Cannot use Final inside a loop", s)
    if self.type and self.type.is_protocol:
        self.msg.protocol_members_cant_be_final(s)
    if (
        isinstance(s.rvalue, TempNode)
        and s.rvalue.no_rhs
        and not self.is_stub_file
        and not self.is_class_scope()
    ):
        if not invalid_bare_final:  # Skip extra error messages.
            self.msg.final_without_value(s)
    return True

</t>
<t tx="ekr.20221004064035.1302">def check_final_implicit_def(self, s: AssignmentStmt) -&gt; None:
    """Do basic checks for final declaration on self in __init__.

    Additional re-definition checks are performed by `analyze_lvalue`.
    """
    if not s.is_final_def:
        return
    lval = s.lvalues[0]
    assert isinstance(lval, RefExpr)
    if isinstance(lval, MemberExpr):
        if not self.is_self_member_ref(lval):
            self.fail("Final can be only applied to a name or an attribute on self", s)
            s.is_final_def = False
            return
        else:
            assert self.function_stack
            if self.function_stack[-1].name != "__init__":
                self.fail("Can only declare a final attribute in class body or __init__", s)
                s.is_final_def = False
                return

</t>
<t tx="ekr.20221004064035.1303">def store_final_status(self, s: AssignmentStmt) -&gt; None:
    """If this is a locally valid final declaration, set the corresponding flag on `Var`."""
    if s.is_final_def:
        if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
            node = s.lvalues[0].node
            if isinstance(node, Var):
                node.is_final = True
                node.final_value = self.unbox_literal(s.rvalue)
                if self.is_class_scope() and (
                    isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs
                ):
                    node.final_unset_in_class = True
    else:
        for lval in self.flatten_lvalues(s.lvalues):
            # Special case: we are working with an `Enum`:
            #
            #   class MyEnum(Enum):
            #       key = 'some value'
            #
            # Here `key` is implicitly final. In runtime, code like
            #
            #     MyEnum.key = 'modified'
            #
            # will fail with `AttributeError: Cannot reassign members.`
            # That's why we need to replicate this.
            if (
                isinstance(lval, NameExpr)
                and isinstance(self.type, TypeInfo)
                and self.type.is_enum
            ):
                cur_node = self.type.names.get(lval.name, None)
                if (
                    cur_node
                    and isinstance(cur_node.node, Var)
                    and not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)
                ):
                    # Double underscored members are writable on an `Enum`.
                    # (Except read-only `__members__` but that is handled in type checker)
                    cur_node.node.is_final = s.is_final_def = not is_dunder(cur_node.node.name)

            # Special case: deferred initialization of a final attribute in __init__.
            # In this case we just pretend this is a valid final definition to suppress
            # errors about assigning to final attribute.
            if isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                assert self.type, "Self member outside a class"
                cur_node = self.type.names.get(lval.name, None)
                if cur_node and isinstance(cur_node.node, Var) and cur_node.node.is_final:
                    assert self.function_stack
                    top_function = self.function_stack[-1]
                    if (
                        top_function.name == "__init__"
                        and cur_node.node.final_unset_in_class
                        and not cur_node.node.final_set_in_init
                        and not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)
                    ):
                        cur_node.node.final_set_in_init = True
                        s.is_final_def = True

</t>
<t tx="ekr.20221004064035.1304">def flatten_lvalues(self, lvalues: list[Expression]) -&gt; list[Expression]:
    res: list[Expression] = []
    for lv in lvalues:
        if isinstance(lv, (TupleExpr, ListExpr)):
            res.extend(self.flatten_lvalues(lv.items))
        else:
            res.append(lv)
    return res

</t>
<t tx="ekr.20221004064035.1305">def unbox_literal(self, e: Expression) -&gt; int | float | bool | str | None:
    if isinstance(e, (IntExpr, FloatExpr, StrExpr)):
        return e.value
    elif isinstance(e, NameExpr) and e.name in ("True", "False"):
        return True if e.name == "True" else False
    return None

</t>
<t tx="ekr.20221004064035.1306">def process_type_annotation(self, s: AssignmentStmt) -&gt; None:
    """Analyze type annotation or infer simple literal type."""
    if s.type:
        lvalue = s.lvalues[-1]
        allow_tuple_literal = isinstance(lvalue, TupleExpr)
        analyzed = self.anal_type(s.type, allow_tuple_literal=allow_tuple_literal)
        # Don't store not ready types (including placeholders).
        if analyzed is None or has_placeholder(analyzed):
            self.defer(s)
            return
        s.type = analyzed
        if (
            self.type
            and self.type.is_protocol
            and isinstance(lvalue, NameExpr)
            and isinstance(s.rvalue, TempNode)
            and s.rvalue.no_rhs
        ):
            if isinstance(lvalue.node, Var):
                lvalue.node.is_abstract_var = True
    else:
        if (
            self.type
            and self.type.is_protocol
            and self.is_annotated_protocol_member(s)
            and not self.is_func_scope()
        ):
            self.fail("All protocol members must have explicitly declared types", s)
        # Set the type if the rvalue is a simple literal (even if the above error occurred).
        if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
            ref_expr = s.lvalues[0]
            safe_literal_inference = True
            if self.type and isinstance(ref_expr, NameExpr) and len(self.type.mro) &gt; 1:
                # Check if there is a definition in supertype. If yes, we can't safely
                # decide here what to infer: int or Literal[42].
                safe_literal_inference = self.type.mro[1].get(ref_expr.name) is None
            if safe_literal_inference and ref_expr.is_inferred_def:
                s.type = self.analyze_simple_literal_type(s.rvalue, s.is_final_def)
    if s.type:
        # Store type into nodes.
        for lvalue in s.lvalues:
            self.store_declared_types(lvalue, s.type)

</t>
<t tx="ekr.20221004064035.1307">def is_annotated_protocol_member(self, s: AssignmentStmt) -&gt; bool:
    """Check whether a protocol member is annotated.

    There are some exceptions that can be left unannotated, like ``__slots__``."""
    return any(
        (isinstance(lv, NameExpr) and lv.name != "__slots__" and lv.is_inferred_def)
        for lv in s.lvalues
    )

</t>
<t tx="ekr.20221004064035.1308">def analyze_simple_literal_type(self, rvalue: Expression, is_final: bool) -&gt; Type | None:
    """Return builtins.int if rvalue is an int literal, etc.
    If this is a 'Final' context, we return "Literal[...]" instead."""
    if self.options.semantic_analysis_only or self.function_stack:
        # Skip this if we're only doing the semantic analysis pass.
        # This is mostly to avoid breaking unit tests.
        # Also skip inside a function; this is to avoid confusing
        # the code that handles dead code due to isinstance()
        # inside type variables with value restrictions (like
        # AnyStr).
        return None
    if isinstance(rvalue, FloatExpr):
        return self.named_type_or_none("builtins.float")

    value: LiteralValue | None = None
    type_name: str | None = None
    if isinstance(rvalue, IntExpr):
        value, type_name = rvalue.value, "builtins.int"
    if isinstance(rvalue, StrExpr):
        value, type_name = rvalue.value, "builtins.str"
    if isinstance(rvalue, BytesExpr):
        value, type_name = rvalue.value, "builtins.bytes"

    if type_name is not None:
        assert value is not None
        typ = self.named_type_or_none(type_name)
        if typ and is_final:
            return typ.copy_modified(
                last_known_value=LiteralType(
                    value=value, fallback=typ, line=typ.line, column=typ.column
                )
            )
        return typ

    return None

</t>
<t tx="ekr.20221004064035.1309">def analyze_alias(
    self, rvalue: Expression, allow_placeholder: bool = False
) -&gt; tuple[Type | None, list[str], set[str], list[str]]:
    """Check if 'rvalue' is a valid type allowed for aliasing (e.g. not a type variable).

    If yes, return the corresponding type, a list of
    qualified type variable names for generic aliases, a set of names the alias depends on,
    and a list of type variables if the alias is generic.
    An schematic example for the dependencies:
        A = int
        B = str
        analyze_alias(Dict[A, B])[2] == {'__main__.A', '__main__.B'}
    """
    dynamic = bool(self.function_stack and self.function_stack[-1].is_dynamic())
    global_scope = not self.type and not self.function_stack
    res = analyze_type_alias(
        rvalue,
        self,
        self.tvar_scope,
        self.plugin,
        self.options,
        self.is_typeshed_stub_file,
        allow_placeholder=allow_placeholder,
        in_dynamic_func=dynamic,
        global_scope=global_scope,
    )
    typ: Type | None = None
    if res:
        typ, depends_on = res
        found_type_vars = typ.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
        alias_tvars = [name for (name, node) in found_type_vars]
        qualified_tvars = [node.fullname for (name, node) in found_type_vars]
    else:
        alias_tvars = []
        depends_on = set()
        qualified_tvars = []
    return typ, alias_tvars, depends_on, qualified_tvars

</t>
<t tx="ekr.20221004064035.131"># =====================
# Help-printing methods
# =====================
def print_usage(self, file: IO[str] | None = None) -&gt; None:
    if file is None:
        file = self.stdout
    self._print_message(self.format_usage(), file)

</t>
<t tx="ekr.20221004064035.1310">def is_pep_613(self, s: AssignmentStmt) -&gt; bool:
    if s.unanalyzed_type is not None and isinstance(s.unanalyzed_type, UnboundType):
        lookup = self.lookup_qualified(s.unanalyzed_type.name, s, suppress_errors=True)
        if lookup and lookup.fullname in TYPE_ALIAS_NAMES:
            return True
    return False

</t>
<t tx="ekr.20221004064035.1311">def check_and_set_up_type_alias(self, s: AssignmentStmt) -&gt; bool:
    """Check if assignment creates a type alias and set it up as needed.

    Return True if it is a type alias (even if the target is not ready),
    or False otherwise.

    Note: the resulting types for subscripted (including generic) aliases
    are also stored in rvalue.analyzed.
    """
    if s.invalid_recursive_alias:
        return True
    lvalue = s.lvalues[0]
    if len(s.lvalues) &gt; 1 or not isinstance(lvalue, NameExpr):
        # First rule: Only simple assignments like Alias = ... create aliases.
        return False

    pep_613 = self.is_pep_613(s)
    if not pep_613 and s.unanalyzed_type is not None:
        # Second rule: Explicit type (cls: Type[A] = A) always creates variable, not alias.
        # unless using PEP 613 `cls: TypeAlias = A`
        return False

    if isinstance(s.rvalue, CallExpr) and s.rvalue.analyzed:
        return False

    existing = self.current_symbol_table().get(lvalue.name)
    # Third rule: type aliases can't be re-defined. For example:
    #     A: Type[float] = int
    #     A = float  # OK, but this doesn't define an alias
    #     B = int
    #     B = float  # Error!
    # Don't create an alias in these cases:
    if existing and (
        isinstance(existing.node, Var)  # existing variable
        or (isinstance(existing.node, TypeAlias) and not s.is_alias_def)  # existing alias
        or (isinstance(existing.node, PlaceholderNode) and existing.node.node.line &lt; s.line)
    ):  # previous incomplete definition
        # TODO: find a more robust way to track the order of definitions.
        # Note: if is_alias_def=True, this is just a node from previous iteration.
        if isinstance(existing.node, TypeAlias) and not s.is_alias_def:
            self.fail(
                'Cannot assign multiple types to name "{}"'
                ' without an explicit "Type[...]" annotation'.format(lvalue.name),
                lvalue,
            )
        return False

    non_global_scope = self.type or self.is_func_scope()
    if not pep_613 and isinstance(s.rvalue, RefExpr) and non_global_scope:
        # Fourth rule (special case): Non-subscripted right hand side creates a variable
        # at class and function scopes. For example:
        #
        #   class Model:
        #       ...
        #   class C:
        #       model = Model # this is automatically a variable with type 'Type[Model]'
        #
        # without this rule, this typical use case will require a lot of explicit
        # annotations (see the second rule).
        return False
    rvalue = s.rvalue
    if not pep_613 and not self.can_be_type_alias(rvalue):
        return False

    if existing and not isinstance(existing.node, (PlaceholderNode, TypeAlias)):
        # Cannot redefine existing node as type alias.
        return False

    res: Type | None = None
    if self.is_none_alias(rvalue):
        res = NoneType()
        alias_tvars: list[str] = []
        depends_on: set[str] = set()
        qualified_tvars: list[str] = []
    else:
        tag = self.track_incomplete_refs()
        res, alias_tvars, depends_on, qualified_tvars = self.analyze_alias(
            rvalue, allow_placeholder=True
        )
        if not res:
            return False
        if not self.options.disable_recursive_aliases and not self.is_func_scope():
            # Only marking incomplete for top-level placeholders makes recursive aliases like
            # `A = Sequence[str | A]` valid here, similar to how we treat base classes in class
            # definitions, allowing `class str(Sequence[str]): ...`
            incomplete_target = isinstance(res, ProperType) and isinstance(
                res, PlaceholderType
            )
        else:
            incomplete_target = has_placeholder(res)
        if self.found_incomplete_ref(tag) or incomplete_target:
            # Since we have got here, we know this must be a type alias (incomplete refs
            # may appear in nested positions), therefore use becomes_typeinfo=True.
            self.mark_incomplete(lvalue.name, rvalue, becomes_typeinfo=True)
            return True
    self.add_type_alias_deps(depends_on)
    # In addition to the aliases used, we add deps on unbound
    # type variables, since they are erased from target type.
    self.add_type_alias_deps(qualified_tvars)
    # The above are only direct deps on other aliases.
    # For subscripted aliases, type deps from expansion are added in deps.py
    # (because the type is stored).
    check_for_explicit_any(res, self.options, self.is_typeshed_stub_file, self.msg, context=s)
    # When this type alias gets "inlined", the Any is not explicit anymore,
    # so we need to replace it with non-explicit Anys.
    res = make_any_non_explicit(res)
    # Note: with the new (lazy) type alias representation we only need to set no_args to True
    # if the expected number of arguments is non-zero, so that aliases like A = List work.
    # However, eagerly expanding aliases like Text = str is a nice performance optimization.
    no_args = isinstance(res, Instance) and not res.args  # type: ignore[misc]
    fix_instance_types(res, self.fail, self.note, self.options.python_version)
    # Aliases defined within functions can't be accessed outside
    # the function, since the symbol table will no longer
    # exist. Work around by expanding them eagerly when used.
    eager = self.is_func_scope()
    alias_node = TypeAlias(
        res,
        self.qualified_name(lvalue.name),
        s.line,
        s.column,
        alias_tvars=alias_tvars,
        no_args=no_args,
        eager=eager,
    )
    if isinstance(s.rvalue, (IndexExpr, CallExpr)):  # CallExpr is for `void = type(None)`
        s.rvalue.analyzed = TypeAliasExpr(alias_node)
        s.rvalue.analyzed.line = s.line
        # we use the column from resulting target, to get better location for errors
        s.rvalue.analyzed.column = res.column
    elif isinstance(s.rvalue, RefExpr):
        s.rvalue.is_alias_rvalue = True

    if existing:
        # An alias gets updated.
        updated = False
        if isinstance(existing.node, TypeAlias):
            if existing.node.target != res:
                # Copy expansion to the existing alias, this matches how we update base classes
                # for a TypeInfo _in place_ if there are nested placeholders.
                existing.node.target = res
                existing.node.alias_tvars = alias_tvars
                existing.node.no_args = no_args
                updated = True
        else:
            # Otherwise just replace existing placeholder with type alias.
            existing.node = alias_node
            updated = True
        if updated:
            if self.final_iteration:
                self.cannot_resolve_name(lvalue.name, "name", s)
                return True
            else:
                # We need to defer so that this change can get propagated to base classes.
                self.defer(s, force_progress=True)
    else:
        self.add_symbol(lvalue.name, alias_node, s)
    if isinstance(rvalue, RefExpr) and isinstance(rvalue.node, TypeAlias):
        alias_node.normalized = rvalue.node.normalized
    current_node = existing.node if existing else alias_node
    assert isinstance(current_node, TypeAlias)
    self.disable_invalid_recursive_aliases(s, current_node)
    if self.is_class_scope():
        assert self.type is not None
        if self.type.is_protocol:
            self.fail("Type aliases are prohibited in protocol bodies", s)
            if not lvalue.name[0].isupper():
                self.note("Use variable annotation syntax to define protocol members", s)
    return True

</t>
<t tx="ekr.20221004064035.1312">def disable_invalid_recursive_aliases(
    self, s: AssignmentStmt, current_node: TypeAlias
) -&gt; None:
    """Prohibit and fix recursive type aliases that are invalid/unsupported."""
    messages = []
    if invalid_recursive_alias({current_node}, current_node.target):
        messages.append("Invalid recursive alias: a union item of itself")
    if detect_diverging_alias(
        current_node, current_node.target, self.lookup_qualified, self.tvar_scope
    ):
        messages.append("Invalid recursive alias: type variable nesting on right hand side")
    if messages:
        current_node.target = AnyType(TypeOfAny.from_error)
        s.invalid_recursive_alias = True
    for msg in messages:
        self.fail(msg, s.rvalue)

</t>
<t tx="ekr.20221004064035.1313">def analyze_lvalue(
    self,
    lval: Lvalue,
    nested: bool = False,
    explicit_type: bool = False,
    is_final: bool = False,
    escape_comprehensions: bool = False,
    has_explicit_value: bool = False,
) -&gt; None:
    """Analyze an lvalue or assignment target.

    Args:
        lval: The target lvalue
        nested: If true, the lvalue is within a tuple or list lvalue expression
        explicit_type: Assignment has type annotation
        escape_comprehensions: If we are inside a comprehension, set the variable
            in the enclosing scope instead. This implements
            https://www.python.org/dev/peps/pep-0572/#scope-of-the-target
    """
    if escape_comprehensions:
        assert isinstance(lval, NameExpr), "assignment expression target must be NameExpr"
    if isinstance(lval, NameExpr):
        self.analyze_name_lvalue(
            lval,
            explicit_type,
            is_final,
            escape_comprehensions,
            has_explicit_value=has_explicit_value,
        )
    elif isinstance(lval, MemberExpr):
        self.analyze_member_lvalue(lval, explicit_type, is_final)
        if explicit_type and not self.is_self_member_ref(lval):
            self.fail("Type cannot be declared in assignment to non-self attribute", lval)
    elif isinstance(lval, IndexExpr):
        if explicit_type:
            self.fail("Unexpected type declaration", lval)
        lval.accept(self)
    elif isinstance(lval, TupleExpr):
        self.analyze_tuple_or_list_lvalue(lval, explicit_type)
    elif isinstance(lval, StarExpr):
        if nested:
            self.analyze_lvalue(lval.expr, nested, explicit_type)
        else:
            self.fail("Starred assignment target must be in a list or tuple", lval)
    else:
        self.fail("Invalid assignment target", lval)

</t>
<t tx="ekr.20221004064035.1314">def analyze_name_lvalue(
    self,
    lvalue: NameExpr,
    explicit_type: bool,
    is_final: bool,
    escape_comprehensions: bool,
    has_explicit_value: bool,
) -&gt; None:
    """Analyze an lvalue that targets a name expression.

    Arguments are similar to "analyze_lvalue".
    """
    if lvalue.node:
        # This has been bound already in a previous iteration.
        return

    name = lvalue.name
    if self.is_alias_for_final_name(name):
        if is_final:
            self.fail("Cannot redefine an existing name as final", lvalue)
        else:
            self.msg.cant_assign_to_final(name, self.type is not None, lvalue)

    kind = self.current_symbol_kind()
    names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
    existing = names.get(name)

    outer = self.is_global_or_nonlocal(name)
    if kind == MDEF and isinstance(self.type, TypeInfo) and self.type.is_enum:
        # Special case: we need to be sure that `Enum` keys are unique.
        if existing is not None and not isinstance(existing.node, PlaceholderNode):
            self.fail(
                'Attempted to reuse member name "{}" in Enum definition "{}"'.format(
                    name, self.type.name
                ),
                lvalue,
            )

    if (not existing or isinstance(existing.node, PlaceholderNode)) and not outer:
        # Define new variable.
        var = self.make_name_lvalue_var(lvalue, kind, not explicit_type, has_explicit_value)
        added = self.add_symbol(name, var, lvalue, escape_comprehensions=escape_comprehensions)
        # Only bind expression if we successfully added name to symbol table.
        if added:
            lvalue.is_new_def = True
            lvalue.is_inferred_def = True
            lvalue.kind = kind
            lvalue.node = var
            if kind == GDEF:
                lvalue.fullname = var._fullname
            else:
                lvalue.fullname = lvalue.name
            if self.is_func_scope():
                if unmangle(name) == "_":
                    # Special case for assignment to local named '_': always infer 'Any'.
                    typ = AnyType(TypeOfAny.special_form)
                    self.store_declared_types(lvalue, typ)
        if is_final and self.is_final_redefinition(kind, name):
            self.fail("Cannot redefine an existing name as final", lvalue)
    else:
        self.make_name_lvalue_point_to_existing_def(lvalue, explicit_type, is_final)

</t>
<t tx="ekr.20221004064035.1315">def is_final_redefinition(self, kind: int, name: str) -&gt; bool:
    if kind == GDEF:
        return self.is_mangled_global(name) and not self.is_initial_mangled_global(name)
    elif kind == MDEF and self.type:
        return unmangle(name) + "'" in self.type.names
    return False

</t>
<t tx="ekr.20221004064035.1316">def is_alias_for_final_name(self, name: str) -&gt; bool:
    if self.is_func_scope():
        if not name.endswith("'"):
            # Not a mangled name -- can't be an alias
            return False
        name = unmangle(name)
        assert self.locals[-1] is not None, "No locals at function scope"
        existing = self.locals[-1].get(name)
        return existing is not None and is_final_node(existing.node)
    elif self.type is not None:
        orig_name = unmangle(name) + "'"
        if name == orig_name:
            return False
        existing = self.type.names.get(orig_name)
        return existing is not None and is_final_node(existing.node)
    else:
        orig_name = unmangle(name) + "'"
        if name == orig_name:
            return False
        existing = self.globals.get(orig_name)
        return existing is not None and is_final_node(existing.node)

</t>
<t tx="ekr.20221004064035.1317">def make_name_lvalue_var(
    self, lvalue: NameExpr, kind: int, inferred: bool, has_explicit_value: bool
) -&gt; Var:
    """Return a Var node for an lvalue that is a name expression."""
    name = lvalue.name
    v = Var(name)
    v.set_line(lvalue)
    v.is_inferred = inferred
    if kind == MDEF:
        assert self.type is not None
        v.info = self.type
        v.is_initialized_in_class = True
        v.allow_incompatible_override = name in ALLOW_INCOMPATIBLE_OVERRIDE
    if kind != LDEF:
        v._fullname = self.qualified_name(name)
    else:
        # fullanme should never stay None
        v._fullname = name
    v.is_ready = False  # Type not inferred yet
    v.has_explicit_value = has_explicit_value
    return v

</t>
<t tx="ekr.20221004064035.1318">def make_name_lvalue_point_to_existing_def(
    self, lval: NameExpr, explicit_type: bool, is_final: bool
) -&gt; None:
    """Update an lvalue to point to existing definition in the same scope.

    Arguments are similar to "analyze_lvalue".

    Assume that an existing name exists.
    """
    if is_final:
        # Redefining an existing name with final is always an error.
        self.fail("Cannot redefine an existing name as final", lval)
    original_def = self.lookup(lval.name, lval, suppress_errors=True)
    if original_def is None and self.type and not self.is_func_scope():
        # Workaround to allow "x, x = ..." in class body.
        original_def = self.type.get(lval.name)
    if explicit_type:
        # Don't re-bind if there is a type annotation.
        self.name_already_defined(lval.name, lval, original_def)
    else:
        # Bind to an existing name.
        if original_def:
            self.bind_name_expr(lval, original_def)
        else:
            self.name_not_defined(lval.name, lval)
        self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20221004064035.1319">def analyze_tuple_or_list_lvalue(self, lval: TupleExpr, explicit_type: bool = False) -&gt; None:
    """Analyze an lvalue or assignment target that is a list or tuple."""
    items = lval.items
    star_exprs = [item for item in items if isinstance(item, StarExpr)]

    if len(star_exprs) &gt; 1:
        self.fail("Two starred expressions in assignment", lval)
    else:
        if len(star_exprs) == 1:
            star_exprs[0].valid = True
        for i in items:
            self.analyze_lvalue(
                lval=i,
                nested=True,
                explicit_type=explicit_type,
                # Lists and tuples always have explicit values defined:
                # `a, b, c = value`
                has_explicit_value=True,
            )

</t>
<t tx="ekr.20221004064035.132">def print_help(self, file: IO[str] | None = None) -&gt; None:
    if file is None:
        file = self.stdout
    self._print_message(self.format_help(), file)

</t>
<t tx="ekr.20221004064035.1320">def analyze_member_lvalue(self, lval: MemberExpr, explicit_type: bool, is_final: bool) -&gt; None:
    """Analyze lvalue that is a member expression.

    Arguments:
        lval: The target lvalue
        explicit_type: Assignment has type annotation
        is_final: Is the target final
    """
    if lval.node:
        # This has been bound already in a previous iteration.
        return
    lval.accept(self)
    if self.is_self_member_ref(lval):
        assert self.type, "Self member outside a class"
        cur_node = self.type.names.get(lval.name)
        node = self.type.get(lval.name)
        if cur_node and is_final:
            # Overrides will be checked in type checker.
            self.fail("Cannot redefine an existing name as final", lval)
        # On first encounter with this definition, if this attribute was defined before
        # with an inferred type and it's marked with an explicit type now, give an error.
        if (
            not lval.node
            and cur_node
            and isinstance(cur_node.node, Var)
            and cur_node.node.is_inferred
            and explicit_type
        ):
            self.attribute_already_defined(lval.name, lval, cur_node)
        # If the attribute of self is not defined in superclasses, create a new Var, ...
        if (
            node is None
            or (isinstance(node.node, Var) and node.node.is_abstract_var)
            # ... also an explicit declaration on self also creates a new Var.
            # Note that `explicit_type` might has been erased for bare `Final`,
            # so we also check if `is_final` is passed.
            or (cur_node is None and (explicit_type or is_final))
        ):
            if self.type.is_protocol and node is None:
                self.fail("Protocol members cannot be defined via assignment to self", lval)
            else:
                # Implicit attribute definition in __init__.
                lval.is_new_def = True
                lval.is_inferred_def = True
                v = Var(lval.name)
                v.set_line(lval)
                v._fullname = self.qualified_name(lval.name)
                v.info = self.type
                v.is_ready = False
                v.explicit_self_type = explicit_type or is_final
                lval.def_var = v
                lval.node = v
                # TODO: should we also set lval.kind = MDEF?
                self.type.names[lval.name] = SymbolTableNode(MDEF, v, implicit=True)
    self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20221004064035.1321">def is_self_member_ref(self, memberexpr: MemberExpr) -&gt; bool:
    """Does memberexpr to refer to an attribute of self?"""
    if not isinstance(memberexpr.expr, NameExpr):
        return False
    node = memberexpr.expr.node
    return isinstance(node, Var) and node.is_self

</t>
<t tx="ekr.20221004064035.1322">def check_lvalue_validity(self, node: Expression | SymbolNode | None, ctx: Context) -&gt; None:
    if isinstance(node, TypeVarExpr):
        self.fail("Invalid assignment target", ctx)
    elif isinstance(node, TypeInfo):
        self.fail(message_registry.CANNOT_ASSIGN_TO_TYPE, ctx)

</t>
<t tx="ekr.20221004064035.1323">def store_declared_types(self, lvalue: Lvalue, typ: Type) -&gt; None:
    if isinstance(typ, StarType) and not isinstance(lvalue, StarExpr):
        self.fail("Star type only allowed for starred expressions", lvalue)
    if isinstance(lvalue, RefExpr):
        lvalue.is_inferred_def = False
        if isinstance(lvalue.node, Var):
            var = lvalue.node
            var.type = typ
            var.is_ready = True
        # If node is not a variable, we'll catch it elsewhere.
    elif isinstance(lvalue, TupleExpr):
        typ = get_proper_type(typ)
        if isinstance(typ, TupleType):
            if len(lvalue.items) != len(typ.items):
                self.fail("Incompatible number of tuple items", lvalue)
                return
            for item, itemtype in zip(lvalue.items, typ.items):
                self.store_declared_types(item, itemtype)
        else:
            self.fail("Tuple type expected for multiple variables", lvalue)
    elif isinstance(lvalue, StarExpr):
        # Historical behavior for the old parser
        if isinstance(typ, StarType):
            self.store_declared_types(lvalue.expr, typ.type)
        else:
            self.store_declared_types(lvalue.expr, typ)
    else:
        # This has been flagged elsewhere as an error, so just ignore here.
        pass

</t>
<t tx="ekr.20221004064035.1324">def process_typevar_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Check if s declares a TypeVar; it yes, store it in symbol table.

    Return True if this looks like a type variable declaration (but maybe
    with errors), otherwise return False.
    """
    call = self.get_typevarlike_declaration(s, ("typing.TypeVar",))
    if not call:
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # Constraining types
    n_values = call.arg_kinds[1:].count(ARG_POS)
    values = self.analyze_value_types(call.args[1 : 1 + n_values])

    res = self.process_typevar_parameters(
        call.args[1 + n_values :],
        call.arg_names[1 + n_values :],
        call.arg_kinds[1 + n_values :],
        n_values,
        s,
    )
    if res is None:
        return False
    variance, upper_bound = res

    existing = self.current_symbol_table().get(name)
    if existing and not (
        isinstance(existing.node, PlaceholderNode)
        or
        # Also give error for another type variable with the same name.
        (isinstance(existing.node, TypeVarExpr) and existing.node is call.analyzed)
    ):
        self.fail(f'Cannot redefine "{name}" as a type variable', s)
        return False

    if self.options.disallow_any_unimported:
        for idx, constraint in enumerate(values, start=1):
            if has_any_from_unimported_type(constraint):
                prefix = f"Constraint {idx}"
                self.msg.unimported_type_becomes_any(prefix, constraint, s)

        if has_any_from_unimported_type(upper_bound):
            prefix = "Upper bound of type variable"
            self.msg.unimported_type_becomes_any(prefix, upper_bound, s)

    for t in values + [upper_bound]:
        check_for_explicit_any(
            t, self.options, self.is_typeshed_stub_file, self.msg, context=s
        )

    # mypyc suppresses making copies of a function to check each
    # possible type, so set the upper bound to Any to prevent that
    # from causing errors.
    if values and self.options.mypyc:
        upper_bound = AnyType(TypeOfAny.implementation_artifact)

    # Yes, it's a valid type variable definition! Add it to the symbol table.
    if not call.analyzed:
        type_var = TypeVarExpr(name, self.qualified_name(name), values, upper_bound, variance)
        type_var.line = call.line
        call.analyzed = type_var
    else:
        assert isinstance(call.analyzed, TypeVarExpr)
        call.analyzed.upper_bound = upper_bound
        call.analyzed.values = values
    if any(has_placeholder(v) for v in values) or has_placeholder(upper_bound):
        self.defer(force_progress=True)

    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20221004064035.1325">def check_typevarlike_name(self, call: CallExpr, name: str, context: Context) -&gt; bool:
    """Checks that the name of a TypeVar or ParamSpec matches its variable."""
    name = unmangle(name)
    assert isinstance(call.callee, RefExpr)
    typevarlike_type = (
        call.callee.name if isinstance(call.callee, NameExpr) else call.callee.fullname
    )
    if len(call.args) &lt; 1:
        self.fail(f"Too few arguments for {typevarlike_type}()", context)
        return False
    if not isinstance(call.args[0], StrExpr) or not call.arg_kinds[0] == ARG_POS:
        self.fail(f"{typevarlike_type}() expects a string literal as first argument", context)
        return False
    elif call.args[0].value != name:
        msg = 'String argument 1 "{}" to {}(...) does not match variable name "{}"'
        self.fail(msg.format(call.args[0].value, typevarlike_type, name), context)
        return False
    return True

</t>
<t tx="ekr.20221004064035.1326">def get_typevarlike_declaration(
    self, s: AssignmentStmt, typevarlike_types: tuple[str, ...]
) -&gt; CallExpr | None:
    """Returns the call expression if `s` is a declaration of `typevarlike_type`
    (TypeVar or ParamSpec), or None otherwise.
    """
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], NameExpr):
        return None
    if not isinstance(s.rvalue, CallExpr):
        return None
    call = s.rvalue
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None
    if callee.fullname not in typevarlike_types:
        return None
    return call

</t>
<t tx="ekr.20221004064035.1327">def process_typevar_parameters(
    self,
    args: list[Expression],
    names: list[str | None],
    kinds: list[ArgKind],
    num_values: int,
    context: Context,
) -&gt; tuple[int, Type] | None:
    has_values = num_values &gt; 0
    covariant = False
    contravariant = False
    upper_bound: Type = self.object_type()
    for param_value, param_name, param_kind in zip(args, names, kinds):
        if not param_kind.is_named():
            self.fail(message_registry.TYPEVAR_UNEXPECTED_ARGUMENT, context)
            return None
        if param_name == "covariant":
            if isinstance(param_value, NameExpr) and param_value.name in ("True", "False"):
                covariant = param_value.name == "True"
            else:
                self.fail(message_registry.TYPEVAR_VARIANCE_DEF.format("covariant"), context)
                return None
        elif param_name == "contravariant":
            if isinstance(param_value, NameExpr) and param_value.name in ("True", "False"):
                contravariant = param_value.name == "True"
            else:
                self.fail(
                    message_registry.TYPEVAR_VARIANCE_DEF.format("contravariant"), context
                )
                return None
        elif param_name == "bound":
            if has_values:
                self.fail("TypeVar cannot have both values and an upper bound", context)
                return None
            try:
                # We want to use our custom error message below, so we suppress
                # the default error message for invalid types here.
                analyzed = self.expr_to_analyzed_type(
                    param_value, allow_placeholder=True, report_invalid_types=False
                )
                if analyzed is None:
                    # Type variables are special: we need to place them in the symbol table
                    # soon, even if upper bound is not ready yet. Otherwise avoiding
                    # a "deadlock" in this common pattern would be tricky:
                    #     T = TypeVar('T', bound=Custom[Any])
                    #     class Custom(Generic[T]):
                    #         ...
                    analyzed = PlaceholderType(None, [], context.line)
                upper_bound = get_proper_type(analyzed)
                if isinstance(upper_bound, AnyType) and upper_bound.is_from_error:
                    self.fail(message_registry.TYPEVAR_BOUND_MUST_BE_TYPE, param_value)
                    # Note: we do not return 'None' here -- we want to continue
                    # using the AnyType as the upper bound.
            except TypeTranslationError:
                self.fail(message_registry.TYPEVAR_BOUND_MUST_BE_TYPE, param_value)
                return None
        elif param_name == "values":
            # Probably using obsolete syntax with values=(...). Explain the current syntax.
            self.fail('TypeVar "values" argument not supported', context)
            self.fail(
                "Use TypeVar('T', t, ...) instead of TypeVar('T', values=(t, ...))", context
            )
            return None
        else:
            self.fail(
                f'{message_registry.TYPEVAR_UNEXPECTED_ARGUMENT}: "{param_name}"', context
            )
            return None

    if covariant and contravariant:
        self.fail("TypeVar cannot be both covariant and contravariant", context)
        return None
    elif num_values == 1:
        self.fail("TypeVar cannot have only a single constraint", context)
        return None
    elif covariant:
        variance = COVARIANT
    elif contravariant:
        variance = CONTRAVARIANT
    else:
        variance = INVARIANT
    return variance, upper_bound

</t>
<t tx="ekr.20221004064035.1328">def extract_typevarlike_name(self, s: AssignmentStmt, call: CallExpr) -&gt; str | None:
    if not call:
        return None

    lvalue = s.lvalues[0]
    assert isinstance(lvalue, NameExpr)
    if s.type:
        self.fail("Cannot declare the type of a TypeVar or similar construct", s)
        return None

    if not self.check_typevarlike_name(call, lvalue.name, s):
        return None
    return lvalue.name

</t>
<t tx="ekr.20221004064035.1329">def process_paramspec_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Checks if s declares a ParamSpec; if yes, store it in symbol table.

    Return True if this looks like a ParamSpec (maybe with errors), otherwise return False.

    In the future, ParamSpec may accept bounds and variance arguments, in which
    case more aggressive sharing of code with process_typevar_declaration should be pursued.
    """
    call = self.get_typevarlike_declaration(
        s, ("typing_extensions.ParamSpec", "typing.ParamSpec")
    )
    if not call:
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # ParamSpec is different from a regular TypeVar:
    # arguments are not semantically valid. But, allowed in runtime.
    # So, we need to warn users about possible invalid usage.
    if len(call.args) &gt; 1:
        self.fail("Only the first argument to ParamSpec has defined semantics", s)

    # PEP 612 reserves the right to define bound, covariant and contravariant arguments to
    # ParamSpec in a later PEP. If and when that happens, we should do something
    # on the lines of process_typevar_parameters

    if not call.analyzed:
        paramspec_var = ParamSpecExpr(
            name, self.qualified_name(name), self.object_type(), INVARIANT
        )
        paramspec_var.line = call.line
        call.analyzed = paramspec_var
    else:
        assert isinstance(call.analyzed, ParamSpecExpr)
    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20221004064035.133">def _print_message(self, message: str, file: IO[str] | None = None) -&gt; None:
    if message:
        if file is None:
            file = self.stderr
        file.write(message)

</t>
<t tx="ekr.20221004064035.1330">def process_typevartuple_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Checks if s declares a TypeVarTuple; if yes, store it in symbol table.

    Return True if this looks like a TypeVarTuple (maybe with errors), otherwise return False.
    """
    call = self.get_typevarlike_declaration(
        s, ("typing_extensions.TypeVarTuple", "typing.TypeVarTuple")
    )
    if not call:
        return False

    if len(call.args) &gt; 1:
        self.fail("Only the first argument to TypeVarTuple has defined semantics", s)

    if not self.options.enable_incomplete_features:
        self.fail('"TypeVarTuple" is not supported by mypy yet', s)
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # PEP 646 does not specify the behavior of variance, constraints, or bounds.
    if not call.analyzed:
        typevartuple_var = TypeVarTupleExpr(
            name, self.qualified_name(name), self.object_type(), INVARIANT
        )
        typevartuple_var.line = call.line
        call.analyzed = typevartuple_var
    else:
        assert isinstance(call.analyzed, TypeVarTupleExpr)
    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20221004064035.1331">def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    if self.is_func_scope() and not self.type and "@" not in name:
        name += "@" + str(line)
    class_def = ClassDef(name, Block([]))
    if self.is_func_scope() and not self.type:
        # Full names of generated classes should always be prefixed with the module names
        # even if they are nested in a function, since these classes will be (de-)serialized.
        # (Note that the caller should append @line to the name to avoid collisions.)
        # TODO: clean this up, see #6422.
        class_def.fullname = self.cur_mod_id + "." + self.qualified_name(name)
    else:
        class_def.fullname = self.qualified_name(name)

    info = TypeInfo(SymbolTable(), class_def, self.cur_mod_id)
    class_def.info = info
    mro = basetype_or_fallback.type.mro
    if not mro:
        # Probably an error, we should not crash so generate something meaningful.
        mro = [basetype_or_fallback.type, self.object_type().type]
    info.mro = [info] + mro
    info.bases = [basetype_or_fallback]
    return info

</t>
<t tx="ekr.20221004064035.1332">def analyze_value_types(self, items: list[Expression]) -&gt; list[Type]:
    """Analyze types from values expressions in type variable definition."""
    result: list[Type] = []
    for node in items:
        try:
            analyzed = self.anal_type(
                self.expr_to_unanalyzed_type(node), allow_placeholder=True
            )
            if analyzed is None:
                # Type variables are special: we need to place them in the symbol table
                # soon, even if some value is not ready yet, see process_typevar_parameters()
                # for an example.
                analyzed = PlaceholderType(None, [], node.line)
            result.append(analyzed)
        except TypeTranslationError:
            self.fail("Type expected", node)
            result.append(AnyType(TypeOfAny.from_error))
    return result

</t>
<t tx="ekr.20221004064035.1333">def check_classvar(self, s: AssignmentStmt) -&gt; None:
    """Check if assignment defines a class variable."""
    lvalue = s.lvalues[0]
    if len(s.lvalues) != 1 or not isinstance(lvalue, RefExpr):
        return
    if not s.type or not self.is_classvar(s.type):
        return
    if self.is_class_scope() and isinstance(lvalue, NameExpr):
        node = lvalue.node
        if isinstance(node, Var):
            node.is_classvar = True
        analyzed = self.anal_type(s.type)
        assert self.type is not None
        if analyzed is not None and set(get_type_vars(analyzed)) &amp; set(
            self.type.defn.type_vars
        ):
            # This means that we have a type var defined inside of a ClassVar.
            # This is not allowed by PEP526.
            # See https://github.com/python/mypy/issues/11538

            self.fail(message_registry.CLASS_VAR_WITH_TYPEVARS, s)
    elif not isinstance(lvalue, MemberExpr) or self.is_self_member_ref(lvalue):
        # In case of member access, report error only when assigning to self
        # Other kinds of member assignments should be already reported
        self.fail_invalid_classvar(lvalue)

</t>
<t tx="ekr.20221004064035.1334">def is_classvar(self, typ: Type) -&gt; bool:
    if not isinstance(typ, UnboundType):
        return False
    sym = self.lookup_qualified(typ.name, typ)
    if not sym or not sym.node:
        return False
    return sym.node.fullname == "typing.ClassVar"

</t>
<t tx="ekr.20221004064035.1335">def is_final_type(self, typ: Type | None) -&gt; bool:
    if not isinstance(typ, UnboundType):
        return False
    sym = self.lookup_qualified(typ.name, typ)
    if not sym or not sym.node:
        return False
    return sym.node.fullname in FINAL_TYPE_NAMES

</t>
<t tx="ekr.20221004064035.1336">def fail_invalid_classvar(self, context: Context) -&gt; None:
    self.fail(message_registry.CLASS_VAR_OUTSIDE_OF_CLASS, context)

</t>
<t tx="ekr.20221004064035.1337">def process_module_assignment(
    self, lvals: list[Lvalue], rval: Expression, ctx: AssignmentStmt
) -&gt; None:
    """Propagate module references across assignments.

    Recursively handles the simple form of iterable unpacking; doesn't
    handle advanced unpacking with *rest, dictionary unpacking, etc.

    In an expression like x = y = z, z is the rval and lvals will be [x,
    y].

    """
    if isinstance(rval, (TupleExpr, ListExpr)) and all(
        isinstance(v, TupleExpr) for v in lvals
    ):
        # rval and all lvals are either list or tuple, so we are dealing
        # with unpacking assignment like `x, y = a, b`. Mypy didn't
        # understand our all(isinstance(...)), so cast them as TupleExpr
        # so mypy knows it is safe to access their .items attribute.
        seq_lvals = cast(List[TupleExpr], lvals)
        # given an assignment like:
        #     (x, y) = (m, n) = (a, b)
        # we now have:
        #     seq_lvals = [(x, y), (m, n)]
        #     seq_rval = (a, b)
        # We now zip this into:
        #     elementwise_assignments = [(a, x, m), (b, y, n)]
        # where each elementwise assignment includes one element of rval and the
        # corresponding element of each lval. Basically we unpack
        #     (x, y) = (m, n) = (a, b)
        # into elementwise assignments
        #     x = m = a
        #     y = n = b
        # and then we recursively call this method for each of those assignments.
        # If the rval and all lvals are not all of the same length, zip will just ignore
        # extra elements, so no error will be raised here; mypy will later complain
        # about the length mismatch in type-checking.
        elementwise_assignments = zip(rval.items, *[v.items for v in seq_lvals])
        for rv, *lvs in elementwise_assignments:
            self.process_module_assignment(lvs, rv, ctx)
    elif isinstance(rval, RefExpr):
        rnode = self.lookup_type_node(rval)
        if rnode and isinstance(rnode.node, MypyFile):
            for lval in lvals:
                if not isinstance(lval, RefExpr):
                    continue
                # respect explicitly annotated type
                if isinstance(lval.node, Var) and lval.node.type is not None:
                    continue

                # We can handle these assignments to locals and to self
                if isinstance(lval, NameExpr):
                    lnode = self.current_symbol_table().get(lval.name)
                elif isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                    assert self.type is not None
                    lnode = self.type.names.get(lval.name)
                else:
                    continue

                if lnode:
                    if isinstance(lnode.node, MypyFile) and lnode.node is not rnode.node:
                        assert isinstance(lval, (NameExpr, MemberExpr))
                        self.fail(
                            'Cannot assign multiple modules to name "{}" '
                            'without explicit "types.ModuleType" annotation'.format(lval.name),
                            ctx,
                        )
                    # never create module alias except on initial var definition
                    elif lval.is_inferred_def:
                        assert rnode.node is not None
                        lnode.node = rnode.node

</t>
<t tx="ekr.20221004064035.1338">def process__all__(self, s: AssignmentStmt) -&gt; None:
    """Export names if argument is a __all__ assignment."""
    if (
        len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and s.lvalues[0].name == "__all__"
        and s.lvalues[0].kind == GDEF
        and isinstance(s.rvalue, (ListExpr, TupleExpr))
    ):
        self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20221004064035.1339">def process__deletable__(self, s: AssignmentStmt) -&gt; None:
    if not self.options.mypyc:
        return
    if (
        len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and s.lvalues[0].name == "__deletable__"
        and s.lvalues[0].kind == MDEF
    ):
        rvalue = s.rvalue
        if not isinstance(rvalue, (ListExpr, TupleExpr)):
            self.fail('"__deletable__" must be initialized with a list or tuple expression', s)
            return
        items = rvalue.items
        attrs = []
        for item in items:
            if not isinstance(item, StrExpr):
                self.fail('Invalid "__deletable__" item; string literal expected', item)
            else:
                attrs.append(item.value)
        assert self.type
        self.type.deletable_attributes = attrs

</t>
<t tx="ekr.20221004064035.134"># ===============
# Exiting methods
# ===============
def exit(self, status: int = 0, message: str | None = None) -&gt; NoReturn:
    if message:
        self._print_message(message, self.stderr)
    sys.exit(status)

</t>
<t tx="ekr.20221004064035.1340">def process__slots__(self, s: AssignmentStmt) -&gt; None:
    """
    Processing ``__slots__`` if defined in type.

    See: https://docs.python.org/3/reference/datamodel.html#slots
    """
    # Later we can support `__slots__` defined as `__slots__ = other = ('a', 'b')`
    if (
        isinstance(self.type, TypeInfo)
        and len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and s.lvalues[0].name == "__slots__"
        and s.lvalues[0].kind == MDEF
    ):

        # We understand `__slots__` defined as string, tuple, list, set, and dict:
        if not isinstance(s.rvalue, (StrExpr, ListExpr, TupleExpr, SetExpr, DictExpr)):
            # For example, `__slots__` can be defined as a variable,
            # we don't support it for now.
            return

        if any(p.slots is None for p in self.type.mro[1:-1]):
            # At least one type in mro (excluding `self` and `object`)
            # does not have concrete `__slots__` defined. Ignoring.
            return

        concrete_slots = True
        rvalue: list[Expression] = []
        if isinstance(s.rvalue, StrExpr):
            rvalue.append(s.rvalue)
        elif isinstance(s.rvalue, (ListExpr, TupleExpr, SetExpr)):
            rvalue.extend(s.rvalue.items)
        else:
            # We have a special treatment of `dict` with possible `{**kwargs}` usage.
            # In this case we consider all `__slots__` to be non-concrete.
            for key, _ in s.rvalue.items:
                if concrete_slots and key is not None:
                    rvalue.append(key)
                else:
                    concrete_slots = False

        slots = []
        for item in rvalue:
            # Special case for `'__dict__'` value:
            # when specified it will still allow any attribute assignment.
            if isinstance(item, StrExpr) and item.value != "__dict__":
                slots.append(item.value)
            else:
                concrete_slots = False
        if not concrete_slots:
            # Some slot items are dynamic, we don't want any false positives,
            # so, we just pretend that this type does not have any slots at all.
            return

        # We need to copy all slots from super types:
        for super_type in self.type.mro[1:-1]:
            assert super_type.slots is not None
            slots.extend(super_type.slots)
        self.type.slots = set(slots)

</t>
<t tx="ekr.20221004064035.1341">#
# Misc statements
#

</t>
<t tx="ekr.20221004064035.1342">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        return
    self.block_depth[-1] += 1
    for s in b.body:
        self.accept(s)
    self.block_depth[-1] -= 1

</t>
<t tx="ekr.20221004064035.1343">def visit_block_maybe(self, b: Block | None) -&gt; None:
    if b:
        self.visit_block(b)

</t>
<t tx="ekr.20221004064035.1344">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1345">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    self.statement = s
    if not self.is_func_scope():
        self.fail('"return" outside function', s)
    if s.expr:
        s.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1346">def visit_raise_stmt(self, s: RaiseStmt) -&gt; None:
    self.statement = s
    if s.expr:
        s.expr.accept(self)
    if s.from_expr:
        s.from_expr.accept(self)

</t>
<t tx="ekr.20221004064035.1347">def visit_assert_stmt(self, s: AssertStmt) -&gt; None:
    self.statement = s
    if s.expr:
        s.expr.accept(self)
    if s.msg:
        s.msg.accept(self)

</t>
<t tx="ekr.20221004064035.1348">def visit_operator_assignment_stmt(self, s: OperatorAssignmentStmt) -&gt; None:
    self.statement = s
    s.lvalue.accept(self)
    s.rvalue.accept(self)
    if (
        isinstance(s.lvalue, NameExpr)
        and s.lvalue.name == "__all__"
        and s.lvalue.kind == GDEF
        and isinstance(s.rvalue, (ListExpr, TupleExpr))
    ):
        self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20221004064035.1349">def visit_while_stmt(self, s: WhileStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)
    self.loop_depth += 1
    s.body.accept(self)
    self.loop_depth -= 1
    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20221004064035.135">def error(self, message: str) -&gt; NoReturn:
    """error(message: string)

    Prints a usage message incorporating the message to stderr and
    exits.

    If you override this in a subclass, it should not return -- it
    should either exit or raise an exception.
    """
    self.print_usage(self.stderr)
    args = {"prog": self.prog, "message": message}
    self.exit(2, gettext("%(prog)s: error: %(message)s\n") % args)


</t>
<t tx="ekr.20221004064035.1350">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    if s.is_async:
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

    self.statement = s
    s.expr.accept(self)

    # Bind index variables and check if they define new names.
    self.analyze_lvalue(s.index, explicit_type=s.index_type is not None)
    if s.index_type:
        if self.is_classvar(s.index_type):
            self.fail_invalid_classvar(s.index)
        allow_tuple_literal = isinstance(s.index, TupleExpr)
        analyzed = self.anal_type(s.index_type, allow_tuple_literal=allow_tuple_literal)
        if analyzed is not None:
            self.store_declared_types(s.index, analyzed)
            s.index_type = analyzed

    self.loop_depth += 1
    self.visit_block(s.body)
    self.loop_depth -= 1

    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20221004064035.1351">def visit_break_stmt(self, s: BreakStmt) -&gt; None:
    self.statement = s
    if self.loop_depth == 0:
        self.fail('"break" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20221004064035.1352">def visit_continue_stmt(self, s: ContinueStmt) -&gt; None:
    self.statement = s
    if self.loop_depth == 0:
        self.fail('"continue" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20221004064035.1353">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    self.statement = s
    infer_reachability_of_if_statement(s, self.options)
    for i in range(len(s.expr)):
        s.expr[i].accept(self)
        self.visit_block(s.body[i])
    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20221004064035.1354">def visit_try_stmt(self, s: TryStmt) -&gt; None:
    self.statement = s
    self.analyze_try_stmt(s, self)

</t>
<t tx="ekr.20221004064035.1355">def analyze_try_stmt(self, s: TryStmt, visitor: NodeVisitor[None]) -&gt; None:
    s.body.accept(visitor)
    for type, var, handler in zip(s.types, s.vars, s.handlers):
        if type:
            type.accept(visitor)
        if var:
            self.analyze_lvalue(var)
        handler.accept(visitor)
    if s.else_body:
        s.else_body.accept(visitor)
    if s.finally_body:
        s.finally_body.accept(visitor)

</t>
<t tx="ekr.20221004064035.1356">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    self.statement = s
    types: list[Type] = []

    if s.is_async:
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_WITH_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

    if s.unanalyzed_type:
        assert isinstance(s.unanalyzed_type, ProperType)
        actual_targets = [t for t in s.target if t is not None]
        if len(actual_targets) == 0:
            # We have a type for no targets
            self.fail('Invalid type comment: "with" statement has no targets', s)
        elif len(actual_targets) == 1:
            # We have one target and one type
            types = [s.unanalyzed_type]
        elif isinstance(s.unanalyzed_type, TupleType):
            # We have multiple targets and multiple types
            if len(actual_targets) == len(s.unanalyzed_type.items):
                types = s.unanalyzed_type.items.copy()
            else:
                # But it's the wrong number of items
                self.fail('Incompatible number of types for "with" targets', s)
        else:
            # We have multiple targets and one type
            self.fail('Multiple types expected for multiple "with" targets', s)

    new_types: list[Type] = []
    for e, n in zip(s.expr, s.target):
        e.accept(self)
        if n:
            self.analyze_lvalue(n, explicit_type=s.unanalyzed_type is not None)

            # Since we have a target, pop the next type from types
            if types:
                t = types.pop(0)
                if self.is_classvar(t):
                    self.fail_invalid_classvar(n)
                allow_tuple_literal = isinstance(n, TupleExpr)
                analyzed = self.anal_type(t, allow_tuple_literal=allow_tuple_literal)
                if analyzed is not None:
                    # TODO: Deal with this better
                    new_types.append(analyzed)
                    self.store_declared_types(n, analyzed)

    s.analyzed_types = new_types

    self.visit_block(s.body)

</t>
<t tx="ekr.20221004064035.1357">def visit_del_stmt(self, s: DelStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)
    if not self.is_valid_del_target(s.expr):
        self.fail("Invalid delete target", s)

</t>
<t tx="ekr.20221004064035.1358">def is_valid_del_target(self, s: Expression) -&gt; bool:
    if isinstance(s, (IndexExpr, NameExpr, MemberExpr)):
        return True
    elif isinstance(s, (TupleExpr, ListExpr)):
        return all(self.is_valid_del_target(item) for item in s.items)
    else:
        return False

</t>
<t tx="ekr.20221004064035.1359">def visit_global_decl(self, g: GlobalDecl) -&gt; None:
    self.statement = g
    for name in g.names:
        if name in self.nonlocal_decls[-1]:
            self.fail(f'Name "{name}" is nonlocal and global', g)
        self.global_decls[-1].add(name)

</t>
<t tx="ekr.20221004064035.136">class CapturableVersionAction(argparse.Action):

    """Supplement CapturableArgumentParser to handle --version.

    This is nearly identical to argparse._VersionAction except,
    like CapturableArgumentParser, it allows output to be captured.

    Another notable difference is that version is mandatory.
    This allows removing a line in __call__ that falls back to parser.version
    (which does not appear to exist).
    """

    @others
</t>
<t tx="ekr.20221004064035.1360">def visit_nonlocal_decl(self, d: NonlocalDecl) -&gt; None:
    self.statement = d
    if self.is_module_scope():
        self.fail("nonlocal declaration not allowed at module level", d)
    else:
        for name in d.names:
            for table in reversed(self.locals[:-1]):
                if table is not None and name in table:
                    break
            else:
                self.fail(f'No binding for nonlocal "{name}" found', d)

            if self.locals[-1] is not None and name in self.locals[-1]:
                self.fail(
                    'Name "{}" is already defined in local '
                    "scope before nonlocal declaration".format(name),
                    d,
                )

            if name in self.global_decls[-1]:
                self.fail(f'Name "{name}" is nonlocal and global', d)
            self.nonlocal_decls[-1].add(name)

</t>
<t tx="ekr.20221004064035.1361">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    self.statement = s
    infer_reachability_of_match_statement(s, self.options)
    s.subject.accept(self)
    for i in range(len(s.patterns)):
        s.patterns[i].accept(self)
        guard = s.guards[i]
        if guard is not None:
            guard.accept(self)
        self.visit_block(s.bodies[i])

</t>
<t tx="ekr.20221004064035.1362">#
# Expressions
#

</t>
<t tx="ekr.20221004064035.1363">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    n = self.lookup(expr.name, expr)
    if n:
        self.bind_name_expr(expr, n)

</t>
<t tx="ekr.20221004064035.1364">def bind_name_expr(self, expr: NameExpr, sym: SymbolTableNode) -&gt; None:
    """Bind name expression to a symbol table node."""
    if isinstance(sym.node, TypeVarExpr) and self.tvar_scope.get_binding(sym):
        self.fail(
            '"{}" is a type variable and only valid in type ' "context".format(expr.name), expr
        )
    elif isinstance(sym.node, PlaceholderNode):
        self.process_placeholder(expr.name, "name", expr)
    else:
        expr.kind = sym.kind
        expr.node = sym.node
        expr.fullname = sym.fullname

</t>
<t tx="ekr.20221004064035.1365">def visit_super_expr(self, expr: SuperExpr) -&gt; None:
    if not self.type and not expr.call.args:
        self.fail('"super" used outside class', expr)
        return
    expr.info = self.type
    for arg in expr.call.args:
        arg.accept(self)

</t>
<t tx="ekr.20221004064035.1366">def visit_tuple_expr(self, expr: TupleExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20221004064035.1367">def visit_list_expr(self, expr: ListExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20221004064035.1368">def visit_set_expr(self, expr: SetExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20221004064035.1369">def visit_dict_expr(self, expr: DictExpr) -&gt; None:
    for key, value in expr.items:
        if key is not None:
            key.accept(self)
        value.accept(self)

</t>
<t tx="ekr.20221004064035.137">def __init__(
    self,
    option_strings: Sequence[str],
    version: str,
    dest: str = argparse.SUPPRESS,
    default: str = argparse.SUPPRESS,
    help: str = "show program's version number and exit",
    stdout: IO[str] | None = None,
):
    super().__init__(
        option_strings=option_strings, dest=dest, default=default, nargs=0, help=help
    )
    self.version = version
    self.stdout = stdout or sys.stdout

</t>
<t tx="ekr.20221004064035.1370">def visit_star_expr(self, expr: StarExpr) -&gt; None:
    if not expr.valid:
        # XXX TODO Change this error message
        self.fail("Can use starred expression only as assignment target", expr)
    else:
        expr.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1371">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"yield from" outside function', e, serious=True, blocker=True)
    elif self.is_comprehension_stack[-1]:
        self.fail(
            '"yield from" inside comprehension or generator expression',
            e,
            serious=True,
            blocker=True,
        )
    elif self.function_stack[-1].is_coroutine:
        self.fail('"yield from" in async function', e, serious=True, blocker=True)
    else:
        self.function_stack[-1].is_generator = True
    if e.expr:
        e.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1372">def visit_call_expr(self, expr: CallExpr) -&gt; None:
    """Analyze a call expression.

    Some call expressions are recognized as special forms, including
    cast(...).
    """
    expr.callee.accept(self)
    if refers_to_fullname(expr.callee, "typing.cast"):
        # Special form cast(...).
        if not self.check_fixed_args(expr, 2, "cast"):
            return
        # Translate first argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[0])
        except TypeTranslationError:
            self.fail("Cast target is not a type", expr)
            return
        # Piggyback CastExpr object to the CallExpr object; it takes
        # precedence over the CallExpr semantics.
        expr.analyzed = CastExpr(expr.args[1], target)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, ASSERT_TYPE_NAMES):
        if not self.check_fixed_args(expr, 2, "assert_type"):
            return
        # Translate second argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[1])
        except TypeTranslationError:
            self.fail("assert_type() type is not a type", expr)
            return
        expr.analyzed = AssertTypeExpr(expr.args[0], target)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, REVEAL_TYPE_NAMES):
        if not self.check_fixed_args(expr, 1, "reveal_type"):
            return
        expr.analyzed = RevealExpr(kind=REVEAL_TYPE, expr=expr.args[0])
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, "builtins.reveal_locals"):
        # Store the local variable names into the RevealExpr for use in the
        # type checking pass
        local_nodes: list[Var] = []
        if self.is_module_scope():
            # try to determine just the variable declarations in module scope
            # self.globals.values() contains SymbolTableNode's
            # Each SymbolTableNode has an attribute node that is nodes.Var
            # look for variable nodes that marked as is_inferred
            # Each symboltable node has a Var node as .node
            local_nodes = [
                n.node
                for name, n in self.globals.items()
                if getattr(n.node, "is_inferred", False) and isinstance(n.node, Var)
            ]
        elif self.is_class_scope():
            # type = None  # type: Optional[TypeInfo]
            if self.type is not None:
                local_nodes = [
                    st.node for st in self.type.names.values() if isinstance(st.node, Var)
                ]
        elif self.is_func_scope():
            # locals = None  # type: List[Optional[SymbolTable]]
            if self.locals is not None:
                symbol_table = self.locals[-1]
                if symbol_table is not None:
                    local_nodes = [
                        st.node for st in symbol_table.values() if isinstance(st.node, Var)
                    ]
        expr.analyzed = RevealExpr(kind=REVEAL_LOCALS, local_nodes=local_nodes)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, "typing.Any"):
        # Special form Any(...) no longer supported.
        self.fail("Any(...) is no longer supported. Use cast(Any, ...) instead", expr)
    elif refers_to_fullname(expr.callee, "typing._promote"):
        # Special form _promote(...).
        if not self.check_fixed_args(expr, 1, "_promote"):
            return
        # Translate first argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[0])
        except TypeTranslationError:
            self.fail("Argument 1 to _promote is not a type", expr)
            return
        expr.analyzed = PromoteExpr(target)
        expr.analyzed.line = expr.line
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, "builtins.dict"):
        expr.analyzed = self.translate_dict_call(expr)
    elif refers_to_fullname(expr.callee, "builtins.divmod"):
        if not self.check_fixed_args(expr, 2, "divmod"):
            return
        expr.analyzed = OpExpr("divmod", expr.args[0], expr.args[1])
        expr.analyzed.line = expr.line
        expr.analyzed.accept(self)
    else:
        # Normal call expression.
        for a in expr.args:
            a.accept(self)

        if (
            isinstance(expr.callee, MemberExpr)
            and isinstance(expr.callee.expr, NameExpr)
            and expr.callee.expr.name == "__all__"
            and expr.callee.expr.kind == GDEF
            and expr.callee.name in ("append", "extend")
        ):
            if expr.callee.name == "append" and expr.args:
                self.add_exports(expr.args[0])
            elif (
                expr.callee.name == "extend"
                and expr.args
                and isinstance(expr.args[0], (ListExpr, TupleExpr))
            ):
                self.add_exports(expr.args[0].items)

</t>
<t tx="ekr.20221004064035.1373">def translate_dict_call(self, call: CallExpr) -&gt; DictExpr | None:
    """Translate 'dict(x=y, ...)' to {'x': y, ...} and 'dict()' to {}.

    For other variants of dict(...), return None.
    """
    if not all(kind == ARG_NAMED for kind in call.arg_kinds):
        # Must still accept those args.
        for a in call.args:
            a.accept(self)
        return None
    expr = DictExpr(
        [
            (StrExpr(cast(str, key)), value)  # since they are all ARG_NAMED
            for key, value in zip(call.arg_names, call.args)
        ]
    )
    expr.set_line(call)
    expr.accept(self)
    return expr

</t>
<t tx="ekr.20221004064035.1374">def check_fixed_args(self, expr: CallExpr, numargs: int, name: str) -&gt; bool:
    """Verify that expr has specified number of positional args.

    Return True if the arguments are valid.
    """
    s = "s"
    if numargs == 1:
        s = ""
    if len(expr.args) != numargs:
        self.fail('"%s" expects %d argument%s' % (name, numargs, s), expr)
        return False
    if expr.arg_kinds != [ARG_POS] * numargs:
        self.fail(f'"{name}" must be called with {numargs} positional argument{s}', expr)
        return False
    return True

</t>
<t tx="ekr.20221004064035.1375">def visit_member_expr(self, expr: MemberExpr) -&gt; None:
    base = expr.expr
    base.accept(self)
    if isinstance(base, RefExpr) and isinstance(base.node, MypyFile):
        # Handle module attribute.
        sym = self.get_module_symbol(base.node, expr.name)
        if sym:
            if isinstance(sym.node, PlaceholderNode):
                self.process_placeholder(expr.name, "attribute", expr)
                return
            expr.kind = sym.kind
            expr.fullname = sym.fullname
            expr.node = sym.node
    elif isinstance(base, RefExpr):
        # This branch handles the case C.bar (or cls.bar or self.bar inside
        # a classmethod/method), where C is a class and bar is a type
        # definition or a module resulting from `import bar` (or a module
        # assignment) inside class C. We look up bar in the class' TypeInfo
        # namespace.  This is done only when bar is a module or a type;
        # other things (e.g. methods) are handled by other code in
        # checkmember.
        type_info = None
        if isinstance(base.node, TypeInfo):
            # C.bar where C is a class
            type_info = base.node
        elif isinstance(base.node, Var) and self.type and self.function_stack:
            # check for self.bar or cls.bar in method/classmethod
            func_def = self.function_stack[-1]
            if not func_def.is_static and isinstance(func_def.type, CallableType):
                formal_arg = func_def.type.argument_by_name(base.node.name)
                if formal_arg and formal_arg.pos == 0:
                    type_info = self.type
        elif isinstance(base.node, TypeAlias) and base.node.no_args:
            assert isinstance(base.node.target, ProperType)
            if isinstance(base.node.target, Instance):
                type_info = base.node.target.type

        if type_info:
            n = type_info.names.get(expr.name)
            if n is not None and isinstance(n.node, (MypyFile, TypeInfo, TypeAlias)):
                if not n:
                    return
                expr.kind = n.kind
                expr.fullname = n.fullname
                expr.node = n.node

</t>
<t tx="ekr.20221004064035.1376">def visit_op_expr(self, expr: OpExpr) -&gt; None:
    expr.left.accept(self)

    if expr.op in ("and", "or"):
        inferred = infer_condition_value(expr.left, self.options)
        if (inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "and") or (
            inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "or"
        ):
            expr.right_unreachable = True
            return
        elif (inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == "and") or (
            inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == "or"
        ):
            expr.right_always = True

    expr.right.accept(self)

</t>
<t tx="ekr.20221004064035.1377">def visit_comparison_expr(self, expr: ComparisonExpr) -&gt; None:
    for operand in expr.operands:
        operand.accept(self)

</t>
<t tx="ekr.20221004064035.1378">def visit_unary_expr(self, expr: UnaryExpr) -&gt; None:
    expr.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1379">def visit_index_expr(self, expr: IndexExpr) -&gt; None:
    base = expr.base
    base.accept(self)
    if (
        isinstance(base, RefExpr)
        and isinstance(base.node, TypeInfo)
        and not base.node.is_generic()
    ):
        expr.index.accept(self)
    elif (
        isinstance(base, RefExpr) and isinstance(base.node, TypeAlias)
    ) or refers_to_class_or_function(base):
        # We need to do full processing on every iteration, since some type
        # arguments may contain placeholder types.
        self.analyze_type_application(expr)
    else:
        expr.index.accept(self)

</t>
<t tx="ekr.20221004064035.138">def __call__(
    self,
    parser: argparse.ArgumentParser,
    namespace: argparse.Namespace,
    values: str | Sequence[Any] | None,
    option_string: str | None = None,
) -&gt; NoReturn:
    formatter = parser._get_formatter()
    formatter.add_text(self.version)
    parser._print_message(formatter.format_help(), self.stdout)
    parser.exit()


</t>
<t tx="ekr.20221004064035.1380">def analyze_type_application(self, expr: IndexExpr) -&gt; None:
    """Analyze special form -- type application (either direct or via type aliasing)."""
    types = self.analyze_type_application_args(expr)
    if types is None:
        return
    base = expr.base
    expr.analyzed = TypeApplication(base, types)
    expr.analyzed.line = expr.line
    expr.analyzed.column = expr.column
    # Types list, dict, set are not subscriptable, prohibit this if
    # subscripted either via type alias...
    if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
        alias = base.node
        target = get_proper_type(alias.target)
        if isinstance(target, Instance):
            name = target.type.fullname
            if (
                alias.no_args
                and name  # this avoids bogus errors for already reported aliases
                in get_nongen_builtins(self.options.python_version)
                and not self.is_stub_file
                and not alias.normalized
            ):
                self.fail(no_subscript_builtin_alias(name, propose_alt=False), expr)
    # ...or directly.
    else:
        n = self.lookup_type_node(base)
        if (
            n
            and n.fullname in get_nongen_builtins(self.options.python_version)
            and not self.is_stub_file
        ):
            self.fail(no_subscript_builtin_alias(n.fullname, propose_alt=False), expr)

</t>
<t tx="ekr.20221004064035.1381">def analyze_type_application_args(self, expr: IndexExpr) -&gt; list[Type] | None:
    """Analyze type arguments (index) in a type application.

    Return None if anything was incomplete.
    """
    index = expr.index
    tag = self.track_incomplete_refs()
    self.analyze_type_expr(index)
    if self.found_incomplete_ref(tag):
        return None
    if self.basic_type_applications:
        # Postpone the rest until we have more information (for r.h.s. of an assignment)
        return None
    types: list[Type] = []
    if isinstance(index, TupleExpr):
        items = index.items
        is_tuple = isinstance(expr.base, RefExpr) and expr.base.fullname == "builtins.tuple"
        if is_tuple and len(items) == 2 and isinstance(items[-1], EllipsisExpr):
            items = items[:-1]
    else:
        items = [index]

    # whether param spec literals be allowed here
    # TODO: should this be computed once and passed in?
    #   or is there a better way to do this?
    base = expr.base
    if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
        alias = base.node
        target = get_proper_type(alias.target)
        if isinstance(target, Instance):
            has_param_spec = target.type.has_param_spec_type
            num_args = len(target.type.type_vars)
        else:
            has_param_spec = False
            num_args = -1
    elif isinstance(base, NameExpr) and isinstance(base.node, TypeInfo):
        has_param_spec = base.node.has_param_spec_type
        num_args = len(base.node.type_vars)
    else:
        has_param_spec = False
        num_args = -1

    for item in items:
        try:
            typearg = self.expr_to_unanalyzed_type(item)
        except TypeTranslationError:
            self.fail("Type expected within [...]", expr)
            return None
        # We always allow unbound type variables in IndexExpr, since we
        # may be analysing a type alias definition rvalue. The error will be
        # reported elsewhere if it is not the case.
        analyzed = self.anal_type(
            typearg,
            allow_unbound_tvars=True,
            allow_placeholder=True,
            allow_param_spec_literals=has_param_spec,
        )
        if analyzed is None:
            return None
        types.append(analyzed)

    if has_param_spec and num_args == 1 and len(types) &gt; 0:
        first_arg = get_proper_type(types[0])
        if not (
            len(types) == 1
            and (
                isinstance(first_arg, Parameters)
                or isinstance(first_arg, ParamSpecType)
                or isinstance(first_arg, AnyType)
            )
        ):
            types = [Parameters(types, [ARG_POS] * len(types), [None] * len(types))]

    return types

</t>
<t tx="ekr.20221004064035.1382">def visit_slice_expr(self, expr: SliceExpr) -&gt; None:
    if expr.begin_index:
        expr.begin_index.accept(self)
    if expr.end_index:
        expr.end_index.accept(self)
    if expr.stride:
        expr.stride.accept(self)

</t>
<t tx="ekr.20221004064035.1383">def visit_cast_expr(self, expr: CastExpr) -&gt; None:
    expr.expr.accept(self)
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20221004064035.1384">def visit_assert_type_expr(self, expr: AssertTypeExpr) -&gt; None:
    expr.expr.accept(self)
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20221004064035.1385">def visit_reveal_expr(self, expr: RevealExpr) -&gt; None:
    if expr.kind == REVEAL_TYPE:
        if expr.expr is not None:
            expr.expr.accept(self)
    else:
        # Reveal locals doesn't have an inner expression, there's no
        # need to traverse inside it
        pass

</t>
<t tx="ekr.20221004064035.1386">def visit_type_application(self, expr: TypeApplication) -&gt; None:
    expr.expr.accept(self)
    for i in range(len(expr.types)):
        analyzed = self.anal_type(expr.types[i])
        if analyzed is not None:
            expr.types[i] = analyzed

</t>
<t tx="ekr.20221004064035.1387">def visit_list_comprehension(self, expr: ListComprehension) -&gt; None:
    if any(expr.generator.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    expr.generator.accept(self)

</t>
<t tx="ekr.20221004064035.1388">def visit_set_comprehension(self, expr: SetComprehension) -&gt; None:
    if any(expr.generator.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    expr.generator.accept(self)

</t>
<t tx="ekr.20221004064035.1389">def visit_dictionary_comprehension(self, expr: DictionaryComprehension) -&gt; None:
    if any(expr.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    with self.enter(expr):
        self.analyze_comp_for(expr)
        expr.key.accept(self)
        expr.value.accept(self)
    self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20221004064035.139">def process_options(
    args: list[str],
    stdout: TextIO | None = None,
    stderr: TextIO | None = None,
    require_targets: bool = True,
    server_options: bool = False,
    fscache: FileSystemCache | None = None,
    program: str = "mypy",
    header: str = HEADER,
) -&gt; tuple[list[BuildSource], Options]:
    """Parse command line arguments.

    If a FileSystemCache is passed in, and package_root options are given,
    call fscache.set_package_root() to set the cache's package root.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr

    parser = CapturableArgumentParser(
        prog=program,
        usage=header,
        description=DESCRIPTION,
        epilog=FOOTER,
        fromfile_prefix_chars="@",
        formatter_class=AugmentedHelpFormatter,
        add_help=False,
        stdout=stdout,
        stderr=stderr,
    )

    strict_flag_names: list[str] = []
    strict_flag_assignments: list[tuple[str, bool]] = []

    @others
    # Parse config file first, so command line can override.
    parse_config_file(options, set_strict_flags, config_file, stdout, stderr)

    # Set strict flags before parsing (if strict mode enabled), so other command
    # line options can override.
    if getattr(dummy, "special-opts:strict"):
        set_strict_flags()

    # Override cache_dir if provided in the environment
    environ_cache_dir = os.getenv("MYPY_CACHE_DIR", "")
    if environ_cache_dir.strip():
        options.cache_dir = environ_cache_dir
    options.cache_dir = os.path.expanduser(options.cache_dir)

    # Parse command line for real, using a split namespace.
    special_opts = argparse.Namespace()
    parser.parse_args(args, SplitNamespace(options, special_opts, "special-opts:"))

    # The python_version is either the default, which can be overridden via a config file,
    # or stored in special_opts and is passed via the command line.
    options.python_version = special_opts.python_version or options.python_version
    if options.python_version &lt; (3,):
        parser.error(
            "Mypy no longer supports checking Python 2 code. "
            "Consider pinning to mypy&lt;0.980 if you need to check Python 2 code."
        )
    try:
        infer_python_executable(options, special_opts)
    except PythonExecutableInferenceError as e:
        parser.error(str(e))

    if special_opts.no_executable or options.no_site_packages:
        options.python_executable = None

    # Paths listed in the config file will be ignored if any paths, modules or packages
    # are passed on the command line.
    if options.files and not (special_opts.files or special_opts.packages or special_opts.modules):
        special_opts.files = options.files

    # Check for invalid argument combinations.
    if require_targets:
        code_methods = sum(
            bool(c)
            for c in [
                special_opts.modules + special_opts.packages,
                special_opts.command,
                special_opts.files,
            ]
        )
        if code_methods == 0 and not options.install_types:
            parser.error("Missing target module, package, files, or command.")
        elif code_methods &gt; 1:
            parser.error("May only specify one of: module/package, files, or command.")
    if options.explicit_package_bases and not options.namespace_packages:
        parser.error(
            "Can only use --explicit-package-bases with --namespace-packages, since otherwise "
            "examining __init__.py's is sufficient to determine module names for files"
        )

    # Check for overlapping `--always-true` and `--always-false` flags.
    overlap = set(options.always_true) &amp; set(options.always_false)
    if overlap:
        parser.error(
            "You can't make a variable always true and always false (%s)"
            % ", ".join(sorted(overlap))
        )

    # Process `--enable-error-code` and `--disable-error-code` flags
    disabled_codes = set(options.disable_error_code)
    enabled_codes = set(options.enable_error_code)

    valid_error_codes = set(error_codes.keys())

    invalid_codes = (enabled_codes | disabled_codes) - valid_error_codes
    if invalid_codes:
        parser.error(f"Invalid error code(s): {', '.join(sorted(invalid_codes))}")

    options.disabled_error_codes |= {error_codes[code] for code in disabled_codes}
    options.enabled_error_codes |= {error_codes[code] for code in enabled_codes}

    # Enabling an error code always overrides disabling
    options.disabled_error_codes -= options.enabled_error_codes

    # Compute absolute path for custom typeshed (if present).
    if options.custom_typeshed_dir is not None:
        options.abs_custom_typeshed_dir = os.path.abspath(options.custom_typeshed_dir)

    # Set build flags.
    if special_opts.find_occurrences:
        state.find_occurrences = special_opts.find_occurrences.split(".")
        assert state.find_occurrences is not None
        if len(state.find_occurrences) &lt; 2:
            parser.error("Can only find occurrences of class members.")
        if len(state.find_occurrences) != 2:
            parser.error("Can only find occurrences of non-nested class members.")

    # Set reports.
    for flag, val in vars(special_opts).items():
        if flag.endswith("_report") and val is not None:
            report_type = flag[:-7].replace("_", "-")
            report_dir = val
            options.report_dirs[report_type] = report_dir

    # Process --package-root.
    if options.package_root:
        process_package_roots(fscache, parser, options)

    # Process --cache-map.
    if special_opts.cache_map:
        if options.sqlite_cache:
            parser.error("--cache-map is incompatible with --sqlite-cache")

        process_cache_map(parser, special_opts, options)

    # An explicitly specified cache_fine_grained implies local_partial_types
    # (because otherwise the cache is not compatible with dmypy)
    if options.cache_fine_grained:
        options.local_partial_types = True

    #  Implicitly show column numbers if error location end is shown
    if options.show_error_end:
        options.show_column_numbers = True

    # Let logical_deps imply cache_fine_grained (otherwise the former is useless).
    if options.logical_deps:
        options.cache_fine_grained = True

    # Set target.
    if special_opts.modules + special_opts.packages:
        options.build_type = BuildType.MODULE
        sys_path, _ = get_search_dirs(options.python_executable)
        search_paths = SearchPaths(
            (os.getcwd(),), tuple(mypy_path() + options.mypy_path), tuple(sys_path), ()
        )
        targets = []
        # TODO: use the same cache that the BuildManager will
        cache = FindModuleCache(search_paths, fscache, options)
        for p in special_opts.packages:
            if os.sep in p or os.altsep and os.altsep in p:
                fail(f"Package name '{p}' cannot have a slash in it.", stderr, options)
            p_targets = cache.find_modules_recursive(p)
            if not p_targets:
                fail(f"Can't find package '{p}'", stderr, options)
            targets.extend(p_targets)
        for m in special_opts.modules:
            targets.append(BuildSource(None, m, None))
        return targets, options
    elif special_opts.command:
        options.build_type = BuildType.PROGRAM_TEXT
        targets = [BuildSource(None, None, "\n".join(special_opts.command))]
        return targets, options
    else:
        try:
            targets = create_source_list(special_opts.files, options, fscache)
        # Variable named e2 instead of e to work around mypyc bug #620
        # which causes issues when using the same variable to catch
        # exceptions of different types.
        except InvalidSourceList as e2:
            fail(str(e2), stderr, options)
        return targets, options


</t>
<t tx="ekr.20221004064035.1390">def visit_generator_expr(self, expr: GeneratorExpr) -&gt; None:
    with self.enter(expr):
        self.analyze_comp_for(expr)
        expr.left_expr.accept(self)
    self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20221004064035.1391">def analyze_comp_for(self, expr: GeneratorExpr | DictionaryComprehension) -&gt; None:
    """Analyses the 'comp_for' part of comprehensions (part 1).

    That is the part after 'for' in (x for x in l if p). This analyzes
    variables and conditions which are analyzed in a local scope.
    """
    for i, (index, sequence, conditions) in enumerate(
        zip(expr.indices, expr.sequences, expr.condlists)
    ):
        if i &gt; 0:
            sequence.accept(self)
        # Bind index variables.
        self.analyze_lvalue(index)
        for cond in conditions:
            cond.accept(self)

</t>
<t tx="ekr.20221004064035.1392">def analyze_comp_for_2(self, expr: GeneratorExpr | DictionaryComprehension) -&gt; None:
    """Analyses the 'comp_for' part of comprehensions (part 2).

    That is the part after 'for' in (x for x in l if p). This analyzes
    the 'l' part which is analyzed in the surrounding scope.
    """
    expr.sequences[0].accept(self)

</t>
<t tx="ekr.20221004064035.1393">def visit_lambda_expr(self, expr: LambdaExpr) -&gt; None:
    self.analyze_arg_initializers(expr)
    self.analyze_function_body(expr)

</t>
<t tx="ekr.20221004064035.1394">def visit_conditional_expr(self, expr: ConditionalExpr) -&gt; None:
    expr.if_expr.accept(self)
    expr.cond.accept(self)
    expr.else_expr.accept(self)

</t>
<t tx="ekr.20221004064035.1395">def visit__promote_expr(self, expr: PromoteExpr) -&gt; None:
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20221004064035.1396">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"yield" outside function', e, serious=True, blocker=True)
    elif self.is_comprehension_stack[-1]:
        self.fail(
            '"yield" inside comprehension or generator expression',
            e,
            serious=True,
            blocker=True,
        )
    elif self.function_stack[-1].is_coroutine:
        if self.options.python_version &lt; (3, 6):
            self.fail('"yield" in async function', e, serious=True, blocker=True)
        else:
            self.function_stack[-1].is_generator = True
            self.function_stack[-1].is_async_generator = True
    else:
        self.function_stack[-1].is_generator = True
    if e.expr:
        e.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1397">def visit_await_expr(self, expr: AwaitExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"await" outside function', expr)
    elif not self.function_stack[-1].is_coroutine:
        self.fail('"await" outside coroutine ("async def")', expr)
    expr.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1398">#
# Patterns
#

</t>
<t tx="ekr.20221004064035.1399">def visit_as_pattern(self, p: AsPattern) -&gt; None:
    if p.pattern is not None:
        p.pattern.accept(self)
    if p.name is not None:
        self.analyze_lvalue(p.name)

</t>
<t tx="ekr.20221004064035.14">def __exit__(
    self,
    exc_ty: type[BaseException] | None = None,
    exc_val: BaseException | None = None,
    exc_tb: TracebackType | None = None,
) -&gt; None:
    if sys.platform == "win32":
        try:
            # Wait for the client to finish reading the last write before disconnecting
            if not FlushFileBuffers(self.connection):
                raise IPCException(
                    "Failed to flush NamedPipe buffer, maybe the client hung up?"
                )
        finally:
            DisconnectNamedPipe(self.connection)
    else:
        self.close()

</t>
<t tx="ekr.20221004064035.140">def add_invertible_flag(
    flag: str,
    *,
    inverse: str | None = None,
    default: bool,
    dest: str | None = None,
    help: str,
    strict_flag: bool = False,
    group: argparse._ActionsContainer | None = None,
) -&gt; None:
    if inverse is None:
        inverse = invert_flag_name(flag)
    if group is None:
        group = parser

    if help is not argparse.SUPPRESS:
        help += f" (inverse: {inverse})"

    arg = group.add_argument(
        flag, action="store_false" if default else "store_true", dest=dest, help=help
    )
    dest = arg.dest
    group.add_argument(
        inverse,
        action="store_true" if default else "store_false",
        dest=dest,
        help=argparse.SUPPRESS,
    )
    if strict_flag:
        assert dest is not None
        strict_flag_names.append(flag)
        strict_flag_assignments.append((dest, not default))

</t>
<t tx="ekr.20221004064035.1400">def visit_or_pattern(self, p: OrPattern) -&gt; None:
    for pattern in p.patterns:
        pattern.accept(self)

</t>
<t tx="ekr.20221004064035.1401">def visit_value_pattern(self, p: ValuePattern) -&gt; None:
    p.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1402">def visit_sequence_pattern(self, p: SequencePattern) -&gt; None:
    for pattern in p.patterns:
        pattern.accept(self)

</t>
<t tx="ekr.20221004064035.1403">def visit_starred_pattern(self, p: StarredPattern) -&gt; None:
    if p.capture is not None:
        self.analyze_lvalue(p.capture)

</t>
<t tx="ekr.20221004064035.1404">def visit_mapping_pattern(self, p: MappingPattern) -&gt; None:
    for key in p.keys:
        key.accept(self)
    for value in p.values:
        value.accept(self)
    if p.rest is not None:
        self.analyze_lvalue(p.rest)

</t>
<t tx="ekr.20221004064035.1405">def visit_class_pattern(self, p: ClassPattern) -&gt; None:
    p.class_ref.accept(self)
    for pos in p.positionals:
        pos.accept(self)
    for v in p.keyword_values:
        v.accept(self)

</t>
<t tx="ekr.20221004064035.1406">#
# Lookup functions
#

</t>
<t tx="ekr.20221004064035.1407">def lookup(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    """Look up an unqualified (no dots) name in all active namespaces.

    Note that the result may contain a PlaceholderNode. The caller may
    want to defer in that case.

    Generate an error if the name is not defined unless suppress_errors
    is true or the current namespace is incomplete. In the latter case
    defer.
    """
    implicit_name = False
    # 1a. Name declared using 'global x' takes precedence
    if name in self.global_decls[-1]:
        if name in self.globals:
            return self.globals[name]
        if not suppress_errors:
            self.name_not_defined(name, ctx)
        return None
    # 1b. Name declared using 'nonlocal x' takes precedence
    if name in self.nonlocal_decls[-1]:
        for table in reversed(self.locals[:-1]):
            if table is not None and name in table:
                return table[name]
        if not suppress_errors:
            self.name_not_defined(name, ctx)
        return None
    # 2. Class attributes (if within class definition)
    if self.type and not self.is_func_scope() and name in self.type.names:
        node = self.type.names[name]
        if not node.implicit:
            if self.is_active_symbol_in_class_body(node.node):
                return node
        else:
            # Defined through self.x assignment
            implicit_name = True
            implicit_node = node
    # 3. Local (function) scopes
    for table in reversed(self.locals):
        if table is not None and name in table:
            return table[name]
    # 4. Current file global scope
    if name in self.globals:
        return self.globals[name]
    # 5. Builtins
    b = self.globals.get("__builtins__", None)
    if b:
        assert isinstance(b.node, MypyFile)
        table = b.node.names
        if name in table:
            if len(name) &gt; 1 and name[0] == "_" and name[1] != "_":
                if not suppress_errors:
                    self.name_not_defined(name, ctx)
                return None
            node = table[name]
            return node
    # Give up.
    if not implicit_name and not suppress_errors:
        self.name_not_defined(name, ctx)
    else:
        if implicit_name:
            return implicit_node
    return None

</t>
<t tx="ekr.20221004064035.1408">def is_active_symbol_in_class_body(self, node: SymbolNode | None) -&gt; bool:
    """Can a symbol defined in class body accessed at current statement?

    Only allow access to class attributes textually after
    the definition, so that it's possible to fall back to the
    outer scope. Example:

        class X: ...

        class C:
            X = X  # Initializer refers to outer scope

    Nested classes are an exception, since we want to support
    arbitrary forward references in type annotations. Also, we
    allow forward references to type aliases to support recursive
    types.
    """
    # TODO: Forward reference to name imported in class body is not
    #       caught.
    if self.statement is None:
        # Assume it's fine -- don't have enough context to check
        return True
    return (
        node is None
        or self.is_textually_before_statement(node)
        or not self.is_defined_in_current_module(node.fullname)
        or isinstance(node, (TypeInfo, TypeAlias))
        or (isinstance(node, PlaceholderNode) and node.becomes_typeinfo)
    )

</t>
<t tx="ekr.20221004064035.1409">def is_textually_before_statement(self, node: SymbolNode) -&gt; bool:
    """Check if a node is defined textually before the current statement

    Note that decorated functions' line number are the same as
    the top decorator.
    """
    assert self.statement
    line_diff = self.statement.line - node.line

    # The first branch handles reference an overloaded function variant inside itself,
    # this is a corner case where mypy technically deviates from runtime name resolution,
    # but it is fine because we want an overloaded function to be treated as a single unit.
    if self.is_overloaded_item(node, self.statement):
        return False
    elif isinstance(node, Decorator) and not node.is_overload:
        return line_diff &gt; len(node.original_decorators)
    else:
        return line_diff &gt; 0

</t>
<t tx="ekr.20221004064035.141"># Unless otherwise specified, arguments will be parsed directly onto an
# Options object.  Options that require further processing should have
# their `dest` prefixed with `special-opts:`, which will cause them to be
# parsed into the separate special_opts namespace object.

# Note: we have a style guide for formatting the mypy --help text. See
# https://github.com/python/mypy/wiki/Documentation-Conventions

general_group = parser.add_argument_group(title="Optional arguments")
general_group.add_argument(
    "-h", "--help", action="help", help="Show this help message and exit"
)
general_group.add_argument(
    "-v", "--verbose", action="count", dest="verbosity", help="More verbose messages"
)

compilation_status = "no" if __file__.endswith(".py") else "yes"
general_group.add_argument(
    "-V",
    "--version",
    action=CapturableVersionAction,
    version="%(prog)s " + __version__ + f" (compiled: {compilation_status})",
    help="Show program's version number and exit",
    stdout=stdout,
)

config_group = parser.add_argument_group(
    title="Config file",
    description="Use a config file instead of command line arguments. "
    "This is useful if you are using many flags or want "
    "to set different options per each module.",
)
config_group.add_argument(
    "--config-file",
    help="Configuration file, must have a [mypy] section "
    "(defaults to {})".format(", ".join(defaults.CONFIG_FILES)),
)
add_invertible_flag(
    "--warn-unused-configs",
    default=False,
    strict_flag=True,
    help="Warn about unused '[mypy-&lt;pattern&gt;]' or '[[tool.mypy.overrides]]' "
    "config sections",
    group=config_group,
)

imports_group = parser.add_argument_group(
    title="Import discovery", description="Configure how imports are discovered and followed."
)
add_invertible_flag(
    "--no-namespace-packages",
    dest="namespace_packages",
    default=True,
    help="Support namespace packages (PEP 420, __init__.py-less)",
    group=imports_group,
)
imports_group.add_argument(
    "--ignore-missing-imports",
    action="store_true",
    help="Silently ignore imports of missing modules",
)
imports_group.add_argument(
    "--follow-imports",
    choices=["normal", "silent", "skip", "error"],
    default="normal",
    help="How to treat imports (default normal)",
)
imports_group.add_argument(
    "--python-executable",
    action="store",
    metavar="EXECUTABLE",
    help="Python executable used for finding PEP 561 compliant installed"
    " packages and stubs",
    dest="special-opts:python_executable",
)
imports_group.add_argument(
    "--no-site-packages",
    action="store_true",
    dest="special-opts:no_executable",
    help="Do not search for installed PEP 561 compliant packages",
)
imports_group.add_argument(
    "--no-silence-site-packages",
    action="store_true",
    help="Do not silence errors in PEP 561 compliant installed packages",
)

platform_group = parser.add_argument_group(
    title="Platform configuration",
    description="Type check code assuming it will be run under certain "
    "runtime conditions. By default, mypy assumes your code "
    "will be run using the same operating system and Python "
    "version you are using to run mypy itself.",
)
platform_group.add_argument(
    "--python-version",
    type=parse_version,
    metavar="x.y",
    help="Type check code assuming it will be running on Python x.y",
    dest="special-opts:python_version",
)
platform_group.add_argument(
    "-2",
    "--py2",
    dest="special-opts:python_version",
    action="store_const",
    const=defaults.PYTHON2_VERSION,
    help="Use Python 2 mode (same as --python-version 2.7)",
)
platform_group.add_argument(
    "--platform",
    action="store",
    metavar="PLATFORM",
    help="Type check special-cased code for the given OS platform "
    "(defaults to sys.platform)",
)
platform_group.add_argument(
    "--always-true",
    metavar="NAME",
    action="append",
    default=[],
    help="Additional variable to be considered True (may be repeated)",
)
platform_group.add_argument(
    "--always-false",
    metavar="NAME",
    action="append",
    default=[],
    help="Additional variable to be considered False (may be repeated)",
)

disallow_any_group = parser.add_argument_group(
    title="Disallow dynamic typing",
    description="Disallow the use of the dynamic 'Any' type under certain conditions.",
)
disallow_any_group.add_argument(
    "--disallow-any-unimported",
    default=False,
    action="store_true",
    help="Disallow Any types resulting from unfollowed imports",
)
disallow_any_group.add_argument(
    "--disallow-any-expr",
    default=False,
    action="store_true",
    help="Disallow all expressions that have type Any",
)
disallow_any_group.add_argument(
    "--disallow-any-decorated",
    default=False,
    action="store_true",
    help="Disallow functions that have Any in their signature "
    "after decorator transformation",
)
disallow_any_group.add_argument(
    "--disallow-any-explicit",
    default=False,
    action="store_true",
    help="Disallow explicit Any in type positions",
)
add_invertible_flag(
    "--disallow-any-generics",
    default=False,
    strict_flag=True,
    help="Disallow usage of generic types that do not specify explicit type parameters",
    group=disallow_any_group,
)
add_invertible_flag(
    "--disallow-subclassing-any",
    default=False,
    strict_flag=True,
    help="Disallow subclassing values of type 'Any' when defining classes",
    group=disallow_any_group,
)

untyped_group = parser.add_argument_group(
    title="Untyped definitions and calls",
    description="Configure how untyped definitions and calls are handled. "
    "Note: by default, mypy ignores any untyped function definitions "
    "and assumes any calls to such functions have a return "
    "type of 'Any'.",
)
add_invertible_flag(
    "--disallow-untyped-calls",
    default=False,
    strict_flag=True,
    help="Disallow calling functions without type annotations"
    " from functions with type annotations",
    group=untyped_group,
)
add_invertible_flag(
    "--disallow-untyped-defs",
    default=False,
    strict_flag=True,
    help="Disallow defining functions without type annotations"
    " or with incomplete type annotations",
    group=untyped_group,
)
add_invertible_flag(
    "--disallow-incomplete-defs",
    default=False,
    strict_flag=True,
    help="Disallow defining functions with incomplete type annotations",
    group=untyped_group,
)
add_invertible_flag(
    "--check-untyped-defs",
    default=False,
    strict_flag=True,
    help="Type check the interior of functions without type annotations",
    group=untyped_group,
)
add_invertible_flag(
    "--disallow-untyped-decorators",
    default=False,
    strict_flag=True,
    help="Disallow decorating typed functions with untyped decorators",
    group=untyped_group,
)

none_group = parser.add_argument_group(
    title="None and Optional handling",
    description="Adjust how values of type 'None' are handled. For more context on "
    "how mypy handles values of type 'None', see: "
    "https://mypy.readthedocs.io/en/stable/kinds_of_types.html#no-strict-optional",
)
add_invertible_flag(
    "--implicit-optional",
    default=False,
    help="Assume arguments with default values of None are Optional",
    group=none_group,
)
none_group.add_argument("--strict-optional", action="store_true", help=argparse.SUPPRESS)
none_group.add_argument(
    "--no-strict-optional",
    action="store_false",
    dest="strict_optional",
    help="Disable strict Optional checks (inverse: --strict-optional)",
)

lint_group = parser.add_argument_group(
    title="Configuring warnings",
    description="Detect code that is sound but redundant or problematic.",
)
add_invertible_flag(
    "--warn-redundant-casts",
    default=False,
    strict_flag=True,
    help="Warn about casting an expression to its inferred type",
    group=lint_group,
)
add_invertible_flag(
    "--warn-unused-ignores",
    default=False,
    strict_flag=True,
    help="Warn about unneeded '# type: ignore' comments",
    group=lint_group,
)
add_invertible_flag(
    "--no-warn-no-return",
    dest="warn_no_return",
    default=True,
    help="Do not warn about functions that end without returning",
    group=lint_group,
)
add_invertible_flag(
    "--warn-return-any",
    default=False,
    strict_flag=True,
    help="Warn about returning values of type Any from non-Any typed functions",
    group=lint_group,
)
add_invertible_flag(
    "--warn-unreachable",
    default=False,
    strict_flag=False,
    help="Warn about statements or expressions inferred to be unreachable",
    group=lint_group,
)

# Note: this group is intentionally added here even though we don't add
# --strict to this group near the end.
#
# That way, this group will appear after the various strictness groups
# but before the remaining flags.
# We add `--strict` near the end so we don't accidentally miss any strictness
# flags that are added after this group.
strictness_group = parser.add_argument_group(title="Miscellaneous strictness flags")

add_invertible_flag(
    "--allow-untyped-globals",
    default=False,
    strict_flag=False,
    help="Suppress toplevel errors caused by missing annotations",
    group=strictness_group,
)

add_invertible_flag(
    "--allow-redefinition",
    default=False,
    strict_flag=False,
    help="Allow unconditional variable redefinition with a new type",
    group=strictness_group,
)

add_invertible_flag(
    "--no-implicit-reexport",
    default=True,
    strict_flag=True,
    dest="implicit_reexport",
    help="Treat imports as private unless aliased",
    group=strictness_group,
)

add_invertible_flag(
    "--strict-equality",
    default=False,
    strict_flag=True,
    help="Prohibit equality, identity, and container checks for non-overlapping types",
    group=strictness_group,
)

add_invertible_flag(
    "--strict-concatenate",
    default=False,
    strict_flag=True,
    help="Make arguments prepended via Concatenate be truly positional-only",
    group=strictness_group,
)

strict_help = "Strict mode; enables the following flags: {}".format(
    ", ".join(strict_flag_names)
)
strictness_group.add_argument(
    "--strict", action="store_true", dest="special-opts:strict", help=strict_help
)

strictness_group.add_argument(
    "--disable-error-code",
    metavar="NAME",
    action="append",
    default=[],
    help="Disable a specific error code",
)
strictness_group.add_argument(
    "--enable-error-code",
    metavar="NAME",
    action="append",
    default=[],
    help="Enable a specific error code",
)

error_group = parser.add_argument_group(
    title="Configuring error messages",
    description="Adjust the amount of detail shown in error messages.",
)
add_invertible_flag(
    "--show-error-context",
    default=False,
    dest="show_error_context",
    help='Precede errors with "note:" messages explaining context',
    group=error_group,
)
add_invertible_flag(
    "--show-column-numbers",
    default=False,
    help="Show column numbers in error messages",
    group=error_group,
)
add_invertible_flag(
    "--show-error-end",
    default=False,
    help="Show end line/end column numbers in error messages."
    " This implies --show-column-numbers",
    group=error_group,
)
add_invertible_flag(
    "--hide-error-codes",
    default=False,
    help="Hide error codes in error messages",
    group=error_group,
)
add_invertible_flag(
    "--pretty",
    default=False,
    help="Use visually nicer output in error messages:"
    " Use soft word wrap, show source code snippets,"
    " and show error location markers",
    group=error_group,
)
add_invertible_flag(
    "--no-color-output",
    dest="color_output",
    default=True,
    help="Do not colorize error messages",
    group=error_group,
)
add_invertible_flag(
    "--no-error-summary",
    dest="error_summary",
    default=True,
    help="Do not show error stats summary",
    group=error_group,
)
add_invertible_flag(
    "--show-absolute-path",
    default=False,
    help="Show absolute paths to files",
    group=error_group,
)
error_group.add_argument(
    "--soft-error-limit",
    default=defaults.MANY_ERRORS_THRESHOLD,
    type=int,
    dest="many_errors_threshold",
    help=argparse.SUPPRESS,
)

incremental_group = parser.add_argument_group(
    title="Incremental mode",
    description="Adjust how mypy incrementally type checks and caches modules. "
    "Mypy caches type information about modules into a cache to "
    "let you speed up future invocations of mypy. Also see "
    "mypy's daemon mode: "
    "mypy.readthedocs.io/en/stable/mypy_daemon.html#mypy-daemon",
)
incremental_group.add_argument(
    "-i", "--incremental", action="store_true", help=argparse.SUPPRESS
)
incremental_group.add_argument(
    "--no-incremental",
    action="store_false",
    dest="incremental",
    help="Disable module cache (inverse: --incremental)",
)
incremental_group.add_argument(
    "--cache-dir",
    action="store",
    metavar="DIR",
    help="Store module cache info in the given folder in incremental mode "
    "(defaults to '{}')".format(defaults.CACHE_DIR),
)
add_invertible_flag(
    "--sqlite-cache",
    default=False,
    help="Use a sqlite database to store the cache",
    group=incremental_group,
)
incremental_group.add_argument(
    "--cache-fine-grained",
    action="store_true",
    help="Include fine-grained dependency information in the cache for the mypy daemon",
)
incremental_group.add_argument(
    "--skip-version-check",
    action="store_true",
    help="Allow using cache written by older mypy version",
)
incremental_group.add_argument(
    "--skip-cache-mtime-checks",
    action="store_true",
    help="Skip cache internal consistency checks based on mtime",
)

internals_group = parser.add_argument_group(
    title="Advanced options", description="Debug and customize mypy internals."
)
internals_group.add_argument("--pdb", action="store_true", help="Invoke pdb on fatal error")
internals_group.add_argument(
    "--show-traceback", "--tb", action="store_true", help="Show traceback on fatal error"
)
internals_group.add_argument(
    "--raise-exceptions", action="store_true", help="Raise exception on fatal error"
)
internals_group.add_argument(
    "--custom-typing-module",
    metavar="MODULE",
    dest="custom_typing_module",
    help="Use a custom typing module",
)
internals_group.add_argument(
    "--disable-recursive-aliases",
    action="store_true",
    help="Disable experimental support for recursive type aliases",
)
internals_group.add_argument(
    "--custom-typeshed-dir", metavar="DIR", help="Use the custom typeshed in DIR"
)
add_invertible_flag(
    "--warn-incomplete-stub",
    default=False,
    help="Warn if missing type annotation in typeshed, only relevant with"
    " --disallow-untyped-defs or --disallow-incomplete-defs enabled",
    group=internals_group,
)
internals_group.add_argument(
    "--shadow-file",
    nargs=2,
    metavar=("SOURCE_FILE", "SHADOW_FILE"),
    dest="shadow_file",
    action="append",
    help="When encountering SOURCE_FILE, read and type check "
    "the contents of SHADOW_FILE instead.",
)
add_invertible_flag("--fast-exit", default=True, help=argparse.SUPPRESS, group=internals_group)
# This flag is useful for mypy tests, where function bodies may be omitted. Plugin developers
# may want to use this as well in their tests.
add_invertible_flag(
    "--allow-empty-bodies", default=False, help=argparse.SUPPRESS, group=internals_group
)

report_group = parser.add_argument_group(
    title="Report generation", description="Generate a report in the specified format."
)
for report_type in sorted(defaults.REPORTER_NAMES):
    if report_type not in {"memory-xml"}:
        report_group.add_argument(
            f"--{report_type.replace('_', '-')}-report",
            metavar="DIR",
            dest=f"special-opts:{report_type}_report",
        )

other_group = parser.add_argument_group(title="Miscellaneous")
other_group.add_argument("--quickstart-file", help=argparse.SUPPRESS)
other_group.add_argument("--junit-xml", help="Write junit.xml to the given file")
other_group.add_argument(
    "--find-occurrences",
    metavar="CLASS.MEMBER",
    dest="special-opts:find_occurrences",
    help="Print out all usages of a class member (experimental)",
)
other_group.add_argument(
    "--scripts-are-modules",
    action="store_true",
    help="Script x becomes module x instead of __main__",
)

add_invertible_flag(
    "--install-types",
    default=False,
    strict_flag=False,
    help="Install detected missing library stub packages using pip",
    group=other_group,
)
add_invertible_flag(
    "--non-interactive",
    default=False,
    strict_flag=False,
    help=(
        "Install stubs without asking for confirmation and hide "
        + "errors, with --install-types"
    ),
    group=other_group,
    inverse="--interactive",
)

if server_options:
    # TODO: This flag is superfluous; remove after a short transition (2018-03-16)
    other_group.add_argument(
        "--experimental",
        action="store_true",
        dest="fine_grained_incremental",
        help="Enable fine-grained incremental mode",
    )
    other_group.add_argument(
        "--use-fine-grained-cache",
        action="store_true",
        help="Use the cache in fine-grained incremental mode",
    )

# hidden options
parser.add_argument(
    "--stats", action="store_true", dest="dump_type_stats", help=argparse.SUPPRESS
)
parser.add_argument(
    "--inferstats", action="store_true", dest="dump_inference_stats", help=argparse.SUPPRESS
)
parser.add_argument("--dump-build-stats", action="store_true", help=argparse.SUPPRESS)
# dump timing  stats for each processed file into the given output file
parser.add_argument("--timing-stats", dest="timing_stats", help=argparse.SUPPRESS)
# --debug-cache will disable any cache-related compressions/optimizations,
# which will make the cache writing process output pretty-printed JSON (which
# is easier to debug).
parser.add_argument("--debug-cache", action="store_true", help=argparse.SUPPRESS)
# --dump-deps will dump all fine-grained dependencies to stdout
parser.add_argument("--dump-deps", action="store_true", help=argparse.SUPPRESS)
# --dump-graph will dump the contents of the graph of SCCs and exit.
parser.add_argument("--dump-graph", action="store_true", help=argparse.SUPPRESS)
# --semantic-analysis-only does exactly that.
parser.add_argument("--semantic-analysis-only", action="store_true", help=argparse.SUPPRESS)
# --local-partial-types disallows partial types spanning module top level and a function
# (implicitly defined in fine-grained incremental mode)
parser.add_argument("--local-partial-types", action="store_true", help=argparse.SUPPRESS)
# --logical-deps adds some more dependencies that are not semantically needed, but
# may be helpful to determine relative importance of classes and functions for overall
# type precision in a code base. It also _removes_ some deps, so this flag should be never
# used except for generating code stats. This also automatically enables --cache-fine-grained.
# NOTE: This is an experimental option that may be modified or removed at any time.
parser.add_argument("--logical-deps", action="store_true", help=argparse.SUPPRESS)
# --bazel changes some behaviors for use with Bazel (https://bazel.build).
parser.add_argument("--bazel", action="store_true", help=argparse.SUPPRESS)
# --package-root adds a directory below which directories are considered
# packages even without __init__.py.  May be repeated.
parser.add_argument(
    "--package-root", metavar="ROOT", action="append", default=[], help=argparse.SUPPRESS
)
# --cache-map FILE ... gives a mapping from source files to cache files.
# Each triple of arguments is a source file, a cache meta file, and a cache data file.
# Modules not mentioned in the file will go through cache_dir.
# Must be followed by another flag or by '--' (and then only file args may follow).
parser.add_argument(
    "--cache-map", nargs="+", dest="special-opts:cache_map", help=argparse.SUPPRESS
)
parser.add_argument(
    "--enable-incomplete-features", action="store_true", help=argparse.SUPPRESS
)

# options specifying code to check
code_group = parser.add_argument_group(
    title="Running code",
    description="Specify the code you want to type check. For more details, see "
    "mypy.readthedocs.io/en/stable/running_mypy.html#running-mypy",
)
add_invertible_flag(
    "--explicit-package-bases",
    default=False,
    help="Use current directory and MYPYPATH to determine module names of files passed",
    group=code_group,
)
add_invertible_flag(
    "--fast-module-lookup", default=False, help=argparse.SUPPRESS, group=code_group
)
code_group.add_argument(
    "--exclude",
    action="append",
    metavar="PATTERN",
    default=[],
    help=(
        "Regular expression to match file names, directory names or paths which mypy should "
        "ignore while recursively discovering files to check, e.g. --exclude '/setup\\.py$'. "
        "May be specified more than once, eg. --exclude a --exclude b"
    ),
)
code_group.add_argument(
    "-m",
    "--module",
    action="append",
    metavar="MODULE",
    default=[],
    dest="special-opts:modules",
    help="Type-check module; can repeat for more modules",
)
code_group.add_argument(
    "-p",
    "--package",
    action="append",
    metavar="PACKAGE",
    default=[],
    dest="special-opts:packages",
    help="Type-check package recursively; can be repeated",
)
code_group.add_argument(
    "-c",
    "--command",
    action="append",
    metavar="PROGRAM_TEXT",
    dest="special-opts:command",
    help="Type-check program passed in as string",
)
code_group.add_argument(
    metavar="files",
    nargs="*",
    dest="special-opts:files",
    help="Type-check given files or directories",
)

# Parse arguments once into a dummy namespace so we can get the
# filename for the config file and know if the user requested all strict options.
dummy = argparse.Namespace()
parser.parse_args(args, dummy)
config_file = dummy.config_file
# Don't explicitly test if "config_file is not None" for this check.
# This lets `--config-file=` (an empty string) be used to disable all config files.
if config_file and not os.path.exists(config_file):
    parser.error(f"Cannot find config file '{config_file}'")

options = Options()

</t>
<t tx="ekr.20221004064035.1410">def is_overloaded_item(self, node: SymbolNode, statement: Statement) -&gt; bool:
    """Check whether the function belongs to the overloaded variants"""
    if isinstance(node, OverloadedFuncDef) and isinstance(statement, FuncDef):
        in_items = statement in {
            item.func if isinstance(item, Decorator) else item for item in node.items
        }
        in_impl = node.impl is not None and (
            (isinstance(node.impl, Decorator) and statement is node.impl.func)
            or statement is node.impl
        )
        return in_items or in_impl
    return False

</t>
<t tx="ekr.20221004064035.1411">def is_defined_in_current_module(self, fullname: str | None) -&gt; bool:
    if fullname is None:
        return False
    return module_prefix(self.modules, fullname) == self.cur_mod_id

</t>
<t tx="ekr.20221004064035.1412">def lookup_qualified(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    """Lookup a qualified name in all activate namespaces.

    Note that the result may contain a PlaceholderNode. The caller may
    want to defer in that case.

    Generate an error if the name is not defined unless suppress_errors
    is true or the current namespace is incomplete. In the latter case
    defer.
    """
    if "." not in name:
        # Simple case: look up a short name.
        return self.lookup(name, ctx, suppress_errors=suppress_errors)
    parts = name.split(".")
    namespace = self.cur_mod_id
    sym = self.lookup(parts[0], ctx, suppress_errors=suppress_errors)
    if sym:
        for i in range(1, len(parts)):
            node = sym.node
            part = parts[i]
            if isinstance(node, TypeInfo):
                nextsym = node.get(part)
            elif isinstance(node, MypyFile):
                nextsym = self.get_module_symbol(node, part)
                namespace = node.fullname
            elif isinstance(node, PlaceholderNode):
                return sym
            elif isinstance(node, TypeAlias) and node.no_args:
                assert isinstance(node.target, ProperType)
                if isinstance(node.target, Instance):
                    nextsym = node.target.type.get(part)
                else:
                    nextsym = None
            else:
                if isinstance(node, Var):
                    typ = get_proper_type(node.type)
                    if isinstance(typ, AnyType):
                        # Allow access through Var with Any type without error.
                        return self.implicit_symbol(sym, name, parts[i:], typ)
                # This might be something like valid `P.args` or invalid `P.__bound__` access.
                # Important note that `ParamSpecExpr` is also ignored in other places.
                # See https://github.com/python/mypy/pull/13468
                if isinstance(node, ParamSpecExpr) and part in ("args", "kwargs"):
                    return None
                # Lookup through invalid node, such as variable or function
                nextsym = None
            if not nextsym or nextsym.module_hidden:
                if not suppress_errors:
                    self.name_not_defined(name, ctx, namespace=namespace)
                return None
            sym = nextsym
    return sym

</t>
<t tx="ekr.20221004064035.1413">def lookup_type_node(self, expr: Expression) -&gt; SymbolTableNode | None:
    try:
        t = self.expr_to_unanalyzed_type(expr)
    except TypeTranslationError:
        return None
    if isinstance(t, UnboundType):
        n = self.lookup_qualified(t.name, expr, suppress_errors=True)
        return n
    return None

</t>
<t tx="ekr.20221004064035.1414">def get_module_symbol(self, node: MypyFile, name: str) -&gt; SymbolTableNode | None:
    """Look up a symbol from a module.

    Return None if no matching symbol could be bound.
    """
    module = node.fullname
    names = node.names
    sym = names.get(name)
    if not sym:
        fullname = module + "." + name
        if fullname in self.modules:
            sym = SymbolTableNode(GDEF, self.modules[fullname])
        elif self.is_incomplete_namespace(module):
            self.record_incomplete_ref()
        elif "__getattr__" in names and (
            node.is_stub or self.options.python_version &gt;= (3, 7)
        ):
            gvar = self.create_getattr_var(names["__getattr__"], name, fullname)
            if gvar:
                sym = SymbolTableNode(GDEF, gvar)
        elif self.is_missing_module(fullname):
            # We use the fullname of the original definition so that we can
            # detect whether two names refer to the same thing.
            var_type = AnyType(TypeOfAny.from_unimported_type)
            v = Var(name, type=var_type)
            v._fullname = fullname
            sym = SymbolTableNode(GDEF, v)
    elif sym.module_hidden:
        sym = None
    return sym

</t>
<t tx="ekr.20221004064035.1415">def is_missing_module(self, module: str) -&gt; bool:
    return module in self.missing_modules

</t>
<t tx="ekr.20221004064035.1416">def implicit_symbol(
    self, sym: SymbolTableNode, name: str, parts: list[str], source_type: AnyType
) -&gt; SymbolTableNode:
    """Create symbol for a qualified name reference through Any type."""
    if sym.node is None:
        basename = None
    else:
        basename = sym.node.fullname
    if basename is None:
        fullname = name
    else:
        fullname = basename + "." + ".".join(parts)
    var_type = AnyType(TypeOfAny.from_another_any, source_type)
    var = Var(parts[-1], var_type)
    var._fullname = fullname
    return SymbolTableNode(GDEF, var)

</t>
<t tx="ekr.20221004064035.1417">def create_getattr_var(
    self, getattr_defn: SymbolTableNode, name: str, fullname: str
) -&gt; Var | None:
    """Create a dummy variable using module-level __getattr__ return type.

    If not possible, return None.

    Note that multiple Var nodes can be created for a single name. We
    can use the from_module_getattr and the fullname attributes to
    check if two dummy Var nodes refer to the same thing. Reusing Var
    nodes would require non-local mutable state, which we prefer to
    avoid.
    """
    if isinstance(getattr_defn.node, (FuncDef, Var)):
        node_type = get_proper_type(getattr_defn.node.type)
        if isinstance(node_type, CallableType):
            typ = node_type.ret_type
        else:
            typ = AnyType(TypeOfAny.from_error)
        v = Var(name, type=typ)
        v._fullname = fullname
        v.from_module_getattr = True
        return v
    return None

</t>
<t tx="ekr.20221004064035.1418">def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode:
    ret = self.lookup_fully_qualified_or_none(fullname)
    assert ret is not None, fullname
    return ret

</t>
<t tx="ekr.20221004064035.1419">def lookup_fully_qualified_or_none(self, fullname: str) -&gt; SymbolTableNode | None:
    """Lookup a fully qualified name that refers to a module-level definition.

    Don't assume that the name is defined. This happens in the global namespace --
    the local module namespace is ignored. This does not dereference indirect
    refs.

    Note that this can't be used for names nested in class namespaces.
    """
    # TODO: unify/clean-up/simplify lookup methods, see #4157.
    # TODO: support nested classes (but consider performance impact,
    #       we might keep the module level only lookup for thing like 'builtins.int').
    assert "." in fullname
    module, name = fullname.rsplit(".", maxsplit=1)
    if module not in self.modules:
        return None
    filenode = self.modules[module]
    result = filenode.names.get(name)
    if result is None and self.is_incomplete_namespace(module):
        # TODO: More explicit handling of incomplete refs?
        self.record_incomplete_ref()
    return result

</t>
<t tx="ekr.20221004064035.142">def set_strict_flags() -&gt; None:
    for dest, value in strict_flag_assignments:
        setattr(options, dest, value)

</t>
<t tx="ekr.20221004064035.1420">def object_type(self) -&gt; Instance:
    return self.named_type("builtins.object")

</t>
<t tx="ekr.20221004064035.1421">def str_type(self) -&gt; Instance:
    return self.named_type("builtins.str")

</t>
<t tx="ekr.20221004064035.1422">def named_type(self, fullname: str, args: list[Type] | None = None) -&gt; Instance:
    sym = self.lookup_fully_qualified(fullname)
    assert sym, "Internal error: attempted to construct unknown type"
    node = sym.node
    assert isinstance(node, TypeInfo)
    if args:
        # TODO: assert len(args) == len(node.defn.type_vars)
        return Instance(node, args)
    return Instance(node, [AnyType(TypeOfAny.special_form)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20221004064035.1423">def named_type_or_none(self, fullname: str, args: list[Type] | None = None) -&gt; Instance | None:
    sym = self.lookup_fully_qualified_or_none(fullname)
    if not sym or isinstance(sym.node, PlaceholderNode):
        return None
    node = sym.node
    if isinstance(node, TypeAlias):
        assert isinstance(node.target, Instance)  # type: ignore[misc]
        node = node.target.type
    assert isinstance(node, TypeInfo), node
    if args is not None:
        # TODO: assert len(args) == len(node.defn.type_vars)
        return Instance(node, args)
    return Instance(node, [AnyType(TypeOfAny.unannotated)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20221004064035.1424">def builtin_type(self, fully_qualified_name: str) -&gt; Instance:
    """Legacy function -- use named_type() instead."""
    return self.named_type(fully_qualified_name)

</t>
<t tx="ekr.20221004064035.1425">def lookup_current_scope(self, name: str) -&gt; SymbolTableNode | None:
    if self.locals[-1] is not None:
        return self.locals[-1].get(name)
    elif self.type is not None:
        return self.type.names.get(name)
    else:
        return self.globals.get(name)

</t>
<t tx="ekr.20221004064035.1426">#
# Adding symbols
#

</t>
<t tx="ekr.20221004064035.1427">def add_symbol(
    self,
    name: str,
    node: SymbolNode,
    context: Context,
    module_public: bool = True,
    module_hidden: bool = False,
    can_defer: bool = True,
    escape_comprehensions: bool = False,
) -&gt; bool:
    """Add symbol to the currently active symbol table.

    Generally additions to symbol table should go through this method or
    one of the methods below so that kinds, redefinitions, conditional
    definitions, and skipped names are handled consistently.

    Return True if we actually added the symbol, or False if we refused to do so
    (because something is not ready).

    If can_defer is True, defer current target if adding a placeholder.
    """
    if self.is_func_scope():
        kind = LDEF
    elif self.type is not None:
        kind = MDEF
    else:
        kind = GDEF
    symbol = SymbolTableNode(
        kind, node, module_public=module_public, module_hidden=module_hidden
    )
    return self.add_symbol_table_node(name, symbol, context, can_defer, escape_comprehensions)

</t>
<t tx="ekr.20221004064035.1428">def add_symbol_skip_local(self, name: str, node: SymbolNode) -&gt; None:
    """Same as above, but skipping the local namespace.

    This doesn't check for previous definition and is only used
    for serialization of method-level classes.

    Classes defined within methods can be exposed through an
    attribute type, but method-level symbol tables aren't serialized.
    This method can be used to add such classes to an enclosing,
    serialized symbol table.
    """
    # TODO: currently this is only used by named tuples and typed dicts.
    # Use this method also by normal classes, see issue #6422.
    if self.type is not None:
        names = self.type.names
        kind = MDEF
    else:
        names = self.globals
        kind = GDEF
    symbol = SymbolTableNode(kind, node)
    names[name] = symbol

</t>
<t tx="ekr.20221004064035.1429">def add_symbol_table_node(
    self,
    name: str,
    symbol: SymbolTableNode,
    context: Context | None = None,
    can_defer: bool = True,
    escape_comprehensions: bool = False,
) -&gt; bool:
    """Add symbol table node to the currently active symbol table.

    Return True if we actually added the symbol, or False if we refused
    to do so (because something is not ready or it was a no-op).

    Generate an error if there is an invalid redefinition.

    If context is None, unconditionally add node, since we can't report
    an error. Note that this is used by plugins to forcibly replace nodes!

    TODO: Prevent plugins from replacing nodes, as it could cause problems?

    Args:
        name: short name of symbol
        symbol: Node to add
        can_defer: if True, defer current target if adding a placeholder
        context: error context (see above about None value)
    """
    names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
    existing = names.get(name)
    if isinstance(symbol.node, PlaceholderNode) and can_defer:
        if context is not None:
            self.process_placeholder(name, "name", context)
        else:
            # see note in docstring describing None contexts
            self.defer()
    if (
        existing is not None
        and context is not None
        and not is_valid_replacement(existing, symbol)
    ):
        # There is an existing node, so this may be a redefinition.
        # If the new node points to the same node as the old one,
        # or if both old and new nodes are placeholders, we don't
        # need to do anything.
        old = existing.node
        new = symbol.node
        if isinstance(new, PlaceholderNode):
            # We don't know whether this is okay. Let's wait until the next iteration.
            return False
        if not is_same_symbol(old, new):
            if isinstance(new, (FuncDef, Decorator, OverloadedFuncDef, TypeInfo)):
                self.add_redefinition(names, name, symbol)
            if not (isinstance(new, (FuncDef, Decorator)) and self.set_original_def(old, new)):
                self.name_already_defined(name, context, existing)
    elif name not in self.missing_names[-1] and "*" not in self.missing_names[-1]:
        names[name] = symbol
        self.progress = True
        return True
    return False

</t>
<t tx="ekr.20221004064035.143">def process_package_roots(
    fscache: FileSystemCache | None, parser: argparse.ArgumentParser, options: Options
) -&gt; None:
    """Validate and normalize package_root."""
    if fscache is None:
        parser.error("--package-root does not work here (no fscache)")
    assert fscache is not None  # Since mypy doesn't know parser.error() raises.
    # Do some stuff with drive letters to make Windows happy (esp. tests).
    current_drive, _ = os.path.splitdrive(os.getcwd())
    dot = os.curdir
    dotslash = os.curdir + os.sep
    dotdotslash = os.pardir + os.sep
    trivial_paths = {dot, dotslash}
    package_root = []
    for root in options.package_root:
        if os.path.isabs(root):
            parser.error(f"Package root cannot be absolute: {root!r}")
        drive, root = os.path.splitdrive(root)
        if drive and drive != current_drive:
            parser.error(f"Package root must be on current drive: {drive + root!r}")
        # Empty package root is always okay.
        if root:
            root = os.path.relpath(root)  # Normalize the heck out of it.
            if not root.endswith(os.sep):
                root = root + os.sep
            if root.startswith(dotdotslash):
                parser.error(f"Package root cannot be above current directory: {root!r}")
            if root in trivial_paths:
                root = ""
        package_root.append(root)
    options.package_root = package_root
    # Pass the package root on the the filesystem cache.
    fscache.set_package_root(package_root)


</t>
<t tx="ekr.20221004064035.1430">def add_redefinition(self, names: SymbolTable, name: str, symbol: SymbolTableNode) -&gt; None:
    """Add a symbol table node that reflects a redefinition as a function or a class.

    Redefinitions need to be added to the symbol table so that they can be found
    through AST traversal, but they have dummy names of form 'name-redefinition[N]',
    where N ranges over 2, 3, ... (omitted for the first redefinition).

    Note: we always store redefinitions independently of whether they are valid or not
    (so they will be semantically analyzed), the caller should give an error for invalid
    redefinitions (such as e.g. variable redefined as a class).
    """
    i = 1
    # Don't serialize redefined nodes. They are likely to have
    # busted internal references which can cause problems with
    # serialization and they can't have any external references to
    # them.
    symbol.no_serialize = True
    while True:
        if i == 1:
            new_name = f"{name}-redefinition"
        else:
            new_name = f"{name}-redefinition{i}"
        existing = names.get(new_name)
        if existing is None:
            names[new_name] = symbol
            return
        elif existing.node is symbol.node:
            # Already there
            return
        i += 1

</t>
<t tx="ekr.20221004064035.1431">def add_local(self, node: Var | FuncDef | OverloadedFuncDef, context: Context) -&gt; None:
    """Add local variable or function."""
    assert self.is_func_scope()
    name = node.name
    node._fullname = name
    self.add_symbol(name, node, context)

</t>
<t tx="ekr.20221004064035.1432">def add_module_symbol(
    self, id: str, as_id: str, context: Context, module_public: bool, module_hidden: bool
) -&gt; None:
    """Add symbol that is a reference to a module object."""
    if id in self.modules:
        node = self.modules[id]
        self.add_symbol(
            as_id, node, context, module_public=module_public, module_hidden=module_hidden
        )
    else:
        self.add_unknown_imported_symbol(
            as_id,
            context,
            target_name=id,
            module_public=module_public,
            module_hidden=module_hidden,
        )

</t>
<t tx="ekr.20221004064035.1433">def _get_node_for_class_scoped_import(
    self, name: str, symbol_node: SymbolNode | None, context: Context
) -&gt; SymbolNode | None:
    if symbol_node is None:
        return None
    # I promise this type checks; I'm just making mypyc issues go away.
    # mypyc is absolutely convinced that `symbol_node` narrows to a Var in the following,
    # when it can also be a FuncBase. Once fixed, `f` in the following can be removed.
    # See also https://github.com/mypyc/mypyc/issues/892
    f = cast(Any, lambda x: x)
    if isinstance(f(symbol_node), (Decorator, FuncBase, Var)):
        # For imports in class scope, we construct a new node to represent the symbol and
        # set its `info` attribute to `self.type`.
        existing = self.current_symbol_table().get(name)
        if (
            # The redefinition checks in `add_symbol_table_node` don't work for our
            # constructed Var / FuncBase, so check for possible redefinitions here.
            existing is not None
            and isinstance(f(existing.node), (Decorator, FuncBase, Var))
            and (
                isinstance(f(existing.type), f(AnyType))
                or f(existing.type) == f(symbol_node).type
            )
        ):
            return existing.node

        # Construct the new node
        if isinstance(f(symbol_node), (FuncBase, Decorator)):
            # In theory we could construct a new node here as well, but in practice
            # it doesn't work well, see #12197
            typ: Type | None = AnyType(TypeOfAny.from_error)
            self.fail("Unsupported class scoped import", context)
        else:
            typ = f(symbol_node).type
        symbol_node = Var(name, typ)
        symbol_node._fullname = self.qualified_name(name)
        assert self.type is not None  # guaranteed by is_class_scope
        symbol_node.info = self.type
        symbol_node.line = context.line
        symbol_node.column = context.column
    return symbol_node

</t>
<t tx="ekr.20221004064035.1434">def add_imported_symbol(
    self,
    name: str,
    node: SymbolTableNode,
    context: Context,
    module_public: bool,
    module_hidden: bool,
) -&gt; None:
    """Add an alias to an existing symbol through import."""
    assert not module_hidden or not module_public

    symbol_node: SymbolNode | None = node.node

    if self.is_class_scope():
        symbol_node = self._get_node_for_class_scoped_import(name, symbol_node, context)

    symbol = SymbolTableNode(
        node.kind, symbol_node, module_public=module_public, module_hidden=module_hidden
    )
    self.add_symbol_table_node(name, symbol, context)

</t>
<t tx="ekr.20221004064035.1435">def add_unknown_imported_symbol(
    self,
    name: str,
    context: Context,
    target_name: str | None,
    module_public: bool,
    module_hidden: bool,
) -&gt; None:
    """Add symbol that we don't know what it points to because resolving an import failed.

    This can happen if a module is missing, or it is present, but doesn't have
    the imported attribute. The `target_name` is the name of symbol in the namespace
    it is imported from. For example, for 'from mod import x as y' the target_name is
    'mod.x'. This is currently used only to track logical dependencies.
    """
    existing = self.current_symbol_table().get(name)
    if existing and isinstance(existing.node, Var) and existing.node.is_suppressed_import:
        # This missing import was already added -- nothing to do here.
        return
    var = Var(name)
    if self.options.logical_deps and target_name is not None:
        # This makes it possible to add logical fine-grained dependencies
        # from a missing module. We can't use this by default, since in a
        # few places we assume that the full name points to a real
        # definition, but this name may point to nothing.
        var._fullname = target_name
    elif self.type:
        var._fullname = self.type.fullname + "." + name
        var.info = self.type
    else:
        var._fullname = self.qualified_name(name)
    var.is_ready = True
    any_type = AnyType(TypeOfAny.from_unimported_type, missing_import_name=var._fullname)
    var.type = any_type
    var.is_suppressed_import = True
    self.add_symbol(
        name, var, context, module_public=module_public, module_hidden=module_hidden
    )

</t>
<t tx="ekr.20221004064035.1436">#
# Other helpers
#

</t>
<t tx="ekr.20221004064035.1437">@contextmanager
def tvar_scope_frame(self, frame: TypeVarLikeScope) -&gt; Iterator[None]:
    old_scope = self.tvar_scope
    self.tvar_scope = frame
    yield
    self.tvar_scope = old_scope

</t>
<t tx="ekr.20221004064035.1438">def defer(self, debug_context: Context | None = None, force_progress: bool = False) -&gt; None:
    """Defer current analysis target to be analyzed again.

    This must be called if something in the current target is
    incomplete or has a placeholder node. However, this must *not*
    be called during the final analysis iteration! Instead, an error
    should be generated. Often 'process_placeholder' is a good
    way to either defer or generate an error.

    NOTE: Some methods, such as 'anal_type', 'mark_incomplete' and
          'record_incomplete_ref', call this implicitly, or when needed.
          They are usually preferable to a direct defer() call.
    """
    assert not self.final_iteration, "Must not defer during final iteration"
    if force_progress:
        # Usually, we report progress if we have replaced a placeholder node
        # with an actual valid node. However, sometimes we need to update an
        # existing node *in-place*. For example, this is used by type aliases
        # in context of forward references and/or recursive aliases, and in
        # similar situations (recursive named tuples etc).
        self.progress = True
    self.deferred = True
    # Store debug info for this deferral.
    line = (
        debug_context.line if debug_context else self.statement.line if self.statement else -1
    )
    self.deferral_debug_context.append((self.cur_mod_id, line))

</t>
<t tx="ekr.20221004064035.1439">def track_incomplete_refs(self) -&gt; Tag:
    """Return tag that can be used for tracking references to incomplete names."""
    return self.num_incomplete_refs

</t>
<t tx="ekr.20221004064035.144">def process_cache_map(
    parser: argparse.ArgumentParser, special_opts: argparse.Namespace, options: Options
) -&gt; None:
    """Validate cache_map and copy into options.cache_map."""
    n = len(special_opts.cache_map)
    if n % 3 != 0:
        parser.error("--cache-map requires one or more triples (see source)")
    for i in range(0, n, 3):
        source, meta_file, data_file = special_opts.cache_map[i : i + 3]
        if source in options.cache_map:
            parser.error(f"Duplicate --cache-map source {source})")
        if not source.endswith(".py") and not source.endswith(".pyi"):
            parser.error(f"Invalid --cache-map source {source} (triple[0] must be *.py[i])")
        if not meta_file.endswith(".meta.json"):
            parser.error(
                "Invalid --cache-map meta_file %s (triple[1] must be *.meta.json)" % meta_file
            )
        if not data_file.endswith(".data.json"):
            parser.error(
                "Invalid --cache-map data_file %s (triple[2] must be *.data.json)" % data_file
            )
        options.cache_map[source] = (meta_file, data_file)


</t>
<t tx="ekr.20221004064035.1440">def found_incomplete_ref(self, tag: Tag) -&gt; bool:
    """Have we encountered an incomplete reference since starting tracking?"""
    return self.num_incomplete_refs != tag

</t>
<t tx="ekr.20221004064035.1441">def record_incomplete_ref(self) -&gt; None:
    """Record the encounter of an incomplete reference and defer current analysis target."""
    self.defer()
    self.num_incomplete_refs += 1

</t>
<t tx="ekr.20221004064035.1442">def mark_incomplete(
    self,
    name: str,
    node: Node,
    becomes_typeinfo: bool = False,
    module_public: bool = True,
    module_hidden: bool = False,
) -&gt; None:
    """Mark a definition as incomplete (and defer current analysis target).

    Also potentially mark the current namespace as incomplete.

    Args:
        name: The name that we weren't able to define (or '*' if the name is unknown)
        node: The node that refers to the name (definition or lvalue)
        becomes_typeinfo: Pass this to PlaceholderNode (used by special forms like
            named tuples that will create TypeInfos).
    """
    self.defer(node)
    if name == "*":
        self.incomplete = True
    elif not self.is_global_or_nonlocal(name):
        fullname = self.qualified_name(name)
        assert self.statement
        placeholder = PlaceholderNode(
            fullname, node, self.statement.line, becomes_typeinfo=becomes_typeinfo
        )
        self.add_symbol(
            name,
            placeholder,
            module_public=module_public,
            module_hidden=module_hidden,
            context=dummy_context(),
        )
    self.missing_names[-1].add(name)

</t>
<t tx="ekr.20221004064035.1443">def is_incomplete_namespace(self, fullname: str) -&gt; bool:
    """Is a module or class namespace potentially missing some definitions?

    If a name is missing from an incomplete namespace, we'll need to defer the
    current analysis target.
    """
    return fullname in self.incomplete_namespaces

</t>
<t tx="ekr.20221004064035.1444">def process_placeholder(self, name: str, kind: str, ctx: Context) -&gt; None:
    """Process a reference targeting placeholder node.

    If this is not a final iteration, defer current node,
    otherwise report an error.

    The 'kind' argument indicates if this a name or attribute expression
    (used for better error message).
    """
    if self.final_iteration:
        self.cannot_resolve_name(name, kind, ctx)
    else:
        self.defer(ctx)

</t>
<t tx="ekr.20221004064035.1445">def cannot_resolve_name(self, name: str, kind: str, ctx: Context) -&gt; None:
    self.fail(f'Cannot resolve {kind} "{name}" (possible cyclic definition)', ctx)
    if not self.options.disable_recursive_aliases and self.is_func_scope():
        self.note("Recursive types are not allowed at function scope", ctx)

</t>
<t tx="ekr.20221004064035.1446">def qualified_name(self, name: str) -&gt; str:
    if self.type is not None:
        return self.type._fullname + "." + name
    elif self.is_func_scope():
        return name
    else:
        return self.cur_mod_id + "." + name

</t>
<t tx="ekr.20221004064035.1447">@contextmanager
def enter(
    self, function: FuncItem | GeneratorExpr | DictionaryComprehension
) -&gt; Iterator[None]:
    """Enter a function, generator or comprehension scope."""
    names = self.saved_locals.setdefault(function, SymbolTable())
    self.locals.append(names)
    is_comprehension = isinstance(function, (GeneratorExpr, DictionaryComprehension))
    self.is_comprehension_stack.append(is_comprehension)
    self.global_decls.append(set())
    self.nonlocal_decls.append(set())
    # -1 since entering block will increment this to 0.
    self.block_depth.append(-1)
    self.missing_names.append(set())
    try:
        yield
    finally:
        self.locals.pop()
        self.is_comprehension_stack.pop()
        self.global_decls.pop()
        self.nonlocal_decls.pop()
        self.block_depth.pop()
        self.missing_names.pop()

</t>
<t tx="ekr.20221004064035.1448">def is_func_scope(self) -&gt; bool:
    return self.locals[-1] is not None

</t>
<t tx="ekr.20221004064035.1449">def is_nested_within_func_scope(self) -&gt; bool:
    """Are we underneath a function scope, even if we are in a nested class also?"""
    return any(l is not None for l in self.locals)

</t>
<t tx="ekr.20221004064035.145">def maybe_write_junit_xml(td: float, serious: bool, messages: list[str], options: Options) -&gt; None:
    if options.junit_xml:
        py_version = f"{options.python_version[0]}_{options.python_version[1]}"
        util.write_junit_xml(
            td, serious, messages, options.junit_xml, py_version, options.platform
        )


</t>
<t tx="ekr.20221004064035.1450">def is_class_scope(self) -&gt; bool:
    return self.type is not None and not self.is_func_scope()

</t>
<t tx="ekr.20221004064035.1451">def is_module_scope(self) -&gt; bool:
    return not (self.is_class_scope() or self.is_func_scope())

</t>
<t tx="ekr.20221004064035.1452">def current_symbol_kind(self) -&gt; int:
    if self.is_class_scope():
        kind = MDEF
    elif self.is_func_scope():
        kind = LDEF
    else:
        kind = GDEF
    return kind

</t>
<t tx="ekr.20221004064035.1453">def current_symbol_table(self, escape_comprehensions: bool = False) -&gt; SymbolTable:
    if self.is_func_scope():
        assert self.locals[-1] is not None
        if escape_comprehensions:
            assert len(self.locals) == len(self.is_comprehension_stack)
            # Retrieve the symbol table from the enclosing non-comprehension scope.
            for i, is_comprehension in enumerate(reversed(self.is_comprehension_stack)):
                if not is_comprehension:
                    if i == len(self.locals) - 1:  # The last iteration.
                        # The caller of the comprehension is in the global space.
                        names = self.globals
                    else:
                        names_candidate = self.locals[-1 - i]
                        assert (
                            names_candidate is not None
                        ), "Escaping comprehension from invalid scope"
                        names = names_candidate
                    break
            else:
                assert False, "Should have at least one non-comprehension scope"
        else:
            names = self.locals[-1]
        assert names is not None
    elif self.type is not None:
        names = self.type.names
    else:
        names = self.globals
    return names

</t>
<t tx="ekr.20221004064035.1454">def is_global_or_nonlocal(self, name: str) -&gt; bool:
    return self.is_func_scope() and (
        name in self.global_decls[-1] or name in self.nonlocal_decls[-1]
    )

</t>
<t tx="ekr.20221004064035.1455">def add_exports(self, exp_or_exps: Iterable[Expression] | Expression) -&gt; None:
    exps = [exp_or_exps] if isinstance(exp_or_exps, Expression) else exp_or_exps
    for exp in exps:
        if isinstance(exp, StrExpr):
            self.all_exports.append(exp.value)

</t>
<t tx="ekr.20221004064035.1456">def name_not_defined(self, name: str, ctx: Context, namespace: str | None = None) -&gt; None:
    incomplete = self.is_incomplete_namespace(namespace or self.cur_mod_id)
    if (
        namespace is None
        and self.type
        and not self.is_func_scope()
        and self.incomplete_type_stack[-1]
        and not self.final_iteration
    ):
        # We are processing a class body for the first time, so it is incomplete.
        incomplete = True
    if incomplete:
        # Target namespace is incomplete, so it's possible that the name will be defined
        # later on. Defer current target.
        self.record_incomplete_ref()
        return
    message = f'Name "{name}" is not defined'
    self.fail(message, ctx, code=codes.NAME_DEFINED)

    if f"builtins.{name}" in SUGGESTED_TEST_FIXTURES:
        # The user probably has a missing definition in a test fixture. Let's verify.
        fullname = f"builtins.{name}"
        if self.lookup_fully_qualified_or_none(fullname) is None:
            # Yes. Generate a helpful note.
            self.msg.add_fixture_note(fullname, ctx)

    modules_with_unimported_hints = {
        name.split(".", 1)[0] for name in TYPES_FOR_UNIMPORTED_HINTS
    }
    lowercased = {name.lower(): name for name in TYPES_FOR_UNIMPORTED_HINTS}
    for module in modules_with_unimported_hints:
        fullname = f"{module}.{name}".lower()
        if fullname not in lowercased:
            continue
        # User probably forgot to import these types.
        hint = (
            'Did you forget to import it from "{module}"?'
            ' (Suggestion: "from {module} import {name}")'
        ).format(module=module, name=lowercased[fullname].rsplit(".", 1)[-1])
        self.note(hint, ctx, code=codes.NAME_DEFINED)

</t>
<t tx="ekr.20221004064035.1457">def already_defined(
    self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None, noun: str
) -&gt; None:
    if isinstance(original_ctx, SymbolTableNode):
        node: SymbolNode | None = original_ctx.node
    elif isinstance(original_ctx, SymbolNode):
        node = original_ctx
    else:
        node = None

    if isinstance(original_ctx, SymbolTableNode) and isinstance(original_ctx.node, MypyFile):
        # Since this is an import, original_ctx.node points to the module definition.
        # Therefore its line number is always 1, which is not useful for this
        # error message.
        extra_msg = " (by an import)"
    elif node and node.line != -1 and self.is_local_name(node.fullname):
        # TODO: Using previous symbol node may give wrong line. We should use
        #       the line number where the binding was established instead.
        extra_msg = f" on line {node.line}"
    else:
        extra_msg = " (possibly by an import)"
    self.fail(
        f'{noun} "{unmangle(name)}" already defined{extra_msg}', ctx, code=codes.NO_REDEF
    )

</t>
<t tx="ekr.20221004064035.1458">def name_already_defined(
    self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None = None
) -&gt; None:
    self.already_defined(name, ctx, original_ctx, noun="Name")

</t>
<t tx="ekr.20221004064035.1459">def attribute_already_defined(
    self, name: str, ctx: Context, original_ctx: SymbolTableNode | SymbolNode | None = None
) -&gt; None:
    self.already_defined(name, ctx, original_ctx, noun="Attribute")

</t>
<t tx="ekr.20221004064035.146">def fail(msg: str, stderr: TextIO, options: Options) -&gt; NoReturn:
    """Fail with a serious error."""
    stderr.write(f"{msg}\n")
    maybe_write_junit_xml(0.0, serious=True, messages=[msg], options=options)
    sys.exit(2)


</t>
<t tx="ekr.20221004064035.1460">def is_local_name(self, name: str) -&gt; bool:
    """Does name look like reference to a definition in the current module?"""
    return self.is_defined_in_current_module(name) or "." not in name

</t>
<t tx="ekr.20221004064035.1461">def in_checked_function(self) -&gt; bool:
    """Should we type-check the current function?

    - Yes if --check-untyped-defs is set.
    - Yes outside functions.
    - Yes in annotated functions.
    - No otherwise.
    """
    if self.options.check_untyped_defs or not self.function_stack:
        return True

    current_index = len(self.function_stack) - 1
    while current_index &gt;= 0:
        current_func = self.function_stack[current_index]
        if not isinstance(current_func, LambdaExpr):
            return not current_func.is_dynamic()

        # Special case, `lambda` inherits the "checked" state from its parent.
        # Because `lambda` itself cannot be annotated.
        # `lambdas` can be deeply nested, so we try to find at least one other parent.
        current_index -= 1

    # This means that we only have a stack of `lambda` functions,
    # no regular functions.
    return True

</t>
<t tx="ekr.20221004064035.1462">def fail(
    self,
    msg: str,
    ctx: Context,
    serious: bool = False,
    *,
    code: ErrorCode | None = None,
    blocker: bool = False,
) -&gt; None:
    if not serious and not self.in_checked_function():
        return
    # In case it's a bug and we don't really have context
    assert ctx is not None, msg
    self.errors.report(ctx.get_line(), ctx.get_column(), msg, blocker=blocker, code=code)

</t>
<t tx="ekr.20221004064035.1463">def note(self, msg: str, ctx: Context, code: ErrorCode | None = None) -&gt; None:
    if not self.in_checked_function():
        return
    self.errors.report(ctx.get_line(), ctx.get_column(), msg, severity="note", code=code)

</t>
<t tx="ekr.20221004064035.1464">def accept(self, node: Node) -&gt; None:
    try:
        node.accept(self)
    except Exception as err:
        report_internal_error(err, self.errors.file, node.line, self.errors, self.options)

</t>
<t tx="ekr.20221004064035.1465">def expr_to_analyzed_type(
    self, expr: Expression, report_invalid_types: bool = True, allow_placeholder: bool = False
) -&gt; Type | None:
    if isinstance(expr, CallExpr):
        # This is a legacy syntax intended mostly for Python 2, we keep it for
        # backwards compatibility, but new features like generic named tuples
        # and recursive named tuples will be not supported.
        expr.accept(self)
        internal_name, info, tvar_defs = self.named_tuple_analyzer.check_namedtuple(
            expr, None, self.is_func_scope()
        )
        if tvar_defs:
            self.fail("Generic named tuples are not supported for legacy class syntax", expr)
            self.note("Use either Python 3 class syntax, or the assignment syntax", expr)
        if internal_name is None:
            # Some form of namedtuple is the only valid type that looks like a call
            # expression. This isn't a valid type.
            raise TypeTranslationError()
        elif not info:
            self.defer(expr)
            return None
        assert info.tuple_type, "NamedTuple without tuple type"
        fallback = Instance(info, [])
        return TupleType(info.tuple_type.items, fallback=fallback)
    typ = self.expr_to_unanalyzed_type(expr)
    return self.anal_type(
        typ, report_invalid_types=report_invalid_types, allow_placeholder=allow_placeholder
    )

</t>
<t tx="ekr.20221004064035.1466">def analyze_type_expr(self, expr: Expression) -&gt; None:
    # There are certain expressions that mypy does not need to semantically analyze,
    # since they analyzed solely as type. (For example, indexes in type alias definitions
    # and base classes in class defs). External consumers of the mypy AST may need
    # them semantically analyzed, however, if they need to treat it as an expression
    # and not a type. (Which is to say, mypyc needs to do this.) Do the analysis
    # in a fresh tvar scope in order to suppress any errors about using type variables.
    with self.tvar_scope_frame(TypeVarLikeScope()):
        expr.accept(self)

</t>
<t tx="ekr.20221004064035.1467">def type_analyzer(
    self,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_placeholder: bool = False,
    allow_required: bool = False,
    allow_param_spec_literals: bool = False,
    report_invalid_types: bool = True,
) -&gt; TypeAnalyser:
    if tvar_scope is None:
        tvar_scope = self.tvar_scope
    tpan = TypeAnalyser(
        self,
        tvar_scope,
        self.plugin,
        self.options,
        self.is_typeshed_stub_file,
        allow_unbound_tvars=allow_unbound_tvars,
        allow_tuple_literal=allow_tuple_literal,
        report_invalid_types=report_invalid_types,
        allow_placeholder=allow_placeholder,
        allow_required=allow_required,
        allow_param_spec_literals=allow_param_spec_literals,
    )
    tpan.in_dynamic_func = bool(self.function_stack and self.function_stack[-1].is_dynamic())
    tpan.global_scope = not self.type and not self.function_stack
    return tpan

</t>
<t tx="ekr.20221004064035.1468">def expr_to_unanalyzed_type(self, node: Expression) -&gt; ProperType:
    return expr_to_unanalyzed_type(node, self.options, self.is_stub_file)

</t>
<t tx="ekr.20221004064035.1469">def anal_type(
    self,
    typ: Type,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_placeholder: bool = False,
    allow_required: bool = False,
    allow_param_spec_literals: bool = False,
    report_invalid_types: bool = True,
    third_pass: bool = False,
) -&gt; Type | None:
    """Semantically analyze a type.

    Args:
        typ: Type to analyze (if already analyzed, this is a no-op)
        allow_placeholder: If True, may return PlaceholderType if
            encountering an incomplete definition
        third_pass: Unused; only for compatibility with old semantic
            analyzer

    Return None only if some part of the type couldn't be bound *and* it
    referred to an incomplete namespace or definition. In this case also
    defer as needed. During a final iteration this won't return None;
    instead report an error if the type can't be analyzed and return
    AnyType.

    In case of other errors, report an error message and return AnyType.

    NOTE: The caller shouldn't defer even if this returns None or a
          placeholder type.
    """
    a = self.type_analyzer(
        tvar_scope=tvar_scope,
        allow_unbound_tvars=allow_unbound_tvars,
        allow_tuple_literal=allow_tuple_literal,
        allow_placeholder=allow_placeholder,
        allow_required=allow_required,
        allow_param_spec_literals=allow_param_spec_literals,
        report_invalid_types=report_invalid_types,
    )
    tag = self.track_incomplete_refs()
    typ = typ.accept(a)
    if self.found_incomplete_ref(tag):
        # Something could not be bound yet.
        return None
    self.add_type_alias_deps(a.aliases_used)
    return typ

</t>
<t tx="ekr.20221004064035.147">def read_types_packages_to_install(cache_dir: str, after_run: bool) -&gt; list[str]:
    if not os.path.isdir(cache_dir):
        if not after_run:
            sys.stderr.write(
                "error: Can't determine which types to install with no files to check "
                + "(and no cache from previous mypy run)\n"
            )
        else:
            sys.stderr.write("error: --install-types failed (no mypy cache directory)\n")
        sys.exit(2)
    fnam = build.missing_stubs_file(cache_dir)
    if not os.path.isfile(fnam):
        # No missing stubs.
        return []
    with open(fnam) as f:
        return [line.strip() for line in f.readlines()]


</t>
<t tx="ekr.20221004064035.1470">def class_type(self, self_type: Type) -&gt; Type:
    return TypeType.make_normalized(self_type)

</t>
<t tx="ekr.20221004064035.1471">def schedule_patch(self, priority: int, patch: Callable[[], None]) -&gt; None:
    self.patches.append((priority, patch))

</t>
<t tx="ekr.20221004064035.1472">def report_hang(self) -&gt; None:
    print("Deferral trace:")
    for mod, line in self.deferral_debug_context:
        print(f"    {mod}:{line}")
    self.errors.report(
        -1,
        -1,
        "INTERNAL ERROR: maximum semantic analysis iteration count reached",
        blocker=True,
    )

</t>
<t tx="ekr.20221004064035.1473">def add_plugin_dependency(self, trigger: str, target: str | None = None) -&gt; None:
    """Add dependency from trigger to a target.

    If the target is not given explicitly, use the current target.
    """
    if target is None:
        target = self.scope.current_target()
    self.cur_mod_node.plugin_deps.setdefault(trigger, set()).add(target)

</t>
<t tx="ekr.20221004064035.1474">def add_type_alias_deps(self, aliases_used: Iterable[str], target: str | None = None) -&gt; None:
    """Add full names of type aliases on which the current node depends.

    This is used by fine-grained incremental mode to re-check the corresponding nodes.
    If `target` is None, then the target node used will be the current scope.
    """
    if not aliases_used:
        # A basic optimization to avoid adding targets with no dependencies to
        # the `alias_deps` dict.
        return
    if target is None:
        target = self.scope.current_target()
    self.cur_mod_node.alias_deps[target].update(aliases_used)

</t>
<t tx="ekr.20221004064035.1475">def is_mangled_global(self, name: str) -&gt; bool:
    # A global is mangled if there exists at least one renamed variant.
    return unmangle(name) + "'" in self.globals

</t>
<t tx="ekr.20221004064035.1476">def is_initial_mangled_global(self, name: str) -&gt; bool:
    # If there are renamed definitions for a global, the first one has exactly one prime.
    return name == unmangle(name) + "'"

</t>
<t tx="ekr.20221004064035.1477">def parse_bool(self, expr: Expression) -&gt; bool | None:
    if isinstance(expr, NameExpr):
        if expr.fullname == "builtins.True":
            return True
        if expr.fullname == "builtins.False":
            return False
    return None

</t>
<t tx="ekr.20221004064035.1478">def set_future_import_flags(self, module_name: str) -&gt; None:
    if module_name in FUTURE_IMPORTS:
        self.modules[self.cur_mod_id].future_import_flags.add(FUTURE_IMPORTS[module_name])

</t>
<t tx="ekr.20221004064035.1479">def is_future_flag_set(self, flag: str) -&gt; bool:
    return self.modules[self.cur_mod_id].is_future_flag_set(flag)


</t>
<t tx="ekr.20221004064035.148">def install_types(
    formatter: util.FancyFormatter,
    options: Options,
    *,
    after_run: bool = False,
    non_interactive: bool = False,
) -&gt; bool:
    """Install stub packages using pip if some missing stubs were detected."""
    packages = read_types_packages_to_install(options.cache_dir, after_run)
    if not packages:
        # If there are no missing stubs, generate no output.
        return False
    if after_run and not non_interactive:
        print()
    print("Installing missing stub packages:")
    assert options.python_executable, "Python executable required to install types"
    cmd = [options.python_executable, "-m", "pip", "install"] + packages
    print(formatter.style(" ".join(cmd), "none", bold=True))
    print()
    if not non_interactive:
        x = input("Install? [yN] ")
        if not x.strip() or not x.lower().startswith("y"):
            print(formatter.style("mypy: Skipping installation", "red", bold=True))
            sys.exit(2)
        print()
    subprocess.run(cmd)
    return True
</t>
<t tx="ekr.20221004064035.1480">def replace_implicit_first_type(sig: FunctionLike, new: Type) -&gt; FunctionLike:
    if isinstance(sig, CallableType):
        if len(sig.arg_types) == 0:
            return sig
        return sig.copy_modified(arg_types=[new] + sig.arg_types[1:])
    elif isinstance(sig, Overloaded):
        return Overloaded(
            [cast(CallableType, replace_implicit_first_type(i, new)) for i in sig.items]
        )
    else:
        assert False


</t>
<t tx="ekr.20221004064035.1481">def refers_to_fullname(node: Expression, fullnames: str | tuple[str, ...]) -&gt; bool:
    """Is node a name or member expression with the given full name?"""
    if not isinstance(fullnames, tuple):
        fullnames = (fullnames,)

    if not isinstance(node, RefExpr):
        return False
    if node.fullname in fullnames:
        return True
    if isinstance(node.node, TypeAlias):
        return is_named_instance(node.node.target, fullnames)
    return False


</t>
<t tx="ekr.20221004064035.1482">def refers_to_class_or_function(node: Expression) -&gt; bool:
    """Does semantically analyzed node refer to a class?"""
    return isinstance(node, RefExpr) and isinstance(
        node.node, (TypeInfo, FuncDef, OverloadedFuncDef)
    )


</t>
<t tx="ekr.20221004064035.1483">def find_duplicate(list: list[T]) -&gt; T | None:
    """If the list has duplicates, return one of the duplicates.

    Otherwise, return None.
    """
    for i in range(1, len(list)):
        if list[i] in list[:i]:
            return list[i]
    return None


</t>
<t tx="ekr.20221004064035.1484">def remove_imported_names_from_symtable(names: SymbolTable, module: str) -&gt; None:
    """Remove all imported names from the symbol table of a module."""
    removed: list[str] = []
    for name, node in names.items():
        if node.node is None:
            continue
        fullname = node.node.fullname
        prefix = fullname[: fullname.rfind(".")]
        if prefix != module:
            removed.append(name)
    for name in removed:
        del names[name]


</t>
<t tx="ekr.20221004064035.1485">def make_any_non_explicit(t: Type) -&gt; Type:
    """Replace all Any types within in with Any that has attribute 'explicit' set to False"""
    return t.accept(MakeAnyNonExplicit())


</t>
<t tx="ekr.20221004064035.1486">class MakeAnyNonExplicit(TrivialSyntheticTypeTranslator):
    @others
</t>
<t tx="ekr.20221004064035.1487">def visit_any(self, t: AnyType) -&gt; Type:
    if t.type_of_any == TypeOfAny.explicit:
        return t.copy_modified(TypeOfAny.special_form)
    return t

</t>
<t tx="ekr.20221004064035.1488">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20221004064035.1489">def apply_semantic_analyzer_patches(patches: list[tuple[int, Callable[[], None]]]) -&gt; None:
    """Call patch callbacks in the right order.

    This should happen after semantic analyzer pass 3.
    """
    patches_by_priority = sorted(patches, key=lambda x: x[0])
    for priority, patch_func in patches_by_priority:
        patch_func()


</t>
<t tx="ekr.20221004064035.149">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import mypy.typeops
from mypy.expandtype import expand_type
from mypy.nodes import TypeInfo
from mypy.types import AnyType, Instance, TupleType, Type, TypeOfAny, TypeVarId, has_type_vars


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1490">def names_modified_by_assignment(s: AssignmentStmt) -&gt; list[NameExpr]:
    """Return all unqualified (short) names assigned to in an assignment statement."""
    result: list[NameExpr] = []
    for lvalue in s.lvalues:
        result += names_modified_in_lvalue(lvalue)
    return result


</t>
<t tx="ekr.20221004064035.1491">def names_modified_in_lvalue(lvalue: Lvalue) -&gt; list[NameExpr]:
    """Return all NameExpr assignment targets in an Lvalue."""
    if isinstance(lvalue, NameExpr):
        return [lvalue]
    elif isinstance(lvalue, StarExpr):
        return names_modified_in_lvalue(lvalue.expr)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        result: list[NameExpr] = []
        for item in lvalue.items:
            result += names_modified_in_lvalue(item)
        return result
    return []


</t>
<t tx="ekr.20221004064035.1492">def is_same_var_from_getattr(n1: SymbolNode | None, n2: SymbolNode | None) -&gt; bool:
    """Do n1 and n2 refer to the same Var derived from module-level __getattr__?"""
    return (
        isinstance(n1, Var)
        and n1.from_module_getattr
        and isinstance(n2, Var)
        and n2.from_module_getattr
        and n1.fullname == n2.fullname
    )


</t>
<t tx="ekr.20221004064035.1493">def dummy_context() -&gt; Context:
    return TempNode(AnyType(TypeOfAny.special_form))


</t>
<t tx="ekr.20221004064035.1494">def is_valid_replacement(old: SymbolTableNode, new: SymbolTableNode) -&gt; bool:
    """Can symbol table node replace an existing one?

    These are the only valid cases:

    1. Placeholder gets replaced with a non-placeholder
    2. Placeholder that isn't known to become type replaced with a
       placeholder that can become a type
    """
    if isinstance(old.node, PlaceholderNode):
        if isinstance(new.node, PlaceholderNode):
            return not old.node.becomes_typeinfo and new.node.becomes_typeinfo
        else:
            return True
    return False


</t>
<t tx="ekr.20221004064035.1495">def is_same_symbol(a: SymbolNode | None, b: SymbolNode | None) -&gt; bool:
    return (
        a == b
        or (isinstance(a, PlaceholderNode) and isinstance(b, PlaceholderNode))
        or is_same_var_from_getattr(a, b)
    )


</t>
<t tx="ekr.20221004064035.1496">def is_trivial_body(block: Block) -&gt; bool:
    """Returns 'true' if the given body is "trivial" -- if it contains just a "pass",
    "..." (ellipsis), or "raise NotImplementedError()". A trivial body may also
    start with a statement containing just a string (e.g. a docstring).

    Note: functions that raise other kinds of exceptions do not count as
    "trivial". We use this function to help us determine when it's ok to
    relax certain checks on body, but functions that raise arbitrary exceptions
    are more likely to do non-trivial work. For example:

       def halt(self, reason: str = ...) -&gt; NoReturn:
           raise MyCustomError("Fatal error: " + reason, self.line, self.context)

    A function that raises just NotImplementedError is much less likely to be
    this complex.
    """
    body = block.body

    # Skip a docstring
    if body and isinstance(body[0], ExpressionStmt) and isinstance(body[0].expr, StrExpr):
        body = block.body[1:]

    if len(body) == 0:
        # There's only a docstring (or no body at all).
        return True
    elif len(body) &gt; 1:
        return False

    stmt = body[0]

    if isinstance(stmt, RaiseStmt):
        expr = stmt.expr
        if expr is None:
            return False
        if isinstance(expr, CallExpr):
            expr = expr.callee

        return isinstance(expr, NameExpr) and expr.fullname == "builtins.NotImplementedError"

    return isinstance(stmt, PassStmt) or (
        isinstance(stmt, ExpressionStmt) and isinstance(stmt.expr, EllipsisExpr)
    )
</t>
<t tx="ekr.20221004064035.1497">@path C:/Repos/ekr-mypy2/mypy/
"""Calculate some properties of classes.

These happen after semantic analysis and before type checking.
"""

from __future__ import annotations

from typing_extensions import Final

from mypy.errors import Errors
from mypy.nodes import (
    IMPLICITLY_ABSTRACT,
    IS_ABSTRACT,
    CallExpr,
    Decorator,
    FuncDef,
    Node,
    OverloadedFuncDef,
    PromoteExpr,
    SymbolTable,
    TypeInfo,
    Var,
)
from mypy.options import Options
from mypy.types import Instance, Type

# Hard coded type promotions (shared between all Python versions).
# These add extra ad-hoc edges to the subtyping relation. For example,
# int is considered a subtype of float, even though there is no
# subclass relationship.
# Note that the bytearray -&gt; bytes promotion is a little unsafe
# as some functions only accept bytes objects. Here convenience
# trumps safety.
TYPE_PROMOTIONS: Final = {
    "builtins.int": "float",
    "builtins.float": "complex",
    "builtins.bytearray": "bytes",
    "builtins.memoryview": "bytes",
}


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1498">def calculate_class_abstract_status(typ: TypeInfo, is_stub_file: bool, errors: Errors) -&gt; None:
    """Calculate abstract status of a class.

    Set is_abstract of the type to True if the type has an unimplemented
    abstract attribute.  Also compute a list of abstract attributes.
    Report error is required ABCMeta metaclass is missing.
    """
    if typ.typeddict_type:
        return  # TypedDict can't be abstract
    concrete: set[str] = set()
    # List of abstract attributes together with their abstract status
    abstract: list[tuple[str, int]] = []
    abstract_in_this_class: list[str] = []
    if typ.is_newtype:
        # Special case: NewTypes are considered as always non-abstract, so they can be used as:
        #     Config = NewType('Config', Mapping[str, str])
        #     default = Config({'cannot': 'modify'})  # OK
        typ.abstract_attributes = []
        return
    for base in typ.mro:
        for name, symnode in base.names.items():
            node = symnode.node
            if isinstance(node, OverloadedFuncDef):
                # Unwrap an overloaded function definition. We can just
                # check arbitrarily the first overload item. If the
                # different items have a different abstract status, there
                # should be an error reported elsewhere.
                if node.items:  # can be empty for invalid overloads
                    func: Node | None = node.items[0]
                else:
                    func = None
            else:
                func = node
            if isinstance(func, Decorator):
                func = func.func
            if isinstance(func, FuncDef):
                if (
                    func.abstract_status in (IS_ABSTRACT, IMPLICITLY_ABSTRACT)
                    and name not in concrete
                ):
                    typ.is_abstract = True
                    abstract.append((name, func.abstract_status))
                    if base is typ:
                        abstract_in_this_class.append(name)
            elif isinstance(node, Var):
                if node.is_abstract_var and name not in concrete:
                    typ.is_abstract = True
                    abstract.append((name, IS_ABSTRACT))
                    if base is typ:
                        abstract_in_this_class.append(name)
            concrete.add(name)
    # In stubs, abstract classes need to be explicitly marked because it is too
    # easy to accidentally leave a concrete class abstract by forgetting to
    # implement some methods.
    typ.abstract_attributes = sorted(abstract)
    if is_stub_file:
        if typ.declared_metaclass and typ.declared_metaclass.type.has_base("abc.ABCMeta"):
            return
        if typ.is_protocol:
            return
        if abstract and not abstract_in_this_class:

            def report(message: str, severity: str) -&gt; None:
                errors.report(typ.line, typ.column, message, severity=severity)

            attrs = ", ".join(f'"{attr}"' for attr, _ in sorted(abstract))
            report(f"Class {typ.fullname} has abstract attributes {attrs}", "error")
            report(
                "If it is meant to be abstract, add 'abc.ABCMeta' as an explicit metaclass", "note"
            )
    if typ.is_final and abstract:
        attrs = ", ".join(f'"{attr}"' for attr, _ in sorted(abstract))
        errors.report(
            typ.line, typ.column, f"Final class {typ.fullname} has abstract attributes {attrs}"
        )


</t>
<t tx="ekr.20221004064035.1499">def check_protocol_status(info: TypeInfo, errors: Errors) -&gt; None:
    """Check that all classes in MRO of a protocol are protocols"""
    if info.is_protocol:
        for type in info.bases:
            if not type.type.is_protocol and type.type.fullname != "builtins.object":

                def report(message: str, severity: str) -&gt; None:
                    errors.report(info.line, info.column, message, severity=severity)

                report("All bases of a protocol must be protocols", "error")


</t>
<t tx="ekr.20221004064035.15">def cleanup(self) -&gt; None:
    if sys.platform == "win32":
        self.close()
    else:
        shutil.rmtree(self.sock_directory)

</t>
<t tx="ekr.20221004064035.150">def map_instance_to_supertype(instance: Instance, superclass: TypeInfo) -&gt; Instance:
    """Produce a supertype of `instance` that is an Instance
    of `superclass`, mapping type arguments up the chain of bases.

    If `superclass` is not a nominal superclass of `instance.type`,
    then all type arguments are mapped to 'Any'.
    """
    if instance.type == superclass:
        # Fast path: `instance` already belongs to `superclass`.
        return instance

    if superclass.fullname == "builtins.tuple" and instance.type.tuple_type:
        if has_type_vars(instance.type.tuple_type):
            # We special case mapping generic tuple types to tuple base, because for
            # such tuples fallback can't be calculated before applying type arguments.
            alias = instance.type.special_alias
            assert alias is not None
            if not alias._is_recursive:
                # Unfortunately we can't support this for generic recursive tuples.
                # If we skip this special casing we will fall back to tuple[Any, ...].
                env = instance_to_type_environment(instance)
                tuple_type = expand_type(instance.type.tuple_type, env)
                if isinstance(tuple_type, TupleType):
                    return mypy.typeops.tuple_fallback(tuple_type)

    if not superclass.type_vars:
        # Fast path: `superclass` has no type variables to map to.
        return Instance(superclass, [])

    return map_instance_to_supertypes(instance, superclass)[0]


</t>
<t tx="ekr.20221004064035.1500">def calculate_class_vars(info: TypeInfo) -&gt; None:
    """Try to infer additional class variables.

    Subclass attribute assignments with no type annotation are assumed
    to be classvar if overriding a declared classvar from the base
    class.

    This must happen after the main semantic analysis pass, since
    this depends on base class bodies having been fully analyzed.
    """
    for name, sym in info.names.items():
        node = sym.node
        if isinstance(node, Var) and node.info and node.is_inferred and not node.is_classvar:
            for base in info.mro[1:]:
                member = base.names.get(name)
                if member is not None and isinstance(member.node, Var) and member.node.is_classvar:
                    node.is_classvar = True


</t>
<t tx="ekr.20221004064035.1501">def add_type_promotion(
    info: TypeInfo, module_names: SymbolTable, options: Options, builtin_names: SymbolTable
) -&gt; None:
    """Setup extra, ad-hoc subtyping relationships between classes (promotion).

    This includes things like 'int' being compatible with 'float'.
    """
    defn = info.defn
    promote_targets: list[Type] = []
    for decorator in defn.decorators:
        if isinstance(decorator, CallExpr):
            analyzed = decorator.analyzed
            if isinstance(analyzed, PromoteExpr):
                # _promote class decorator (undocumented feature).
                promote_targets.append(analyzed.type)
    if not promote_targets:
        if defn.fullname in TYPE_PROMOTIONS:
            target_sym = module_names.get(TYPE_PROMOTIONS[defn.fullname])
            # With test stubs, the target may not exist.
            if target_sym:
                target_info = target_sym.node
                assert isinstance(target_info, TypeInfo)
                promote_targets.append(Instance(target_info, []))
    # Special case the promotions between 'int' and native integer types.
    # These have promotions going both ways, such as from 'int' to 'i64'
    # and 'i64' to 'int', for convenience.
    if defn.fullname == "mypy_extensions.i64" or defn.fullname == "mypy_extensions.i32":
        int_sym = builtin_names["int"]
        assert isinstance(int_sym.node, TypeInfo)
        int_sym.node._promote.append(Instance(defn.info, []))
        defn.info.alt_promote = int_sym.node
    if promote_targets:
        defn.info._promote.extend(promote_targets)
</t>
<t tx="ekr.20221004064035.1502">@path C:/Repos/ekr-mypy2/mypy/
"""Semantic analysis of call-based Enum definitions.

This is conceptually part of mypy.semanal (semantic analyzer pass 2).
"""

from __future__ import annotations

from typing import cast
from typing_extensions import Final

from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    MDEF,
    AssignmentStmt,
    CallExpr,
    Context,
    DictExpr,
    EnumCallExpr,
    Expression,
    ListExpr,
    MemberExpr,
    NameExpr,
    RefExpr,
    StrExpr,
    SymbolTableNode,
    TupleExpr,
    TypeInfo,
    Var,
)
from mypy.options import Options
from mypy.semanal_shared import SemanticAnalyzerInterface
from mypy.types import ENUM_REMOVED_PROPS, LiteralType, get_proper_type

# Note: 'enum.EnumMeta' is deliberately excluded from this list. Classes that directly use
# enum.EnumMeta do not necessarily automatically have the 'name' and 'value' attributes.
ENUM_BASES: Final = frozenset(
    ("enum.Enum", "enum.IntEnum", "enum.Flag", "enum.IntFlag", "enum.StrEnum")
)
ENUM_SPECIAL_PROPS: Final = frozenset(
    (
        "name",
        "value",
        "_name_",
        "_value_",
        *ENUM_REMOVED_PROPS,
        # Also attributes from `object`:
        "__module__",
        "__annotations__",
        "__doc__",
        "__slots__",
        "__dict__",
    )
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1503">class EnumCallAnalyzer:
    @others
</t>
<t tx="ekr.20221004064035.1504">def __init__(self, options: Options, api: SemanticAnalyzerInterface) -&gt; None:
    self.options = options
    self.api = api

</t>
<t tx="ekr.20221004064035.1505">def process_enum_call(self, s: AssignmentStmt, is_func_scope: bool) -&gt; bool:
    """Check if s defines an Enum; if yes, store the definition in symbol table.

    Return True if this looks like an Enum definition (but maybe with errors),
    otherwise return False.
    """
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    enum_call = self.check_enum_call(s.rvalue, name, is_func_scope)
    if enum_call is None:
        return False
    if isinstance(lvalue, MemberExpr):
        self.fail("Enum type as attribute is not supported", lvalue)
        return False
    # Yes, it's a valid Enum definition. Add it to the symbol table.
    self.api.add_symbol(name, enum_call, s)
    return True

</t>
<t tx="ekr.20221004064035.1506">def check_enum_call(
    self, node: Expression, var_name: str, is_func_scope: bool
) -&gt; TypeInfo | None:
    """Check if a call defines an Enum.

    Example:

      A = enum.Enum('A', 'foo bar')

    is equivalent to:

      class A(enum.Enum):
          foo = 1
          bar = 2
    """
    if not isinstance(node, CallExpr):
        return None
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None
    fullname = callee.fullname
    if fullname not in ENUM_BASES:
        return None
    items, values, ok = self.parse_enum_call_args(call, fullname.split(".")[-1])
    if not ok:
        # Error. Construct dummy return value.
        info = self.build_enum_call_typeinfo(var_name, [], fullname, node.line)
    else:
        name = cast(StrExpr, call.args[0]).value
        if name != var_name or is_func_scope:
            # Give it a unique name derived from the line number.
            name += "@" + str(call.line)
        info = self.build_enum_call_typeinfo(name, items, fullname, call.line)
        # Store generated TypeInfo under both names, see semanal_namedtuple for more details.
        if name != var_name or is_func_scope:
            self.api.add_symbol_skip_local(name, info)
    call.analyzed = EnumCallExpr(info, items, values)
    call.analyzed.set_line(call)
    info.line = node.line
    return info

</t>
<t tx="ekr.20221004064035.1507">def build_enum_call_typeinfo(
    self, name: str, items: list[str], fullname: str, line: int
) -&gt; TypeInfo:
    base = self.api.named_type_or_none(fullname)
    assert base is not None
    info = self.api.basic_new_typeinfo(name, base, line)
    info.metaclass_type = info.calculate_metaclass_type()
    info.is_enum = True
    for item in items:
        var = Var(item)
        var.info = info
        var.is_property = True
        var._fullname = f"{info.fullname}.{item}"
        info.names[item] = SymbolTableNode(MDEF, var)
    return info

</t>
<t tx="ekr.20221004064035.1508">def parse_enum_call_args(
    self, call: CallExpr, class_name: str
) -&gt; tuple[list[str], list[Expression | None], bool]:
    """Parse arguments of an Enum call.

    Return a tuple of fields, values, was there an error.
    """
    args = call.args
    if not all([arg_kind in [ARG_POS, ARG_NAMED] for arg_kind in call.arg_kinds]):
        return self.fail_enum_call_arg(f"Unexpected arguments to {class_name}()", call)
    if len(args) &lt; 2:
        return self.fail_enum_call_arg(f"Too few arguments for {class_name}()", call)
    if len(args) &gt; 6:
        return self.fail_enum_call_arg(f"Too many arguments for {class_name}()", call)
    valid_name = [None, "value", "names", "module", "qualname", "type", "start"]
    for arg_name in call.arg_names:
        if arg_name not in valid_name:
            self.fail_enum_call_arg(f'Unexpected keyword argument "{arg_name}"', call)
    value, names = None, None
    for arg_name, arg in zip(call.arg_names, args):
        if arg_name == "value":
            value = arg
        if arg_name == "names":
            names = arg
    if value is None:
        value = args[0]
    if names is None:
        names = args[1]
    if not isinstance(value, StrExpr):
        return self.fail_enum_call_arg(
            f"{class_name}() expects a string literal as the first argument", call
        )
    items = []
    values: list[Expression | None] = []
    if isinstance(names, StrExpr):
        fields = names.value
        for field in fields.replace(",", " ").split():
            items.append(field)
    elif isinstance(names, (TupleExpr, ListExpr)):
        seq_items = names.items
        if all(isinstance(seq_item, StrExpr) for seq_item in seq_items):
            items = [cast(StrExpr, seq_item).value for seq_item in seq_items]
        elif all(
            isinstance(seq_item, (TupleExpr, ListExpr))
            and len(seq_item.items) == 2
            and isinstance(seq_item.items[0], StrExpr)
            for seq_item in seq_items
        ):
            for seq_item in seq_items:
                assert isinstance(seq_item, (TupleExpr, ListExpr))
                name, value = seq_item.items
                assert isinstance(name, StrExpr)
                items.append(name.value)
                values.append(value)
        else:
            return self.fail_enum_call_arg(
                "%s() with tuple or list expects strings or (name, value) pairs" % class_name,
                call,
            )
    elif isinstance(names, DictExpr):
        for key, value in names.items:
            if not isinstance(key, StrExpr):
                return self.fail_enum_call_arg(
                    f"{class_name}() with dict literal requires string literals", call
                )
            items.append(key.value)
            values.append(value)
    elif isinstance(args[1], RefExpr) and isinstance(args[1].node, Var):
        proper_type = get_proper_type(args[1].node.type)
        if (
            proper_type is not None
            and isinstance(proper_type, LiteralType)
            and isinstance(proper_type.value, str)
        ):
            fields = proper_type.value
            for field in fields.replace(",", " ").split():
                items.append(field)
        elif args[1].node.is_final and isinstance(args[1].node.final_value, str):
            fields = args[1].node.final_value
            for field in fields.replace(",", " ").split():
                items.append(field)
        else:
            return self.fail_enum_call_arg(
                "%s() expects a string, tuple, list or dict literal as the second argument"
                % class_name,
                call,
            )
    else:
        # TODO: Allow dict(x=1, y=2) as a substitute for {'x': 1, 'y': 2}?
        return self.fail_enum_call_arg(
            "%s() expects a string, tuple, list or dict literal as the second argument"
            % class_name,
            call,
        )
    if len(items) == 0:
        return self.fail_enum_call_arg(f"{class_name}() needs at least one item", call)
    if not values:
        values = [None] * len(items)
    assert len(items) == len(values)
    return items, values, True

</t>
<t tx="ekr.20221004064035.1509">def fail_enum_call_arg(
    self, message: str, context: Context
) -&gt; tuple[list[str], list[Expression | None], bool]:
    self.fail(message, context)
    return [], [], False

</t>
<t tx="ekr.20221004064035.151">def map_instance_to_supertypes(instance: Instance, supertype: TypeInfo) -&gt; list[Instance]:
    # FIX: Currently we should only have one supertype per interface, so no
    #      need to return an array
    result: list[Instance] = []
    for path in class_derivation_paths(instance.type, supertype):
        types = [instance]
        for sup in path:
            a: list[Instance] = []
            for t in types:
                a.extend(map_instance_to_direct_supertypes(t, sup))
            types = a
        result.extend(types)
    if result:
        return result
    else:
        # Nothing. Presumably due to an error. Construct a dummy using Any.
        any_type = AnyType(TypeOfAny.from_error)
        return [Instance(supertype, [any_type] * len(supertype.type_vars))]


</t>
<t tx="ekr.20221004064035.1510"># Helpers

</t>
<t tx="ekr.20221004064035.1511">def fail(self, msg: str, ctx: Context) -&gt; None:
    self.api.fail(msg, ctx)
</t>
<t tx="ekr.20221004064035.1512">@path C:/Repos/ekr-mypy2/mypy/
"""Simple type inference for decorated functions during semantic analysis."""

from __future__ import annotations

from mypy.nodes import ARG_POS, CallExpr, Decorator, Expression, FuncDef, RefExpr, Var
from mypy.semanal_shared import SemanticAnalyzerInterface
from mypy.typeops import function_type
from mypy.types import (
    AnyType,
    CallableType,
    ProperType,
    Type,
    TypeOfAny,
    TypeVarType,
    get_proper_type,
)
from mypy.typevars import has_no_typevars


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1513">def infer_decorator_signature_if_simple(
    dec: Decorator, analyzer: SemanticAnalyzerInterface
) -&gt; None:
    """Try to infer the type of the decorated function.

    This lets us resolve additional references to decorated functions
    during type checking. Otherwise the type might not be available
    when we need it, since module top levels can't be deferred.

    This basically uses a simple special-purpose type inference
    engine just for decorators.
    """
    if dec.var.is_property:
        # Decorators are expected to have a callable type (it's a little odd).
        if dec.func.type is None:
            dec.var.type = CallableType(
                [AnyType(TypeOfAny.special_form)],
                [ARG_POS],
                [None],
                AnyType(TypeOfAny.special_form),
                analyzer.named_type("builtins.function"),
                name=dec.var.name,
            )
        elif isinstance(dec.func.type, CallableType):
            dec.var.type = dec.func.type
        return
    decorator_preserves_type = True
    for expr in dec.decorators:
        preserve_type = False
        if isinstance(expr, RefExpr) and isinstance(expr.node, FuncDef):
            if expr.node.type and is_identity_signature(expr.node.type):
                preserve_type = True
        if not preserve_type:
            decorator_preserves_type = False
            break
    if decorator_preserves_type:
        # No non-identity decorators left. We can trivially infer the type
        # of the function here.
        dec.var.type = function_type(dec.func, analyzer.named_type("builtins.function"))
    if dec.decorators:
        return_type = calculate_return_type(dec.decorators[0])
        if return_type and isinstance(return_type, AnyType):
            # The outermost decorator will return Any so we know the type of the
            # decorated function.
            dec.var.type = AnyType(TypeOfAny.from_another_any, source_any=return_type)
        sig = find_fixed_callable_return(dec.decorators[0])
        if sig:
            # The outermost decorator always returns the same kind of function,
            # so we know that this is the type of the decorated function.
            orig_sig = function_type(dec.func, analyzer.named_type("builtins.function"))
            sig.name = orig_sig.items[0].name
            dec.var.type = sig


</t>
<t tx="ekr.20221004064035.1514">def is_identity_signature(sig: Type) -&gt; bool:
    """Is type a callable of form T -&gt; T (where T is a type variable)?"""
    sig = get_proper_type(sig)
    if isinstance(sig, CallableType) and sig.arg_kinds == [ARG_POS]:
        if isinstance(sig.arg_types[0], TypeVarType) and isinstance(sig.ret_type, TypeVarType):
            return sig.arg_types[0].id == sig.ret_type.id
    return False


</t>
<t tx="ekr.20221004064035.1515">def calculate_return_type(expr: Expression) -&gt; ProperType | None:
    """Return the return type if we can calculate it.

    This only uses information available during semantic analysis so this
    will sometimes return None because of insufficient information (as
    type inference hasn't run yet).
    """
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, FuncDef):
            typ = expr.node.type
            if typ is None:
                # No signature -&gt; default to Any.
                return AnyType(TypeOfAny.unannotated)
            # Explicit Any return?
            if isinstance(typ, CallableType):
                return get_proper_type(typ.ret_type)
            return None
        elif isinstance(expr.node, Var):
            return get_proper_type(expr.node.type)
    elif isinstance(expr, CallExpr):
        return calculate_return_type(expr.callee)
    return None


</t>
<t tx="ekr.20221004064035.1516">def find_fixed_callable_return(expr: Expression) -&gt; CallableType | None:
    """Return the return type, if expression refers to a callable that returns a callable.

    But only do this if the return type has no type variables. Return None otherwise.
    This approximates things a lot as this is supposed to be called before type checking
    when full type information is not available yet.
    """
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, FuncDef):
            typ = expr.node.type
            if typ:
                if isinstance(typ, CallableType) and has_no_typevars(typ.ret_type):
                    ret_type = get_proper_type(typ.ret_type)
                    if isinstance(ret_type, CallableType):
                        return ret_type
    elif isinstance(expr, CallExpr):
        t = find_fixed_callable_return(expr.callee)
        if t:
            ret_type = get_proper_type(t.ret_type)
            if isinstance(ret_type, CallableType):
                return ret_type
    return None
</t>
<t tx="ekr.20221004064035.1517">@path C:/Repos/ekr-mypy2/mypy/
"""Top-level logic for the semantic analyzer.

The semantic analyzer binds names, resolves imports, detects various
special constructs that don't have dedicated AST nodes after parse
(such as 'cast' which looks like a call), populates symbol tables, and
performs various simple consistency checks.

Semantic analysis of each SCC (strongly connected component; import
cycle) is performed in one unit. Each module is analyzed as multiple
separate *targets*; the module top level is one target and each function
is a target. Nested functions are not separate targets, however. This is
mostly identical to targets used by mypy daemon (but classes aren't
targets in semantic analysis).

We first analyze each module top level in an SCC. If we encounter some
names that we can't bind because the target of the name may not have
been processed yet, we *defer* the current target for further
processing. Deferred targets will be analyzed additional times until
everything can be bound, or we reach a maximum number of iterations.

We keep track of a set of incomplete namespaces, i.e. namespaces that we
haven't finished populating yet. References to these namespaces cause a
deferral if they can't be satisfied. Initially every module in the SCC
will be incomplete.
"""

from __future__ import annotations

from contextlib import nullcontext
from typing import TYPE_CHECKING, Callable, List, Optional, Tuple, Union
from typing_extensions import Final, TypeAlias as _TypeAlias

import mypy.build
import mypy.state
from mypy.checker import FineGrainedDeferredNode
from mypy.errors import Errors
from mypy.nodes import Decorator, FuncDef, MypyFile, OverloadedFuncDef, TypeInfo, Var
from mypy.options import Options
from mypy.plugin import ClassDefContext
from mypy.semanal import (
    SemanticAnalyzer,
    apply_semantic_analyzer_patches,
    remove_imported_names_from_symtable,
)
from mypy.semanal_classprop import (
    add_type_promotion,
    calculate_class_abstract_status,
    calculate_class_vars,
    check_protocol_status,
)
from mypy.semanal_infer import infer_decorator_signature_if_simple
from mypy.semanal_typeargs import TypeArgumentAnalyzer
from mypy.server.aststrip import SavedAttributes
from mypy.util import is_typeshed_file

if TYPE_CHECKING:
    from mypy.build import Graph, State


Patches: _TypeAlias = List[Tuple[int, Callable[[], None]]]


# If we perform this many iterations, raise an exception since we are likely stuck.
MAX_ITERATIONS: Final = 20


# Number of passes over core modules before going on to the rest of the builtin SCC.
CORE_WARMUP: Final = 2
core_modules: Final = ["typing", "builtins", "abc", "collections"]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1518">def semantic_analysis_for_scc(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    """Perform semantic analysis for all modules in a SCC (import cycle).

    Assume that reachability analysis has already been performed.

    The scc will be processed roughly in the order the modules are included
    in the list.
    """
    patches: Patches = []
    # Note that functions can't define new module-level attributes
    # using 'global x', since module top levels are fully processed
    # before functions. This limitation is unlikely to go away soon.
    process_top_levels(graph, scc, patches)
    process_functions(graph, scc, patches)
    # We use patch callbacks to fix up things when we expect relatively few
    # callbacks to be required.
    apply_semantic_analyzer_patches(patches)
    # Run class decorator hooks (they requite complete MROs and no placeholders).
    apply_class_plugin_hooks(graph, scc, errors)
    # This pass might need fallbacks calculated above and the results of hooks.
    check_type_arguments(graph, scc, errors)
    calculate_class_properties(graph, scc, errors)
    check_blockers(graph, scc)
    # Clean-up builtins, so that TypeVar etc. are not accessible without importing.
    if "builtins" in scc:
        cleanup_builtin_scc(graph["builtins"])


</t>
<t tx="ekr.20221004064035.1519">def cleanup_builtin_scc(state: State) -&gt; None:
    """Remove imported names from builtins namespace.

    This way names imported from typing in builtins.pyi aren't available
    by default (without importing them). We can only do this after processing
    the whole SCC is finished, when the imported names aren't needed for
    processing builtins.pyi itself.
    """
    assert state.tree is not None
    remove_imported_names_from_symtable(state.tree.names, "builtins")


</t>
<t tx="ekr.20221004064035.152">def class_derivation_paths(typ: TypeInfo, supertype: TypeInfo) -&gt; list[list[TypeInfo]]:
    """Return an array of non-empty paths of direct base classes from
    type to supertype.  Return [] if no such path could be found.

      InterfaceImplementationPaths(A, B) == [[B]] if A inherits B
      InterfaceImplementationPaths(A, C) == [[B, C]] if A inherits B and
                                                        B inherits C
    """
    # FIX: Currently we might only ever have a single path, so this could be
    #      simplified
    result: list[list[TypeInfo]] = []

    for base in typ.bases:
        btype = base.type
        if btype == supertype:
            result.append([btype])
        else:
            # Try constructing a longer path via the base class.
            for path in class_derivation_paths(btype, supertype):
                result.append([btype] + path)

    return result


</t>
<t tx="ekr.20221004064035.1520">def semantic_analysis_for_targets(
    state: State, nodes: list[FineGrainedDeferredNode], graph: Graph, saved_attrs: SavedAttributes
) -&gt; None:
    """Semantically analyze only selected nodes in a given module.

    This essentially mirrors the logic of semantic_analysis_for_scc()
    except that we process only some targets. This is used in fine grained
    incremental mode, when propagating an update.

    The saved_attrs are implicitly declared instance attributes (attributes
    defined on self) removed by AST stripper that may need to be reintroduced
    here.  They must be added before any methods are analyzed.
    """
    patches: Patches = []
    if any(isinstance(n.node, MypyFile) for n in nodes):
        # Process module top level first (if needed).
        process_top_levels(graph, [state.id], patches)
    restore_saved_attrs(saved_attrs)
    analyzer = state.manager.semantic_analyzer
    for n in nodes:
        if isinstance(n.node, MypyFile):
            # Already done above.
            continue
        process_top_level_function(
            analyzer, state, state.id, n.node.fullname, n.node, n.active_typeinfo, patches
        )
    apply_semantic_analyzer_patches(patches)
    apply_class_plugin_hooks(graph, [state.id], state.manager.errors)
    check_type_arguments_in_targets(nodes, state, state.manager.errors)
    calculate_class_properties(graph, [state.id], state.manager.errors)


</t>
<t tx="ekr.20221004064035.1521">def restore_saved_attrs(saved_attrs: SavedAttributes) -&gt; None:
    """Restore instance variables removed during AST strip that haven't been added yet."""
    for (cdef, name), sym in saved_attrs.items():
        info = cdef.info
        existing = info.get(name)
        defined_in_this_class = name in info.names
        assert isinstance(sym.node, Var)
        # This needs to mimic the logic in SemanticAnalyzer.analyze_member_lvalue()
        # regarding the existing variable in class body or in a superclass:
        # If the attribute of self is not defined in superclasses, create a new Var.
        if (
            existing is None
            or
            # (An abstract Var is considered as not defined.)
            (isinstance(existing.node, Var) and existing.node.is_abstract_var)
            or
            # Also an explicit declaration on self creates a new Var unless
            # there is already one defined in the class body.
            sym.node.explicit_self_type
            and not defined_in_this_class
        ):
            info.names[name] = sym


</t>
<t tx="ekr.20221004064035.1522">def process_top_levels(graph: Graph, scc: list[str], patches: Patches) -&gt; None:
    # Process top levels until everything has been bound.

    # Reverse order of the scc so the first modules in the original list will be
    # be processed first. This helps with performance.
    scc = list(reversed(scc))

    # Initialize ASTs and symbol tables.
    for id in scc:
        state = graph[id]
        assert state.tree is not None
        state.manager.semantic_analyzer.prepare_file(state.tree)

    # Initially all namespaces in the SCC are incomplete (well they are empty).
    state.manager.incomplete_namespaces.update(scc)

    worklist = scc[:]
    # HACK: process core stuff first. This is mostly needed to support defining
    # named tuples in builtin SCC.
    if all(m in worklist for m in core_modules):
        worklist += list(reversed(core_modules)) * CORE_WARMUP
    final_iteration = False
    iteration = 0
    analyzer = state.manager.semantic_analyzer
    analyzer.deferral_debug_context.clear()

    while worklist:
        iteration += 1
        if iteration &gt; MAX_ITERATIONS:
            # Just pick some module inside the current SCC for error context.
            assert state.tree is not None
            with analyzer.file_context(state.tree, state.options):
                analyzer.report_hang()
            break
        if final_iteration:
            # Give up. It's impossible to bind all names.
            state.manager.incomplete_namespaces.clear()
        all_deferred: list[str] = []
        any_progress = False
        while worklist:
            next_id = worklist.pop()
            state = graph[next_id]
            assert state.tree is not None
            deferred, incomplete, progress = semantic_analyze_target(
                next_id, state, state.tree, None, final_iteration, patches
            )
            all_deferred += deferred
            any_progress = any_progress or progress
            if not incomplete:
                state.manager.incomplete_namespaces.discard(next_id)
        if final_iteration:
            assert not all_deferred, "Must not defer during final iteration"
        # Reverse to process the targets in the same order on every iteration. This avoids
        # processing the same target twice in a row, which is inefficient.
        worklist = list(reversed(all_deferred))
        final_iteration = not any_progress


</t>
<t tx="ekr.20221004064035.1523">def process_functions(graph: Graph, scc: list[str], patches: Patches) -&gt; None:
    # Process functions.
    for module in scc:
        tree = graph[module].tree
        assert tree is not None
        analyzer = graph[module].manager.semantic_analyzer
        # In principle, functions can be processed in arbitrary order,
        # but _methods_ must be processed in the order they are defined,
        # because some features (most notably partial types) depend on
        # order of definitions on self.
        #
        # There can be multiple generated methods per line. Use target
        # name as the second sort key to get a repeatable sort order on
        # Python 3.5, which doesn't preserve dictionary order.
        targets = sorted(get_all_leaf_targets(tree), key=lambda x: (x[1].line, x[0]))
        for target, node, active_type in targets:
            assert isinstance(node, (FuncDef, OverloadedFuncDef, Decorator))
            process_top_level_function(
                analyzer, graph[module], module, target, node, active_type, patches
            )


</t>
<t tx="ekr.20221004064035.1524">def process_top_level_function(
    analyzer: SemanticAnalyzer,
    state: State,
    module: str,
    target: str,
    node: FuncDef | OverloadedFuncDef | Decorator,
    active_type: TypeInfo | None,
    patches: Patches,
) -&gt; None:
    """Analyze single top-level function or method.

    Process the body of the function (including nested functions) again and again,
    until all names have been resolved (or iteration limit reached).
    """
    # We need one more iteration after incomplete is False (e.g. to report errors, if any).
    final_iteration = False
    incomplete = True
    # Start in the incomplete state (no missing names will be reported on first pass).
    # Note that we use module name, since functions don't create qualified names.
    deferred = [module]
    analyzer.deferral_debug_context.clear()
    analyzer.incomplete_namespaces.add(module)
    iteration = 0
    while deferred:
        iteration += 1
        if iteration == MAX_ITERATIONS:
            # Just pick some module inside the current SCC for error context.
            assert state.tree is not None
            with analyzer.file_context(state.tree, state.options):
                analyzer.report_hang()
            break
        if not (deferred or incomplete) or final_iteration:
            # OK, this is one last pass, now missing names will be reported.
            analyzer.incomplete_namespaces.discard(module)
        deferred, incomplete, progress = semantic_analyze_target(
            target, state, node, active_type, final_iteration, patches
        )
        if final_iteration:
            assert not deferred, "Must not defer during final iteration"
        if not progress:
            final_iteration = True

    analyzer.incomplete_namespaces.discard(module)
    # After semantic analysis is done, discard local namespaces
    # to avoid memory hoarding.
    analyzer.saved_locals.clear()


</t>
<t tx="ekr.20221004064035.1525">TargetInfo: _TypeAlias = Tuple[
    str, Union[MypyFile, FuncDef, OverloadedFuncDef, Decorator], Optional[TypeInfo]
]


</t>
<t tx="ekr.20221004064035.1526">def get_all_leaf_targets(file: MypyFile) -&gt; list[TargetInfo]:
    """Return all leaf targets in a symbol table (module-level and methods)."""
    result: list[TargetInfo] = []
    for fullname, node, active_type in file.local_definitions():
        if isinstance(node.node, (FuncDef, OverloadedFuncDef, Decorator)):
            result.append((fullname, node.node, active_type))
    return result


</t>
<t tx="ekr.20221004064035.1527">def semantic_analyze_target(
    target: str,
    state: State,
    node: MypyFile | FuncDef | OverloadedFuncDef | Decorator,
    active_type: TypeInfo | None,
    final_iteration: bool,
    patches: Patches,
) -&gt; tuple[list[str], bool, bool]:
    """Semantically analyze a single target.

    Return tuple with these items:
    - list of deferred targets
    - was some definition incomplete (need to run another pass)
    - were any new names defined (or placeholders replaced)
    """
    state.manager.processed_targets.append(target)
    tree = state.tree
    assert tree is not None
    analyzer = state.manager.semantic_analyzer
    # TODO: Move initialization to somewhere else
    analyzer.global_decls = [set()]
    analyzer.nonlocal_decls = [set()]
    analyzer.globals = tree.names
    analyzer.progress = False
    with state.wrap_context(check_blockers=False):
        refresh_node = node
        if isinstance(refresh_node, Decorator):
            # Decorator expressions will be processed as part of the module top level.
            refresh_node = refresh_node.func
        analyzer.refresh_partial(
            refresh_node,
            patches,
            final_iteration,
            file_node=tree,
            options=state.options,
            active_type=active_type,
        )
        if isinstance(node, Decorator):
            infer_decorator_signature_if_simple(node, analyzer)
    for dep in analyzer.imports:
        state.add_dependency(dep)
        priority = mypy.build.PRI_LOW
        if priority &lt;= state.priorities.get(dep, priority):
            state.priorities[dep] = priority

    # Clear out some stale data to avoid memory leaks and astmerge
    # validity check confusion
    analyzer.statement = None
    del analyzer.cur_mod_node

    if analyzer.deferred:
        return [target], analyzer.incomplete, analyzer.progress
    else:
        return [], analyzer.incomplete, analyzer.progress


</t>
<t tx="ekr.20221004064035.1528">def check_type_arguments(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    for module in scc:
        state = graph[module]
        assert state.tree
        analyzer = TypeArgumentAnalyzer(
            errors,
            state.options,
            is_typeshed_file(state.options.abs_custom_typeshed_dir, state.path or ""),
        )
        with state.wrap_context():
            with mypy.state.state.strict_optional_set(state.options.strict_optional):
                state.tree.accept(analyzer)


</t>
<t tx="ekr.20221004064035.1529">def check_type_arguments_in_targets(
    targets: list[FineGrainedDeferredNode], state: State, errors: Errors
) -&gt; None:
    """Check type arguments against type variable bounds and restrictions.

    This mirrors the logic in check_type_arguments() except that we process only
    some targets. This is used in fine grained incremental mode.
    """
    analyzer = TypeArgumentAnalyzer(
        errors,
        state.options,
        is_typeshed_file(state.options.abs_custom_typeshed_dir, state.path or ""),
    )
    with state.wrap_context():
        with mypy.state.state.strict_optional_set(state.options.strict_optional):
            for target in targets:
                func: FuncDef | OverloadedFuncDef | None = None
                if isinstance(target.node, (FuncDef, OverloadedFuncDef)):
                    func = target.node
                saved = (state.id, target.active_typeinfo, func)  # module, class, function
                with errors.scope.saved_scope(saved) if errors.scope else nullcontext():
                    analyzer.recurse_into_functions = func is not None
                    target.node.accept(analyzer)


</t>
<t tx="ekr.20221004064035.153">def map_instance_to_direct_supertypes(instance: Instance, supertype: TypeInfo) -&gt; list[Instance]:
    # FIX: There should only be one supertypes, always.
    typ = instance.type
    result: list[Instance] = []

    for b in typ.bases:
        if b.type == supertype:
            env = instance_to_type_environment(instance)
            t = expand_type(b, env)
            assert isinstance(t, Instance)
            result.append(t)

    if result:
        return result
    else:
        # Relationship with the supertype not specified explicitly. Use dynamic
        # type arguments implicitly.
        any_type = AnyType(TypeOfAny.unannotated)
        return [Instance(supertype, [any_type] * len(supertype.type_vars))]


</t>
<t tx="ekr.20221004064035.1530">def apply_class_plugin_hooks(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    """Apply class plugin hooks within a SCC.

    We run these after to the main semantic analysis so that the hooks
    don't need to deal with incomplete definitions such as placeholder
    types.

    Note that some hooks incorrectly run during the main semantic
    analysis pass, for historical reasons.
    """
    num_passes = 0
    incomplete = True
    # If we encounter a base class that has not been processed, we'll run another
    # pass. This should eventually reach a fixed point.
    while incomplete:
        assert num_passes &lt; 10, "Internal error: too many class plugin hook passes"
        num_passes += 1
        incomplete = False
        for module in scc:
            state = graph[module]
            tree = state.tree
            assert tree
            for _, node, _ in tree.local_definitions():
                if isinstance(node.node, TypeInfo):
                    if not apply_hooks_to_class(
                        state.manager.semantic_analyzer,
                        module,
                        node.node,
                        state.options,
                        tree,
                        errors,
                    ):
                        incomplete = True


</t>
<t tx="ekr.20221004064035.1531">def apply_hooks_to_class(
    self: SemanticAnalyzer,
    module: str,
    info: TypeInfo,
    options: Options,
    file_node: MypyFile,
    errors: Errors,
) -&gt; bool:
    # TODO: Move more class-related hooks here?
    defn = info.defn
    ok = True
    for decorator in defn.decorators:
        with self.file_context(file_node, options, info):
            decorator_name = self.get_fullname_for_hook(decorator)
            if decorator_name:
                hook = self.plugin.get_class_decorator_hook_2(decorator_name)
                if hook:
                    ok = ok and hook(ClassDefContext(defn, decorator, self))
    return ok


</t>
<t tx="ekr.20221004064035.1532">def calculate_class_properties(graph: Graph, scc: list[str], errors: Errors) -&gt; None:
    builtins = graph["builtins"].tree
    assert builtins
    for module in scc:
        state = graph[module]
        tree = state.tree
        assert tree
        for _, node, _ in tree.local_definitions():
            if isinstance(node.node, TypeInfo):
                with state.manager.semantic_analyzer.file_context(tree, state.options, node.node):
                    calculate_class_abstract_status(node.node, tree.is_stub, errors)
                    check_protocol_status(node.node, errors)
                    calculate_class_vars(node.node)
                    add_type_promotion(
                        node.node, tree.names, graph[module].options, builtins.names
                    )


</t>
<t tx="ekr.20221004064035.1533">def check_blockers(graph: Graph, scc: list[str]) -&gt; None:
    for module in scc:
        graph[module].check_blockers()
</t>
<t tx="ekr.20221004064035.1534">@path C:/Repos/ekr-mypy2/mypy/
"""Semantic analysis of named tuple definitions.

This is conceptually part of mypy.semanal.
"""

from __future__ import annotations

from contextlib import contextmanager
from typing import Iterator, List, Mapping, cast
from typing_extensions import Final

from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.nodes import (
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    MDEF,
    Argument,
    AssignmentStmt,
    Block,
    CallExpr,
    ClassDef,
    Context,
    Decorator,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    FuncBase,
    FuncDef,
    ListExpr,
    NamedTupleExpr,
    NameExpr,
    PassStmt,
    RefExpr,
    StrExpr,
    SymbolTable,
    SymbolTableNode,
    TempNode,
    TupleExpr,
    TypeInfo,
    TypeVarExpr,
    Var,
)
from mypy.options import Options
from mypy.semanal_shared import (
    PRIORITY_FALLBACKS,
    SemanticAnalyzerInterface,
    calculate_tuple_fallback,
    has_placeholder,
    set_callable_name,
)
from mypy.types import (
    TYPED_NAMEDTUPLE_NAMES,
    AnyType,
    CallableType,
    LiteralType,
    TupleType,
    Type,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarType,
    UnboundType,
    has_type_vars,
)
from mypy.util import get_unique_redefinition_name

# Matches "_prohibited" in typing.py, but adds __annotations__, which works at runtime but can't
# easily be supported in a static checker.
NAMEDTUPLE_PROHIBITED_NAMES: Final = (
    "__new__",
    "__init__",
    "__slots__",
    "__getnewargs__",
    "_fields",
    "_field_defaults",
    "_field_types",
    "_make",
    "_replace",
    "_asdict",
    "_source",
    "__annotations__",
)

NAMEDTUP_CLASS_ERROR: Final = (
    "Invalid statement in NamedTuple definition; " 'expected "field_name: field_type [= default]"'
)

SELF_TVAR_NAME: Final = "_NT"


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1535">class NamedTupleAnalyzer:
    @others
</t>
<t tx="ekr.20221004064035.1536">def __init__(self, options: Options, api: SemanticAnalyzerInterface) -&gt; None:
    self.options = options
    self.api = api

</t>
<t tx="ekr.20221004064035.1537">def analyze_namedtuple_classdef(
    self, defn: ClassDef, is_stub_file: bool, is_func_scope: bool
) -&gt; tuple[bool, TypeInfo | None]:
    """Analyze if given class definition can be a named tuple definition.

    Return a tuple where first item indicates whether this can possibly be a named tuple,
    and the second item is the corresponding TypeInfo (may be None if not ready and should be
    deferred).
    """
    for base_expr in defn.base_type_exprs:
        if isinstance(base_expr, RefExpr):
            self.api.accept(base_expr)
            if base_expr.fullname in TYPED_NAMEDTUPLE_NAMES:
                result = self.check_namedtuple_classdef(defn, is_stub_file)
                if result is None:
                    # This is a valid named tuple, but some types are incomplete.
                    return True, None
                items, types, default_items = result
                if is_func_scope and "@" not in defn.name:
                    defn.name += "@" + str(defn.line)
                existing_info = None
                if isinstance(defn.analyzed, NamedTupleExpr):
                    existing_info = defn.analyzed.info
                info = self.build_namedtuple_typeinfo(
                    defn.name, items, types, default_items, defn.line, existing_info
                )
                defn.analyzed = NamedTupleExpr(info, is_typed=True)
                defn.analyzed.line = defn.line
                defn.analyzed.column = defn.column
                # All done: this is a valid named tuple with all types known.
                return True, info
    # This can't be a valid named tuple.
    return False, None

</t>
<t tx="ekr.20221004064035.1538">def check_namedtuple_classdef(
    self, defn: ClassDef, is_stub_file: bool
) -&gt; tuple[list[str], list[Type], dict[str, Expression]] | None:
    """Parse and validate fields in named tuple class definition.

    Return a three tuple:
      * field names
      * field types
      * field default values
    or None, if any of the types are not ready.
    """
    if self.options.python_version &lt; (3, 6) and not is_stub_file:
        self.fail("NamedTuple class syntax is only supported in Python 3.6", defn)
        return [], [], {}
    if len(defn.base_type_exprs) &gt; 1:
        self.fail("NamedTuple should be a single base", defn)
    items: list[str] = []
    types: list[Type] = []
    default_items: dict[str, Expression] = {}
    for stmt in defn.defs.body:
        if not isinstance(stmt, AssignmentStmt):
            # Still allow pass or ... (for empty namedtuples).
            if isinstance(stmt, PassStmt) or (
                isinstance(stmt, ExpressionStmt) and isinstance(stmt.expr, EllipsisExpr)
            ):
                continue
            # Also allow methods, including decorated ones.
            if isinstance(stmt, (Decorator, FuncBase)):
                continue
            # And docstrings.
            if isinstance(stmt, ExpressionStmt) and isinstance(stmt.expr, StrExpr):
                continue
            self.fail(NAMEDTUP_CLASS_ERROR, stmt)
        elif len(stmt.lvalues) &gt; 1 or not isinstance(stmt.lvalues[0], NameExpr):
            # An assignment, but an invalid one.
            self.fail(NAMEDTUP_CLASS_ERROR, stmt)
        else:
            # Append name and type in this case...
            name = stmt.lvalues[0].name
            items.append(name)
            if stmt.type is None:
                types.append(AnyType(TypeOfAny.unannotated))
            else:
                # We never allow recursive types at function scope. Although it is
                # possible to support this for named tuples, it is still tricky, and
                # it would be inconsistent with type aliases.
                analyzed = self.api.anal_type(
                    stmt.type,
                    allow_placeholder=not self.options.disable_recursive_aliases
                    and not self.api.is_func_scope(),
                )
                if analyzed is None:
                    # Something is incomplete. We need to defer this named tuple.
                    return None
                types.append(analyzed)
            # ...despite possible minor failures that allow further analyzis.
            if name.startswith("_"):
                self.fail(
                    f"NamedTuple field name cannot start with an underscore: {name}", stmt
                )
            if stmt.type is None or hasattr(stmt, "new_syntax") and not stmt.new_syntax:
                self.fail(NAMEDTUP_CLASS_ERROR, stmt)
            elif isinstance(stmt.rvalue, TempNode):
                # x: int assigns rvalue to TempNode(AnyType())
                if default_items:
                    self.fail(
                        "Non-default NamedTuple fields cannot follow default fields", stmt
                    )
            else:
                default_items[name] = stmt.rvalue
    return items, types, default_items

</t>
<t tx="ekr.20221004064035.1539">def check_namedtuple(
    self, node: Expression, var_name: str | None, is_func_scope: bool
) -&gt; tuple[str | None, TypeInfo | None, list[TypeVarLikeType]]:
    """Check if a call defines a namedtuple.

    The optional var_name argument is the name of the variable to
    which this is assigned, if any.

    Return a tuple of two items:
      * Internal name of the named tuple (e.g. the name passed as an argument to namedtuple)
        or None if it is not a valid named tuple
      * Corresponding TypeInfo, or None if not ready.

    If the definition is invalid but looks like a namedtuple,
    report errors but return (some) TypeInfo.
    """
    if not isinstance(node, CallExpr):
        return None, None, []
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None, None, []
    fullname = callee.fullname
    if fullname == "collections.namedtuple":
        is_typed = False
    elif fullname in TYPED_NAMEDTUPLE_NAMES:
        is_typed = True
    else:
        return None, None, []
    result = self.parse_namedtuple_args(call, fullname)
    if result:
        items, types, defaults, typename, tvar_defs, ok = result
    else:
        # Error. Construct dummy return value.
        if var_name:
            name = var_name
            if is_func_scope:
                name += "@" + str(call.line)
        else:
            name = var_name = "namedtuple@" + str(call.line)
        info = self.build_namedtuple_typeinfo(name, [], [], {}, node.line, None)
        self.store_namedtuple_info(info, var_name, call, is_typed)
        if name != var_name or is_func_scope:
            # NOTE: we skip local namespaces since they are not serialized.
            self.api.add_symbol_skip_local(name, info)
        return var_name, info, []
    if not ok:
        # This is a valid named tuple but some types are not ready.
        return typename, None, []

    # We use the variable name as the class name if it exists. If
    # it doesn't, we use the name passed as an argument. We prefer
    # the variable name because it should be unique inside a
    # module, and so we don't need to disambiguate it with a line
    # number.
    if var_name:
        name = var_name
    else:
        name = typename

    if var_name is None or is_func_scope:
        # There are two special cases where need to give it a unique name derived
        # from the line number:
        #   * This is a base class expression, since it often matches the class name:
        #         class NT(NamedTuple('NT', [...])):
        #             ...
        #   * This is a local (function or method level) named tuple, since
        #     two methods of a class can define a named tuple with the same name,
        #     and they will be stored in the same namespace (see below).
        name += "@" + str(call.line)
    if len(defaults) &gt; 0:
        default_items = {
            arg_name: default for arg_name, default in zip(items[-len(defaults) :], defaults)
        }
    else:
        default_items = {}

    existing_info = None
    if isinstance(node.analyzed, NamedTupleExpr):
        existing_info = node.analyzed.info
    info = self.build_namedtuple_typeinfo(
        name, items, types, default_items, node.line, existing_info
    )

    # If var_name is not None (i.e. this is not a base class expression), we always
    # store the generated TypeInfo under var_name in the current scope, so that
    # other definitions can use it.
    if var_name:
        self.store_namedtuple_info(info, var_name, call, is_typed)
    else:
        call.analyzed = NamedTupleExpr(info, is_typed=is_typed)
        call.analyzed.set_line(call)
    # There are three cases where we need to store the generated TypeInfo
    # second time (for the purpose of serialization):
    #   * If there is a name mismatch like One = NamedTuple('Other', [...])
    #     we also store the info under name 'Other@lineno', this is needed
    #     because classes are (de)serialized using their actual fullname, not
    #     the name of l.h.s.
    #   * If this is a method level named tuple. It can leak from the method
    #     via assignment to self attribute and therefore needs to be serialized
    #     (local namespaces are not serialized).
    #   * If it is a base class expression. It was not stored above, since
    #     there is no var_name (but it still needs to be serialized
    #     since it is in MRO of some class).
    if name != var_name or is_func_scope:
        # NOTE: we skip local namespaces since they are not serialized.
        self.api.add_symbol_skip_local(name, info)
    return typename, info, tvar_defs

</t>
<t tx="ekr.20221004064035.154">def instance_to_type_environment(instance: Instance) -&gt; dict[TypeVarId, Type]:
    """Given an Instance, produce the resulting type environment for type
    variables bound by the Instance's class definition.

    An Instance is a type application of a class (a TypeInfo) to its
    required number of type arguments.  So this environment consists
    of the class's type variables mapped to the Instance's actual
    arguments.  The type variables are mapped by their `id`.

    """
    return {binder.id: arg for binder, arg in zip(instance.type.defn.type_vars, instance.args)}
</t>
<t tx="ekr.20221004064035.1540">def store_namedtuple_info(
    self, info: TypeInfo, name: str, call: CallExpr, is_typed: bool
) -&gt; None:
    self.api.add_symbol(name, info, call)
    call.analyzed = NamedTupleExpr(info, is_typed=is_typed)
    call.analyzed.set_line(call)

</t>
<t tx="ekr.20221004064035.1541">def parse_namedtuple_args(
    self, call: CallExpr, fullname: str
) -&gt; None | (tuple[list[str], list[Type], list[Expression], str, list[TypeVarLikeType], bool]):
    """Parse a namedtuple() call into data needed to construct a type.

    Returns a 6-tuple:
    - List of argument names
    - List of argument types
    - List of default values
    - First argument of namedtuple
    - All typevars found in the field definition
    - Whether all types are ready.

    Return None if the definition didn't typecheck.
    """
    type_name = "NamedTuple" if fullname in TYPED_NAMEDTUPLE_NAMES else "namedtuple"
    # TODO: Share code with check_argument_count in checkexpr.py?
    args = call.args
    if len(args) &lt; 2:
        self.fail(f'Too few arguments for "{type_name}()"', call)
        return None
    defaults: list[Expression] = []
    if len(args) &gt; 2:
        # Typed namedtuple doesn't support additional arguments.
        if fullname in TYPED_NAMEDTUPLE_NAMES:
            self.fail('Too many arguments for "NamedTuple()"', call)
            return None
        for i, arg_name in enumerate(call.arg_names[2:], 2):
            if arg_name == "defaults":
                arg = args[i]
                # We don't care what the values are, as long as the argument is an iterable
                # and we can count how many defaults there are.
                if isinstance(arg, (ListExpr, TupleExpr)):
                    defaults = list(arg.items)
                else:
                    self.fail(
                        "List or tuple literal expected as the defaults argument to "
                        "{}()".format(type_name),
                        arg,
                    )
                break
    if call.arg_kinds[:2] != [ARG_POS, ARG_POS]:
        self.fail(f'Unexpected arguments to "{type_name}()"', call)
        return None
    if not isinstance(args[0], StrExpr):
        self.fail(f'"{type_name}()" expects a string literal as the first argument', call)
        return None
    typename = cast(StrExpr, call.args[0]).value
    types: list[Type] = []
    tvar_defs = []
    if not isinstance(args[1], (ListExpr, TupleExpr)):
        if fullname == "collections.namedtuple" and isinstance(args[1], StrExpr):
            str_expr = args[1]
            items = str_expr.value.replace(",", " ").split()
        else:
            self.fail(
                'List or tuple literal expected as the second argument to "{}()"'.format(
                    type_name
                ),
                call,
            )
            return None
    else:
        listexpr = args[1]
        if fullname == "collections.namedtuple":
            # The fields argument contains just names, with implicit Any types.
            if any(not isinstance(item, StrExpr) for item in listexpr.items):
                self.fail('String literal expected as "namedtuple()" item', call)
                return None
            items = [cast(StrExpr, item).value for item in listexpr.items]
        else:
            type_exprs = [
                t.items[1]
                for t in listexpr.items
                if isinstance(t, TupleExpr) and len(t.items) == 2
            ]
            tvar_defs = self.api.get_and_bind_all_tvars(type_exprs)
            # The fields argument contains (name, type) tuples.
            result = self.parse_namedtuple_fields_with_types(listexpr.items, call)
            if result is None:
                # One of the types is not ready, defer.
                return None
            items, types, _, ok = result
            if not ok:
                return [], [], [], typename, [], False
    if not types:
        types = [AnyType(TypeOfAny.unannotated) for _ in items]
    underscore = [item for item in items if item.startswith("_")]
    if underscore:
        self.fail(
            f'"{type_name}()" field names cannot start with an underscore: '
            + ", ".join(underscore),
            call,
        )
    if len(defaults) &gt; len(items):
        self.fail(f'Too many defaults given in call to "{type_name}()"', call)
        defaults = defaults[: len(items)]
    return items, types, defaults, typename, tvar_defs, True

</t>
<t tx="ekr.20221004064035.1542">def parse_namedtuple_fields_with_types(
    self, nodes: list[Expression], context: Context
) -&gt; tuple[list[str], list[Type], list[Expression], bool] | None:
    """Parse typed named tuple fields.

    Return (names, types, defaults, whether types are all ready), or None if error occurred.
    """
    items: list[str] = []
    types: list[Type] = []
    for item in nodes:
        if isinstance(item, TupleExpr):
            if len(item.items) != 2:
                self.fail('Invalid "NamedTuple()" field definition', item)
                return None
            name, type_node = item.items
            if isinstance(name, StrExpr):
                items.append(name.value)
            else:
                self.fail('Invalid "NamedTuple()" field name', item)
                return None
            try:
                type = expr_to_unanalyzed_type(type_node, self.options, self.api.is_stub_file)
            except TypeTranslationError:
                self.fail("Invalid field type", type_node)
                return None
            # We never allow recursive types at function scope.
            analyzed = self.api.anal_type(
                type,
                allow_placeholder=not self.options.disable_recursive_aliases
                and not self.api.is_func_scope(),
            )
            # Workaround #4987 and avoid introducing a bogus UnboundType
            if isinstance(analyzed, UnboundType):
                analyzed = AnyType(TypeOfAny.from_error)
            # These should be all known, otherwise we would defer in visit_assignment_stmt().
            if analyzed is None:
                return [], [], [], False
            types.append(analyzed)
        else:
            self.fail('Tuple expected as "NamedTuple()" field', item)
            return None
    return items, types, [], True

</t>
<t tx="ekr.20221004064035.1543">def build_namedtuple_typeinfo(
    self,
    name: str,
    items: list[str],
    types: list[Type],
    default_items: Mapping[str, Expression],
    line: int,
    existing_info: TypeInfo | None,
) -&gt; TypeInfo:
    strtype = self.api.named_type("builtins.str")
    implicit_any = AnyType(TypeOfAny.special_form)
    basetuple_type = self.api.named_type("builtins.tuple", [implicit_any])
    dictype = self.api.named_type_or_none(
        "builtins.dict", [strtype, implicit_any]
    ) or self.api.named_type("builtins.object")
    # Actual signature should return OrderedDict[str, Union[types]]
    ordereddictype = self.api.named_type_or_none(
        "builtins.dict", [strtype, implicit_any]
    ) or self.api.named_type("builtins.object")
    fallback = self.api.named_type("builtins.tuple", [implicit_any])
    # Note: actual signature should accept an invariant version of Iterable[UnionType[types]].
    # but it can't be expressed. 'new' and 'len' should be callable types.
    iterable_type = self.api.named_type_or_none("typing.Iterable", [implicit_any])
    function_type = self.api.named_type("builtins.function")

    literals: list[Type] = [LiteralType(item, strtype) for item in items]
    match_args_type = TupleType(literals, basetuple_type)

    info = existing_info or self.api.basic_new_typeinfo(name, fallback, line)
    info.is_named_tuple = True
    tuple_base = TupleType(types, fallback)
    if info.special_alias and has_placeholder(info.special_alias.target):
        self.api.defer(force_progress=True)
    info.update_tuple_type(tuple_base)
    info.line = line
    # For use by mypyc.
    info.metadata["namedtuple"] = {"fields": items.copy()}

    # We can't calculate the complete fallback type until after semantic
    # analysis, since otherwise base classes might be incomplete. Postpone a
    # callback function that patches the fallback.
    if not has_placeholder(tuple_base) and not has_type_vars(tuple_base):
        self.api.schedule_patch(
            PRIORITY_FALLBACKS, lambda: calculate_tuple_fallback(tuple_base)
        )

    def add_field(
        var: Var, is_initialized_in_class: bool = False, is_property: bool = False
    ) -&gt; None:
        var.info = info
        var.is_initialized_in_class = is_initialized_in_class
        var.is_property = is_property
        var._fullname = f"{info.fullname}.{var.name}"
        info.names[var.name] = SymbolTableNode(MDEF, var)

    fields = [Var(item, typ) for item, typ in zip(items, types)]
    for var in fields:
        add_field(var, is_property=True)
    # We can't share Vars between fields and method arguments, since they
    # have different full names (the latter are normally used as local variables
    # in functions, so their full names are set to short names when generated methods
    # are analyzed).
    vars = [Var(item, typ) for item, typ in zip(items, types)]

    tuple_of_strings = TupleType([strtype for _ in items], basetuple_type)
    add_field(Var("_fields", tuple_of_strings), is_initialized_in_class=True)
    add_field(Var("_field_types", dictype), is_initialized_in_class=True)
    add_field(Var("_field_defaults", dictype), is_initialized_in_class=True)
    add_field(Var("_source", strtype), is_initialized_in_class=True)
    add_field(Var("__annotations__", ordereddictype), is_initialized_in_class=True)
    add_field(Var("__doc__", strtype), is_initialized_in_class=True)
    if self.options.python_version &gt;= (3, 10):
        add_field(Var("__match_args__", match_args_type), is_initialized_in_class=True)

    assert info.tuple_type is not None  # Set by update_tuple_type() above.
    tvd = TypeVarType(
        SELF_TVAR_NAME,
        info.fullname + "." + SELF_TVAR_NAME,
        self.api.tvar_scope.new_unique_func_id(),
        [],
        info.tuple_type,
    )
    selftype = tvd

    def add_method(
        funcname: str,
        ret: Type,
        args: list[Argument],
        is_classmethod: bool = False,
        is_new: bool = False,
    ) -&gt; None:
        if is_classmethod or is_new:
            first = [Argument(Var("_cls"), TypeType.make_normalized(selftype), None, ARG_POS)]
        else:
            first = [Argument(Var("_self"), selftype, None, ARG_POS)]
        args = first + args

        types = [arg.type_annotation for arg in args]
        items = [arg.variable.name for arg in args]
        arg_kinds = [arg.kind for arg in args]
        assert None not in types
        signature = CallableType(cast(List[Type], types), arg_kinds, items, ret, function_type)
        signature.variables = [tvd]
        func = FuncDef(funcname, args, Block([]))
        func.info = info
        func.is_class = is_classmethod
        func.type = set_callable_name(signature, func)
        func._fullname = info.fullname + "." + funcname
        func.line = line
        if is_classmethod:
            v = Var(funcname, func.type)
            v.is_classmethod = True
            v.info = info
            v._fullname = func._fullname
            func.is_decorated = True
            dec = Decorator(func, [NameExpr("classmethod")], v)
            dec.line = line
            sym = SymbolTableNode(MDEF, dec)
        else:
            sym = SymbolTableNode(MDEF, func)
        sym.plugin_generated = True
        info.names[funcname] = sym

    add_method(
        "_replace",
        ret=selftype,
        args=[Argument(var, var.type, EllipsisExpr(), ARG_NAMED_OPT) for var in vars],
    )

    def make_init_arg(var: Var) -&gt; Argument:
        default = default_items.get(var.name, None)
        kind = ARG_POS if default is None else ARG_OPT
        return Argument(var, var.type, default, kind)

    add_method("__new__", ret=selftype, args=[make_init_arg(var) for var in vars], is_new=True)
    add_method("_asdict", args=[], ret=ordereddictype)
    special_form_any = AnyType(TypeOfAny.special_form)
    add_method(
        "_make",
        ret=selftype,
        is_classmethod=True,
        args=[
            Argument(Var("iterable", iterable_type), iterable_type, None, ARG_POS),
            Argument(Var("new"), special_form_any, EllipsisExpr(), ARG_NAMED_OPT),
            Argument(Var("len"), special_form_any, EllipsisExpr(), ARG_NAMED_OPT),
        ],
    )

    self_tvar_expr = TypeVarExpr(
        SELF_TVAR_NAME, info.fullname + "." + SELF_TVAR_NAME, [], info.tuple_type
    )
    info.names[SELF_TVAR_NAME] = SymbolTableNode(MDEF, self_tvar_expr)
    return info

</t>
<t tx="ekr.20221004064035.1544">@contextmanager
def save_namedtuple_body(self, named_tuple_info: TypeInfo) -&gt; Iterator[None]:
    """Preserve the generated body of class-based named tuple and then restore it.

    Temporarily clear the names dict so we don't get errors about duplicate names
    that were already set in build_namedtuple_typeinfo (we already added the tuple
    field names while generating the TypeInfo, and actual duplicates are
    already reported).
    """
    nt_names = named_tuple_info.names
    named_tuple_info.names = SymbolTable()

    yield

    # Make sure we didn't use illegal names, then reset the names in the typeinfo.
    for prohibited in NAMEDTUPLE_PROHIBITED_NAMES:
        if prohibited in named_tuple_info.names:
            if nt_names.get(prohibited) is named_tuple_info.names[prohibited]:
                continue
            ctx = named_tuple_info.names[prohibited].node
            assert ctx is not None
            self.fail(f'Cannot overwrite NamedTuple attribute "{prohibited}"', ctx)

    # Restore the names in the original symbol table. This ensures that the symbol
    # table contains the field objects created by build_namedtuple_typeinfo. Exclude
    # __doc__, which can legally be overwritten by the class.
    for key, value in nt_names.items():
        if key in named_tuple_info.names:
            if key == "__doc__":
                continue
            sym = named_tuple_info.names[key]
            if isinstance(sym.node, (FuncBase, Decorator)) and not sym.plugin_generated:
                # Keep user-defined methods as is.
                continue
            # Keep existing (user-provided) definitions under mangled names, so they
            # get semantically analyzed.
            r_key = get_unique_redefinition_name(key, named_tuple_info.names)
            named_tuple_info.names[r_key] = sym
        named_tuple_info.names[key] = value

</t>
<t tx="ekr.20221004064035.1545"># Helpers

</t>
<t tx="ekr.20221004064035.1546">def fail(self, msg: str, ctx: Context) -&gt; None:
    self.api.fail(msg, ctx)
</t>
<t tx="ekr.20221004064035.1547">@path C:/Repos/ekr-mypy2/mypy/
"""Semantic analysis of NewType definitions.

This is conceptually part of mypy.semanal (semantic analyzer pass 2).
"""

from __future__ import annotations

from mypy import errorcodes as codes
from mypy.errorcodes import ErrorCode
from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.messages import MessageBuilder, format_type
from mypy.nodes import (
    ARG_POS,
    MDEF,
    Argument,
    AssignmentStmt,
    Block,
    CallExpr,
    Context,
    FuncDef,
    NameExpr,
    NewTypeExpr,
    PlaceholderNode,
    RefExpr,
    StrExpr,
    SymbolTableNode,
    TypeInfo,
    Var,
)
from mypy.options import Options
from mypy.semanal_shared import SemanticAnalyzerInterface, has_placeholder
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type
from mypy.types import (
    AnyType,
    CallableType,
    Instance,
    NoneType,
    PlaceholderType,
    TupleType,
    Type,
    TypeOfAny,
    get_proper_type,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1548">class NewTypeAnalyzer:
    @others
</t>
<t tx="ekr.20221004064035.1549">def __init__(
    self, options: Options, api: SemanticAnalyzerInterface, msg: MessageBuilder
) -&gt; None:
    self.options = options
    self.api = api
    self.msg = msg

</t>
<t tx="ekr.20221004064035.155">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Callable

from mypy import join
from mypy.erasetype import erase_type
from mypy.maptype import map_instance_to_supertype
from mypy.state import state
from mypy.subtypes import (
    is_callable_compatible,
    is_equivalent,
    is_proper_subtype,
    is_same_type,
    is_subtype,
)
from mypy.typeops import is_recursive_pair, make_simplified_union, tuple_fallback
from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeGuardedType,
    TypeOfAny,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)

# TODO Describe this module.


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1550">def process_newtype_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Check if s declares a NewType; if yes, store it in symbol table.

    Return True if it's a NewType declaration. The current target may be
    deferred as a side effect if the base type is not ready, even if
    the return value is True.

    The logic in this function mostly copies the logic for visit_class_def()
    with a single (non-Generic) base.
    """
    var_name, call = self.analyze_newtype_declaration(s)
    if var_name is None or call is None:
        return False
    name = var_name
    # OK, now we know this is a NewType. But the base type may be not ready yet,
    # add placeholder as we do for ClassDef.

    if self.api.is_func_scope():
        name += "@" + str(s.line)
    fullname = self.api.qualified_name(name)

    if not call.analyzed or isinstance(call.analyzed, NewTypeExpr) and not call.analyzed.info:
        # Start from labeling this as a future class, as we do for normal ClassDefs.
        placeholder = PlaceholderNode(fullname, s, s.line, becomes_typeinfo=True)
        self.api.add_symbol(var_name, placeholder, s, can_defer=False)

    old_type, should_defer = self.check_newtype_args(var_name, call, s)
    old_type = get_proper_type(old_type)
    if not isinstance(call.analyzed, NewTypeExpr):
        call.analyzed = NewTypeExpr(var_name, old_type, line=call.line, column=call.column)
    else:
        call.analyzed.old_type = old_type
    if old_type is None:
        if should_defer:
            # Base type is not ready.
            self.api.defer()
            return True

    # Create the corresponding class definition if the aliased type is subtypeable
    assert isinstance(call.analyzed, NewTypeExpr)
    if isinstance(old_type, TupleType):
        newtype_class_info = self.build_newtype_typeinfo(
            name, old_type, old_type.partial_fallback, s.line, call.analyzed.info
        )
        newtype_class_info.update_tuple_type(old_type)
    elif isinstance(old_type, Instance):
        if old_type.type.is_protocol:
            self.fail("NewType cannot be used with protocol classes", s)
        newtype_class_info = self.build_newtype_typeinfo(
            name, old_type, old_type, s.line, call.analyzed.info
        )
    else:
        if old_type is not None:
            message = "Argument 2 to NewType(...) must be subclassable (got {})"
            self.fail(message.format(format_type(old_type)), s, code=codes.VALID_NEWTYPE)
        # Otherwise the error was already reported.
        old_type = AnyType(TypeOfAny.from_error)
        object_type = self.api.named_type("builtins.object")
        newtype_class_info = self.build_newtype_typeinfo(
            name, old_type, object_type, s.line, call.analyzed.info
        )
        newtype_class_info.fallback_to_any = True

    check_for_explicit_any(
        old_type, self.options, self.api.is_typeshed_stub_file, self.msg, context=s
    )

    if self.options.disallow_any_unimported and has_any_from_unimported_type(old_type):
        self.msg.unimported_type_becomes_any("Argument 2 to NewType(...)", old_type, s)

    # If so, add it to the symbol table.
    assert isinstance(call.analyzed, NewTypeExpr)
    # As we do for normal classes, create the TypeInfo only once, then just
    # update base classes on next iterations (to get rid of placeholders there).
    if not call.analyzed.info:
        call.analyzed.info = newtype_class_info
    else:
        call.analyzed.info.bases = newtype_class_info.bases
    self.api.add_symbol(var_name, call.analyzed.info, s)
    if self.api.is_func_scope():
        self.api.add_symbol_skip_local(name, call.analyzed.info)
    newtype_class_info.line = s.line
    return True

</t>
<t tx="ekr.20221004064035.1551">def analyze_newtype_declaration(self, s: AssignmentStmt) -&gt; tuple[str | None, CallExpr | None]:
    """Return the NewType call expression if `s` is a newtype declaration or None otherwise."""
    name, call = None, None
    if (
        len(s.lvalues) == 1
        and isinstance(s.lvalues[0], NameExpr)
        and isinstance(s.rvalue, CallExpr)
        and isinstance(s.rvalue.callee, RefExpr)
        and s.rvalue.callee.fullname == "typing.NewType"
    ):
        name = s.lvalues[0].name

        if s.type:
            self.fail("Cannot declare the type of a NewType declaration", s)

        names = self.api.current_symbol_table()
        existing = names.get(name)
        # Give a better error message than generic "Name already defined".
        if (
            existing
            and not isinstance(existing.node, PlaceholderNode)
            and not s.rvalue.analyzed
        ):
            self.fail(f'Cannot redefine "{name}" as a NewType', s)

        # This dummy NewTypeExpr marks the call as sufficiently analyzed; it will be
        # overwritten later with a fully complete NewTypeExpr if there are no other
        # errors with the NewType() call.
        call = s.rvalue

    return name, call

</t>
<t tx="ekr.20221004064035.1552">def check_newtype_args(
    self, name: str, call: CallExpr, context: Context
) -&gt; tuple[Type | None, bool]:
    """Ananlyze base type in NewType call.

    Return a tuple (type, should defer).
    """
    has_failed = False
    args, arg_kinds = call.args, call.arg_kinds
    if len(args) != 2 or arg_kinds[0] != ARG_POS or arg_kinds[1] != ARG_POS:
        self.fail("NewType(...) expects exactly two positional arguments", context)
        return None, False

    # Check first argument
    if not isinstance(args[0], StrExpr):
        self.fail("Argument 1 to NewType(...) must be a string literal", context)
        has_failed = True
    elif args[0].value != name:
        msg = 'String argument 1 "{}" to NewType(...) does not match variable name "{}"'
        self.fail(msg.format(args[0].value, name), context)
        has_failed = True

    # Check second argument
    msg = "Argument 2 to NewType(...) must be a valid type"
    try:
        unanalyzed_type = expr_to_unanalyzed_type(args[1], self.options, self.api.is_stub_file)
    except TypeTranslationError:
        self.fail(msg, context)
        return None, False

    # We want to use our custom error message (see above), so we suppress
    # the default error message for invalid types here.
    old_type = get_proper_type(
        self.api.anal_type(
            unanalyzed_type,
            report_invalid_types=False,
            allow_placeholder=not self.options.disable_recursive_aliases
            and not self.api.is_func_scope(),
        )
    )
    should_defer = False
    if isinstance(old_type, PlaceholderType):
        old_type = None
    if old_type is None:
        should_defer = True

    # The caller of this function assumes that if we return a Type, it's always
    # a valid one. So, we translate AnyTypes created from errors into None.
    if isinstance(old_type, AnyType) and old_type.is_from_error:
        self.fail(msg, context)
        return None, False

    return None if has_failed else old_type, should_defer

</t>
<t tx="ekr.20221004064035.1553">def build_newtype_typeinfo(
    self,
    name: str,
    old_type: Type,
    base_type: Instance,
    line: int,
    existing_info: TypeInfo | None,
) -&gt; TypeInfo:
    info = existing_info or self.api.basic_new_typeinfo(name, base_type, line)
    info.bases = [base_type]  # Update in case there were nested placeholders.
    info.is_newtype = True

    # Add __init__ method
    args = [
        Argument(Var("self"), NoneType(), None, ARG_POS),
        self.make_argument("item", old_type),
    ]
    signature = CallableType(
        arg_types=[Instance(info, []), old_type],
        arg_kinds=[arg.kind for arg in args],
        arg_names=["self", "item"],
        ret_type=NoneType(),
        fallback=self.api.named_type("builtins.function"),
        name=name,
    )
    init_func = FuncDef("__init__", args, Block([]), typ=signature)
    init_func.info = info
    init_func._fullname = info.fullname + ".__init__"
    info.names["__init__"] = SymbolTableNode(MDEF, init_func)

    if has_placeholder(old_type) or info.tuple_type and has_placeholder(info.tuple_type):
        self.api.defer(force_progress=True)
    return info

</t>
<t tx="ekr.20221004064035.1554"># Helpers

</t>
<t tx="ekr.20221004064035.1555">def make_argument(self, name: str, type: Type) -&gt; Argument:
    return Argument(Var(name), type, None, ARG_POS)

</t>
<t tx="ekr.20221004064035.1556">def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.api.fail(msg, ctx, code=code)
</t>
<t tx="ekr.20221004064035.1557">@path C:/Repos/ekr-mypy2/mypy/
"""Block/import reachability analysis."""

from __future__ import annotations

from mypy.nodes import (
    AssertStmt,
    AssignmentStmt,
    Block,
    ClassDef,
    ExpressionStmt,
    ForStmt,
    FuncDef,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    MatchStmt,
    MypyFile,
    ReturnStmt,
)
from mypy.options import Options
from mypy.reachability import (
    assert_will_always_fail,
    infer_reachability_of_if_statement,
    infer_reachability_of_match_statement,
)
from mypy.traverser import TraverserVisitor


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1558">class SemanticAnalyzerPreAnalysis(TraverserVisitor):
    """Analyze reachability of blocks and imports and other local things.

    This runs before semantic analysis, so names have not been bound. Imports are
    also not resolved yet, so we can only access the current module.

    This determines static reachability of blocks and imports due to version and
    platform checks, among others.

    The main entry point is 'visit_file'.

    Reachability of imports needs to be determined very early in the build since
    this affects which modules will ultimately be processed.

    Consider this example:

      import sys

      def do_stuff():
          # type: () -&gt; None:
          if sys.python_version &lt; (3,):
              import xyz  # Only available in Python 2
              xyz.whatever()
          ...

    The block containing 'import xyz' is unreachable in Python 3 mode. The import
    shouldn't be processed in Python 3 mode, even if the module happens to exist.
    """

    @others
</t>
<t tx="ekr.20221004064035.1559">def visit_file(self, file: MypyFile, fnam: str, mod_id: str, options: Options) -&gt; None:
    self.platform = options.platform
    self.cur_mod_id = mod_id
    self.cur_mod_node = file
    self.options = options
    self.is_global_scope = True

    for i, defn in enumerate(file.defs):
        defn.accept(self)
        if isinstance(defn, AssertStmt) and assert_will_always_fail(defn, options):
            # We've encountered an assert that's always false,
            # e.g. assert sys.platform == 'lol'.  Truncate the
            # list of statements.  This mutates file.defs too.
            del file.defs[i + 1 :]
            break

</t>
<t tx="ekr.20221004064035.156">def trivial_meet(s: Type, t: Type) -&gt; ProperType:
    """Return one of types (expanded) if it is a subtype of other, otherwise bottom type."""
    if is_subtype(s, t):
        return get_proper_type(s)
    elif is_subtype(t, s):
        return get_proper_type(t)
    else:
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()


</t>
<t tx="ekr.20221004064035.1560">def visit_func_def(self, node: FuncDef) -&gt; None:
    old_global_scope = self.is_global_scope
    self.is_global_scope = False
    super().visit_func_def(node)
    self.is_global_scope = old_global_scope
    file_node = self.cur_mod_node
    if (
        self.is_global_scope
        and file_node.is_stub
        and node.name == "__getattr__"
        and file_node.is_package_init_file()
    ):
        # __init__.pyi with __getattr__ means that any submodules are assumed
        # to exist, even if there is no stub. Note that we can't verify that the
        # return type is compatible, since we haven't bound types yet.
        file_node.is_partial_stub_package = True

</t>
<t tx="ekr.20221004064035.1561">def visit_class_def(self, node: ClassDef) -&gt; None:
    old_global_scope = self.is_global_scope
    self.is_global_scope = False
    super().visit_class_def(node)
    self.is_global_scope = old_global_scope

</t>
<t tx="ekr.20221004064035.1562">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import_from(node)

</t>
<t tx="ekr.20221004064035.1563">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import_all(node)

</t>
<t tx="ekr.20221004064035.1564">def visit_import(self, node: Import) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import(node)

</t>
<t tx="ekr.20221004064035.1565">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    infer_reachability_of_if_statement(s, self.options)
    for expr in s.expr:
        expr.accept(self)
    for node in s.body:
        node.accept(self)
    if s.else_body:
        s.else_body.accept(self)

</t>
<t tx="ekr.20221004064035.1566">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        return
    super().visit_block(b)

</t>
<t tx="ekr.20221004064035.1567">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    infer_reachability_of_match_statement(s, self.options)
    for guard in s.guards:
        if guard is not None:
            guard.accept(self)
    for body in s.bodies:
        body.accept(self)

</t>
<t tx="ekr.20221004064035.1568"># The remaining methods are an optimization: don't visit nested expressions
# of common statements, since they can have no effect.

</t>
<t tx="ekr.20221004064035.1569">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.157">def meet_types(s: Type, t: Type) -&gt; ProperType:
    """Return the greatest lower bound of two types."""
    if is_recursive_pair(s, t):
        # This case can trigger an infinite recursion, general support for this will be
        # tricky, so we use a trivial meet (like for protocols).
        return trivial_meet(s, t)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if isinstance(s, Instance) and isinstance(t, Instance) and s.type == t.type:
        # Code in checker.py should merge any extra_items where possible, so we
        # should have only compatible extra_items here. We check this before
        # the below subtype check, so that extra_attrs will not get erased.
        if is_same_type(s, t) and (s.extra_attrs or t.extra_attrs):
            if s.extra_attrs and t.extra_attrs:
                if len(s.extra_attrs.attrs) &gt; len(t.extra_attrs.attrs):
                    # Return the one that has more precise information.
                    return s
                return t
            if s.extra_attrs:
                return s
            return t

    if not isinstance(s, UnboundType) and not isinstance(t, UnboundType):
        if is_proper_subtype(s, t, ignore_promotions=True):
            return s
        if is_proper_subtype(t, s, ignore_promotions=True):
            return t

    if isinstance(s, ErasedType):
        return s
    if isinstance(s, AnyType):
        return t
    if isinstance(s, UnionType) and not isinstance(t, UnionType):
        s, t = t, s

    # Meets/joins require callable type normalization.
    s, t = join.normalize_callables(s, t)

    return t.accept(TypeMeetVisitor(s))


</t>
<t tx="ekr.20221004064035.1570">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.1571">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.1572">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    s.body.accept(self)
    if s.else_body is not None:
        s.else_body.accept(self)
</t>
<t tx="ekr.20221004064035.1573">@path C:/Repos/ekr-mypy2/mypy/
"""Shared definitions used by different parts of semantic analysis."""

from __future__ import annotations

from abc import abstractmethod
from typing import Callable
from typing_extensions import Final, Protocol

from mypy_extensions import trait

from mypy import join
from mypy.errorcodes import ErrorCode
from mypy.nodes import (
    Context,
    Expression,
    FuncDef,
    Node,
    SymbolNode,
    SymbolTable,
    SymbolTableNode,
    TypeInfo,
)
from mypy.tvar_scope import TypeVarLikeScope
from mypy.type_visitor import TypeQuery
from mypy.types import (
    TPDICT_FB_NAMES,
    FunctionLike,
    Instance,
    Parameters,
    ParamSpecFlavor,
    ParamSpecType,
    PlaceholderType,
    ProperType,
    TupleType,
    Type,
    TypeVarId,
    TypeVarLikeType,
    get_proper_type,
)

# Priorities for ordering of patches within the "patch" phase of semantic analysis
# (after the main pass):

# Fix fallbacks (does joins)
PRIORITY_FALLBACKS: Final = 1


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1574">@trait
class SemanticAnalyzerCoreInterface:
    """A core abstract interface to generic semantic analyzer functionality.

    This is implemented by both semantic analyzer passes 2 and 3.
    """

    @others
</t>
<t tx="ekr.20221004064035.1575">@abstractmethod
def lookup_qualified(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1576">@abstractmethod
def lookup_fully_qualified(self, name: str) -&gt; SymbolTableNode:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1577">@abstractmethod
def lookup_fully_qualified_or_none(self, name: str) -&gt; SymbolTableNode | None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1578">@abstractmethod
def fail(
    self,
    msg: str,
    ctx: Context,
    serious: bool = False,
    *,
    blocker: bool = False,
    code: ErrorCode | None = None,
) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1579">@abstractmethod
def note(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.158">def narrow_declared_type(declared: Type, narrowed: Type) -&gt; Type:
    """Return the declared type narrowed down to another type."""
    # TODO: check infinite recursion for aliases here.
    if isinstance(narrowed, TypeGuardedType):  # type: ignore[misc]
        # A type guard forces the new type even if it doesn't overlap the old.
        return narrowed.type_guard

    original_declared = declared
    original_narrowed = narrowed
    declared = get_proper_type(declared)
    narrowed = get_proper_type(narrowed)

    if declared == narrowed:
        return original_declared
    if isinstance(declared, UnionType):
        return make_simplified_union(
            [
                narrow_declared_type(x, narrowed)
                for x in declared.relevant_items()
                if is_overlapping_types(x, narrowed, ignore_promotions=True)
            ]
        )
    if is_enum_overlapping_union(declared, narrowed):
        return original_narrowed
    elif not is_overlapping_types(declared, narrowed, prohibit_none_typevar_overlap=True):
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()
    elif isinstance(narrowed, UnionType):
        return make_simplified_union(
            [narrow_declared_type(declared, x) for x in narrowed.relevant_items()]
        )
    elif isinstance(narrowed, AnyType):
        return original_narrowed
    elif isinstance(narrowed, TypeVarType) and is_subtype(narrowed.upper_bound, declared):
        return narrowed
    elif isinstance(declared, TypeType) and isinstance(narrowed, TypeType):
        return TypeType.make_normalized(narrow_declared_type(declared.item, narrowed.item))
    elif (
        isinstance(declared, TypeType)
        and isinstance(narrowed, Instance)
        and narrowed.type.is_metaclass()
    ):
        # We'd need intersection types, so give up.
        return original_declared
    elif isinstance(declared, Instance):
        if declared.type.alt_promote:
            # Special case: low-level integer type can't be narrowed
            return original_declared
        if (
            isinstance(narrowed, Instance)
            and narrowed.type.alt_promote
            and narrowed.type.alt_promote is declared.type
        ):
            # Special case: 'int' can't be narrowed down to a native int type such as
            # i64, since they have different runtime representations.
            return original_declared
        return meet_types(original_declared, original_narrowed)
    elif isinstance(declared, (TupleType, TypeType, LiteralType)):
        return meet_types(original_declared, original_narrowed)
    elif isinstance(declared, TypedDictType) and isinstance(narrowed, Instance):
        # Special case useful for selecting TypedDicts from unions using isinstance(x, dict).
        if narrowed.type.fullname == "builtins.dict" and all(
            isinstance(t, AnyType) for t in get_proper_types(narrowed.args)
        ):
            return original_declared
        return meet_types(original_declared, original_narrowed)
    return original_narrowed


</t>
<t tx="ekr.20221004064035.1580">@abstractmethod
def record_incomplete_ref(self) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1581">@abstractmethod
def defer(self, debug_context: Context | None = None, force_progress: bool = False) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1582">@abstractmethod
def is_incomplete_namespace(self, fullname: str) -&gt; bool:
    """Is a module or class namespace potentially missing some definitions?"""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1583">@property
@abstractmethod
def final_iteration(self) -&gt; bool:
    """Is this the final iteration of semantic analysis?"""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1584">@abstractmethod
def is_future_flag_set(self, flag: str) -&gt; bool:
    """Is the specific __future__ feature imported"""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1585">@property
@abstractmethod
def is_stub_file(self) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1586">@abstractmethod
def is_func_scope(self) -&gt; bool:
    raise NotImplementedError


</t>
<t tx="ekr.20221004064035.1587">@trait
class SemanticAnalyzerInterface(SemanticAnalyzerCoreInterface):
    """A limited abstract interface to some generic semantic analyzer pass 2 functionality.

    We use this interface for various reasons:

    * Looser coupling
    * Cleaner import graph
    * Less need to pass around callback functions
    """

    tvar_scope: TypeVarLikeScope

    @others
</t>
<t tx="ekr.20221004064035.1588">@abstractmethod
def lookup(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1589">@abstractmethod
def named_type(self, fullname: str, args: list[Type] | None = None) -&gt; Instance:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.159">def get_possible_variants(typ: Type) -&gt; list[Type]:
    """This function takes any "Union-like" type and returns a list of the available "options".

    Specifically, there are currently exactly three different types that can have
    "variants" or are "union-like":

    - Unions
    - TypeVars with value restrictions
    - Overloads

    This function will return a list of each "option" present in those types.

    If this function receives any other type, we return a list containing just that
    original type. (E.g. pretend the type was contained within a singleton union).

    The only current exceptions are regular TypeVars and ParamSpecs. For these "TypeVarLike"s,
    we return a list containing that TypeVarLike's upper bound.

    This function is useful primarily when checking to see if two types are overlapping:
    the algorithm to check if two unions are overlapping is fundamentally the same as
    the algorithm for checking if two overloads are overlapping.

    Normalizing both kinds of types in the same way lets us reuse the same algorithm
    for both.
    """
    typ = get_proper_type(typ)

    if isinstance(typ, TypeVarType):
        if len(typ.values) &gt; 0:
            return typ.values
        else:
            return [typ.upper_bound]
    elif isinstance(typ, ParamSpecType):
        return [typ.upper_bound]
    elif isinstance(typ, UnionType):
        return list(typ.items)
    elif isinstance(typ, Overloaded):
        # Note: doing 'return typ.items()' makes mypy
        # infer a too-specific return type of List[CallableType]
        return list(typ.items)
    else:
        return [typ]


</t>
<t tx="ekr.20221004064035.1590">@abstractmethod
def named_type_or_none(self, fullname: str, args: list[Type] | None = None) -&gt; Instance | None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1591">@abstractmethod
def accept(self, node: Node) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1592">@abstractmethod
def anal_type(
    self,
    t: Type,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_required: bool = False,
    allow_placeholder: bool = False,
    report_invalid_types: bool = True,
) -&gt; Type | None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1593">@abstractmethod
def get_and_bind_all_tvars(self, type_exprs: list[Expression]) -&gt; list[TypeVarLikeType]:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1594">@abstractmethod
def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1595">@abstractmethod
def schedule_patch(self, priority: int, fn: Callable[[], None]) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1596">@abstractmethod
def add_symbol_table_node(self, name: str, stnode: SymbolTableNode) -&gt; bool:
    """Add node to the current symbol table."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1597">@abstractmethod
def current_symbol_table(self) -&gt; SymbolTable:
    """Get currently active symbol table.

    May be module, class, or local namespace.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1598">@abstractmethod
def add_symbol(
    self,
    name: str,
    node: SymbolNode,
    context: Context,
    module_public: bool = True,
    module_hidden: bool = False,
    can_defer: bool = True,
) -&gt; bool:
    """Add symbol to the current symbol table."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1599">@abstractmethod
def add_symbol_skip_local(self, name: str, node: SymbolNode) -&gt; None:
    """Add symbol to the current symbol table, skipping locals.

    This is used to store symbol nodes in a symbol table that
    is going to be serialized (local namespaces are not serialized).
    See implementation docstring for more details.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.16">@property
def connection_name(self) -&gt; str:
    if sys.platform == "win32":
        return self.name
    else:
        name = self.sock.getsockname()
        assert isinstance(name, str)
        return name
</t>
<t tx="ekr.20221004064035.160">def is_enum_overlapping_union(x: ProperType, y: ProperType) -&gt; bool:
    """Return True if x is an Enum, and y is an Union with at least one Literal from x"""
    return (
        isinstance(x, Instance)
        and x.type.is_enum
        and isinstance(y, UnionType)
        and any(
            isinstance(p, LiteralType) and x.type == p.fallback.type
            for p in (get_proper_type(z) for z in y.relevant_items())
        )
    )


</t>
<t tx="ekr.20221004064035.1600">@abstractmethod
def parse_bool(self, expr: Expression) -&gt; bool | None:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1601">@abstractmethod
def qualified_name(self, n: str) -&gt; str:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1602">@property
@abstractmethod
def is_typeshed_stub_file(self) -&gt; bool:
    raise NotImplementedError


</t>
<t tx="ekr.20221004064035.1603">def set_callable_name(sig: Type, fdef: FuncDef) -&gt; ProperType:
    sig = get_proper_type(sig)
    if isinstance(sig, FunctionLike):
        if fdef.info:
            if fdef.info.fullname in TPDICT_FB_NAMES:
                # Avoid exposing the internal _TypedDict name.
                class_name = "TypedDict"
            else:
                class_name = fdef.info.name
            return sig.with_name(f"{fdef.name} of {class_name}")
        else:
            return sig.with_name(fdef.name)
    else:
        return sig


</t>
<t tx="ekr.20221004064035.1604">def calculate_tuple_fallback(typ: TupleType) -&gt; None:
    """Calculate a precise item type for the fallback of a tuple type.

    This must be called only after the main semantic analysis pass, since joins
    aren't available before that.

    Note that there is an apparent chicken and egg problem with respect
    to verifying type arguments against bounds. Verifying bounds might
    require fallbacks, but we might use the bounds to calculate the
    fallbacks. In practice this is not a problem, since the worst that
    can happen is that we have invalid type argument values, and these
    can happen in later stages as well (they will generate errors, but
    we don't prevent their existence).
    """
    fallback = typ.partial_fallback
    assert fallback.type.fullname == "builtins.tuple"
    fallback.args = (join.join_type_list(list(typ.items)),) + fallback.args[1:]


</t>
<t tx="ekr.20221004064035.1605">class _NamedTypeCallback(Protocol):
    def __call__(self, fully_qualified_name: str, args: list[Type] | None = None) -&gt; Instance:
        ...


</t>
<t tx="ekr.20221004064035.1606">def paramspec_args(
    name: str,
    fullname: str,
    id: TypeVarId | int,
    *,
    named_type_func: _NamedTypeCallback,
    line: int = -1,
    column: int = -1,
    prefix: Parameters | None = None,
) -&gt; ParamSpecType:
    return ParamSpecType(
        name,
        fullname,
        id,
        flavor=ParamSpecFlavor.ARGS,
        upper_bound=named_type_func("builtins.tuple", [named_type_func("builtins.object")]),
        line=line,
        column=column,
        prefix=prefix,
    )


</t>
<t tx="ekr.20221004064035.1607">def paramspec_kwargs(
    name: str,
    fullname: str,
    id: TypeVarId | int,
    *,
    named_type_func: _NamedTypeCallback,
    line: int = -1,
    column: int = -1,
    prefix: Parameters | None = None,
) -&gt; ParamSpecType:
    return ParamSpecType(
        name,
        fullname,
        id,
        flavor=ParamSpecFlavor.KWARGS,
        upper_bound=named_type_func(
            "builtins.dict", [named_type_func("builtins.str"), named_type_func("builtins.object")]
        ),
        line=line,
        column=column,
        prefix=prefix,
    )


</t>
<t tx="ekr.20221004064035.1608">class HasPlaceholders(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_placeholder_type(self, t: PlaceholderType) -&gt; bool:
        return True


</t>
<t tx="ekr.20221004064035.1609">def has_placeholder(typ: Type) -&gt; bool:
    """Check if a type contains any placeholder types (recursively)."""
    return typ.accept(HasPlaceholders())
</t>
<t tx="ekr.20221004064035.161">def is_literal_in_union(x: ProperType, y: ProperType) -&gt; bool:
    """Return True if x is a Literal and y is an Union that includes x"""
    return (
        isinstance(x, LiteralType)
        and isinstance(y, UnionType)
        and any(x == get_proper_type(z) for z in y.items)
    )


</t>
<t tx="ekr.20221004064035.1610">@path C:/Repos/ekr-mypy2/mypy/
"""Verify properties of type arguments, like 'int' in C[int] being valid.

This must happen after semantic analysis since there can be placeholder
types until the end of semantic analysis, and these break various type
operations, including subtype checks.
"""

from __future__ import annotations

from mypy import errorcodes as codes, message_registry
from mypy.errorcodes import ErrorCode
from mypy.errors import Errors
from mypy.messages import format_type
from mypy.mixedtraverser import MixedTraverserVisitor
from mypy.nodes import Block, ClassDef, Context, FakeInfo, FuncItem, MypyFile, TypeInfo
from mypy.options import Options
from mypy.scope import Scope
from mypy.subtypes import is_same_type, is_subtype
from mypy.types import (
    AnyType,
    Instance,
    ParamSpecType,
    TupleType,
    Type,
    TypeAliasType,
    TypeOfAny,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1611">class TypeArgumentAnalyzer(MixedTraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.1612">def __init__(self, errors: Errors, options: Options, is_typeshed_file: bool) -&gt; None:
    self.errors = errors
    self.options = options
    self.is_typeshed_file = is_typeshed_file
    self.scope = Scope()
    # Should we also analyze function definitions, or only module top-levels?
    self.recurse_into_functions = True
    # Keep track of the type aliases already visited. This is needed to avoid
    # infinite recursion on types like A = Union[int, List[A]].
    self.seen_aliases: set[TypeAliasType] = set()

</t>
<t tx="ekr.20221004064035.1613">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.errors.set_file(o.path, o.fullname, scope=self.scope, options=self.options)
    with self.scope.module_scope(o.fullname):
        super().visit_mypy_file(o)

</t>
<t tx="ekr.20221004064035.1614">def visit_func(self, defn: FuncItem) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.scope.function_scope(defn):
        super().visit_func(defn)

</t>
<t tx="ekr.20221004064035.1615">def visit_class_def(self, defn: ClassDef) -&gt; None:
    with self.scope.class_scope(defn.info):
        super().visit_class_def(defn)

</t>
<t tx="ekr.20221004064035.1616">def visit_block(self, o: Block) -&gt; None:
    if not o.is_unreachable:
        super().visit_block(o)

</t>
<t tx="ekr.20221004064035.1617">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    super().visit_type_alias_type(t)
    if t in self.seen_aliases:
        # Avoid infinite recursion on recursive type aliases.
        # Note: it is fine to skip the aliases we have already seen in non-recursive
        # types, since errors there have already been reported.
        return
    self.seen_aliases.add(t)
    # Some recursive aliases may produce spurious args. In principle this is not very
    # important, as we would simply ignore them when expanding, but it is better to keep
    # correct aliases.
    if t.alias and len(t.args) != len(t.alias.alias_tvars):
        t.args = [AnyType(TypeOfAny.from_error) for _ in t.alias.alias_tvars]
    get_proper_type(t).accept(self)

</t>
<t tx="ekr.20221004064035.1618">def visit_instance(self, t: Instance) -&gt; None:
    # Type argument counts were checked in the main semantic analyzer pass. We assume
    # that the counts are correct here.
    info = t.type
    if isinstance(info, FakeInfo):
        return  # https://github.com/python/mypy/issues/11079
    for (i, arg), tvar in zip(enumerate(t.args), info.defn.type_vars):
        if isinstance(tvar, TypeVarType):
            if isinstance(arg, ParamSpecType):
                # TODO: Better message
                self.fail(f'Invalid location for ParamSpec "{arg.name}"', t)
                continue
            if tvar.values:
                if isinstance(arg, TypeVarType):
                    arg_values = arg.values
                    if not arg_values:
                        self.fail(
                            message_registry.INVALID_TYPEVAR_AS_TYPEARG.format(
                                arg.name, info.name
                            ),
                            t,
                            code=codes.TYPE_VAR,
                        )
                        continue
                else:
                    arg_values = [arg]
                self.check_type_var_values(info, arg_values, tvar.name, tvar.values, i + 1, t)
            if not is_subtype(arg, tvar.upper_bound):
                self.fail(
                    message_registry.INVALID_TYPEVAR_ARG_BOUND.format(
                        format_type(arg), info.name, format_type(tvar.upper_bound)
                    ),
                    t,
                    code=codes.TYPE_VAR,
                )
    super().visit_instance(t)

</t>
<t tx="ekr.20221004064035.1619">def visit_unpack_type(self, typ: UnpackType) -&gt; None:
    proper_type = get_proper_type(typ.type)
    if isinstance(proper_type, TupleType):
        return
    if isinstance(proper_type, TypeVarTupleType):
        return
    if isinstance(proper_type, Instance) and proper_type.type.fullname == "builtins.tuple":
        return
    if isinstance(proper_type, AnyType) and proper_type.type_of_any == TypeOfAny.from_error:
        return

    # TODO: Infer something when it can't be unpacked to allow rest of
    # typechecking to work.
    self.fail(message_registry.INVALID_UNPACK.format(proper_type), typ)

</t>
<t tx="ekr.20221004064035.162">def is_overlapping_types(
    left: Type,
    right: Type,
    ignore_promotions: bool = False,
    prohibit_none_typevar_overlap: bool = False,
    ignore_uninhabited: bool = False,
) -&gt; bool:
    """Can a value of type 'left' also be of type 'right' or vice-versa?

    If 'ignore_promotions' is True, we ignore promotions while checking for overlaps.
    If 'prohibit_none_typevar_overlap' is True, we disallow None from overlapping with
    TypeVars (in both strict-optional and non-strict-optional mode).
    """
    if isinstance(left, TypeGuardedType) or isinstance(  # type: ignore[misc]
        right, TypeGuardedType
    ):
        # A type guard forces the new type even if it doesn't overlap the old.
        return True

    left, right = get_proper_types((left, right))

    @others
    if isinstance(left, TypeType) or isinstance(right, TypeType):
        return _type_object_overlap(left, right) or _type_object_overlap(right, left)

    if isinstance(left, CallableType) and isinstance(right, CallableType):

        def _callable_overlap(left: CallableType, right: CallableType) -&gt; bool:
            return is_callable_compatible(
                left,
                right,
                is_compat=_is_overlapping_types,
                ignore_pos_arg_names=True,
                allow_partial_overlap=True,
            )

        # Compare both directions to handle type objects.
        return _callable_overlap(left, right) or _callable_overlap(right, left)
    elif isinstance(left, CallableType):
        left = left.fallback
    elif isinstance(right, CallableType):
        right = right.fallback

    if isinstance(left, LiteralType) and isinstance(right, LiteralType):
        if left.value == right.value:
            # If values are the same, we still need to check if fallbacks are overlapping,
            # this is done below.
            left = left.fallback
            right = right.fallback
        else:
            return False
    elif isinstance(left, LiteralType):
        left = left.fallback
    elif isinstance(right, LiteralType):
        right = right.fallback

    # Finally, we handle the case where left and right are instances.

    if isinstance(left, Instance) and isinstance(right, Instance):
        # First we need to handle promotions and structural compatibility for instances
        # that came as fallbacks, so simply call is_subtype() to avoid code duplication.
        if is_subtype(
            left, right, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
        ) or is_subtype(
            right, left, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
        ):
            return True

        # Two unrelated types cannot be partially overlapping: they're disjoint.
        if left.type.has_base(right.type.fullname):
            left = map_instance_to_supertype(left, right.type)
        elif right.type.has_base(left.type.fullname):
            right = map_instance_to_supertype(right, left.type)
        else:
            return False

        if len(left.args) == len(right.args):
            # Note: we don't really care about variance here, since the overlapping check
            # is symmetric and since we want to return 'True' even for partial overlaps.
            #
            # For example, suppose we have two types Wrapper[Parent] and Wrapper[Child].
            # It doesn't matter whether Wrapper is covariant or contravariant since
            # either way, one of the two types will overlap with the other.
            #
            # Similarly, if Wrapper was invariant, the two types could still be partially
            # overlapping -- what if Wrapper[Parent] happened to contain only instances of
            # specifically Child?
            #
            # Or, to use a more concrete example, List[Union[A, B]] and List[Union[B, C]]
            # would be considered partially overlapping since it's possible for both lists
            # to contain only instances of B at runtime.
            if all(
                _is_overlapping_types(left_arg, right_arg)
                for left_arg, right_arg in zip(left.args, right.args)
            ):
                return True

        return False

    # We ought to have handled every case by now: we conclude the
    # two types are not overlapping, either completely or partially.
    #
    # Note: it's unclear however, whether returning False is the right thing
    # to do when inferring reachability -- see  https://github.com/python/mypy/issues/5529

    assert type(left) != type(right), f"{type(left)} vs {type(right)}"
    return False


</t>
<t tx="ekr.20221004064035.1620">def check_type_var_values(
    self,
    type: TypeInfo,
    actuals: list[Type],
    arg_name: str,
    valids: list[Type],
    arg_number: int,
    context: Context,
) -&gt; None:
    for actual in get_proper_types(actuals):
        # TODO: bind type variables in class bases/alias targets
        # so we can safely check this, currently we miss some errors.
        if not isinstance(actual, (AnyType, UnboundType)) and not any(
            is_same_type(actual, value) for value in valids
        ):
            if len(actuals) &gt; 1 or not isinstance(actual, Instance):
                self.fail(
                    message_registry.INVALID_TYPEVAR_ARG_VALUE.format(type.name),
                    context,
                    code=codes.TYPE_VAR,
                )
            else:
                class_name = f'"{type.name}"'
                actual_type_name = f'"{actual.type.name}"'
                self.fail(
                    message_registry.INCOMPATIBLE_TYPEVAR_VALUE.format(
                        arg_name, class_name, actual_type_name
                    ),
                    context,
                    code=codes.TYPE_VAR,
                )

</t>
<t tx="ekr.20221004064035.1621">def fail(self, msg: str, context: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.errors.report(context.get_line(), context.get_column(), msg, code=code)
</t>
<t tx="ekr.20221004064035.1622">@path C:/Repos/ekr-mypy2/mypy/
"""Semantic analysis of TypedDict definitions."""

from __future__ import annotations

from typing_extensions import Final

from mypy import errorcodes as codes, message_registry
from mypy.errorcodes import ErrorCode
from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    AssignmentStmt,
    CallExpr,
    ClassDef,
    Context,
    DictExpr,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    IndexExpr,
    NameExpr,
    PassStmt,
    RefExpr,
    StrExpr,
    TempNode,
    TupleExpr,
    TypedDictExpr,
    TypeInfo,
)
from mypy.options import Options
from mypy.semanal_shared import SemanticAnalyzerInterface, has_placeholder
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type
from mypy.types import (
    TPDICT_NAMES,
    AnyType,
    RequiredType,
    Type,
    TypedDictType,
    TypeOfAny,
    TypeVarLikeType,
    replace_alias_tvars,
)

TPDICT_CLASS_ERROR: Final = (
    "Invalid statement in TypedDict definition; " 'expected "field_name: field_type"'
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1623">class TypedDictAnalyzer:
    @others
</t>
<t tx="ekr.20221004064035.1624">def __init__(
    self, options: Options, api: SemanticAnalyzerInterface, msg: MessageBuilder
) -&gt; None:
    self.options = options
    self.api = api
    self.msg = msg

</t>
<t tx="ekr.20221004064035.1625">def analyze_typeddict_classdef(self, defn: ClassDef) -&gt; tuple[bool, TypeInfo | None]:
    """Analyze a class that may define a TypedDict.

    Assume that base classes have been analyzed already.

    Note: Unlike normal classes, we won't create a TypeInfo until
    the whole definition of the TypeDict (including the body and all
    key names and types) is complete.  This is mostly because we
    store the corresponding TypedDictType in the TypeInfo.

    Return (is this a TypedDict, new TypeInfo). Specifics:
     * If we couldn't finish due to incomplete reference anywhere in
       the definition, return (True, None).
     * If this is not a TypedDict, return (False, None).
    """
    possible = False
    for base_expr in defn.base_type_exprs:
        if isinstance(base_expr, IndexExpr):
            base_expr = base_expr.base
        if isinstance(base_expr, RefExpr):
            self.api.accept(base_expr)
            if base_expr.fullname in TPDICT_NAMES or self.is_typeddict(base_expr):
                possible = True
                if isinstance(base_expr.node, TypeInfo) and base_expr.node.is_final:
                    err = message_registry.CANNOT_INHERIT_FROM_FINAL
                    self.fail(err.format(base_expr.node.name).value, defn, code=err.code)
    if not possible:
        return False, None
    existing_info = None
    if isinstance(defn.analyzed, TypedDictExpr):
        existing_info = defn.analyzed.info
    if (
        len(defn.base_type_exprs) == 1
        and isinstance(defn.base_type_exprs[0], RefExpr)
        and defn.base_type_exprs[0].fullname in TPDICT_NAMES
    ):
        # Building a new TypedDict
        fields, types, required_keys = self.analyze_typeddict_classdef_fields(defn)
        if fields is None:
            return True, None  # Defer
        info = self.build_typeddict_typeinfo(
            defn.name, fields, types, required_keys, defn.line, existing_info
        )
        defn.analyzed = TypedDictExpr(info)
        defn.analyzed.line = defn.line
        defn.analyzed.column = defn.column
        return True, info

    # Extending/merging existing TypedDicts
    typeddict_bases: list[Expression] = []
    typeddict_bases_set = set()
    for expr in defn.base_type_exprs:
        if isinstance(expr, RefExpr) and expr.fullname in TPDICT_NAMES:
            if "TypedDict" not in typeddict_bases_set:
                typeddict_bases_set.add("TypedDict")
            else:
                self.fail('Duplicate base class "TypedDict"', defn)
        elif isinstance(expr, RefExpr) and self.is_typeddict(expr):
            assert expr.fullname
            if expr.fullname not in typeddict_bases_set:
                typeddict_bases_set.add(expr.fullname)
                typeddict_bases.append(expr)
            else:
                assert isinstance(expr.node, TypeInfo)
                self.fail(f'Duplicate base class "{expr.node.name}"', defn)
        elif isinstance(expr, IndexExpr) and self.is_typeddict(expr.base):
            assert isinstance(expr.base, RefExpr)
            assert expr.base.fullname
            if expr.base.fullname not in typeddict_bases_set:
                typeddict_bases_set.add(expr.base.fullname)
                typeddict_bases.append(expr)
            else:
                assert isinstance(expr.base.node, TypeInfo)
                self.fail(f'Duplicate base class "{expr.base.node.name}"', defn)
        else:
            self.fail("All bases of a new TypedDict must be TypedDict types", defn)

    keys: list[str] = []
    types = []
    required_keys = set()
    # Iterate over bases in reverse order so that leftmost base class' keys take precedence
    for base in reversed(typeddict_bases):
        self.add_keys_and_types_from_base(base, keys, types, required_keys, defn)
    new_keys, new_types, new_required_keys = self.analyze_typeddict_classdef_fields(defn, keys)
    if new_keys is None:
        return True, None  # Defer
    keys.extend(new_keys)
    types.extend(new_types)
    required_keys.update(new_required_keys)
    info = self.build_typeddict_typeinfo(
        defn.name, keys, types, required_keys, defn.line, existing_info
    )
    defn.analyzed = TypedDictExpr(info)
    defn.analyzed.line = defn.line
    defn.analyzed.column = defn.column
    return True, info

</t>
<t tx="ekr.20221004064035.1626">def add_keys_and_types_from_base(
    self,
    base: Expression,
    keys: list[str],
    types: list[Type],
    required_keys: set[str],
    ctx: Context,
) -&gt; None:
    if isinstance(base, RefExpr):
        assert isinstance(base.node, TypeInfo)
        info = base.node
        base_args: list[Type] = []
    else:
        assert isinstance(base, IndexExpr)
        assert isinstance(base.base, RefExpr)
        assert isinstance(base.base.node, TypeInfo)
        info = base.base.node
        args = self.analyze_base_args(base, ctx)
        if args is None:
            return
        base_args = args

    assert info.typeddict_type is not None
    base_typed_dict = info.typeddict_type
    base_items = base_typed_dict.items
    valid_items = base_items.copy()

    # Always fix invalid bases to avoid crashes.
    tvars = info.type_vars
    if len(base_args) != len(tvars):
        any_kind = TypeOfAny.from_omitted_generics
        if base_args:
            self.fail(f'Invalid number of type arguments for "{info.name}"', ctx)
            any_kind = TypeOfAny.from_error
        base_args = [AnyType(any_kind) for _ in tvars]

    valid_items = self.map_items_to_base(valid_items, tvars, base_args)
    for key in base_items:
        if key in keys:
            self.fail(f'Overwriting TypedDict field "{key}" while merging', ctx)
    keys.extend(valid_items.keys())
    types.extend(valid_items.values())
    required_keys.update(base_typed_dict.required_keys)

</t>
<t tx="ekr.20221004064035.1627">def analyze_base_args(self, base: IndexExpr, ctx: Context) -&gt; list[Type] | None:
    """Analyze arguments of base type expressions as types.

    We need to do this, because normal base class processing happens after
    the TypedDict special-casing (plus we get a custom error message).
    """
    base_args = []
    if isinstance(base.index, TupleExpr):
        args = base.index.items
    else:
        args = [base.index]

    for arg_expr in args:
        try:
            type = expr_to_unanalyzed_type(arg_expr, self.options, self.api.is_stub_file)
        except TypeTranslationError:
            self.fail("Invalid TypedDict type argument", ctx)
            return None
        analyzed = self.api.anal_type(
            type,
            allow_required=True,
            allow_placeholder=not self.options.disable_recursive_aliases
            and not self.api.is_func_scope(),
        )
        if analyzed is None:
            return None
        base_args.append(analyzed)
    return base_args

</t>
<t tx="ekr.20221004064035.1628">def map_items_to_base(
    self, valid_items: dict[str, Type], tvars: list[str], base_args: list[Type]
) -&gt; dict[str, Type]:
    """Map item types to how they would look in their base with type arguments applied.

    We would normally use expand_type() for such task, but we can't use it during
    semantic analysis, because it can (indirectly) call is_subtype() etc., and it
    will crash on placeholder types. So we hijack replace_alias_tvars() that was initially
    intended to deal with eager expansion of generic type aliases during semantic analysis.
    """
    mapped_items = {}
    for key in valid_items:
        type_in_base = valid_items[key]
        if not tvars:
            mapped_items[key] = type_in_base
            continue
        mapped_type = replace_alias_tvars(
            type_in_base, tvars, base_args, type_in_base.line, type_in_base.column
        )
        mapped_items[key] = mapped_type
    return mapped_items

</t>
<t tx="ekr.20221004064035.1629">def analyze_typeddict_classdef_fields(
    self, defn: ClassDef, oldfields: list[str] | None = None
) -&gt; tuple[list[str] | None, list[Type], set[str]]:
    """Analyze fields defined in a TypedDict class definition.

    This doesn't consider inherited fields (if any). Also consider totality,
    if given.

    Return tuple with these items:
     * List of keys (or None if found an incomplete reference --&gt; deferral)
     * List of types for each key
     * Set of required keys
    """
    fields: list[str] = []
    types: list[Type] = []
    for stmt in defn.defs.body:
        if not isinstance(stmt, AssignmentStmt):
            # Still allow pass or ... (for empty TypedDict's).
            if not isinstance(stmt, PassStmt) and not (
                isinstance(stmt, ExpressionStmt)
                and isinstance(stmt.expr, (EllipsisExpr, StrExpr))
            ):
                self.fail(TPDICT_CLASS_ERROR, stmt)
        elif len(stmt.lvalues) &gt; 1 or not isinstance(stmt.lvalues[0], NameExpr):
            # An assignment, but an invalid one.
            self.fail(TPDICT_CLASS_ERROR, stmt)
        else:
            name = stmt.lvalues[0].name
            if name in (oldfields or []):
                self.fail(f'Overwriting TypedDict field "{name}" while extending', stmt)
            if name in fields:
                self.fail(f'Duplicate TypedDict key "{name}"', stmt)
                continue
            # Append name and type in this case...
            fields.append(name)
            if stmt.type is None:
                types.append(AnyType(TypeOfAny.unannotated))
            else:
                analyzed = self.api.anal_type(
                    stmt.type,
                    allow_required=True,
                    allow_placeholder=not self.options.disable_recursive_aliases
                    and not self.api.is_func_scope(),
                )
                if analyzed is None:
                    return None, [], set()  # Need to defer
                types.append(analyzed)
            # ...despite possible minor failures that allow further analyzis.
            if stmt.type is None or hasattr(stmt, "new_syntax") and not stmt.new_syntax:
                self.fail(TPDICT_CLASS_ERROR, stmt)
            elif not isinstance(stmt.rvalue, TempNode):
                # x: int assigns rvalue to TempNode(AnyType())
                self.fail("Right hand side values are not supported in TypedDict", stmt)
    total: bool | None = True
    if "total" in defn.keywords:
        total = self.api.parse_bool(defn.keywords["total"])
        if total is None:
            self.fail('Value of "total" must be True or False', defn)
            total = True
    required_keys = {
        field
        for (field, t) in zip(fields, types)
        if (total or (isinstance(t, RequiredType) and t.required))
        and not (isinstance(t, RequiredType) and not t.required)
    }
    types = [  # unwrap Required[T] to just T
        t.item if isinstance(t, RequiredType) else t for t in types
    ]

    return fields, types, required_keys

</t>
<t tx="ekr.20221004064035.163">def _is_overlapping_types(left: Type, right: Type) -&gt; bool:
    """Encode the kind of overlapping check to perform.

    This function mostly exists so we don't have to repeat keyword arguments everywhere."""
    return is_overlapping_types(
        left,
        right,
        ignore_promotions=ignore_promotions,
        prohibit_none_typevar_overlap=prohibit_none_typevar_overlap,
        ignore_uninhabited=ignore_uninhabited,
    )

</t>
<t tx="ekr.20221004064035.1630">def check_typeddict(
    self, node: Expression, var_name: str | None, is_func_scope: bool
) -&gt; tuple[bool, TypeInfo | None, list[TypeVarLikeType]]:
    """Check if a call defines a TypedDict.

    The optional var_name argument is the name of the variable to
    which this is assigned, if any.

    Return a pair (is it a typed dict, corresponding TypeInfo).

    If the definition is invalid but looks like a TypedDict,
    report errors but return (some) TypeInfo. If some type is not ready,
    return (True, None).
    """
    if not isinstance(node, CallExpr):
        return False, None, []
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return False, None, []
    fullname = callee.fullname
    if fullname not in TPDICT_NAMES:
        return False, None, []
    res = self.parse_typeddict_args(call)
    if res is None:
        # This is a valid typed dict, but some type is not ready.
        # The caller should defer this until next iteration.
        return True, None, []
    name, items, types, total, tvar_defs, ok = res
    if not ok:
        # Error. Construct dummy return value.
        info = self.build_typeddict_typeinfo("TypedDict", [], [], set(), call.line, None)
    else:
        if var_name is not None and name != var_name:
            self.fail(
                'First argument "{}" to TypedDict() does not match variable name "{}"'.format(
                    name, var_name
                ),
                node,
                code=codes.NAME_MATCH,
            )
        if name != var_name or is_func_scope:
            # Give it a unique name derived from the line number.
            name += "@" + str(call.line)
        required_keys = {
            field
            for (field, t) in zip(items, types)
            if (total or (isinstance(t, RequiredType) and t.required))
            and not (isinstance(t, RequiredType) and not t.required)
        }
        types = [  # unwrap Required[T] to just T
            t.item if isinstance(t, RequiredType) else t for t in types
        ]
        existing_info = None
        if isinstance(node.analyzed, TypedDictExpr):
            existing_info = node.analyzed.info
        info = self.build_typeddict_typeinfo(
            name, items, types, required_keys, call.line, existing_info
        )
        info.line = node.line
        # Store generated TypeInfo under both names, see semanal_namedtuple for more details.
        if name != var_name or is_func_scope:
            self.api.add_symbol_skip_local(name, info)
    if var_name:
        self.api.add_symbol(var_name, info, node)
    call.analyzed = TypedDictExpr(info)
    call.analyzed.set_line(call)
    return True, info, tvar_defs

</t>
<t tx="ekr.20221004064035.1631">def parse_typeddict_args(
    self, call: CallExpr
) -&gt; tuple[str, list[str], list[Type], bool, list[TypeVarLikeType], bool] | None:
    """Parse typed dict call expression.

    Return names, types, totality, was there an error during parsing.
    If some type is not ready, return None.
    """
    # TODO: Share code with check_argument_count in checkexpr.py?
    args = call.args
    if len(args) &lt; 2:
        return self.fail_typeddict_arg("Too few arguments for TypedDict()", call)
    if len(args) &gt; 3:
        return self.fail_typeddict_arg("Too many arguments for TypedDict()", call)
    # TODO: Support keyword arguments
    if call.arg_kinds not in ([ARG_POS, ARG_POS], [ARG_POS, ARG_POS, ARG_NAMED]):
        return self.fail_typeddict_arg("Unexpected arguments to TypedDict()", call)
    if len(args) == 3 and call.arg_names[2] != "total":
        return self.fail_typeddict_arg(
            f'Unexpected keyword argument "{call.arg_names[2]}" for "TypedDict"', call
        )
    if not isinstance(args[0], StrExpr):
        return self.fail_typeddict_arg(
            "TypedDict() expects a string literal as the first argument", call
        )
    if not isinstance(args[1], DictExpr):
        return self.fail_typeddict_arg(
            "TypedDict() expects a dictionary literal as the second argument", call
        )
    total: bool | None = True
    if len(args) == 3:
        total = self.api.parse_bool(call.args[2])
        if total is None:
            return self.fail_typeddict_arg(
                'TypedDict() "total" argument must be True or False', call
            )
    dictexpr = args[1]
    tvar_defs = self.api.get_and_bind_all_tvars([t for k, t in dictexpr.items])
    res = self.parse_typeddict_fields_with_types(dictexpr.items, call)
    if res is None:
        # One of the types is not ready, defer.
        return None
    items, types, ok = res
    for t in types:
        check_for_explicit_any(
            t, self.options, self.api.is_typeshed_stub_file, self.msg, context=call
        )

    if self.options.disallow_any_unimported:
        for t in types:
            if has_any_from_unimported_type(t):
                self.msg.unimported_type_becomes_any("Type of a TypedDict key", t, dictexpr)
    assert total is not None
    return args[0].value, items, types, total, tvar_defs, ok

</t>
<t tx="ekr.20221004064035.1632">def parse_typeddict_fields_with_types(
    self, dict_items: list[tuple[Expression | None, Expression]], context: Context
) -&gt; tuple[list[str], list[Type], bool] | None:
    """Parse typed dict items passed as pairs (name expression, type expression).

    Return names, types, was there an error. If some type is not ready, return None.
    """
    seen_keys = set()
    items: list[str] = []
    types: list[Type] = []
    for (field_name_expr, field_type_expr) in dict_items:
        if isinstance(field_name_expr, StrExpr):
            key = field_name_expr.value
            items.append(key)
            if key in seen_keys:
                self.fail(f'Duplicate TypedDict key "{key}"', field_name_expr)
            seen_keys.add(key)
        else:
            name_context = field_name_expr or field_type_expr
            self.fail_typeddict_arg("Invalid TypedDict() field name", name_context)
            return [], [], False
        try:
            type = expr_to_unanalyzed_type(
                field_type_expr, self.options, self.api.is_stub_file
            )
        except TypeTranslationError:
            if (
                isinstance(field_type_expr, CallExpr)
                and isinstance(field_type_expr.callee, RefExpr)
                and field_type_expr.callee.fullname in TPDICT_NAMES
            ):
                self.fail_typeddict_arg(
                    "Inline TypedDict types not supported; use assignment to define TypedDict",
                    field_type_expr,
                )
            else:
                self.fail_typeddict_arg("Invalid field type", field_type_expr)
            return [], [], False
        analyzed = self.api.anal_type(
            type,
            allow_required=True,
            allow_placeholder=not self.options.disable_recursive_aliases
            and not self.api.is_func_scope(),
        )
        if analyzed is None:
            return None
        types.append(analyzed)
    return items, types, True

</t>
<t tx="ekr.20221004064035.1633">def fail_typeddict_arg(
    self, message: str, context: Context
) -&gt; tuple[str, list[str], list[Type], bool, list[TypeVarLikeType], bool]:
    self.fail(message, context)
    return "", [], [], True, [], False

</t>
<t tx="ekr.20221004064035.1634">def build_typeddict_typeinfo(
    self,
    name: str,
    items: list[str],
    types: list[Type],
    required_keys: set[str],
    line: int,
    existing_info: TypeInfo | None,
) -&gt; TypeInfo:
    # Prefer typing then typing_extensions if available.
    fallback = (
        self.api.named_type_or_none("typing._TypedDict", [])
        or self.api.named_type_or_none("typing_extensions._TypedDict", [])
        or self.api.named_type_or_none("mypy_extensions._TypedDict", [])
    )
    assert fallback is not None
    info = existing_info or self.api.basic_new_typeinfo(name, fallback, line)
    typeddict_type = TypedDictType(dict(zip(items, types)), required_keys, fallback)
    if info.special_alias and has_placeholder(info.special_alias.target):
        self.api.defer(force_progress=True)
    info.update_typeddict_type(typeddict_type)
    return info

</t>
<t tx="ekr.20221004064035.1635"># Helpers

</t>
<t tx="ekr.20221004064035.1636">def is_typeddict(self, expr: Expression) -&gt; bool:
    return (
        isinstance(expr, RefExpr)
        and isinstance(expr.node, TypeInfo)
        and expr.node.typeddict_type is not None
    )

</t>
<t tx="ekr.20221004064035.1637">def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.api.fail(msg, ctx, code=code)

</t>
<t tx="ekr.20221004064035.1638">def note(self, msg: str, ctx: Context) -&gt; None:
    self.api.note(msg, ctx)
</t>
<t tx="ekr.20221004064035.1639">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing_extensions import Final

"""Shared logic between our three mypy parser files."""


_NON_BINARY_MAGIC_METHODS: Final = {
    "__abs__",
    "__call__",
    "__complex__",
    "__contains__",
    "__del__",
    "__delattr__",
    "__delitem__",
    "__enter__",
    "__exit__",
    "__float__",
    "__getattr__",
    "__getattribute__",
    "__getitem__",
    "__hex__",
    "__init__",
    "__init_subclass__",
    "__int__",
    "__invert__",
    "__iter__",
    "__len__",
    "__long__",
    "__neg__",
    "__new__",
    "__oct__",
    "__pos__",
    "__repr__",
    "__reversed__",
    "__setattr__",
    "__setitem__",
    "__str__",
}

MAGIC_METHODS_ALLOWING_KWARGS: Final = {
    "__init__",
    "__init_subclass__",
    "__new__",
    "__call__",
    "__setattr__",
}

BINARY_MAGIC_METHODS: Final = {
    "__add__",
    "__and__",
    "__divmod__",
    "__eq__",
    "__floordiv__",
    "__ge__",
    "__gt__",
    "__iadd__",
    "__iand__",
    "__idiv__",
    "__ifloordiv__",
    "__ilshift__",
    "__imatmul__",
    "__imod__",
    "__imul__",
    "__ior__",
    "__ipow__",
    "__irshift__",
    "__isub__",
    "__itruediv__",
    "__ixor__",
    "__le__",
    "__lshift__",
    "__lt__",
    "__matmul__",
    "__mod__",
    "__mul__",
    "__ne__",
    "__or__",
    "__pow__",
    "__radd__",
    "__rand__",
    "__rdiv__",
    "__rfloordiv__",
    "__rlshift__",
    "__rmatmul__",
    "__rmod__",
    "__rmul__",
    "__ror__",
    "__rpow__",
    "__rrshift__",
    "__rshift__",
    "__rsub__",
    "__rtruediv__",
    "__rxor__",
    "__sub__",
    "__truediv__",
    "__xor__",
}

assert not (_NON_BINARY_MAGIC_METHODS &amp; BINARY_MAGIC_METHODS)

MAGIC_METHODS: Final = _NON_BINARY_MAGIC_METHODS | BINARY_MAGIC_METHODS

MAGIC_METHODS_POS_ARGS_ONLY: Final = MAGIC_METHODS - MAGIC_METHODS_ALLOWING_KWARGS


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.164"># We should never encounter this type.
if isinstance(left, PartialType) or isinstance(right, PartialType):
    assert False, "Unexpectedly encountered partial type"

# We should also never encounter these types, but it's possible a few
# have snuck through due to unrelated bugs. For now, we handle these
# in the same way we handle 'Any'.
#
# TODO: Replace these with an 'assert False' once we are more confident.
illegal_types = (UnboundType, ErasedType, DeletedType)
if isinstance(left, illegal_types) or isinstance(right, illegal_types):
    return True

# When running under non-strict optional mode, simplify away types of
# the form 'Union[A, B, C, None]' into just 'Union[A, B, C]'.

if not state.strict_optional:
    if isinstance(left, UnionType):
        left = UnionType.make_union(left.relevant_items())
    if isinstance(right, UnionType):
        right = UnionType.make_union(right.relevant_items())
    left, right = get_proper_types((left, right))

# 'Any' may or may not be overlapping with the other type
if isinstance(left, AnyType) or isinstance(right, AnyType):
    return True

# We check for complete overlaps next as a general-purpose failsafe.
# If this check fails, we start checking to see if there exists a
# *partial* overlap between types.
#
# These checks will also handle the NoneType and UninhabitedType cases for us.

# enums are sometimes expanded into an Union of Literals
# when that happens we want to make sure we treat the two as overlapping
# and crucially, we want to do that *fast* in case the enum is large
# so we do it before expanding variants below to avoid O(n**2) behavior
if (
    is_enum_overlapping_union(left, right)
    or is_enum_overlapping_union(right, left)
    or is_literal_in_union(left, right)
    or is_literal_in_union(right, left)
):
    return True

if is_proper_subtype(
    left, right, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
) or is_proper_subtype(
    right, left, ignore_promotions=ignore_promotions, ignore_uninhabited=ignore_uninhabited
):
    return True

# See the docstring for 'get_possible_variants' for more info on what the
# following lines are doing.

left_possible = get_possible_variants(left)
right_possible = get_possible_variants(right)

# We start by checking multi-variant types like Unions first. We also perform
# the same logic if either type happens to be a TypeVar/ParamSpec/TypeVarTuple.
#
# Handling the TypeVarLikes now lets us simulate having them bind to the corresponding
# type -- if we deferred these checks, the "return-early" logic of the other
# checks will prevent us from detecting certain overlaps.
#
# If both types are singleton variants (and are not TypeVarLikes), we've hit the base case:
# we skip these checks to avoid infinitely recursing.

</t>
<t tx="ekr.20221004064035.1640">def special_function_elide_names(name: str) -&gt; bool:
    return name in MAGIC_METHODS_POS_ARGS_ONLY


</t>
<t tx="ekr.20221004064035.1641">def argument_elide_name(name: str | None) -&gt; bool:
    return name is not None and name.startswith("__") and not name.endswith("__")
</t>
<t tx="ekr.20221004064035.1642">@path C:/Repos/ekr-mypy2/mypy/
"""Type inference constraint solving"""

from __future__ import annotations

from collections import defaultdict

from mypy.constraints import SUPERTYPE_OF, Constraint
from mypy.join import join_types
from mypy.meet import meet_types
from mypy.subtypes import is_subtype
from mypy.types import (
    AnyType,
    ProperType,
    Type,
    TypeOfAny,
    TypeVarId,
    UninhabitedType,
    UnionType,
    get_proper_type,
)
from mypy.typestate import TypeState


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1643">def solve_constraints(
    vars: list[TypeVarId], constraints: list[Constraint], strict: bool = True
) -&gt; list[Type | None]:
    """Solve type constraints.

    Return the best type(s) for type variables; each type can be None if the value of the variable
    could not be solved.

    If a variable has no constraints, if strict=True then arbitrarily
    pick NoneType as the value of the type variable.  If strict=False,
    pick AnyType.
    """
    # Collect a list of constraints for each type variable.
    cmap: dict[TypeVarId, list[Constraint]] = defaultdict(list)
    for con in constraints:
        cmap[con.type_var].append(con)

    res: list[Type | None] = []

    # Solve each type variable separately.
    for tvar in vars:
        bottom: Type | None = None
        top: Type | None = None
        candidate: Type | None = None

        # Process each constraint separately, and calculate the lower and upper
        # bounds based on constraints. Note that we assume that the constraint
        # targets do not have constraint references.
        for c in cmap.get(tvar, []):
            if c.op == SUPERTYPE_OF:
                if bottom is None:
                    bottom = c.target
                else:
                    if TypeState.infer_unions:
                        # This deviates from the general mypy semantics because
                        # recursive types are union-heavy in 95% of cases.
                        bottom = UnionType.make_union([bottom, c.target])
                    else:
                        bottom = join_types(bottom, c.target)
            else:
                if top is None:
                    top = c.target
                else:
                    top = meet_types(top, c.target)

        p_top = get_proper_type(top)
        p_bottom = get_proper_type(bottom)
        if isinstance(p_top, AnyType) or isinstance(p_bottom, AnyType):
            source_any = top if isinstance(p_top, AnyType) else bottom
            assert isinstance(source_any, ProperType) and isinstance(source_any, AnyType)
            res.append(AnyType(TypeOfAny.from_another_any, source_any=source_any))
            continue
        elif bottom is None:
            if top:
                candidate = top
            else:
                # No constraints for type variable -- 'UninhabitedType' is the most specific type.
                if strict:
                    candidate = UninhabitedType()
                    candidate.ambiguous = True
                else:
                    candidate = AnyType(TypeOfAny.special_form)
        elif top is None:
            candidate = bottom
        elif is_subtype(bottom, top):
            candidate = bottom
        else:
            candidate = None
        res.append(candidate)

    return res
</t>
<t tx="ekr.20221004064035.1644">@path C:/Repos/ekr-mypy2/mypy/
"""Split namespace for argparse to allow separating options by prefix.

We use this to direct some options to an Options object and some to a
regular namespace.
"""

# In its own file largely because mypyc doesn't support its use of
# __getattr__/__setattr__ and has some issues with __dict__

from __future__ import annotations

import argparse
from typing import Any


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1645">class SplitNamespace(argparse.Namespace):
    @others
</t>
<t tx="ekr.20221004064035.1646">def __init__(self, standard_namespace: object, alt_namespace: object, alt_prefix: str) -&gt; None:
    self.__dict__["_standard_namespace"] = standard_namespace
    self.__dict__["_alt_namespace"] = alt_namespace
    self.__dict__["_alt_prefix"] = alt_prefix

</t>
<t tx="ekr.20221004064035.1647">def _get(self) -&gt; tuple[Any, Any]:
    return (self._standard_namespace, self._alt_namespace)

</t>
<t tx="ekr.20221004064035.1648">def __setattr__(self, name: str, value: Any) -&gt; None:
    if name.startswith(self._alt_prefix):
        setattr(self._alt_namespace, name[len(self._alt_prefix) :], value)
    else:
        setattr(self._standard_namespace, name, value)

</t>
<t tx="ekr.20221004064035.1649">def __getattr__(self, name: str) -&gt; Any:
    if name.startswith(self._alt_prefix):
        return getattr(self._alt_namespace, name[len(self._alt_prefix) :])
    else:
        return getattr(self._standard_namespace, name)
</t>
<t tx="ekr.20221004064035.165">def is_none_typevarlike_overlap(t1: Type, t2: Type) -&gt; bool:
    t1, t2 = get_proper_types((t1, t2))
    return isinstance(t1, NoneType) and isinstance(t2, TypeVarLikeType)

</t>
<t tx="ekr.20221004064035.1650">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from contextlib import contextmanager
from typing import Iterator
from typing_extensions import Final

# These are global mutable state. Don't add anything here unless there's a very
# good reason.


@others
state: Final = StrictOptionalState(strict_optional=False)
find_occurrences: tuple[str, str] | None = None
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1651">class StrictOptionalState:
    # Wrap this in a class since it's faster that using a module-level attribute.

    @others
</t>
<t tx="ekr.20221004064035.1652">def __init__(self, strict_optional: bool) -&gt; None:
    # Value varies by file being processed
    self.strict_optional = strict_optional

</t>
<t tx="ekr.20221004064035.1653">@contextmanager
def strict_optional_set(self, value: bool) -&gt; Iterator[None]:
    saved = self.strict_optional
    self.strict_optional = value
    try:
        yield
    finally:
        self.strict_optional = saved


</t>
<t tx="ekr.20221004064035.1654">@path C:/Repos/ekr-mypy2/mypy/
"""Utilities for calculating and reporting statistics about types."""

from __future__ import annotations

import os
from collections import Counter
from contextlib import contextmanager
from typing import Iterator, cast
from typing_extensions import Final

from mypy import nodes
from mypy.argmap import map_formals_to_actuals
from mypy.nodes import (
    AssignmentExpr,
    AssignmentStmt,
    BreakStmt,
    BytesExpr,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ContinueStmt,
    EllipsisExpr,
    Expression,
    ExpressionStmt,
    FloatExpr,
    FuncDef,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    MemberExpr,
    MypyFile,
    NameExpr,
    Node,
    OpExpr,
    PassStmt,
    RefExpr,
    StrExpr,
    TypeApplication,
    UnaryExpr,
    YieldFromExpr,
)
from mypy.traverser import TraverserVisitor
from mypy.typeanal import collect_all_inner_types
from mypy.types import (
    AnyType,
    CallableType,
    FunctionLike,
    Instance,
    TupleType,
    Type,
    TypeOfAny,
    TypeQuery,
    TypeVarType,
    get_proper_type,
    get_proper_types,
)
from mypy.util import correct_relative_import

TYPE_EMPTY: Final = 0
TYPE_UNANALYZED: Final = 1  # type of non-typechecked code
TYPE_PRECISE: Final = 2
TYPE_IMPRECISE: Final = 3
TYPE_ANY: Final = 4

precision_names: Final = ["empty", "unanalyzed", "precise", "imprecise", "any"]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1655">class StatisticsVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.1656">def __init__(
    self,
    inferred: bool,
    filename: str,
    modules: dict[str, MypyFile],
    typemap: dict[Expression, Type] | None = None,
    all_nodes: bool = False,
    visit_untyped_defs: bool = True,
) -&gt; None:
    self.inferred = inferred
    self.filename = filename
    self.modules = modules
    self.typemap = typemap
    self.all_nodes = all_nodes
    self.visit_untyped_defs = visit_untyped_defs

    self.num_precise_exprs = 0
    self.num_imprecise_exprs = 0
    self.num_any_exprs = 0

    self.num_simple_types = 0
    self.num_generic_types = 0
    self.num_tuple_types = 0
    self.num_function_types = 0
    self.num_typevar_types = 0
    self.num_complex_types = 0
    self.num_any_types = 0

    self.line = -1

    self.line_map: dict[int, int] = {}

    self.type_of_any_counter: Counter[int] = Counter()
    self.any_line_map: dict[int, list[AnyType]] = {}

    # For each scope (top level/function), whether the scope was type checked
    # (annotated function).
    #
    # TODO: Handle --check-untyped-defs
    self.checked_scopes = [True]

    self.output: list[str] = []

    TraverserVisitor.__init__(self)

</t>
<t tx="ekr.20221004064035.1657">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.cur_mod_node = o
    self.cur_mod_id = o.fullname
    super().visit_mypy_file(o)

</t>
<t tx="ekr.20221004064035.1658">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    self.process_import(imp)

</t>
<t tx="ekr.20221004064035.1659">def visit_import_all(self, imp: ImportAll) -&gt; None:
    self.process_import(imp)

</t>
<t tx="ekr.20221004064035.166">if prohibit_none_typevar_overlap:
    if is_none_typevarlike_overlap(left, right) or is_none_typevarlike_overlap(right, left):
        return False

if (
    len(left_possible) &gt; 1
    or len(right_possible) &gt; 1
    or isinstance(left, TypeVarLikeType)
    or isinstance(right, TypeVarLikeType)
):
    for l in left_possible:
        for r in right_possible:
            if _is_overlapping_types(l, r):
                return True
    return False

# Now that we've finished handling TypeVarLikes, we're free to end early
# if one one of the types is None and we're running in strict-optional mode.
# (None only overlaps with None in strict-optional mode).
#
# We must perform this check after the TypeVarLike checks because
# a TypeVar could be bound to None, for example.

if state.strict_optional and isinstance(left, NoneType) != isinstance(right, NoneType):
    return False

# Next, we handle single-variant types that may be inherently partially overlapping:
#
# - TypedDicts
# - Tuples
#
# If we cannot identify a partial overlap and end early, we degrade these two types
# into their 'Instance' fallbacks.

if isinstance(left, TypedDictType) and isinstance(right, TypedDictType):
    return are_typed_dicts_overlapping(left, right, ignore_promotions=ignore_promotions)
elif typed_dict_mapping_pair(left, right):
    # Overlaps between TypedDicts and Mappings require dedicated logic.
    return typed_dict_mapping_overlap(left, right, overlapping=_is_overlapping_types)
elif isinstance(left, TypedDictType):
    left = left.fallback
elif isinstance(right, TypedDictType):
    right = right.fallback

if is_tuple(left) and is_tuple(right):
    return are_tuples_overlapping(left, right, ignore_promotions=ignore_promotions)
elif isinstance(left, TupleType):
    left = tuple_fallback(left)
elif isinstance(right, TupleType):
    right = tuple_fallback(right)

# Next, we handle single-variant types that cannot be inherently partially overlapping,
# but do require custom logic to inspect.
#
# As before, we degrade into 'Instance' whenever possible.

if isinstance(left, TypeType) and isinstance(right, TypeType):
    return _is_overlapping_types(left.item, right.item)

</t>
<t tx="ekr.20221004064035.1660">def process_import(self, imp: ImportFrom | ImportAll) -&gt; None:
    import_id, ok = correct_relative_import(
        self.cur_mod_id, imp.relative, imp.id, self.cur_mod_node.is_package_init_file()
    )
    if ok and import_id in self.modules:
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(imp.line, kind)

</t>
<t tx="ekr.20221004064035.1661">def visit_import(self, imp: Import) -&gt; None:
    if all(id in self.modules for id, _ in imp.ids):
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(imp.line, kind)

</t>
<t tx="ekr.20221004064035.1662">def visit_func_def(self, o: FuncDef) -&gt; None:
    with self.enter_scope(o):
        self.line = o.line
        if len(o.expanded) &gt; 1 and o.expanded != [o] * len(o.expanded):
            if o in o.expanded:
                print(
                    "{}:{}: ERROR: cycle in function expansion; skipping".format(
                        self.filename, o.get_line()
                    )
                )
                return
            for defn in o.expanded:
                self.visit_func_def(cast(FuncDef, defn))
        else:
            if o.type:
                sig = cast(CallableType, o.type)
                arg_types = sig.arg_types
                if sig.arg_names and sig.arg_names[0] == "self" and not self.inferred:
                    arg_types = arg_types[1:]
                for arg in arg_types:
                    self.type(arg)
                self.type(sig.ret_type)
            elif self.all_nodes:
                self.record_line(self.line, TYPE_ANY)
            if not o.is_dynamic() or self.visit_untyped_defs:
                super().visit_func_def(o)

</t>
<t tx="ekr.20221004064035.1663">@contextmanager
def enter_scope(self, o: FuncDef) -&gt; Iterator[None]:
    self.checked_scopes.append(o.type is not None and self.checked_scopes[-1])
    yield None
    self.checked_scopes.pop()

</t>
<t tx="ekr.20221004064035.1664">def is_checked_scope(self) -&gt; bool:
    return self.checked_scopes[-1]

</t>
<t tx="ekr.20221004064035.1665">def visit_class_def(self, o: ClassDef) -&gt; None:
    self.record_line(o.line, TYPE_PRECISE)  # TODO: Look at base classes
    # Override this method because we don't want to analyze base_type_exprs (base_type_exprs
    # are base classes in a class declaration).
    # While base_type_exprs are technically expressions, type analyzer does not visit them and
    # they are not in the typemap.
    for d in o.decorators:
        d.accept(self)
    o.defs.accept(self)

</t>
<t tx="ekr.20221004064035.1666">def visit_type_application(self, o: TypeApplication) -&gt; None:
    self.line = o.line
    for t in o.types:
        self.type(t)
    super().visit_type_application(o)

</t>
<t tx="ekr.20221004064035.1667">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    self.line = o.line
    if isinstance(o.rvalue, nodes.CallExpr) and isinstance(
        o.rvalue.analyzed, nodes.TypeVarExpr
    ):
        # Type variable definition -- not a real assignment.
        return
    if o.type:
        self.type(o.type)
    elif self.inferred and not self.all_nodes:
        # if self.all_nodes is set, lvalues will be visited later
        for lvalue in o.lvalues:
            if isinstance(lvalue, nodes.TupleExpr):
                items = lvalue.items
            else:
                items = [lvalue]
            for item in items:
                if isinstance(item, RefExpr) and item.is_inferred_def:
                    if self.typemap is not None:
                        self.type(self.typemap.get(item))
    super().visit_assignment_stmt(o)

</t>
<t tx="ekr.20221004064035.1668">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    if isinstance(o.expr, (StrExpr, BytesExpr)):
        # Docstring
        self.record_line(o.line, TYPE_EMPTY)
    else:
        super().visit_expression_stmt(o)

</t>
<t tx="ekr.20221004064035.1669">def visit_pass_stmt(self, o: PassStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.167">def _type_object_overlap(left: Type, right: Type) -&gt; bool:
    """Special cases for type object types overlaps."""
    # TODO: these checks are a bit in gray area, adjust if they cause problems.
    left, right = get_proper_types((left, right))
    # 1. Type[C] vs Callable[..., C] overlap even if the latter is not class object.
    if isinstance(left, TypeType) and isinstance(right, CallableType):
        return _is_overlapping_types(left.item, right.ret_type)
    # 2. Type[C] vs Meta, where Meta is a metaclass for C.
    if isinstance(left, TypeType) and isinstance(right, Instance):
        if isinstance(left.item, Instance):
            left_meta = left.item.type.metaclass_type
            if left_meta is not None:
                return _is_overlapping_types(left_meta, right)
            # builtins.type (default metaclass) overlaps with all metaclasses
            return right.type.has_base("builtins.type")
        elif isinstance(left.item, AnyType):
            return right.type.has_base("builtins.type")
    # 3. Callable[..., C] vs Meta is considered below, when we switch to fallbacks.
    return False

</t>
<t tx="ekr.20221004064035.1670">def visit_break_stmt(self, o: BreakStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1671">def visit_continue_stmt(self, o: ContinueStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1672">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if o.fullname in ("builtins.None", "builtins.True", "builtins.False", "builtins.Ellipsis"):
        self.record_precise_if_checked_scope(o)
    else:
        self.process_node(o)
        super().visit_name_expr(o)

</t>
<t tx="ekr.20221004064035.1673">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    if o.expr:
        o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.1674">def visit_call_expr(self, o: CallExpr) -&gt; None:
    self.process_node(o)
    if o.analyzed:
        o.analyzed.accept(self)
    else:
        o.callee.accept(self)
        for a in o.args:
            a.accept(self)
        self.record_call_target_precision(o)

</t>
<t tx="ekr.20221004064035.1675">def record_call_target_precision(self, o: CallExpr) -&gt; None:
    """Record precision of formal argument types used in a call."""
    if not self.typemap or o.callee not in self.typemap:
        # Type not available.
        return
    callee_type = get_proper_type(self.typemap[o.callee])
    if isinstance(callee_type, CallableType):
        self.record_callable_target_precision(o, callee_type)
    else:
        pass  # TODO: Handle overloaded functions, etc.

</t>
<t tx="ekr.20221004064035.1676">def record_callable_target_precision(self, o: CallExpr, callee: CallableType) -&gt; None:
    """Record imprecision caused by callee argument types.

    This only considers arguments passed in a call expression. Arguments
    with default values that aren't provided in a call arguably don't
    contribute to typing imprecision at the *call site* (but they
    contribute at the function definition).
    """
    assert self.typemap
    typemap = self.typemap
    actual_to_formal = map_formals_to_actuals(
        o.arg_kinds,
        o.arg_names,
        callee.arg_kinds,
        callee.arg_names,
        lambda n: typemap[o.args[n]],
    )
    for formals in actual_to_formal:
        for n in formals:
            formal = get_proper_type(callee.arg_types[n])
            if isinstance(formal, AnyType):
                self.record_line(o.line, TYPE_ANY)
            elif is_imprecise(formal):
                self.record_line(o.line, TYPE_IMPRECISE)

</t>
<t tx="ekr.20221004064035.1677">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    self.process_node(o)
    super().visit_member_expr(o)

</t>
<t tx="ekr.20221004064035.1678">def visit_op_expr(self, o: OpExpr) -&gt; None:
    self.process_node(o)
    super().visit_op_expr(o)

</t>
<t tx="ekr.20221004064035.1679">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    self.process_node(o)
    super().visit_comparison_expr(o)

</t>
<t tx="ekr.20221004064035.168">def is_overlapping_erased_types(
    left: Type, right: Type, *, ignore_promotions: bool = False
) -&gt; bool:
    """The same as 'is_overlapping_erased_types', except the types are erased first."""
    return is_overlapping_types(
        erase_type(left),
        erase_type(right),
        ignore_promotions=ignore_promotions,
        prohibit_none_typevar_overlap=True,
    )


</t>
<t tx="ekr.20221004064035.1680">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    self.process_node(o)
    super().visit_index_expr(o)

</t>
<t tx="ekr.20221004064035.1681">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    self.process_node(o)
    super().visit_assignment_expr(o)

</t>
<t tx="ekr.20221004064035.1682">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    self.process_node(o)
    super().visit_unary_expr(o)

</t>
<t tx="ekr.20221004064035.1683">def visit_str_expr(self, o: StrExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1684">def visit_bytes_expr(self, o: BytesExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1685">def visit_int_expr(self, o: IntExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1686">def visit_float_expr(self, o: FloatExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1687">def visit_complex_expr(self, o: ComplexExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1688">def visit_ellipsis(self, o: EllipsisExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20221004064035.1689"># Helpers

</t>
<t tx="ekr.20221004064035.169">def are_typed_dicts_overlapping(
    left: TypedDictType,
    right: TypedDictType,
    *,
    ignore_promotions: bool = False,
    prohibit_none_typevar_overlap: bool = False,
) -&gt; bool:
    """Returns 'true' if left and right are overlapping TypeDictTypes."""
    # All required keys in left are present and overlapping with something in right
    for key in left.required_keys:
        if key not in right.items:
            return False
        if not is_overlapping_types(
            left.items[key],
            right.items[key],
            ignore_promotions=ignore_promotions,
            prohibit_none_typevar_overlap=prohibit_none_typevar_overlap,
        ):
            return False

    # Repeat check in the other direction
    for key in right.required_keys:
        if key not in left.items:
            return False
        if not is_overlapping_types(
            left.items[key], right.items[key], ignore_promotions=ignore_promotions
        ):
            return False

    # The presence of any additional optional keys does not affect whether the two
    # TypedDicts are partially overlapping: the dicts would be overlapping if the
    # keys happened to be missing.
    return True


</t>
<t tx="ekr.20221004064035.1690">def process_node(self, node: Expression) -&gt; None:
    if self.all_nodes:
        if self.typemap is not None:
            self.line = node.line
            self.type(self.typemap.get(node))

</t>
<t tx="ekr.20221004064035.1691">def record_precise_if_checked_scope(self, node: Node) -&gt; None:
    if isinstance(node, Expression) and self.typemap and node not in self.typemap:
        kind = TYPE_UNANALYZED
    elif self.is_checked_scope():
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(node.line, kind)

</t>
<t tx="ekr.20221004064035.1692">def type(self, t: Type | None) -&gt; None:
    t = get_proper_type(t)

    if not t:
        # If an expression does not have a type, it is often due to dead code.
        # Don't count these because there can be an unanalyzed value on a line with other
        # analyzed expressions, which overwrite the TYPE_UNANALYZED.
        self.record_line(self.line, TYPE_UNANALYZED)
        return

    if isinstance(t, AnyType) and is_special_form_any(t):
        # TODO: What if there is an error in special form definition?
        self.record_line(self.line, TYPE_PRECISE)
        return

    if isinstance(t, AnyType):
        self.log("  !! Any type around line %d" % self.line)
        self.num_any_exprs += 1
        self.record_line(self.line, TYPE_ANY)
    elif (not self.all_nodes and is_imprecise(t)) or (self.all_nodes and is_imprecise2(t)):
        self.log("  !! Imprecise type around line %d" % self.line)
        self.num_imprecise_exprs += 1
        self.record_line(self.line, TYPE_IMPRECISE)
    else:
        self.num_precise_exprs += 1
        self.record_line(self.line, TYPE_PRECISE)

    for typ in get_proper_types(collect_all_inner_types(t)) + [t]:
        if isinstance(typ, AnyType):
            typ = get_original_any(typ)
            if is_special_form_any(typ):
                continue
            self.type_of_any_counter[typ.type_of_any] += 1
            self.num_any_types += 1
            if self.line in self.any_line_map:
                self.any_line_map[self.line].append(typ)
            else:
                self.any_line_map[self.line] = [typ]
        elif isinstance(typ, Instance):
            if typ.args:
                if any(is_complex(arg) for arg in typ.args):
                    self.num_complex_types += 1
                else:
                    self.num_generic_types += 1
            else:
                self.num_simple_types += 1
        elif isinstance(typ, FunctionLike):
            self.num_function_types += 1
        elif isinstance(typ, TupleType):
            if any(is_complex(item) for item in typ.items):
                self.num_complex_types += 1
            else:
                self.num_tuple_types += 1
        elif isinstance(typ, TypeVarType):
            self.num_typevar_types += 1

</t>
<t tx="ekr.20221004064035.1693">def log(self, string: str) -&gt; None:
    self.output.append(string)

</t>
<t tx="ekr.20221004064035.1694">def record_line(self, line: int, precision: int) -&gt; None:
    self.line_map[line] = max(precision, self.line_map.get(line, TYPE_EMPTY))


</t>
<t tx="ekr.20221004064035.1695">def dump_type_stats(
    tree: MypyFile,
    path: str,
    modules: dict[str, MypyFile],
    inferred: bool = False,
    typemap: dict[Expression, Type] | None = None,
) -&gt; None:
    if is_special_module(path):
        return
    print(path)
    visitor = StatisticsVisitor(inferred, filename=tree.fullname, modules=modules, typemap=typemap)
    tree.accept(visitor)
    for line in visitor.output:
        print(line)
    print("  ** precision **")
    print("  precise  ", visitor.num_precise_exprs)
    print("  imprecise", visitor.num_imprecise_exprs)
    print("  any      ", visitor.num_any_exprs)
    print("  ** kinds **")
    print("  simple   ", visitor.num_simple_types)
    print("  generic  ", visitor.num_generic_types)
    print("  function ", visitor.num_function_types)
    print("  tuple    ", visitor.num_tuple_types)
    print("  TypeVar  ", visitor.num_typevar_types)
    print("  complex  ", visitor.num_complex_types)
    print("  any      ", visitor.num_any_types)


</t>
<t tx="ekr.20221004064035.1696">def is_special_module(path: str) -&gt; bool:
    return os.path.basename(path) in ("abc.pyi", "typing.pyi", "builtins.pyi")


</t>
<t tx="ekr.20221004064035.1697">def is_imprecise(t: Type) -&gt; bool:
    return t.accept(HasAnyQuery())


</t>
<t tx="ekr.20221004064035.1698">class HasAnyQuery(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_any(self, t: AnyType) -&gt; bool:
        return not is_special_form_any(t)


</t>
<t tx="ekr.20221004064035.1699">def is_imprecise2(t: Type) -&gt; bool:
    return t.accept(HasAnyQuery2())


</t>
<t tx="ekr.20221004064035.17">@path C:/Repos/ekr-mypy2/mypy/
"""Calculation of the least upper bound types (joins)."""

from __future__ import annotations

import mypy.typeops
from mypy.maptype import map_instance_to_supertype
from mypy.nodes import CONTRAVARIANT, COVARIANT, INVARIANT
from mypy.state import state
from mypy.subtypes import (
    find_member,
    is_equivalent,
    is_proper_subtype,
    is_protocol_implementation,
    is_subtype,
)
from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    PlaceholderType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    get_proper_types,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.170">def are_tuples_overlapping(
    left: Type,
    right: Type,
    *,
    ignore_promotions: bool = False,
    prohibit_none_typevar_overlap: bool = False,
) -&gt; bool:
    """Returns true if left and right are overlapping tuples."""
    left, right = get_proper_types((left, right))
    left = adjust_tuple(left, right) or left
    right = adjust_tuple(right, left) or right
    assert isinstance(left, TupleType), f"Type {left} is not a tuple"
    assert isinstance(right, TupleType), f"Type {right} is not a tuple"
    if len(left.items) != len(right.items):
        return False
    return all(
        is_overlapping_types(
            l,
            r,
            ignore_promotions=ignore_promotions,
            prohibit_none_typevar_overlap=prohibit_none_typevar_overlap,
        )
        for l, r in zip(left.items, right.items)
    )


</t>
<t tx="ekr.20221004064035.1700">class HasAnyQuery2(HasAnyQuery):
    def visit_callable_type(self, t: CallableType) -&gt; bool:
        # We don't want to flag references to functions with some Any
        # argument types (etc.) since they generally don't mean trouble.
        return False


</t>
<t tx="ekr.20221004064035.1701">def is_generic(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, Instance) and bool(t.args)


</t>
<t tx="ekr.20221004064035.1702">def is_complex(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return is_generic(t) or isinstance(t, (FunctionLike, TupleType, TypeVarType))


</t>
<t tx="ekr.20221004064035.1703">def ensure_dir_exists(dir: str) -&gt; None:
    if not os.path.exists(dir):
        os.makedirs(dir)


</t>
<t tx="ekr.20221004064035.1704">def is_special_form_any(t: AnyType) -&gt; bool:
    return get_original_any(t).type_of_any == TypeOfAny.special_form


</t>
<t tx="ekr.20221004064035.1705">def get_original_any(t: AnyType) -&gt; AnyType:
    if t.type_of_any == TypeOfAny.from_another_any:
        assert t.source_any
        assert t.source_any.type_of_any != TypeOfAny.from_another_any
        t = t.source_any
    return t
</t>
<t tx="ekr.20221004064035.1706">@path C:/Repos/ekr-mypy2/mypy/
"""Conversion of parse tree nodes to strings."""

from __future__ import annotations

import os
import re
from typing import TYPE_CHECKING, Any, Sequence

import mypy.nodes
from mypy.util import IdMapper, short_type
from mypy.visitor import NodeVisitor

if TYPE_CHECKING:
    import mypy.patterns


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1707">class StrConv(NodeVisitor[str]):
    """Visitor for converting a node to a human-readable string.

    For example, an MypyFile node from program '1' is converted into
    something like this:

      MypyFile:1(
        fnam
        ExpressionStmt:1(
          IntExpr(1)))
    """

    @others
</t>
<t tx="ekr.20221004064035.1708">def __init__(self, show_ids: bool = False) -&gt; None:
    self.show_ids = show_ids
    self.id_mapper: IdMapper | None = None
    if show_ids:
        self.id_mapper = IdMapper()

</t>
<t tx="ekr.20221004064035.1709">def get_id(self, o: object) -&gt; int | None:
    if self.id_mapper:
        return self.id_mapper.id(o)
    return None

</t>
<t tx="ekr.20221004064035.171">def adjust_tuple(left: ProperType, r: ProperType) -&gt; TupleType | None:
    """Find out if `left` is a Tuple[A, ...], and adjust its length to `right`"""
    if isinstance(left, Instance) and left.type.fullname == "builtins.tuple":
        n = r.length() if isinstance(r, TupleType) else 1
        return TupleType([left.args[0]] * n, left)
    return None


</t>
<t tx="ekr.20221004064035.1710">def format_id(self, o: object) -&gt; str:
    if self.id_mapper:
        return f"&lt;{self.get_id(o)}&gt;"
    else:
        return ""

</t>
<t tx="ekr.20221004064035.1711">def dump(self, nodes: Sequence[object], obj: mypy.nodes.Context) -&gt; str:
    """Convert a list of items to a multiline pretty-printed string.

    The tag is produced from the type name of obj and its line
    number. See mypy.util.dump_tagged for a description of the nodes
    argument.
    """
    tag = short_type(obj) + ":" + str(obj.get_line())
    if self.show_ids:
        assert self.id_mapper is not None
        tag += f"&lt;{self.get_id(obj)}&gt;"
    return dump_tagged(nodes, tag, self)

</t>
<t tx="ekr.20221004064035.1712">def func_helper(self, o: mypy.nodes.FuncItem) -&gt; list[object]:
    """Return a list in a format suitable for dump() that represents the
    arguments and the body of a function. The caller can then decorate the
    array with information specific to methods, global functions or
    anonymous functions.
    """
    args: list[mypy.nodes.Var | tuple[str, list[mypy.nodes.Node]]] = []
    extra: list[tuple[str, list[mypy.nodes.Var]]] = []
    for arg in o.arguments:
        kind: mypy.nodes.ArgKind = arg.kind
        if kind.is_required():
            args.append(arg.variable)
        elif kind.is_optional():
            assert arg.initializer is not None
            args.append(("default", [arg.variable, arg.initializer]))
        elif kind == mypy.nodes.ARG_STAR:
            extra.append(("VarArg", [arg.variable]))
        elif kind == mypy.nodes.ARG_STAR2:
            extra.append(("DictVarArg", [arg.variable]))
    a: list[Any] = []
    if args:
        a.append(("Args", args))
    if o.type:
        a.append(o.type)
    if o.is_generator:
        a.append("Generator")
    a.extend(extra)
    a.append(o.body)
    return a

</t>
<t tx="ekr.20221004064035.1713"># Top-level structures

</t>
<t tx="ekr.20221004064035.1714">def visit_mypy_file(self, o: mypy.nodes.MypyFile) -&gt; str:
    # Skip implicit definitions.
    a: list[Any] = [o.defs]
    if o.is_bom:
        a.insert(0, "BOM")
    # Omit path to special file with name "main". This is used to simplify
    # test case descriptions; the file "main" is used by default in many
    # test cases.
    if o.path != "main":
        # Insert path. Normalize directory separators to / to unify test
        # case# output in all platforms.
        a.insert(0, o.path.replace(os.sep, "/"))
    if o.ignored_lines:
        a.append("IgnoredLines(%s)" % ", ".join(str(line) for line in sorted(o.ignored_lines)))
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1715">def visit_import(self, o: mypy.nodes.Import) -&gt; str:
    a = []
    for id, as_id in o.ids:
        if as_id is not None:
            a.append(f"{id} : {as_id}")
        else:
            a.append(id)
    return f"Import:{o.line}({', '.join(a)})"

</t>
<t tx="ekr.20221004064035.1716">def visit_import_from(self, o: mypy.nodes.ImportFrom) -&gt; str:
    a = []
    for name, as_name in o.names:
        if as_name is not None:
            a.append(f"{name} : {as_name}")
        else:
            a.append(name)
    return f"ImportFrom:{o.line}({'.' * o.relative + o.id}, [{', '.join(a)}])"

</t>
<t tx="ekr.20221004064035.1717">def visit_import_all(self, o: mypy.nodes.ImportAll) -&gt; str:
    return f"ImportAll:{o.line}({'.' * o.relative + o.id})"

</t>
<t tx="ekr.20221004064035.1718"># Definitions

</t>
<t tx="ekr.20221004064035.1719">def visit_func_def(self, o: mypy.nodes.FuncDef) -&gt; str:
    a = self.func_helper(o)
    a.insert(0, o.name)
    arg_kinds = {arg.kind for arg in o.arguments}
    if len(arg_kinds &amp; {mypy.nodes.ARG_NAMED, mypy.nodes.ARG_NAMED_OPT}) &gt; 0:
        a.insert(1, f"MaxPos({o.max_pos})")
    if o.abstract_status in (mypy.nodes.IS_ABSTRACT, mypy.nodes.IMPLICITLY_ABSTRACT):
        a.insert(-1, "Abstract")
    if o.is_static:
        a.insert(-1, "Static")
    if o.is_class:
        a.insert(-1, "Class")
    if o.is_property:
        a.insert(-1, "Property")
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.172">def is_tuple(typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, TupleType) or (
        isinstance(typ, Instance) and typ.type.fullname == "builtins.tuple"
    )


</t>
<t tx="ekr.20221004064035.1720">def visit_overloaded_func_def(self, o: mypy.nodes.OverloadedFuncDef) -&gt; str:
    a: Any = o.items[:]
    if o.type:
        a.insert(0, o.type)
    if o.impl:
        a.insert(0, o.impl)
    if o.is_static:
        a.insert(-1, "Static")
    if o.is_class:
        a.insert(-1, "Class")
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1721">def visit_class_def(self, o: mypy.nodes.ClassDef) -&gt; str:
    a = [o.name, o.defs.body]
    # Display base types unless they are implicitly just builtins.object
    # (in this case base_type_exprs is empty).
    if o.base_type_exprs:
        if o.info and o.info.bases:
            if len(o.info.bases) != 1 or o.info.bases[0].type.fullname != "builtins.object":
                a.insert(1, ("BaseType", o.info.bases))
        else:
            a.insert(1, ("BaseTypeExpr", o.base_type_exprs))
    if o.type_vars:
        a.insert(1, ("TypeVars", o.type_vars))
    if o.metaclass:
        a.insert(1, f"Metaclass({o.metaclass})")
    if o.decorators:
        a.insert(1, ("Decorators", o.decorators))
    if o.info and o.info._promote:
        a.insert(1, f"Promote({o.info._promote})")
    if o.info and o.info.tuple_type:
        a.insert(1, ("TupleType", [o.info.tuple_type]))
    if o.info and o.info.fallback_to_any:
        a.insert(1, "FallbackToAny")
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1722">def visit_var(self, o: mypy.nodes.Var) -&gt; str:
    lst = ""
    # Add :nil line number tag if no line number is specified to remain
    # compatible with old test case descriptions that assume this.
    if o.line &lt; 0:
        lst = ":nil"
    return "Var" + lst + "(" + o.name + ")"

</t>
<t tx="ekr.20221004064035.1723">def visit_global_decl(self, o: mypy.nodes.GlobalDecl) -&gt; str:
    return self.dump([o.names], o)

</t>
<t tx="ekr.20221004064035.1724">def visit_nonlocal_decl(self, o: mypy.nodes.NonlocalDecl) -&gt; str:
    return self.dump([o.names], o)

</t>
<t tx="ekr.20221004064035.1725">def visit_decorator(self, o: mypy.nodes.Decorator) -&gt; str:
    return self.dump([o.var, o.decorators, o.func], o)

</t>
<t tx="ekr.20221004064035.1726"># Statements

</t>
<t tx="ekr.20221004064035.1727">def visit_block(self, o: mypy.nodes.Block) -&gt; str:
    return self.dump(o.body, o)

</t>
<t tx="ekr.20221004064035.1728">def visit_expression_stmt(self, o: mypy.nodes.ExpressionStmt) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.1729">def visit_assignment_stmt(self, o: mypy.nodes.AssignmentStmt) -&gt; str:
    a: list[Any] = []
    if len(o.lvalues) &gt; 1:
        a = [("Lvalues", o.lvalues)]
    else:
        a = [o.lvalues[0]]
    a.append(o.rvalue)
    if o.type:
        a.append(o.type)
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.173">class TypeMeetVisitor(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20221004064035.1730">def visit_operator_assignment_stmt(self, o: mypy.nodes.OperatorAssignmentStmt) -&gt; str:
    return self.dump([o.op, o.lvalue, o.rvalue], o)

</t>
<t tx="ekr.20221004064035.1731">def visit_while_stmt(self, o: mypy.nodes.WhileStmt) -&gt; str:
    a: list[Any] = [o.expr, o.body]
    if o.else_body:
        a.append(("Else", o.else_body.body))
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1732">def visit_for_stmt(self, o: mypy.nodes.ForStmt) -&gt; str:
    a: list[Any] = []
    if o.is_async:
        a.append(("Async", ""))
    a.append(o.index)
    if o.index_type:
        a.append(o.index_type)
    a.extend([o.expr, o.body])
    if o.else_body:
        a.append(("Else", o.else_body.body))
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1733">def visit_return_stmt(self, o: mypy.nodes.ReturnStmt) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.1734">def visit_if_stmt(self, o: mypy.nodes.IfStmt) -&gt; str:
    a: list[Any] = []
    for i in range(len(o.expr)):
        a.append(("If", [o.expr[i]]))
        a.append(("Then", o.body[i].body))

    if not o.else_body:
        return self.dump(a, o)
    else:
        return self.dump([a, ("Else", o.else_body.body)], o)

</t>
<t tx="ekr.20221004064035.1735">def visit_break_stmt(self, o: mypy.nodes.BreakStmt) -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20221004064035.1736">def visit_continue_stmt(self, o: mypy.nodes.ContinueStmt) -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20221004064035.1737">def visit_pass_stmt(self, o: mypy.nodes.PassStmt) -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20221004064035.1738">def visit_raise_stmt(self, o: mypy.nodes.RaiseStmt) -&gt; str:
    return self.dump([o.expr, o.from_expr], o)

</t>
<t tx="ekr.20221004064035.1739">def visit_assert_stmt(self, o: mypy.nodes.AssertStmt) -&gt; str:
    if o.msg is not None:
        return self.dump([o.expr, o.msg], o)
    else:
        return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.174">def __init__(self, s: ProperType) -&gt; None:
    self.s = s

</t>
<t tx="ekr.20221004064035.1740">def visit_await_expr(self, o: mypy.nodes.AwaitExpr) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.1741">def visit_del_stmt(self, o: mypy.nodes.DelStmt) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.1742">def visit_try_stmt(self, o: mypy.nodes.TryStmt) -&gt; str:
    a: list[Any] = [o.body]

    for i in range(len(o.vars)):
        a.append(o.types[i])
        if o.vars[i]:
            a.append(o.vars[i])
        a.append(o.handlers[i])

    if o.else_body:
        a.append(("Else", o.else_body.body))
    if o.finally_body:
        a.append(("Finally", o.finally_body.body))

    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1743">def visit_with_stmt(self, o: mypy.nodes.WithStmt) -&gt; str:
    a: list[Any] = []
    if o.is_async:
        a.append(("Async", ""))
    for i in range(len(o.expr)):
        a.append(("Expr", [o.expr[i]]))
        if o.target[i]:
            a.append(("Target", [o.target[i]]))
    if o.unanalyzed_type:
        a.append(o.unanalyzed_type)
    return self.dump(a + [o.body], o)

</t>
<t tx="ekr.20221004064035.1744">def visit_match_stmt(self, o: mypy.nodes.MatchStmt) -&gt; str:
    a: list[Any] = [o.subject]
    for i in range(len(o.patterns)):
        a.append(("Pattern", [o.patterns[i]]))
        if o.guards[i] is not None:
            a.append(("Guard", [o.guards[i]]))
        a.append(("Body", o.bodies[i].body))
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1745"># Expressions

# Simple expressions

</t>
<t tx="ekr.20221004064035.1746">def visit_int_expr(self, o: mypy.nodes.IntExpr) -&gt; str:
    return f"IntExpr({o.value})"

</t>
<t tx="ekr.20221004064035.1747">def visit_str_expr(self, o: mypy.nodes.StrExpr) -&gt; str:
    return f"StrExpr({self.str_repr(o.value)})"

</t>
<t tx="ekr.20221004064035.1748">def visit_bytes_expr(self, o: mypy.nodes.BytesExpr) -&gt; str:
    return f"BytesExpr({self.str_repr(o.value)})"

</t>
<t tx="ekr.20221004064035.1749">def str_repr(self, s: str) -&gt; str:
    s = re.sub(r"\\u[0-9a-fA-F]{4}", lambda m: "\\" + m.group(0), s)
    return re.sub("[^\\x20-\\x7e]", lambda m: r"\u%.4x" % ord(m.group(0)), s)

</t>
<t tx="ekr.20221004064035.175">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    if isinstance(self.s, NoneType):
        if state.strict_optional:
            return AnyType(TypeOfAny.special_form)
        else:
            return self.s
    elif isinstance(self.s, UninhabitedType):
        return self.s
    else:
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064035.1750">def visit_float_expr(self, o: mypy.nodes.FloatExpr) -&gt; str:
    return f"FloatExpr({o.value})"

</t>
<t tx="ekr.20221004064035.1751">def visit_complex_expr(self, o: mypy.nodes.ComplexExpr) -&gt; str:
    return f"ComplexExpr({o.value})"

</t>
<t tx="ekr.20221004064035.1752">def visit_ellipsis(self, o: mypy.nodes.EllipsisExpr) -&gt; str:
    return "Ellipsis"

</t>
<t tx="ekr.20221004064035.1753">def visit_star_expr(self, o: mypy.nodes.StarExpr) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.1754">def visit_name_expr(self, o: mypy.nodes.NameExpr) -&gt; str:
    pretty = self.pretty_name(
        o.name, o.kind, o.fullname, o.is_inferred_def or o.is_special_form, o.node
    )
    if isinstance(o.node, mypy.nodes.Var) and o.node.is_final:
        pretty += f" = {o.node.final_value}"
    return short_type(o) + "(" + pretty + ")"

</t>
<t tx="ekr.20221004064035.1755">def pretty_name(
    self,
    name: str,
    kind: int | None,
    fullname: str | None,
    is_inferred_def: bool,
    target_node: mypy.nodes.Node | None = None,
) -&gt; str:
    n = name
    if is_inferred_def:
        n += "*"
    if target_node:
        id = self.format_id(target_node)
    else:
        id = ""
    if isinstance(target_node, mypy.nodes.MypyFile) and name == fullname:
        n += id
    elif kind == mypy.nodes.GDEF or (fullname != name and fullname is not None):
        # Append fully qualified name for global references.
        n += f" [{fullname}{id}]"
    elif kind == mypy.nodes.LDEF:
        # Add tag to signify a local reference.
        n += f" [l{id}]"
    elif kind == mypy.nodes.MDEF:
        # Add tag to signify a member reference.
        n += f" [m{id}]"
    else:
        n += id
    return n

</t>
<t tx="ekr.20221004064035.1756">def visit_member_expr(self, o: mypy.nodes.MemberExpr) -&gt; str:
    pretty = self.pretty_name(o.name, o.kind, o.fullname, o.is_inferred_def, o.node)
    return self.dump([o.expr, pretty], o)

</t>
<t tx="ekr.20221004064035.1757">def visit_yield_expr(self, o: mypy.nodes.YieldExpr) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.1758">def visit_yield_from_expr(self, o: mypy.nodes.YieldFromExpr) -&gt; str:
    if o.expr:
        return self.dump([o.expr.accept(self)], o)
    else:
        return self.dump([], o)

</t>
<t tx="ekr.20221004064035.1759">def visit_call_expr(self, o: mypy.nodes.CallExpr) -&gt; str:
    if o.analyzed:
        return o.analyzed.accept(self)
    args: list[mypy.nodes.Expression] = []
    extra: list[str | tuple[str, list[Any]]] = []
    for i, kind in enumerate(o.arg_kinds):
        if kind in [mypy.nodes.ARG_POS, mypy.nodes.ARG_STAR]:
            args.append(o.args[i])
            if kind == mypy.nodes.ARG_STAR:
                extra.append("VarArg")
        elif kind == mypy.nodes.ARG_NAMED:
            extra.append(("KwArgs", [o.arg_names[i], o.args[i]]))
        elif kind == mypy.nodes.ARG_STAR2:
            extra.append(("DictVarArg", [o.args[i]]))
        else:
            raise RuntimeError(f"unknown kind {kind}")
    a: list[Any] = [o.callee, ("Args", args)]
    return self.dump(a + extra, o)

</t>
<t tx="ekr.20221004064035.176">def visit_any(self, t: AnyType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20221004064035.1760">def visit_op_expr(self, o: mypy.nodes.OpExpr) -&gt; str:
    return self.dump([o.op, o.left, o.right], o)

</t>
<t tx="ekr.20221004064035.1761">def visit_comparison_expr(self, o: mypy.nodes.ComparisonExpr) -&gt; str:
    return self.dump([o.operators, o.operands], o)

</t>
<t tx="ekr.20221004064035.1762">def visit_cast_expr(self, o: mypy.nodes.CastExpr) -&gt; str:
    return self.dump([o.expr, o.type], o)

</t>
<t tx="ekr.20221004064035.1763">def visit_assert_type_expr(self, o: mypy.nodes.AssertTypeExpr) -&gt; str:
    return self.dump([o.expr, o.type], o)

</t>
<t tx="ekr.20221004064035.1764">def visit_reveal_expr(self, o: mypy.nodes.RevealExpr) -&gt; str:
    if o.kind == mypy.nodes.REVEAL_TYPE:
        return self.dump([o.expr], o)
    else:
        # REVEAL_LOCALS
        return self.dump([o.local_nodes], o)

</t>
<t tx="ekr.20221004064035.1765">def visit_assignment_expr(self, o: mypy.nodes.AssignmentExpr) -&gt; str:
    return self.dump([o.target, o.value], o)

</t>
<t tx="ekr.20221004064035.1766">def visit_unary_expr(self, o: mypy.nodes.UnaryExpr) -&gt; str:
    return self.dump([o.op, o.expr], o)

</t>
<t tx="ekr.20221004064035.1767">def visit_list_expr(self, o: mypy.nodes.ListExpr) -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20221004064035.1768">def visit_dict_expr(self, o: mypy.nodes.DictExpr) -&gt; str:
    return self.dump([[k, v] for k, v in o.items], o)

</t>
<t tx="ekr.20221004064035.1769">def visit_set_expr(self, o: mypy.nodes.SetExpr) -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20221004064035.177">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    if isinstance(self.s, UnionType):
        meets: list[Type] = []
        for x in t.items:
            for y in self.s.items:
                meets.append(meet_types(x, y))
    else:
        meets = [meet_types(x, self.s) for x in t.items]
    return make_simplified_union(meets)

</t>
<t tx="ekr.20221004064035.1770">def visit_tuple_expr(self, o: mypy.nodes.TupleExpr) -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20221004064035.1771">def visit_index_expr(self, o: mypy.nodes.IndexExpr) -&gt; str:
    if o.analyzed:
        return o.analyzed.accept(self)
    return self.dump([o.base, o.index], o)

</t>
<t tx="ekr.20221004064035.1772">def visit_super_expr(self, o: mypy.nodes.SuperExpr) -&gt; str:
    return self.dump([o.name, o.call], o)

</t>
<t tx="ekr.20221004064035.1773">def visit_type_application(self, o: mypy.nodes.TypeApplication) -&gt; str:
    return self.dump([o.expr, ("Types", o.types)], o)

</t>
<t tx="ekr.20221004064035.1774">def visit_type_var_expr(self, o: mypy.nodes.TypeVarExpr) -&gt; str:
    import mypy.types

    a: list[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ["Variance(COVARIANT)"]
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ["Variance(CONTRAVARIANT)"]
    if o.values:
        a += [("Values", o.values)]
    if not mypy.types.is_named_instance(o.upper_bound, "builtins.object"):
        a += [f"UpperBound({o.upper_bound})"]
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1775">def visit_paramspec_expr(self, o: mypy.nodes.ParamSpecExpr) -&gt; str:
    import mypy.types

    a: list[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ["Variance(COVARIANT)"]
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ["Variance(CONTRAVARIANT)"]
    if not mypy.types.is_named_instance(o.upper_bound, "builtins.object"):
        a += [f"UpperBound({o.upper_bound})"]
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1776">def visit_type_var_tuple_expr(self, o: mypy.nodes.TypeVarTupleExpr) -&gt; str:
    import mypy.types

    a: list[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ["Variance(COVARIANT)"]
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ["Variance(CONTRAVARIANT)"]
    if not mypy.types.is_named_instance(o.upper_bound, "builtins.object"):
        a += [f"UpperBound({o.upper_bound})"]
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1777">def visit_type_alias_expr(self, o: mypy.nodes.TypeAliasExpr) -&gt; str:
    return f"TypeAliasExpr({o.type})"

</t>
<t tx="ekr.20221004064035.1778">def visit_namedtuple_expr(self, o: mypy.nodes.NamedTupleExpr) -&gt; str:
    return f"NamedTupleExpr:{o.line}({o.info.name}, {o.info.tuple_type})"

</t>
<t tx="ekr.20221004064035.1779">def visit_enum_call_expr(self, o: mypy.nodes.EnumCallExpr) -&gt; str:
    return f"EnumCallExpr:{o.line}({o.info.name}, {o.items})"

</t>
<t tx="ekr.20221004064035.178">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    if state.strict_optional:
        if isinstance(self.s, NoneType) or (
            isinstance(self.s, Instance) and self.s.type.fullname == "builtins.object"
        ):
            return t
        else:
            return UninhabitedType()
    else:
        return t

</t>
<t tx="ekr.20221004064035.1780">def visit_typeddict_expr(self, o: mypy.nodes.TypedDictExpr) -&gt; str:
    return f"TypedDictExpr:{o.line}({o.info.name})"

</t>
<t tx="ekr.20221004064035.1781">def visit__promote_expr(self, o: mypy.nodes.PromoteExpr) -&gt; str:
    return f"PromoteExpr:{o.line}({o.type})"

</t>
<t tx="ekr.20221004064035.1782">def visit_newtype_expr(self, o: mypy.nodes.NewTypeExpr) -&gt; str:
    return f"NewTypeExpr:{o.line}({o.name}, {self.dump([o.old_type], o)})"

</t>
<t tx="ekr.20221004064035.1783">def visit_lambda_expr(self, o: mypy.nodes.LambdaExpr) -&gt; str:
    a = self.func_helper(o)
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1784">def visit_generator_expr(self, o: mypy.nodes.GeneratorExpr) -&gt; str:
    condlists = o.condlists if any(o.condlists) else None
    return self.dump([o.left_expr, o.indices, o.sequences, condlists], o)

</t>
<t tx="ekr.20221004064035.1785">def visit_list_comprehension(self, o: mypy.nodes.ListComprehension) -&gt; str:
    return self.dump([o.generator], o)

</t>
<t tx="ekr.20221004064035.1786">def visit_set_comprehension(self, o: mypy.nodes.SetComprehension) -&gt; str:
    return self.dump([o.generator], o)

</t>
<t tx="ekr.20221004064035.1787">def visit_dictionary_comprehension(self, o: mypy.nodes.DictionaryComprehension) -&gt; str:
    condlists = o.condlists if any(o.condlists) else None
    return self.dump([o.key, o.value, o.indices, o.sequences, condlists], o)

</t>
<t tx="ekr.20221004064035.1788">def visit_conditional_expr(self, o: mypy.nodes.ConditionalExpr) -&gt; str:
    return self.dump([("Condition", [o.cond]), o.if_expr, o.else_expr], o)

</t>
<t tx="ekr.20221004064035.1789">def visit_slice_expr(self, o: mypy.nodes.SliceExpr) -&gt; str:
    a: list[Any] = [o.begin_index, o.end_index, o.stride]
    if not a[0]:
        a[0] = "&lt;empty&gt;"
    if not a[1]:
        a[1] = "&lt;empty&gt;"
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.179">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064035.1790">def visit_temp_node(self, o: mypy.nodes.TempNode) -&gt; str:
    return self.dump([o.type], o)

</t>
<t tx="ekr.20221004064035.1791">def visit_as_pattern(self, o: mypy.patterns.AsPattern) -&gt; str:
    return self.dump([o.pattern, o.name], o)

</t>
<t tx="ekr.20221004064035.1792">def visit_or_pattern(self, o: mypy.patterns.OrPattern) -&gt; str:
    return self.dump(o.patterns, o)

</t>
<t tx="ekr.20221004064035.1793">def visit_value_pattern(self, o: mypy.patterns.ValuePattern) -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20221004064035.1794">def visit_singleton_pattern(self, o: mypy.patterns.SingletonPattern) -&gt; str:
    return self.dump([o.value], o)

</t>
<t tx="ekr.20221004064035.1795">def visit_sequence_pattern(self, o: mypy.patterns.SequencePattern) -&gt; str:
    return self.dump(o.patterns, o)

</t>
<t tx="ekr.20221004064035.1796">def visit_starred_pattern(self, o: mypy.patterns.StarredPattern) -&gt; str:
    return self.dump([o.capture], o)

</t>
<t tx="ekr.20221004064035.1797">def visit_mapping_pattern(self, o: mypy.patterns.MappingPattern) -&gt; str:
    a: list[Any] = []
    for i in range(len(o.keys)):
        a.append(("Key", [o.keys[i]]))
        a.append(("Value", [o.values[i]]))
    if o.rest is not None:
        a.append(("Rest", [o.rest]))
    return self.dump(a, o)

</t>
<t tx="ekr.20221004064035.1798">def visit_class_pattern(self, o: mypy.patterns.ClassPattern) -&gt; str:
    a: list[Any] = [o.class_ref]
    if len(o.positionals) &gt; 0:
        a.append(("Positionals", o.positionals))
    for i in range(len(o.keyword_keys)):
        a.append(("Keyword", [o.keyword_keys[i], o.keyword_values[i]]))

    return self.dump(a, o)


</t>
<t tx="ekr.20221004064035.1799">def dump_tagged(nodes: Sequence[object], tag: str | None, str_conv: StrConv) -&gt; str:
    """Convert an array into a pretty-printed multiline string representation.

    The format is
      tag(
        item1..
        itemN)
    Individual items are formatted like this:
     - arrays are flattened
     - pairs (str, array) are converted recursively, so that str is the tag
     - other items are converted to strings and indented
    """
    from mypy.types import Type, TypeStrVisitor

    a: list[str] = []
    if tag:
        a.append(tag + "(")
    for n in nodes:
        if isinstance(n, list):
            if n:
                a.append(dump_tagged(n, None, str_conv))
        elif isinstance(n, tuple):
            s = dump_tagged(n[1], n[0], str_conv)
            a.append(indent(s, 2))
        elif isinstance(n, mypy.nodes.Node):
            a.append(indent(n.accept(str_conv), 2))
        elif isinstance(n, Type):
            a.append(indent(n.accept(TypeStrVisitor(str_conv.id_mapper)), 2))
        elif n is not None:
            a.append(indent(str(n), 2))
    if tag:
        a[-1] += ")"
    return "\n".join(a)


</t>
<t tx="ekr.20221004064035.18">class InstanceJoiner:
    @others
</t>
<t tx="ekr.20221004064035.180">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    if isinstance(self.s, NoneType):
        if state.strict_optional:
            return t
        else:
            return self.s
    elif isinstance(self.s, UninhabitedType):
        return self.s
    else:
        return t

</t>
<t tx="ekr.20221004064035.1800">def indent(s: str, n: int) -&gt; str:
    """Indent all the lines in s (separated by newlines) by n spaces."""
    s = " " * n + s
    s = s.replace("\n", "\n" + " " * n)
    return s
</t>
<t tx="ekr.20221004064035.1801">@path C:/Repos/ekr-mypy2/mypy/
"""Parsing/inferring signatures from documentation.

This module provides several functions to generate better stubs using
docstrings and Sphinx docs (.rst files).
"""

from __future__ import annotations

import contextlib
import io
import re
import tokenize
from typing import Any, MutableMapping, MutableSequence, NamedTuple, Sequence, Tuple
from typing_extensions import Final, TypeAlias as _TypeAlias

# Type alias for signatures strings in format ('func_name', '(arg, opt_arg=False)').
Sig: _TypeAlias = Tuple[str, str]


_TYPE_RE: Final = re.compile(r"^[a-zA-Z_][\w\[\], ]*(\.[a-zA-Z_][\w\[\], ]*)*$")
_ARG_NAME_RE: Final = re.compile(r"\**[A-Za-z_][A-Za-z0-9_]*$")


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1802">def is_valid_type(s: str) -&gt; bool:
    """Try to determine whether a string might be a valid type annotation."""
    if s in ("True", "False", "retval"):
        return False
    if "," in s and "[" not in s:
        return False
    return _TYPE_RE.match(s) is not None


</t>
<t tx="ekr.20221004064035.1803">class ArgSig:
    """Signature info for a single argument."""

    @others
</t>
<t tx="ekr.20221004064035.1804">def __init__(self, name: str, type: str | None = None, default: bool = False):
    self.name = name
    if type and not is_valid_type(type):
        raise ValueError("Invalid type: " + type)
    self.type = type
    # Does this argument have a default value?
    self.default = default

</t>
<t tx="ekr.20221004064035.1805">def __repr__(self) -&gt; str:
    return "ArgSig(name={}, type={}, default={})".format(
        repr(self.name), repr(self.type), repr(self.default)
    )

</t>
<t tx="ekr.20221004064035.1806">def __eq__(self, other: Any) -&gt; bool:
    if isinstance(other, ArgSig):
        return (
            self.name == other.name
            and self.type == other.type
            and self.default == other.default
        )
    return False


</t>
<t tx="ekr.20221004064035.1807">class FunctionSig(NamedTuple):
    name: str
    args: list[ArgSig]
    ret_type: str


</t>
<t tx="ekr.20221004064035.1808"># States of the docstring parser.
STATE_INIT: Final = 1
STATE_FUNCTION_NAME: Final = 2
STATE_ARGUMENT_LIST: Final = 3
STATE_ARGUMENT_TYPE: Final = 4
STATE_ARGUMENT_DEFAULT: Final = 5
STATE_RETURN_VALUE: Final = 6
STATE_OPEN_BRACKET: Final = 7  # For generic types.


</t>
<t tx="ekr.20221004064035.1809">class DocStringParser:
    """Parse function signatures in documentation."""

    @others
</t>
<t tx="ekr.20221004064035.181">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20221004064035.1810">def __init__(self, function_name: str) -&gt; None:
    # Only search for signatures of function with this name.
    self.function_name = function_name
    self.state = [STATE_INIT]
    self.accumulator = ""
    self.arg_type: str | None = None
    self.arg_name = ""
    self.arg_default: str | None = None
    self.ret_type = "Any"
    self.found = False
    self.args: list[ArgSig] = []
    # Valid signatures found so far.
    self.signatures: list[FunctionSig] = []

</t>
<t tx="ekr.20221004064035.1811">def add_token(self, token: tokenize.TokenInfo) -&gt; None:
    """Process next token from the token stream."""
    if (
        token.type == tokenize.NAME
        and token.string == self.function_name
        and self.state[-1] == STATE_INIT
    ):
        self.state.append(STATE_FUNCTION_NAME)

    elif (
        token.type == tokenize.OP
        and token.string == "("
        and self.state[-1] == STATE_FUNCTION_NAME
    ):
        self.state.pop()
        self.accumulator = ""
        self.found = True
        self.state.append(STATE_ARGUMENT_LIST)

    elif self.state[-1] == STATE_FUNCTION_NAME:
        # Reset state, function name not followed by '('.
        self.state.pop()

    elif (
        token.type == tokenize.OP
        and token.string in ("[", "(", "{")
        and self.state[-1] != STATE_INIT
    ):
        self.accumulator += token.string
        self.state.append(STATE_OPEN_BRACKET)

    elif (
        token.type == tokenize.OP
        and token.string in ("]", ")", "}")
        and self.state[-1] == STATE_OPEN_BRACKET
    ):
        self.accumulator += token.string
        self.state.pop()

    elif (
        token.type == tokenize.OP
        and token.string == ":"
        and self.state[-1] == STATE_ARGUMENT_LIST
    ):
        self.arg_name = self.accumulator
        self.accumulator = ""
        self.state.append(STATE_ARGUMENT_TYPE)

    elif (
        token.type == tokenize.OP
        and token.string == "="
        and self.state[-1] in (STATE_ARGUMENT_LIST, STATE_ARGUMENT_TYPE)
    ):
        if self.state[-1] == STATE_ARGUMENT_TYPE:
            self.arg_type = self.accumulator
            self.state.pop()
        else:
            self.arg_name = self.accumulator
        self.accumulator = ""
        self.state.append(STATE_ARGUMENT_DEFAULT)

    elif (
        token.type == tokenize.OP
        and token.string in (",", ")")
        and self.state[-1]
        in (STATE_ARGUMENT_LIST, STATE_ARGUMENT_DEFAULT, STATE_ARGUMENT_TYPE)
    ):
        if self.state[-1] == STATE_ARGUMENT_DEFAULT:
            self.arg_default = self.accumulator
            self.state.pop()
        elif self.state[-1] == STATE_ARGUMENT_TYPE:
            self.arg_type = self.accumulator
            self.state.pop()
        elif self.state[-1] == STATE_ARGUMENT_LIST:
            self.arg_name = self.accumulator
            if not (
                token.string == ")" and self.accumulator.strip() == ""
            ) and not _ARG_NAME_RE.match(self.arg_name):
                # Invalid argument name.
                self.reset()
                return

        if token.string == ")":
            self.state.pop()

        # arg_name is empty when there are no args. e.g. func()
        if self.arg_name:
            try:
                self.args.append(
                    ArgSig(
                        name=self.arg_name, type=self.arg_type, default=bool(self.arg_default)
                    )
                )
            except ValueError:
                # wrong type, use Any
                self.args.append(
                    ArgSig(name=self.arg_name, type=None, default=bool(self.arg_default))
                )
        self.arg_name = ""
        self.arg_type = None
        self.arg_default = None
        self.accumulator = ""

    elif token.type == tokenize.OP and token.string == "-&gt;" and self.state[-1] == STATE_INIT:
        self.accumulator = ""
        self.state.append(STATE_RETURN_VALUE)

    # ENDMAKER is necessary for python 3.4 and 3.5.
    elif token.type in (tokenize.NEWLINE, tokenize.ENDMARKER) and self.state[-1] in (
        STATE_INIT,
        STATE_RETURN_VALUE,
    ):
        if self.state[-1] == STATE_RETURN_VALUE:
            if not is_valid_type(self.accumulator):
                self.reset()
                return
            self.ret_type = self.accumulator
            self.accumulator = ""
            self.state.pop()

        if self.found:
            self.signatures.append(
                FunctionSig(name=self.function_name, args=self.args, ret_type=self.ret_type)
            )
            self.found = False
        self.args = []
        self.ret_type = "Any"
        # Leave state as INIT.
    else:
        self.accumulator += token.string

</t>
<t tx="ekr.20221004064035.1812">def reset(self) -&gt; None:
    self.state = [STATE_INIT]
    self.args = []
    self.found = False
    self.accumulator = ""

</t>
<t tx="ekr.20221004064035.1813">def get_signatures(self) -&gt; list[FunctionSig]:
    """Return sorted copy of the list of signatures found so far."""

    def has_arg(name: str, signature: FunctionSig) -&gt; bool:
        return any(x.name == name for x in signature.args)

    def args_kwargs(signature: FunctionSig) -&gt; bool:
        return has_arg("*args", signature) and has_arg("**kwargs", signature)

    # Move functions with (*args, **kwargs) in their signature to last place.
    return list(sorted(self.signatures, key=lambda x: 1 if args_kwargs(x) else 0))


</t>
<t tx="ekr.20221004064035.1814">def infer_sig_from_docstring(docstr: str | None, name: str) -&gt; list[FunctionSig] | None:
    """Convert function signature to list of TypedFunctionSig

    Look for function signatures of function in docstring. Signature is a string of
    the format &lt;function_name&gt;(&lt;signature&gt;) -&gt; &lt;return type&gt; or perhaps without
    the return type.

    Returns empty list, when no signature is found, one signature in typical case,
    multiple signatures, if docstring specifies multiple signatures for overload functions.
    Return None if the docstring is empty.

    Arguments:
        * docstr: docstring
        * name: name of function for which signatures are to be found
    """
    if not docstr:
        return None

    state = DocStringParser(name)
    # Return all found signatures, even if there is a parse error after some are found.
    with contextlib.suppress(tokenize.TokenError):
        try:
            tokens = tokenize.tokenize(io.BytesIO(docstr.encode("utf-8")).readline)
            for token in tokens:
                state.add_token(token)
        except IndentationError:
            return None
    sigs = state.get_signatures()

    @others
    # Return only signatures that have unique argument names. Mypy fails on non-unique arg names.
    return [sig for sig in sigs if is_unique_args(sig)]


</t>
<t tx="ekr.20221004064035.1815">def is_unique_args(sig: FunctionSig) -&gt; bool:
    """return true if function argument names are unique"""
    return len(sig.args) == len({arg.name for arg in sig.args})

</t>
<t tx="ekr.20221004064035.1816">def infer_arg_sig_from_anon_docstring(docstr: str) -&gt; list[ArgSig]:
    """Convert signature in form of "(self: TestClass, arg0: str='ada')" to List[TypedArgList]."""
    ret = infer_sig_from_docstring("stub" + docstr, "stub")
    if ret:
        return ret[0].args
    return []


</t>
<t tx="ekr.20221004064035.1817">def infer_ret_type_sig_from_docstring(docstr: str, name: str) -&gt; str | None:
    """Convert signature in form of "func(self: TestClass, arg0) -&gt; int" to their return type."""
    ret = infer_sig_from_docstring(docstr, name)
    if ret:
        return ret[0].ret_type
    return None


</t>
<t tx="ekr.20221004064035.1818">def infer_ret_type_sig_from_anon_docstring(docstr: str) -&gt; str | None:
    """Convert signature in form of "(self: TestClass, arg0) -&gt; int" to their return type."""
    return infer_ret_type_sig_from_docstring("stub" + docstr.strip(), "stub")


</t>
<t tx="ekr.20221004064035.1819">def parse_signature(sig: str) -&gt; tuple[str, list[str], list[str]] | None:
    """Split function signature into its name, positional an optional arguments.

    The expected format is "func_name(arg, opt_arg=False)". Return the name of function
    and lists of positional and optional argument names.
    """
    m = re.match(r"([.a-zA-Z0-9_]+)\(([^)]*)\)", sig)
    if not m:
        return None
    name = m.group(1)
    name = name.split(".")[-1]
    arg_string = m.group(2)
    if not arg_string.strip():
        # Simple case -- no arguments.
        return name, [], []

    args = [arg.strip() for arg in arg_string.split(",")]
    positional = []
    optional = []
    i = 0
    while i &lt; len(args):
        # Accept optional arguments as in both formats: x=None and [x].
        if args[i].startswith("[") or "=" in args[i]:
            break
        positional.append(args[i].rstrip("["))
        i += 1
        if args[i - 1].endswith("["):
            break
    while i &lt; len(args):
        arg = args[i]
        arg = arg.strip("[]")
        arg = arg.split("=")[0]
        optional.append(arg)
        i += 1
    return name, positional, optional


</t>
<t tx="ekr.20221004064035.182">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    if isinstance(self.s, TypeVarType) and self.s.id == t.id:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1820">def build_signature(positional: Sequence[str], optional: Sequence[str]) -&gt; str:
    """Build function signature from lists of positional and optional argument names."""
    args: MutableSequence[str] = []
    args.extend(positional)
    for arg in optional:
        if arg.startswith("*"):
            args.append(arg)
        else:
            args.append(f"{arg}=...")
    sig = f"({', '.join(args)})"
    # Ad-hoc fixes.
    sig = sig.replace("(self)", "")
    return sig


</t>
<t tx="ekr.20221004064035.1821">def parse_all_signatures(lines: Sequence[str]) -&gt; tuple[list[Sig], list[Sig]]:
    """Parse all signatures in a given reST document.

    Return lists of found signatures for functions and classes.
    """
    sigs = []
    class_sigs = []
    for line in lines:
        line = line.strip()
        m = re.match(r"\.\. *(function|method|class) *:: *[a-zA-Z_]", line)
        if m:
            sig = line.split("::")[1].strip()
            parsed = parse_signature(sig)
            if parsed:
                name, fixed, optional = parsed
                if m.group(1) != "class":
                    sigs.append((name, build_signature(fixed, optional)))
                else:
                    class_sigs.append((name, build_signature(fixed, optional)))

    return sorted(sigs), sorted(class_sigs)


</t>
<t tx="ekr.20221004064035.1822">def find_unique_signatures(sigs: Sequence[Sig]) -&gt; list[Sig]:
    """Remove names with duplicate found signatures."""
    sig_map: MutableMapping[str, list[str]] = {}
    for name, sig in sigs:
        sig_map.setdefault(name, []).append(sig)

    result = []
    for name, name_sigs in sig_map.items():
        if len(set(name_sigs)) == 1:
            result.append((name, name_sigs[0]))
    return sorted(result)


</t>
<t tx="ekr.20221004064035.1823">def infer_prop_type_from_docstring(docstr: str | None) -&gt; str | None:
    """Check for Google/Numpy style docstring type annotation for a property.

    The docstring has the format "&lt;type&gt;: &lt;descriptions&gt;".
    In the type string, we allow the following characters:
    * dot: because sometimes classes are annotated using full path
    * brackets: to allow type hints like List[int]
    * comma/space: things like Tuple[int, int]
    """
    if not docstr:
        return None
    test_str = r"^([a-zA-Z0-9_, \.\[\]]*): "
    m = re.match(test_str, docstr)
    return m.group(1) if m else None
</t>
<t tx="ekr.20221004064035.1824">@path C:/Repos/ekr-mypy2/mypy/
#!/usr/bin/env python3
"""Generator of dynamically typed draft stubs for arbitrary modules.

The logic of this script can be split in three steps:
* parsing options and finding sources:
  - use runtime imports be default (to find also C modules)
  - or use mypy's mechanisms, if importing is prohibited
* (optionally) semantically analysing the sources using mypy (as a single set)
* emitting the stubs text:
  - for Python modules: from ASTs using StubGenerator
  - for C modules using runtime introspection and (optionally) Sphinx docs

During first and third steps some problematic files can be skipped, but any
blocking error during second step will cause the whole program to stop.

Basic usage:

  $ stubgen foo.py bar.py some_directory
  =&gt; Generate out/foo.pyi, out/bar.pyi, and stubs for some_directory (recursively).

  $ stubgen -m urllib.parse
  =&gt; Generate out/urllib/parse.pyi.

  $ stubgen -p urllib
  =&gt; Generate stubs for whole urlib package (recursively).

For C modules, you can get more precise function signatures by parsing .rst (Sphinx)
documentation for extra information. For this, use the --doc-dir option:

  $ stubgen --doc-dir &lt;DIR&gt;/Python-3.4.2/Doc/library -m curses

Note: The generated stubs should be verified manually.

TODO:
 - maybe use .rst docs also for Python modules
 - maybe export more imported names if there is no __all__ (this affects ssl.SSLError, for example)
   - a quick and dirty heuristic would be to turn this on if a module has something like
     'from x import y as _y'
 - we don't seem to always detect properties ('closed' in 'io', for example)
"""

from __future__ import annotations

import argparse
import glob
import os
import os.path
import sys
import traceback
from collections import defaultdict
from typing import Iterable, List, Mapping, cast
from typing_extensions import Final

import mypy.build
import mypy.mixedtraverser
import mypy.parse
import mypy.traverser
import mypy.util
from mypy.build import build
from mypy.errors import CompileError, Errors
from mypy.find_sources import InvalidSourceList, create_source_list
from mypy.modulefinder import (
    BuildSource,
    FindModuleCache,
    ModuleNotFoundReason,
    SearchPaths,
    default_lib_path,
)
from mypy.moduleinspect import ModuleInspect
from mypy.nodes import (
    ARG_NAMED,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    IS_ABSTRACT,
    AssignmentStmt,
    Block,
    BytesExpr,
    CallExpr,
    ClassDef,
    ComparisonExpr,
    Decorator,
    EllipsisExpr,
    Expression,
    FloatExpr,
    FuncBase,
    FuncDef,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    ListExpr,
    MemberExpr,
    MypyFile,
    NameExpr,
    OverloadedFuncDef,
    Statement,
    StrExpr,
    TupleExpr,
    TypeInfo,
    UnaryExpr,
)
from mypy.options import Options as MypyOptions
from mypy.stubdoc import Sig, find_unique_signatures, parse_all_signatures
from mypy.stubgenc import (
    DocstringSignatureGenerator,
    ExternalSignatureGenerator,
    FallbackSignatureGenerator,
    SignatureGenerator,
    generate_stub_for_c_module,
)
from mypy.stubutil import (
    CantImport,
    common_dir_prefix,
    fail_missing,
    find_module_path_and_all_py3,
    generate_guarded,
    remove_misplaced_type_comments,
    report_missing,
    walk_packages,
)
from mypy.traverser import all_yield_expressions, has_return_statement, has_yield_expression
from mypy.types import (
    OVERLOAD_NAMES,
    AnyType,
    CallableType,
    Instance,
    NoneType,
    TupleType,
    Type,
    TypeList,
    TypeStrVisitor,
    UnboundType,
    get_proper_type,
)
from mypy.visitor import NodeVisitor

TYPING_MODULE_NAMES: Final = ("typing", "typing_extensions")

# Common ways of naming package containing vendored modules.
VENDOR_PACKAGES: Final = ["packages", "vendor", "vendored", "_vendor", "_vendored_packages"]

# Avoid some file names that are unnecessary or likely to cause trouble (\n for end of path).
BLACKLIST: Final = [
    "/six.py\n",  # Likely vendored six; too dynamic for us to handle
    "/vendored/",  # Vendored packages
    "/vendor/",  # Vendored packages
    "/_vendor/",
    "/_vendored_packages/",
]

# Special-cased names that are implicitly exported from the stub (from m import y as y).
EXTRA_EXPORTED: Final = {
    "pyasn1_modules.rfc2437.univ",
    "pyasn1_modules.rfc2459.char",
    "pyasn1_modules.rfc2459.univ",
}

# These names should be omitted from generated stubs.
IGNORED_DUNDERS: Final = {
    "__all__",
    "__author__",
    "__version__",
    "__about__",
    "__copyright__",
    "__email__",
    "__license__",
    "__summary__",
    "__title__",
    "__uri__",
    "__str__",
    "__repr__",
    "__getstate__",
    "__setstate__",
    "__slots__",
}

# These methods are expected to always return a non-trivial value.
METHODS_WITH_RETURN_VALUE: Final = {
    "__ne__",
    "__eq__",
    "__lt__",
    "__le__",
    "__gt__",
    "__ge__",
    "__hash__",
    "__iter__",
}


@others
if __name__ == "__main__":
    main()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1825">class Options:
    """Represents stubgen options.

    This class is mutable to simplify testing.
    """

    @others
</t>
<t tx="ekr.20221004064035.1826">def __init__(
    self,
    pyversion: tuple[int, int],
    no_import: bool,
    doc_dir: str,
    search_path: list[str],
    interpreter: str,
    parse_only: bool,
    ignore_errors: bool,
    include_private: bool,
    output_dir: str,
    modules: list[str],
    packages: list[str],
    files: list[str],
    verbose: bool,
    quiet: bool,
    export_less: bool,
) -&gt; None:
    # See parse_options for descriptions of the flags.
    self.pyversion = pyversion
    self.no_import = no_import
    self.doc_dir = doc_dir
    self.search_path = search_path
    self.interpreter = interpreter
    self.decointerpreter = interpreter
    self.parse_only = parse_only
    self.ignore_errors = ignore_errors
    self.include_private = include_private
    self.output_dir = output_dir
    self.modules = modules
    self.packages = packages
    self.files = files
    self.verbose = verbose
    self.quiet = quiet
    self.export_less = export_less


</t>
<t tx="ekr.20221004064035.1827">class StubSource:
    """A single source for stub: can be a Python or C module.

    A simple extension of BuildSource that also carries the AST and
    the value of __all__ detected at runtime.
    """

    @others
</t>
<t tx="ekr.20221004064035.1828">def __init__(
    self, module: str, path: str | None = None, runtime_all: list[str] | None = None
) -&gt; None:
    self.source = BuildSource(path, module, None)
    self.runtime_all = runtime_all
    self.ast: MypyFile | None = None

</t>
<t tx="ekr.20221004064035.1829">@property
def module(self) -&gt; str:
    return self.source.module

</t>
<t tx="ekr.20221004064035.183">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    if self.s == t:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1830">@property
def path(self) -&gt; str | None:
    return self.source.path


</t>
<t tx="ekr.20221004064035.1831"># What was generated previously in the stub file. We keep track of these to generate
# nicely formatted output (add empty line between non-empty classes, for example).
EMPTY: Final = "EMPTY"
FUNC: Final = "FUNC"
CLASS: Final = "CLASS"
EMPTY_CLASS: Final = "EMPTY_CLASS"
VAR: Final = "VAR"
NOT_IN_ALL: Final = "NOT_IN_ALL"

# Indicates that we failed to generate a reasonable output
# for a given node. These should be manually replaced by a user.

ERROR_MARKER: Final = "&lt;ERROR&gt;"


</t>
<t tx="ekr.20221004064035.1832">class AnnotationPrinter(TypeStrVisitor):
    """Visitor used to print existing annotations in a file.

    The main difference from TypeStrVisitor is a better treatment of
    unbound types.

    Notes:
    * This visitor doesn't add imports necessary for annotations, this is done separately
      by ImportTracker.
    * It can print all kinds of types, but the generated strings may not be valid (notably
      callable types) since it prints the same string that reveal_type() does.
    * For Instance types it prints the fully qualified names.
    """

    @others
</t>
<t tx="ekr.20221004064035.1833"># TODO: Generate valid string representation for callable types.
# TODO: Use short names for Instances.
def __init__(self, stubgen: StubGenerator) -&gt; None:
    super().__init__()
    self.stubgen = stubgen

</t>
<t tx="ekr.20221004064035.1834">def visit_any(self, t: AnyType) -&gt; str:
    s = super().visit_any(t)
    self.stubgen.import_tracker.require_name(s)
    return s

</t>
<t tx="ekr.20221004064035.1835">def visit_unbound_type(self, t: UnboundType) -&gt; str:
    s = t.name
    self.stubgen.import_tracker.require_name(s)
    if t.args:
        s += f"[{self.args_str(t.args)}]"
    return s

</t>
<t tx="ekr.20221004064035.1836">def visit_none_type(self, t: NoneType) -&gt; str:
    return "None"

</t>
<t tx="ekr.20221004064035.1837">def visit_type_list(self, t: TypeList) -&gt; str:
    return f"[{self.list_str(t.items)}]"

</t>
<t tx="ekr.20221004064035.1838">def args_str(self, args: Iterable[Type]) -&gt; str:
    """Convert an array of arguments to strings and join the results with commas.

    The main difference from list_str is the preservation of quotes for string
    arguments
    """
    types = ["builtins.bytes", "builtins.str"]
    res = []
    for arg in args:
        arg_str = arg.accept(self)
        if isinstance(arg, UnboundType) and arg.original_str_fallback in types:
            res.append(f"'{arg_str}'")
        else:
            res.append(arg_str)
    return ", ".join(res)


</t>
<t tx="ekr.20221004064035.1839">class AliasPrinter(NodeVisitor[str]):
    """Visitor used to collect type aliases _and_ type variable definitions.

    Visit r.h.s of the definition to get the string representation of type alias.
    """

    @others
</t>
<t tx="ekr.20221004064035.184">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    if self.s == t:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1840">def __init__(self, stubgen: StubGenerator) -&gt; None:
    self.stubgen = stubgen
    super().__init__()

</t>
<t tx="ekr.20221004064035.1841">def visit_call_expr(self, node: CallExpr) -&gt; str:
    # Call expressions are not usually types, but we also treat `X = TypeVar(...)` as a
    # type alias that has to be preserved (even if TypeVar is not the same as an alias)
    callee = node.callee.accept(self)
    args = []
    for name, arg, kind in zip(node.arg_names, node.args, node.arg_kinds):
        if kind == ARG_POS:
            args.append(arg.accept(self))
        elif kind == ARG_STAR:
            args.append("*" + arg.accept(self))
        elif kind == ARG_STAR2:
            args.append("**" + arg.accept(self))
        elif kind == ARG_NAMED:
            args.append(f"{name}={arg.accept(self)}")
        else:
            raise ValueError(f"Unknown argument kind {kind} in call")
    return f"{callee}({', '.join(args)})"

</t>
<t tx="ekr.20221004064035.1842">def visit_name_expr(self, node: NameExpr) -&gt; str:
    self.stubgen.import_tracker.require_name(node.name)
    return node.name

</t>
<t tx="ekr.20221004064035.1843">def visit_member_expr(self, o: MemberExpr) -&gt; str:
    node: Expression = o
    trailer = ""
    while isinstance(node, MemberExpr):
        trailer = "." + node.name + trailer
        node = node.expr
    if not isinstance(node, NameExpr):
        return ERROR_MARKER
    self.stubgen.import_tracker.require_name(node.name)
    return node.name + trailer

</t>
<t tx="ekr.20221004064035.1844">def visit_str_expr(self, node: StrExpr) -&gt; str:
    return repr(node.value)

</t>
<t tx="ekr.20221004064035.1845">def visit_index_expr(self, node: IndexExpr) -&gt; str:
    base = node.base.accept(self)
    index = node.index.accept(self)
    return f"{base}[{index}]"

</t>
<t tx="ekr.20221004064035.1846">def visit_tuple_expr(self, node: TupleExpr) -&gt; str:
    return ", ".join(n.accept(self) for n in node.items)

</t>
<t tx="ekr.20221004064035.1847">def visit_list_expr(self, node: ListExpr) -&gt; str:
    return f"[{', '.join(n.accept(self) for n in node.items)}]"

</t>
<t tx="ekr.20221004064035.1848">def visit_ellipsis(self, node: EllipsisExpr) -&gt; str:
    return "..."


</t>
<t tx="ekr.20221004064035.1849">class ImportTracker:
    """Record necessary imports during stub generation."""

    @others
</t>
<t tx="ekr.20221004064035.185">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.1850">def __init__(self) -&gt; None:
    # module_for['foo'] has the module name where 'foo' was imported from, or None if
    # 'foo' is a module imported directly; examples
    #     'from pkg.m import f as foo' ==&gt; module_for['foo'] == 'pkg.m'
    #     'from m import f' ==&gt; module_for['f'] == 'm'
    #     'import m' ==&gt; module_for['m'] == None
    #     'import pkg.m' ==&gt; module_for['pkg.m'] == None
    #                    ==&gt; module_for['pkg'] == None
    self.module_for: dict[str, str | None] = {}

    # direct_imports['foo'] is the module path used when the name 'foo' was added to the
    # namespace.
    #   import foo.bar.baz  ==&gt; direct_imports['foo'] == 'foo.bar.baz'
    #                       ==&gt; direct_imports['foo.bar'] == 'foo.bar.baz'
    #                       ==&gt; direct_imports['foo.bar.baz'] == 'foo.bar.baz'
    self.direct_imports: dict[str, str] = {}

    # reverse_alias['foo'] is the name that 'foo' had originally when imported with an
    # alias; examples
    #     'import numpy as np' ==&gt; reverse_alias['np'] == 'numpy'
    #     'import foo.bar as bar' ==&gt; reverse_alias['bar'] == 'foo.bar'
    #     'from decimal import Decimal as D' ==&gt; reverse_alias['D'] == 'Decimal'
    self.reverse_alias: dict[str, str] = {}

    # required_names is the set of names that are actually used in a type annotation
    self.required_names: set[str] = set()

    # Names that should be reexported if they come from another module
    self.reexports: set[str] = set()

</t>
<t tx="ekr.20221004064035.1851">def add_import_from(self, module: str, names: list[tuple[str, str | None]]) -&gt; None:
    for name, alias in names:
        if alias:
            # 'from {module} import {name} as {alias}'
            self.module_for[alias] = module
            self.reverse_alias[alias] = name
        else:
            # 'from {module} import {name}'
            self.module_for[name] = module
            self.reverse_alias.pop(name, None)
        self.direct_imports.pop(alias or name, None)

</t>
<t tx="ekr.20221004064035.1852">def add_import(self, module: str, alias: str | None = None) -&gt; None:
    if alias:
        # 'import {module} as {alias}'
        self.module_for[alias] = None
        self.reverse_alias[alias] = module
    else:
        # 'import {module}'
        name = module
        # add module and its parent packages
        while name:
            self.module_for[name] = None
            self.direct_imports[name] = module
            self.reverse_alias.pop(name, None)
            name = name.rpartition(".")[0]

</t>
<t tx="ekr.20221004064035.1853">def require_name(self, name: str) -&gt; None:
    self.required_names.add(name.split(".")[0])

</t>
<t tx="ekr.20221004064035.1854">def reexport(self, name: str) -&gt; None:
    """Mark a given non qualified name as needed in __all__.

    This means that in case it comes from a module, it should be
    imported with an alias even is the alias is the same as the name.
    """
    self.require_name(name)
    self.reexports.add(name)

</t>
<t tx="ekr.20221004064035.1855">def import_lines(self) -&gt; list[str]:
    """The list of required import lines (as strings with python code)."""
    result = []

    # To summarize multiple names imported from a same module, we collect those
    # in the `module_map` dictionary, mapping a module path to the list of names that should
    # be imported from it. the names can also be alias in the form 'original as alias'
    module_map: Mapping[str, list[str]] = defaultdict(list)

    for name in sorted(self.required_names):
        # If we haven't seen this name in an import statement, ignore it
        if name not in self.module_for:
            continue

        m = self.module_for[name]
        if m is not None:
            # This name was found in a from ... import ...
            # Collect the name in the module_map
            if name in self.reverse_alias:
                name = f"{self.reverse_alias[name]} as {name}"
            elif name in self.reexports:
                name = f"{name} as {name}"
            module_map[m].append(name)
        else:
            # This name was found in an import ...
            # We can already generate the import line
            if name in self.reverse_alias:
                source = self.reverse_alias[name]
                result.append(f"import {source} as {name}\n")
            elif name in self.reexports:
                assert "." not in name  # Because reexports only has nonqualified names
                result.append(f"import {name} as {name}\n")
            else:
                result.append(f"import {self.direct_imports[name]}\n")

    # Now generate all the from ... import ... lines collected in module_map
    for module, names in sorted(module_map.items()):
        result.append(f"from {module} import {', '.join(sorted(names))}\n")
    return result


</t>
<t tx="ekr.20221004064035.1856">def find_defined_names(file: MypyFile) -&gt; set[str]:
    finder = DefinitionFinder()
    file.accept(finder)
    return finder.names


</t>
<t tx="ekr.20221004064035.1857">class DefinitionFinder(mypy.traverser.TraverserVisitor):
    """Find names of things defined at the top level of a module."""

    # TODO: Assignment statements etc.

    @others
</t>
<t tx="ekr.20221004064035.1858">def __init__(self) -&gt; None:
    # Short names of things defined at the top level.
    self.names: set[str] = set()

</t>
<t tx="ekr.20221004064035.1859">def visit_class_def(self, o: ClassDef) -&gt; None:
    # Don't recurse into classes, as we only keep track of top-level definitions.
    self.names.add(o.name)

</t>
<t tx="ekr.20221004064035.186">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    # TODO: is this the right variance?
    if isinstance(self.s, Parameters) or isinstance(self.s, CallableType):
        if len(t.arg_types) != len(self.s.arg_types):
            return self.default(self.s)
        return t.copy_modified(
            arg_types=[meet_types(s_a, t_a) for s_a, t_a in zip(self.s.arg_types, t.arg_types)]
        )
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1860">def visit_func_def(self, o: FuncDef) -&gt; None:
    # Don't recurse, as we only keep track of top-level definitions.
    self.names.add(o.name)


</t>
<t tx="ekr.20221004064035.1861">def find_referenced_names(file: MypyFile) -&gt; set[str]:
    finder = ReferenceFinder()
    file.accept(finder)
    return finder.refs


</t>
<t tx="ekr.20221004064035.1862">class ReferenceFinder(mypy.mixedtraverser.MixedTraverserVisitor):
    """Find all name references (both local and global)."""

    # TODO: Filter out local variable and class attribute references

    @others
</t>
<t tx="ekr.20221004064035.1863">def __init__(self) -&gt; None:
    # Short names of things defined at the top level.
    self.refs: set[str] = set()

</t>
<t tx="ekr.20221004064035.1864">def visit_block(self, block: Block) -&gt; None:
    if not block.is_unreachable:
        super().visit_block(block)

</t>
<t tx="ekr.20221004064035.1865">def visit_name_expr(self, e: NameExpr) -&gt; None:
    self.refs.add(e.name)

</t>
<t tx="ekr.20221004064035.1866">def visit_instance(self, t: Instance) -&gt; None:
    self.add_ref(t.type.fullname)
    super().visit_instance(t)

</t>
<t tx="ekr.20221004064035.1867">def visit_unbound_type(self, t: UnboundType) -&gt; None:
    if t.name:
        self.add_ref(t.name)

</t>
<t tx="ekr.20221004064035.1868">def visit_tuple_type(self, t: TupleType) -&gt; None:
    # Ignore fallback
    for item in t.items:
        item.accept(self)

</t>
<t tx="ekr.20221004064035.1869">def visit_callable_type(self, t: CallableType) -&gt; None:
    # Ignore fallback
    for arg in t.arg_types:
        arg.accept(self)
    t.ret_type.accept(self)

</t>
<t tx="ekr.20221004064035.187">def visit_instance(self, t: Instance) -&gt; ProperType:
    if isinstance(self.s, Instance):
        if t.type == self.s.type:
            if is_subtype(t, self.s) or is_subtype(self.s, t):
                # Combine type arguments. We could have used join below
                # equivalently.
                args: list[Type] = []
                # N.B: We use zip instead of indexing because the lengths might have
                # mismatches during daemon reprocessing.
                for ta, sia in zip(t.args, self.s.args):
                    args.append(self.meet(ta, sia))
                return Instance(t.type, args)
            else:
                if state.strict_optional:
                    return UninhabitedType()
                else:
                    return NoneType()
        else:
            alt_promote = t.type.alt_promote
            if alt_promote and alt_promote is self.s.type:
                return t
            alt_promote = self.s.type.alt_promote
            if alt_promote and alt_promote is t.type:
                return self.s
            if is_subtype(t, self.s):
                return t
            elif is_subtype(self.s, t):
                # See also above comment.
                return self.s
            else:
                if state.strict_optional:
                    return UninhabitedType()
                else:
                    return NoneType()
    elif isinstance(self.s, FunctionLike) and t.type.is_protocol:
        call = join.unpack_callback_protocol(t)
        if call:
            return meet_types(call, self.s)
    elif isinstance(self.s, FunctionLike) and self.s.is_type_obj() and t.type.is_metaclass():
        if is_subtype(self.s.fallback, t):
            return self.s
        return self.default(self.s)
    elif isinstance(self.s, TypeType):
        return meet_types(t, self.s)
    elif isinstance(self.s, TupleType):
        return meet_types(t, self.s)
    elif isinstance(self.s, LiteralType):
        return meet_types(t, self.s)
    elif isinstance(self.s, TypedDictType):
        return meet_types(t, self.s)
    return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1870">def add_ref(self, fullname: str) -&gt; None:
    self.refs.add(fullname.split(".")[-1])


</t>
<t tx="ekr.20221004064035.1871">class StubGenerator(mypy.traverser.TraverserVisitor):
    """Generate stub text from a mypy AST."""

    @others
</t>
<t tx="ekr.20221004064035.1872">def __init__(
    self,
    _all_: list[str] | None,
    include_private: bool = False,
    analyzed: bool = False,
    export_less: bool = False,
) -&gt; None:
    # Best known value of __all__.
    self._all_ = _all_
    self._output: list[str] = []
    self._decorators: list[str] = []
    self._import_lines: list[str] = []
    # Current indent level (indent is hardcoded to 4 spaces).
    self._indent = ""
    # Stack of defined variables (per scope).
    self._vars: list[list[str]] = [[]]
    # What was generated previously in the stub file.
    self._state = EMPTY
    self._toplevel_names: list[str] = []
    self._include_private = include_private
    self.import_tracker = ImportTracker()
    # Was the tree semantically analysed before?
    self.analyzed = analyzed
    # Disable implicit exports of package-internal imports?
    self.export_less = export_less
    # Add imports that could be implicitly generated
    self.import_tracker.add_import_from("typing", [("NamedTuple", None)])
    # Names in __all__ are required
    for name in _all_ or ():
        if name not in IGNORED_DUNDERS:
            self.import_tracker.reexport(name)
    self.defined_names: set[str] = set()
    # Short names of methods defined in the body of the current class
    self.method_names: set[str] = set()

</t>
<t tx="ekr.20221004064035.1873">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.module = o.fullname  # Current module being processed
    self.path = o.path
    self.defined_names = find_defined_names(o)
    self.referenced_names = find_referenced_names(o)
    known_imports = {
        "_typeshed": ["Incomplete"],
        "typing": ["Any", "TypeVar"],
        "collections.abc": ["Generator"],
    }
    for pkg, imports in known_imports.items():
        for t in imports:
            if t not in self.defined_names:
                alias = None
            else:
                alias = "_" + t
            self.import_tracker.add_import_from(pkg, [(t, alias)])
    super().visit_mypy_file(o)
    undefined_names = [name for name in self._all_ or [] if name not in self._toplevel_names]
    if undefined_names:
        if self._state != EMPTY:
            self.add("\n")
        self.add("# Names in __all__ with no definition:\n")
        for name in sorted(undefined_names):
            self.add(f"#   {name}\n")

</t>
<t tx="ekr.20221004064035.1874">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    """@property with setters and getters, or @overload chain"""
    overload_chain = False
    for item in o.items:
        if not isinstance(item, Decorator):
            continue

        if self.is_private_name(item.func.name, item.func.fullname):
            continue

        is_abstract, is_overload = self.process_decorator(item)

        if not overload_chain:
            self.visit_func_def(item.func, is_abstract=is_abstract, is_overload=is_overload)
            if is_overload:
                overload_chain = True
        elif is_overload:
            self.visit_func_def(item.func, is_abstract=is_abstract, is_overload=is_overload)
        else:
            # skip the overload implementation and clear the decorator we just processed
            self.clear_decorators()

</t>
<t tx="ekr.20221004064035.1875">def visit_func_def(
    self, o: FuncDef, is_abstract: bool = False, is_overload: bool = False
) -&gt; None:
    if (
        self.is_private_name(o.name, o.fullname)
        or self.is_not_in_all(o.name)
        or (self.is_recorded_name(o.name) and not is_overload)
    ):
        self.clear_decorators()
        return
    if not self._indent and self._state not in (EMPTY, FUNC) and not o.is_awaitable_coroutine:
        self.add("\n")
    if not self.is_top_level():
        self_inits = find_self_initializers(o)
        for init, value in self_inits:
            if init in self.method_names:
                # Can't have both an attribute and a method/property with the same name.
                continue
            init_code = self.get_init(init, value)
            if init_code:
                self.add(init_code)
    # dump decorators, just before "def ..."
    for s in self._decorators:
        self.add(s)
    self.clear_decorators()
    self.add(f"{self._indent}{'async ' if o.is_coroutine else ''}def {o.name}(")
    self.record_name(o.name)
    args: list[str] = []
    for i, arg_ in enumerate(o.arguments):
        var = arg_.variable
        kind = arg_.kind
        name = var.name
        annotated_type = (
            o.unanalyzed_type.arg_types[i]
            if isinstance(o.unanalyzed_type, CallableType)
            else None
        )
        # I think the name check is incorrect: there are libraries which
        # name their 0th argument other than self/cls
        is_self_arg = i == 0 and name == "self"
        is_cls_arg = i == 0 and name == "cls"
        annotation = ""
        if annotated_type and not is_self_arg and not is_cls_arg:
            # Luckily, an argument explicitly annotated with "Any" has
            # type "UnboundType" and will not match.
            if not isinstance(get_proper_type(annotated_type), AnyType):
                annotation = f": {self.print_annotation(annotated_type)}"

        if kind.is_named() and not any(arg.startswith("*") for arg in args):
            args.append("*")

        if arg_.initializer:
            if not annotation:
                typename = self.get_str_type_of_node(arg_.initializer, True, False)
                if typename == "":
                    annotation = "=..."
                else:
                    annotation = f": {typename} = ..."
            else:
                annotation += " = ..."
            arg = name + annotation
        elif kind == ARG_STAR:
            arg = f"*{name}{annotation}"
        elif kind == ARG_STAR2:
            arg = f"**{name}{annotation}"
        else:
            arg = name + annotation
        args.append(arg)
    retname = None
    if o.name != "__init__" and isinstance(o.unanalyzed_type, CallableType):
        if isinstance(get_proper_type(o.unanalyzed_type.ret_type), AnyType):
            # Luckily, a return type explicitly annotated with "Any" has
            # type "UnboundType" and will enter the else branch.
            retname = None  # implicit Any
        else:
            retname = self.print_annotation(o.unanalyzed_type.ret_type)
    elif o.abstract_status == IS_ABSTRACT or o.name in METHODS_WITH_RETURN_VALUE:
        # Always assume abstract methods return Any unless explicitly annotated. Also
        # some dunder methods should not have a None return type.
        retname = None  # implicit Any
    elif has_yield_expression(o):
        self.add_abc_import("Generator")
        yield_name = "None"
        send_name = "None"
        return_name = "None"
        for expr, in_assignment in all_yield_expressions(o):
            if expr.expr is not None and not self.is_none_expr(expr.expr):
                self.add_typing_import("Incomplete")
                yield_name = "Incomplete"
            if in_assignment:
                self.add_typing_import("Incomplete")
                send_name = "Incomplete"
        if has_return_statement(o):
            self.add_typing_import("Incomplete")
            return_name = "Incomplete"
        generator_name = self.typing_name("Generator")
        retname = f"{generator_name}[{yield_name}, {send_name}, {return_name}]"
    elif not has_return_statement(o) and not is_abstract:
        retname = "None"
    retfield = ""
    if retname is not None:
        retfield = " -&gt; " + retname

    self.add(", ".join(args))
    self.add(f"){retfield}: ...\n")
    self._state = FUNC

</t>
<t tx="ekr.20221004064035.1876">def is_none_expr(self, expr: Expression) -&gt; bool:
    return isinstance(expr, NameExpr) and expr.name == "None"

</t>
<t tx="ekr.20221004064035.1877">def visit_decorator(self, o: Decorator) -&gt; None:
    if self.is_private_name(o.func.name, o.func.fullname):
        return

    is_abstract, _ = self.process_decorator(o)
    self.visit_func_def(o.func, is_abstract=is_abstract)

</t>
<t tx="ekr.20221004064035.1878">def process_decorator(self, o: Decorator) -&gt; tuple[bool, bool]:
    """Process a series of decorators.

    Only preserve certain special decorators such as @abstractmethod.

    Return a pair of booleans:
    - True if any of the decorators makes a method abstract.
    - True if any of the decorators is typing.overload.
    """
    is_abstract = False
    is_overload = False
    for decorator in o.original_decorators:
        if isinstance(decorator, NameExpr):
            i_is_abstract, i_is_overload = self.process_name_expr_decorator(decorator, o)
            is_abstract = is_abstract or i_is_abstract
            is_overload = is_overload or i_is_overload
        elif isinstance(decorator, MemberExpr):
            i_is_abstract, i_is_overload = self.process_member_expr_decorator(decorator, o)
            is_abstract = is_abstract or i_is_abstract
            is_overload = is_overload or i_is_overload
    return is_abstract, is_overload

</t>
<t tx="ekr.20221004064035.1879">def process_name_expr_decorator(self, expr: NameExpr, context: Decorator) -&gt; tuple[bool, bool]:
    """Process a function decorator of form @foo.

    Only preserve certain special decorators such as @abstractmethod.

    Return a pair of booleans:
    - True if the decorator makes a method abstract.
    - True if the decorator is typing.overload.
    """
    is_abstract = False
    is_overload = False
    name = expr.name
    if name in ("property", "staticmethod", "classmethod"):
        self.add_decorator(name)
    elif self.import_tracker.module_for.get(name) in (
        "asyncio",
        "asyncio.coroutines",
        "types",
    ):
        self.add_coroutine_decorator(context.func, name, name)
    elif self.refers_to_fullname(name, "abc.abstractmethod"):
        self.add_decorator(name)
        self.import_tracker.require_name(name)
        is_abstract = True
    elif self.refers_to_fullname(name, "abc.abstractproperty"):
        self.add_decorator("property")
        self.add_decorator("abc.abstractmethod")
        is_abstract = True
    elif self.refers_to_fullname(name, OVERLOAD_NAMES):
        self.add_decorator(name)
        self.add_typing_import("overload")
        is_overload = True
    return is_abstract, is_overload

</t>
<t tx="ekr.20221004064035.188">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    if isinstance(self.s, CallableType) and join.is_similar_callables(t, self.s):
        if is_equivalent(t, self.s):
            return join.combine_similar_callables(t, self.s)
        result = meet_similar_callables(t, self.s)
        # We set the from_type_type flag to suppress error when a collection of
        # concrete class objects gets inferred as their common abstract superclass.
        if not (
            (t.is_type_obj() and t.type_object().is_abstract)
            or (self.s.is_type_obj() and self.s.type_object().is_abstract)
        ):
            result.from_type_type = True
        if isinstance(get_proper_type(result.ret_type), UninhabitedType):
            # Return a plain None or &lt;uninhabited&gt; instead of a weird function.
            return self.default(self.s)
        return result
    elif isinstance(self.s, TypeType) and t.is_type_obj() and not t.is_generic():
        # In this case we are able to potentially produce a better meet.
        res = meet_types(self.s.item, t.ret_type)
        if not isinstance(res, (NoneType, UninhabitedType)):
            return TypeType.make_normalized(res)
        return self.default(self.s)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = join.unpack_callback_protocol(self.s)
        if call:
            return meet_types(t, call)
    return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1880">def refers_to_fullname(self, name: str, fullname: str | tuple[str, ...]) -&gt; bool:
    if isinstance(fullname, tuple):
        return any(self.refers_to_fullname(name, fname) for fname in fullname)
    module, short = fullname.rsplit(".", 1)
    return self.import_tracker.module_for.get(name) == module and (
        name == short or self.import_tracker.reverse_alias.get(name) == short
    )

</t>
<t tx="ekr.20221004064035.1881">def process_member_expr_decorator(
    self, expr: MemberExpr, context: Decorator
) -&gt; tuple[bool, bool]:
    """Process a function decorator of form @foo.bar.

    Only preserve certain special decorators such as @abstractmethod.

    Return a pair of booleans:
    - True if the decorator makes a method abstract.
    - True if the decorator is typing.overload.
    """
    is_abstract = False
    is_overload = False
    if expr.name == "setter" and isinstance(expr.expr, NameExpr):
        self.add_decorator(f"{expr.expr.name}.setter")
    elif (
        isinstance(expr.expr, NameExpr)
        and (
            expr.expr.name == "abc"
            or self.import_tracker.reverse_alias.get(expr.expr.name) == "abc"
        )
        and expr.name in ("abstractmethod", "abstractproperty")
    ):
        if expr.name == "abstractproperty":
            self.import_tracker.require_name(expr.expr.name)
            self.add_decorator("%s" % ("property"))
            self.add_decorator("{}.{}".format(expr.expr.name, "abstractmethod"))
        else:
            self.import_tracker.require_name(expr.expr.name)
            self.add_decorator(f"{expr.expr.name}.{expr.name}")
        is_abstract = True
    elif expr.name == "coroutine":
        if (
            isinstance(expr.expr, MemberExpr)
            and expr.expr.name == "coroutines"
            and isinstance(expr.expr.expr, NameExpr)
            and (
                expr.expr.expr.name == "asyncio"
                or self.import_tracker.reverse_alias.get(expr.expr.expr.name) == "asyncio"
            )
        ):
            self.add_coroutine_decorator(
                context.func,
                f"{expr.expr.expr.name}.coroutines.coroutine",
                expr.expr.expr.name,
            )
        elif isinstance(expr.expr, NameExpr) and (
            expr.expr.name in ("asyncio", "types")
            or self.import_tracker.reverse_alias.get(expr.expr.name)
            in ("asyncio", "asyncio.coroutines", "types")
        ):
            self.add_coroutine_decorator(
                context.func, expr.expr.name + ".coroutine", expr.expr.name
            )
    elif (
        isinstance(expr.expr, NameExpr)
        and (
            expr.expr.name in TYPING_MODULE_NAMES
            or self.import_tracker.reverse_alias.get(expr.expr.name) in TYPING_MODULE_NAMES
        )
        and expr.name == "overload"
    ):
        self.import_tracker.require_name(expr.expr.name)
        self.add_decorator(f"{expr.expr.name}.overload")
        is_overload = True
    return is_abstract, is_overload

</t>
<t tx="ekr.20221004064035.1882">def visit_class_def(self, o: ClassDef) -&gt; None:
    self.method_names = find_method_names(o.defs.body)
    sep: int | None = None
    if not self._indent and self._state != EMPTY:
        sep = len(self._output)
        self.add("\n")
    self.add(f"{self._indent}class {o.name}")
    self.record_name(o.name)
    base_types = self.get_base_types(o)
    if base_types:
        for base in base_types:
            self.import_tracker.require_name(base)
    if isinstance(o.metaclass, (NameExpr, MemberExpr)):
        meta = o.metaclass.accept(AliasPrinter(self))
        base_types.append("metaclass=" + meta)
    elif self.analyzed and o.info.is_protocol:
        type_str = "Protocol"
        if o.info.type_vars:
            type_str += f'[{", ".join(o.info.type_vars)}]'
        base_types.append(type_str)
        self.add_typing_import("Protocol")
    elif self.analyzed and o.info.is_abstract:
        base_types.append("metaclass=abc.ABCMeta")
        self.import_tracker.add_import("abc")
        self.import_tracker.require_name("abc")
    if base_types:
        self.add(f"({', '.join(base_types)})")
    self.add(":\n")
    n = len(self._output)
    self._indent += "    "
    self._vars.append([])
    super().visit_class_def(o)
    self._indent = self._indent[:-4]
    self._vars.pop()
    self._vars[-1].append(o.name)
    if len(self._output) == n:
        if self._state == EMPTY_CLASS and sep is not None:
            self._output[sep] = ""
        self._output[-1] = self._output[-1][:-1] + " ...\n"
        self._state = EMPTY_CLASS
    else:
        self._state = CLASS
    self.method_names = set()

</t>
<t tx="ekr.20221004064035.1883">def get_base_types(self, cdef: ClassDef) -&gt; list[str]:
    """Get list of base classes for a class."""
    base_types: list[str] = []
    for base in cdef.base_type_exprs:
        if isinstance(base, NameExpr):
            if base.name != "object":
                base_types.append(base.name)
        elif isinstance(base, MemberExpr):
            modname = get_qualified_name(base.expr)
            base_types.append(f"{modname}.{base.name}")
        elif isinstance(base, IndexExpr):
            p = AliasPrinter(self)
            base_types.append(base.accept(p))
    return base_types

</t>
<t tx="ekr.20221004064035.1884">def visit_block(self, o: Block) -&gt; None:
    # Unreachable statements may be partially uninitialized and that may
    # cause trouble.
    if not o.is_unreachable:
        super().visit_block(o)

</t>
<t tx="ekr.20221004064035.1885">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    foundl = []

    for lvalue in o.lvalues:
        if isinstance(lvalue, NameExpr) and self.is_namedtuple(o.rvalue):
            assert isinstance(o.rvalue, CallExpr)
            self.process_namedtuple(lvalue, o.rvalue)
            continue
        if (
            self.is_top_level()
            and isinstance(lvalue, NameExpr)
            and not self.is_private_name(lvalue.name)
            and
            # it is never an alias with explicit annotation
            not o.unanalyzed_type
            and self.is_alias_expression(o.rvalue)
        ):
            self.process_typealias(lvalue, o.rvalue)
            continue
        if isinstance(lvalue, TupleExpr) or isinstance(lvalue, ListExpr):
            items = lvalue.items
            if isinstance(o.unanalyzed_type, TupleType):  # type: ignore[misc]
                annotations: Iterable[Type | None] = o.unanalyzed_type.items
            else:
                annotations = [None] * len(items)
        else:
            items = [lvalue]
            annotations = [o.unanalyzed_type]
        sep = False
        found = False
        for item, annotation in zip(items, annotations):
            if isinstance(item, NameExpr):
                init = self.get_init(item.name, o.rvalue, annotation)
                if init:
                    found = True
                    if not sep and not self._indent and self._state not in (EMPTY, VAR):
                        init = "\n" + init
                        sep = True
                    self.add(init)
                    self.record_name(item.name)
        foundl.append(found)

    if all(foundl):
        self._state = VAR

</t>
<t tx="ekr.20221004064035.1886">def is_namedtuple(self, expr: Expression) -&gt; bool:
    if not isinstance(expr, CallExpr):
        return False
    callee = expr.callee
    return (isinstance(callee, NameExpr) and callee.name.endswith("namedtuple")) or (
        isinstance(callee, MemberExpr) and callee.name == "namedtuple"
    )

</t>
<t tx="ekr.20221004064035.1887">def process_namedtuple(self, lvalue: NameExpr, rvalue: CallExpr) -&gt; None:
    if self._state != EMPTY:
        self.add("\n")
    if isinstance(rvalue.args[1], StrExpr):
        items = rvalue.args[1].value.replace(",", " ").split()
    elif isinstance(rvalue.args[1], (ListExpr, TupleExpr)):
        list_items = cast(List[StrExpr], rvalue.args[1].items)
        items = [item.value for item in list_items]
    else:
        self.add(f"{self._indent}{lvalue.name}: Incomplete")
        self.import_tracker.require_name("Incomplete")
        return
    self.import_tracker.require_name("NamedTuple")
    self.add(f"{self._indent}class {lvalue.name}(NamedTuple):")
    if len(items) == 0:
        self.add(" ...\n")
    else:
        self.import_tracker.require_name("Incomplete")
        self.add("\n")
        for item in items:
            self.add(f"{self._indent}    {item}: Incomplete\n")
    self._state = CLASS

</t>
<t tx="ekr.20221004064035.1888">def is_alias_expression(self, expr: Expression, top_level: bool = True) -&gt; bool:
    """Return True for things that look like target for an alias.

    Used to know if assignments look like type aliases, function alias,
    or module alias.
    """
    # Assignment of TypeVar(...) are passed through
    if (
        isinstance(expr, CallExpr)
        and isinstance(expr.callee, NameExpr)
        and expr.callee.name == "TypeVar"
    ):
        return True
    elif isinstance(expr, EllipsisExpr):
        return not top_level
    elif isinstance(expr, NameExpr):
        if expr.name in ("True", "False"):
            return False
        elif expr.name == "None":
            return not top_level
        else:
            return not self.is_private_name(expr.name)
    elif isinstance(expr, MemberExpr) and self.analyzed:
        # Also add function and module aliases.
        return (
            top_level
            and isinstance(expr.node, (FuncDef, Decorator, MypyFile))
            or isinstance(expr.node, TypeInfo)
        ) and not self.is_private_member(expr.node.fullname)
    elif (
        isinstance(expr, IndexExpr)
        and isinstance(expr.base, NameExpr)
        and not self.is_private_name(expr.base.name)
    ):
        if isinstance(expr.index, TupleExpr):
            indices = expr.index.items
        else:
            indices = [expr.index]
        if expr.base.name == "Callable" and len(indices) == 2:
            args, ret = indices
            if isinstance(args, EllipsisExpr):
                indices = [ret]
            elif isinstance(args, ListExpr):
                indices = args.items + [ret]
            else:
                return False
        return all(self.is_alias_expression(i, top_level=False) for i in indices)
    else:
        return False

</t>
<t tx="ekr.20221004064035.1889">def process_typealias(self, lvalue: NameExpr, rvalue: Expression) -&gt; None:
    p = AliasPrinter(self)
    self.add(f"{lvalue.name} = {rvalue.accept(p)}\n")
    self.record_name(lvalue.name)
    self._vars[-1].append(lvalue.name)

</t>
<t tx="ekr.20221004064035.189">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    # TODO: Implement a better algorithm that covers at least the same cases
    # as TypeJoinVisitor.visit_overloaded().
    s = self.s
    if isinstance(s, FunctionLike):
        if s.items == t.items:
            return Overloaded(t.items)
        elif is_subtype(s, t):
            return s
        elif is_subtype(t, s):
            return t
        else:
            return meet_types(t.fallback, s.fallback)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = join.unpack_callback_protocol(self.s)
        if call:
            return meet_types(t, call)
    return meet_types(t.fallback, s)

</t>
<t tx="ekr.20221004064035.1890">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    # Ignore if __name__ == '__main__'.
    expr = o.expr[0]
    if (
        isinstance(expr, ComparisonExpr)
        and isinstance(expr.operands[0], NameExpr)
        and isinstance(expr.operands[1], StrExpr)
        and expr.operands[0].name == "__name__"
        and "__main__" in expr.operands[1].value
    ):
        return
    super().visit_if_stmt(o)

</t>
<t tx="ekr.20221004064035.1891">def visit_import_all(self, o: ImportAll) -&gt; None:
    self.add_import_line(f"from {'.' * o.relative}{o.id} import *\n")

</t>
<t tx="ekr.20221004064035.1892">def visit_import_from(self, o: ImportFrom) -&gt; None:
    exported_names: set[str] = set()
    import_names = []
    module, relative = translate_module_name(o.id, o.relative)
    if self.module:
        full_module, ok = mypy.util.correct_relative_import(
            self.module, relative, module, self.path.endswith(".__init__.py")
        )
        if not ok:
            full_module = module
    else:
        full_module = module
    if module == "__future__":
        return  # Not preserved
    for name, as_name in o.names:
        if name == "six":
            # Vendored six -- translate into plain 'import six'.
            self.visit_import(Import([("six", None)]))
            continue
        exported = False
        if as_name is None and self.module and (self.module + "." + name) in EXTRA_EXPORTED:
            # Special case certain names that should be exported, against our general rules.
            exported = True
        is_private = self.is_private_name(name, full_module + "." + name)
        if (
            as_name is None
            and name not in self.referenced_names
            and (not self._all_ or name in IGNORED_DUNDERS)
            and not is_private
            and module not in ("abc", "asyncio") + TYPING_MODULE_NAMES
        ):
            # An imported name that is never referenced in the module is assumed to be
            # exported, unless there is an explicit __all__. Note that we need to special
            # case 'abc' since some references are deleted during semantic analysis.
            exported = True
        top_level = full_module.split(".")[0]
        if (
            as_name is None
            and not self.export_less
            and (not self._all_ or name in IGNORED_DUNDERS)
            and self.module
            and not is_private
            and top_level in (self.module.split(".")[0], "_" + self.module.split(".")[0])
        ):
            # Export imports from the same package, since we can't reliably tell whether they
            # are part of the public API.
            exported = True
        if exported:
            self.import_tracker.reexport(name)
            as_name = name
        import_names.append((name, as_name))
    self.import_tracker.add_import_from("." * relative + module, import_names)
    self._vars[-1].extend(alias or name for name, alias in import_names)
    for name, alias in import_names:
        self.record_name(alias or name)

    if self._all_:
        # Include "import from"s that import names defined in __all__.
        names = [
            name
            for name, alias in o.names
            if name in self._all_ and alias is None and name not in IGNORED_DUNDERS
        ]
        exported_names.update(names)

</t>
<t tx="ekr.20221004064035.1893">def visit_import(self, o: Import) -&gt; None:
    for id, as_id in o.ids:
        self.import_tracker.add_import(id, as_id)
        if as_id is None:
            target_name = id.split(".")[0]
        else:
            target_name = as_id
        self._vars[-1].append(target_name)
        self.record_name(target_name)

</t>
<t tx="ekr.20221004064035.1894">def get_init(
    self, lvalue: str, rvalue: Expression, annotation: Type | None = None
) -&gt; str | None:
    """Return initializer for a variable.

    Return None if we've generated one already or if the variable is internal.
    """
    if lvalue in self._vars[-1]:
        # We've generated an initializer already for this variable.
        return None
    # TODO: Only do this at module top level.
    if self.is_private_name(lvalue) or self.is_not_in_all(lvalue):
        return None
    self._vars[-1].append(lvalue)
    if annotation is not None:
        typename = self.print_annotation(annotation)
        if (
            isinstance(annotation, UnboundType)
            and not annotation.args
            and annotation.name == "Final"
            and self.import_tracker.module_for.get("Final") in TYPING_MODULE_NAMES
        ):
            # Final without type argument is invalid in stubs.
            final_arg = self.get_str_type_of_node(rvalue)
            typename += f"[{final_arg}]"
    else:
        typename = self.get_str_type_of_node(rvalue)
    return f"{self._indent}{lvalue}: {typename}\n"

</t>
<t tx="ekr.20221004064035.1895">def add(self, string: str) -&gt; None:
    """Add text to generated stub."""
    self._output.append(string)

</t>
<t tx="ekr.20221004064035.1896">def add_decorator(self, name: str) -&gt; None:
    if not self._indent and self._state not in (EMPTY, FUNC):
        self._decorators.append("\n")
    self._decorators.append(f"{self._indent}@{name}\n")

</t>
<t tx="ekr.20221004064035.1897">def clear_decorators(self) -&gt; None:
    self._decorators.clear()

</t>
<t tx="ekr.20221004064035.1898">def typing_name(self, name: str) -&gt; str:
    if name in self.defined_names:
        # Avoid name clash between name from typing and a name defined in stub.
        return "_" + name
    else:
        return name

</t>
<t tx="ekr.20221004064035.1899">def add_typing_import(self, name: str) -&gt; None:
    """Add a name to be imported from typing, unless it's imported already.

    The import will be internal to the stub.
    """
    name = self.typing_name(name)
    self.import_tracker.require_name(name)

</t>
<t tx="ekr.20221004064035.19">def __init__(self) -&gt; None:
    self.seen_instances: list[tuple[Instance, Instance]] = []

</t>
<t tx="ekr.20221004064035.190">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    if isinstance(self.s, TupleType) and self.s.length() == t.length():
        items: list[Type] = []
        for i in range(t.length()):
            items.append(self.meet(t.items[i], self.s.items[i]))
        # TODO: What if the fallbacks are different?
        return TupleType(items, tuple_fallback(t))
    elif isinstance(self.s, Instance):
        # meet(Tuple[t1, t2, &lt;...&gt;], Tuple[s, ...]) == Tuple[meet(t1, s), meet(t2, s), &lt;...&gt;].
        if self.s.type.fullname == "builtins.tuple" and self.s.args:
            return t.copy_modified(items=[meet_types(it, self.s.args[0]) for it in t.items])
        elif is_proper_subtype(t, self.s):
            # A named tuple that inherits from a normal class
            return t
    return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1900">def add_abc_import(self, name: str) -&gt; None:
    """Add a name to be imported from collections.abc, unless it's imported already.

    The import will be internal to the stub.
    """
    name = self.typing_name(name)
    self.import_tracker.require_name(name)

</t>
<t tx="ekr.20221004064035.1901">def add_import_line(self, line: str) -&gt; None:
    """Add a line of text to the import section, unless it's already there."""
    if line not in self._import_lines:
        self._import_lines.append(line)

</t>
<t tx="ekr.20221004064035.1902">def add_coroutine_decorator(self, func: FuncDef, name: str, require_name: str) -&gt; None:
    func.is_awaitable_coroutine = True
    self.add_decorator(name)
    self.import_tracker.require_name(require_name)

</t>
<t tx="ekr.20221004064035.1903">def output(self) -&gt; str:
    """Return the text for the stub."""
    imports = ""
    if self._import_lines:
        imports += "".join(self._import_lines)
    imports += "".join(self.import_tracker.import_lines())
    if imports and self._output:
        imports += "\n"
    return imports + "".join(self._output)

</t>
<t tx="ekr.20221004064035.1904">def is_not_in_all(self, name: str) -&gt; bool:
    if self.is_private_name(name):
        return False
    if self._all_:
        return self.is_top_level() and name not in self._all_
    return False

</t>
<t tx="ekr.20221004064035.1905">def is_private_name(self, name: str, fullname: str | None = None) -&gt; bool:
    if self._include_private:
        return False
    if fullname in EXTRA_EXPORTED:
        return False
    return name.startswith("_") and (not name.endswith("__") or name in IGNORED_DUNDERS)

</t>
<t tx="ekr.20221004064035.1906">def is_private_member(self, fullname: str) -&gt; bool:
    parts = fullname.split(".")
    for part in parts:
        if self.is_private_name(part):
            return True
    return False

</t>
<t tx="ekr.20221004064035.1907">def get_str_type_of_node(
    self, rvalue: Expression, can_infer_optional: bool = False, can_be_any: bool = True
) -&gt; str:
    if isinstance(rvalue, IntExpr):
        return "int"
    if isinstance(rvalue, StrExpr):
        return "str"
    if isinstance(rvalue, BytesExpr):
        return "bytes"
    if isinstance(rvalue, FloatExpr):
        return "float"
    if isinstance(rvalue, UnaryExpr) and isinstance(rvalue.expr, IntExpr):
        return "int"
    if isinstance(rvalue, NameExpr) and rvalue.name in ("True", "False"):
        return "bool"
    if can_infer_optional and isinstance(rvalue, NameExpr) and rvalue.name == "None":
        self.add_typing_import("Incomplete")
        return f"{self.typing_name('Incomplete')} | None"
    if can_be_any:
        self.add_typing_import("Incomplete")
        return self.typing_name("Incomplete")
    else:
        return ""

</t>
<t tx="ekr.20221004064035.1908">def print_annotation(self, t: Type) -&gt; str:
    printer = AnnotationPrinter(self)
    return t.accept(printer)

</t>
<t tx="ekr.20221004064035.1909">def is_top_level(self) -&gt; bool:
    """Are we processing the top level of a file?"""
    return self._indent == ""

</t>
<t tx="ekr.20221004064035.191">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    if isinstance(self.s, TypedDictType):
        for (name, l, r) in self.s.zip(t):
            if not is_equivalent(l, r) or (name in t.required_keys) != (
                name in self.s.required_keys
            ):
                return self.default(self.s)
        item_list: list[tuple[str, Type]] = []
        for (item_name, s_item_type, t_item_type) in self.s.zipall(t):
            if s_item_type is not None:
                item_list.append((item_name, s_item_type))
            else:
                # at least one of s_item_type and t_item_type is not None
                assert t_item_type is not None
                item_list.append((item_name, t_item_type))
        items = dict(item_list)
        fallback = self.s.create_anonymous_fallback()
        required_keys = t.required_keys | self.s.required_keys
        return TypedDictType(items, required_keys, fallback)
    elif isinstance(self.s, Instance) and is_subtype(t, self.s):
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1910">def record_name(self, name: str) -&gt; None:
    """Mark a name as defined.

    This only does anything if at the top level of a module.
    """
    if self.is_top_level():
        self._toplevel_names.append(name)

</t>
<t tx="ekr.20221004064035.1911">def is_recorded_name(self, name: str) -&gt; bool:
    """Has this name been recorded previously?"""
    return self.is_top_level() and name in self._toplevel_names


</t>
<t tx="ekr.20221004064035.1912">def find_method_names(defs: list[Statement]) -&gt; set[str]:
    # TODO: Traverse into nested definitions
    result = set()
    for defn in defs:
        if isinstance(defn, FuncDef):
            result.add(defn.name)
        elif isinstance(defn, Decorator):
            result.add(defn.func.name)
        elif isinstance(defn, OverloadedFuncDef):
            for item in defn.items:
                result.update(find_method_names([item]))
    return result


</t>
<t tx="ekr.20221004064035.1913">class SelfTraverser(mypy.traverser.TraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.1914">def __init__(self) -&gt; None:
    self.results: list[tuple[str, Expression]] = []

</t>
<t tx="ekr.20221004064035.1915">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    lvalue = o.lvalues[0]
    if (
        isinstance(lvalue, MemberExpr)
        and isinstance(lvalue.expr, NameExpr)
        and lvalue.expr.name == "self"
    ):
        self.results.append((lvalue.name, o.rvalue))


</t>
<t tx="ekr.20221004064035.1916">def find_self_initializers(fdef: FuncBase) -&gt; list[tuple[str, Expression]]:
    """Find attribute initializers in a method.

    Return a list of pairs (attribute name, r.h.s. expression).
    """
    traverser = SelfTraverser()
    fdef.accept(traverser)
    return traverser.results


</t>
<t tx="ekr.20221004064035.1917">def get_qualified_name(o: Expression) -&gt; str:
    if isinstance(o, NameExpr):
        return o.name
    elif isinstance(o, MemberExpr):
        return f"{get_qualified_name(o.expr)}.{o.name}"
    else:
        return ERROR_MARKER


</t>
<t tx="ekr.20221004064035.1918">def remove_blacklisted_modules(modules: list[StubSource]) -&gt; list[StubSource]:
    return [
        module for module in modules if module.path is None or not is_blacklisted_path(module.path)
    ]


</t>
<t tx="ekr.20221004064035.1919">def is_blacklisted_path(path: str) -&gt; bool:
    return any(substr in (normalize_path_separators(path) + "\n") for substr in BLACKLIST)


</t>
<t tx="ekr.20221004064035.192">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    if isinstance(self.s, LiteralType) and self.s == t:
        return t
    elif isinstance(self.s, Instance) and is_subtype(t.fallback, self.s):
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1920">def normalize_path_separators(path: str) -&gt; str:
    if sys.platform == "win32":
        return path.replace("\\", "/")
    return path


</t>
<t tx="ekr.20221004064035.1921">def collect_build_targets(
    options: Options, mypy_opts: MypyOptions
) -&gt; tuple[list[StubSource], list[StubSource]]:
    """Collect files for which we need to generate stubs.

    Return list of Python modules and C modules.
    """
    if options.packages or options.modules:
        if options.no_import:
            py_modules = find_module_paths_using_search(
                options.modules, options.packages, options.search_path, options.pyversion
            )
            c_modules: list[StubSource] = []
        else:
            # Using imports is the default, since we can also find C modules.
            py_modules, c_modules = find_module_paths_using_imports(
                options.modules, options.packages, options.verbose, options.quiet
            )
    else:
        # Use mypy native source collection for files and directories.
        try:
            source_list = create_source_list(options.files, mypy_opts)
        except InvalidSourceList as e:
            raise SystemExit(str(e)) from e
        py_modules = [StubSource(m.module, m.path) for m in source_list]
        c_modules = []

    py_modules = remove_blacklisted_modules(py_modules)

    return py_modules, c_modules


</t>
<t tx="ekr.20221004064035.1922">def find_module_paths_using_imports(
    modules: list[str], packages: list[str], verbose: bool, quiet: bool
) -&gt; tuple[list[StubSource], list[StubSource]]:
    """Find path and runtime value of __all__ (if possible) for modules and packages.

    This function uses runtime Python imports to get the information.
    """
    with ModuleInspect() as inspect:
        py_modules: list[StubSource] = []
        c_modules: list[StubSource] = []
        found = list(walk_packages(inspect, packages, verbose))
        modules = modules + found
        modules = [
            mod for mod in modules if not is_non_library_module(mod)
        ]  # We don't want to run any tests or scripts
        for mod in modules:
            try:
                result = find_module_path_and_all_py3(inspect, mod, verbose)
            except CantImport as e:
                tb = traceback.format_exc()
                if verbose:
                    sys.stdout.write(tb)
                if not quiet:
                    report_missing(mod, e.message, tb)
                continue
            if not result:
                c_modules.append(StubSource(mod))
            else:
                path, runtime_all = result
                py_modules.append(StubSource(mod, path, runtime_all))
        return py_modules, c_modules


</t>
<t tx="ekr.20221004064035.1923">def is_non_library_module(module: str) -&gt; bool:
    """Does module look like a test module or a script?"""
    if module.endswith(
        (
            ".tests",
            ".test",
            ".testing",
            "_tests",
            "_test_suite",
            "test_util",
            "test_utils",
            "test_base",
            ".__main__",
            ".conftest",  # Used by pytest
            ".setup",  # Typically an install script
        )
    ):
        return True
    if module.split(".")[-1].startswith("test_"):
        return True
    if (
        ".tests." in module
        or ".test." in module
        or ".testing." in module
        or ".SelfTest." in module
    ):
        return True
    return False


</t>
<t tx="ekr.20221004064035.1924">def translate_module_name(module: str, relative: int) -&gt; tuple[str, int]:
    for pkg in VENDOR_PACKAGES:
        for alt in "six.moves", "six":
            substr = f"{pkg}.{alt}"
            if module.endswith("." + substr) or (module == substr and relative):
                return alt, 0
            if "." + substr + "." in module:
                return alt + "." + module.partition("." + substr + ".")[2], 0
    return module, relative


</t>
<t tx="ekr.20221004064035.1925">def find_module_paths_using_search(
    modules: list[str], packages: list[str], search_path: list[str], pyversion: tuple[int, int]
) -&gt; list[StubSource]:
    """Find sources for modules and packages requested.

    This function just looks for source files at the file system level.
    This is used if user passes --no-import, and will not find C modules.
    Exit if some of the modules or packages can't be found.
    """
    result: list[StubSource] = []
    typeshed_path = default_lib_path(mypy.build.default_data_dir(), pyversion, None)
    search_paths = SearchPaths((".",) + tuple(search_path), (), (), tuple(typeshed_path))
    cache = FindModuleCache(search_paths, fscache=None, options=None)
    for module in modules:
        m_result = cache.find_module(module)
        if isinstance(m_result, ModuleNotFoundReason):
            fail_missing(module, m_result)
            module_path = None
        else:
            module_path = m_result
        result.append(StubSource(module, module_path))
    for package in packages:
        p_result = cache.find_modules_recursive(package)
        if p_result:
            fail_missing(package, ModuleNotFoundReason.NOT_FOUND)
        sources = [StubSource(m.module, m.path) for m in p_result]
        result.extend(sources)

    result = [m for m in result if not is_non_library_module(m.module)]

    return result


</t>
<t tx="ekr.20221004064035.1926">def mypy_options(stubgen_options: Options) -&gt; MypyOptions:
    """Generate mypy options using the flag passed by user."""
    options = MypyOptions()
    options.follow_imports = "skip"
    options.incremental = False
    options.ignore_errors = True
    options.semantic_analysis_only = True
    options.python_version = stubgen_options.pyversion
    options.show_traceback = True
    options.transform_source = remove_misplaced_type_comments
    return options


</t>
<t tx="ekr.20221004064035.1927">def parse_source_file(mod: StubSource, mypy_options: MypyOptions) -&gt; None:
    """Parse a source file.

    On success, store AST in the corresponding attribute of the stub source.
    If there are syntax errors, print them and exit.
    """
    assert mod.path is not None, "Not found module was not skipped"
    with open(mod.path, "rb") as f:
        data = f.read()
    source = mypy.util.decode_python_encoding(data)
    errors = Errors()
    mod.ast = mypy.parse.parse(
        source, fnam=mod.path, module=mod.module, errors=errors, options=mypy_options
    )
    mod.ast._fullname = mod.module
    if errors.is_blockers():
        # Syntax error!
        for m in errors.new_messages():
            sys.stderr.write(f"{m}\n")
        sys.exit(1)


</t>
<t tx="ekr.20221004064035.1928">def generate_asts_for_modules(
    py_modules: list[StubSource], parse_only: bool, mypy_options: MypyOptions, verbose: bool
) -&gt; None:
    """Use mypy to parse (and optionally analyze) source files."""
    if not py_modules:
        return  # Nothing to do here, but there may be C modules
    if verbose:
        print(f"Processing {len(py_modules)} files...")
    if parse_only:
        for mod in py_modules:
            parse_source_file(mod, mypy_options)
        return
    # Perform full semantic analysis of the source set.
    try:
        res = build([module.source for module in py_modules], mypy_options)
    except CompileError as e:
        raise SystemExit(f"Critical error during semantic analysis: {e}") from e

    for mod in py_modules:
        mod.ast = res.graph[mod.module].tree
        # Use statically inferred __all__ if there is no runtime one.
        if mod.runtime_all is None:
            mod.runtime_all = res.manager.semantic_analyzer.export_map[mod.module]


</t>
<t tx="ekr.20221004064035.1929">def generate_stub_from_ast(
    mod: StubSource,
    target: str,
    parse_only: bool = False,
    include_private: bool = False,
    export_less: bool = False,
) -&gt; None:
    """Use analysed (or just parsed) AST to generate type stub for single file.

    If directory for target doesn't exist it will created. Existing stub
    will be overwritten.
    """
    gen = StubGenerator(
        mod.runtime_all,
        include_private=include_private,
        analyzed=not parse_only,
        export_less=export_less,
    )
    assert mod.ast is not None, "This function must be used only with analyzed modules"
    mod.ast.accept(gen)

    # Write output to file.
    subdir = os.path.dirname(target)
    if subdir and not os.path.isdir(subdir):
        os.makedirs(subdir)
    with open(target, "w") as file:
        file.write("".join(gen.output()))


</t>
<t tx="ekr.20221004064035.193">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # We can't determine the meet of partial types. We should never get here.
    assert False, "Internal error"

</t>
<t tx="ekr.20221004064035.1930">def get_sig_generators(options: Options) -&gt; List[SignatureGenerator]:
    sig_generators: List[SignatureGenerator] = [
        DocstringSignatureGenerator(),
        FallbackSignatureGenerator(),
    ]
    if options.doc_dir:
        # Collect info from docs (if given). Always check these first.
        sigs, class_sigs = collect_docs_signatures(options.doc_dir)
        sig_generators.insert(0, ExternalSignatureGenerator(sigs, class_sigs))
    return sig_generators


</t>
<t tx="ekr.20221004064035.1931">def collect_docs_signatures(doc_dir: str) -&gt; tuple[dict[str, str], dict[str, str]]:
    """Gather all function and class signatures in the docs.

    Return a tuple (function signatures, class signatures).
    Currently only used for C modules.
    """
    all_sigs: list[Sig] = []
    all_class_sigs: list[Sig] = []
    for path in glob.glob(f"{doc_dir}/*.rst"):
        with open(path) as f:
            loc_sigs, loc_class_sigs = parse_all_signatures(f.readlines())
        all_sigs += loc_sigs
        all_class_sigs += loc_class_sigs
    sigs = dict(find_unique_signatures(all_sigs))
    class_sigs = dict(find_unique_signatures(all_class_sigs))
    return sigs, class_sigs


</t>
<t tx="ekr.20221004064035.1932">def generate_stubs(options: Options) -&gt; None:
    """Main entry point for the program."""
    mypy_opts = mypy_options(options)
    py_modules, c_modules = collect_build_targets(options, mypy_opts)
    sig_generators = get_sig_generators(options)
    # Use parsed sources to generate stubs for Python modules.
    generate_asts_for_modules(py_modules, options.parse_only, mypy_opts, options.verbose)
    files = []
    for mod in py_modules:
        assert mod.path is not None, "Not found module was not skipped"
        target = mod.module.replace(".", "/")
        if os.path.basename(mod.path) == "__init__.py":
            target += "/__init__.pyi"
        else:
            target += ".pyi"
        target = os.path.join(options.output_dir, target)
        files.append(target)
        with generate_guarded(mod.module, target, options.ignore_errors, options.verbose):
            generate_stub_from_ast(
                mod, target, options.parse_only, options.include_private, options.export_less
            )

    # Separately analyse C modules using different logic.
    for mod in c_modules:
        if any(py_mod.module.startswith(mod.module + ".") for py_mod in py_modules + c_modules):
            target = mod.module.replace(".", "/") + "/__init__.pyi"
        else:
            target = mod.module.replace(".", "/") + ".pyi"
        target = os.path.join(options.output_dir, target)
        files.append(target)
        with generate_guarded(mod.module, target, options.ignore_errors, options.verbose):
            generate_stub_for_c_module(mod.module, target, sig_generators=sig_generators)
    num_modules = len(py_modules) + len(c_modules)
    if not options.quiet and num_modules &gt; 0:
        print("Processed %d modules" % num_modules)
        if len(files) == 1:
            print(f"Generated {files[0]}")
        else:
            print(f"Generated files under {common_dir_prefix(files)}" + os.sep)


</t>
<t tx="ekr.20221004064035.1933">HEADER = """%(prog)s [-h] [more options, see -h]
                     [-m MODULE] [-p PACKAGE] [files ...]"""

DESCRIPTION = """
Generate draft stubs for modules.

Stubs are generated in directory ./out, to avoid overriding files with
manual changes.  This directory is assumed to exist.
"""


</t>
<t tx="ekr.20221004064035.1934">def parse_options(args: list[str]) -&gt; Options:
    parser = argparse.ArgumentParser(prog="stubgen", usage=HEADER, description=DESCRIPTION)

    parser.add_argument(
        "--ignore-errors",
        action="store_true",
        help="ignore errors when trying to generate stubs for modules",
    )
    parser.add_argument(
        "--no-import",
        action="store_true",
        help="don't import the modules, just parse and analyze them "
        "(doesn't work with C extension modules and might not "
        "respect __all__)",
    )
    parser.add_argument(
        "--parse-only",
        action="store_true",
        help="don't perform semantic analysis of sources, just parse them "
        "(only applies to Python modules, might affect quality of stubs)",
    )
    parser.add_argument(
        "--include-private",
        action="store_true",
        help="generate stubs for objects and members considered private "
        "(single leading underscore and no trailing underscores)",
    )
    parser.add_argument(
        "--export-less",
        action="store_true",
        help="don't implicitly export all names imported from other modules in the same package",
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="show more verbose messages")
    parser.add_argument("-q", "--quiet", action="store_true", help="show fewer messages")
    parser.add_argument(
        "--doc-dir",
        metavar="PATH",
        default="",
        help="use .rst documentation in PATH (this may result in "
        "better stubs in some cases; consider setting this to "
        "DIR/Python-X.Y.Z/Doc/library)",
    )
    parser.add_argument(
        "--search-path",
        metavar="PATH",
        default="",
        help="specify module search directories, separated by ':' "
        "(currently only used if --no-import is given)",
    )
    parser.add_argument(
        "-o",
        "--output",
        metavar="PATH",
        dest="output_dir",
        default="out",
        help="change the output directory [default: %(default)s]",
    )
    parser.add_argument(
        "-m",
        "--module",
        action="append",
        metavar="MODULE",
        dest="modules",
        default=[],
        help="generate stub for module; can repeat for more modules",
    )
    parser.add_argument(
        "-p",
        "--package",
        action="append",
        metavar="PACKAGE",
        dest="packages",
        default=[],
        help="generate stubs for package recursively; can be repeated",
    )
    parser.add_argument(
        metavar="files",
        nargs="*",
        dest="files",
        help="generate stubs for given files or directories",
    )

    ns = parser.parse_args(args)

    pyversion = sys.version_info[:2]
    ns.interpreter = sys.executable

    if ns.modules + ns.packages and ns.files:
        parser.error("May only specify one of: modules/packages or files.")
    if ns.quiet and ns.verbose:
        parser.error("Cannot specify both quiet and verbose messages")

    # Create the output folder if it doesn't already exist.
    if not os.path.exists(ns.output_dir):
        os.makedirs(ns.output_dir)

    return Options(
        pyversion=pyversion,
        no_import=ns.no_import,
        doc_dir=ns.doc_dir,
        search_path=ns.search_path.split(":"),
        interpreter=ns.interpreter,
        ignore_errors=ns.ignore_errors,
        parse_only=ns.parse_only,
        include_private=ns.include_private,
        output_dir=ns.output_dir,
        modules=ns.modules,
        packages=ns.packages,
        files=ns.files,
        verbose=ns.verbose,
        quiet=ns.quiet,
        export_less=ns.export_less,
    )


</t>
<t tx="ekr.20221004064035.1935">def main(args: list[str] | None = None) -&gt; None:
    mypy.util.check_python_version("stubgen")
    # Make sure that the current directory is in sys.path so that
    # stubgen can be run on packages in the current directory.
    if not ("" in sys.path or "." in sys.path):
        sys.path.insert(0, "")

    options = parse_options(sys.argv[1:] if args is None else args)
    generate_stubs(options)


</t>
<t tx="ekr.20221004064035.1936">@path C:/Repos/ekr-mypy2/mypy/
#!/usr/bin/env python3
"""Stub generator for C modules.

The public interface is via the mypy.stubgen module.
"""

from __future__ import annotations

import importlib
import inspect
import os.path
import re
from abc import abstractmethod
from types import ModuleType
from typing import Any, Iterable, Mapping
from typing_extensions import Final

from mypy.moduleinspect import is_c_module
from mypy.stubdoc import (
    ArgSig,
    FunctionSig,
    infer_arg_sig_from_anon_docstring,
    infer_prop_type_from_docstring,
    infer_ret_type_sig_from_anon_docstring,
    infer_ret_type_sig_from_docstring,
    infer_sig_from_docstring,
)

# Members of the typing module to consider for importing by default.
_DEFAULT_TYPING_IMPORTS: Final = (
    "Any",
    "Callable",
    "ClassVar",
    "Dict",
    "Iterable",
    "Iterator",
    "List",
    "Optional",
    "Tuple",
    "Union",
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1937">class SignatureGenerator:
    """Abstract base class for extracting a list of FunctionSigs for each function."""

    @others
</t>
<t tx="ekr.20221004064035.1938">@abstractmethod
def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    pass

</t>
<t tx="ekr.20221004064035.1939">@abstractmethod
def get_method_sig(
    self, func: object, module_name: str, class_name: str, name: str, self_var: str
) -&gt; list[FunctionSig] | None:
    pass


</t>
<t tx="ekr.20221004064035.194">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    if isinstance(self.s, TypeType):
        typ = self.meet(t.item, self.s.item)
        if not isinstance(typ, NoneType):
            typ = TypeType.make_normalized(typ, line=t.line)
        return typ
    elif isinstance(self.s, Instance) and self.s.type.fullname == "builtins.type":
        return t
    elif isinstance(self.s, CallableType):
        return self.meet(t, self.s)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.1940">class ExternalSignatureGenerator(SignatureGenerator):
    @others
</t>
<t tx="ekr.20221004064035.1941">def __init__(
    self, func_sigs: dict[str, str] | None = None, class_sigs: dict[str, str] | None = None
):
    """
    Takes a mapping of function/method names to signatures and class name to
    class signatures (usually corresponds to __init__).
    """
    self.func_sigs = func_sigs or {}
    self.class_sigs = class_sigs or {}

</t>
<t tx="ekr.20221004064035.1942">def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    if name in self.func_sigs:
        return [
            FunctionSig(
                name=name,
                args=infer_arg_sig_from_anon_docstring(self.func_sigs[name]),
                ret_type="Any",
            )
        ]
    else:
        return None

</t>
<t tx="ekr.20221004064035.1943">def get_method_sig(
    self, func: object, module_name: str, class_name: str, name: str, self_var: str
) -&gt; list[FunctionSig] | None:
    if (
        name in ("__new__", "__init__")
        and name not in self.func_sigs
        and class_name in self.class_sigs
    ):
        return [
            FunctionSig(
                name=name,
                args=infer_arg_sig_from_anon_docstring(self.class_sigs[class_name]),
                ret_type="None" if name == "__init__" else "Any",
            )
        ]
    return self.get_function_sig(func, module_name, name)


</t>
<t tx="ekr.20221004064035.1944">class DocstringSignatureGenerator(SignatureGenerator):
    @others
</t>
<t tx="ekr.20221004064035.1945">def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    docstr = getattr(func, "__doc__", None)
    inferred = infer_sig_from_docstring(docstr, name)
    if inferred:
        assert docstr is not None
        if is_pybind11_overloaded_function_docstring(docstr, name):
            # Remove pybind11 umbrella (*args, **kwargs) for overloaded functions
            del inferred[-1]
    return inferred

</t>
<t tx="ekr.20221004064035.1946">def get_method_sig(
    self, func: object, module_name: str, class_name: str, name: str, self_var: str
) -&gt; list[FunctionSig] | None:
    return self.get_function_sig(func, module_name, name)


</t>
<t tx="ekr.20221004064035.1947">class FallbackSignatureGenerator(SignatureGenerator):
    @others
</t>
<t tx="ekr.20221004064035.1948">def get_function_sig(
    self, func: object, module_name: str, name: str
) -&gt; list[FunctionSig] | None:
    return [
        FunctionSig(
            name=name,
            args=infer_arg_sig_from_anon_docstring("(*args, **kwargs)"),
            ret_type="Any",
        )
    ]

</t>
<t tx="ekr.20221004064035.1949">def get_method_sig(
    self, func: object, module_name: str, class_name: str, name: str, self_var: str
) -&gt; list[FunctionSig] | None:
    return [
        FunctionSig(
            name=name,
            args=infer_method_sig(name, self_var),
            ret_type="None" if name == "__init__" else "Any",
        )
    ]


</t>
<t tx="ekr.20221004064035.195">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, f"This should be never called, got {t}"

</t>
<t tx="ekr.20221004064035.1950">def generate_stub_for_c_module(
    module_name: str, target: str, sig_generators: Iterable[SignatureGenerator]
) -&gt; None:
    """Generate stub for C module.

    Signature generators are called in order until a list of signatures is returned.  The order
    is:
    - signatures inferred from .rst documentation (if given)
    - simple runtime introspection (looking for docstrings and attributes
      with simple builtin types)
    - fallback based special method names or "(*args, **kwargs)"

    If directory for target doesn't exist it will be created. Existing stub
    will be overwritten.
    """
    module = importlib.import_module(module_name)
    assert is_c_module(module), f"{module_name} is not a C module"
    subdir = os.path.dirname(target)
    if subdir and not os.path.isdir(subdir):
        os.makedirs(subdir)
    imports: list[str] = []
    functions: list[str] = []
    done = set()
    items = sorted(module.__dict__.items(), key=lambda x: x[0])
    for name, obj in items:
        if is_c_function(obj):
            generate_c_function_stub(
                module, name, obj, functions, imports=imports, sig_generators=sig_generators
            )
            done.add(name)
    types: list[str] = []
    for name, obj in items:
        if name.startswith("__") and name.endswith("__"):
            continue
        if is_c_type(obj):
            generate_c_type_stub(
                module, name, obj, types, imports=imports, sig_generators=sig_generators
            )
            done.add(name)
    variables = []
    for name, obj in items:
        if name.startswith("__") and name.endswith("__"):
            continue
        if name not in done and not inspect.ismodule(obj):
            type_str = strip_or_import(get_type_fullname(type(obj)), module, imports)
            variables.append(f"{name}: {type_str}")
    output = sorted(set(imports))
    for line in variables:
        output.append(line)
    for line in types:
        if line.startswith("class") and output and output[-1]:
            output.append("")
        output.append(line)
    if output and functions:
        output.append("")
    for line in functions:
        output.append(line)
    output = add_typing_import(output)
    with open(target, "w") as file:
        for line in output:
            file.write(f"{line}\n")


</t>
<t tx="ekr.20221004064035.1951">def add_typing_import(output: list[str]) -&gt; list[str]:
    """Add typing imports for collections/types that occur in the generated stub."""
    names = []
    for name in _DEFAULT_TYPING_IMPORTS:
        if any(re.search(r"\b%s\b" % name, line) for line in output):
            names.append(name)
    if names:
        return [f"from typing import {', '.join(names)}", ""] + output
    else:
        return output[:]


</t>
<t tx="ekr.20221004064035.1952">def is_c_function(obj: object) -&gt; bool:
    return inspect.isbuiltin(obj) or type(obj) is type(ord)


</t>
<t tx="ekr.20221004064035.1953">def is_c_method(obj: object) -&gt; bool:
    return inspect.ismethoddescriptor(obj) or type(obj) in (
        type(str.index),
        type(str.__add__),
        type(str.__new__),
    )


</t>
<t tx="ekr.20221004064035.1954">def is_c_classmethod(obj: object) -&gt; bool:
    return inspect.isbuiltin(obj) or type(obj).__name__ in (
        "classmethod",
        "classmethod_descriptor",
    )


</t>
<t tx="ekr.20221004064035.1955">def is_c_property(obj: object) -&gt; bool:
    return inspect.isdatadescriptor(obj) or hasattr(obj, "fget")


</t>
<t tx="ekr.20221004064035.1956">def is_c_property_readonly(prop: Any) -&gt; bool:
    return hasattr(prop, "fset") and prop.fset is None


</t>
<t tx="ekr.20221004064035.1957">def is_c_type(obj: object) -&gt; bool:
    return inspect.isclass(obj) or type(obj) is type(int)


</t>
<t tx="ekr.20221004064035.1958">def is_pybind11_overloaded_function_docstring(docstr: str, name: str) -&gt; bool:
    return docstr.startswith(f"{name}(*args, **kwargs)\n" + "Overloaded function.\n\n")


</t>
<t tx="ekr.20221004064035.1959">def generate_c_function_stub(
    module: ModuleType,
    name: str,
    obj: object,
    output: list[str],
    imports: list[str],
    sig_generators: Iterable[SignatureGenerator],
    self_var: str | None = None,
    class_name: str | None = None,
) -&gt; None:
    """Generate stub for a single function or method.

    The result (always a single line) will be appended to 'output'.
    If necessary, any required names will be added to 'imports'.
    The 'class_name' is used to find signature of __init__ or __new__ in
    'class_sigs'.
    """
    inferred: list[FunctionSig] | None = None
    if class_name:
        # method:
        assert self_var is not None, "self_var should be provided for methods"
        for sig_gen in sig_generators:
            inferred = sig_gen.get_method_sig(obj, module.__name__, class_name, name, self_var)
            if inferred:
                # add self/cls var, if not present
                for sig in inferred:
                    if not sig.args or sig.args[0].name != self_var:
                        sig.args.insert(0, ArgSig(name=self_var))
                break
    else:
        # function:
        for sig_gen in sig_generators:
            inferred = sig_gen.get_function_sig(obj, module.__name__, name)
            if inferred:
                break

    if not inferred:
        raise ValueError(
            "No signature was found. This should never happen "
            "if FallbackSignatureGenerator is provided"
        )

    is_classmethod = self_var == "cls"
    is_overloaded = len(inferred) &gt; 1 if inferred else False
    if is_overloaded:
        imports.append("from typing import overload")
    if inferred:
        for signature in inferred:
            args: list[str] = []
            for arg in signature.args:
                if arg.name == self_var:
                    arg_def = self_var
                else:
                    arg_def = arg.name
                    if arg_def == "None":
                        arg_def = "_none"  # None is not a valid argument name

                    if arg.type:
                        arg_def += ": " + strip_or_import(arg.type, module, imports)

                    if arg.default:
                        arg_def += " = ..."

                args.append(arg_def)

            if is_overloaded:
                output.append("@overload")
            if is_classmethod:
                output.append("@classmethod")
            output.append(
                "def {function}({args}) -&gt; {ret}: ...".format(
                    function=name,
                    args=", ".join(args),
                    ret=strip_or_import(signature.ret_type, module, imports),
                )
            )


</t>
<t tx="ekr.20221004064035.196">def meet(self, s: Type, t: Type) -&gt; ProperType:
    return meet_types(s, t)

</t>
<t tx="ekr.20221004064035.1960">def strip_or_import(typ: str, module: ModuleType, imports: list[str]) -&gt; str:
    """Strips unnecessary module names from typ.

    If typ represents a type that is inside module or is a type coming from builtins, remove
    module declaration from it. Return stripped name of the type.

    Arguments:
        typ: name of the type
        module: in which this type is used
        imports: list of import statements (may be modified during the call)
    """
    stripped_type = typ
    if any(c in typ for c in "[,"):
        for subtyp in re.split(r"[\[,\]]", typ):
            strip_or_import(subtyp.strip(), module, imports)
        if module:
            stripped_type = re.sub(r"(^|[\[, ]+)" + re.escape(module.__name__ + "."), r"\1", typ)
    elif module and typ.startswith(module.__name__ + "."):
        stripped_type = typ[len(module.__name__) + 1 :]
    elif "." in typ:
        arg_module = typ[: typ.rindex(".")]
        if arg_module == "builtins":
            stripped_type = typ[len("builtins") + 1 :]
        else:
            imports.append(f"import {arg_module}")
    if stripped_type == "NoneType":
        stripped_type = "None"
    return stripped_type


</t>
<t tx="ekr.20221004064035.1961">def is_static_property(obj: object) -&gt; bool:
    return type(obj).__name__ == "pybind11_static_property"


</t>
<t tx="ekr.20221004064035.1962">def generate_c_property_stub(
    name: str,
    obj: object,
    static_properties: list[str],
    rw_properties: list[str],
    ro_properties: list[str],
    readonly: bool,
    module: ModuleType | None = None,
    imports: list[str] | None = None,
) -&gt; None:
    """Generate property stub using introspection of 'obj'.

    Try to infer type from docstring, append resulting lines to 'output'.
    """

    @others
    # Ignore special properties/attributes.
    if is_skipped_attribute(name):
        return

    inferred = infer_prop_type(getattr(obj, "__doc__", None))
    if not inferred:
        fget = getattr(obj, "fget", None)
        inferred = infer_prop_type(getattr(fget, "__doc__", None))
    if not inferred:
        inferred = "Any"

    if module is not None and imports is not None:
        inferred = strip_or_import(inferred, module, imports)

    if is_static_property(obj):
        trailing_comment = "  # read-only" if readonly else ""
        static_properties.append(f"{name}: ClassVar[{inferred}] = ...{trailing_comment}")
    else:  # regular property
        if readonly:
            ro_properties.append("@property")
            ro_properties.append(f"def {name}(self) -&gt; {inferred}: ...")
        else:
            rw_properties.append(f"{name}: {inferred}")


</t>
<t tx="ekr.20221004064035.1963">def infer_prop_type(docstr: str | None) -&gt; str | None:
    """Infer property type from docstring or docstring signature."""
    if docstr is not None:
        inferred = infer_ret_type_sig_from_anon_docstring(docstr)
        if not inferred:
            inferred = infer_ret_type_sig_from_docstring(docstr, name)
        if not inferred:
            inferred = infer_prop_type_from_docstring(docstr)
        return inferred
    else:
        return None

</t>
<t tx="ekr.20221004064035.1964">def generate_c_type_stub(
    module: ModuleType,
    class_name: str,
    obj: type,
    output: list[str],
    imports: list[str],
    sig_generators: Iterable[SignatureGenerator],
) -&gt; None:
    """Generate stub for a single class using runtime introspection.

    The result lines will be appended to 'output'. If necessary, any
    required names will be added to 'imports'.
    """
    # typeshed gives obj.__dict__ the not quite correct type Dict[str, Any]
    # (it could be a mappingproxy!), which makes mypyc mad, so obfuscate it.
    obj_dict: Mapping[str, Any] = getattr(obj, "__dict__")  # noqa: B009
    items = sorted(obj_dict.items(), key=lambda x: method_name_sort_key(x[0]))
    methods: list[str] = []
    types: list[str] = []
    static_properties: list[str] = []
    rw_properties: list[str] = []
    ro_properties: list[str] = []
    done: set[str] = set()
    for attr, value in items:
        if is_c_method(value) or is_c_classmethod(value):
            done.add(attr)
            if not is_skipped_attribute(attr):
                if attr == "__new__":
                    # TODO: We should support __new__.
                    if "__init__" in obj_dict:
                        # Avoid duplicate functions if both are present.
                        # But is there any case where .__new__() has a
                        # better signature than __init__() ?
                        continue
                    attr = "__init__"
                if is_c_classmethod(value):
                    self_var = "cls"
                else:
                    self_var = "self"
                generate_c_function_stub(
                    module,
                    attr,
                    value,
                    methods,
                    imports=imports,
                    self_var=self_var,
                    class_name=class_name,
                    sig_generators=sig_generators,
                )
        elif is_c_property(value):
            done.add(attr)
            generate_c_property_stub(
                attr,
                value,
                static_properties,
                rw_properties,
                ro_properties,
                is_c_property_readonly(value),
                module=module,
                imports=imports,
            )
        elif is_c_type(value):
            generate_c_type_stub(
                module, attr, value, types, imports=imports, sig_generators=sig_generators
            )
            done.add(attr)

    for attr, value in items:
        if is_skipped_attribute(attr):
            continue
        if attr not in done:
            static_properties.append(
                "{}: ClassVar[{}] = ...".format(
                    attr, strip_or_import(get_type_fullname(type(value)), module, imports)
                )
            )
    all_bases = type.mro(obj)
    if all_bases[-1] is object:
        # TODO: Is this always object?
        del all_bases[-1]
    # remove pybind11_object. All classes generated by pybind11 have pybind11_object in their MRO,
    # which only overrides a few functions in object type
    if all_bases and all_bases[-1].__name__ == "pybind11_object":
        del all_bases[-1]
    # remove the class itself
    all_bases = all_bases[1:]
    # Remove base classes of other bases as redundant.
    bases: list[type] = []
    for base in all_bases:
        if not any(issubclass(b, base) for b in bases):
            bases.append(base)
    if bases:
        bases_str = "(%s)" % ", ".join(
            strip_or_import(get_type_fullname(base), module, imports) for base in bases
        )
    else:
        bases_str = ""
    if types or static_properties or rw_properties or methods or ro_properties:
        output.append(f"class {class_name}{bases_str}:")
        for line in types:
            if (
                output
                and output[-1]
                and not output[-1].startswith("class")
                and line.startswith("class")
            ):
                output.append("")
            output.append("    " + line)
        for line in static_properties:
            output.append(f"    {line}")
        for line in rw_properties:
            output.append(f"    {line}")
        for line in methods:
            output.append(f"    {line}")
        for line in ro_properties:
            output.append(f"    {line}")
    else:
        output.append(f"class {class_name}{bases_str}: ...")


</t>
<t tx="ekr.20221004064035.1965">def get_type_fullname(typ: type) -&gt; str:
    return f"{typ.__module__}.{getattr(typ, '__qualname__', typ.__name__)}"


</t>
<t tx="ekr.20221004064035.1966">def method_name_sort_key(name: str) -&gt; tuple[int, str]:
    """Sort methods in classes in a typical order.

    I.e.: constructor, normal methods, special methods.
    """
    if name in ("__new__", "__init__"):
        return 0, name
    if name.startswith("__") and name.endswith("__"):
        return 2, name
    return 1, name


</t>
<t tx="ekr.20221004064035.1967">def is_pybind_skipped_attribute(attr: str) -&gt; bool:
    return attr.startswith("__pybind11_module_local_")


</t>
<t tx="ekr.20221004064035.1968">def is_skipped_attribute(attr: str) -&gt; bool:
    return attr in (
        "__getattribute__",
        "__str__",
        "__repr__",
        "__doc__",
        "__dict__",
        "__module__",
        "__weakref__",
    ) or is_pybind_skipped_attribute(  # For pickling
        attr
    )


</t>
<t tx="ekr.20221004064035.1969">def infer_method_sig(name: str, self_var: str | None = None) -&gt; list[ArgSig]:
    args: list[ArgSig] | None = None
    if name.startswith("__") and name.endswith("__"):
        name = name[2:-2]
        if name in (
            "hash",
            "iter",
            "next",
            "sizeof",
            "copy",
            "deepcopy",
            "reduce",
            "getinitargs",
            "int",
            "float",
            "trunc",
            "complex",
            "bool",
            "abs",
            "bytes",
            "dir",
            "len",
            "reversed",
            "round",
            "index",
            "enter",
        ):
            args = []
        elif name == "getitem":
            args = [ArgSig(name="index")]
        elif name == "setitem":
            args = [ArgSig(name="index"), ArgSig(name="object")]
        elif name in ("delattr", "getattr"):
            args = [ArgSig(name="name")]
        elif name == "setattr":
            args = [ArgSig(name="name"), ArgSig(name="value")]
        elif name == "getstate":
            args = []
        elif name == "setstate":
            args = [ArgSig(name="state")]
        elif name in (
            "eq",
            "ne",
            "lt",
            "le",
            "gt",
            "ge",
            "add",
            "radd",
            "sub",
            "rsub",
            "mul",
            "rmul",
            "mod",
            "rmod",
            "floordiv",
            "rfloordiv",
            "truediv",
            "rtruediv",
            "divmod",
            "rdivmod",
            "pow",
            "rpow",
            "xor",
            "rxor",
            "or",
            "ror",
            "and",
            "rand",
            "lshift",
            "rlshift",
            "rshift",
            "rrshift",
            "contains",
            "delitem",
            "iadd",
            "iand",
            "ifloordiv",
            "ilshift",
            "imod",
            "imul",
            "ior",
            "ipow",
            "irshift",
            "isub",
            "itruediv",
            "ixor",
        ):
            args = [ArgSig(name="other")]
        elif name in ("neg", "pos", "invert"):
            args = []
        elif name == "get":
            args = [ArgSig(name="instance"), ArgSig(name="owner")]
        elif name == "set":
            args = [ArgSig(name="instance"), ArgSig(name="value")]
        elif name == "reduce_ex":
            args = [ArgSig(name="protocol")]
        elif name == "exit":
            args = [ArgSig(name="type"), ArgSig(name="value"), ArgSig(name="traceback")]
    if args is None:
        args = [ArgSig(name="*args"), ArgSig(name="**kwargs")]
    return [ArgSig(name=self_var or "self")] + args
</t>
<t tx="ekr.20221004064035.197">def default(self, typ: Type) -&gt; ProperType:
    if isinstance(typ, UnboundType):
        return AnyType(TypeOfAny.special_form)
    else:
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()


</t>
<t tx="ekr.20221004064035.1970">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations


@others
# Stubs for these third-party packages used to be shipped with mypy.
#
# Map package name to PyPI stub distribution name.
#
# Package name can have one or two components ('a' or 'a.b').
legacy_bundled_packages = {
    "aiofiles": "types-aiofiles",
    "atomicwrites": "types-atomicwrites",
    "attr": "types-attrs",
    "backports": "types-backports",
    "backports_abc": "types-backports_abc",
    "bleach": "types-bleach",
    "boto": "types-boto",
    "cachetools": "types-cachetools",
    "chardet": "types-chardet",
    "click_spinner": "types-click-spinner",
    "contextvars": "types-contextvars",
    "croniter": "types-croniter",
    "dataclasses": "types-dataclasses",
    "dateparser": "types-dateparser",
    "datetimerange": "types-DateTimeRange",
    "dateutil": "types-python-dateutil",
    "decorator": "types-decorator",
    "deprecated": "types-Deprecated",
    "docutils": "types-docutils",
    "emoji": "types-emoji",
    "first": "types-first",
    "geoip2": "types-geoip2",
    "gflags": "types-python-gflags",
    "google.protobuf": "types-protobuf",
    "markdown": "types-Markdown",
    "maxminddb": "types-maxminddb",
    "mock": "types-mock",
    "OpenSSL": "types-pyOpenSSL",
    "paramiko": "types-paramiko",
    "pkg_resources": "types-setuptools",
    "polib": "types-polib",
    "pycurl": "types-pycurl",
    "pymysql": "types-PyMySQL",
    "pyrfc3339": "types-pyRFC3339",
    "python2": "types-six",
    "pytz": "types-pytz",
    "pyVmomi": "types-pyvmomi",
    "redis": "types-redis",
    "requests": "types-requests",
    "retry": "types-retry",
    "simplejson": "types-simplejson",
    "singledispatch": "types-singledispatch",
    "six": "types-six",
    "slugify": "types-python-slugify",
    "tabulate": "types-tabulate",
    "termcolor": "types-termcolor",
    "toml": "types-toml",
    "typed_ast": "types-typed-ast",
    "tzlocal": "types-tzlocal",
    "ujson": "types-ujson",
    "waitress": "types-waitress",
    "yaml": "types-PyYAML",
}

# Map package name to PyPI stub distribution name from typeshed.
# Stubs for these packages were never bundled with mypy. Don't
# include packages that have a release that includes PEP 561 type
# information.
#
# Package name can have one or two components ('a' or 'a.b').
#
# Note that these packages are omitted for now:
#   sqlalchemy: It's unclear which stub package to suggest. There's also
#               a mypy plugin available.
non_bundled_packages = {
    "MySQLdb": "types-mysqlclient",
    "PIL": "types-Pillow",
    "PyInstaller": "types-pyinstaller",
    "annoy": "types-annoy",
    "appdirs": "types-appdirs",
    "aws_xray_sdk": "types-aws-xray-sdk",
    "babel": "types-babel",
    "backports.ssl_match_hostname": "types-backports.ssl_match_hostname",
    "braintree": "types-braintree",
    "bs4": "types-beautifulsoup4",
    "bugbear": "types-flake8-bugbear",
    "caldav": "types-caldav",
    "cffi": "types-cffi",
    "chevron": "types-chevron",
    "colorama": "types-colorama",
    "commonmark": "types-commonmark",
    "cryptography": "types-cryptography",
    "d3dshot": "types-D3DShot",
    "dj_database_url": "types-dj-database-url",
    "docopt": "types-docopt",
    "editdistance": "types-editdistance",
    "entrypoints": "types-entrypoints",
    "farmhash": "types-pyfarmhash",
    "flake8_2020": "types-flake8-2020",
    "flake8_builtins": "types-flake8-builtins",
    "flake8_docstrings": "types-flake8-docstrings",
    "flake8_plugin_utils": "types-flake8-plugin-utils",
    "flake8_rst_docstrings": "types-flake8-rst-docstrings",
    "flake8_simplify": "types-flake8-simplify",
    "flake8_typing_imports": "types-flake8-typing-imports",
    "flask_cors": "types-Flask-Cors",
    "flask_sqlalchemy": "types-Flask-SQLAlchemy",
    "fpdf": "types-fpdf2",
    "gdb": "types-gdb",
    "google.cloud": "types-google-cloud-ndb",
    "hdbcli": "types-hdbcli",
    "html5lib": "types-html5lib",
    "httplib2": "types-httplib2",
    "humanfriendly": "types-humanfriendly",
    "invoke": "types-invoke",
    "jack": "types-JACK-Client",
    "jmespath": "types-jmespath",
    "jose": "types-python-jose",
    "jsonschema": "types-jsonschema",
    "keyboard": "types-keyboard",
    "ldap3": "types-ldap3",
    "nmap": "types-python-nmap",
    "oauthlib": "types-oauthlib",
    "openpyxl": "types-openpyxl",
    "opentracing": "types-opentracing",
    "parsimonious": "types-parsimonious",
    "passlib": "types-passlib",
    "passpy": "types-passpy",
    "pep8ext_naming": "types-pep8-naming",
    "playsound": "types-playsound",
    "prettytable": "types-prettytable",
    "psutil": "types-psutil",
    "psycopg2": "types-psycopg2",
    "pyaudio": "types-pyaudio",
    "pyautogui": "types-PyAutoGUI",
    "pyflakes": "types-pyflakes",
    "pygments": "types-Pygments",
    "pyi_splash": "types-pyinstaller",
    "pynput": "types-pynput",
    "pysftp": "types-pysftp",
    "pytest_lazyfixture": "types-pytest-lazy-fixture",
    "regex": "types-regex",
    "send2trash": "types-Send2Trash",
    "slumber": "types-slumber",
    "stdlib_list": "types-stdlib-list",
    "stripe": "types-stripe",
    "toposort": "types-toposort",
    "tqdm": "types-tqdm",
    "tree_sitter": "types-tree-sitter",
    "tree_sitter_languages": "types-tree-sitter-languages",
    "ttkthemes": "types-ttkthemes",
    "urllib3": "types-urllib3",
    "vobject": "types-vobject",
    "whatthepatch": "types-whatthepatch",
    "xmltodict": "types-xmltodict",
    "xxhash": "types-xxhash",
    "zxcvbn": "types-zxcvbn",
}
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1971">def is_legacy_bundled_package(prefix: str) -&gt; bool:
    return prefix in legacy_bundled_packages


</t>
<t tx="ekr.20221004064035.1972">def approved_stub_package_exists(prefix: str) -&gt; bool:
    return is_legacy_bundled_package(prefix) or prefix in non_bundled_packages


</t>
<t tx="ekr.20221004064035.1973">def stub_package_name(prefix: str) -&gt; str:
    return legacy_bundled_packages.get(prefix) or non_bundled_packages[prefix]


</t>
<t tx="ekr.20221004064035.1974">@path C:/Repos/ekr-mypy2/mypy/
"""Tests for stubs.

Verify that various things in stubs are consistent with how things behave at runtime.

"""

from __future__ import annotations

import argparse
import collections.abc
import copy
import enum
import importlib
import inspect
import os
import pkgutil
import re
import sys
import traceback
import types
import typing
import typing_extensions
import warnings
from contextlib import redirect_stderr, redirect_stdout
from functools import singledispatch
from pathlib import Path
from typing import Any, Generic, Iterator, TypeVar, Union, cast
from typing_extensions import get_origin

import mypy.build
import mypy.modulefinder
import mypy.state
import mypy.types
import mypy.version
from mypy import nodes
from mypy.config_parser import parse_config_file
from mypy.options import Options
from mypy.util import FancyFormatter, bytes_to_human_readable_repr, is_dunder, plural_s


@others
if __name__ == "__main__":
    sys.exit(main())
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.1975">class Missing:
    """Marker object for things that are missing (from a stub or the runtime)."""

    def __repr__(self) -&gt; str:
        return "MISSING"


</t>
<t tx="ekr.20221004064035.1976">MISSING: typing_extensions.Final = Missing()

T = TypeVar("T")
MaybeMissing: typing_extensions.TypeAlias = Union[T, Missing]

_formatter: typing_extensions.Final = FancyFormatter(sys.stdout, sys.stderr, False)


</t>
<t tx="ekr.20221004064035.1977">def _style(message: str, **kwargs: Any) -&gt; str:
    """Wrapper around mypy.util for fancy formatting."""
    kwargs.setdefault("color", "none")
    return _formatter.style(message, **kwargs)


</t>
<t tx="ekr.20221004064035.1978">def _truncate(message: str, length: int) -&gt; str:
    if len(message) &gt; length:
        return message[: length - 3] + "..."
    return message


</t>
<t tx="ekr.20221004064035.1979">class StubtestFailure(Exception):
    pass


</t>
<t tx="ekr.20221004064035.198">def meet_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    from mypy.join import join_types

    arg_types: list[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(join_types(t.arg_types[i], s.arg_types[i]))
    # TODO in combine_similar_callables also applies here (names and kinds)
    # The fallback type can be either 'function' or 'type'. The result should have 'function' as
    # fallback only if both operands have it as 'function'.
    if t.fallback.type.fullname != "builtins.function":
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(
        arg_types=arg_types,
        ret_type=meet_types(t.ret_type, s.ret_type),
        fallback=fallback,
        name=None,
    )


</t>
<t tx="ekr.20221004064035.1980">class Error:
    @others
</t>
<t tx="ekr.20221004064035.1981">def __init__(
    self,
    object_path: list[str],
    message: str,
    stub_object: MaybeMissing[nodes.Node],
    runtime_object: MaybeMissing[Any],
    *,
    stub_desc: str | None = None,
    runtime_desc: str | None = None,
) -&gt; None:
    """Represents an error found by stubtest.

    :param object_path: Location of the object with the error,
        e.g. ``["module", "Class", "method"]``
    :param message: Error message
    :param stub_object: The mypy node representing the stub
    :param runtime_object: Actual object obtained from the runtime
    :param stub_desc: Specialised description for the stub object, should you wish
    :param runtime_desc: Specialised description for the runtime object, should you wish

    """
    self.object_path = object_path
    self.object_desc = ".".join(object_path)
    self.message = message
    self.stub_object = stub_object
    self.runtime_object = runtime_object
    self.stub_desc = stub_desc or str(getattr(stub_object, "type", stub_object))
    self.runtime_desc = runtime_desc or _truncate(repr(runtime_object), 100)

</t>
<t tx="ekr.20221004064035.1982">def is_missing_stub(self) -&gt; bool:
    """Whether or not the error is for something missing from the stub."""
    return isinstance(self.stub_object, Missing)

</t>
<t tx="ekr.20221004064035.1983">def is_positional_only_related(self) -&gt; bool:
    """Whether or not the error is for something being (or not being) positional-only."""
    # TODO: This is hacky, use error codes or something more resilient
    return "leading double underscore" in self.message

</t>
<t tx="ekr.20221004064035.1984">def get_description(self, concise: bool = False) -&gt; str:
    """Returns a description of the error.

    :param concise: Whether to return a concise, one-line description

    """
    if concise:
        return _style(self.object_desc, bold=True) + " " + self.message

    stub_line = None
    stub_file = None
    if not isinstance(self.stub_object, Missing):
        stub_line = self.stub_object.line
    stub_node = get_stub(self.object_path[0])
    if stub_node is not None:
        stub_file = stub_node.path or None

    stub_loc_str = ""
    if stub_line:
        stub_loc_str += f" at line {stub_line}"
    if stub_file:
        stub_loc_str += f" in file {Path(stub_file)}"

    runtime_line = None
    runtime_file = None
    if not isinstance(self.runtime_object, Missing):
        try:
            runtime_line = inspect.getsourcelines(self.runtime_object)[1]
        except (OSError, TypeError):
            pass
        try:
            runtime_file = inspect.getsourcefile(self.runtime_object)
        except TypeError:
            pass

    runtime_loc_str = ""
    if runtime_line:
        runtime_loc_str += f" at line {runtime_line}"
    if runtime_file:
        runtime_loc_str += f" in file {Path(runtime_file)}"

    output = [
        _style("error: ", color="red", bold=True),
        _style(self.object_desc, bold=True),
        " ",
        self.message,
        "\n",
        "Stub:",
        _style(stub_loc_str, dim=True),
        "\n",
        _style(self.stub_desc + "\n", color="blue", dim=True),
        "Runtime:",
        _style(runtime_loc_str, dim=True),
        "\n",
        _style(self.runtime_desc + "\n", color="blue", dim=True),
    ]
    return "".join(output)


</t>
<t tx="ekr.20221004064035.1985"># ====================
# Core logic
# ====================


</t>
<t tx="ekr.20221004064035.1986">def silent_import_module(module_name: str) -&gt; types.ModuleType:
    with open(os.devnull, "w") as devnull:
        with warnings.catch_warnings(), redirect_stdout(devnull), redirect_stderr(devnull):
            warnings.simplefilter("ignore")
            runtime = importlib.import_module(module_name)
            # Also run the equivalent of `from module import *`
            # This could have the additional effect of loading not-yet-loaded submodules
            # mentioned in __all__
            __import__(module_name, fromlist=["*"])
    return runtime


</t>
<t tx="ekr.20221004064035.1987">def test_module(module_name: str) -&gt; Iterator[Error]:
    """Tests a given module's stub against introspecting it at runtime.

    Requires the stub to have been built already, accomplished by a call to ``build_stubs``.

    :param module_name: The module to test

    """
    stub = get_stub(module_name)
    if stub is None:
        if not is_probably_private(module_name.split(".")[-1]):
            runtime_desc = repr(sys.modules[module_name]) if module_name in sys.modules else "N/A"
            yield Error(
                [module_name], "failed to find stubs", MISSING, None, runtime_desc=runtime_desc
            )
        return

    try:
        runtime = silent_import_module(module_name)
    except Exception as e:
        yield Error([module_name], f"failed to import, {type(e).__name__}: {e}", stub, MISSING)
        return

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        try:
            yield from verify(stub, runtime, [module_name])
        except Exception as e:
            bottom_frame = list(traceback.walk_tb(e.__traceback__))[-1][0]
            bottom_module = bottom_frame.f_globals.get("__name__", "")
            # Pass on any errors originating from stubtest or mypy
            # These can occur expectedly, e.g. StubtestFailure
            if bottom_module == "__main__" or bottom_module.split(".")[0] == "mypy":
                raise
            yield Error(
                [module_name],
                f"encountered unexpected error, {type(e).__name__}: {e}",
                stub,
                runtime,
                stub_desc="N/A",
                runtime_desc=(
                    "This is most likely the fault of something very dynamic in your library. "
                    "It's also possible this is a bug in stubtest.\nIf in doubt, please "
                    "open an issue at https://github.com/python/mypy\n\n"
                    + traceback.format_exc().strip()
                ),
            )


</t>
<t tx="ekr.20221004064035.1988">@singledispatch
def verify(
    stub: MaybeMissing[nodes.Node], runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    """Entry point for comparing a stub to a runtime object.

    We use single dispatch based on the type of ``stub``.

    :param stub: The mypy node representing a part of the stub
    :param runtime: The runtime object corresponding to ``stub``

    """
    yield Error(object_path, "is an unknown mypy node", stub, runtime)


</t>
<t tx="ekr.20221004064035.1989">def _verify_exported_names(
    object_path: list[str], stub: nodes.MypyFile, runtime_all_as_set: set[str]
) -&gt; Iterator[Error]:
    # note that this includes the case the stub simply defines `__all__: list[str]`
    assert "__all__" in stub.names
    public_names_in_stub = {m for m, o in stub.names.items() if o.module_public}
    names_in_stub_not_runtime = sorted(public_names_in_stub - runtime_all_as_set)
    names_in_runtime_not_stub = sorted(runtime_all_as_set - public_names_in_stub)
    if not (names_in_runtime_not_stub or names_in_stub_not_runtime):
        return
    yield Error(
        object_path,
        (
            "names exported from the stub do not correspond to the names exported at runtime. "
            "This is probably due to an inaccurate `__all__` in the stub or things being missing from the stub."
        ),
        # Pass in MISSING instead of the stub and runtime objects, as the line numbers aren't very
        # relevant here, and it makes for a prettier error message
        # This means this error will be ignored when using `--ignore-missing-stub`, which is
        # desirable in at least the `names_in_runtime_not_stub` case
        stub_object=MISSING,
        runtime_object=MISSING,
        stub_desc=(
            f"Names exported in the stub but not at runtime: " f"{names_in_stub_not_runtime}"
        ),
        runtime_desc=(
            f"Names exported at runtime but not in the stub: " f"{names_in_runtime_not_stub}"
        ),
    )


</t>
<t tx="ekr.20221004064035.199">def meet_type_list(types: list[Type]) -&gt; Type:
    if not types:
        # This should probably be builtins.object but that is hard to get and
        # it doesn't matter for any current users.
        return AnyType(TypeOfAny.implementation_artifact)
    met = types[0]
    for t in types[1:]:
        met = meet_types(met, t)
    return met


</t>
<t tx="ekr.20221004064035.1990">@verify.register(nodes.MypyFile)
def verify_mypyfile(
    stub: nodes.MypyFile, runtime: MaybeMissing[types.ModuleType], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if not isinstance(runtime, types.ModuleType):
        yield Error(object_path, "is not a module", stub, runtime)
        return

    runtime_all_as_set: set[str] | None

    if hasattr(runtime, "__all__"):
        runtime_all_as_set = set(runtime.__all__)
        if "__all__" in stub.names:
            # Only verify the contents of the stub's __all__
            # if the stub actually defines __all__
            yield from _verify_exported_names(object_path, stub, runtime_all_as_set)
    else:
        runtime_all_as_set = None

    # Check things in the stub
    to_check = {
        m
        for m, o in stub.names.items()
        if not o.module_hidden and (not is_probably_private(m) or hasattr(runtime, m))
    }

    @others
    runtime_public_contents = (
        runtime_all_as_set
        if runtime_all_as_set is not None
        else {
            m
            for m in dir(runtime)
            if not is_probably_private(m)
            # Ensure that the object's module is `runtime`, since in the absence of __all__ we
            # don't have a good way to detect re-exports at runtime.
            and _belongs_to_runtime(runtime, m)
        }
    )
    # Check all things declared in module's __all__, falling back to our best guess
    to_check.update(runtime_public_contents)
    to_check.difference_update(IGNORED_MODULE_DUNDERS)

    for entry in sorted(to_check):
        stub_entry = stub.names[entry].node if entry in stub.names else MISSING
        if isinstance(stub_entry, nodes.MypyFile):
            # Don't recursively check exported modules, since that leads to infinite recursion
            continue
        assert stub_entry is not None
        try:
            runtime_entry = getattr(runtime, entry, MISSING)
        except Exception:
            # Catch all exceptions in case the runtime raises an unexpected exception
            # from __getattr__ or similar.
            continue
        yield from verify(stub_entry, runtime_entry, object_path + [entry])


</t>
<t tx="ekr.20221004064035.1991">def _belongs_to_runtime(r: types.ModuleType, attr: str) -&gt; bool:
    obj = getattr(r, attr)
    try:
        obj_mod = getattr(obj, "__module__", None)
    except Exception:
        return False
    if obj_mod is not None:
        return bool(obj_mod == r.__name__)
    return not isinstance(obj, types.ModuleType)

</t>
<t tx="ekr.20221004064035.1992">def _verify_final(
    stub: nodes.TypeInfo, runtime: type[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    try:

        class SubClass(runtime):  # type: ignore[misc,valid-type]
            pass

    except TypeError:
        # Enum classes are implicitly @final
        if not stub.is_final and not issubclass(runtime, enum.Enum):
            yield Error(
                object_path,
                "cannot be subclassed at runtime, but isn't marked with @final in the stub",
                stub,
                runtime,
                stub_desc=repr(stub),
            )
    except Exception:
        # The class probably wants its subclasses to do something special.
        # Examples: ctypes.Array, ctypes._SimpleCData
        pass


</t>
<t tx="ekr.20221004064035.1993">def _verify_metaclass(
    stub: nodes.TypeInfo, runtime: type[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    # We exclude protocols, because of how complex their implementation is in different versions of
    # python. Enums are also hard, ignoring.
    # TODO: check that metaclasses are identical?
    if not stub.is_protocol and not stub.is_enum:
        runtime_metaclass = type(runtime)
        if runtime_metaclass is not type and stub.metaclass_type is None:
            # This means that runtime has a custom metaclass, but a stub does not.
            yield Error(
                object_path,
                "is inconsistent, metaclass differs",
                stub,
                runtime,
                stub_desc="N/A",
                runtime_desc=f"{runtime_metaclass}",
            )
        elif (
            runtime_metaclass is type
            and stub.metaclass_type is not None
            # We ignore extra `ABCMeta` metaclass on stubs, this might be typing hack.
            # We also ignore `builtins.type` metaclass as an implementation detail in mypy.
            and not mypy.types.is_named_instance(
                stub.metaclass_type, ("abc.ABCMeta", "builtins.type")
            )
        ):
            # This means that our stub has a metaclass that is not present at runtime.
            yield Error(
                object_path,
                "metaclass mismatch",
                stub,
                runtime,
                stub_desc=f"{stub.metaclass_type.type.fullname}",
                runtime_desc="N/A",
            )


</t>
<t tx="ekr.20221004064035.1994">@verify.register(nodes.TypeInfo)
def verify_typeinfo(
    stub: nodes.TypeInfo, runtime: MaybeMissing[type[Any]], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime, stub_desc=repr(stub))
        return
    if not isinstance(runtime, type):
        yield Error(object_path, "is not a type", stub, runtime, stub_desc=repr(stub))
        return

    yield from _verify_final(stub, runtime, object_path)
    yield from _verify_metaclass(stub, runtime, object_path)

    # Check everything already defined on the stub class itself (i.e. not inherited)
    to_check = set(stub.names)
    # Check all public things on the runtime class
    to_check.update(
        # cast to workaround mypyc complaints
        m
        for m in cast(Any, vars)(runtime)
        if not is_probably_private(m) and m not in IGNORABLE_CLASS_DUNDERS
    )
    # Special-case the __init__ method for Protocols
    #
    # TODO: On Python &lt;3.11, __init__ methods on Protocol classes
    # are silently discarded and replaced.
    # However, this is not the case on Python 3.11+.
    # Ideally, we'd figure out a good way of validating Protocol __init__ methods on 3.11+.
    if stub.is_protocol:
        to_check.discard("__init__")

    for entry in sorted(to_check):
        mangled_entry = entry
        if entry.startswith("__") and not entry.endswith("__"):
            mangled_entry = f"_{stub.name}{entry}"
        stub_to_verify = next((t.names[entry].node for t in stub.mro if entry in t.names), MISSING)
        assert stub_to_verify is not None
        try:
            try:
                runtime_attr = getattr(runtime, mangled_entry)
            except AttributeError:
                runtime_attr = inspect.getattr_static(runtime, mangled_entry, MISSING)
        except Exception:
            # Catch all exceptions in case the runtime raises an unexpected exception
            # from __getattr__ or similar.
            continue
        # Do not error for an object missing from the stub
        # If the runtime object is a types.WrapperDescriptorType object
        # and has a non-special dunder name.
        # The vast majority of these are false positives.
        if not (
            isinstance(stub_to_verify, Missing)
            and isinstance(runtime_attr, types.WrapperDescriptorType)
            and is_dunder(mangled_entry, exclude_special=True)
        ):
            yield from verify(stub_to_verify, runtime_attr, object_path + [entry])


</t>
<t tx="ekr.20221004064035.1995">def _verify_static_class_methods(
    stub: nodes.FuncBase, runtime: Any, object_path: list[str]
) -&gt; Iterator[str]:
    if stub.name in ("__new__", "__init_subclass__", "__class_getitem__"):
        # Special cased by Python, so don't bother checking
        return
    if inspect.isbuiltin(runtime):
        # The isinstance checks don't work reliably for builtins, e.g. datetime.datetime.now, so do
        # something a little hacky that seems to work well
        probably_class_method = isinstance(getattr(runtime, "__self__", None), type)
        if probably_class_method and not stub.is_class:
            yield "runtime is a classmethod but stub is not"
        if not probably_class_method and stub.is_class:
            yield "stub is a classmethod but runtime is not"
        return

    # Look the object up statically, to avoid binding by the descriptor protocol
    static_runtime = importlib.import_module(object_path[0])
    for entry in object_path[1:]:
        try:
            static_runtime = inspect.getattr_static(static_runtime, entry)
        except AttributeError:
            # This can happen with mangled names, ignore for now.
            # TODO: pass more information about ancestors of nodes/objects to verify, so we don't
            # have to do this hacky lookup. Would be useful in a couple other places too.
            return

    if isinstance(static_runtime, classmethod) and not stub.is_class:
        yield "runtime is a classmethod but stub is not"
    if not isinstance(static_runtime, classmethod) and stub.is_class:
        yield "stub is a classmethod but runtime is not"
    if isinstance(static_runtime, staticmethod) and not stub.is_static:
        yield "runtime is a staticmethod but stub is not"
    if not isinstance(static_runtime, staticmethod) and stub.is_static:
        yield "stub is a staticmethod but runtime is not"


</t>
<t tx="ekr.20221004064035.1996">def _verify_arg_name(
    stub_arg: nodes.Argument, runtime_arg: inspect.Parameter, function_name: str
) -&gt; Iterator[str]:
    """Checks whether argument names match."""
    # Ignore exact names for most dunder methods
    if is_dunder(function_name, exclude_special=True):
        return

    @others
    # Be more permissive about names matching for positional-only arguments
    if runtime_arg.kind == inspect.Parameter.POSITIONAL_ONLY and names_approx_match(
        stub_arg.variable.name, runtime_arg.name
    ):
        return
    # This comes up with namedtuples, so ignore
    if stub_arg.variable.name == "_self":
        return
    yield (
        f'stub argument "{stub_arg.variable.name}" '
        f'differs from runtime argument "{runtime_arg.name}"'
    )


</t>
<t tx="ekr.20221004064035.1997">def strip_prefix(s: str, prefix: str) -&gt; str:
    return s[len(prefix) :] if s.startswith(prefix) else s

</t>
<t tx="ekr.20221004064035.1998">if strip_prefix(stub_arg.variable.name, "__") == runtime_arg.name:
    return

</t>
<t tx="ekr.20221004064035.1999">def names_approx_match(a: str, b: str) -&gt; bool:
    a = a.strip("_")
    b = b.strip("_")
    return a.startswith(b) or b.startswith(a) or len(a) == 1 or len(b) == 1

</t>
<t tx="ekr.20221004064035.2">class IPCBase:
    """Base class for communication between the dmypy client and server.

    This contains logic shared between the client and server, such as reading
    and writing.
    """

    connection: _IPCHandle

    @others
</t>
<t tx="ekr.20221004064035.20">def join_instances(self, t: Instance, s: Instance) -&gt; ProperType:
    if (t, s) in self.seen_instances or (s, t) in self.seen_instances:
        return object_from_instance(t)

    self.seen_instances.append((t, s))

    # Calculate the join of two instance types
    if t.type == s.type:
        # Simplest case: join two types with the same base type (but
        # potentially different arguments).

        # Combine type arguments.
        args: list[Type] = []
        # N.B: We use zip instead of indexing because the lengths might have
        # mismatches during daemon reprocessing.
        for ta, sa, type_var in zip(t.args, s.args, t.type.defn.type_vars):
            ta_proper = get_proper_type(ta)
            sa_proper = get_proper_type(sa)
            new_type: Type | None = None
            if isinstance(ta_proper, AnyType):
                new_type = AnyType(TypeOfAny.from_another_any, ta_proper)
            elif isinstance(sa_proper, AnyType):
                new_type = AnyType(TypeOfAny.from_another_any, sa_proper)
            elif isinstance(type_var, TypeVarType):
                if type_var.variance == COVARIANT:
                    new_type = join_types(ta, sa, self)
                    if len(type_var.values) != 0 and new_type not in type_var.values:
                        self.seen_instances.pop()
                        return object_from_instance(t)
                    if not is_subtype(new_type, type_var.upper_bound):
                        self.seen_instances.pop()
                        return object_from_instance(t)
                # TODO: contravariant case should use meet but pass seen instances as
                # an argument to keep track of recursive checks.
                elif type_var.variance in (INVARIANT, CONTRAVARIANT):
                    if not is_equivalent(ta, sa):
                        self.seen_instances.pop()
                        return object_from_instance(t)
                    # If the types are different but equivalent, then an Any is involved
                    # so using a join in the contravariant case is also OK.
                    new_type = join_types(ta, sa, self)
            else:
                # ParamSpec type variables behave the same, independent of variance
                if not is_equivalent(ta, sa):
                    return get_proper_type(type_var.upper_bound)
                new_type = join_types(ta, sa, self)
            assert new_type is not None
            args.append(new_type)
        result: ProperType = Instance(t.type, args)
    elif t.type.bases and is_subtype(t, s, ignore_type_params=True):
        result = self.join_instances_via_supertype(t, s)
    else:
        # Now t is not a subtype of s, and t != s. Now s could be a subtype
        # of t; alternatively, we need to find a common supertype. This works
        # in of the both cases.
        result = self.join_instances_via_supertype(s, t)

    self.seen_instances.pop()
    return result

</t>
<t tx="ekr.20221004064035.200">def typed_dict_mapping_pair(left: Type, right: Type) -&gt; bool:
    """Is this a pair where one type is a TypedDict and another one is an instance of Mapping?

    This case requires a precise/principled consideration because there are two use cases
    that push the boundary the opposite ways: we need to avoid spurious overlaps to avoid
    false positives for overloads, but we also need to avoid spuriously non-overlapping types
    to avoid false positives with --strict-equality.
    """
    left, right = get_proper_types((left, right))
    assert not isinstance(left, TypedDictType) or not isinstance(right, TypedDictType)

    if isinstance(left, TypedDictType):
        _, other = left, right
    elif isinstance(right, TypedDictType):
        _, other = right, left
    else:
        return False
    return isinstance(other, Instance) and other.type.has_base("typing.Mapping")


</t>
<t tx="ekr.20221004064035.2000">def _verify_arg_default_value(
    stub_arg: nodes.Argument, runtime_arg: inspect.Parameter
) -&gt; Iterator[str]:
    """Checks whether argument default values are compatible."""
    if runtime_arg.default != inspect.Parameter.empty:
        if stub_arg.kind.is_required():
            yield (
                f'runtime argument "{runtime_arg.name}" '
                "has a default value but stub argument does not"
            )
        else:
            runtime_type = get_mypy_type_of_runtime_value(runtime_arg.default)
            # Fallback to the type annotation type if var type is missing. The type annotation
            # is an UnboundType, but I don't know enough to know what the pros and cons here are.
            # UnboundTypes have ugly question marks following them, so default to var type.
            # Note we do this same fallback when constructing signatures in from_overloadedfuncdef
            stub_type = stub_arg.variable.type or stub_arg.type_annotation
            if isinstance(stub_type, mypy.types.TypeVarType):
                stub_type = stub_type.upper_bound
            if (
                runtime_type is not None
                and stub_type is not None
                # Avoid false positives for marker objects
                and type(runtime_arg.default) != object
                # And ellipsis
                and runtime_arg.default is not ...
                and not is_subtype_helper(runtime_type, stub_type)
            ):
                yield (
                    f'runtime argument "{runtime_arg.name}" '
                    f"has a default value of type {runtime_type}, "
                    f"which is incompatible with stub argument type {stub_type}"
                )
    else:
        if stub_arg.kind.is_optional():
            yield (
                f'stub argument "{stub_arg.variable.name}" has a default value '
                f"but runtime argument does not"
            )


</t>
<t tx="ekr.20221004064035.2001">def maybe_strip_cls(name: str, args: list[nodes.Argument]) -&gt; list[nodes.Argument]:
    if name in ("__init_subclass__", "__class_getitem__"):
        # These are implicitly classmethods. If the stub chooses not to have @classmethod, we
        # should remove the cls argument
        if args[0].variable.name == "cls":
            return args[1:]
    return args


</t>
<t tx="ekr.20221004064035.2002">class Signature(Generic[T]):
    @others
</t>
<t tx="ekr.20221004064035.2003">def __init__(self) -&gt; None:
    self.pos: list[T] = []
    self.kwonly: dict[str, T] = {}
    self.varpos: T | None = None
    self.varkw: T | None = None

</t>
<t tx="ekr.20221004064035.2004">def __str__(self) -&gt; str:
    def get_name(arg: Any) -&gt; str:
        if isinstance(arg, inspect.Parameter):
            return arg.name
        if isinstance(arg, nodes.Argument):
            return arg.variable.name
        raise AssertionError

    def get_type(arg: Any) -&gt; str | None:
        if isinstance(arg, inspect.Parameter):
            return None
        if isinstance(arg, nodes.Argument):
            return str(arg.variable.type or arg.type_annotation)
        raise AssertionError

    def has_default(arg: Any) -&gt; bool:
        if isinstance(arg, inspect.Parameter):
            return bool(arg.default != inspect.Parameter.empty)
        if isinstance(arg, nodes.Argument):
            return arg.kind.is_optional()
        raise AssertionError

    def get_desc(arg: Any) -&gt; str:
        arg_type = get_type(arg)
        return (
            get_name(arg)
            + (f": {arg_type}" if arg_type else "")
            + (" = ..." if has_default(arg) else "")
        )

    kw_only = sorted(self.kwonly.values(), key=lambda a: (has_default(a), get_name(a)))
    ret = "def ("
    ret += ", ".join(
        [get_desc(arg) for arg in self.pos]
        + (["*" + get_name(self.varpos)] if self.varpos else (["*"] if self.kwonly else []))
        + [get_desc(arg) for arg in kw_only]
        + (["**" + get_name(self.varkw)] if self.varkw else [])
    )
    ret += ")"
    return ret

</t>
<t tx="ekr.20221004064035.2005">@staticmethod
def from_funcitem(stub: nodes.FuncItem) -&gt; Signature[nodes.Argument]:
    stub_sig: Signature[nodes.Argument] = Signature()
    stub_args = maybe_strip_cls(stub.name, stub.arguments)
    for stub_arg in stub_args:
        if stub_arg.kind.is_positional():
            stub_sig.pos.append(stub_arg)
        elif stub_arg.kind.is_named():
            stub_sig.kwonly[stub_arg.variable.name] = stub_arg
        elif stub_arg.kind == nodes.ARG_STAR:
            stub_sig.varpos = stub_arg
        elif stub_arg.kind == nodes.ARG_STAR2:
            stub_sig.varkw = stub_arg
        else:
            raise AssertionError
    return stub_sig

</t>
<t tx="ekr.20221004064035.2006">@staticmethod
def from_inspect_signature(signature: inspect.Signature) -&gt; Signature[inspect.Parameter]:
    runtime_sig: Signature[inspect.Parameter] = Signature()
    for runtime_arg in signature.parameters.values():
        if runtime_arg.kind in (
            inspect.Parameter.POSITIONAL_ONLY,
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
        ):
            runtime_sig.pos.append(runtime_arg)
        elif runtime_arg.kind == inspect.Parameter.KEYWORD_ONLY:
            runtime_sig.kwonly[runtime_arg.name] = runtime_arg
        elif runtime_arg.kind == inspect.Parameter.VAR_POSITIONAL:
            runtime_sig.varpos = runtime_arg
        elif runtime_arg.kind == inspect.Parameter.VAR_KEYWORD:
            runtime_sig.varkw = runtime_arg
        else:
            raise AssertionError
    return runtime_sig

</t>
<t tx="ekr.20221004064035.2007">@staticmethod
def from_overloadedfuncdef(stub: nodes.OverloadedFuncDef) -&gt; Signature[nodes.Argument]:
    """Returns a Signature from an OverloadedFuncDef.

    If life were simple, to verify_overloadedfuncdef, we'd just verify_funcitem for each of its
    items. Unfortunately, life isn't simple and overloads are pretty deceitful. So instead, we
    try and combine the overload's items into a single signature that is compatible with any
    lies it might try to tell.

    """
    # For most dunder methods, just assume all args are positional-only
    assume_positional_only = is_dunder(stub.name, exclude_special=True)

    all_args: dict[str, list[tuple[nodes.Argument, int]]] = {}
    for func in map(_resolve_funcitem_from_decorator, stub.items):
        assert func is not None
        args = maybe_strip_cls(stub.name, func.arguments)
        for index, arg in enumerate(args):
            # For positional-only args, we allow overloads to have different names for the same
            # argument. To accomplish this, we just make up a fake index-based name.
            name = (
                f"__{index}"
                if arg.variable.name.startswith("__") or assume_positional_only
                else arg.variable.name
            )
            all_args.setdefault(name, []).append((arg, index))

    def get_position(arg_name: str) -&gt; int:
        # We just need this to return the positional args in the correct order.
        return max(index for _, index in all_args[arg_name])

    def get_type(arg_name: str) -&gt; mypy.types.ProperType:
        with mypy.state.state.strict_optional_set(True):
            all_types = [
                arg.variable.type or arg.type_annotation for arg, _ in all_args[arg_name]
            ]
            return mypy.typeops.make_simplified_union([t for t in all_types if t])

    def get_kind(arg_name: str) -&gt; nodes.ArgKind:
        kinds = {arg.kind for arg, _ in all_args[arg_name]}
        if nodes.ARG_STAR in kinds:
            return nodes.ARG_STAR
        if nodes.ARG_STAR2 in kinds:
            return nodes.ARG_STAR2
        # The logic here is based on two tenets:
        # 1) If an arg is ever optional (or unspecified), it is optional
        # 2) If an arg is ever positional, it is positional
        is_opt = (
            len(all_args[arg_name]) &lt; len(stub.items)
            or nodes.ARG_OPT in kinds
            or nodes.ARG_NAMED_OPT in kinds
        )
        is_pos = nodes.ARG_OPT in kinds or nodes.ARG_POS in kinds
        if is_opt:
            return nodes.ARG_OPT if is_pos else nodes.ARG_NAMED_OPT
        return nodes.ARG_POS if is_pos else nodes.ARG_NAMED

    sig: Signature[nodes.Argument] = Signature()
    for arg_name in sorted(all_args, key=get_position):
        # example_arg_name gives us a real name (in case we had a fake index-based name)
        example_arg_name = all_args[arg_name][0][0].variable.name
        arg = nodes.Argument(
            nodes.Var(example_arg_name, get_type(arg_name)),
            type_annotation=None,
            initializer=None,
            kind=get_kind(arg_name),
        )
        if arg.kind.is_positional():
            sig.pos.append(arg)
        elif arg.kind.is_named():
            sig.kwonly[arg.variable.name] = arg
        elif arg.kind == nodes.ARG_STAR:
            sig.varpos = arg
        elif arg.kind == nodes.ARG_STAR2:
            sig.varkw = arg
        else:
            raise AssertionError
    return sig


</t>
<t tx="ekr.20221004064035.2008">def _verify_signature(
    stub: Signature[nodes.Argument], runtime: Signature[inspect.Parameter], function_name: str
) -&gt; Iterator[str]:
    # Check positional arguments match up
    for stub_arg, runtime_arg in zip(stub.pos, runtime.pos):
        yield from _verify_arg_name(stub_arg, runtime_arg, function_name)
        yield from _verify_arg_default_value(stub_arg, runtime_arg)
        if (
            runtime_arg.kind == inspect.Parameter.POSITIONAL_ONLY
            and not stub_arg.pos_only
            and not stub_arg.variable.name.startswith("__")
            and not stub_arg.variable.name.strip("_") == "self"
            and not is_dunder(function_name, exclude_special=True)  # noisy for dunder methods
        ):
            yield (
                f'stub argument "{stub_arg.variable.name}" should be positional-only '
                f'(rename with a leading double underscore, i.e. "__{runtime_arg.name}")'
            )
        if (
            runtime_arg.kind != inspect.Parameter.POSITIONAL_ONLY
            and (stub_arg.pos_only or stub_arg.variable.name.startswith("__"))
            and not is_dunder(function_name, exclude_special=True)  # noisy for dunder methods
        ):
            yield (
                f'stub argument "{stub_arg.variable.name}" should be positional or keyword '
                "(remove leading double underscore)"
            )

    # Check unmatched positional args
    if len(stub.pos) &gt; len(runtime.pos):
        # There are cases where the stub exhaustively lists out the extra parameters the function
        # would take through *args. Hence, a) if runtime accepts *args, we don't check whether the
        # runtime has all of the stub's parameters, b) below, we don't enforce that the stub takes
        # *args, since runtime logic may prevent arbitrary arguments from actually being accepted.
        if runtime.varpos is None:
            for stub_arg in stub.pos[len(runtime.pos) :]:
                # If the variable is in runtime.kwonly, it's just mislabelled as not a
                # keyword-only argument
                if stub_arg.variable.name not in runtime.kwonly:
                    yield f'runtime does not have argument "{stub_arg.variable.name}"'
                else:
                    yield f'stub argument "{stub_arg.variable.name}" is not keyword-only'
            if stub.varpos is not None:
                yield f'runtime does not have *args argument "{stub.varpos.variable.name}"'
    elif len(stub.pos) &lt; len(runtime.pos):
        for runtime_arg in runtime.pos[len(stub.pos) :]:
            if runtime_arg.name not in stub.kwonly:
                yield f'stub does not have argument "{runtime_arg.name}"'
            else:
                yield f'runtime argument "{runtime_arg.name}" is not keyword-only'

    # Checks involving *args
    if len(stub.pos) &lt;= len(runtime.pos) or runtime.varpos is None:
        if stub.varpos is None and runtime.varpos is not None:
            yield f'stub does not have *args argument "{runtime.varpos.name}"'
        if stub.varpos is not None and runtime.varpos is None:
            yield f'runtime does not have *args argument "{stub.varpos.variable.name}"'

    # Check keyword-only args
    for arg in sorted(set(stub.kwonly) &amp; set(runtime.kwonly)):
        stub_arg, runtime_arg = stub.kwonly[arg], runtime.kwonly[arg]
        yield from _verify_arg_name(stub_arg, runtime_arg, function_name)
        yield from _verify_arg_default_value(stub_arg, runtime_arg)

    # Check unmatched keyword-only args
    if runtime.varkw is None or not set(runtime.kwonly).issubset(set(stub.kwonly)):
        # There are cases where the stub exhaustively lists out the extra parameters the function
        # would take through **kwargs. Hence, a) if runtime accepts **kwargs (and the stub hasn't
        # exhaustively listed out params), we don't check whether the runtime has all of the stub's
        # parameters, b) below, we don't enforce that the stub takes **kwargs, since runtime logic
        # may prevent arbitrary keyword arguments from actually being accepted.
        for arg in sorted(set(stub.kwonly) - set(runtime.kwonly)):
            if arg in {runtime_arg.name for runtime_arg in runtime.pos}:
                # Don't report this if we've reported it before
                if arg not in {runtime_arg.name for runtime_arg in runtime.pos[len(stub.pos) :]}:
                    yield f'runtime argument "{arg}" is not keyword-only'
            else:
                yield f'runtime does not have argument "{arg}"'
    for arg in sorted(set(runtime.kwonly) - set(stub.kwonly)):
        if arg in {stub_arg.variable.name for stub_arg in stub.pos}:
            # Don't report this if we've reported it before
            if not (
                runtime.varpos is None
                and arg in {stub_arg.variable.name for stub_arg in stub.pos[len(runtime.pos) :]}
            ):
                yield f'stub argument "{arg}" is not keyword-only'
        else:
            yield f'stub does not have argument "{arg}"'

    # Checks involving **kwargs
    if stub.varkw is None and runtime.varkw is not None:
        # As mentioned above, don't enforce that the stub takes **kwargs.
        # Also check against positional parameters, to avoid a nitpicky message when an argument
        # isn't marked as keyword-only
        stub_pos_names = {stub_arg.variable.name for stub_arg in stub.pos}
        # Ideally we'd do a strict subset check, but in practice the errors from that aren't useful
        if not set(runtime.kwonly).issubset(set(stub.kwonly) | stub_pos_names):
            yield f'stub does not have **kwargs argument "{runtime.varkw.name}"'
    if stub.varkw is not None and runtime.varkw is None:
        yield f'runtime does not have **kwargs argument "{stub.varkw.variable.name}"'


</t>
<t tx="ekr.20221004064035.2009">@verify.register(nodes.FuncItem)
def verify_funcitem(
    stub: nodes.FuncItem, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if not is_probably_a_function(runtime):
        yield Error(object_path, "is not a function", stub, runtime)
        if not callable(runtime):
            return

    if isinstance(stub, nodes.FuncDef):
        for error_text in _verify_abstract_status(stub, runtime):
            yield Error(object_path, error_text, stub, runtime)

    for message in _verify_static_class_methods(stub, runtime, object_path):
        yield Error(object_path, "is inconsistent, " + message, stub, runtime)

    signature = safe_inspect_signature(runtime)
    runtime_is_coroutine = inspect.iscoroutinefunction(runtime)

    if signature:
        stub_sig = Signature.from_funcitem(stub)
        runtime_sig = Signature.from_inspect_signature(signature)
        runtime_sig_desc = f'{"async " if runtime_is_coroutine else ""}def {signature}'
        stub_desc = str(stub_sig)
    else:
        runtime_sig_desc, stub_desc = None, None

    # Don't raise an error if the stub is a coroutine, but the runtime isn't.
    # That results in false positives.
    # See https://github.com/python/typeshed/issues/7344
    if runtime_is_coroutine and not stub.is_coroutine:
        yield Error(
            object_path,
            'is an "async def" function at runtime, but not in the stub',
            stub,
            runtime,
            stub_desc=stub_desc,
            runtime_desc=runtime_sig_desc,
        )

    if not signature:
        return

    for message in _verify_signature(stub_sig, runtime_sig, function_name=stub.name):
        yield Error(
            object_path,
            "is inconsistent, " + message,
            stub,
            runtime,
            runtime_desc=runtime_sig_desc,
        )


</t>
<t tx="ekr.20221004064035.201">def typed_dict_mapping_overlap(
    left: Type, right: Type, overlapping: Callable[[Type, Type], bool]
) -&gt; bool:
    """Check if a TypedDict type is overlapping with a Mapping.

    The basic logic here consists of two rules:

    * A TypedDict with some required keys is overlapping with Mapping[str, &lt;some type&gt;]
      if and only if every key type is overlapping with &lt;some type&gt;. For example:

      - TypedDict(x=int, y=str) overlaps with Dict[str, Union[str, int]]
      - TypedDict(x=int, y=str) doesn't overlap with Dict[str, int]

      Note that any additional non-required keys can't change the above result.

    * A TypedDict with no required keys overlaps with Mapping[str, &lt;some type&gt;] if and
      only if at least one of key types overlaps with &lt;some type&gt;. For example:

      - TypedDict(x=str, y=str, total=False) overlaps with Dict[str, str]
      - TypedDict(x=str, y=str, total=False) doesn't overlap with Dict[str, int]
      - TypedDict(x=int, y=str, total=False) overlaps with Dict[str, str]

    As usual empty, dictionaries lie in a gray area. In general, List[str] and List[str]
    are considered non-overlapping despite empty list belongs to both. However, List[int]
    and List[&lt;nothing&gt;] are considered overlapping.

    So here we follow the same logic: a TypedDict with no required keys is considered
    non-overlapping with Mapping[str, &lt;some type&gt;], but is considered overlapping with
    Mapping[&lt;nothing&gt;, &lt;nothing&gt;]. This way we avoid false positives for overloads, and also
    avoid false positives for comparisons like SomeTypedDict == {} under --strict-equality.
    """
    left, right = get_proper_types((left, right))
    assert not isinstance(left, TypedDictType) or not isinstance(right, TypedDictType)

    if isinstance(left, TypedDictType):
        assert isinstance(right, Instance)
        typed, other = left, right
    else:
        assert isinstance(left, Instance)
        assert isinstance(right, TypedDictType)
        typed, other = right, left

    mapping = next(base for base in other.type.mro if base.fullname == "typing.Mapping")
    other = map_instance_to_supertype(other, mapping)
    key_type, value_type = get_proper_types(other.args)

    # TODO: is there a cleaner way to get str_type here?
    fallback = typed.as_anonymous().fallback
    str_type = fallback.type.bases[0].args[0]  # typing._TypedDict inherits Mapping[str, object]

    # Special case: a TypedDict with no required keys overlaps with an empty dict.
    if isinstance(key_type, UninhabitedType) and isinstance(value_type, UninhabitedType):
        return not typed.required_keys

    if typed.required_keys:
        if not overlapping(key_type, str_type):
            return False
        return all(overlapping(typed.items[k], value_type) for k in typed.required_keys)
    else:
        if not overlapping(key_type, str_type):
            return False
        non_required = set(typed.items.keys()) - typed.required_keys
        return any(overlapping(typed.items[k], value_type) for k in non_required)
</t>
<t tx="ekr.20221004064035.2010">@verify.register(Missing)
def verify_none(
    stub: Missing, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    yield Error(object_path, "is not present in stub", stub, runtime)


</t>
<t tx="ekr.20221004064035.2011">@verify.register(nodes.Var)
def verify_var(
    stub: nodes.Var, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        # Don't always yield an error here, because we often can't find instance variables
        if len(object_path) &lt;= 2:
            yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if (
        stub.is_initialized_in_class
        and is_read_only_property(runtime)
        and (stub.is_settable_property or not stub.is_property)
    ):
        yield Error(object_path, "is read-only at runtime but not in the stub", stub, runtime)

    runtime_type = get_mypy_type_of_runtime_value(runtime)
    if (
        runtime_type is not None
        and stub.type is not None
        and not is_subtype_helper(runtime_type, stub.type)
    ):
        should_error = True
        # Avoid errors when defining enums, since runtime_type is the enum itself, but we'd
        # annotate it with the type of runtime.value
        if isinstance(runtime, enum.Enum):
            runtime_type = get_mypy_type_of_runtime_value(runtime.value)
            if runtime_type is not None and is_subtype_helper(runtime_type, stub.type):
                should_error = False

        if should_error:
            yield Error(
                object_path, f"variable differs from runtime type {runtime_type}", stub, runtime
            )


</t>
<t tx="ekr.20221004064035.2012">@verify.register(nodes.OverloadedFuncDef)
def verify_overloadedfuncdef(
    stub: nodes.OverloadedFuncDef, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if stub.is_property:
        # Any property with a setter is represented as an OverloadedFuncDef
        if is_read_only_property(runtime):
            yield Error(object_path, "is read-only at runtime but not in the stub", stub, runtime)
        return

    if not is_probably_a_function(runtime):
        yield Error(object_path, "is not a function", stub, runtime)
        if not callable(runtime):
            return

    for message in _verify_static_class_methods(stub, runtime, object_path):
        yield Error(object_path, "is inconsistent, " + message, stub, runtime)

    signature = safe_inspect_signature(runtime)
    if not signature:
        return

    stub_sig = Signature.from_overloadedfuncdef(stub)
    runtime_sig = Signature.from_inspect_signature(signature)

    for message in _verify_signature(stub_sig, runtime_sig, function_name=stub.name):
        # TODO: This is a little hacky, but the addition here is super useful
        if "has a default value of type" in message:
            message += (
                ". This is often caused by overloads failing to account for explicitly passing "
                "in the default value."
            )
        yield Error(
            object_path,
            "is inconsistent, " + message,
            stub,
            runtime,
            stub_desc=str(stub.type) + f"\nInferred signature: {stub_sig}",
            runtime_desc="def " + str(signature),
        )


</t>
<t tx="ekr.20221004064035.2013">@verify.register(nodes.TypeVarExpr)
def verify_typevarexpr(
    stub: nodes.TypeVarExpr, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        # We seem to insert these typevars into NamedTuple stubs, but they
        # don't exist at runtime. Just ignore!
        if stub.name == "_NT":
            return
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if not isinstance(runtime, TypeVar):
        yield Error(object_path, "is not a TypeVar", stub, runtime)
        return


</t>
<t tx="ekr.20221004064035.2014">@verify.register(nodes.ParamSpecExpr)
def verify_paramspecexpr(
    stub: nodes.ParamSpecExpr, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    maybe_paramspec_types = (
        getattr(typing, "ParamSpec", None),
        getattr(typing_extensions, "ParamSpec", None),
    )
    paramspec_types = tuple(t for t in maybe_paramspec_types if t is not None)
    if not paramspec_types or not isinstance(runtime, paramspec_types):
        yield Error(object_path, "is not a ParamSpec", stub, runtime)
        return


</t>
<t tx="ekr.20221004064035.2015">def _verify_readonly_property(stub: nodes.Decorator, runtime: Any) -&gt; Iterator[str]:
    assert stub.func.is_property
    if isinstance(runtime, property):
        return
    if inspect.isdatadescriptor(runtime):
        # It's enough like a property...
        return
    # Sometimes attributes pretend to be properties, for instance, to express that they
    # are read only. So allowlist if runtime_type matches the return type of stub.
    runtime_type = get_mypy_type_of_runtime_value(runtime)
    func_type = (
        stub.func.type.ret_type if isinstance(stub.func.type, mypy.types.CallableType) else None
    )
    if (
        runtime_type is not None
        and func_type is not None
        and is_subtype_helper(runtime_type, func_type)
    ):
        return
    yield "is inconsistent, cannot reconcile @property on stub with runtime object"


</t>
<t tx="ekr.20221004064035.2016">def _verify_abstract_status(stub: nodes.FuncDef, runtime: Any) -&gt; Iterator[str]:
    stub_abstract = stub.abstract_status == nodes.IS_ABSTRACT
    runtime_abstract = getattr(runtime, "__isabstractmethod__", False)
    # The opposite can exist: some implementations omit `@abstractmethod` decorators
    if runtime_abstract and not stub_abstract:
        item_type = "property" if stub.is_property else "method"
        yield f"is inconsistent, runtime {item_type} is abstract but stub is not"


</t>
<t tx="ekr.20221004064035.2017">def _resolve_funcitem_from_decorator(dec: nodes.OverloadPart) -&gt; nodes.FuncItem | None:
    """Returns a FuncItem that corresponds to the output of the decorator.

    Returns None if we can't figure out what that would be. For convenience, this function also
    accepts FuncItems.
    """
    if isinstance(dec, nodes.FuncItem):
        return dec
    if dec.func.is_property:
        return None

    @others
    func: nodes.FuncItem = dec.func
    for decorator in dec.original_decorators:
        resulting_func = apply_decorator_to_funcitem(decorator, func)
        if resulting_func is None:
            return None
        func = resulting_func
    return func


</t>
<t tx="ekr.20221004064035.2018">def apply_decorator_to_funcitem(
    decorator: nodes.Expression, func: nodes.FuncItem
) -&gt; nodes.FuncItem | None:
    if not isinstance(decorator, nodes.RefExpr):
        return None
    if decorator.fullname is None:
        # Happens with namedtuple
        return None
    if (
        decorator.fullname in ("builtins.staticmethod", "abc.abstractmethod")
        or decorator.fullname in mypy.types.OVERLOAD_NAMES
    ):
        return func
    if decorator.fullname == "builtins.classmethod":
        if func.arguments[0].variable.name not in ("cls", "mcs", "metacls"):
            raise StubtestFailure(
                f"unexpected class argument name {func.arguments[0].variable.name!r} "
                f"in {dec.fullname}"
            )
        # FuncItem is written so that copy.copy() actually works, even when compiled
        ret = copy.copy(func)
        # Remove the cls argument, since it's not present in inspect.signature of classmethods
        ret.arguments = ret.arguments[1:]
        return ret
    # Just give up on any other decorators. After excluding properties, we don't run into
    # anything else when running on typeshed's stdlib.
    return None

</t>
<t tx="ekr.20221004064035.2019">@verify.register(nodes.Decorator)
def verify_decorator(
    stub: nodes.Decorator, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if stub.func.is_property:
        for message in _verify_readonly_property(stub, runtime):
            yield Error(object_path, message, stub, runtime)
        for message in _verify_abstract_status(stub.func, runtime):
            yield Error(object_path, message, stub, runtime)
        return

    func = _resolve_funcitem_from_decorator(stub)
    if func is not None:
        yield from verify(func, runtime, object_path)


</t>
<t tx="ekr.20221004064035.202">@path C:/Repos/ekr-mypy2/mypy/
"""Utility for dumping memory usage stats.

This is tailored to mypy and knows (a little) about which list objects are
owned by particular AST nodes, etc.
"""

from __future__ import annotations

import gc
import sys
from collections import defaultdict
from typing import Dict, Iterable, cast

from mypy.nodes import FakeInfo, Node
from mypy.types import Type
from mypy.util import get_class_descriptors


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2020">@verify.register(nodes.TypeAlias)
def verify_typealias(
    stub: nodes.TypeAlias, runtime: MaybeMissing[Any], object_path: list[str]
) -&gt; Iterator[Error]:
    stub_target = mypy.types.get_proper_type(stub.target)
    stub_desc = f"Type alias for {stub_target}"
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime, stub_desc=stub_desc)
        return
    runtime_origin = get_origin(runtime) or runtime
    if isinstance(stub_target, mypy.types.Instance):
        if not isinstance(runtime_origin, type):
            yield Error(
                object_path,
                "is inconsistent, runtime is not a type",
                stub,
                runtime,
                stub_desc=stub_desc,
            )
            return

        stub_origin = stub_target.type
        # Do our best to figure out the fullname of the runtime object...
        runtime_name: object
        try:
            runtime_name = runtime_origin.__qualname__
        except AttributeError:
            runtime_name = getattr(runtime_origin, "__name__", MISSING)
        if isinstance(runtime_name, str):
            runtime_module: object = getattr(runtime_origin, "__module__", MISSING)
            if isinstance(runtime_module, str):
                if runtime_module == "collections.abc" or (
                    runtime_module == "re" and runtime_name in {"Match", "Pattern"}
                ):
                    runtime_module = "typing"
                runtime_fullname = f"{runtime_module}.{runtime_name}"
                if re.fullmatch(rf"_?{re.escape(stub_origin.fullname)}", runtime_fullname):
                    # Okay, we're probably fine.
                    return

        # Okay, either we couldn't construct a fullname
        # or the fullname of the stub didn't match the fullname of the runtime.
        # Fallback to a full structural check of the runtime vis-a-vis the stub.
        yield from verify(stub_origin, runtime_origin, object_path)
        return
    if isinstance(stub_target, mypy.types.UnionType):
        # complain if runtime is not a Union or UnionType
        if runtime_origin is not Union and (
            not (sys.version_info &gt;= (3, 10) and isinstance(runtime, types.UnionType))
        ):
            yield Error(object_path, "is not a Union", stub, runtime, stub_desc=str(stub_target))
        # could check Union contents here...
        return
    if isinstance(stub_target, mypy.types.TupleType):
        if tuple not in getattr(runtime_origin, "__mro__", ()):
            yield Error(
                object_path, "is not a subclass of tuple", stub, runtime, stub_desc=stub_desc
            )
        # could check Tuple contents here...
        return
    if isinstance(stub_target, mypy.types.CallableType):
        if runtime_origin is not collections.abc.Callable:
            yield Error(
                object_path, "is not a type alias for Callable", stub, runtime, stub_desc=stub_desc
            )
        # could check Callable contents here...
        return
    if isinstance(stub_target, mypy.types.AnyType):
        return
    yield Error(object_path, "is not a recognised type alias", stub, runtime, stub_desc=stub_desc)


</t>
<t tx="ekr.20221004064035.2021"># ====================
# Helpers
# ====================


IGNORED_MODULE_DUNDERS: typing_extensions.Final = frozenset(
    {
        "__file__",
        "__doc__",
        "__name__",
        "__builtins__",
        "__package__",
        "__cached__",
        "__loader__",
        "__spec__",
        "__annotations__",
        "__path__",  # mypy adds __path__ to packages, but C packages don't have it
        "__getattr__",  # resulting behaviour might be typed explicitly
        # TODO: remove the following from this list
        "__author__",
        "__version__",
        "__copyright__",
    }
)

IGNORABLE_CLASS_DUNDERS: typing_extensions.Final = frozenset(
    {
        # Special attributes
        "__dict__",
        "__annotations__",
        "__text_signature__",
        "__weakref__",
        "__del__",  # Only ever called when an object is being deleted, who cares?
        "__hash__",
        "__getattr__",  # resulting behaviour might be typed explicitly
        "__setattr__",  # defining this on a class can cause worse type checking
        "__vectorcalloffset__",  # undocumented implementation detail of the vectorcall protocol
        # isinstance/issubclass hooks that type-checkers don't usually care about
        "__instancecheck__",
        "__subclasshook__",
        "__subclasscheck__",
        # python2 only magic methods:
        "__cmp__",
        "__nonzero__",
        "__unicode__",
        "__div__",
        # cython methods
        "__pyx_vtable__",
        # Pickle methods
        "__setstate__",
        "__getstate__",
        "__getnewargs__",
        "__getinitargs__",
        "__reduce_ex__",
        "__reduce__",
        # ctypes weirdness
        "__ctype_be__",
        "__ctype_le__",
        "__ctypes_from_outparam__",
        # mypy limitations
        "__abstractmethods__",  # Classes with metaclass=ABCMeta inherit this attribute
        "__new_member__",  # If an enum defines __new__, the method is renamed as __new_member__
        "__dataclass_fields__",  # Generated by dataclasses
        "__dataclass_params__",  # Generated by dataclasses
        "__doc__",  # mypy's semanal for namedtuples assumes this is str, not Optional[str]
        # typing implementation details, consider removing some of these:
        "__parameters__",
        "__origin__",
        "__args__",
        "__orig_bases__",
        "__final__",
        # Consider removing __slots__?
        "__slots__",
    }
)


</t>
<t tx="ekr.20221004064035.2022">def is_probably_private(name: str) -&gt; bool:
    return name.startswith("_") and not is_dunder(name)


</t>
<t tx="ekr.20221004064035.2023">def is_probably_a_function(runtime: Any) -&gt; bool:
    return (
        isinstance(runtime, (types.FunctionType, types.BuiltinFunctionType))
        or isinstance(runtime, (types.MethodType, types.BuiltinMethodType))
        or (inspect.ismethoddescriptor(runtime) and callable(runtime))
    )


</t>
<t tx="ekr.20221004064035.2024">def is_read_only_property(runtime: object) -&gt; bool:
    return isinstance(runtime, property) and runtime.fset is None


</t>
<t tx="ekr.20221004064035.2025">def safe_inspect_signature(runtime: Any) -&gt; inspect.Signature | None:
    try:
        return inspect.signature(runtime)
    except Exception:
        # inspect.signature throws ValueError all the time
        # catch RuntimeError because of https://bugs.python.org/issue39504
        # catch TypeError because of https://github.com/python/typeshed/pull/5762
        # catch AttributeError because of inspect.signature(_curses.window.border)
        return None


</t>
<t tx="ekr.20221004064035.2026">def is_subtype_helper(left: mypy.types.Type, right: mypy.types.Type) -&gt; bool:
    """Checks whether ``left`` is a subtype of ``right``."""
    left = mypy.types.get_proper_type(left)
    right = mypy.types.get_proper_type(right)
    if (
        isinstance(left, mypy.types.LiteralType)
        and isinstance(left.value, int)
        and left.value in (0, 1)
        and mypy.types.is_named_instance(right, "builtins.bool")
    ):
        # Pretend Literal[0, 1] is a subtype of bool to avoid unhelpful errors.
        return True

    if isinstance(right, mypy.types.TypedDictType) and mypy.types.is_named_instance(
        left, "builtins.dict"
    ):
        # Special case checks against TypedDicts
        return True

    with mypy.state.state.strict_optional_set(True):
        return mypy.subtypes.is_subtype(left, right)


</t>
<t tx="ekr.20221004064035.2027">def get_mypy_type_of_runtime_value(runtime: Any) -&gt; mypy.types.Type | None:
    """Returns a mypy type object representing the type of ``runtime``.

    Returns None if we can't find something that works.

    """
    if runtime is None:
        return mypy.types.NoneType()
    if isinstance(runtime, property):
        # Give up on properties to avoid issues with things that are typed as attributes.
        return None

    @others
    if isinstance(
        runtime,
        (types.FunctionType, types.BuiltinFunctionType, types.MethodType, types.BuiltinMethodType),
    ):
        builtins = get_stub("builtins")
        assert builtins is not None
        type_info = builtins.names["function"].node
        assert isinstance(type_info, nodes.TypeInfo)
        fallback = mypy.types.Instance(type_info, [anytype()])
        signature = safe_inspect_signature(runtime)
        if signature:
            arg_types = []
            arg_kinds = []
            arg_names = []
            for arg in signature.parameters.values():
                arg_types.append(anytype())
                arg_names.append(
                    None if arg.kind == inspect.Parameter.POSITIONAL_ONLY else arg.name
                )
                has_default = arg.default == inspect.Parameter.empty
                if arg.kind == inspect.Parameter.POSITIONAL_ONLY:
                    arg_kinds.append(nodes.ARG_POS if has_default else nodes.ARG_OPT)
                elif arg.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
                    arg_kinds.append(nodes.ARG_POS if has_default else nodes.ARG_OPT)
                elif arg.kind == inspect.Parameter.KEYWORD_ONLY:
                    arg_kinds.append(nodes.ARG_NAMED if has_default else nodes.ARG_NAMED_OPT)
                elif arg.kind == inspect.Parameter.VAR_POSITIONAL:
                    arg_kinds.append(nodes.ARG_STAR)
                elif arg.kind == inspect.Parameter.VAR_KEYWORD:
                    arg_kinds.append(nodes.ARG_STAR2)
                else:
                    raise AssertionError
        else:
            arg_types = [anytype(), anytype()]
            arg_kinds = [nodes.ARG_STAR, nodes.ARG_STAR2]
            arg_names = [None, None]

        return mypy.types.CallableType(
            arg_types,
            arg_kinds,
            arg_names,
            ret_type=anytype(),
            fallback=fallback,
            is_ellipsis_args=True,
        )

    # Try and look up a stub for the runtime object
    stub = get_stub(type(runtime).__module__)
    if stub is None:
        return None
    type_name = type(runtime).__name__
    if type_name not in stub.names:
        return None
    type_info = stub.names[type_name].node
    if isinstance(type_info, nodes.Var):
        return type_info.type
    if not isinstance(type_info, nodes.TypeInfo):
        return None

    if isinstance(runtime, tuple):
        # Special case tuples so we construct a valid mypy.types.TupleType
        optional_items = [get_mypy_type_of_runtime_value(v) for v in runtime]
        items = [(i if i is not None else anytype()) for i in optional_items]
        fallback = mypy.types.Instance(type_info, [anytype()])
        return mypy.types.TupleType(items, fallback)

    fallback = mypy.types.Instance(type_info, [anytype() for _ in type_info.type_vars])

    value: bool | int | str
    if isinstance(runtime, bytes):
        value = bytes_to_human_readable_repr(runtime)
    elif isinstance(runtime, enum.Enum):
        value = runtime.name
    elif isinstance(runtime, (bool, int, str)):
        value = runtime
    else:
        return fallback

    return mypy.types.LiteralType(value=value, fallback=fallback)


</t>
<t tx="ekr.20221004064035.2028">def anytype() -&gt; mypy.types.AnyType:
    return mypy.types.AnyType(mypy.types.TypeOfAny.unannotated)

</t>
<t tx="ekr.20221004064035.2029"># ====================
# Build and entrypoint
# ====================


_all_stubs: dict[str, nodes.MypyFile] = {}


</t>
<t tx="ekr.20221004064035.203">def collect_memory_stats() -&gt; tuple[dict[str, int], dict[str, int]]:
    """Return stats about memory use.

    Return a tuple with these items:
      - Dict from object kind to number of instances of that kind
      - Dict from object kind to total bytes used by all instances of that kind
    """
    objs = gc.get_objects()
    find_recursive_objects(objs)

    inferred = {}
    for obj in objs:
        if type(obj) is FakeInfo:
            # Processing these would cause a crash.
            continue
        n = type(obj).__name__
        if hasattr(obj, "__dict__"):
            # Keep track of which class a particular __dict__ is associated with.
            inferred[id(obj.__dict__)] = f"{n} (__dict__)"
        if isinstance(obj, (Node, Type)):  # type: ignore[misc]
            if hasattr(obj, "__dict__"):
                for x in obj.__dict__.values():
                    if isinstance(x, list):
                        # Keep track of which node a list is associated with.
                        inferred[id(x)] = f"{n} (list)"
                    if isinstance(x, tuple):
                        # Keep track of which node a list is associated with.
                        inferred[id(x)] = f"{n} (tuple)"

            for k in get_class_descriptors(type(obj)):
                x = getattr(obj, k, None)
                if isinstance(x, list):
                    inferred[id(x)] = f"{n} (list)"
                if isinstance(x, tuple):
                    inferred[id(x)] = f"{n} (tuple)"

    freqs: dict[str, int] = {}
    memuse: dict[str, int] = {}
    for obj in objs:
        if id(obj) in inferred:
            name = inferred[id(obj)]
        else:
            name = type(obj).__name__
        freqs[name] = freqs.get(name, 0) + 1
        memuse[name] = memuse.get(name, 0) + sys.getsizeof(obj)

    return freqs, memuse


</t>
<t tx="ekr.20221004064035.2030">def build_stubs(modules: list[str], options: Options, find_submodules: bool = False) -&gt; list[str]:
    """Uses mypy to construct stub objects for the given modules.

    This sets global state that ``get_stub`` can access.

    Returns all modules we might want to check. If ``find_submodules`` is False, this is equal
    to ``modules``.

    :param modules: List of modules to build stubs for.
    :param options: Mypy options for finding and building stubs.
    :param find_submodules: Whether to attempt to find submodules of the given modules as well.

    """
    data_dir = mypy.build.default_data_dir()
    search_path = mypy.modulefinder.compute_search_paths([], options, data_dir)
    find_module_cache = mypy.modulefinder.FindModuleCache(
        search_path, fscache=None, options=options
    )

    all_modules = []
    sources = []
    for module in modules:
        all_modules.append(module)
        if not find_submodules:
            module_path = find_module_cache.find_module(module)
            if not isinstance(module_path, str):
                # test_module will yield an error later when it can't find stubs
                continue
            sources.append(mypy.modulefinder.BuildSource(module_path, module, None))
        else:
            found_sources = find_module_cache.find_modules_recursive(module)
            sources.extend(found_sources)
            # find submodules via mypy
            all_modules.extend(s.module for s in found_sources if s.module not in all_modules)
            # find submodules via pkgutil
            try:
                runtime = silent_import_module(module)
                all_modules.extend(
                    m.name
                    for m in pkgutil.walk_packages(runtime.__path__, runtime.__name__ + ".")
                    if m.name not in all_modules
                )
            except Exception:
                pass

    if sources:
        try:
            res = mypy.build.build(sources=sources, options=options)
        except mypy.errors.CompileError as e:
            raise StubtestFailure(f"failed mypy compile:\n{e}") from e
        if res.errors:
            raise StubtestFailure("mypy build errors:\n" + "\n".join(res.errors))

        global _all_stubs
        _all_stubs = res.files

    return all_modules


</t>
<t tx="ekr.20221004064035.2031">def get_stub(module: str) -&gt; nodes.MypyFile | None:
    """Returns a stub object for the given module, if we've built one."""
    return _all_stubs.get(module)


</t>
<t tx="ekr.20221004064035.2032">def get_typeshed_stdlib_modules(
    custom_typeshed_dir: str | None, version_info: tuple[int, int] | None = None
) -&gt; list[str]:
    """Returns a list of stdlib modules in typeshed (for current Python version)."""
    stdlib_py_versions = mypy.modulefinder.load_stdlib_py_versions(custom_typeshed_dir)
    if version_info is None:
        version_info = sys.version_info[0:2]
    # Typeshed's minimum supported Python 3 is Python 3.7
    if sys.version_info &lt; (3, 7):
        version_info = (3, 7)

    @others
    if custom_typeshed_dir:
        typeshed_dir = Path(custom_typeshed_dir)
    else:
        typeshed_dir = Path(mypy.build.default_data_dir()) / "typeshed"
    stdlib_dir = typeshed_dir / "stdlib"

    modules = []
    for path in stdlib_dir.rglob("*.pyi"):
        if path.stem == "__init__":
            path = path.parent
        module = ".".join(path.relative_to(stdlib_dir).parts[:-1] + (path.stem,))
        if exists_in_version(module):
            modules.append(module)
    return sorted(modules)


</t>
<t tx="ekr.20221004064035.2033">def exists_in_version(module: str) -&gt; bool:
    assert version_info is not None
    parts = module.split(".")
    for i in range(len(parts), 0, -1):
        current_module = ".".join(parts[:i])
        if current_module in stdlib_py_versions:
            minver, maxver = stdlib_py_versions[current_module]
            return version_info &gt;= minver and (maxver is None or version_info &lt;= maxver)
    return False

</t>
<t tx="ekr.20221004064035.2034">def get_allowlist_entries(allowlist_file: str) -&gt; Iterator[str]:
    @others
    with open(allowlist_file) as f:
        for line in f.readlines():
            entry = strip_comments(line)
            if entry:
                yield entry


</t>
<t tx="ekr.20221004064035.2035">def strip_comments(s: str) -&gt; str:
    try:
        return s[: s.index("#")].strip()
    except ValueError:
        return s.strip()

</t>
<t tx="ekr.20221004064035.2036">class _Arguments:
    modules: list[str]
    concise: bool
    ignore_missing_stub: bool
    ignore_positional_only: bool
    allowlist: list[str]
    generate_allowlist: bool
    ignore_unused_allowlist: bool
    mypy_config_file: str
    custom_typeshed_dir: str
    check_typeshed: bool
    version: str


</t>
<t tx="ekr.20221004064035.2037">def test_stubs(args: _Arguments, use_builtins_fixtures: bool = False) -&gt; int:
    """This is stubtest! It's time to test the stubs!"""
    # Load the allowlist. This is a series of strings corresponding to Error.object_desc
    # Values in the dict will store whether we used the allowlist entry or not.
    allowlist = {
        entry: False
        for allowlist_file in args.allowlist
        for entry in get_allowlist_entries(allowlist_file)
    }
    allowlist_regexes = {entry: re.compile(entry) for entry in allowlist}

    # If we need to generate an allowlist, we store Error.object_desc for each error here.
    generated_allowlist = set()

    modules = args.modules
    if args.check_typeshed:
        if args.modules:
            print(
                _style("error:", color="red", bold=True),
                "cannot pass both --check-typeshed and a list of modules",
            )
            return 1
        modules = get_typeshed_stdlib_modules(args.custom_typeshed_dir)
        # typeshed added a stub for __main__, but that causes stubtest to check itself
        annoying_modules = {"antigravity", "this", "__main__"}
        modules = [m for m in modules if m not in annoying_modules]

    if not modules:
        print(_style("error:", color="red", bold=True), "no modules to check")
        return 1

    options = Options()
    options.incremental = False
    options.custom_typeshed_dir = args.custom_typeshed_dir
    if options.custom_typeshed_dir:
        options.abs_custom_typeshed_dir = os.path.abspath(args.custom_typeshed_dir)
    options.config_file = args.mypy_config_file
    options.use_builtins_fixtures = use_builtins_fixtures

    if options.config_file:

        def set_strict_flags() -&gt; None:  # not needed yet
            return

        parse_config_file(options, set_strict_flags, options.config_file, sys.stdout, sys.stderr)

    try:
        modules = build_stubs(modules, options, find_submodules=not args.check_typeshed)
    except StubtestFailure as stubtest_failure:
        print(
            _style("error:", color="red", bold=True),
            f"not checking stubs due to {stubtest_failure}",
        )
        return 1

    exit_code = 0
    error_count = 0
    for module in modules:
        for error in test_module(module):
            # Filter errors
            if args.ignore_missing_stub and error.is_missing_stub():
                continue
            if args.ignore_positional_only and error.is_positional_only_related():
                continue
            if error.object_desc in allowlist:
                allowlist[error.object_desc] = True
                continue
            is_allowlisted = False
            for w in allowlist:
                if allowlist_regexes[w].fullmatch(error.object_desc):
                    allowlist[w] = True
                    is_allowlisted = True
                    break
            if is_allowlisted:
                continue

            # We have errors, so change exit code, and output whatever necessary
            exit_code = 1
            if args.generate_allowlist:
                generated_allowlist.add(error.object_desc)
                continue
            print(error.get_description(concise=args.concise))
            error_count += 1

    # Print unused allowlist entries
    if not args.ignore_unused_allowlist:
        for w in allowlist:
            # Don't consider an entry unused if it regex-matches the empty string
            # This lets us allowlist errors that don't manifest at all on some systems
            if not allowlist[w] and not allowlist_regexes[w].fullmatch(""):
                exit_code = 1
                error_count += 1
                print(f"note: unused allowlist entry {w}")

    # Print the generated allowlist
    if args.generate_allowlist:
        for e in sorted(generated_allowlist):
            print(e)
        exit_code = 0
    elif not args.concise:
        if error_count:
            print(
                _style(
                    f"Found {error_count} error{plural_s(error_count)}"
                    f" (checked {len(modules)} module{plural_s(modules)})",
                    color="red",
                    bold=True,
                )
            )
        else:
            print(
                _style(
                    f"Success: no issues found in {len(modules)} module{plural_s(modules)}",
                    color="green",
                    bold=True,
                )
            )

    return exit_code


</t>
<t tx="ekr.20221004064035.2038">def parse_options(args: list[str]) -&gt; _Arguments:
    parser = argparse.ArgumentParser(
        description="Compares stubs to objects introspected from the runtime."
    )
    parser.add_argument("modules", nargs="*", help="Modules to test")
    parser.add_argument(
        "--concise",
        action="store_true",
        help="Makes stubtest's output more concise, one line per error",
    )
    parser.add_argument(
        "--ignore-missing-stub",
        action="store_true",
        help="Ignore errors for stub missing things that are present at runtime",
    )
    parser.add_argument(
        "--ignore-positional-only",
        action="store_true",
        help="Ignore errors for whether an argument should or shouldn't be positional-only",
    )
    parser.add_argument(
        "--allowlist",
        "--whitelist",
        action="append",
        metavar="FILE",
        default=[],
        help=(
            "Use file as an allowlist. Can be passed multiple times to combine multiple "
            "allowlists. Allowlists can be created with --generate-allowlist. Allowlists "
            "support regular expressions."
        ),
    )
    parser.add_argument(
        "--generate-allowlist",
        "--generate-whitelist",
        action="store_true",
        help="Print an allowlist (to stdout) to be used with --allowlist",
    )
    parser.add_argument(
        "--ignore-unused-allowlist",
        "--ignore-unused-whitelist",
        action="store_true",
        help="Ignore unused allowlist entries",
    )
    parser.add_argument(
        "--mypy-config-file",
        metavar="FILE",
        help=("Use specified mypy config file to determine mypy plugins and mypy path"),
    )
    parser.add_argument(
        "--custom-typeshed-dir", metavar="DIR", help="Use the custom typeshed in DIR"
    )
    parser.add_argument(
        "--check-typeshed", action="store_true", help="Check all stdlib modules in typeshed"
    )
    parser.add_argument(
        "--version", action="version", version="%(prog)s " + mypy.version.__version__
    )

    return parser.parse_args(args, namespace=_Arguments())


</t>
<t tx="ekr.20221004064035.2039">def main() -&gt; int:
    mypy.util.check_python_version("stubtest")
    return test_stubs(parse_options(sys.argv[1:]))


</t>
<t tx="ekr.20221004064035.204">def print_memory_profile(run_gc: bool = True) -&gt; None:
    if not sys.platform.startswith("win"):
        import resource

        system_memuse = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    else:
        system_memuse = -1  # TODO: Support this on Windows
    if run_gc:
        gc.collect()
    freqs, memuse = collect_memory_stats()
    print("%7s  %7s  %7s  %s" % ("Freq", "Size(k)", "AvgSize", "Type"))
    print("-------------------------------------------")
    totalmem = 0
    i = 0
    for n, mem in sorted(memuse.items(), key=lambda x: -x[1]):
        f = freqs[n]
        if i &lt; 50:
            print("%7d  %7d  %7.0f  %s" % (f, mem // 1024, mem / f, n))
        i += 1
        totalmem += mem
    print()
    print("Mem usage RSS   ", system_memuse // 1024)
    print("Total reachable ", totalmem // 1024)


</t>
<t tx="ekr.20221004064035.2040">@path C:/Repos/ekr-mypy2/mypy/
"""Utilities for mypy.stubgen, mypy.stubgenc, and mypy.stubdoc modules."""

from __future__ import annotations

import os.path
import re
import sys
from contextlib import contextmanager
from typing import Iterator
from typing_extensions import overload

from mypy.modulefinder import ModuleNotFoundReason
from mypy.moduleinspect import InspectError, ModuleInspect

# Modules that may fail when imported, or that may have side effects (fully qualified).
NOT_IMPORTABLE_MODULES = ()


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2041">class CantImport(Exception):
    def __init__(self, module: str, message: str):
        self.module = module
        self.message = message


</t>
<t tx="ekr.20221004064035.2042">def walk_packages(
    inspect: ModuleInspect, packages: list[str], verbose: bool = False
) -&gt; Iterator[str]:
    """Iterates through all packages and sub-packages in the given list.

    This uses runtime imports (in another process) to find both Python and C modules.
    For Python packages we simply pass the __path__ attribute to pkgutil.walk_packages() to
    get the content of the package (all subpackages and modules).  However, packages in C
    extensions do not have this attribute, so we have to roll out our own logic: recursively
    find all modules imported in the package that have matching names.
    """
    for package_name in packages:
        if package_name in NOT_IMPORTABLE_MODULES:
            print(f"{package_name}: Skipped (blacklisted)")
            continue
        if verbose:
            print(f"Trying to import {package_name!r} for runtime introspection")
        try:
            prop = inspect.get_package_properties(package_name)
        except InspectError:
            report_missing(package_name)
            continue
        yield prop.name
        if prop.is_c_module:
            # Recursively iterate through the subpackages
            yield from walk_packages(inspect, prop.subpackages, verbose)
        else:
            yield from prop.subpackages


</t>
<t tx="ekr.20221004064035.2043">def find_module_path_using_sys_path(module: str, sys_path: list[str]) -&gt; str | None:
    relative_candidates = (
        module.replace(".", "/") + ".py",
        os.path.join(module.replace(".", "/"), "__init__.py"),
    )
    for base in sys_path:
        for relative_path in relative_candidates:
            path = os.path.join(base, relative_path)
            if os.path.isfile(path):
                return path
    return None


</t>
<t tx="ekr.20221004064035.2044">def find_module_path_and_all_py3(
    inspect: ModuleInspect, module: str, verbose: bool
) -&gt; tuple[str | None, list[str] | None] | None:
    """Find module and determine __all__ for a Python 3 module.

    Return None if the module is a C module. Return (module_path, __all__) if
    it is a Python module. Raise CantImport if import failed.
    """
    if module in NOT_IMPORTABLE_MODULES:
        raise CantImport(module, "")

    # TODO: Support custom interpreters.
    if verbose:
        print(f"Trying to import {module!r} for runtime introspection")
    try:
        mod = inspect.get_package_properties(module)
    except InspectError as e:
        # Fall back to finding the module using sys.path.
        path = find_module_path_using_sys_path(module, sys.path)
        if path is None:
            raise CantImport(module, str(e)) from e
        return path, None
    if mod.is_c_module:
        return None
    return mod.file, mod.all


</t>
<t tx="ekr.20221004064035.2045">@contextmanager
def generate_guarded(
    mod: str, target: str, ignore_errors: bool = True, verbose: bool = False
) -&gt; Iterator[None]:
    """Ignore or report errors during stub generation.

    Optionally report success.
    """
    if verbose:
        print(f"Processing {mod}")
    try:
        yield
    except Exception as e:
        if not ignore_errors:
            raise e
        else:
            # --ignore-errors was passed
            print("Stub generation failed for", mod, file=sys.stderr)
    else:
        if verbose:
            print(f"Created {target}")


</t>
<t tx="ekr.20221004064035.2046">def report_missing(mod: str, message: str | None = "", traceback: str = "") -&gt; None:
    if message:
        message = " with error: " + message
    print(f"{mod}: Failed to import, skipping{message}")


</t>
<t tx="ekr.20221004064035.2047">def fail_missing(mod: str, reason: ModuleNotFoundReason) -&gt; None:
    if reason is ModuleNotFoundReason.NOT_FOUND:
        clarification = "(consider using --search-path)"
    elif reason is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
        clarification = "(module likely exists, but is not PEP 561 compatible)"
    else:
        clarification = f"(unknown reason '{reason}')"
    raise SystemExit(f"Can't find module '{mod}' {clarification}")


</t>
<t tx="ekr.20221004064035.2048">@overload
def remove_misplaced_type_comments(source: bytes) -&gt; bytes:
    ...


</t>
<t tx="ekr.20221004064035.2049">@overload
def remove_misplaced_type_comments(source: str) -&gt; str:
    ...


</t>
<t tx="ekr.20221004064035.205">def find_recursive_objects(objs: list[object]) -&gt; None:
    """Find additional objects referenced by objs and append them to objs.

    We use this since gc.get_objects() does not return objects without pointers
    in them such as strings.
    """
    seen = {id(o) for o in objs}

    @others
    for obj in objs[:]:
        if type(obj) is FakeInfo:
            # Processing these would cause a crash.
            continue
        if type(obj) in (dict, defaultdict):
            for key, val in cast(Dict[object, object], obj).items():
                visit(key)
                visit(val)
        if type(obj) in (list, tuple, set):
            for x in cast(Iterable[object], obj):
                visit(x)
        if hasattr(obj, "__slots__"):
            for base in type.mro(type(obj)):
                for slot in getattr(base, "__slots__", ()):
                    if hasattr(obj, slot):
                        visit(getattr(obj, slot))
</t>
<t tx="ekr.20221004064035.2050">def remove_misplaced_type_comments(source: str | bytes) -&gt; str | bytes:
    """Remove comments from source that could be understood as misplaced type comments.

    Normal comments may look like misplaced type comments, and since they cause blocking
    parse errors, we want to avoid them.
    """
    if isinstance(source, bytes):
        # This gives us a 1-1 character code mapping, so it's roundtrippable.
        text = source.decode("latin1")
    else:
        text = source

    # Remove something that looks like a variable type comment but that's by itself
    # on a line, as it will often generate a parse error (unless it's # type: ignore).
    text = re.sub(r'^[ \t]*# +type: +["\'a-zA-Z_].*$', "", text, flags=re.MULTILINE)

    # Remove something that looks like a function type comment after docstring,
    # which will result in a parse error.
    text = re.sub(r'""" *\n[ \t\n]*# +type: +\(.*$', '"""\n', text, flags=re.MULTILINE)
    text = re.sub(r"''' *\n[ \t\n]*# +type: +\(.*$", "'''\n", text, flags=re.MULTILINE)

    # Remove something that looks like a badly formed function type comment.
    text = re.sub(r"^[ \t]*# +type: +\([^()]+(\)[ \t]*)?$", "", text, flags=re.MULTILINE)

    if isinstance(source, bytes):
        return text.encode("latin1")
    else:
        return text


</t>
<t tx="ekr.20221004064035.2051">def common_dir_prefix(paths: list[str]) -&gt; str:
    if not paths:
        return "."
    cur = os.path.dirname(os.path.normpath(paths[0]))
    for path in paths[1:]:
        while True:
            path = os.path.dirname(os.path.normpath(path))
            if (cur + os.sep).startswith(path + os.sep):
                cur = path
                break
    return cur or "."
</t>
<t tx="ekr.20221004064035.2052">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from contextlib import contextmanager
from typing import Any, Callable, Iterator, List, TypeVar, cast
from typing_extensions import Final, TypeAlias as _TypeAlias

import mypy.applytype
import mypy.constraints
import mypy.typeops
from mypy.erasetype import erase_type
from mypy.expandtype import expand_type_by_instance
from mypy.maptype import map_instance_to_supertype

# Circular import; done in the function instead.
# import mypy.solve
from mypy.nodes import (
    ARG_STAR,
    ARG_STAR2,
    CONTRAVARIANT,
    COVARIANT,
    Decorator,
    FuncBase,
    OverloadedFuncDef,
    TypeInfo,
    Var,
)
from mypy.options import Options
from mypy.state import state
from mypy.types import (
    TUPLE_LIKE_INSTANCE_NAMES,
    TYPED_NAMEDTUPLE_NAMES,
    AnyType,
    CallableType,
    DeletedType,
    ErasedType,
    FormalArgument,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    NormalizedCallableType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    TypeVisitor,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
    is_named_instance,
)
from mypy.typestate import SubtypeKind, TypeState
from mypy.typevars import fill_typevars_with_any
from mypy.typevartuples import extract_unpack, split_with_instance

# Flags for detected protocol members
IS_SETTABLE: Final = 1
IS_CLASSVAR: Final = 2
IS_CLASS_OR_STATIC: Final = 3
IS_VAR: Final = 4

TypeParameterChecker: _TypeAlias = Callable[[Type, Type, int, bool, "SubtypeContext"], bool]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2053">class SubtypeContext:
    @others
</t>
<t tx="ekr.20221004064035.2054">def __init__(
    self,
    *,
    # Non-proper subtype flags
    ignore_type_params: bool = False,
    ignore_pos_arg_names: bool = False,
    ignore_declared_variance: bool = False,
    # Supported for both proper and non-proper
    ignore_promotions: bool = False,
    ignore_uninhabited: bool = False,
    # Proper subtype flags
    erase_instances: bool = False,
    keep_erased_types: bool = False,
    options: Options | None = None,
) -&gt; None:
    self.ignore_type_params = ignore_type_params
    self.ignore_pos_arg_names = ignore_pos_arg_names
    self.ignore_declared_variance = ignore_declared_variance
    self.ignore_promotions = ignore_promotions
    self.ignore_uninhabited = ignore_uninhabited
    self.erase_instances = erase_instances
    self.keep_erased_types = keep_erased_types
    self.options = options

</t>
<t tx="ekr.20221004064035.2055">def check_context(self, proper_subtype: bool) -&gt; None:
    # Historically proper and non-proper subtypes were defined using different helpers
    # and different visitors. Check if flag values are such that we definitely support.
    if proper_subtype:
        assert (
            not self.ignore_type_params
            and not self.ignore_pos_arg_names
            and not self.ignore_declared_variance
        )
    else:
        assert not self.erase_instances and not self.keep_erased_types


</t>
<t tx="ekr.20221004064035.2056">def is_subtype(
    left: Type,
    right: Type,
    *,
    subtype_context: SubtypeContext | None = None,
    ignore_type_params: bool = False,
    ignore_pos_arg_names: bool = False,
    ignore_declared_variance: bool = False,
    ignore_promotions: bool = False,
    ignore_uninhabited: bool = False,
    options: Options | None = None,
) -&gt; bool:
    """Is 'left' subtype of 'right'?

    Also consider Any to be a subtype of any type, and vice versa. This
    recursively applies to components of composite types (List[int] is subtype
    of List[Any], for example).

    type_parameter_checker is used to check the type parameters (for example,
    A with B in is_subtype(C[A], C[B]). The default checks for subtype relation
    between the type arguments (e.g., A and B), taking the variance of the
    type var into account.
    """
    if subtype_context is None:
        subtype_context = SubtypeContext(
            ignore_type_params=ignore_type_params,
            ignore_pos_arg_names=ignore_pos_arg_names,
            ignore_declared_variance=ignore_declared_variance,
            ignore_promotions=ignore_promotions,
            ignore_uninhabited=ignore_uninhabited,
            options=options,
        )
    else:
        assert not any(
            {
                ignore_type_params,
                ignore_pos_arg_names,
                ignore_declared_variance,
                ignore_promotions,
                ignore_uninhabited,
                options,
            }
        ), "Don't pass both context and individual flags"
    if TypeState.is_assumed_subtype(left, right):
        return True
    if mypy.typeops.is_recursive_pair(left, right):
        # This case requires special care because it may cause infinite recursion.
        # Our view on recursive types is known under a fancy name of iso-recursive mu-types.
        # Roughly this means that a recursive type is defined as an alias where right hand side
        # can refer to the type as a whole, for example:
        #     A = Union[int, Tuple[A, ...]]
        # and an alias unrolled once represents the *same type*, in our case all these represent
        # the same type:
        #    A
        #    Union[int, Tuple[A, ...]]
        #    Union[int, Tuple[Union[int, Tuple[A, ...]], ...]]
        # The algorithm for subtyping is then essentially under the assumption that left &lt;: right,
        # check that get_proper_type(left) &lt;: get_proper_type(right). On the example above,
        # If we start with:
        #     A = Union[int, Tuple[A, ...]]
        #     B = Union[int, Tuple[B, ...]]
        # When checking if A &lt;: B we push pair (A, B) onto 'assuming' stack, then when after few
        # steps we come back to initial call is_subtype(A, B) and immediately return True.
        with pop_on_exit(TypeState.get_assumptions(is_proper=False), left, right):
            return _is_subtype(left, right, subtype_context, proper_subtype=False)
    return _is_subtype(left, right, subtype_context, proper_subtype=False)


</t>
<t tx="ekr.20221004064035.2057">def is_proper_subtype(
    left: Type,
    right: Type,
    *,
    subtype_context: SubtypeContext | None = None,
    ignore_promotions: bool = False,
    ignore_uninhabited: bool = False,
    erase_instances: bool = False,
    keep_erased_types: bool = False,
) -&gt; bool:
    """Is left a proper subtype of right?

    For proper subtypes, there's no need to rely on compatibility due to
    Any types. Every usable type is a proper subtype of itself.

    If erase_instances is True, erase left instance *after* mapping it to supertype
    (this is useful for runtime isinstance() checks). If keep_erased_types is True,
    do not consider ErasedType a subtype of all types (used by type inference against unions).
    """
    if subtype_context is None:
        subtype_context = SubtypeContext(
            ignore_promotions=ignore_promotions,
            ignore_uninhabited=ignore_uninhabited,
            erase_instances=erase_instances,
            keep_erased_types=keep_erased_types,
        )
    else:
        assert not any(
            {
                ignore_promotions,
                ignore_uninhabited,
                erase_instances,
                keep_erased_types,
                ignore_uninhabited,
            }
        ), "Don't pass both context and individual flags"
    if TypeState.is_assumed_proper_subtype(left, right):
        return True
    if mypy.typeops.is_recursive_pair(left, right):
        # Same as for non-proper subtype, see detailed comment there for explanation.
        with pop_on_exit(TypeState.get_assumptions(is_proper=True), left, right):
            return _is_subtype(left, right, subtype_context, proper_subtype=True)
    return _is_subtype(left, right, subtype_context, proper_subtype=True)


</t>
<t tx="ekr.20221004064035.2058">def is_equivalent(
    a: Type,
    b: Type,
    *,
    ignore_type_params: bool = False,
    ignore_pos_arg_names: bool = False,
    options: Options | None = None,
    subtype_context: SubtypeContext | None = None,
) -&gt; bool:
    return is_subtype(
        a,
        b,
        ignore_type_params=ignore_type_params,
        ignore_pos_arg_names=ignore_pos_arg_names,
        options=options,
        subtype_context=subtype_context,
    ) and is_subtype(
        b,
        a,
        ignore_type_params=ignore_type_params,
        ignore_pos_arg_names=ignore_pos_arg_names,
        options=options,
        subtype_context=subtype_context,
    )


</t>
<t tx="ekr.20221004064035.2059">def is_same_type(
    a: Type, b: Type, ignore_promotions: bool = True, subtype_context: SubtypeContext | None = None
) -&gt; bool:
    """Are these types proper subtypes of each other?

    This means types may have different representation (e.g. an alias, or
    a non-simplified union) but are semantically exchangeable in all contexts.
    """
    # Note that using ignore_promotions=True (default) makes types like int and int64
    # considered not the same type (which is the case at runtime).
    # Also Union[bool, int] (if it wasn't simplified before) will be different
    # from plain int, etc.
    return is_proper_subtype(
        a, b, ignore_promotions=ignore_promotions, subtype_context=subtype_context
    ) and is_proper_subtype(
        b, a, ignore_promotions=ignore_promotions, subtype_context=subtype_context
    )


</t>
<t tx="ekr.20221004064035.206">def visit(o: object) -&gt; None:
    if id(o) not in seen:
        objs.append(o)
        seen.add(id(o))

</t>
<t tx="ekr.20221004064035.2060"># This is a common entry point for subtyping checks (both proper and non-proper).
# Never call this private function directly, use the public versions.
def _is_subtype(
    left: Type, right: Type, subtype_context: SubtypeContext, proper_subtype: bool
) -&gt; bool:
    subtype_context.check_context(proper_subtype)
    orig_right = right
    orig_left = left
    left = get_proper_type(left)
    right = get_proper_type(right)

    if not proper_subtype and (
        isinstance(right, AnyType)
        or isinstance(right, UnboundType)
        or isinstance(right, ErasedType)
    ):
        # TODO: should we consider all types proper subtypes of UnboundType and/or
        # ErasedType as we do for non-proper subtyping.
        return True

    @others
    if isinstance(right, UnionType) and not isinstance(left, UnionType):
        # Normally, when 'left' is not itself a union, the only way
        # 'left' can be a subtype of the union 'right' is if it is a
        # subtype of one of the items making up the union.
        is_subtype_of_item = any(
            check_item(orig_left, item, subtype_context) for item in right.items
        )
        # Recombine rhs literal types, to make an enum type a subtype
        # of a union of all enum items as literal types. Only do it if
        # the previous check didn't succeed, since recombining can be
        # expensive.
        # `bool` is a special case, because `bool` is `Literal[True, False]`.
        if (
            not is_subtype_of_item
            and isinstance(left, Instance)
            and (left.type.is_enum or left.type.fullname == "builtins.bool")
        ):
            right = UnionType(mypy.typeops.try_contracting_literals_in_union(right.items))
            is_subtype_of_item = any(
                check_item(orig_left, item, subtype_context) for item in right.items
            )
        # However, if 'left' is a type variable T, T might also have
        # an upper bound which is itself a union. This case will be
        # handled below by the SubtypeVisitor. We have to check both
        # possibilities, to handle both cases like T &lt;: Union[T, U]
        # and cases like T &lt;: B where B is the upper bound of T and is
        # a union. (See #2314.)
        if not isinstance(left, TypeVarType):
            return is_subtype_of_item
        elif is_subtype_of_item:
            return True
        # otherwise, fall through
    return left.accept(SubtypeVisitor(orig_right, subtype_context, proper_subtype))


</t>
<t tx="ekr.20221004064035.2061">def check_item(left: Type, right: Type, subtype_context: SubtypeContext) -&gt; bool:
    if proper_subtype:
        return is_proper_subtype(left, right, subtype_context=subtype_context)
    return is_subtype(left, right, subtype_context=subtype_context)

</t>
<t tx="ekr.20221004064035.2062">def check_type_parameter(
    lefta: Type, righta: Type, variance: int, proper_subtype: bool, subtype_context: SubtypeContext
) -&gt; bool:
    @others
    if variance == COVARIANT:
        return check(lefta, righta)
    elif variance == CONTRAVARIANT:
        return check(righta, lefta)
    else:
        if proper_subtype:
            # We pass ignore_promotions=False because it is a default for subtype checks.
            # The actual value will be taken from the subtype_context, and it is whatever
            # the original caller passed.
            return is_same_type(
                lefta, righta, ignore_promotions=False, subtype_context=subtype_context
            )
        return is_equivalent(lefta, righta, subtype_context=subtype_context)


</t>
<t tx="ekr.20221004064035.2063">def check(left: Type, right: Type) -&gt; bool:
    return (
        is_proper_subtype(left, right, subtype_context=subtype_context)
        if proper_subtype
        else is_subtype(left, right, subtype_context=subtype_context)
    )

</t>
<t tx="ekr.20221004064035.2064">def ignore_type_parameter(
    lefta: Type, righta: Type, variance: int, proper_subtype: bool, subtype_context: SubtypeContext
) -&gt; bool:
    return True


</t>
<t tx="ekr.20221004064035.2065">class SubtypeVisitor(TypeVisitor[bool]):
    @others
</t>
<t tx="ekr.20221004064035.2066">def __init__(self, right: Type, subtype_context: SubtypeContext, proper_subtype: bool) -&gt; None:
    self.right = get_proper_type(right)
    self.orig_right = right
    self.proper_subtype = proper_subtype
    self.subtype_context = subtype_context
    self.check_type_parameter = (
        ignore_type_parameter if subtype_context.ignore_type_params else check_type_parameter
    )
    self.options = subtype_context.options
    self._subtype_kind = SubtypeVisitor.build_subtype_kind(subtype_context, proper_subtype)

</t>
<t tx="ekr.20221004064035.2067">@staticmethod
def build_subtype_kind(subtype_context: SubtypeContext, proper_subtype: bool) -&gt; SubtypeKind:
    return (
        state.strict_optional,
        proper_subtype,
        subtype_context.ignore_type_params,
        subtype_context.ignore_pos_arg_names,
        subtype_context.ignore_declared_variance,
        subtype_context.ignore_promotions,
        subtype_context.erase_instances,
        subtype_context.keep_erased_types,
    )

</t>
<t tx="ekr.20221004064035.2068">def _is_subtype(self, left: Type, right: Type) -&gt; bool:
    if self.proper_subtype:
        return is_proper_subtype(left, right, subtype_context=self.subtype_context)
    return is_subtype(left, right, subtype_context=self.subtype_context)

</t>
<t tx="ekr.20221004064035.2069"># visit_x(left) means: is left (which is an instance of X) a subtype of
# right?

</t>
<t tx="ekr.20221004064035.207">@path C:/Repos/ekr-mypy2/mypy/
"""Facilities for generating error messages during type checking.

Don't add any non-trivial message construction logic to the type
checker, as it can compromise clarity and make messages less
consistent. Add such logic to this module instead. Literal messages, including those
with format args, should be defined as constants in mypy.message_registry.

Historically we tried to avoid all message string literals in the type
checker but we are moving away from this convention.
"""

from __future__ import annotations

import difflib
import re
from contextlib import contextmanager
from textwrap import dedent
from typing import Any, Callable, Iterable, Iterator, List, Sequence, cast
from typing_extensions import Final

from mypy import errorcodes as codes, message_registry
from mypy.erasetype import erase_type
from mypy.errorcodes import ErrorCode
from mypy.errors import ErrorInfo, Errors, ErrorWatcher
from mypy.nodes import (
    ARG_NAMED,
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    CONTRAVARIANT,
    COVARIANT,
    SYMBOL_FUNCBASE_TYPES,
    ArgKind,
    CallExpr,
    ClassDef,
    Context,
    Expression,
    FuncDef,
    IndexExpr,
    MypyFile,
    NameExpr,
    ReturnStmt,
    StrExpr,
    SymbolNode,
    SymbolTable,
    TypeInfo,
    Var,
    reverse_builtin_aliases,
)
from mypy.operators import op_methods, op_methods_to_symbols
from mypy.subtypes import (
    IS_CLASS_OR_STATIC,
    IS_CLASSVAR,
    IS_SETTABLE,
    IS_VAR,
    find_member,
    get_member_flags,
    is_same_type,
    is_subtype,
)
from mypy.typeops import separate_union_literals
from mypy.types import (
    AnyType,
    CallableType,
    DeletedType,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    get_proper_type,
    get_proper_types,
)
from mypy.typetraverser import TypeTraverserVisitor
from mypy.util import plural_s, unmangle

TYPES_FOR_UNIMPORTED_HINTS: Final = {
    "typing.Any",
    "typing.Callable",
    "typing.Dict",
    "typing.Iterable",
    "typing.Iterator",
    "typing.List",
    "typing.Optional",
    "typing.Set",
    "typing.Tuple",
    "typing.TypeVar",
    "typing.Union",
    "typing.cast",
}


ARG_CONSTRUCTOR_NAMES: Final = {
    ARG_POS: "Arg",
    ARG_OPT: "DefaultArg",
    ARG_NAMED: "NamedArg",
    ARG_NAMED_OPT: "DefaultNamedArg",
    ARG_STAR: "VarArg",
    ARG_STAR2: "KwArg",
}


# Map from the full name of a missing definition to the test fixture (under
# test-data/unit/fixtures/) that provides the definition. This is used for
# generating better error messages when running mypy tests only.
SUGGESTED_TEST_FIXTURES: Final = {
    "builtins.list": "list.pyi",
    "builtins.dict": "dict.pyi",
    "builtins.set": "set.pyi",
    "builtins.tuple": "tuple.pyi",
    "builtins.bool": "bool.pyi",
    "builtins.Exception": "exception.pyi",
    "builtins.BaseException": "exception.pyi",
    "builtins.isinstance": "isinstancelist.pyi",
    "builtins.property": "property.pyi",
    "builtins.classmethod": "classmethod.pyi",
}


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2070">def visit_unbound_type(self, left: UnboundType) -&gt; bool:
    # This can be called if there is a bad type annotation. The result probably
    # doesn't matter much but by returning True we simplify these bad types away
    # from unions, which could filter out some bogus messages.
    return True

</t>
<t tx="ekr.20221004064035.2071">def visit_any(self, left: AnyType) -&gt; bool:
    return isinstance(self.right, AnyType) if self.proper_subtype else True

</t>
<t tx="ekr.20221004064035.2072">def visit_none_type(self, left: NoneType) -&gt; bool:
    if state.strict_optional:
        if isinstance(self.right, NoneType) or is_named_instance(
            self.right, "builtins.object"
        ):
            return True
        if isinstance(self.right, Instance) and self.right.type.is_protocol:
            members = self.right.type.protocol_members
            # None is compatible with Hashable (and other similar protocols). This is
            # slightly sloppy since we don't check the signature of "__hash__".
            # None is also compatible with `SupportsStr` protocol.
            return not members or all(member in ("__hash__", "__str__") for member in members)
        return False
    else:
        return True

</t>
<t tx="ekr.20221004064035.2073">def visit_uninhabited_type(self, left: UninhabitedType) -&gt; bool:
    # We ignore this for unsafe overload checks, so that and empty list and
    # a list of int will be considered non-overlapping.
    if isinstance(self.right, UninhabitedType):
        return True
    return not self.subtype_context.ignore_uninhabited

</t>
<t tx="ekr.20221004064035.2074">def visit_erased_type(self, left: ErasedType) -&gt; bool:
    # This may be encountered during type inference. The result probably doesn't
    # matter much.
    # TODO: it actually does matter, figure out more principled logic about this.
    if self.subtype_context.keep_erased_types:
        return False
    return True

</t>
<t tx="ekr.20221004064035.2075">def visit_deleted_type(self, left: DeletedType) -&gt; bool:
    return True

</t>
<t tx="ekr.20221004064035.2076">def visit_instance(self, left: Instance) -&gt; bool:
    if left.type.fallback_to_any and not self.proper_subtype:
        if isinstance(self.right, NoneType):
            # NOTE: `None` is a *non-subclassable* singleton, therefore no class
            # can by a subtype of it, even with an `Any` fallback.
            # This special case is needed to treat descriptors in classes with
            # dynamic base classes correctly, see #5456.
            return False
        return True
    right = self.right
    if isinstance(right, TupleType) and mypy.typeops.tuple_fallback(right).type.is_enum:
        return self._is_subtype(left, mypy.typeops.tuple_fallback(right))
    if isinstance(right, Instance):
        if TypeState.is_cached_subtype_check(self._subtype_kind, left, right):
            return True
        if not self.subtype_context.ignore_promotions:
            for base in left.type.mro:
                if base._promote and any(
                    self._is_subtype(p, self.right) for p in base._promote
                ):
                    TypeState.record_subtype_cache_entry(self._subtype_kind, left, right)
                    return True
            # Special case: Low-level integer types are compatible with 'int'. We can't
            # use promotions, since 'int' is already promoted to low-level integer types,
            # and we can't have circular promotions.
            if left.type.alt_promote is right.type:
                return True
        rname = right.type.fullname
        # Always try a nominal check if possible,
        # there might be errors that a user wants to silence *once*.
        # NamedTuples are a special case, because `NamedTuple` is not listed
        # in `TypeInfo.mro`, so when `(a: NamedTuple) -&gt; None` is used,
        # we need to check for `is_named_tuple` property
        if (
            left.type.has_base(rname)
            or rname == "builtins.object"
            or (
                rname in TYPED_NAMEDTUPLE_NAMES
                and any(l.is_named_tuple for l in left.type.mro)
            )
        ) and not self.subtype_context.ignore_declared_variance:
            # Map left type to corresponding right instances.
            t = map_instance_to_supertype(left, right.type)
            if self.subtype_context.erase_instances:
                erased = erase_type(t)
                assert isinstance(erased, Instance)
                t = erased
            nominal = True
            if right.type.has_type_var_tuple_type:
                left_prefix, left_middle, left_suffix = split_with_instance(left)
                right_prefix, right_middle, right_suffix = split_with_instance(right)

                left_unpacked = extract_unpack(left_middle)
                right_unpacked = extract_unpack(right_middle)

                # Helper for case 2 below so we can treat them the same.
                def check_mixed(
                    unpacked_type: ProperType, compare_to: tuple[Type, ...]
                ) -&gt; bool:
                    if isinstance(unpacked_type, TypeVarTupleType):
                        return False
                    if isinstance(unpacked_type, AnyType):
                        return True
                    if isinstance(unpacked_type, TupleType):
                        if len(unpacked_type.items) != len(compare_to):
                            return False
                        for t1, t2 in zip(unpacked_type.items, compare_to):
                            if not is_equivalent(t1, t2):
                                return False
                        return True
                    return False

                # Case 1: Both are unpacks, in this case we check what is being
                # unpacked is the same.
                if left_unpacked is not None and right_unpacked is not None:
                    if not is_equivalent(left_unpacked, right_unpacked):
                        return False

                # Case 2: Only one of the types is an unpack. The equivalence
                # case is mostly the same but we check some additional
                # things when unpacking on the right.
                elif left_unpacked is not None and right_unpacked is None:
                    if not check_mixed(left_unpacked, right_middle):
                        return False
                elif left_unpacked is None and right_unpacked is not None:
                    if (
                        isinstance(right_unpacked, Instance)
                        and right_unpacked.type.fullname == "builtins.tuple"
                    ):
                        return all(
                            is_equivalent(l, right_unpacked.args[0]) for l in left_middle
                        )
                    if not check_mixed(right_unpacked, left_middle):
                        return False

                # Case 3: Neither type is an unpack. In this case we just compare
                # the items themselves.
                else:
                    if len(left_middle) != len(right_middle):
                        return False
                    for left_t, right_t in zip(left_middle, right_middle):
                        if not is_equivalent(left_t, right_t):
                            return False

                left_items = t.args[: right.type.type_var_tuple_prefix]
                right_items = right.args[: right.type.type_var_tuple_prefix]
                if right.type.type_var_tuple_suffix:
                    left_items += t.args[-right.type.type_var_tuple_suffix :]
                    right_items += right.args[-right.type.type_var_tuple_suffix :]

                unpack_index = right.type.type_var_tuple_prefix
                assert unpack_index is not None
                type_params = zip(
                    left_prefix + right_suffix,
                    right_prefix + right_suffix,
                    right.type.defn.type_vars[:unpack_index]
                    + right.type.defn.type_vars[unpack_index + 1 :],
                )
            else:
                type_params = zip(t.args, right.args, right.type.defn.type_vars)
            for lefta, righta, tvar in type_params:
                if isinstance(tvar, TypeVarType):
                    if not self.check_type_parameter(
                        lefta, righta, tvar.variance, self.proper_subtype, self.subtype_context
                    ):
                        nominal = False
                else:
                    if not self.check_type_parameter(
                        lefta, righta, COVARIANT, self.proper_subtype, self.subtype_context
                    ):
                        nominal = False
            if nominal:
                TypeState.record_subtype_cache_entry(self._subtype_kind, left, right)
            return nominal
        if right.type.is_protocol and is_protocol_implementation(
            left, right, proper_subtype=self.proper_subtype
        ):
            return True
        return False
    if isinstance(right, TypeType):
        item = right.item
        if isinstance(item, TupleType):
            item = mypy.typeops.tuple_fallback(item)
        # TODO: this is a bit arbitrary, we should only skip Any-related cases.
        if not self.proper_subtype:
            if is_named_instance(left, "builtins.type"):
                return self._is_subtype(TypeType(AnyType(TypeOfAny.special_form)), right)
            if left.type.is_metaclass():
                if isinstance(item, AnyType):
                    return True
                if isinstance(item, Instance):
                    return is_named_instance(item, "builtins.object")
    if isinstance(right, LiteralType) and left.last_known_value is not None:
        return self._is_subtype(left.last_known_value, right)
    if isinstance(right, CallableType):
        # Special case: Instance can be a subtype of Callable.
        call = find_member("__call__", left, left, is_operator=True)
        if call:
            return self._is_subtype(call, right)
        return False
    else:
        return False

</t>
<t tx="ekr.20221004064035.2077">def visit_type_var(self, left: TypeVarType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeVarType) and left.id == right.id:
        return True
    if left.values and self._is_subtype(UnionType.make_union(left.values), right):
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20221004064035.2078">def visit_param_spec(self, left: ParamSpecType) -&gt; bool:
    right = self.right
    if (
        isinstance(right, ParamSpecType)
        and right.id == left.id
        and right.flavor == left.flavor
    ):
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20221004064035.2079">def visit_type_var_tuple(self, left: TypeVarTupleType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeVarTupleType) and right.id == left.id:
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20221004064035.208">class MessageBuilder:
    """Helper class for reporting type checker error messages with parameters.

    The methods of this class need to be provided with the context within a
    file; the errors member manages the wider context.

    IDEA: Support a 'verbose mode' that includes full information about types
          in error messages and that may otherwise produce more detailed error
          messages.
    """

    # Report errors using this instance. It knows about the current file and
    # import context.
    errors: Errors

    modules: dict[str, MypyFile]

    # Hack to deduplicate error messages from union types
    _disable_type_names: list[bool]

    @others
</t>
<t tx="ekr.20221004064035.2080">def visit_unpack_type(self, left: UnpackType) -&gt; bool:
    if isinstance(self.right, UnpackType):
        return self._is_subtype(left.type, self.right.type)
    return False

</t>
<t tx="ekr.20221004064035.2081">def visit_parameters(self, left: Parameters) -&gt; bool:
    if isinstance(self.right, Parameters) or isinstance(self.right, CallableType):
        right = self.right
        if isinstance(right, CallableType):
            right = right.with_unpacked_kwargs()
        return are_parameters_compatible(
            left,
            right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
        )
    else:
        return False

</t>
<t tx="ekr.20221004064035.2082">def visit_callable_type(self, left: CallableType) -&gt; bool:
    right = self.right
    if isinstance(right, CallableType):
        if left.type_guard is not None and right.type_guard is not None:
            if not self._is_subtype(left.type_guard, right.type_guard):
                return False
        elif right.type_guard is not None and left.type_guard is None:
            # This means that one function has `TypeGuard` and other does not.
            # They are not compatible. See https://github.com/python/mypy/issues/11307
            return False
        return is_callable_compatible(
            left,
            right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
            strict_concatenate=self.options.strict_concatenate if self.options else True,
        )
    elif isinstance(right, Overloaded):
        return all(self._is_subtype(left, item) for item in right.items)
    elif isinstance(right, Instance):
        if right.type.is_protocol and right.type.protocol_members == ["__call__"]:
            # OK, a callable can implement a protocol with a single `__call__` member.
            # TODO: we should probably explicitly exclude self-types in this case.
            call = find_member("__call__", right, left, is_operator=True)
            assert call is not None
            if self._is_subtype(left, call):
                return True
        if right.type.is_protocol and left.is_type_obj():
            ret_type = get_proper_type(left.ret_type)
            if isinstance(ret_type, TupleType):
                ret_type = mypy.typeops.tuple_fallback(ret_type)
            if isinstance(ret_type, Instance) and is_protocol_implementation(
                ret_type, right, proper_subtype=self.proper_subtype, class_obj=True
            ):
                return True
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, TypeType):
        # This is unsound, we don't check the __init__ signature.
        return left.is_type_obj() and self._is_subtype(left.ret_type, right.item)
    elif isinstance(right, Parameters):
        # this doesn't check return types.... but is needed for is_equivalent
        return are_parameters_compatible(
            left.with_unpacked_kwargs(),
            right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
        )
    else:
        return False

</t>
<t tx="ekr.20221004064035.2083">def visit_tuple_type(self, left: TupleType) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if is_named_instance(right, "typing.Sized"):
            return True
        elif is_named_instance(right, TUPLE_LIKE_INSTANCE_NAMES):
            if right.args:
                iter_type = right.args[0]
            else:
                if self.proper_subtype:
                    return False
                iter_type = AnyType(TypeOfAny.special_form)
            if is_named_instance(right, "builtins.tuple") and isinstance(
                get_proper_type(iter_type), AnyType
            ):
                # TODO: We shouldn't need this special case. This is currently needed
                #       for isinstance(x, tuple), though it's unclear why.
                return True
            return all(self._is_subtype(li, iter_type) for li in left.items)
        elif self._is_subtype(mypy.typeops.tuple_fallback(left), right):
            return True
        return False
    elif isinstance(right, TupleType):
        if len(left.items) != len(right.items):
            return False
        for l, r in zip(left.items, right.items):
            if not self._is_subtype(l, r):
                return False
        rfallback = mypy.typeops.tuple_fallback(right)
        if is_named_instance(rfallback, "builtins.tuple"):
            # No need to verify fallback. This is useful since the calculated fallback
            # may be inconsistent due to how we calculate joins between unions vs.
            # non-unions. For example, join(int, str) == object, whereas
            # join(Union[int, C], Union[str, C]) == Union[int, str, C].
            return True
        lfallback = mypy.typeops.tuple_fallback(left)
        if not self._is_subtype(lfallback, rfallback):
            return False
        return True
    else:
        return False

</t>
<t tx="ekr.20221004064035.2084">def visit_typeddict_type(self, left: TypedDictType) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, TypedDictType):
        if not left.names_are_wider_than(right):
            return False
        for name, l, r in left.zip(right):
            # TODO: should we pass on the full subtype_context here and below?
            if self.proper_subtype:
                check = is_same_type(l, r)
            else:
                check = is_equivalent(
                    l,
                    r,
                    ignore_type_params=self.subtype_context.ignore_type_params,
                    options=self.options,
                )
            if not check:
                return False
            # Non-required key is not compatible with a required key since
            # indexing may fail unexpectedly if a required key is missing.
            # Required key is not compatible with a non-required key since
            # the prior doesn't support 'del' but the latter should support
            # it.
            #
            # NOTE: 'del' support is currently not implemented (#3550). We
            #       don't want to have to change subtyping after 'del' support
            #       lands so here we are anticipating that change.
            if (name in left.required_keys) != (name in right.required_keys):
                return False
        # (NOTE: Fallbacks don't matter.)
        return True
    else:
        return False

</t>
<t tx="ekr.20221004064035.2085">def visit_literal_type(self, left: LiteralType) -&gt; bool:
    if isinstance(self.right, LiteralType):
        return left == self.right
    else:
        return self._is_subtype(left.fallback, self.right)

</t>
<t tx="ekr.20221004064035.2086">def visit_overloaded(self, left: Overloaded) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if right.type.is_protocol and right.type.protocol_members == ["__call__"]:
            # same as for CallableType
            call = find_member("__call__", right, left, is_operator=True)
            assert call is not None
            if self._is_subtype(left, call):
                return True
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, CallableType):
        for item in left.items:
            if self._is_subtype(item, right):
                return True
        return False
    elif isinstance(right, Overloaded):
        if left == self.right:
            # When it is the same overload, then the types are equal.
            return True

        # Ensure each overload in the right side (the supertype) is accounted for.
        previous_match_left_index = -1
        matched_overloads = set()
        possible_invalid_overloads = set()

        for right_index, right_item in enumerate(right.items):
            found_match = False

            for left_index, left_item in enumerate(left.items):
                subtype_match = self._is_subtype(left_item, right_item)

                # Order matters: we need to make sure that the index of
                # this item is at least the index of the previous one.
                if subtype_match and previous_match_left_index &lt;= left_index:
                    if not found_match:
                        # Update the index of the previous match.
                        previous_match_left_index = left_index
                        found_match = True
                        matched_overloads.add(left_item)
                        possible_invalid_overloads.discard(left_item)
                else:
                    # If this one overlaps with the supertype in any way, but it wasn't
                    # an exact match, then it's a potential error.
                    strict_concat = self.options.strict_concatenate if self.options else True
                    if is_callable_compatible(
                        left_item,
                        right_item,
                        is_compat=self._is_subtype,
                        ignore_return=True,
                        ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
                        strict_concatenate=strict_concat,
                    ) or is_callable_compatible(
                        right_item,
                        left_item,
                        is_compat=self._is_subtype,
                        ignore_return=True,
                        ignore_pos_arg_names=self.subtype_context.ignore_pos_arg_names,
                        strict_concatenate=strict_concat,
                    ):
                        # If this is an overload that's already been matched, there's no
                        # problem.
                        if left_item not in matched_overloads:
                            possible_invalid_overloads.add(left_item)

            if not found_match:
                return False

        if possible_invalid_overloads:
            # There were potentially invalid overloads that were never matched to the
            # supertype.
            return False
        return True
    elif isinstance(right, UnboundType):
        return True
    elif isinstance(right, TypeType):
        # All the items must have the same type object status, so
        # it's sufficient to query only (any) one of them.
        # This is unsound, we don't check all the __init__ signatures.
        return left.is_type_obj() and self._is_subtype(left.items[0], right)
    else:
        return False

</t>
<t tx="ekr.20221004064035.2087">def visit_union_type(self, left: UnionType) -&gt; bool:
    if isinstance(self.right, Instance):
        literal_types: set[Instance] = set()
        # avoid redundant check for union of literals
        for item in left.relevant_items():
            p_item = get_proper_type(item)
            lit_type = mypy.typeops.simple_literal_type(p_item)
            if lit_type is not None:
                if lit_type in literal_types:
                    continue
                literal_types.add(lit_type)
                item = lit_type
            if not self._is_subtype(item, self.orig_right):
                return False
        return True
    return all(self._is_subtype(item, self.orig_right) for item in left.items)

</t>
<t tx="ekr.20221004064035.2088">def visit_partial_type(self, left: PartialType) -&gt; bool:
    # This is indeterminate as we don't really know the complete type yet.
    if self.proper_subtype:
        # TODO: What's the right thing to do here?
        return False
    if left.type is None:
        # Special case, partial `None`. This might happen when defining
        # class-level attributes with explicit `None`.
        # We can still recover from this.
        # https://github.com/python/mypy/issues/11105
        return self.visit_none_type(NoneType())
    raise RuntimeError(f'Partial type "{left}" cannot be checked with "issubtype()"')

</t>
<t tx="ekr.20221004064035.2089">def visit_type_type(self, left: TypeType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeType):
        return self._is_subtype(left.item, right.item)
    if isinstance(right, CallableType):
        if self.proper_subtype and not right.is_type_obj():
            # We can't accept `Type[X]` as a *proper* subtype of Callable[P, X]
            # since this will break transitivity of subtyping.
            return False
        # This is unsound, we don't check the __init__ signature.
        return self._is_subtype(left.item, right.ret_type)
    if isinstance(right, Instance):
        if right.type.fullname in ["builtins.object", "builtins.type"]:
            # TODO: Strictly speaking, the type builtins.type is considered equivalent to
            #       Type[Any]. However, this would break the is_proper_subtype check in
            #       conditional_types for cases like isinstance(x, type) when the type
            #       of x is Type[int]. It's unclear what's the right way to address this.
            return True
        item = left.item
        if isinstance(item, TypeVarType):
            item = get_proper_type(item.upper_bound)
        if isinstance(item, Instance):
            if right.type.is_protocol and is_protocol_implementation(
                item, right, proper_subtype=self.proper_subtype, class_obj=True
            ):
                return True
            metaclass = item.type.metaclass_type
            return metaclass is not None and self._is_subtype(metaclass, right)
    return False

</t>
<t tx="ekr.20221004064035.209">def __init__(self, errors: Errors, modules: dict[str, MypyFile]) -&gt; None:
    self.errors = errors
    self.modules = modules
    self._disable_type_names = []

</t>
<t tx="ekr.20221004064035.2090">def visit_type_alias_type(self, left: TypeAliasType) -&gt; bool:
    assert False, f"This should be never called, got {left}"


</t>
<t tx="ekr.20221004064035.2091">T = TypeVar("T", bound=Type)


</t>
<t tx="ekr.20221004064035.2092">@contextmanager
def pop_on_exit(stack: list[tuple[T, T]], left: T, right: T) -&gt; Iterator[None]:
    stack.append((left, right))
    yield
    stack.pop()


</t>
<t tx="ekr.20221004064035.2093">def is_protocol_implementation(
    left: Instance, right: Instance, proper_subtype: bool = False, class_obj: bool = False
) -&gt; bool:
    """Check whether 'left' implements the protocol 'right'.

    If 'proper_subtype' is True, then check for a proper subtype.
    Treat recursive protocols by using the 'assuming' structural subtype matrix
    (in sparse representation, i.e. as a list of pairs (subtype, supertype)),
    see also comment in nodes.TypeInfo. When we enter a check for classes
    (A, P), defined as following::

      class P(Protocol):
          def f(self) -&gt; P: ...
      class A:
          def f(self) -&gt; A: ...

    this results in A being a subtype of P without infinite recursion.
    On every false result, we pop the assumption, thus avoiding an infinite recursion
    as well.
    """
    assert right.type.is_protocol
    # We need to record this check to generate protocol fine-grained dependencies.
    TypeState.record_protocol_subtype_check(left.type, right.type)
    # nominal subtyping currently ignores '__init__' and '__new__' signatures
    members_not_to_check = {"__init__", "__new__"}
    # Trivial check that circumvents the bug described in issue 9771:
    if left.type.is_protocol:
        members_right = set(right.type.protocol_members) - members_not_to_check
        members_left = set(left.type.protocol_members) - members_not_to_check
        if not members_right.issubset(members_left):
            return False
    assuming = right.type.assuming_proper if proper_subtype else right.type.assuming
    for (l, r) in reversed(assuming):
        if l == left and r == right:
            return True
    with pop_on_exit(assuming, left, right):
        for member in right.type.protocol_members:
            if member in members_not_to_check:
                continue
            ignore_names = member != "__call__"  # __call__ can be passed kwargs
            # The third argument below indicates to what self type is bound.
            # We always bind self to the subtype. (Similarly to nominal types).
            supertype = get_proper_type(find_member(member, right, left))
            assert supertype is not None
            if member == "__call__" and class_obj:
                # Special case: class objects always have __call__ that is just the constructor.
                # TODO: move this helper function to typeops.py?
                import mypy.checkmember

                def named_type(fullname: str) -&gt; Instance:
                    return Instance(left.type.mro[-1], [])

                subtype: ProperType | None = mypy.checkmember.type_object_type(
                    left.type, named_type
                )
            else:
                subtype = get_proper_type(find_member(member, left, left, class_obj=class_obj))
            # Useful for debugging:
            # print(member, 'of', left, 'has type', subtype)
            # print(member, 'of', right, 'has type', supertype)
            if not subtype:
                return False
            if isinstance(subtype, PartialType):
                subtype = (
                    NoneType()
                    if subtype.type is None
                    else Instance(
                        subtype.type,
                        [AnyType(TypeOfAny.unannotated)] * len(subtype.type.type_vars),
                    )
                )
            if not proper_subtype:
                # Nominal check currently ignores arg names
                # NOTE: If we ever change this, be sure to also change the call to
                # SubtypeVisitor.build_subtype_kind(...) down below.
                is_compat = is_subtype(subtype, supertype, ignore_pos_arg_names=ignore_names)
            else:
                is_compat = is_proper_subtype(subtype, supertype)
            if not is_compat:
                return False
            if isinstance(subtype, NoneType) and isinstance(supertype, CallableType):
                # We want __hash__ = None idiom to work even without --strict-optional
                return False
            subflags = get_member_flags(member, left, class_obj=class_obj)
            superflags = get_member_flags(member, right)
            if IS_SETTABLE in superflags:
                # Check opposite direction for settable attributes.
                if not is_subtype(supertype, subtype):
                    return False
            if not class_obj:
                if (IS_CLASSVAR in subflags) != (IS_CLASSVAR in superflags):
                    return False
            else:
                if IS_VAR in superflags and IS_CLASSVAR not in subflags:
                    # Only class variables are allowed for class object access.
                    return False
                if IS_CLASSVAR in superflags:
                    # This can be never matched by a class object.
                    return False
            if IS_SETTABLE in superflags and IS_SETTABLE not in subflags:
                return False
            # This rule is copied from nominal check in checker.py
            if IS_CLASS_OR_STATIC in superflags and IS_CLASS_OR_STATIC not in subflags:
                return False

    if not proper_subtype:
        # Nominal check currently ignores arg names, but __call__ is special for protocols
        ignore_names = right.type.protocol_members != ["__call__"]
    else:
        ignore_names = False
    subtype_kind = SubtypeVisitor.build_subtype_kind(
        subtype_context=SubtypeContext(ignore_pos_arg_names=ignore_names),
        proper_subtype=proper_subtype,
    )
    TypeState.record_subtype_cache_entry(subtype_kind, left, right)
    return True


</t>
<t tx="ekr.20221004064035.2094">def find_member(
    name: str, itype: Instance, subtype: Type, is_operator: bool = False, class_obj: bool = False
) -&gt; Type | None:
    """Find the type of member by 'name' in 'itype's TypeInfo.

    Find the member type after applying type arguments from 'itype', and binding
    'self' to 'subtype'. Return None if member was not found.
    """
    # TODO: this code shares some logic with checkmember.analyze_member_access,
    # consider refactoring.
    info = itype.type
    method = info.get_method(name)
    if method:
        if isinstance(method, Decorator):
            return find_node_type(method.var, itype, subtype, class_obj=class_obj)
        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            return find_node_type(dec.var, itype, subtype, class_obj=class_obj)
        return find_node_type(method, itype, subtype, class_obj=class_obj)
    else:
        # don't have such method, maybe variable or decorator?
        node = info.get(name)
        v = node.node if node else None
        if isinstance(v, Var):
            return find_node_type(v, itype, subtype, class_obj=class_obj)
        if (
            not v
            and name not in ["__getattr__", "__setattr__", "__getattribute__"]
            and not is_operator
            and not class_obj
            and itype.extra_attrs is None  # skip ModuleType.__getattr__
        ):
            for method_name in ("__getattribute__", "__getattr__"):
                # Normally, mypy assumes that instances that define __getattr__ have all
                # attributes with the corresponding return type. If this will produce
                # many false negatives, then this could be prohibited for
                # structural subtyping.
                method = info.get_method(method_name)
                if method and method.info.fullname != "builtins.object":
                    if isinstance(method, Decorator):
                        getattr_type = get_proper_type(find_node_type(method.var, itype, subtype))
                    else:
                        getattr_type = get_proper_type(find_node_type(method, itype, subtype))
                    if isinstance(getattr_type, CallableType):
                        return getattr_type.ret_type
                    return getattr_type
        if itype.type.fallback_to_any:
            return AnyType(TypeOfAny.special_form)
        if isinstance(v, TypeInfo):
            # PEP 544 doesn't specify anything about such use cases. So we just try
            # to do something meaningful (at least we should not crash).
            return TypeType(fill_typevars_with_any(v))
    if itype.extra_attrs and name in itype.extra_attrs.attrs:
        return itype.extra_attrs.attrs[name]
    return None


</t>
<t tx="ekr.20221004064035.2095">def get_member_flags(name: str, itype: Instance, class_obj: bool = False) -&gt; set[int]:
    """Detect whether a member 'name' is settable, whether it is an
    instance or class variable, and whether it is class or static method.

    The flags are defined as following:
    * IS_SETTABLE: whether this attribute can be set, not set for methods and
      non-settable properties;
    * IS_CLASSVAR: set if the variable is annotated as 'x: ClassVar[t]';
    * IS_CLASS_OR_STATIC: set for methods decorated with @classmethod or
      with @staticmethod.
    """
    info = itype.type
    method = info.get_method(name)
    setattr_meth = info.get_method("__setattr__")
    if method:
        if isinstance(method, Decorator):
            if method.var.is_staticmethod or method.var.is_classmethod:
                return {IS_CLASS_OR_STATIC}
            elif method.var.is_property:
                return {IS_VAR}
        elif method.is_property:  # this could be settable property
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            if dec.var.is_settable_property or setattr_meth:
                return {IS_VAR, IS_SETTABLE}
            else:
                return {IS_VAR}
        return set()  # Just a regular method
    node = info.get(name)
    if not node:
        if setattr_meth:
            return {IS_SETTABLE}
        if itype.extra_attrs and name in itype.extra_attrs.attrs:
            flags = set()
            if name not in itype.extra_attrs.immutable:
                flags.add(IS_SETTABLE)
            return flags
        return set()
    v = node.node
    # just a variable
    if isinstance(v, Var):
        if v.is_property:
            return {IS_VAR}
        flags = {IS_VAR}
        if not v.is_final:
            flags.add(IS_SETTABLE)
        if v.is_classvar:
            flags.add(IS_CLASSVAR)
        if class_obj and v.is_inferred:
            flags.add(IS_CLASSVAR)
        return flags
    return set()


</t>
<t tx="ekr.20221004064035.2096">def find_node_type(
    node: Var | FuncBase, itype: Instance, subtype: Type, class_obj: bool = False
) -&gt; Type:
    """Find type of a variable or method 'node' (maybe also a decorated method).
    Apply type arguments from 'itype', and bind 'self' to 'subtype'.
    """
    from mypy.typeops import bind_self

    if isinstance(node, FuncBase):
        typ: Type | None = mypy.typeops.function_type(
            node, fallback=Instance(itype.type.mro[-1], [])
        )
    else:
        typ = node.type
    p_typ = get_proper_type(typ)
    if typ is None:
        return AnyType(TypeOfAny.from_error)
    # We don't need to bind 'self' for static methods, since there is no 'self'.
    if isinstance(node, FuncBase) or (
        isinstance(p_typ, FunctionLike)
        and node.is_initialized_in_class
        and not node.is_staticmethod
    ):
        assert isinstance(p_typ, FunctionLike)
        if class_obj and not (
            node.is_class if isinstance(node, FuncBase) else node.is_classmethod
        ):
            # Don't bind instance methods on class objects.
            signature = p_typ
        else:
            signature = bind_self(
                p_typ, subtype, is_classmethod=isinstance(node, Var) and node.is_classmethod
            )
        if node.is_property and not class_obj:
            assert isinstance(signature, CallableType)
            typ = signature.ret_type
        else:
            typ = signature
    itype = map_instance_to_supertype(itype, node.info)
    typ = expand_type_by_instance(typ, itype)
    return typ


</t>
<t tx="ekr.20221004064035.2097">def non_method_protocol_members(tp: TypeInfo) -&gt; list[str]:
    """Find all non-callable members of a protocol."""

    assert tp.is_protocol
    result: list[str] = []
    anytype = AnyType(TypeOfAny.special_form)
    instance = Instance(tp, [anytype] * len(tp.defn.type_vars))

    for member in tp.protocol_members:
        typ = get_proper_type(find_member(member, instance, instance))
        if not isinstance(typ, (Overloaded, CallableType)):
            result.append(member)
    return result


</t>
<t tx="ekr.20221004064035.2098">def is_callable_compatible(
    left: CallableType,
    right: CallableType,
    *,
    is_compat: Callable[[Type, Type], bool],
    is_compat_return: Callable[[Type, Type], bool] | None = None,
    ignore_return: bool = False,
    ignore_pos_arg_names: bool = False,
    check_args_covariantly: bool = False,
    allow_partial_overlap: bool = False,
    strict_concatenate: bool = False,
) -&gt; bool:
    """Is the left compatible with the right, using the provided compatibility check?

    is_compat:
        The check we want to run against the parameters.

    is_compat_return:
        The check we want to run against the return type.
        If None, use the 'is_compat' check.

    check_args_covariantly:
        If true, check if the left's args is compatible with the right's
        instead of the other way around (contravariantly).

        This function is mostly used to check if the left is a subtype of the right which
        is why the default is to check the args contravariantly. However, it's occasionally
        useful to check the args using some other check, so we leave the variance
        configurable.

        For example, when checking the validity of overloads, it's useful to see if
        the first overload alternative has more precise arguments then the second.
        We would want to check the arguments covariantly in that case.

        Note! The following two function calls are NOT equivalent:

            is_callable_compatible(f, g, is_compat=is_subtype, check_args_covariantly=False)
            is_callable_compatible(g, f, is_compat=is_subtype, check_args_covariantly=True)

        The two calls are similar in that they both check the function arguments in
        the same direction: they both run `is_subtype(argument_from_g, argument_from_f)`.

        However, the two calls differ in which direction they check things like
        keyword arguments. For example, suppose f and g are defined like so:

            def f(x: int, *y: int) -&gt; int: ...
            def g(x: int) -&gt; int: ...

        In this case, the first call will succeed and the second will fail: f is a
        valid stand-in for g but not vice-versa.

    allow_partial_overlap:
        By default this function returns True if and only if *all* calls to left are
        also calls to right (with respect to the provided 'is_compat' function).

        If this parameter is set to 'True', we return True if *there exists at least one*
        call to left that's also a call to right.

        In other words, we perform an existential check instead of a universal one;
        we require left to only overlap with right instead of being a subset.

        For example, suppose we set 'is_compat' to some subtype check and compare following:

            f(x: float, y: str = "...", *args: bool) -&gt; str
            g(*args: int) -&gt; str

        This function would normally return 'False': f is not a subtype of g.
        However, we would return True if this parameter is set to 'True': the two
        calls are compatible if the user runs "f_or_g(3)". In the context of that
        specific call, the two functions effectively have signatures of:

            f2(float) -&gt; str
            g2(int) -&gt; str

        Here, f2 is a valid subtype of g2 so we return True.

        Specifically, if this parameter is set this function will:

        -   Ignore optional arguments on either the left or right that have no
            corresponding match.
        -   No longer mandate optional arguments on either side are also optional
            on the other.
        -   No longer mandate that if right has a *arg or **kwarg that left must also
            have the same.

        Note: when this argument is set to True, this function becomes "symmetric" --
        the following calls are equivalent:

            is_callable_compatible(f, g,
                                   is_compat=some_check,
                                   check_args_covariantly=False,
                                   allow_partial_overlap=True)
            is_callable_compatible(g, f,
                                   is_compat=some_check,
                                   check_args_covariantly=True,
                                   allow_partial_overlap=True)

        If the 'some_check' function is also symmetric, the two calls would be equivalent
        whether or not we check the args covariantly.
    """
    # Normalize both types before comparing them.
    left = left.with_unpacked_kwargs()
    right = right.with_unpacked_kwargs()

    if is_compat_return is None:
        is_compat_return = is_compat

    # If either function is implicitly typed, ignore positional arg names too
    if left.implicit or right.implicit:
        ignore_pos_arg_names = True

    # Non-type cannot be a subtype of type.
    if right.is_type_obj() and not left.is_type_obj():
        return False

    # A callable L is a subtype of a generic callable R if L is a
    # subtype of every type obtained from R by substituting types for
    # the variables of R. We can check this by simply leaving the
    # generic variables of R as type variables, effectively varying
    # over all possible values.

    # It's okay even if these variables share ids with generic
    # type variables of L, because generating and solving
    # constraints for the variables of L to make L a subtype of R
    # (below) treats type variables on the two sides as independent.
    if left.variables:
        # Apply generic type variables away in left via type inference.
        unified = unify_generic_callable(left, right, ignore_return=ignore_return)
        if unified is None:
            return False
        else:
            left = unified

    # If we allow partial overlaps, we don't need to leave R generic:
    # if we can find even just a single typevar assignment which
    # would make these callables compatible, we should return True.

    # So, we repeat the above checks in the opposite direction. This also
    # lets us preserve the 'symmetry' property of allow_partial_overlap.
    if allow_partial_overlap and right.variables:
        unified = unify_generic_callable(right, left, ignore_return=ignore_return)
        if unified is not None:
            right = unified

    # Check return types.
    if not ignore_return and not is_compat_return(left.ret_type, right.ret_type):
        return False

    if check_args_covariantly:
        is_compat = flip_compat_check(is_compat)

    if not strict_concatenate and (left.from_concatenate or right.from_concatenate):
        strict_concatenate_check = False
    else:
        strict_concatenate_check = True

    return are_parameters_compatible(
        left,
        right,
        is_compat=is_compat,
        ignore_pos_arg_names=ignore_pos_arg_names,
        check_args_covariantly=check_args_covariantly,
        allow_partial_overlap=allow_partial_overlap,
        strict_concatenate_check=strict_concatenate_check,
    )


</t>
<t tx="ekr.20221004064035.2099">def are_parameters_compatible(
    left: Parameters | NormalizedCallableType,
    right: Parameters | NormalizedCallableType,
    *,
    is_compat: Callable[[Type, Type], bool],
    ignore_pos_arg_names: bool = False,
    check_args_covariantly: bool = False,
    allow_partial_overlap: bool = False,
    strict_concatenate_check: bool = True,
) -&gt; bool:
    """Helper function for is_callable_compatible, used for Parameter compatibility"""
    if right.is_ellipsis_args:
        return True

    left_star = left.var_arg()
    left_star2 = left.kw_arg()
    right_star = right.var_arg()
    right_star2 = right.kw_arg()

    # Treat "def _(*a: Any, **kw: Any) -&gt; X" similarly to "Callable[..., X]"
    if (
        right.arg_kinds == [ARG_STAR, ARG_STAR2]
        and right_star
        and isinstance(get_proper_type(right_star.typ), AnyType)
        and right_star2
        and isinstance(get_proper_type(right_star2.typ), AnyType)
    ):
        return True

    # Match up corresponding arguments and check them for compatibility. In
    # every pair (argL, argR) of corresponding arguments from L and R, argL must
    # be "more general" than argR if L is to be a subtype of R.

    # Arguments are corresponding if they either share a name, share a position,
    # or both. If L's corresponding argument is ambiguous, L is not a subtype of R.

    # If left has one corresponding argument by name and another by position,
    # consider them to be one "merged" argument (and not ambiguous) if they're
    # both optional, they're name-only and position-only respectively, and they
    # have the same type.  This rule allows functions with (*args, **kwargs) to
    # properly stand in for the full domain of formal arguments that they're
    # used for in practice.

    # Every argument in R must have a corresponding argument in L, and every
    # required argument in L must have a corresponding argument in R.

    # Phase 1: Confirm every argument in R has a corresponding argument in L.

    @others
    if _incompatible(left_star, right_star) or _incompatible(left_star2, right_star2):
        return False

    # Phase 1b: Check non-star args: for every arg right can accept, left must
    #           also accept. The only exception is if we are allowing partial
    #           partial overlaps: in that case, we ignore optional args on the right.
    for right_arg in right.formal_arguments():
        left_arg = mypy.typeops.callable_corresponding_argument(left, right_arg)
        if left_arg is None:
            if allow_partial_overlap and not right_arg.required:
                continue
            return False
        if not are_args_compatible(
            left_arg, right_arg, ignore_pos_arg_names, allow_partial_overlap, is_compat
        ):
            return False

    # Phase 1c: Check var args. Right has an infinite series of optional positional
    #           arguments. Get all further positional args of left, and make sure
    #           they're more general then the corresponding member in right.
    if right_star is not None:
        # Synthesize an anonymous formal argument for the right
        right_by_position = right.try_synthesizing_arg_from_vararg(None)
        assert right_by_position is not None

        i = right_star.pos
        assert i is not None
        while i &lt; len(left.arg_kinds) and left.arg_kinds[i].is_positional():
            if allow_partial_overlap and left.arg_kinds[i].is_optional():
                break

            left_by_position = left.argument_by_position(i)
            assert left_by_position is not None

            if not are_args_compatible(
                left_by_position,
                right_by_position,
                ignore_pos_arg_names,
                allow_partial_overlap,
                is_compat,
            ):
                return False
            i += 1

    # Phase 1d: Check kw args. Right has an infinite series of optional named
    #           arguments. Get all further named args of left, and make sure
    #           they're more general then the corresponding member in right.
    if right_star2 is not None:
        right_names = {name for name in right.arg_names if name is not None}
        left_only_names = set()
        for name, kind in zip(left.arg_names, left.arg_kinds):
            if (
                name is None
                or kind.is_star()
                or name in right_names
                or not strict_concatenate_check
            ):
                continue
            left_only_names.add(name)

        # Synthesize an anonymous formal argument for the right
        right_by_name = right.try_synthesizing_arg_from_kwarg(None)
        assert right_by_name is not None

        for name in left_only_names:
            left_by_name = left.argument_by_name(name)
            assert left_by_name is not None

            if allow_partial_overlap and not left_by_name.required:
                continue

            if not are_args_compatible(
                left_by_name, right_by_name, ignore_pos_arg_names, allow_partial_overlap, is_compat
            ):
                return False

    # Phase 2: Left must not impose additional restrictions.
    #          (Every required argument in L must have a corresponding argument in R)
    #          Note: we already checked the *arg and **kwarg arguments in phase 1a.
    for left_arg in left.formal_arguments():
        right_by_name = (
            right.argument_by_name(left_arg.name) if left_arg.name is not None else None
        )

        right_by_pos = (
            right.argument_by_position(left_arg.pos) if left_arg.pos is not None else None
        )

        # If the left hand argument corresponds to two right-hand arguments,
        # neither of them can be required.
        if (
            right_by_name is not None
            and right_by_pos is not None
            and right_by_name != right_by_pos
            and (right_by_pos.required or right_by_name.required)
            and strict_concatenate_check
        ):
            return False

        # All *required* left-hand arguments must have a corresponding
        # right-hand argument.  Optional args do not matter.
        if left_arg.required and right_by_pos is None and right_by_name is None:
            return False

    return True


</t>
<t tx="ekr.20221004064035.21">def join_instances_via_supertype(self, t: Instance, s: Instance) -&gt; ProperType:
    # Give preference to joins via duck typing relationship, so that
    # join(int, float) == float, for example.
    for p in t.type._promote:
        if is_subtype(p, s):
            return join_types(p, s, self)
    for p in s.type._promote:
        if is_subtype(p, t):
            return join_types(t, p, self)

    # Compute the "best" supertype of t when joined with s.
    # The definition of "best" may evolve; for now it is the one with
    # the longest MRO.  Ties are broken by using the earlier base.
    best: ProperType | None = None
    for base in t.type.bases:
        mapped = map_instance_to_supertype(t, base.type)
        res = self.join_instances(mapped, s)
        if best is None or is_better(res, best):
            best = res
    assert best is not None
    for promote in t.type._promote:
        promote = get_proper_type(promote)
        if isinstance(promote, Instance):
            res = self.join_instances(promote, s)
            if is_better(res, best):
                best = res
    return best


</t>
<t tx="ekr.20221004064035.210">#
# Helpers
#

</t>
<t tx="ekr.20221004064035.2100"># Phase 1a: If left and right can both accept an infinite number of args,
#           their types must be compatible.
#
#           Furthermore, if we're checking for compatibility in all cases,
#           we confirm that if R accepts an infinite number of arguments,
#           L must accept the same.
def _incompatible(left_arg: FormalArgument | None, right_arg: FormalArgument | None) -&gt; bool:
    if right_arg is None:
        return False
    if left_arg is None:
        return not allow_partial_overlap
    return not is_compat(right_arg.typ, left_arg.typ)

</t>
<t tx="ekr.20221004064035.2101">def are_args_compatible(
    left: FormalArgument,
    right: FormalArgument,
    ignore_pos_arg_names: bool,
    allow_partial_overlap: bool,
    is_compat: Callable[[Type, Type], bool],
) -&gt; bool:
    @others
    # If right has a specific name it wants this argument to be, left must
    # have the same.
    if is_different(left.name, right.name):
        # But pay attention to whether we're ignoring positional arg names
        if not ignore_pos_arg_names or right.pos is None:
            return False

    # If right is at a specific position, left must have the same:
    if is_different(left.pos, right.pos):
        return False

    # If right's argument is optional, left's must also be
    # (unless we're relaxing the checks to allow potential
    # rather then definite compatibility).
    if not allow_partial_overlap and not right.required and left.required:
        return False

    # If we're allowing partial overlaps and neither arg is required,
    # the types don't actually need to be the same
    if allow_partial_overlap and not left.required and not right.required:
        return True

    # Left must have a more general type
    return is_compat(right.typ, left.typ)


</t>
<t tx="ekr.20221004064035.2102">def is_different(left_item: object | None, right_item: object | None) -&gt; bool:
    """Checks if the left and right items are different.

    If the right item is unspecified (e.g. if the right callable doesn't care
    about what name or position its arg has), we default to returning False.

    If we're allowing partial overlap, we also default to returning False
    if the left callable also doesn't care."""
    if right_item is None:
        return False
    if allow_partial_overlap and left_item is None:
        return False
    return left_item != right_item

</t>
<t tx="ekr.20221004064035.2103">def flip_compat_check(is_compat: Callable[[Type, Type], bool]) -&gt; Callable[[Type, Type], bool]:
    def new_is_compat(left: Type, right: Type) -&gt; bool:
        return is_compat(right, left)

    return new_is_compat


</t>
<t tx="ekr.20221004064035.2104">def unify_generic_callable(
    type: NormalizedCallableType,
    target: NormalizedCallableType,
    ignore_return: bool,
    return_constraint_direction: int | None = None,
) -&gt; NormalizedCallableType | None:
    """Try to unify a generic callable type with another callable type.

    Return unified CallableType if successful; otherwise, return None.
    """
    import mypy.solve

    if return_constraint_direction is None:
        return_constraint_direction = mypy.constraints.SUBTYPE_OF

    constraints: list[mypy.constraints.Constraint] = []
    for arg_type, target_arg_type in zip(type.arg_types, target.arg_types):
        c = mypy.constraints.infer_constraints(
            arg_type, target_arg_type, mypy.constraints.SUPERTYPE_OF
        )
        constraints.extend(c)
    if not ignore_return:
        c = mypy.constraints.infer_constraints(
            type.ret_type, target.ret_type, return_constraint_direction
        )
        constraints.extend(c)
    type_var_ids = [tvar.id for tvar in type.variables]
    inferred_vars = mypy.solve.solve_constraints(type_var_ids, constraints)
    if None in inferred_vars:
        return None
    non_none_inferred_vars = cast(List[Type], inferred_vars)
    had_errors = False

    @others
    applied = mypy.applytype.apply_generic_arguments(
        type, non_none_inferred_vars, report, context=target
    )
    if had_errors:
        return None
    return cast(NormalizedCallableType, applied)


</t>
<t tx="ekr.20221004064035.2105">def report(*args: Any) -&gt; None:
    nonlocal had_errors
    had_errors = True

</t>
<t tx="ekr.20221004064035.2106">def try_restrict_literal_union(t: UnionType, s: Type) -&gt; list[Type] | None:
    """Return the items of t, excluding any occurrence of s, if and only if
      - t only contains simple literals
      - s is a simple literal

    Otherwise, returns None
    """
    ps = get_proper_type(s)
    if not mypy.typeops.is_simple_literal(ps):
        return None

    new_items: list[Type] = []
    for i in t.relevant_items():
        pi = get_proper_type(i)
        if not mypy.typeops.is_simple_literal(pi):
            return None
        if pi != ps:
            new_items.append(i)
    return new_items


</t>
<t tx="ekr.20221004064035.2107">def restrict_subtype_away(t: Type, s: Type) -&gt; Type:
    """Return t minus s for runtime type assertions.

    If we can't determine a precise result, return a supertype of the
    ideal result (just t is a valid result).

    This is used for type inference of runtime type checks such as
    isinstance(). Currently, this just removes elements of a union type.
    """
    p_t = get_proper_type(t)
    if isinstance(p_t, UnionType):
        new_items = try_restrict_literal_union(p_t, s)
        if new_items is None:
            new_items = [
                restrict_subtype_away(item, s)
                for item in p_t.relevant_items()
                if (isinstance(get_proper_type(item), AnyType) or not covers_at_runtime(item, s))
            ]
        return UnionType.make_union(new_items)
    elif covers_at_runtime(t, s):
        return UninhabitedType()
    else:
        return t


</t>
<t tx="ekr.20221004064035.2108">def covers_at_runtime(item: Type, supertype: Type) -&gt; bool:
    """Will isinstance(item, supertype) always return True at runtime?"""
    item = get_proper_type(item)
    supertype = get_proper_type(supertype)

    # Since runtime type checks will ignore type arguments, erase the types.
    supertype = erase_type(supertype)
    if is_proper_subtype(
        erase_type(item), supertype, ignore_promotions=True, erase_instances=True
    ):
        return True
    if isinstance(supertype, Instance) and supertype.type.is_protocol:
        # TODO: Implement more robust support for runtime isinstance() checks, see issue #3827.
        if is_proper_subtype(item, supertype, ignore_promotions=True):
            return True
    if isinstance(item, TypedDictType) and isinstance(supertype, Instance):
        # Special case useful for selecting TypedDicts from unions using isinstance(x, dict).
        if supertype.type.fullname == "builtins.dict":
            return True
    # TODO: Add more special cases.
    return False


</t>
<t tx="ekr.20221004064035.2109">def is_more_precise(left: Type, right: Type, *, ignore_promotions: bool = False) -&gt; bool:
    """Check if left is a more precise type than right.

    A left is a proper subtype of right, left is also more precise than
    right. Also, if right is Any, left is more precise than right, for
    any left.
    """
    # TODO Should List[int] be more precise than List[Any]?
    right = get_proper_type(right)
    if isinstance(right, AnyType):
        return True
    return is_proper_subtype(left, right, ignore_promotions=ignore_promotions)
</t>
<t tx="ekr.20221004064035.211">def filter_errors(
    self,
    *,
    filter_errors: bool | Callable[[str, ErrorInfo], bool] = True,
    save_filtered_errors: bool = False,
) -&gt; ErrorWatcher:
    return ErrorWatcher(
        self.errors, filter_errors=filter_errors, save_filtered_errors=save_filtered_errors
    )

</t>
<t tx="ekr.20221004064035.2110">@path C:/Repos/ekr-mypy2/mypy/
"""Mechanisms for inferring function types based on callsites.

Currently works by collecting all argument types at callsites,
synthesizing a list of possible function types from that, trying them
all, and picking the one with the fewest errors that we think is the
"best".

Can return JSON that pyannotate can use to apply the annotations to code.

There are a bunch of TODOs here:
 * Maybe want a way to surface the choices not selected??
 * We can generate an exponential number of type suggestions, and probably want
   a way to not always need to check them all.
 * Our heuristics for what types to try are primitive and not yet
   supported by real practice.
 * More!

Other things:
 * This is super brute force. Could we integrate with the typechecker
   more to understand more about what is going on?
 * Like something with tracking constraints/unification variables?
 * No understanding of type variables at *all*
"""

from __future__ import annotations

import itertools
import json
import os
from contextlib import contextmanager
from typing import Callable, Iterator, NamedTuple, TypeVar, cast
from typing_extensions import TypedDict

from mypy.argmap import map_actuals_to_formals
from mypy.build import Graph, State
from mypy.checkexpr import has_any_type
from mypy.find_sources import InvalidSourceList, SourceFinder
from mypy.join import join_type_list
from mypy.meet import meet_type_list
from mypy.modulefinder import PYTHON_EXTENSIONS
from mypy.nodes import (
    ARG_STAR,
    ARG_STAR2,
    ArgKind,
    CallExpr,
    Decorator,
    Expression,
    FuncDef,
    MypyFile,
    RefExpr,
    ReturnStmt,
    SymbolNode,
    SymbolTable,
    TypeInfo,
    reverse_builtin_aliases,
)
from mypy.plugin import FunctionContext, MethodContext, Plugin
from mypy.server.update import FineGrainedBuildManager
from mypy.state import state
from mypy.traverser import TraverserVisitor
from mypy.typeops import make_simplified_union
from mypy.types import (
    AnyType,
    CallableType,
    FunctionLike,
    Instance,
    NoneType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeStrVisitor,
    TypeTranslator,
    TypeVarType,
    UninhabitedType,
    UnionType,
    get_proper_type,
    is_optional,
    remove_optional,
)
from mypy.util import split_target


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2111">class PyAnnotateSignature(TypedDict):
    return_type: str
    arg_types: list[str]


</t>
<t tx="ekr.20221004064035.2112">class Callsite(NamedTuple):
    path: str
    line: int
    arg_kinds: list[list[ArgKind]]
    callee_arg_names: list[str | None]
    arg_names: list[list[str | None]]
    arg_types: list[list[Type]]


</t>
<t tx="ekr.20221004064035.2113">class SuggestionPlugin(Plugin):
    """Plugin that records all calls to a given target."""

    @others
</t>
<t tx="ekr.20221004064035.2114">def __init__(self, target: str) -&gt; None:
    if target.endswith((".__new__", ".__init__")):
        target = target.rsplit(".", 1)[0]

    self.target = target
    # List of call sites found by dmypy suggest:
    # (path, line, &lt;arg kinds&gt;, &lt;arg names&gt;, &lt;arg types&gt;)
    self.mystery_hits: list[Callsite] = []

</t>
<t tx="ekr.20221004064035.2115">def get_function_hook(self, fullname: str) -&gt; Callable[[FunctionContext], Type] | None:
    if fullname == self.target:
        return self.log
    else:
        return None

</t>
<t tx="ekr.20221004064035.2116">def get_method_hook(self, fullname: str) -&gt; Callable[[MethodContext], Type] | None:
    if fullname == self.target:
        return self.log
    else:
        return None

</t>
<t tx="ekr.20221004064035.2117">def log(self, ctx: FunctionContext | MethodContext) -&gt; Type:
    self.mystery_hits.append(
        Callsite(
            ctx.api.path,
            ctx.context.line,
            ctx.arg_kinds,
            ctx.callee_arg_names,
            ctx.arg_names,
            ctx.arg_types,
        )
    )
    return ctx.default_return_type


</t>
<t tx="ekr.20221004064035.2118"># NOTE: We could make this a bunch faster by implementing a StatementVisitor that skips
# traversing into expressions
class ReturnFinder(TraverserVisitor):
    """Visitor for finding all types returned from a function."""

    @others
</t>
<t tx="ekr.20221004064035.2119">def __init__(self, typemap: dict[Expression, Type]) -&gt; None:
    self.typemap = typemap
    self.return_types: list[Type] = []

</t>
<t tx="ekr.20221004064035.212">def add_errors(self, errors: list[ErrorInfo]) -&gt; None:
    """Add errors in messages to this builder."""
    for info in errors:
        self.errors.add_error_info(info)

</t>
<t tx="ekr.20221004064035.2120">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is not None and o.expr in self.typemap:
        self.return_types.append(self.typemap[o.expr])

</t>
<t tx="ekr.20221004064035.2121">def visit_func_def(self, o: FuncDef) -&gt; None:
    # Skip nested functions
    pass


</t>
<t tx="ekr.20221004064035.2122">def get_return_types(typemap: dict[Expression, Type], func: FuncDef) -&gt; list[Type]:
    """Find all the types returned by return statements in func."""
    finder = ReturnFinder(typemap)
    func.body.accept(finder)
    return finder.return_types


</t>
<t tx="ekr.20221004064035.2123">class ArgUseFinder(TraverserVisitor):
    """Visitor for finding all the types of arguments that each arg is passed to.

    This is extremely simple minded but might be effective anyways.
    """

    @others
</t>
<t tx="ekr.20221004064035.2124">def __init__(self, func: FuncDef, typemap: dict[Expression, Type]) -&gt; None:
    self.typemap = typemap
    self.arg_types: dict[SymbolNode, list[Type]] = {arg.variable: [] for arg in func.arguments}

</t>
<t tx="ekr.20221004064035.2125">def visit_call_expr(self, o: CallExpr) -&gt; None:
    if not any(isinstance(e, RefExpr) and e.node in self.arg_types for e in o.args):
        return

    typ = get_proper_type(self.typemap.get(o.callee))
    if not isinstance(typ, CallableType):
        return

    formal_to_actual = map_actuals_to_formals(
        o.arg_kinds,
        o.arg_names,
        typ.arg_kinds,
        typ.arg_names,
        lambda n: AnyType(TypeOfAny.special_form),
    )

    for i, args in enumerate(formal_to_actual):
        for arg_idx in args:
            arg = o.args[arg_idx]
            if isinstance(arg, RefExpr) and arg.node in self.arg_types:
                self.arg_types[arg.node].append(typ.arg_types[i])


</t>
<t tx="ekr.20221004064035.2126">def get_arg_uses(typemap: dict[Expression, Type], func: FuncDef) -&gt; list[list[Type]]:
    """Find all the types of arguments that each arg is passed to.

    For example, given
      def foo(x: int) -&gt; None: ...
      def bar(x: str) -&gt; None: ...
      def test(x, y):
          foo(x)
          bar(y)

    this will return [[int], [str]].
    """
    finder = ArgUseFinder(func, typemap)
    func.body.accept(finder)
    return [finder.arg_types[arg.variable] for arg in func.arguments]


</t>
<t tx="ekr.20221004064035.2127">class SuggestionFailure(Exception):
    pass


</t>
<t tx="ekr.20221004064035.2128">def is_explicit_any(typ: AnyType) -&gt; bool:
    # Originally I wanted to count as explicit anything derived from an explicit any, but that
    # seemed too strict in some testing.
    # return (typ.type_of_any == TypeOfAny.explicit
    #         or (typ.source_any is not None and typ.source_any.type_of_any == TypeOfAny.explicit))
    # Important question: what should we do with source_any stuff? Does that count?
    # And actually should explicit anys count at all?? Maybe not!
    return typ.type_of_any == TypeOfAny.explicit


</t>
<t tx="ekr.20221004064035.2129">def is_implicit_any(typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, AnyType) and not is_explicit_any(typ)


</t>
<t tx="ekr.20221004064035.213">@contextmanager
def disable_type_names(self) -&gt; Iterator[None]:
    self._disable_type_names.append(True)
    try:
        yield
    finally:
        self._disable_type_names.pop()

</t>
<t tx="ekr.20221004064035.2130">class SuggestionEngine:
    """Engine for finding call sites and suggesting signatures."""

    @others
</t>
<t tx="ekr.20221004064035.2131">def __init__(
    self,
    fgmanager: FineGrainedBuildManager,
    *,
    json: bool,
    no_errors: bool = False,
    no_any: bool = False,
    flex_any: float | None = None,
    use_fixme: str | None = None,
    max_guesses: int | None = None,
) -&gt; None:
    self.fgmanager = fgmanager
    self.manager = fgmanager.manager
    self.plugin = self.manager.plugin
    self.graph = fgmanager.graph
    self.finder = SourceFinder(self.manager.fscache, self.manager.options)

    self.give_json = json
    self.no_errors = no_errors
    self.flex_any = flex_any
    if no_any:
        self.flex_any = 1.0

    self.max_guesses = max_guesses or 64
    self.use_fixme = use_fixme

</t>
<t tx="ekr.20221004064035.2132">def suggest(self, function: str) -&gt; str:
    """Suggest an inferred type for function."""
    mod, func_name, node = self.find_node(function)

    with self.restore_after(mod):
        with self.with_export_types():
            suggestion = self.get_suggestion(mod, node)

    if self.give_json:
        return self.json_suggestion(mod, func_name, node, suggestion)
    else:
        return self.format_signature(suggestion)

</t>
<t tx="ekr.20221004064035.2133">def suggest_callsites(self, function: str) -&gt; str:
    """Find a list of call sites of function."""
    mod, _, node = self.find_node(function)
    with self.restore_after(mod):
        callsites, _ = self.get_callsites(node)

    return "\n".join(
        dedup(
            [
                f"{path}:{line}: {self.format_args(arg_kinds, arg_names, arg_types)}"
                for path, line, arg_kinds, _, arg_names, arg_types in callsites
            ]
        )
    )

</t>
<t tx="ekr.20221004064035.2134">@contextmanager
def restore_after(self, module: str) -&gt; Iterator[None]:
    """Context manager that reloads a module after executing the body.

    This should undo any damage done to the module state while mucking around.
    """
    try:
        yield
    finally:
        self.reload(self.graph[module])

</t>
<t tx="ekr.20221004064035.2135">@contextmanager
def with_export_types(self) -&gt; Iterator[None]:
    """Context manager that enables the export_types flag in the body.

    This causes type information to be exported into the manager's all_types variable.
    """
    old = self.manager.options.export_types
    self.manager.options.export_types = True
    try:
        yield
    finally:
        self.manager.options.export_types = old

</t>
<t tx="ekr.20221004064035.2136">def get_trivial_type(self, fdef: FuncDef) -&gt; CallableType:
    """Generate a trivial callable type from a func def, with all Anys"""
    # The Anys are marked as being from the suggestion engine
    # since they need some special treatment (specifically,
    # constraint generation ignores them.)
    return CallableType(
        [AnyType(TypeOfAny.suggestion_engine) for _ in fdef.arg_kinds],
        fdef.arg_kinds,
        fdef.arg_names,
        AnyType(TypeOfAny.suggestion_engine),
        self.named_type("builtins.function"),
    )

</t>
<t tx="ekr.20221004064035.2137">def get_starting_type(self, fdef: FuncDef) -&gt; CallableType:
    if isinstance(fdef.type, CallableType):
        return make_suggestion_anys(fdef.type)
    else:
        return self.get_trivial_type(fdef)

</t>
<t tx="ekr.20221004064035.2138">def get_args(
    self,
    is_method: bool,
    base: CallableType,
    defaults: list[Type | None],
    callsites: list[Callsite],
    uses: list[list[Type]],
) -&gt; list[list[Type]]:
    """Produce a list of type suggestions for each argument type."""
    types: list[list[Type]] = []
    for i in range(len(base.arg_kinds)):
        # Make self args Any but this will get overridden somewhere in the checker
        if i == 0 and is_method:
            types.append([AnyType(TypeOfAny.suggestion_engine)])
            continue

        all_arg_types = []
        for call in callsites:
            for typ in call.arg_types[i - is_method]:
                # Collect all the types except for implicit anys
                if not is_implicit_any(typ):
                    all_arg_types.append(typ)
        all_use_types = []
        for typ in uses[i]:
            # Collect all the types except for implicit anys
            if not is_implicit_any(typ):
                all_use_types.append(typ)
        # Add in any default argument types
        default = defaults[i]
        if default:
            all_arg_types.append(default)
            if all_use_types:
                all_use_types.append(default)

        arg_types = []

        if all_arg_types and all(
            isinstance(get_proper_type(tp), NoneType) for tp in all_arg_types
        ):
            arg_types.append(
                UnionType.make_union([all_arg_types[0], AnyType(TypeOfAny.explicit)])
            )
        elif all_arg_types:
            arg_types.extend(generate_type_combinations(all_arg_types))
        else:
            arg_types.append(AnyType(TypeOfAny.explicit))

        if all_use_types:
            # This is a meet because the type needs to be compatible with all the uses
            arg_types.append(meet_type_list(all_use_types))

        types.append(arg_types)
    return types

</t>
<t tx="ekr.20221004064035.2139">def get_default_arg_types(self, fdef: FuncDef) -&gt; list[Type | None]:
    return [
        self.manager.all_types[arg.initializer] if arg.initializer else None
        for arg in fdef.arguments
    ]

</t>
<t tx="ekr.20221004064035.214">def are_type_names_disabled(self) -&gt; bool:
    return len(self._disable_type_names) &gt; 0 and self._disable_type_names[-1]

</t>
<t tx="ekr.20221004064035.2140">def get_guesses(
    self,
    is_method: bool,
    base: CallableType,
    defaults: list[Type | None],
    callsites: list[Callsite],
    uses: list[list[Type]],
) -&gt; list[CallableType]:
    """Compute a list of guesses for a function's type.

    This focuses just on the argument types, and doesn't change the provided return type.
    """
    options = self.get_args(is_method, base, defaults, callsites, uses)

    # Take the first `max_guesses` guesses.
    product = itertools.islice(itertools.product(*options), 0, self.max_guesses)
    return [refine_callable(base, base.copy_modified(arg_types=list(x))) for x in product]

</t>
<t tx="ekr.20221004064035.2141">def get_callsites(self, func: FuncDef) -&gt; tuple[list[Callsite], list[str]]:
    """Find all call sites of a function."""
    new_type = self.get_starting_type(func)

    collector_plugin = SuggestionPlugin(func.fullname)

    self.plugin._plugins.insert(0, collector_plugin)
    try:
        errors = self.try_type(func, new_type)
    finally:
        self.plugin._plugins.pop(0)

    return collector_plugin.mystery_hits, errors

</t>
<t tx="ekr.20221004064035.2142">def filter_options(
    self, guesses: list[CallableType], is_method: bool, ignore_return: bool
) -&gt; list[CallableType]:
    """Apply any configured filters to the possible guesses.

    Currently the only option is filtering based on Any prevalance."""
    return [
        t
        for t in guesses
        if self.flex_any is None
        or any_score_callable(t, is_method, ignore_return) &gt;= self.flex_any
    ]

</t>
<t tx="ekr.20221004064035.2143">def find_best(self, func: FuncDef, guesses: list[CallableType]) -&gt; tuple[CallableType, int]:
    """From a list of possible function types, find the best one.

    For best, we want the fewest errors, then the best "score" from score_callable.
    """
    if not guesses:
        raise SuggestionFailure("No guesses that match criteria!")
    errors = {guess: self.try_type(func, guess) for guess in guesses}
    best = min(guesses, key=lambda s: (count_errors(errors[s]), self.score_callable(s)))
    return best, count_errors(errors[best])

</t>
<t tx="ekr.20221004064035.2144">def get_guesses_from_parent(self, node: FuncDef) -&gt; list[CallableType]:
    """Try to get a guess of a method type from a parent class."""
    if not node.info:
        return []

    for parent in node.info.mro[1:]:
        pnode = parent.names.get(node.name)
        if pnode and isinstance(pnode.node, (FuncDef, Decorator)):
            typ = get_proper_type(pnode.node.type)
            # FIXME: Doesn't work right with generic tyeps
            if isinstance(typ, CallableType) and len(typ.arg_types) == len(node.arguments):
                # Return the first thing we find, since it probably doesn't make sense
                # to grab things further up in the chain if an earlier parent has it.
                return [typ]

    return []

</t>
<t tx="ekr.20221004064035.2145">def get_suggestion(self, mod: str, node: FuncDef) -&gt; PyAnnotateSignature:
    """Compute a suggestion for a function.

    Return the type and whether the first argument should be ignored.
    """
    graph = self.graph
    callsites, orig_errors = self.get_callsites(node)
    uses = get_arg_uses(self.manager.all_types, node)

    if self.no_errors and orig_errors:
        raise SuggestionFailure("Function does not typecheck.")

    is_method = bool(node.info) and not node.is_static

    with state.strict_optional_set(graph[mod].options.strict_optional):
        guesses = self.get_guesses(
            is_method,
            self.get_starting_type(node),
            self.get_default_arg_types(node),
            callsites,
            uses,
        )
    guesses += self.get_guesses_from_parent(node)
    guesses = self.filter_options(guesses, is_method, ignore_return=True)
    best, _ = self.find_best(node, guesses)

    # Now try to find the return type!
    self.try_type(node, best)
    returns = get_return_types(self.manager.all_types, node)
    with state.strict_optional_set(graph[mod].options.strict_optional):
        if returns:
            ret_types = generate_type_combinations(returns)
        else:
            ret_types = [NoneType()]

    guesses = [best.copy_modified(ret_type=refine_type(best.ret_type, t)) for t in ret_types]
    guesses = self.filter_options(guesses, is_method, ignore_return=False)
    best, errors = self.find_best(node, guesses)

    if self.no_errors and errors:
        raise SuggestionFailure("No annotation without errors")

    return self.pyannotate_signature(mod, is_method, best)

</t>
<t tx="ekr.20221004064035.2146">def format_args(
    self,
    arg_kinds: list[list[ArgKind]],
    arg_names: list[list[str | None]],
    arg_types: list[list[Type]],
) -&gt; str:
    args: list[str] = []
    for i in range(len(arg_types)):
        for kind, name, typ in zip(arg_kinds[i], arg_names[i], arg_types[i]):
            arg = self.format_type(None, typ)
            if kind == ARG_STAR:
                arg = "*" + arg
            elif kind == ARG_STAR2:
                arg = "**" + arg
            elif kind.is_named():
                if name:
                    arg = f"{name}={arg}"
        args.append(arg)
    return f"({', '.join(args)})"

</t>
<t tx="ekr.20221004064035.2147">def find_node(self, key: str) -&gt; tuple[str, str, FuncDef]:
    """From a target name, return module/target names and the func def.

    The 'key' argument can be in one of two formats:
    * As the function full name, e.g., package.module.Cls.method
    * As the function location as file and line separated by column,
      e.g., path/to/file.py:42
    """
    # TODO: Also return OverloadedFuncDef -- currently these are ignored.
    node: SymbolNode | None = None
    if ":" in key:
        if key.count(":") &gt; 1:
            raise SuggestionFailure(
                "Malformed location for function: {}. Must be either"
                " package.module.Class.method or path/to/file.py:line".format(key)
            )
        file, line = key.split(":")
        if not line.isdigit():
            raise SuggestionFailure(f"Line number must be a number. Got {line}")
        line_number = int(line)
        modname, node = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1 :]  # add one to account for '.'
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure(f"Cannot find module for {key}")
        modname, tail = target
        node = self.find_node_by_module_and_name(modname, tail)

    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(f"Object {key} is a decorator we can't handle")

    if not isinstance(node, FuncDef):
        raise SuggestionFailure(f"Object {key} is not a function")

    return modname, tail, node

</t>
<t tx="ekr.20221004064035.2148">def find_node_by_module_and_name(self, modname: str, tail: str) -&gt; SymbolNode | None:
    """Find symbol node by module id and qualified name.

    Raise SuggestionFailure if can't find one.
    """
    tree = self.ensure_loaded(self.fgmanager.graph[modname])

    # N.B. This is reimplemented from update's lookup_target
    # basically just to produce better error messages.

    names: SymbolTable = tree.names

    # Look through any classes
    components = tail.split(".")
    for i, component in enumerate(components[:-1]):
        if component not in names:
            raise SuggestionFailure(
                "Unknown class {}.{}".format(modname, ".".join(components[: i + 1]))
            )
        node: SymbolNode | None = names[component].node
        if not isinstance(node, TypeInfo):
            raise SuggestionFailure(
                "Object {}.{} is not a class".format(modname, ".".join(components[: i + 1]))
            )
        names = node.names

    # Look for the actual function/method
    funcname = components[-1]
    if funcname not in names:
        key = modname + "." + tail
        raise SuggestionFailure(
            "Unknown {} {}".format("method" if len(components) &gt; 1 else "function", key)
        )
    return names[funcname].node

</t>
<t tx="ekr.20221004064035.2149">def find_node_by_file_and_line(self, file: str, line: int) -&gt; tuple[str, SymbolNode]:
    """Find symbol node by path to file and line number.

    Find the first function declared *before or on* the line number.

    Return module id and the node found. Raise SuggestionFailure if can't find one.
    """
    if not any(file.endswith(ext) for ext in PYTHON_EXTENSIONS):
        raise SuggestionFailure("Source file is not a Python file")
    try:
        modname, _ = self.finder.crawl_up(os.path.normpath(file))
    except InvalidSourceList as e:
        raise SuggestionFailure("Invalid source file name: " + file) from e
    if modname not in self.graph:
        raise SuggestionFailure("Unknown module: " + modname)
    # We must be sure about any edits in this file as this might affect the line numbers.
    tree = self.ensure_loaded(self.fgmanager.graph[modname], force=True)
    node: SymbolNode | None = None
    closest_line: int | None = None
    # TODO: Handle nested functions.
    for _, sym, _ in tree.local_definitions():
        if isinstance(sym.node, (FuncDef, Decorator)):
            sym_line = sym.node.line
        # TODO: add support for OverloadedFuncDef.
        else:
            continue

        # We want the closest function above the specified line
        if sym_line &lt;= line and (closest_line is None or sym_line &gt; closest_line):
            closest_line = sym_line
            node = sym.node
    if not node:
        raise SuggestionFailure(f"Cannot find a function at line {line}")
    return modname, node

</t>
<t tx="ekr.20221004064035.215">def report(
    self,
    msg: str,
    context: Context | None,
    severity: str,
    *,
    code: ErrorCode | None = None,
    file: str | None = None,
    origin: Context | None = None,
    offset: int = 0,
    allow_dups: bool = False,
) -&gt; None:
    """Report an error or note (unless disabled).

    Note that context controls where error is reported, while origin controls
    where # type: ignore comments have effect.
    """

    def span_from_context(ctx: Context) -&gt; tuple[int, int]:
        """This determines where a type: ignore for a given context has effect.

        Current logic is a bit tricky, to keep as much backwards compatibility as
        possible. We may reconsider this to always be a single line (or otherwise
        simplify it) when we drop Python 3.7.
        """
        if isinstance(ctx, (ClassDef, FuncDef)):
            return ctx.deco_line or ctx.line, ctx.line
        elif not isinstance(ctx, Expression):
            return ctx.line, ctx.line
        else:
            return ctx.line, ctx.end_line or ctx.line

    origin_span: tuple[int, int] | None
    if origin is not None:
        origin_span = span_from_context(origin)
    elif context is not None:
        origin_span = span_from_context(context)
    else:
        origin_span = None
    self.errors.report(
        context.get_line() if context else -1,
        context.get_column() if context else -1,
        msg,
        severity=severity,
        file=file,
        offset=offset,
        origin_span=origin_span,
        end_line=context.end_line if context else -1,
        end_column=context.end_column if context else -1,
        code=code,
        allow_dups=allow_dups,
    )

</t>
<t tx="ekr.20221004064035.2150">def extract_from_decorator(self, node: Decorator) -&gt; FuncDef | None:
    for dec in node.decorators:
        typ = None
        if isinstance(dec, RefExpr) and isinstance(dec.node, FuncDef):
            typ = dec.node.type
        elif (
            isinstance(dec, CallExpr)
            and isinstance(dec.callee, RefExpr)
            and isinstance(dec.callee.node, FuncDef)
            and isinstance(dec.callee.node.type, CallableType)
        ):
            typ = get_proper_type(dec.callee.node.type.ret_type)

        if not isinstance(typ, FunctionLike):
            return None
        for ct in typ.items:
            if not (
                len(ct.arg_types) == 1
                and isinstance(ct.arg_types[0], TypeVarType)
                and ct.arg_types[0] == ct.ret_type
            ):
                return None

    return node.func

</t>
<t tx="ekr.20221004064035.2151">def try_type(self, func: FuncDef, typ: ProperType) -&gt; list[str]:
    """Recheck a function while assuming it has type typ.

    Return all error messages.
    """
    old = func.unanalyzed_type
    # During reprocessing, unanalyzed_type gets copied to type (by aststrip).
    # We set type to None to ensure that the type always changes during
    # reprocessing.
    func.type = None
    func.unanalyzed_type = typ
    try:
        res = self.fgmanager.trigger(func.fullname)
        # if res:
        #     print('===', typ)
        #     print('\n'.join(res))
        return res
    finally:
        func.unanalyzed_type = old

</t>
<t tx="ekr.20221004064035.2152">def reload(self, state: State) -&gt; list[str]:
    """Recheck the module given by state."""
    assert state.path is not None
    self.fgmanager.flush_cache()
    return self.fgmanager.update([(state.id, state.path)], [])

</t>
<t tx="ekr.20221004064035.2153">def ensure_loaded(self, state: State, force: bool = False) -&gt; MypyFile:
    """Make sure that the module represented by state is fully loaded."""
    if not state.tree or state.tree.is_cache_skeleton or force:
        self.reload(state)
    assert state.tree is not None
    return state.tree

</t>
<t tx="ekr.20221004064035.2154">def named_type(self, s: str) -&gt; Instance:
    return self.manager.semantic_analyzer.named_type(s)

</t>
<t tx="ekr.20221004064035.2155">def json_suggestion(
    self, mod: str, func_name: str, node: FuncDef, suggestion: PyAnnotateSignature
) -&gt; str:
    """Produce a json blob for a suggestion suitable for application by pyannotate."""
    # pyannotate irritatingly drops class names for class and static methods
    if node.is_class or node.is_static:
        func_name = func_name.split(".", 1)[-1]

    # pyannotate works with either paths relative to where the
    # module is rooted or with absolute paths. We produce absolute
    # paths because it is simpler.
    path = os.path.abspath(self.graph[mod].xpath)

    obj = {
        "signature": suggestion,
        "line": node.line,
        "path": path,
        "func_name": func_name,
        "samples": 0,
    }
    return json.dumps([obj], sort_keys=True)

</t>
<t tx="ekr.20221004064035.2156">def pyannotate_signature(
    self, cur_module: str | None, is_method: bool, typ: CallableType
) -&gt; PyAnnotateSignature:
    """Format a callable type as a pyannotate dict"""
    start = int(is_method)
    return {
        "arg_types": [self.format_type(cur_module, t) for t in typ.arg_types[start:]],
        "return_type": self.format_type(cur_module, typ.ret_type),
    }

</t>
<t tx="ekr.20221004064035.2157">def format_signature(self, sig: PyAnnotateSignature) -&gt; str:
    """Format a callable type in a way suitable as an annotation... kind of"""
    return f"({', '.join(sig['arg_types'])}) -&gt; {sig['return_type']}"

</t>
<t tx="ekr.20221004064035.2158">def format_type(self, cur_module: str | None, typ: Type) -&gt; str:
    if self.use_fixme and isinstance(get_proper_type(typ), AnyType):
        return self.use_fixme
    return typ.accept(TypeFormatter(cur_module, self.graph))

</t>
<t tx="ekr.20221004064035.2159">def score_type(self, t: Type, arg_pos: bool) -&gt; int:
    """Generate a score for a type that we use to pick which type to use.

    Lower is better, prefer non-union/non-any types. Don't penalize optionals.
    """
    t = get_proper_type(t)
    if isinstance(t, AnyType):
        return 20
    if arg_pos and isinstance(t, NoneType):
        return 20
    if isinstance(t, UnionType):
        if any(isinstance(get_proper_type(x), AnyType) for x in t.items):
            return 20
        if any(has_any_type(x) for x in t.items):
            return 15
        if not is_optional(t):
            return 10
    if isinstance(t, CallableType) and (has_any_type(t) or is_tricky_callable(t)):
        return 10
    return 0

</t>
<t tx="ekr.20221004064035.216">def fail(
    self,
    msg: str,
    context: Context | None,
    *,
    code: ErrorCode | None = None,
    file: str | None = None,
    allow_dups: bool = False,
) -&gt; None:
    """Report an error message (unless disabled)."""
    self.report(msg, context, "error", code=code, file=file, allow_dups=allow_dups)

</t>
<t tx="ekr.20221004064035.2160">def score_callable(self, t: CallableType) -&gt; int:
    return sum(self.score_type(x, arg_pos=True) for x in t.arg_types) + self.score_type(
        t.ret_type, arg_pos=False
    )


</t>
<t tx="ekr.20221004064035.2161">def any_score_type(ut: Type, arg_pos: bool) -&gt; float:
    """Generate a very made up number representing the Anyness of a type.

    Higher is better, 1.0 is max
    """
    t = get_proper_type(ut)
    if isinstance(t, AnyType) and t.type_of_any != TypeOfAny.suggestion_engine:
        return 0
    if isinstance(t, NoneType) and arg_pos:
        return 0.5
    if isinstance(t, UnionType):
        if any(isinstance(get_proper_type(x), AnyType) for x in t.items):
            return 0.5
        if any(has_any_type(x) for x in t.items):
            return 0.25
    if isinstance(t, CallableType) and is_tricky_callable(t):
        return 0.5
    if has_any_type(t):
        return 0.5

    return 1.0


</t>
<t tx="ekr.20221004064035.2162">def any_score_callable(t: CallableType, is_method: bool, ignore_return: bool) -&gt; float:
    # Ignore the first argument of methods
    scores = [any_score_type(x, arg_pos=True) for x in t.arg_types[int(is_method) :]]
    # Return type counts twice (since it spreads type information), unless it is
    # None in which case it does not count at all. (Though it *does* still count
    # if there are no arguments.)
    if not isinstance(get_proper_type(t.ret_type), NoneType) or not scores:
        ret = 1.0 if ignore_return else any_score_type(t.ret_type, arg_pos=False)
        scores += [ret, ret]

    return sum(scores) / len(scores)


</t>
<t tx="ekr.20221004064035.2163">def is_tricky_callable(t: CallableType) -&gt; bool:
    """Is t a callable that we need to put a ... in for syntax reasons?"""
    return t.is_ellipsis_args or any(k.is_star() or k.is_named() for k in t.arg_kinds)


</t>
<t tx="ekr.20221004064035.2164">class TypeFormatter(TypeStrVisitor):
    """Visitor used to format types"""

    @others
</t>
<t tx="ekr.20221004064035.2165"># TODO: Probably a lot
def __init__(self, module: str | None, graph: Graph) -&gt; None:
    super().__init__()
    self.module = module
    self.graph = graph

</t>
<t tx="ekr.20221004064035.2166">def visit_any(self, t: AnyType) -&gt; str:
    if t.missing_import_name:
        return t.missing_import_name
    else:
        return "Any"

</t>
<t tx="ekr.20221004064035.2167">def visit_instance(self, t: Instance) -&gt; str:
    s = t.type.fullname or t.type.name or None
    if s is None:
        return "&lt;???&gt;"
    if s in reverse_builtin_aliases:
        s = reverse_builtin_aliases[s]

    mod_obj = split_target(self.graph, s)
    assert mod_obj
    mod, obj = mod_obj

    # If a class is imported into the current module, rewrite the reference
    # to point to the current module. This helps the annotation tool avoid
    # inserting redundant imports when a type has been reexported.
    if self.module:
        parts = obj.split(".")  # need to split the object part if it is a nested class
        tree = self.graph[self.module].tree
        if tree and parts[0] in tree.names:
            mod = self.module

    if (mod, obj) == ("builtins", "tuple"):
        mod, obj = "typing", "Tuple[" + t.args[0].accept(self) + ", ...]"
    elif t.args:
        obj += f"[{self.list_str(t.args)}]"

    if mod_obj == ("builtins", "unicode"):
        return "Text"
    elif mod == "builtins":
        return obj
    else:
        delim = "." if "." not in obj else ":"
        return mod + delim + obj

</t>
<t tx="ekr.20221004064035.2168">def visit_tuple_type(self, t: TupleType) -&gt; str:
    if t.partial_fallback and t.partial_fallback.type:
        fallback_name = t.partial_fallback.type.fullname
        if fallback_name != "builtins.tuple":
            return t.partial_fallback.accept(self)
    s = self.list_str(t.items)
    return f"Tuple[{s}]"

</t>
<t tx="ekr.20221004064035.2169">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; str:
    return "Any"

</t>
<t tx="ekr.20221004064035.217">def note(
    self,
    msg: str,
    context: Context,
    file: str | None = None,
    origin: Context | None = None,
    offset: int = 0,
    allow_dups: bool = False,
    *,
    code: ErrorCode | None = None,
) -&gt; None:
    """Report a note (unless disabled)."""
    self.report(
        msg,
        context,
        "note",
        file=file,
        origin=origin,
        offset=offset,
        allow_dups=allow_dups,
        code=code,
    )

</t>
<t tx="ekr.20221004064035.2170">def visit_typeddict_type(self, t: TypedDictType) -&gt; str:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20221004064035.2171">def visit_union_type(self, t: UnionType) -&gt; str:
    if len(t.items) == 2 and is_optional(t):
        return f"Optional[{remove_optional(t).accept(self)}]"
    else:
        return super().visit_union_type(t)

</t>
<t tx="ekr.20221004064035.2172">def visit_callable_type(self, t: CallableType) -&gt; str:
    # TODO: use extended callables?
    if is_tricky_callable(t):
        arg_str = "..."
    else:
        # Note: for default arguments, we just assume that they
        # are required.  This isn't right, but neither is the
        # other thing, and I suspect this will produce more better
        # results than falling back to `...`
        args = [typ.accept(self) for typ in t.arg_types]
        arg_str = f"[{', '.join(args)}]"

    return f"Callable[{arg_str}, {t.ret_type.accept(self)}]"


</t>
<t tx="ekr.20221004064035.2173">TType = TypeVar("TType", bound=Type)


</t>
<t tx="ekr.20221004064035.2174">def make_suggestion_anys(t: TType) -&gt; TType:
    """Make all anys in the type as coming from the suggestion engine.

    This keeps those Anys from influencing constraint generation,
    which allows us to do better when refining types.
    """
    return cast(TType, t.accept(MakeSuggestionAny()))


</t>
<t tx="ekr.20221004064035.2175">class MakeSuggestionAny(TypeTranslator):
    @others
</t>
<t tx="ekr.20221004064035.2176">def visit_any(self, t: AnyType) -&gt; Type:
    if not t.missing_import_name:
        return t.copy_modified(type_of_any=TypeOfAny.suggestion_engine)
    else:
        return t

</t>
<t tx="ekr.20221004064035.2177">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20221004064035.2178">def generate_type_combinations(types: list[Type]) -&gt; list[Type]:
    """Generate possible combinations of a list of types.

    mypy essentially supports two different ways to do this: joining the types
    and unioning the types. We try both.
    """
    joined_type = join_type_list(types)
    union_type = make_simplified_union(types)
    if joined_type == union_type:
        return [joined_type]
    else:
        return [joined_type, union_type]


</t>
<t tx="ekr.20221004064035.2179">def count_errors(msgs: list[str]) -&gt; int:
    return len([x for x in msgs if " error: " in x])


</t>
<t tx="ekr.20221004064035.218">def note_multiline(
    self,
    messages: str,
    context: Context,
    file: str | None = None,
    offset: int = 0,
    allow_dups: bool = False,
    code: ErrorCode | None = None,
) -&gt; None:
    """Report as many notes as lines in the message (unless disabled)."""
    for msg in messages.splitlines():
        self.report(
            msg, context, "note", file=file, offset=offset, allow_dups=allow_dups, code=code
        )

</t>
<t tx="ekr.20221004064035.2180">def refine_type(ti: Type, si: Type) -&gt; Type:
    """Refine `ti` by replacing Anys in it with information taken from `si`

    This basically works by, when the types have the same structure,
    traversing both of them in parallel and replacing Any on the left
    with whatever the type on the right is. If the types don't have the
    same structure (or aren't supported), the left type is chosen.

    For example:
      refine(Any, T) = T,  for all T
      refine(float, int) = float
      refine(List[Any], List[int]) = List[int]
      refine(Dict[int, Any], Dict[Any, int]) = Dict[int, int]
      refine(Tuple[int, Any], Tuple[Any, int]) = Tuple[int, int]

      refine(Callable[[Any], Any], Callable[[int], int]) = Callable[[int], int]
      refine(Callable[..., int], Callable[[int, float], Any]) = Callable[[int, float], int]

      refine(Optional[Any], int) = Optional[int]
      refine(Optional[Any], Optional[int]) = Optional[int]
      refine(Optional[Any], Union[int, str]) = Optional[Union[int, str]]
      refine(Optional[List[Any]], List[int]) = List[int]

    """
    t = get_proper_type(ti)
    s = get_proper_type(si)

    if isinstance(t, AnyType):
        # If s is also an Any, we return if it is a missing_import Any
        return t if isinstance(s, AnyType) and t.missing_import_name else s

    if isinstance(t, Instance) and isinstance(s, Instance) and t.type == s.type:
        return t.copy_modified(args=[refine_type(ta, sa) for ta, sa in zip(t.args, s.args)])

    if (
        isinstance(t, TupleType)
        and isinstance(s, TupleType)
        and t.partial_fallback == s.partial_fallback
        and len(t.items) == len(s.items)
    ):
        return t.copy_modified(items=[refine_type(ta, sa) for ta, sa in zip(t.items, s.items)])

    if isinstance(t, CallableType) and isinstance(s, CallableType):
        return refine_callable(t, s)

    if isinstance(t, UnionType):
        return refine_union(t, s)

    # TODO: Refining of builtins.tuple, Type?

    return t


</t>
<t tx="ekr.20221004064035.2181">def refine_union(t: UnionType, s: ProperType) -&gt; Type:
    """Refine a union type based on another type.

    This is done by refining every component of the union against the
    right hand side type (or every component of its union if it is
    one). If an element of the union is successfully refined, we drop it
    from the union in favor of the refined versions.
    """
    # Don't try to do any union refining if the types are already the
    # same.  This prevents things like refining Optional[Any] against
    # itself and producing None.
    if t == s:
        return t

    rhs_items = s.items if isinstance(s, UnionType) else [s]

    new_items = []
    for lhs in t.items:
        refined = False
        for rhs in rhs_items:
            new = refine_type(lhs, rhs)
            if new != lhs:
                new_items.append(new)
                refined = True
        if not refined:
            new_items.append(lhs)

    # Turn strict optional on when simplifying the union since we
    # don't want to drop Nones.
    with state.strict_optional_set(True):
        return make_simplified_union(new_items)


</t>
<t tx="ekr.20221004064035.2182">def refine_callable(t: CallableType, s: CallableType) -&gt; CallableType:
    """Refine a callable based on another.

    See comments for refine_type.
    """
    if t.fallback != s.fallback:
        return t

    if t.is_ellipsis_args and not is_tricky_callable(s):
        return s.copy_modified(ret_type=refine_type(t.ret_type, s.ret_type))

    if is_tricky_callable(t) or t.arg_kinds != s.arg_kinds:
        return t

    return t.copy_modified(
        arg_types=[refine_type(ta, sa) for ta, sa in zip(t.arg_types, s.arg_types)],
        ret_type=refine_type(t.ret_type, s.ret_type),
    )


</t>
<t tx="ekr.20221004064035.2183">T = TypeVar("T")


</t>
<t tx="ekr.20221004064035.2184">def dedup(old: list[T]) -&gt; list[T]:
    new: list[T] = []
    for x in old:
        if x not in new:
            new.append(x)
    return new
</t>
<t tx="ekr.20221004064035.2185">@path C:/Repos/ekr-mypy2/mypy/
"""Generic node traverser visitor"""

from __future__ import annotations

from mypy_extensions import mypyc_attr

from mypy.nodes import (
    REVEAL_TYPE,
    AssertStmt,
    AssertTypeExpr,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    CastExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    ContinueStmt,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    ExpressionStmt,
    FloatExpr,
    ForStmt,
    FuncBase,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    ParamSpecExpr,
    PassStmt,
    RaiseStmt,
    ReturnStmt,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    StrExpr,
    SuperExpr,
    TryStmt,
    TupleExpr,
    TypeAlias,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
)
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.visitor import NodeVisitor


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2186">@mypyc_attr(allow_interpreted_subclasses=True)
class TraverserVisitor(NodeVisitor[None]):
    """A parse tree visitor that traverses the parse tree during visiting.

    It does not perform any actions outside the traversal. Subclasses
    should override visit methods to perform actions during
    traversal. Calling the superclass method allows reusing the
    traversal implementation.
    """

    @others
</t>
<t tx="ekr.20221004064035.2187">def __init__(self) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.2188"># Visit methods

</t>
<t tx="ekr.20221004064035.2189">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    for d in o.defs:
        d.accept(self)

</t>
<t tx="ekr.20221004064035.219">#
# Specific operations
#

# The following operations are for generating specific error messages. They
# get some information as arguments, and they build an error message based
# on them.

</t>
<t tx="ekr.20221004064035.2190">def visit_block(self, block: Block) -&gt; None:
    for s in block.body:
        s.accept(self)

</t>
<t tx="ekr.20221004064035.2191">def visit_func(self, o: FuncItem) -&gt; None:
    if o.arguments is not None:
        for arg in o.arguments:
            init = arg.initializer
            if init is not None:
                init.accept(self)

        for arg in o.arguments:
            self.visit_var(arg.variable)

    o.body.accept(self)

</t>
<t tx="ekr.20221004064035.2192">def visit_func_def(self, o: FuncDef) -&gt; None:
    self.visit_func(o)

</t>
<t tx="ekr.20221004064035.2193">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    for item in o.items:
        item.accept(self)
    if o.impl:
        o.impl.accept(self)

</t>
<t tx="ekr.20221004064035.2194">def visit_class_def(self, o: ClassDef) -&gt; None:
    for d in o.decorators:
        d.accept(self)
    for base in o.base_type_exprs:
        base.accept(self)
    if o.metaclass:
        o.metaclass.accept(self)
    for v in o.keywords.values():
        v.accept(self)
    o.defs.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20221004064035.2195">def visit_decorator(self, o: Decorator) -&gt; None:
    o.func.accept(self)
    o.var.accept(self)
    for decorator in o.decorators:
        decorator.accept(self)

</t>
<t tx="ekr.20221004064035.2196">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2197">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    o.rvalue.accept(self)
    for l in o.lvalues:
        l.accept(self)

</t>
<t tx="ekr.20221004064035.2198">def visit_operator_assignment_stmt(self, o: OperatorAssignmentStmt) -&gt; None:
    o.rvalue.accept(self)
    o.lvalue.accept(self)

</t>
<t tx="ekr.20221004064035.2199">def visit_while_stmt(self, o: WhileStmt) -&gt; None:
    o.expr.accept(self)
    o.body.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20221004064035.22">def join_simple(declaration: Type | None, s: Type, t: Type) -&gt; ProperType:
    """Return a simple least upper bound given the declared type."""
    # TODO: check infinite recursion for aliases here?
    declaration = get_proper_type(declaration)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if (s.can_be_true, s.can_be_false) != (t.can_be_true, t.can_be_false):
        # if types are restricted in different ways, use the more general versions
        s = mypy.typeops.true_or_false(s)
        t = mypy.typeops.true_or_false(t)

    if isinstance(s, AnyType):
        return s

    if isinstance(s, ErasedType):
        return t

    if is_proper_subtype(s, t):
        return t

    if is_proper_subtype(t, s):
        return s

    if isinstance(declaration, UnionType):
        return mypy.typeops.make_simplified_union([s, t])

    if isinstance(s, NoneType) and not isinstance(t, NoneType):
        s, t = t, s

    if isinstance(s, UninhabitedType) and not isinstance(t, UninhabitedType):
        s, t = t, s

    # Meets/joins require callable type normalization.
    s, t = normalize_callables(s, t)

    value = t.accept(TypeJoinVisitor(s))
    if declaration is None or is_subtype(value, declaration):
        return value

    return declaration


</t>
<t tx="ekr.20221004064035.220">def has_no_attr(
    self,
    original_type: Type,
    typ: Type,
    member: str,
    context: Context,
    module_symbol_table: SymbolTable | None = None,
) -&gt; Type:
    """Report a missing or non-accessible member.

    original_type is the top-level type on which the error occurred.
    typ is the actual type that is missing the member. These can be
    different, e.g., in a union, original_type will be the union and typ
    will be the specific item in the union that does not have the member
    attribute.

    'module_symbol_table' is passed to this function if the type for which we
    are trying to get a member was originally a module. The SymbolTable allows
    us to look up and suggests attributes of the module since they are not
    directly available on original_type

    If member corresponds to an operator, use the corresponding operator
    name in the messages. Return type Any.
    """
    original_type = get_proper_type(original_type)
    typ = get_proper_type(typ)

    if isinstance(original_type, Instance) and original_type.type.has_readable_member(member):
        self.fail(f'Member "{member}" is not assignable', context)
    elif member == "__contains__":
        self.fail(
            f"Unsupported right operand type for in ({format_type(original_type)})",
            context,
            code=codes.OPERATOR,
        )
    elif member in op_methods.values():
        # Access to a binary operator member (e.g. _add). This case does
        # not handle indexing operations.
        for op, method in op_methods.items():
            if method == member:
                self.unsupported_left_operand(op, original_type, context)
                break
    elif member == "__neg__":
        self.fail(
            f"Unsupported operand type for unary - ({format_type(original_type)})",
            context,
            code=codes.OPERATOR,
        )
    elif member == "__pos__":
        self.fail(
            f"Unsupported operand type for unary + ({format_type(original_type)})",
            context,
            code=codes.OPERATOR,
        )
    elif member == "__invert__":
        self.fail(
            f"Unsupported operand type for ~ ({format_type(original_type)})",
            context,
            code=codes.OPERATOR,
        )
    elif member == "__getitem__":
        # Indexed get.
        # TODO: Fix this consistently in format_type
        if isinstance(original_type, CallableType) and original_type.is_type_obj():
            self.fail(
                "The type {} is not generic and not indexable".format(
                    format_type(original_type)
                ),
                context,
            )
        else:
            self.fail(
                f"Value of type {format_type(original_type)} is not indexable",
                context,
                code=codes.INDEX,
            )
    elif member == "__setitem__":
        # Indexed set.
        self.fail(
            "Unsupported target for indexed assignment ({})".format(
                format_type(original_type)
            ),
            context,
            code=codes.INDEX,
        )
    elif member == "__call__":
        if isinstance(original_type, Instance) and (
            original_type.type.fullname == "builtins.function"
        ):
            # "'function' not callable" is a confusing error message.
            # Explain that the problem is that the type of the function is not known.
            self.fail("Cannot call function of unknown type", context, code=codes.OPERATOR)
        else:
            self.fail(
                message_registry.NOT_CALLABLE.format(format_type(original_type)),
                context,
                code=codes.OPERATOR,
            )
    else:
        # The non-special case: a missing ordinary attribute.
        extra = ""
        if member == "__iter__":
            extra = " (not iterable)"
        elif member == "__aiter__":
            extra = " (not async iterable)"
        if not self.are_type_names_disabled():
            failed = False
            if isinstance(original_type, Instance) and original_type.type.names:
                alternatives = set(original_type.type.names.keys())

                if module_symbol_table is not None:
                    alternatives |= {key for key in module_symbol_table.keys()}

                # in some situations, the member is in the alternatives set
                # but since we're in this function, we shouldn't suggest it
                if member in alternatives:
                    alternatives.remove(member)

                matches = [m for m in COMMON_MISTAKES.get(member, []) if m in alternatives]
                matches.extend(best_matches(member, alternatives)[:3])
                if member == "__aiter__" and matches == ["__iter__"]:
                    matches = []  # Avoid misleading suggestion
                if matches:
                    self.fail(
                        '{} has no attribute "{}"; maybe {}?{}'.format(
                            format_type(original_type),
                            member,
                            pretty_seq(matches, "or"),
                            extra,
                        ),
                        context,
                        code=codes.ATTR_DEFINED,
                    )
                    failed = True
            if not failed:
                self.fail(
                    '{} has no attribute "{}"{}'.format(
                        format_type(original_type), member, extra
                    ),
                    context,
                    code=codes.ATTR_DEFINED,
                )
        elif isinstance(original_type, UnionType):
            # The checker passes "object" in lieu of "None" for attribute
            # checks, so we manually convert it back.
            typ_format, orig_type_format = format_type_distinctly(typ, original_type)
            if typ_format == '"object"' and any(
                type(item) == NoneType for item in original_type.items
            ):
                typ_format = '"None"'
            self.fail(
                'Item {} of {} has no attribute "{}"{}'.format(
                    typ_format, orig_type_format, member, extra
                ),
                context,
                code=codes.UNION_ATTR,
            )
        elif isinstance(original_type, TypeVarType):
            bound = get_proper_type(original_type.upper_bound)
            if isinstance(bound, UnionType):
                typ_fmt, bound_fmt = format_type_distinctly(typ, bound)
                original_type_fmt = format_type(original_type)
                self.fail(
                    "Item {} of the upper bound {} of type variable {} has no "
                    'attribute "{}"{}'.format(
                        typ_fmt, bound_fmt, original_type_fmt, member, extra
                    ),
                    context,
                    code=codes.UNION_ATTR,
                )
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064035.2200">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    o.index.accept(self)
    o.expr.accept(self)
    o.body.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20221004064035.2201">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2202">def visit_assert_stmt(self, o: AssertStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)
    if o.msg is not None:
        o.msg.accept(self)

</t>
<t tx="ekr.20221004064035.2203">def visit_del_stmt(self, o: DelStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2204">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    for e in o.expr:
        e.accept(self)
    for b in o.body:
        b.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20221004064035.2205">def visit_raise_stmt(self, o: RaiseStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)
    if o.from_expr is not None:
        o.from_expr.accept(self)

</t>
<t tx="ekr.20221004064035.2206">def visit_try_stmt(self, o: TryStmt) -&gt; None:
    o.body.accept(self)
    for i in range(len(o.types)):
        tp = o.types[i]
        if tp is not None:
            tp.accept(self)
        o.handlers[i].accept(self)
    for v in o.vars:
        if v is not None:
            v.accept(self)
    if o.else_body is not None:
        o.else_body.accept(self)
    if o.finally_body is not None:
        o.finally_body.accept(self)

</t>
<t tx="ekr.20221004064035.2207">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    for i in range(len(o.expr)):
        o.expr[i].accept(self)
        targ = o.target[i]
        if targ is not None:
            targ.accept(self)
    o.body.accept(self)

</t>
<t tx="ekr.20221004064035.2208">def visit_match_stmt(self, o: MatchStmt) -&gt; None:
    o.subject.accept(self)
    for i in range(len(o.patterns)):
        o.patterns[i].accept(self)
        guard = o.guards[i]
        if guard is not None:
            guard.accept(self)
        o.bodies[i].accept(self)

</t>
<t tx="ekr.20221004064035.2209">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.221">def unsupported_operand_types(
    self,
    op: str,
    left_type: Any,
    right_type: Any,
    context: Context,
    *,
    code: ErrorCode = codes.OPERATOR,
) -&gt; None:
    """Report unsupported operand types for a binary operation.

    Types can be Type objects or strings.
    """
    left_str = ""
    if isinstance(left_type, str):
        left_str = left_type
    else:
        left_str = format_type(left_type)

    right_str = ""
    if isinstance(right_type, str):
        right_str = right_type
    else:
        right_str = format_type(right_type)

    if self.are_type_names_disabled():
        msg = f"Unsupported operand types for {op} (likely involving Union)"
    else:
        msg = f"Unsupported operand types for {op} ({left_str} and {right_str})"
    self.fail(msg, context, code=code)

</t>
<t tx="ekr.20221004064035.2210">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2211">def visit_yield_expr(self, o: YieldExpr) -&gt; None:
    if o.expr:
        o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2212">def visit_call_expr(self, o: CallExpr) -&gt; None:
    for a in o.args:
        a.accept(self)
    o.callee.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20221004064035.2213">def visit_op_expr(self, o: OpExpr) -&gt; None:
    o.left.accept(self)
    o.right.accept(self)

</t>
<t tx="ekr.20221004064035.2214">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    for operand in o.operands:
        operand.accept(self)

</t>
<t tx="ekr.20221004064035.2215">def visit_slice_expr(self, o: SliceExpr) -&gt; None:
    if o.begin_index is not None:
        o.begin_index.accept(self)
    if o.end_index is not None:
        o.end_index.accept(self)
    if o.stride is not None:
        o.stride.accept(self)

</t>
<t tx="ekr.20221004064035.2216">def visit_cast_expr(self, o: CastExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2217">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2218">def visit_reveal_expr(self, o: RevealExpr) -&gt; None:
    if o.kind == REVEAL_TYPE:
        assert o.expr is not None
        o.expr.accept(self)
    else:
        # RevealLocalsExpr doesn't have an inner expression
        pass

</t>
<t tx="ekr.20221004064035.2219">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    o.target.accept(self)
    o.value.accept(self)

</t>
<t tx="ekr.20221004064035.222">def unsupported_left_operand(self, op: str, typ: Type, context: Context) -&gt; None:
    if self.are_type_names_disabled():
        msg = f"Unsupported left operand type for {op} (some union)"
    else:
        msg = f"Unsupported left operand type for {op} ({format_type(typ)})"
    self.fail(msg, context, code=codes.OPERATOR)

</t>
<t tx="ekr.20221004064035.2220">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2221">def visit_list_expr(self, o: ListExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20221004064035.2222">def visit_tuple_expr(self, o: TupleExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20221004064035.2223">def visit_dict_expr(self, o: DictExpr) -&gt; None:
    for k, v in o.items:
        if k is not None:
            k.accept(self)
        v.accept(self)

</t>
<t tx="ekr.20221004064035.2224">def visit_set_expr(self, o: SetExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20221004064035.2225">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    o.base.accept(self)
    o.index.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20221004064035.2226">def visit_generator_expr(self, o: GeneratorExpr) -&gt; None:
    for index, sequence, conditions in zip(o.indices, o.sequences, o.condlists):
        sequence.accept(self)
        index.accept(self)
        for cond in conditions:
            cond.accept(self)
    o.left_expr.accept(self)

</t>
<t tx="ekr.20221004064035.2227">def visit_dictionary_comprehension(self, o: DictionaryComprehension) -&gt; None:
    for index, sequence, conditions in zip(o.indices, o.sequences, o.condlists):
        sequence.accept(self)
        index.accept(self)
        for cond in conditions:
            cond.accept(self)
    o.key.accept(self)
    o.value.accept(self)

</t>
<t tx="ekr.20221004064035.2228">def visit_list_comprehension(self, o: ListComprehension) -&gt; None:
    o.generator.accept(self)

</t>
<t tx="ekr.20221004064035.2229">def visit_set_comprehension(self, o: SetComprehension) -&gt; None:
    o.generator.accept(self)

</t>
<t tx="ekr.20221004064035.223">def not_callable(self, typ: Type, context: Context) -&gt; Type:
    self.fail(message_registry.NOT_CALLABLE.format(format_type(typ)), context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064035.2230">def visit_conditional_expr(self, o: ConditionalExpr) -&gt; None:
    o.cond.accept(self)
    o.if_expr.accept(self)
    o.else_expr.accept(self)

</t>
<t tx="ekr.20221004064035.2231">def visit_type_application(self, o: TypeApplication) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2232">def visit_lambda_expr(self, o: LambdaExpr) -&gt; None:
    self.visit_func(o)

</t>
<t tx="ekr.20221004064035.2233">def visit_star_expr(self, o: StarExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2234">def visit_await_expr(self, o: AwaitExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2235">def visit_super_expr(self, o: SuperExpr) -&gt; None:
    o.call.accept(self)

</t>
<t tx="ekr.20221004064035.2236">def visit_as_pattern(self, o: AsPattern) -&gt; None:
    if o.pattern is not None:
        o.pattern.accept(self)
    if o.name is not None:
        o.name.accept(self)

</t>
<t tx="ekr.20221004064035.2237">def visit_or_pattern(self, o: OrPattern) -&gt; None:
    for p in o.patterns:
        p.accept(self)

</t>
<t tx="ekr.20221004064035.2238">def visit_value_pattern(self, o: ValuePattern) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20221004064035.2239">def visit_sequence_pattern(self, o: SequencePattern) -&gt; None:
    for p in o.patterns:
        p.accept(self)

</t>
<t tx="ekr.20221004064035.224">def untyped_function_call(self, callee: CallableType, context: Context) -&gt; Type:
    name = callable_name(callee) or "(unknown)"
    self.fail(
        f"Call to untyped function {name} in typed context",
        context,
        code=codes.NO_UNTYPED_CALL,
    )
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064035.2240">def visit_starred_pattern(self, o: StarredPattern) -&gt; None:
    if o.capture is not None:
        o.capture.accept(self)

</t>
<t tx="ekr.20221004064035.2241">def visit_mapping_pattern(self, o: MappingPattern) -&gt; None:
    for key in o.keys:
        key.accept(self)
    for value in o.values:
        value.accept(self)
    if o.rest is not None:
        o.rest.accept(self)

</t>
<t tx="ekr.20221004064035.2242">def visit_class_pattern(self, o: ClassPattern) -&gt; None:
    o.class_ref.accept(self)
    for p in o.positionals:
        p.accept(self)
    for v in o.keyword_values:
        v.accept(self)

</t>
<t tx="ekr.20221004064035.2243">def visit_import(self, o: Import) -&gt; None:
    for a in o.assignments:
        a.accept(self)

</t>
<t tx="ekr.20221004064035.2244">def visit_import_from(self, o: ImportFrom) -&gt; None:
    for a in o.assignments:
        a.accept(self)


</t>
<t tx="ekr.20221004064035.2245">class ExtendedTraverserVisitor(TraverserVisitor):
    """This is a more flexible traverser.

    In addition to the base traverser it:
        * has visit_ methods for leaf nodes
        * has common method that is called for all nodes
        * allows to skip recursing into a node

    Note that this traverser still doesn't visit some internal
    mypy constructs like _promote expression and Var.
    """

    @others</t>
<t tx="ekr.20221004064035.2247">def visit(self, o: Node) -&gt; bool:
    # If returns True, will continue to nested nodes.
    return True

</t>
<t tx="ekr.20221004064035.2248">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    if not self.visit(o):
        return
    super().visit_mypy_file(o)

</t>
<t tx="ekr.20221004064035.2249"># Module structure

</t>
<t tx="ekr.20221004064035.225">def incompatible_argument(
    self,
    n: int,
    m: int,
    callee: CallableType,
    arg_type: Type,
    arg_kind: ArgKind,
    object_type: Type | None,
    context: Context,
    outer_context: Context,
) -&gt; ErrorCode | None:
    """Report an error about an incompatible argument type.

    The argument type is arg_type, argument number is n and the
    callee type is 'callee'. If the callee represents a method
    that corresponds to an operator, use the corresponding
    operator name in the messages.

    Return the error code that used for the argument (multiple error
    codes are possible).
    """
    arg_type = get_proper_type(arg_type)

    target = ""
    callee_name = callable_name(callee)
    if callee_name is not None:
        name = callee_name
        if callee.bound_args and callee.bound_args[0] is not None:
            base = format_type(callee.bound_args[0])
        else:
            base = extract_type(name)

        for method, op in op_methods_to_symbols.items():
            for variant in method, "__r" + method[2:]:
                # FIX: do not rely on textual formatting
                if name.startswith(f'"{variant}" of'):
                    if op == "in" or variant != method:
                        # Reversed order of base/argument.
                        self.unsupported_operand_types(
                            op, arg_type, base, context, code=codes.OPERATOR
                        )
                    else:
                        self.unsupported_operand_types(
                            op, base, arg_type, context, code=codes.OPERATOR
                        )
                    return codes.OPERATOR

        if name.startswith('"__getitem__" of'):
            self.invalid_index_type(
                arg_type, callee.arg_types[n - 1], base, context, code=codes.INDEX
            )
            return codes.INDEX

        if name.startswith('"__setitem__" of'):
            if n == 1:
                self.invalid_index_type(
                    arg_type, callee.arg_types[n - 1], base, context, code=codes.INDEX
                )
                return codes.INDEX
            else:
                arg_type_str, callee_type_str = format_type_distinctly(
                    arg_type, callee.arg_types[n - 1]
                )
                info = (
                    f" (expression has type {arg_type_str}, "
                    f"target has type {callee_type_str})"
                )
                error_msg = (
                    message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT.with_additional_msg(info)
                )
                self.fail(error_msg.value, context, code=error_msg.code)
                return error_msg.code

        target = f"to {name} "

    msg = ""
    code = codes.MISC
    notes: list[str] = []
    if callee_name == "&lt;list&gt;":
        name = callee_name[1:-1]
        n -= 1
        actual_type_str, expected_type_str = format_type_distinctly(
            arg_type, callee.arg_types[0]
        )
        msg = "{} item {} has incompatible type {}; expected {}".format(
            name.title(), n, actual_type_str, expected_type_str
        )
        code = codes.LIST_ITEM
    elif callee_name == "&lt;dict&gt;":
        name = callee_name[1:-1]
        n -= 1
        key_type, value_type = cast(TupleType, arg_type).items
        expected_key_type, expected_value_type = cast(TupleType, callee.arg_types[0]).items

        # don't increase verbosity unless there is need to do so
        if is_subtype(key_type, expected_key_type):
            key_type_str = format_type(key_type)
            expected_key_type_str = format_type(expected_key_type)
        else:
            key_type_str, expected_key_type_str = format_type_distinctly(
                key_type, expected_key_type
            )
        if is_subtype(value_type, expected_value_type):
            value_type_str = format_type(value_type)
            expected_value_type_str = format_type(expected_value_type)
        else:
            value_type_str, expected_value_type_str = format_type_distinctly(
                value_type, expected_value_type
            )

        msg = "{} entry {} has incompatible type {}: {}; expected {}: {}".format(
            name.title(),
            n,
            key_type_str,
            value_type_str,
            expected_key_type_str,
            expected_value_type_str,
        )
        code = codes.DICT_ITEM
    elif callee_name == "&lt;list-comprehension&gt;":
        actual_type_str, expected_type_str = map(
            strip_quotes, format_type_distinctly(arg_type, callee.arg_types[0])
        )
        msg = "List comprehension has incompatible type List[{}]; expected List[{}]".format(
            actual_type_str, expected_type_str
        )
    elif callee_name == "&lt;set-comprehension&gt;":
        actual_type_str, expected_type_str = map(
            strip_quotes, format_type_distinctly(arg_type, callee.arg_types[0])
        )
        msg = "Set comprehension has incompatible type Set[{}]; expected Set[{}]".format(
            actual_type_str, expected_type_str
        )
    elif callee_name == "&lt;dictionary-comprehension&gt;":
        actual_type_str, expected_type_str = format_type_distinctly(
            arg_type, callee.arg_types[n - 1]
        )
        msg = (
            "{} expression in dictionary comprehension has incompatible type {}; "
            "expected type {}"
        ).format("Key" if n == 1 else "Value", actual_type_str, expected_type_str)
    elif callee_name == "&lt;generator&gt;":
        actual_type_str, expected_type_str = format_type_distinctly(
            arg_type, callee.arg_types[0]
        )
        msg = "Generator has incompatible item type {}; expected {}".format(
            actual_type_str, expected_type_str
        )
    else:
        try:
            expected_type = callee.arg_types[m - 1]
        except IndexError:  # Varargs callees
            expected_type = callee.arg_types[-1]
        arg_type_str, expected_type_str = format_type_distinctly(
            arg_type, expected_type, bare=True
        )
        if arg_kind == ARG_STAR:
            arg_type_str = "*" + arg_type_str
        elif arg_kind == ARG_STAR2:
            arg_type_str = "**" + arg_type_str

        # For function calls with keyword arguments, display the argument name rather than the
        # number.
        arg_label = str(n)
        if isinstance(outer_context, CallExpr) and len(outer_context.arg_names) &gt;= n:
            arg_name = outer_context.arg_names[n - 1]
            if arg_name is not None:
                arg_label = f'"{arg_name}"'
        if (
            arg_kind == ARG_STAR2
            and isinstance(arg_type, TypedDictType)
            and m &lt;= len(callee.arg_names)
            and callee.arg_names[m - 1] is not None
            and callee.arg_kinds[m - 1] != ARG_STAR2
        ):
            arg_name = callee.arg_names[m - 1]
            assert arg_name is not None
            arg_type_str, expected_type_str = format_type_distinctly(
                arg_type.items[arg_name], expected_type, bare=True
            )
            arg_label = f'"{arg_name}"'
        if isinstance(outer_context, IndexExpr) and isinstance(outer_context.index, StrExpr):
            msg = 'Value of "{}" has incompatible type {}; expected {}'.format(
                outer_context.index.value,
                quote_type_string(arg_type_str),
                quote_type_string(expected_type_str),
            )
        else:
            msg = "Argument {} {}has incompatible type {}; expected {}".format(
                arg_label,
                target,
                quote_type_string(arg_type_str),
                quote_type_string(expected_type_str),
            )
        object_type = get_proper_type(object_type)
        if isinstance(object_type, TypedDictType):
            code = codes.TYPEDDICT_ITEM
        else:
            code = codes.ARG_TYPE
        expected_type = get_proper_type(expected_type)
        if isinstance(expected_type, UnionType):
            expected_types = list(expected_type.items)
        else:
            expected_types = [expected_type]
        for type in get_proper_types(expected_types):
            if isinstance(arg_type, Instance) and isinstance(type, Instance):
                notes = append_invariance_notes(notes, arg_type, type)
    self.fail(msg, context, code=code)
    if notes:
        for note_msg in notes:
            self.note(note_msg, context, code=code)
    return code

</t>
<t tx="ekr.20221004064035.2250">def visit_import(self, o: Import) -&gt; None:
    if not self.visit(o):
        return
    super().visit_import(o)

</t>
<t tx="ekr.20221004064035.2251">def visit_import_from(self, o: ImportFrom) -&gt; None:
    if not self.visit(o):
        return
    super().visit_import_from(o)

</t>
<t tx="ekr.20221004064035.2252">def visit_import_all(self, o: ImportAll) -&gt; None:
    if not self.visit(o):
        return
    super().visit_import_all(o)

</t>
<t tx="ekr.20221004064035.2253"># Definitions

</t>
<t tx="ekr.20221004064035.2254">def visit_func_def(self, o: FuncDef) -&gt; None:
    if not self.visit(o):
        return
    super().visit_func_def(o)

</t>
<t tx="ekr.20221004064035.2255">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    if not self.visit(o):
        return
    super().visit_overloaded_func_def(o)

</t>
<t tx="ekr.20221004064035.2256">def visit_class_def(self, o: ClassDef) -&gt; None:
    if not self.visit(o):
        return
    super().visit_class_def(o)

</t>
<t tx="ekr.20221004064035.2257">def visit_global_decl(self, o: GlobalDecl) -&gt; None:
    if not self.visit(o):
        return
    super().visit_global_decl(o)

</t>
<t tx="ekr.20221004064035.2258">def visit_nonlocal_decl(self, o: NonlocalDecl) -&gt; None:
    if not self.visit(o):
        return
    super().visit_nonlocal_decl(o)

</t>
<t tx="ekr.20221004064035.2259">def visit_decorator(self, o: Decorator) -&gt; None:
    if not self.visit(o):
        return
    super().visit_decorator(o)

</t>
<t tx="ekr.20221004064035.226">def incompatible_argument_note(
    self,
    original_caller_type: ProperType,
    callee_type: ProperType,
    context: Context,
    code: ErrorCode | None,
) -&gt; None:
    if isinstance(
        original_caller_type, (Instance, TupleType, TypedDictType, TypeType, CallableType)
    ):
        if isinstance(callee_type, Instance) and callee_type.type.is_protocol:
            self.report_protocol_problems(
                original_caller_type, callee_type, context, code=code
            )
        if isinstance(callee_type, UnionType):
            for item in callee_type.items:
                item = get_proper_type(item)
                if isinstance(item, Instance) and item.type.is_protocol:
                    self.report_protocol_problems(
                        original_caller_type, item, context, code=code
                    )
    if isinstance(callee_type, CallableType) and isinstance(original_caller_type, Instance):
        call = find_member(
            "__call__", original_caller_type, original_caller_type, is_operator=True
        )
        if call:
            self.note_call(original_caller_type, call, context, code=code)

    self.maybe_note_concatenate_pos_args(original_caller_type, callee_type, context, code)

</t>
<t tx="ekr.20221004064035.2260">def visit_type_alias(self, o: TypeAlias) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_alias(o)

</t>
<t tx="ekr.20221004064035.2261"># Statements

</t>
<t tx="ekr.20221004064035.2262">def visit_block(self, block: Block) -&gt; None:
    if not self.visit(block):
        return
    super().visit_block(block)

</t>
<t tx="ekr.20221004064035.2263">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_expression_stmt(o)

</t>
<t tx="ekr.20221004064035.2264">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assignment_stmt(o)

</t>
<t tx="ekr.20221004064035.2265">def visit_operator_assignment_stmt(self, o: OperatorAssignmentStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_operator_assignment_stmt(o)

</t>
<t tx="ekr.20221004064035.2266">def visit_while_stmt(self, o: WhileStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_while_stmt(o)

</t>
<t tx="ekr.20221004064035.2267">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_for_stmt(o)

</t>
<t tx="ekr.20221004064035.2268">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_return_stmt(o)

</t>
<t tx="ekr.20221004064035.2269">def visit_assert_stmt(self, o: AssertStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assert_stmt(o)

</t>
<t tx="ekr.20221004064035.227">def maybe_note_concatenate_pos_args(
    self,
    original_caller_type: ProperType,
    callee_type: ProperType,
    context: Context,
    code: ErrorCode | None = None,
) -&gt; None:
    # pos-only vs positional can be confusing, with Concatenate
    if (
        isinstance(callee_type, CallableType)
        and isinstance(original_caller_type, CallableType)
        and (original_caller_type.from_concatenate or callee_type.from_concatenate)
    ):
        names: list[str] = []
        for c, o in zip(
            callee_type.formal_arguments(), original_caller_type.formal_arguments()
        ):
            if None in (c.pos, o.pos):
                # non-positional
                continue
            if c.name != o.name and c.name is None and o.name is not None:
                names.append(o.name)

        if names:
            missing_arguments = '"' + '", "'.join(names) + '"'
            self.note(
                f'This is likely because "{original_caller_type.name}" has named arguments: '
                f"{missing_arguments}. Consider marking them positional-only",
                context,
                code=code,
            )

</t>
<t tx="ekr.20221004064035.2270">def visit_del_stmt(self, o: DelStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_del_stmt(o)

</t>
<t tx="ekr.20221004064035.2271">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_if_stmt(o)

</t>
<t tx="ekr.20221004064035.2272">def visit_break_stmt(self, o: BreakStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_break_stmt(o)

</t>
<t tx="ekr.20221004064035.2273">def visit_continue_stmt(self, o: ContinueStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_continue_stmt(o)

</t>
<t tx="ekr.20221004064035.2274">def visit_pass_stmt(self, o: PassStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_pass_stmt(o)

</t>
<t tx="ekr.20221004064035.2275">def visit_raise_stmt(self, o: RaiseStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_raise_stmt(o)

</t>
<t tx="ekr.20221004064035.2276">def visit_try_stmt(self, o: TryStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_try_stmt(o)

</t>
<t tx="ekr.20221004064035.2277">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_with_stmt(o)

</t>
<t tx="ekr.20221004064035.2278">def visit_match_stmt(self, o: MatchStmt) -&gt; None:
    if not self.visit(o):
        return
    super().visit_match_stmt(o)

</t>
<t tx="ekr.20221004064035.2279"># Expressions (default no-op implementation)

</t>
<t tx="ekr.20221004064035.228">def invalid_index_type(
    self,
    index_type: Type,
    expected_type: Type,
    base_str: str,
    context: Context,
    *,
    code: ErrorCode,
) -&gt; None:
    index_str, expected_str = format_type_distinctly(index_type, expected_type)
    self.fail(
        "Invalid index type {} for {}; expected type {}".format(
            index_str, base_str, expected_str
        ),
        context,
        code=code,
    )

</t>
<t tx="ekr.20221004064035.2280">def visit_int_expr(self, o: IntExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_int_expr(o)

</t>
<t tx="ekr.20221004064035.2281">def visit_str_expr(self, o: StrExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_str_expr(o)

</t>
<t tx="ekr.20221004064035.2282">def visit_bytes_expr(self, o: BytesExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_bytes_expr(o)

</t>
<t tx="ekr.20221004064035.2283">def visit_float_expr(self, o: FloatExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_float_expr(o)

</t>
<t tx="ekr.20221004064035.2284">def visit_complex_expr(self, o: ComplexExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_complex_expr(o)

</t>
<t tx="ekr.20221004064035.2285">def visit_ellipsis(self, o: EllipsisExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_ellipsis(o)

</t>
<t tx="ekr.20221004064035.2286">def visit_star_expr(self, o: StarExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_star_expr(o)

</t>
<t tx="ekr.20221004064035.2287">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_name_expr(o)

</t>
<t tx="ekr.20221004064035.2288">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_member_expr(o)

</t>
<t tx="ekr.20221004064035.2289">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_yield_from_expr(o)

</t>
<t tx="ekr.20221004064035.229">def too_few_arguments(
    self, callee: CallableType, context: Context, argument_names: Sequence[str | None] | None
) -&gt; None:
    if argument_names is not None:
        num_positional_args = sum(k is None for k in argument_names)
        arguments_left = callee.arg_names[num_positional_args : callee.min_args]
        diff = [k for k in arguments_left if k not in argument_names]
        if len(diff) == 1:
            msg = "Missing positional argument"
        else:
            msg = "Missing positional arguments"
        callee_name = callable_name(callee)
        if callee_name is not None and diff and all(d is not None for d in diff):
            args = '", "'.join(cast(List[str], diff))
            msg += f' "{args}" in call to {callee_name}'
        else:
            msg = "Too few arguments" + for_function(callee)

    else:
        msg = "Too few arguments" + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)

</t>
<t tx="ekr.20221004064035.2290">def visit_yield_expr(self, o: YieldExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_yield_expr(o)

</t>
<t tx="ekr.20221004064035.2291">def visit_call_expr(self, o: CallExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_call_expr(o)

</t>
<t tx="ekr.20221004064035.2292">def visit_op_expr(self, o: OpExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_op_expr(o)

</t>
<t tx="ekr.20221004064035.2293">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_comparison_expr(o)

</t>
<t tx="ekr.20221004064035.2294">def visit_cast_expr(self, o: CastExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_cast_expr(o)

</t>
<t tx="ekr.20221004064035.2295">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assert_type_expr(o)

</t>
<t tx="ekr.20221004064035.2296">def visit_reveal_expr(self, o: RevealExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_reveal_expr(o)

</t>
<t tx="ekr.20221004064035.2297">def visit_super_expr(self, o: SuperExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_super_expr(o)

</t>
<t tx="ekr.20221004064035.2298">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_assignment_expr(o)

</t>
<t tx="ekr.20221004064035.2299">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_unary_expr(o)

</t>
<t tx="ekr.20221004064035.23">def trivial_join(s: Type, t: Type) -&gt; ProperType:
    """Return one of types (expanded) if it is a supertype of other, otherwise top type."""
    if is_subtype(s, t):
        return get_proper_type(t)
    elif is_subtype(t, s):
        return get_proper_type(s)
    else:
        return object_or_any_from_type(get_proper_type(t))


</t>
<t tx="ekr.20221004064035.230">def missing_named_argument(self, callee: CallableType, context: Context, name: str) -&gt; None:
    msg = f'Missing named argument "{name}"' + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)

</t>
<t tx="ekr.20221004064035.2300">def visit_list_expr(self, o: ListExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_list_expr(o)

</t>
<t tx="ekr.20221004064035.2301">def visit_dict_expr(self, o: DictExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_dict_expr(o)

</t>
<t tx="ekr.20221004064035.2302">def visit_tuple_expr(self, o: TupleExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_tuple_expr(o)

</t>
<t tx="ekr.20221004064035.2303">def visit_set_expr(self, o: SetExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_set_expr(o)

</t>
<t tx="ekr.20221004064035.2304">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_index_expr(o)

</t>
<t tx="ekr.20221004064035.2305">def visit_type_application(self, o: TypeApplication) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_application(o)

</t>
<t tx="ekr.20221004064035.2306">def visit_lambda_expr(self, o: LambdaExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_lambda_expr(o)

</t>
<t tx="ekr.20221004064035.2307">def visit_list_comprehension(self, o: ListComprehension) -&gt; None:
    if not self.visit(o):
        return
    super().visit_list_comprehension(o)

</t>
<t tx="ekr.20221004064035.2308">def visit_set_comprehension(self, o: SetComprehension) -&gt; None:
    if not self.visit(o):
        return
    super().visit_set_comprehension(o)

</t>
<t tx="ekr.20221004064035.2309">def visit_dictionary_comprehension(self, o: DictionaryComprehension) -&gt; None:
    if not self.visit(o):
        return
    super().visit_dictionary_comprehension(o)

</t>
<t tx="ekr.20221004064035.231">def too_many_arguments(self, callee: CallableType, context: Context) -&gt; None:
    msg = "Too many arguments" + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)
    self.maybe_note_about_special_args(callee, context)

</t>
<t tx="ekr.20221004064035.2310">def visit_generator_expr(self, o: GeneratorExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_generator_expr(o)

</t>
<t tx="ekr.20221004064035.2311">def visit_slice_expr(self, o: SliceExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_slice_expr(o)

</t>
<t tx="ekr.20221004064035.2312">def visit_conditional_expr(self, o: ConditionalExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_conditional_expr(o)

</t>
<t tx="ekr.20221004064035.2313">def visit_type_var_expr(self, o: TypeVarExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_var_expr(o)

</t>
<t tx="ekr.20221004064035.2314">def visit_paramspec_expr(self, o: ParamSpecExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_paramspec_expr(o)

</t>
<t tx="ekr.20221004064035.2315">def visit_type_var_tuple_expr(self, o: TypeVarTupleExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_var_tuple_expr(o)

</t>
<t tx="ekr.20221004064035.2316">def visit_type_alias_expr(self, o: TypeAliasExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_type_alias_expr(o)

</t>
<t tx="ekr.20221004064035.2317">def visit_namedtuple_expr(self, o: NamedTupleExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_namedtuple_expr(o)

</t>
<t tx="ekr.20221004064035.2318">def visit_enum_call_expr(self, o: EnumCallExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_enum_call_expr(o)

</t>
<t tx="ekr.20221004064035.2319">def visit_typeddict_expr(self, o: TypedDictExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_typeddict_expr(o)

</t>
<t tx="ekr.20221004064035.232">def too_many_arguments_from_typed_dict(
    self, callee: CallableType, arg_type: TypedDictType, context: Context
) -&gt; None:
    # Try to determine the name of the extra argument.
    for key in arg_type.items:
        if key not in callee.arg_names:
            msg = f'Extra argument "{key}" from **args' + for_function(callee)
            break
    else:
        self.too_many_arguments(callee, context)
        return
    self.fail(msg, context)

</t>
<t tx="ekr.20221004064035.2320">def visit_newtype_expr(self, o: NewTypeExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_newtype_expr(o)

</t>
<t tx="ekr.20221004064035.2321">def visit_await_expr(self, o: AwaitExpr) -&gt; None:
    if not self.visit(o):
        return
    super().visit_await_expr(o)

</t>
<t tx="ekr.20221004064035.2322"># Patterns

</t>
<t tx="ekr.20221004064035.2323">def visit_as_pattern(self, o: AsPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_as_pattern(o)

</t>
<t tx="ekr.20221004064035.2324">def visit_or_pattern(self, o: OrPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_or_pattern(o)

</t>
<t tx="ekr.20221004064035.2325">def visit_value_pattern(self, o: ValuePattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_value_pattern(o)

</t>
<t tx="ekr.20221004064035.2326">def visit_singleton_pattern(self, o: SingletonPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_singleton_pattern(o)

</t>
<t tx="ekr.20221004064035.2327">def visit_sequence_pattern(self, o: SequencePattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_sequence_pattern(o)

</t>
<t tx="ekr.20221004064035.2328">def visit_starred_pattern(self, o: StarredPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_starred_pattern(o)

</t>
<t tx="ekr.20221004064035.2329">def visit_mapping_pattern(self, o: MappingPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_mapping_pattern(o)

</t>
<t tx="ekr.20221004064035.233">def too_many_positional_arguments(self, callee: CallableType, context: Context) -&gt; None:
    msg = "Too many positional arguments" + for_function(callee)
    self.fail(msg, context)
    self.maybe_note_about_special_args(callee, context)

</t>
<t tx="ekr.20221004064035.2330">def visit_class_pattern(self, o: ClassPattern) -&gt; None:
    if not self.visit(o):
        return
    super().visit_class_pattern(o)


</t>
<t tx="ekr.20221004064035.2331">class ReturnSeeker(TraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.2332">def __init__(self) -&gt; None:
    self.found = False

</t>
<t tx="ekr.20221004064035.2333">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is None or isinstance(o.expr, NameExpr) and o.expr.name == "None":
        return
    self.found = True


</t>
<t tx="ekr.20221004064035.2334">def has_return_statement(fdef: FuncBase) -&gt; bool:
    """Find if a function has a non-trivial return statement.

    Plain 'return' and 'return None' don't count.
    """
    seeker = ReturnSeeker()
    fdef.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20221004064035.2335">class FuncCollectorBase(TraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.2336">def __init__(self) -&gt; None:
    self.inside_func = False

</t>
<t tx="ekr.20221004064035.2337">def visit_func_def(self, defn: FuncDef) -&gt; None:
    if not self.inside_func:
        self.inside_func = True
        super().visit_func_def(defn)
        self.inside_func = False


</t>
<t tx="ekr.20221004064035.2338">class YieldSeeker(FuncCollectorBase):
    def __init__(self) -&gt; None:
        super().__init__()
        self.found = False

    def visit_yield_expr(self, o: YieldExpr) -&gt; None:
        self.found = True


</t>
<t tx="ekr.20221004064035.2339">def has_yield_expression(fdef: FuncBase) -&gt; bool:
    seeker = YieldSeeker()
    fdef.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20221004064035.234">def maybe_note_about_special_args(self, callee: CallableType, context: Context) -&gt; None:
    # https://github.com/python/mypy/issues/11309
    first_arg = callee.def_extras.get("first_arg")
    if first_arg and first_arg not in {"self", "cls", "mcs"}:
        self.note(
            "Looks like the first special argument in a method "
            'is not named "self", "cls", or "mcs", '
            "maybe it is missing?",
            context,
        )

</t>
<t tx="ekr.20221004064035.2340">class AwaitSeeker(TraverserVisitor):
    def __init__(self) -&gt; None:
        super().__init__()
        self.found = False

    def visit_await_expr(self, o: AwaitExpr) -&gt; None:
        self.found = True


</t>
<t tx="ekr.20221004064035.2341">def has_await_expression(expr: Expression) -&gt; bool:
    seeker = AwaitSeeker()
    expr.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20221004064035.2342">class ReturnCollector(FuncCollectorBase):
    def __init__(self) -&gt; None:
        super().__init__()
        self.return_statements: list[ReturnStmt] = []

    def visit_return_stmt(self, stmt: ReturnStmt) -&gt; None:
        self.return_statements.append(stmt)


</t>
<t tx="ekr.20221004064035.2343">def all_return_statements(node: Node) -&gt; list[ReturnStmt]:
    v = ReturnCollector()
    node.accept(v)
    return v.return_statements


</t>
<t tx="ekr.20221004064035.2344">class YieldCollector(FuncCollectorBase):
    @others
</t>
<t tx="ekr.20221004064035.2345">def __init__(self) -&gt; None:
    super().__init__()
    self.in_assignment = False
    self.yield_expressions: list[tuple[YieldExpr, bool]] = []

</t>
<t tx="ekr.20221004064035.2346">def visit_assignment_stmt(self, stmt: AssignmentStmt) -&gt; None:
    self.in_assignment = True
    super().visit_assignment_stmt(stmt)
    self.in_assignment = False

</t>
<t tx="ekr.20221004064035.2347">def visit_yield_expr(self, expr: YieldExpr) -&gt; None:
    self.yield_expressions.append((expr, self.in_assignment))


</t>
<t tx="ekr.20221004064035.2348">def all_yield_expressions(node: Node) -&gt; list[tuple[YieldExpr, bool]]:
    v = YieldCollector()
    node.accept(v)
    return v.yield_expressions
</t>
<t tx="ekr.20221004064035.2349">@path C:/Repos/ekr-mypy2/mypy/
"""Base visitor that implements an identity AST transform.

Subclass TransformVisitor to perform non-trivial transformations.
"""

from __future__ import annotations

from typing import Iterable, Optional, cast

from mypy.nodes import (
    GDEF,
    REVEAL_TYPE,
    Argument,
    AssertStmt,
    AssertTypeExpr,
    AssignmentExpr,
    AssignmentStmt,
    AwaitExpr,
    Block,
    BreakStmt,
    BytesExpr,
    CallExpr,
    CastExpr,
    ClassDef,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    ContinueStmt,
    Decorator,
    DelStmt,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    ExpressionStmt,
    FloatExpr,
    ForStmt,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    GlobalDecl,
    IfStmt,
    Import,
    ImportAll,
    ImportFrom,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MatchStmt,
    MemberExpr,
    MypyFile,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    Node,
    NonlocalDecl,
    OperatorAssignmentStmt,
    OpExpr,
    OverloadedFuncDef,
    OverloadPart,
    ParamSpecExpr,
    PassStmt,
    PromoteExpr,
    RaiseStmt,
    RefExpr,
    ReturnStmt,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    Statement,
    StrExpr,
    SuperExpr,
    SymbolTable,
    TempNode,
    TryStmt,
    TupleExpr,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    WhileStmt,
    WithStmt,
    YieldExpr,
    YieldFromExpr,
)
from mypy.patterns import (
    AsPattern,
    ClassPattern,
    MappingPattern,
    OrPattern,
    Pattern,
    SequencePattern,
    SingletonPattern,
    StarredPattern,
    ValuePattern,
)
from mypy.traverser import TraverserVisitor
from mypy.types import FunctionLike, ProperType, Type
from mypy.util import replace_object_state
from mypy.visitor import NodeVisitor


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.235">def unexpected_keyword_argument(
    self, callee: CallableType, name: str, arg_type: Type, context: Context
) -&gt; None:
    msg = f'Unexpected keyword argument "{name}"' + for_function(callee)
    # Suggest intended keyword, look for type match else fallback on any match.
    matching_type_args = []
    not_matching_type_args = []
    for i, kwarg_type in enumerate(callee.arg_types):
        callee_arg_name = callee.arg_names[i]
        if callee_arg_name is not None and callee.arg_kinds[i] != ARG_STAR:
            if is_subtype(arg_type, kwarg_type):
                matching_type_args.append(callee_arg_name)
            else:
                not_matching_type_args.append(callee_arg_name)
    matches = best_matches(name, matching_type_args)
    if not matches:
        matches = best_matches(name, not_matching_type_args)
    if matches:
        msg += f"; did you mean {pretty_seq(matches[:3], 'or')}?"
    self.fail(msg, context, code=codes.CALL_ARG)
    module = find_defining_module(self.modules, callee)
    if module:
        assert callee.definition is not None
        fname = callable_name(callee)
        if not fname:  # an alias to function with a different name
            fname = "Called function"
        self.note(
            f"{fname} defined here",
            callee.definition,
            file=module.path,
            origin=context,
            code=codes.CALL_ARG,
        )

</t>
<t tx="ekr.20221004064035.2350">class TransformVisitor(NodeVisitor[Node]):
    """Transform a semantically analyzed AST (or subtree) to an identical copy.

    Use the node() method to transform an AST node.

    Subclass to perform a non-identity transform.

    Notes:

     * This can only be used to transform functions or classes, not top-level
       statements, and/or modules as a whole.
     * Do not duplicate TypeInfo nodes. This would generally not be desirable.
     * Only update some name binding cross-references, but only those that
       refer to Var, Decorator or FuncDef nodes, not those targeting ClassDef or
       TypeInfo nodes.
     * Types are not transformed, but you can override type() to also perform
       type transformation.

    TODO nested classes and functions have not been tested well enough
    """

    @others
</t>
<t tx="ekr.20221004064035.2351">def __init__(self) -&gt; None:
    # To simplify testing, set this flag to True if you want to transform
    # all statements in a file (this is prohibited in normal mode).
    self.test_only = False
    # There may be multiple references to a Var node. Keep track of
    # Var translations using a dictionary.
    self.var_map: dict[Var, Var] = {}
    # These are uninitialized placeholder nodes used temporarily for nested
    # functions while we are transforming a top-level function. This maps an
    # untransformed node to a placeholder (which will later become the
    # transformed node).
    self.func_placeholder_map: dict[FuncDef, FuncDef] = {}

</t>
<t tx="ekr.20221004064035.2352">def visit_mypy_file(self, node: MypyFile) -&gt; MypyFile:
    assert self.test_only, "This visitor should not be used for whole files."
    # NOTE: The 'names' and 'imports' instance variables will be empty!
    ignored_lines = {line: codes[:] for line, codes in node.ignored_lines.items()}
    new = MypyFile(self.statements(node.defs), [], node.is_bom, ignored_lines=ignored_lines)
    new._fullname = node._fullname
    new.path = node.path
    new.names = SymbolTable()
    return new

</t>
<t tx="ekr.20221004064035.2353">def visit_import(self, node: Import) -&gt; Import:
    return Import(node.ids[:])

</t>
<t tx="ekr.20221004064035.2354">def visit_import_from(self, node: ImportFrom) -&gt; ImportFrom:
    return ImportFrom(node.id, node.relative, node.names[:])

</t>
<t tx="ekr.20221004064035.2355">def visit_import_all(self, node: ImportAll) -&gt; ImportAll:
    return ImportAll(node.id, node.relative)

</t>
<t tx="ekr.20221004064035.2356">def copy_argument(self, argument: Argument) -&gt; Argument:
    arg = Argument(
        self.visit_var(argument.variable),
        argument.type_annotation,
        argument.initializer,
        argument.kind,
    )

    # Refresh lines of the inner things
    arg.set_line(argument)

    return arg

</t>
<t tx="ekr.20221004064035.2357">def visit_func_def(self, node: FuncDef) -&gt; FuncDef:
    # Note that a FuncDef must be transformed to a FuncDef.

    # These contortions are needed to handle the case of recursive
    # references inside the function being transformed.
    # Set up placeholder nodes for references within this function
    # to other functions defined inside it.
    # Don't create an entry for this function itself though,
    # since we want self-references to point to the original
    # function if this is the top-level node we are transforming.
    init = FuncMapInitializer(self)
    for stmt in node.body.body:
        stmt.accept(init)

    new = FuncDef(
        node.name,
        [self.copy_argument(arg) for arg in node.arguments],
        self.block(node.body),
        cast(Optional[FunctionLike], self.optional_type(node.type)),
    )

    self.copy_function_attributes(new, node)

    new._fullname = node._fullname
    new.is_decorated = node.is_decorated
    new.is_conditional = node.is_conditional
    new.abstract_status = node.abstract_status
    new.is_static = node.is_static
    new.is_class = node.is_class
    new.is_property = node.is_property
    new.is_final = node.is_final
    new.original_def = node.original_def

    if node in self.func_placeholder_map:
        # There is a placeholder definition for this function. Replace
        # the attributes of the placeholder with those form the transformed
        # function. We know that the classes will be identical (otherwise
        # this wouldn't work).
        result = self.func_placeholder_map[node]
        replace_object_state(result, new)
        return result
    else:
        return new

</t>
<t tx="ekr.20221004064035.2358">def visit_lambda_expr(self, node: LambdaExpr) -&gt; LambdaExpr:
    new = LambdaExpr(
        [self.copy_argument(arg) for arg in node.arguments],
        self.block(node.body),
        cast(Optional[FunctionLike], self.optional_type(node.type)),
    )
    self.copy_function_attributes(new, node)
    return new

</t>
<t tx="ekr.20221004064035.2359">def copy_function_attributes(self, new: FuncItem, original: FuncItem) -&gt; None:
    new.info = original.info
    new.min_args = original.min_args
    new.max_pos = original.max_pos
    new.is_overload = original.is_overload
    new.is_generator = original.is_generator
    new.line = original.line

</t>
<t tx="ekr.20221004064035.236">def duplicate_argument_value(self, callee: CallableType, index: int, context: Context) -&gt; None:
    self.fail(
        '{} gets multiple values for keyword argument "{}"'.format(
            callable_name(callee) or "Function", callee.arg_names[index]
        ),
        context,
    )

</t>
<t tx="ekr.20221004064035.2360">def visit_overloaded_func_def(self, node: OverloadedFuncDef) -&gt; OverloadedFuncDef:
    items = [cast(OverloadPart, item.accept(self)) for item in node.items]
    for newitem, olditem in zip(items, node.items):
        newitem.line = olditem.line
    new = OverloadedFuncDef(items)
    new._fullname = node._fullname
    new_type = self.optional_type(node.type)
    assert isinstance(new_type, ProperType)
    new.type = new_type
    new.info = node.info
    new.is_static = node.is_static
    new.is_class = node.is_class
    new.is_property = node.is_property
    new.is_final = node.is_final
    if node.impl:
        new.impl = cast(OverloadPart, node.impl.accept(self))
    return new

</t>
<t tx="ekr.20221004064035.2361">def visit_class_def(self, node: ClassDef) -&gt; ClassDef:
    new = ClassDef(
        node.name,
        self.block(node.defs),
        node.type_vars,
        self.expressions(node.base_type_exprs),
        self.optional_expr(node.metaclass),
    )
    new.fullname = node.fullname
    new.info = node.info
    new.decorators = [self.expr(decorator) for decorator in node.decorators]
    return new

</t>
<t tx="ekr.20221004064035.2362">def visit_global_decl(self, node: GlobalDecl) -&gt; GlobalDecl:
    return GlobalDecl(node.names[:])

</t>
<t tx="ekr.20221004064035.2363">def visit_nonlocal_decl(self, node: NonlocalDecl) -&gt; NonlocalDecl:
    return NonlocalDecl(node.names[:])

</t>
<t tx="ekr.20221004064035.2364">def visit_block(self, node: Block) -&gt; Block:
    return Block(self.statements(node.body))

</t>
<t tx="ekr.20221004064035.2365">def visit_decorator(self, node: Decorator) -&gt; Decorator:
    # Note that a Decorator must be transformed to a Decorator.
    func = self.visit_func_def(node.func)
    func.line = node.func.line
    new = Decorator(func, self.expressions(node.decorators), self.visit_var(node.var))
    new.is_overload = node.is_overload
    return new

</t>
<t tx="ekr.20221004064035.2366">def visit_var(self, node: Var) -&gt; Var:
    # Note that a Var must be transformed to a Var.
    if node in self.var_map:
        return self.var_map[node]
    new = Var(node.name, self.optional_type(node.type))
    new.line = node.line
    new._fullname = node._fullname
    new.info = node.info
    new.is_self = node.is_self
    new.is_ready = node.is_ready
    new.is_initialized_in_class = node.is_initialized_in_class
    new.is_staticmethod = node.is_staticmethod
    new.is_classmethod = node.is_classmethod
    new.is_property = node.is_property
    new.is_final = node.is_final
    new.final_value = node.final_value
    new.final_unset_in_class = node.final_unset_in_class
    new.final_set_in_init = node.final_set_in_init
    new.set_line(node)
    self.var_map[node] = new
    return new

</t>
<t tx="ekr.20221004064035.2367">def visit_expression_stmt(self, node: ExpressionStmt) -&gt; ExpressionStmt:
    return ExpressionStmt(self.expr(node.expr))

</t>
<t tx="ekr.20221004064035.2368">def visit_assignment_stmt(self, node: AssignmentStmt) -&gt; AssignmentStmt:
    return self.duplicate_assignment(node)

</t>
<t tx="ekr.20221004064035.2369">def duplicate_assignment(self, node: AssignmentStmt) -&gt; AssignmentStmt:
    new = AssignmentStmt(
        self.expressions(node.lvalues),
        self.expr(node.rvalue),
        self.optional_type(node.unanalyzed_type),
    )
    new.line = node.line
    new.is_final_def = node.is_final_def
    new.type = self.optional_type(node.type)
    return new

</t>
<t tx="ekr.20221004064035.237">def does_not_return_value(self, callee_type: Type | None, context: Context) -&gt; None:
    """Report an error about use of an unusable type."""
    name: str | None = None
    callee_type = get_proper_type(callee_type)
    if isinstance(callee_type, FunctionLike):
        name = callable_name(callee_type)
    if name is not None:
        self.fail(
            f"{capitalize(name)} does not return a value",
            context,
            code=codes.FUNC_RETURNS_VALUE,
        )
    else:
        self.fail("Function does not return a value", context, code=codes.FUNC_RETURNS_VALUE)

</t>
<t tx="ekr.20221004064035.2370">def visit_operator_assignment_stmt(
    self, node: OperatorAssignmentStmt
) -&gt; OperatorAssignmentStmt:
    return OperatorAssignmentStmt(node.op, self.expr(node.lvalue), self.expr(node.rvalue))

</t>
<t tx="ekr.20221004064035.2371">def visit_while_stmt(self, node: WhileStmt) -&gt; WhileStmt:
    return WhileStmt(
        self.expr(node.expr), self.block(node.body), self.optional_block(node.else_body)
    )

</t>
<t tx="ekr.20221004064035.2372">def visit_for_stmt(self, node: ForStmt) -&gt; ForStmt:
    new = ForStmt(
        self.expr(node.index),
        self.expr(node.expr),
        self.block(node.body),
        self.optional_block(node.else_body),
        self.optional_type(node.unanalyzed_index_type),
    )
    new.is_async = node.is_async
    new.index_type = self.optional_type(node.index_type)
    return new

</t>
<t tx="ekr.20221004064035.2373">def visit_return_stmt(self, node: ReturnStmt) -&gt; ReturnStmt:
    return ReturnStmt(self.optional_expr(node.expr))

</t>
<t tx="ekr.20221004064035.2374">def visit_assert_stmt(self, node: AssertStmt) -&gt; AssertStmt:
    return AssertStmt(self.expr(node.expr), self.optional_expr(node.msg))

</t>
<t tx="ekr.20221004064035.2375">def visit_del_stmt(self, node: DelStmt) -&gt; DelStmt:
    return DelStmt(self.expr(node.expr))

</t>
<t tx="ekr.20221004064035.2376">def visit_if_stmt(self, node: IfStmt) -&gt; IfStmt:
    return IfStmt(
        self.expressions(node.expr),
        self.blocks(node.body),
        self.optional_block(node.else_body),
    )

</t>
<t tx="ekr.20221004064035.2377">def visit_break_stmt(self, node: BreakStmt) -&gt; BreakStmt:
    return BreakStmt()

</t>
<t tx="ekr.20221004064035.2378">def visit_continue_stmt(self, node: ContinueStmt) -&gt; ContinueStmt:
    return ContinueStmt()

</t>
<t tx="ekr.20221004064035.2379">def visit_pass_stmt(self, node: PassStmt) -&gt; PassStmt:
    return PassStmt()

</t>
<t tx="ekr.20221004064035.238">def deleted_as_rvalue(self, typ: DeletedType, context: Context) -&gt; None:
    """Report an error about using an deleted type as an rvalue."""
    if typ.source is None:
        s = ""
    else:
        s = f' "{typ.source}"'
    self.fail(f"Trying to read deleted variable{s}", context)

</t>
<t tx="ekr.20221004064035.2380">def visit_raise_stmt(self, node: RaiseStmt) -&gt; RaiseStmt:
    return RaiseStmt(self.optional_expr(node.expr), self.optional_expr(node.from_expr))

</t>
<t tx="ekr.20221004064035.2381">def visit_try_stmt(self, node: TryStmt) -&gt; TryStmt:
    return TryStmt(
        self.block(node.body),
        self.optional_names(node.vars),
        self.optional_expressions(node.types),
        self.blocks(node.handlers),
        self.optional_block(node.else_body),
        self.optional_block(node.finally_body),
    )

</t>
<t tx="ekr.20221004064035.2382">def visit_with_stmt(self, node: WithStmt) -&gt; WithStmt:
    new = WithStmt(
        self.expressions(node.expr),
        self.optional_expressions(node.target),
        self.block(node.body),
        self.optional_type(node.unanalyzed_type),
    )
    new.is_async = node.is_async
    new.analyzed_types = [self.type(typ) for typ in node.analyzed_types]
    return new

</t>
<t tx="ekr.20221004064035.2383">def visit_as_pattern(self, p: AsPattern) -&gt; AsPattern:
    return AsPattern(
        pattern=self.pattern(p.pattern) if p.pattern is not None else None,
        name=self.duplicate_name(p.name) if p.name is not None else None,
    )

</t>
<t tx="ekr.20221004064035.2384">def visit_or_pattern(self, p: OrPattern) -&gt; OrPattern:
    return OrPattern([self.pattern(pat) for pat in p.patterns])

</t>
<t tx="ekr.20221004064035.2385">def visit_value_pattern(self, p: ValuePattern) -&gt; ValuePattern:
    return ValuePattern(self.expr(p.expr))

</t>
<t tx="ekr.20221004064035.2386">def visit_singleton_pattern(self, p: SingletonPattern) -&gt; SingletonPattern:
    return SingletonPattern(p.value)

</t>
<t tx="ekr.20221004064035.2387">def visit_sequence_pattern(self, p: SequencePattern) -&gt; SequencePattern:
    return SequencePattern([self.pattern(pat) for pat in p.patterns])

</t>
<t tx="ekr.20221004064035.2388">def visit_starred_pattern(self, p: StarredPattern) -&gt; StarredPattern:
    return StarredPattern(self.duplicate_name(p.capture) if p.capture is not None else None)

</t>
<t tx="ekr.20221004064035.2389">def visit_mapping_pattern(self, p: MappingPattern) -&gt; MappingPattern:
    return MappingPattern(
        keys=[self.expr(expr) for expr in p.keys],
        values=[self.pattern(pat) for pat in p.values],
        rest=self.duplicate_name(p.rest) if p.rest is not None else None,
    )

</t>
<t tx="ekr.20221004064035.239">def deleted_as_lvalue(self, typ: DeletedType, context: Context) -&gt; None:
    """Report an error about using an deleted type as an lvalue.

    Currently, this only occurs when trying to assign to an
    exception variable outside the local except: blocks.
    """
    if typ.source is None:
        s = ""
    else:
        s = f' "{typ.source}"'
    self.fail(f"Assignment to variable{s} outside except: block", context)

</t>
<t tx="ekr.20221004064035.2390">def visit_class_pattern(self, p: ClassPattern) -&gt; ClassPattern:
    class_ref = p.class_ref.accept(self)
    assert isinstance(class_ref, RefExpr)
    return ClassPattern(
        class_ref=class_ref,
        positionals=[self.pattern(pat) for pat in p.positionals],
        keyword_keys=list(p.keyword_keys),
        keyword_values=[self.pattern(pat) for pat in p.keyword_values],
    )

</t>
<t tx="ekr.20221004064035.2391">def visit_match_stmt(self, o: MatchStmt) -&gt; MatchStmt:
    return MatchStmt(
        subject=self.expr(o.subject),
        patterns=[self.pattern(p) for p in o.patterns],
        guards=self.optional_expressions(o.guards),
        bodies=self.blocks(o.bodies),
    )

</t>
<t tx="ekr.20221004064035.2392">def visit_star_expr(self, node: StarExpr) -&gt; StarExpr:
    return StarExpr(node.expr)

</t>
<t tx="ekr.20221004064035.2393">def visit_int_expr(self, node: IntExpr) -&gt; IntExpr:
    return IntExpr(node.value)

</t>
<t tx="ekr.20221004064035.2394">def visit_str_expr(self, node: StrExpr) -&gt; StrExpr:
    return StrExpr(node.value)

</t>
<t tx="ekr.20221004064035.2395">def visit_bytes_expr(self, node: BytesExpr) -&gt; BytesExpr:
    return BytesExpr(node.value)

</t>
<t tx="ekr.20221004064035.2396">def visit_float_expr(self, node: FloatExpr) -&gt; FloatExpr:
    return FloatExpr(node.value)

</t>
<t tx="ekr.20221004064035.2397">def visit_complex_expr(self, node: ComplexExpr) -&gt; ComplexExpr:
    return ComplexExpr(node.value)

</t>
<t tx="ekr.20221004064035.2398">def visit_ellipsis(self, node: EllipsisExpr) -&gt; EllipsisExpr:
    return EllipsisExpr()

</t>
<t tx="ekr.20221004064035.2399">def visit_name_expr(self, node: NameExpr) -&gt; NameExpr:
    return self.duplicate_name(node)

</t>
<t tx="ekr.20221004064035.24">def join_types(s: Type, t: Type, instance_joiner: InstanceJoiner | None = None) -&gt; ProperType:
    """Return the least upper bound of s and t.

    For example, the join of 'int' and 'object' is 'object'.
    """
    if mypy.typeops.is_recursive_pair(s, t):
        # This case can trigger an infinite recursion, general support for this will be
        # tricky so we use a trivial join (like for protocols).
        return trivial_join(s, t)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if (s.can_be_true, s.can_be_false) != (t.can_be_true, t.can_be_false):
        # if types are restricted in different ways, use the more general versions
        s = mypy.typeops.true_or_false(s)
        t = mypy.typeops.true_or_false(t)

    if isinstance(s, UnionType) and not isinstance(t, UnionType):
        s, t = t, s

    if isinstance(s, AnyType):
        return s

    if isinstance(s, ErasedType):
        return t

    if isinstance(s, NoneType) and not isinstance(t, NoneType):
        s, t = t, s

    if isinstance(s, UninhabitedType) and not isinstance(t, UninhabitedType):
        s, t = t, s

    # We shouldn't run into PlaceholderTypes here, but in practice we can encounter them
    # here in the presence of undefined names
    if isinstance(t, PlaceholderType) and not isinstance(s, PlaceholderType):
        # mypyc does not allow switching the values like above.
        return s.accept(TypeJoinVisitor(t))
    elif isinstance(t, PlaceholderType):
        return AnyType(TypeOfAny.from_error)

    # Meets/joins require callable type normalization.
    s, t = normalize_callables(s, t)

    # Use a visitor to handle non-trivial cases.
    return t.accept(TypeJoinVisitor(s, instance_joiner))


</t>
<t tx="ekr.20221004064035.240">def no_variant_matches_arguments(
    self,
    overload: Overloaded,
    arg_types: list[Type],
    context: Context,
    *,
    code: ErrorCode | None = None,
) -&gt; None:
    code = code or codes.CALL_OVERLOAD
    name = callable_name(overload)
    if name:
        name_str = f" of {name}"
    else:
        name_str = ""
    arg_types_str = ", ".join(format_type(arg) for arg in arg_types)
    num_args = len(arg_types)
    if num_args == 0:
        self.fail(
            f"All overload variants{name_str} require at least one argument",
            context,
            code=code,
        )
    elif num_args == 1:
        self.fail(
            f"No overload variant{name_str} matches argument type {arg_types_str}",
            context,
            code=code,
        )
    else:
        self.fail(
            f"No overload variant{name_str} matches argument types {arg_types_str}",
            context,
            code=code,
        )

    self.note(f"Possible overload variant{plural_s(len(overload.items))}:", context, code=code)
    for item in overload.items:
        self.note(pretty_callable(item), context, offset=4, code=code)

</t>
<t tx="ekr.20221004064035.2400">def duplicate_name(self, node: NameExpr) -&gt; NameExpr:
    # This method is used when the transform result must be a NameExpr.
    # visit_name_expr() is used when there is no such restriction.
    new = NameExpr(node.name)
    self.copy_ref(new, node)
    new.is_special_form = node.is_special_form
    return new

</t>
<t tx="ekr.20221004064035.2401">def visit_member_expr(self, node: MemberExpr) -&gt; MemberExpr:
    member = MemberExpr(self.expr(node.expr), node.name)
    if node.def_var:
        # This refers to an attribute and we don't transform attributes by default,
        # just normal variables.
        member.def_var = node.def_var
    self.copy_ref(member, node)
    return member

</t>
<t tx="ekr.20221004064035.2402">def copy_ref(self, new: RefExpr, original: RefExpr) -&gt; None:
    new.kind = original.kind
    new.fullname = original.fullname
    target = original.node
    if isinstance(target, Var):
        # Do not transform references to global variables. See
        # testGenericFunctionAliasExpand for an example where this is important.
        if original.kind != GDEF:
            target = self.visit_var(target)
    elif isinstance(target, Decorator):
        target = self.visit_var(target.var)
    elif isinstance(target, FuncDef):
        # Use a placeholder node for the function if it exists.
        target = self.func_placeholder_map.get(target, target)
    new.node = target
    new.is_new_def = original.is_new_def
    new.is_inferred_def = original.is_inferred_def

</t>
<t tx="ekr.20221004064035.2403">def visit_yield_from_expr(self, node: YieldFromExpr) -&gt; YieldFromExpr:
    return YieldFromExpr(self.expr(node.expr))

</t>
<t tx="ekr.20221004064035.2404">def visit_yield_expr(self, node: YieldExpr) -&gt; YieldExpr:
    return YieldExpr(self.optional_expr(node.expr))

</t>
<t tx="ekr.20221004064035.2405">def visit_await_expr(self, node: AwaitExpr) -&gt; AwaitExpr:
    return AwaitExpr(self.expr(node.expr))

</t>
<t tx="ekr.20221004064035.2406">def visit_call_expr(self, node: CallExpr) -&gt; CallExpr:
    return CallExpr(
        self.expr(node.callee),
        self.expressions(node.args),
        node.arg_kinds[:],
        node.arg_names[:],
        self.optional_expr(node.analyzed),
    )

</t>
<t tx="ekr.20221004064035.2407">def visit_op_expr(self, node: OpExpr) -&gt; OpExpr:
    new = OpExpr(node.op, self.expr(node.left), self.expr(node.right))
    new.method_type = self.optional_type(node.method_type)
    return new

</t>
<t tx="ekr.20221004064035.2408">def visit_comparison_expr(self, node: ComparisonExpr) -&gt; ComparisonExpr:
    new = ComparisonExpr(node.operators, self.expressions(node.operands))
    new.method_types = [self.optional_type(t) for t in node.method_types]
    return new

</t>
<t tx="ekr.20221004064035.2409">def visit_cast_expr(self, node: CastExpr) -&gt; CastExpr:
    return CastExpr(self.expr(node.expr), self.type(node.type))

</t>
<t tx="ekr.20221004064035.241">def wrong_number_values_to_unpack(
    self, provided: int, expected: int, context: Context
) -&gt; None:
    if provided &lt; expected:
        if provided == 1:
            self.fail(f"Need more than 1 value to unpack ({expected} expected)", context)
        else:
            self.fail(
                f"Need more than {provided} values to unpack ({expected} expected)", context
            )
    elif provided &gt; expected:
        self.fail(
            f"Too many values to unpack ({expected} expected, {provided} provided)", context
        )

</t>
<t tx="ekr.20221004064035.2410">def visit_assert_type_expr(self, node: AssertTypeExpr) -&gt; AssertTypeExpr:
    return AssertTypeExpr(self.expr(node.expr), self.type(node.type))

</t>
<t tx="ekr.20221004064035.2411">def visit_reveal_expr(self, node: RevealExpr) -&gt; RevealExpr:
    if node.kind == REVEAL_TYPE:
        assert node.expr is not None
        return RevealExpr(kind=REVEAL_TYPE, expr=self.expr(node.expr))
    else:
        # Reveal locals expressions don't have any sub expressions
        return node

</t>
<t tx="ekr.20221004064035.2412">def visit_super_expr(self, node: SuperExpr) -&gt; SuperExpr:
    call = self.expr(node.call)
    assert isinstance(call, CallExpr)
    new = SuperExpr(node.name, call)
    new.info = node.info
    return new

</t>
<t tx="ekr.20221004064035.2413">def visit_assignment_expr(self, node: AssignmentExpr) -&gt; AssignmentExpr:
    return AssignmentExpr(node.target, node.value)

</t>
<t tx="ekr.20221004064035.2414">def visit_unary_expr(self, node: UnaryExpr) -&gt; UnaryExpr:
    new = UnaryExpr(node.op, self.expr(node.expr))
    new.method_type = self.optional_type(node.method_type)
    return new

</t>
<t tx="ekr.20221004064035.2415">def visit_list_expr(self, node: ListExpr) -&gt; ListExpr:
    return ListExpr(self.expressions(node.items))

</t>
<t tx="ekr.20221004064035.2416">def visit_dict_expr(self, node: DictExpr) -&gt; DictExpr:
    return DictExpr(
        [(self.expr(key) if key else None, self.expr(value)) for key, value in node.items]
    )

</t>
<t tx="ekr.20221004064035.2417">def visit_tuple_expr(self, node: TupleExpr) -&gt; TupleExpr:
    return TupleExpr(self.expressions(node.items))

</t>
<t tx="ekr.20221004064035.2418">def visit_set_expr(self, node: SetExpr) -&gt; SetExpr:
    return SetExpr(self.expressions(node.items))

</t>
<t tx="ekr.20221004064035.2419">def visit_index_expr(self, node: IndexExpr) -&gt; IndexExpr:
    new = IndexExpr(self.expr(node.base), self.expr(node.index))
    if node.method_type:
        new.method_type = self.type(node.method_type)
    if node.analyzed:
        if isinstance(node.analyzed, TypeApplication):
            new.analyzed = self.visit_type_application(node.analyzed)
        else:
            new.analyzed = self.visit_type_alias_expr(node.analyzed)
        new.analyzed.set_line(node.analyzed)
    return new

</t>
<t tx="ekr.20221004064035.242">def unpacking_strings_disallowed(self, context: Context) -&gt; None:
    self.fail("Unpacking a string is disallowed", context)

</t>
<t tx="ekr.20221004064035.2420">def visit_type_application(self, node: TypeApplication) -&gt; TypeApplication:
    return TypeApplication(self.expr(node.expr), self.types(node.types))

</t>
<t tx="ekr.20221004064035.2421">def visit_list_comprehension(self, node: ListComprehension) -&gt; ListComprehension:
    generator = self.duplicate_generator(node.generator)
    generator.set_line(node.generator)
    return ListComprehension(generator)

</t>
<t tx="ekr.20221004064035.2422">def visit_set_comprehension(self, node: SetComprehension) -&gt; SetComprehension:
    generator = self.duplicate_generator(node.generator)
    generator.set_line(node.generator)
    return SetComprehension(generator)

</t>
<t tx="ekr.20221004064035.2423">def visit_dictionary_comprehension(
    self, node: DictionaryComprehension
) -&gt; DictionaryComprehension:
    return DictionaryComprehension(
        self.expr(node.key),
        self.expr(node.value),
        [self.expr(index) for index in node.indices],
        [self.expr(s) for s in node.sequences],
        [[self.expr(cond) for cond in conditions] for conditions in node.condlists],
        node.is_async,
    )

</t>
<t tx="ekr.20221004064035.2424">def visit_generator_expr(self, node: GeneratorExpr) -&gt; GeneratorExpr:
    return self.duplicate_generator(node)

</t>
<t tx="ekr.20221004064035.2425">def duplicate_generator(self, node: GeneratorExpr) -&gt; GeneratorExpr:
    return GeneratorExpr(
        self.expr(node.left_expr),
        [self.expr(index) for index in node.indices],
        [self.expr(s) for s in node.sequences],
        [[self.expr(cond) for cond in conditions] for conditions in node.condlists],
        node.is_async,
    )

</t>
<t tx="ekr.20221004064035.2426">def visit_slice_expr(self, node: SliceExpr) -&gt; SliceExpr:
    return SliceExpr(
        self.optional_expr(node.begin_index),
        self.optional_expr(node.end_index),
        self.optional_expr(node.stride),
    )

</t>
<t tx="ekr.20221004064035.2427">def visit_conditional_expr(self, node: ConditionalExpr) -&gt; ConditionalExpr:
    return ConditionalExpr(
        self.expr(node.cond), self.expr(node.if_expr), self.expr(node.else_expr)
    )

</t>
<t tx="ekr.20221004064035.2428">def visit_type_var_expr(self, node: TypeVarExpr) -&gt; TypeVarExpr:
    return TypeVarExpr(
        node.name,
        node.fullname,
        self.types(node.values),
        self.type(node.upper_bound),
        variance=node.variance,
    )

</t>
<t tx="ekr.20221004064035.2429">def visit_paramspec_expr(self, node: ParamSpecExpr) -&gt; ParamSpecExpr:
    return ParamSpecExpr(
        node.name, node.fullname, self.type(node.upper_bound), variance=node.variance
    )

</t>
<t tx="ekr.20221004064035.243">def type_not_iterable(self, type: Type, context: Context) -&gt; None:
    self.fail(f"{format_type(type)} object is not iterable", context)

</t>
<t tx="ekr.20221004064035.2430">def visit_type_var_tuple_expr(self, node: TypeVarTupleExpr) -&gt; TypeVarTupleExpr:
    return TypeVarTupleExpr(
        node.name, node.fullname, self.type(node.upper_bound), variance=node.variance
    )

</t>
<t tx="ekr.20221004064035.2431">def visit_type_alias_expr(self, node: TypeAliasExpr) -&gt; TypeAliasExpr:
    return TypeAliasExpr(node.node)

</t>
<t tx="ekr.20221004064035.2432">def visit_newtype_expr(self, node: NewTypeExpr) -&gt; NewTypeExpr:
    res = NewTypeExpr(node.name, node.old_type, line=node.line, column=node.column)
    res.info = node.info
    return res

</t>
<t tx="ekr.20221004064035.2433">def visit_namedtuple_expr(self, node: NamedTupleExpr) -&gt; NamedTupleExpr:
    return NamedTupleExpr(node.info)

</t>
<t tx="ekr.20221004064035.2434">def visit_enum_call_expr(self, node: EnumCallExpr) -&gt; EnumCallExpr:
    return EnumCallExpr(node.info, node.items, node.values)

</t>
<t tx="ekr.20221004064035.2435">def visit_typeddict_expr(self, node: TypedDictExpr) -&gt; Node:
    return TypedDictExpr(node.info)

</t>
<t tx="ekr.20221004064035.2436">def visit__promote_expr(self, node: PromoteExpr) -&gt; PromoteExpr:
    return PromoteExpr(node.type)

</t>
<t tx="ekr.20221004064035.2437">def visit_temp_node(self, node: TempNode) -&gt; TempNode:
    return TempNode(self.type(node.type))

</t>
<t tx="ekr.20221004064035.2438">def node(self, node: Node) -&gt; Node:
    new = node.accept(self)
    new.set_line(node)
    return new

</t>
<t tx="ekr.20221004064035.2439">def mypyfile(self, node: MypyFile) -&gt; MypyFile:
    new = node.accept(self)
    assert isinstance(new, MypyFile)
    new.set_line(node)
    return new

</t>
<t tx="ekr.20221004064035.244">def possible_missing_await(self, context: Context) -&gt; None:
    self.note('Maybe you forgot to use "await"?', context)

</t>
<t tx="ekr.20221004064035.2440">def expr(self, expr: Expression) -&gt; Expression:
    new = expr.accept(self)
    assert isinstance(new, Expression)
    new.set_line(expr)
    return new

</t>
<t tx="ekr.20221004064035.2441">def stmt(self, stmt: Statement) -&gt; Statement:
    new = stmt.accept(self)
    assert isinstance(new, Statement)
    new.set_line(stmt)
    return new

</t>
<t tx="ekr.20221004064035.2442">def pattern(self, pattern: Pattern) -&gt; Pattern:
    new = pattern.accept(self)
    assert isinstance(new, Pattern)
    new.set_line(pattern)
    return new

</t>
<t tx="ekr.20221004064035.2443"># Helpers
#
# All the node helpers also propagate line numbers.

</t>
<t tx="ekr.20221004064035.2444">def optional_expr(self, expr: Expression | None) -&gt; Expression | None:
    if expr:
        return self.expr(expr)
    else:
        return None

</t>
<t tx="ekr.20221004064035.2445">def block(self, block: Block) -&gt; Block:
    new = self.visit_block(block)
    new.line = block.line
    return new

</t>
<t tx="ekr.20221004064035.2446">def optional_block(self, block: Block | None) -&gt; Block | None:
    if block:
        return self.block(block)
    else:
        return None

</t>
<t tx="ekr.20221004064035.2447">def statements(self, statements: list[Statement]) -&gt; list[Statement]:
    return [self.stmt(stmt) for stmt in statements]

</t>
<t tx="ekr.20221004064035.2448">def expressions(self, expressions: list[Expression]) -&gt; list[Expression]:
    return [self.expr(expr) for expr in expressions]

</t>
<t tx="ekr.20221004064035.2449">def optional_expressions(
    self, expressions: Iterable[Expression | None]
) -&gt; list[Expression | None]:
    return [self.optional_expr(expr) for expr in expressions]

</t>
<t tx="ekr.20221004064035.245">def incompatible_operator_assignment(self, op: str, context: Context) -&gt; None:
    self.fail(f"Result type of {op} incompatible in assignment", context)

</t>
<t tx="ekr.20221004064035.2450">def blocks(self, blocks: list[Block]) -&gt; list[Block]:
    return [self.block(block) for block in blocks]

</t>
<t tx="ekr.20221004064035.2451">def names(self, names: list[NameExpr]) -&gt; list[NameExpr]:
    return [self.duplicate_name(name) for name in names]

</t>
<t tx="ekr.20221004064035.2452">def optional_names(self, names: Iterable[NameExpr | None]) -&gt; list[NameExpr | None]:
    result: list[NameExpr | None] = []
    for name in names:
        if name:
            result.append(self.duplicate_name(name))
        else:
            result.append(None)
    return result

</t>
<t tx="ekr.20221004064035.2453">def type(self, type: Type) -&gt; Type:
    # Override this method to transform types.
    return type

</t>
<t tx="ekr.20221004064035.2454">def optional_type(self, type: Type | None) -&gt; Type | None:
    if type:
        return self.type(type)
    else:
        return None

</t>
<t tx="ekr.20221004064035.2455">def types(self, types: list[Type]) -&gt; list[Type]:
    return [self.type(type) for type in types]


</t>
<t tx="ekr.20221004064035.2456">class FuncMapInitializer(TraverserVisitor):
    """This traverser creates mappings from nested FuncDefs to placeholder FuncDefs.

    The placeholders will later be replaced with transformed nodes.
    """

    @others
</t>
<t tx="ekr.20221004064035.2457">def __init__(self, transformer: TransformVisitor) -&gt; None:
    self.transformer = transformer

</t>
<t tx="ekr.20221004064035.2458">def visit_func_def(self, node: FuncDef) -&gt; None:
    if node not in self.transformer.func_placeholder_map:
        # Haven't seen this FuncDef before, so create a placeholder node.
        self.transformer.func_placeholder_map[node] = FuncDef(
            node.name, node.arguments, node.body, None
        )
    super().visit_func_def(node)
</t>
<t tx="ekr.20221004064035.2459">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from mypy.nodes import (
    ParamSpecExpr,
    SymbolTableNode,
    TypeVarExpr,
    TypeVarLikeExpr,
    TypeVarTupleExpr,
)
from mypy.types import (
    ParamSpecFlavor,
    ParamSpecType,
    TypeVarId,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.246">def overload_signature_incompatible_with_supertype(
    self, name: str, name_in_super: str, supertype: str, context: Context
) -&gt; None:
    target = self.override_target(name, name_in_super, supertype)
    self.fail(
        f'Signature of "{name}" incompatible with {target}', context, code=codes.OVERRIDE
    )

    note_template = 'Overload variants must be defined in the same order as they are in "{}"'
    self.note(note_template.format(supertype), context, code=codes.OVERRIDE)

</t>
<t tx="ekr.20221004064035.2460">class TypeVarLikeScope:
    """Scope that holds bindings for type variables and parameter specifications.

    Node fullname -&gt; TypeVarLikeType.
    """

    @others
</t>
<t tx="ekr.20221004064035.2461">def __init__(
    self,
    parent: TypeVarLikeScope | None = None,
    is_class_scope: bool = False,
    prohibited: TypeVarLikeScope | None = None,
    namespace: str = "",
) -&gt; None:
    """Initializer for TypeVarLikeScope

    Parameters:
      parent: the outer scope for this scope
      is_class_scope: True if this represents a generic class
      prohibited: Type variables that aren't strictly in scope exactly,
                  but can't be bound because they're part of an outer class's scope.
    """
    self.scope: dict[str, TypeVarLikeType] = {}
    self.parent = parent
    self.func_id = 0
    self.class_id = 0
    self.is_class_scope = is_class_scope
    self.prohibited = prohibited
    self.namespace = namespace
    if parent is not None:
        self.func_id = parent.func_id
        self.class_id = parent.class_id

</t>
<t tx="ekr.20221004064035.2462">def get_function_scope(self) -&gt; TypeVarLikeScope | None:
    """Get the nearest parent that's a function scope, not a class scope"""
    it: TypeVarLikeScope | None = self
    while it is not None and it.is_class_scope:
        it = it.parent
    return it

</t>
<t tx="ekr.20221004064035.2463">def allow_binding(self, fullname: str) -&gt; bool:
    if fullname in self.scope:
        return False
    elif self.parent and not self.parent.allow_binding(fullname):
        return False
    elif self.prohibited and not self.prohibited.allow_binding(fullname):
        return False
    return True

</t>
<t tx="ekr.20221004064035.2464">def method_frame(self) -&gt; TypeVarLikeScope:
    """A new scope frame for binding a method"""
    return TypeVarLikeScope(self, False, None)

</t>
<t tx="ekr.20221004064035.2465">def class_frame(self, namespace: str) -&gt; TypeVarLikeScope:
    """A new scope frame for binding a class. Prohibits *this* class's tvars"""
    return TypeVarLikeScope(self.get_function_scope(), True, self, namespace=namespace)

</t>
<t tx="ekr.20221004064035.2466">def new_unique_func_id(self) -&gt; int:
    """Used by plugin-like code that needs to make synthetic generic functions."""
    self.func_id -= 1
    return self.func_id

</t>
<t tx="ekr.20221004064035.2467">def bind_new(self, name: str, tvar_expr: TypeVarLikeExpr) -&gt; TypeVarLikeType:
    if self.is_class_scope:
        self.class_id += 1
        i = self.class_id
        namespace = self.namespace
    else:
        self.func_id -= 1
        i = self.func_id
        # TODO: Consider also using namespaces for functions
        namespace = ""
    if isinstance(tvar_expr, TypeVarExpr):
        tvar_def: TypeVarLikeType = TypeVarType(
            name,
            tvar_expr.fullname,
            TypeVarId(i, namespace=namespace),
            values=tvar_expr.values,
            upper_bound=tvar_expr.upper_bound,
            variance=tvar_expr.variance,
            line=tvar_expr.line,
            column=tvar_expr.column,
        )
    elif isinstance(tvar_expr, ParamSpecExpr):
        tvar_def = ParamSpecType(
            name,
            tvar_expr.fullname,
            i,
            flavor=ParamSpecFlavor.BARE,
            upper_bound=tvar_expr.upper_bound,
            line=tvar_expr.line,
            column=tvar_expr.column,
        )
    elif isinstance(tvar_expr, TypeVarTupleExpr):
        tvar_def = TypeVarTupleType(
            name,
            tvar_expr.fullname,
            i,
            upper_bound=tvar_expr.upper_bound,
            line=tvar_expr.line,
            column=tvar_expr.column,
        )
    else:
        assert False
    self.scope[tvar_expr.fullname] = tvar_def
    return tvar_def

</t>
<t tx="ekr.20221004064035.2468">def bind_existing(self, tvar_def: TypeVarLikeType) -&gt; None:
    self.scope[tvar_def.fullname] = tvar_def

</t>
<t tx="ekr.20221004064035.2469">def get_binding(self, item: str | SymbolTableNode) -&gt; TypeVarLikeType | None:
    fullname = item.fullname if isinstance(item, SymbolTableNode) else item
    assert fullname is not None
    if fullname in self.scope:
        return self.scope[fullname]
    elif self.parent is not None:
        return self.parent.get_binding(fullname)
    else:
        return None

</t>
<t tx="ekr.20221004064035.247">def signature_incompatible_with_supertype(
    self,
    name: str,
    name_in_super: str,
    supertype: str,
    context: Context,
    original: FunctionLike | None = None,
    override: FunctionLike | None = None,
) -&gt; None:
    code = codes.OVERRIDE
    target = self.override_target(name, name_in_super, supertype)
    self.fail(f'Signature of "{name}" incompatible with {target}', context, code=code)

    INCLUDE_DECORATOR = True  # Include @classmethod and @staticmethod decorators, if any
    ALLOW_DUPS = True  # Allow duplicate notes, needed when signatures are duplicates
    ALIGN_OFFSET = 1  # One space, to account for the difference between error and note
    OFFSET = 4  # Four spaces, so that notes will look like this:
    # error: Signature of "f" incompatible with supertype "A"
    # note:      Superclass:
    # note:          def f(self) -&gt; str
    # note:      Subclass:
    # note:          def f(self, x: str) -&gt; None
    if (
        original is not None
        and isinstance(original, (CallableType, Overloaded))
        and override is not None
        and isinstance(override, (CallableType, Overloaded))
    ):
        self.note("Superclass:", context, offset=ALIGN_OFFSET + OFFSET, code=code)
        self.pretty_callable_or_overload(
            original,
            context,
            offset=ALIGN_OFFSET + 2 * OFFSET,
            add_class_or_static_decorator=INCLUDE_DECORATOR,
            allow_dups=ALLOW_DUPS,
            code=code,
        )

        self.note("Subclass:", context, offset=ALIGN_OFFSET + OFFSET, code=code)
        self.pretty_callable_or_overload(
            override,
            context,
            offset=ALIGN_OFFSET + 2 * OFFSET,
            add_class_or_static_decorator=INCLUDE_DECORATOR,
            allow_dups=ALLOW_DUPS,
            code=code,
        )

</t>
<t tx="ekr.20221004064035.2470">def __str__(self) -&gt; str:
    me = ", ".join(f"{k}: {v.name}`{v.id}" for k, v in self.scope.items())
    if self.parent is None:
        return me
    return f"{self.parent} &lt;- {me}"
</t>
<t tx="ekr.20221004064035.2471">@path C:/Repos/ekr-mypy2/mypy/
"""Semantic analysis of types"""
&lt;&lt; typeanal imports &gt;&gt;
&lt;&lt; typeanal data &gt;&gt;
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2472">def analyze_type_alias(
    node: Expression,
    api: SemanticAnalyzerCoreInterface,
    tvar_scope: TypeVarLikeScope,
    plugin: Plugin,
    options: Options,
    is_typeshed_stub: bool,
    allow_placeholder: bool = False,
    in_dynamic_func: bool = False,
    global_scope: bool = True,
) -&gt; tuple[Type, set[str]] | None:
    """Analyze r.h.s. of a (potential) type alias definition.

    If `node` is valid as a type alias rvalue, return the resulting type and a set of
    full names of type aliases it depends on (directly or indirectly).
    Return None otherwise. 'node' must have been semantically analyzed.
    """
    try:
        type = expr_to_unanalyzed_type(node, options, api.is_stub_file)
    except TypeTranslationError:
        api.fail("Invalid type alias: expression is not a valid type", node, code=codes.VALID_TYPE)
        return None
    analyzer = TypeAnalyser(
        api,
        tvar_scope,
        plugin,
        options,
        is_typeshed_stub,
        defining_alias=True,
        allow_placeholder=allow_placeholder,
    )
    analyzer.in_dynamic_func = in_dynamic_func
    analyzer.global_scope = global_scope
    res = type.accept(analyzer)
    return res, analyzer.aliases_used


</t>
<t tx="ekr.20221004064035.2473">def no_subscript_builtin_alias(name: str, propose_alt: bool = True) -&gt; str:
    class_name = name.split(".")[-1]
    msg = f'"{class_name}" is not subscriptable'
    # This should never be called if the python_version is 3.9 or newer
    nongen_builtins = get_nongen_builtins((3, 8))
    replacement = nongen_builtins[name]
    if replacement and propose_alt:
        msg += f', use "{replacement}" instead'
    return msg


</t>
<t tx="ekr.20221004064035.2474">class TypeAnalyser(SyntheticTypeVisitor[Type], TypeAnalyzerPluginInterface):
    """Semantic analyzer for types.

    Converts unbound types into bound types. This is a no-op for already
    bound types.

    If an incomplete reference is encountered, this does a defer. The
    caller never needs to defer.
    """

    # Is this called from an untyped function definition?
    in_dynamic_func: bool = False
    # Is this called from global scope?
    global_scope: bool = True

    @others
</t>
<t tx="ekr.20221004064035.2475">def __init__(
    self,
    api: SemanticAnalyzerCoreInterface,
    tvar_scope: TypeVarLikeScope,
    plugin: Plugin,
    options: Options,
    is_typeshed_stub: bool,
    *,
    defining_alias: bool = False,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    allow_placeholder: bool = False,
    allow_required: bool = False,
    allow_param_spec_literals: bool = False,
    report_invalid_types: bool = True,
) -&gt; None:
    self.api = api
    self.lookup_qualified = api.lookup_qualified
    self.lookup_fqn_func = api.lookup_fully_qualified
    self.fail_func = api.fail
    self.note_func = api.note
    self.tvar_scope = tvar_scope
    # Are we analysing a type alias definition rvalue?
    self.defining_alias = defining_alias
    self.allow_tuple_literal = allow_tuple_literal
    # Positive if we are analyzing arguments of another (outer) type
    self.nesting_level = 0
    # Should we allow new type syntax when targeting older Python versions
    # like 'list[int]' or 'X | Y' (allowed in stubs and with `__future__` import)?
    self.always_allow_new_syntax = self.api.is_stub_file or self.api.is_future_flag_set(
        "annotations"
    )
    # Should we accept unbound type variables (always OK in aliases)?
    self.allow_unbound_tvars = allow_unbound_tvars or defining_alias
    # If false, record incomplete ref if we generate PlaceholderType.
    self.allow_placeholder = allow_placeholder
    # Are we in a context where Required[] is allowed?
    self.allow_required = allow_required
    # Are we in a context where ParamSpec literals are allowed?
    self.allow_param_spec_literals = allow_param_spec_literals
    # Should we report an error whenever we encounter a RawExpressionType outside
    # of a Literal context: e.g. whenever we encounter an invalid type? Normally,
    # we want to report an error, but the caller may want to do more specialized
    # error handling.
    self.report_invalid_types = report_invalid_types
    self.plugin = plugin
    self.options = options
    self.is_typeshed_stub = is_typeshed_stub
    # Names of type aliases encountered while analysing a type will be collected here.
    self.aliases_used: set[str] = set()

</t>
<t tx="ekr.20221004064035.2476">def visit_unbound_type(self, t: UnboundType, defining_literal: bool = False) -&gt; Type:
    typ = self.visit_unbound_type_nonoptional(t, defining_literal)
    if t.optional:
        # We don't need to worry about double-wrapping Optionals or
        # wrapping Anys: Union simplification will take care of that.
        return make_optional_type(typ)
    return typ

</t>
<t tx="ekr.20221004064035.2477">def visit_unbound_type_nonoptional(self, t: UnboundType, defining_literal: bool) -&gt; Type:
    sym = self.lookup_qualified(t.name, t)
    if sym is not None:
        node = sym.node
        if isinstance(node, PlaceholderNode):
            if node.becomes_typeinfo:
                # Reference to placeholder type.
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                elif self.allow_placeholder:
                    self.api.defer()
                else:
                    self.api.record_incomplete_ref()
                return PlaceholderType(node.fullname, self.anal_array(t.args), t.line)
            else:
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                else:
                    # Reference to an unknown placeholder node.
                    self.api.record_incomplete_ref()
                    return AnyType(TypeOfAny.special_form)
        if node is None:
            self.fail(f"Internal error (node is None, kind={sym.kind})", t)
            return AnyType(TypeOfAny.special_form)
        fullname = node.fullname
        hook = self.plugin.get_type_analyze_hook(fullname)
        if hook is not None:
            return hook(AnalyzeTypeContext(t, t, self))
        if (
            fullname in get_nongen_builtins(self.options.python_version)
            and t.args
            and not self.always_allow_new_syntax
        ):
            self.fail(
                no_subscript_builtin_alias(fullname, propose_alt=not self.defining_alias), t
            )
        tvar_def = self.tvar_scope.get_binding(sym)
        if isinstance(sym.node, ParamSpecExpr):
            if tvar_def is None:
                self.fail(f'ParamSpec "{t.name}" is unbound', t, code=codes.VALID_TYPE)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, ParamSpecType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'ParamSpec "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return ParamSpecType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.flavor,
                tvar_def.upper_bound,
                line=t.line,
                column=t.column,
            )
        if isinstance(sym.node, TypeVarExpr) and tvar_def is not None and self.defining_alias:
            self.fail(
                f'Can\'t use bound type variable "{t.name}" to define generic alias',
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarExpr) and tvar_def is not None:
            assert isinstance(tvar_def, TypeVarType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'Type variable "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return TypeVarType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.values,
                tvar_def.upper_bound,
                tvar_def.variance,
                line=t.line,
                column=t.column,
            )
        if isinstance(sym.node, TypeVarTupleExpr) and (
            tvar_def is not None and self.defining_alias
        ):
            self.fail(
                f'Can\'t use bound type variable "{t.name}" to define generic alias',
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarTupleExpr):
            if tvar_def is None:
                self.fail(f'TypeVarTuple "{t.name}" is unbound', t, code=codes.VALID_TYPE)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, TypeVarTupleType)
            if len(t.args) &gt; 0:
                self.fail(
                    f'Type variable "{t.name}" used with arguments', t, code=codes.VALID_TYPE
                )
            # Change the line number
            return TypeVarTupleType(
                tvar_def.name,
                tvar_def.fullname,
                tvar_def.id,
                tvar_def.upper_bound,
                line=t.line,
                column=t.column,
            )
        special = self.try_analyze_special_unbound_type(t, fullname)
        if special is not None:
            return special
        if isinstance(node, TypeAlias):
            self.aliases_used.add(fullname)
            an_args = self.anal_array(t.args)
            disallow_any = self.options.disallow_any_generics and not self.is_typeshed_stub
            res = expand_type_alias(
                node,
                an_args,
                self.fail,
                node.no_args,
                t,
                unexpanded_type=t,
                disallow_any=disallow_any,
            )
            # The only case where expand_type_alias() can return an incorrect instance is
            # when it is top-level instance, so no need to recurse.
            if (
                isinstance(res, Instance)  # type: ignore[misc]
                and len(res.args) != len(res.type.type_vars)
                and not self.defining_alias
            ):
                fix_instance(
                    res,
                    self.fail,
                    self.note,
                    disallow_any=disallow_any,
                    python_version=self.options.python_version,
                    use_generic_error=True,
                    unexpanded_type=t,
                )
            if node.eager:
                res = get_proper_type(res)
            return res
        elif isinstance(node, TypeInfo):
            return self.analyze_type_with_type_info(node, t.args, t)
        elif node.fullname in TYPE_ALIAS_NAMES:
            return AnyType(TypeOfAny.special_form)
        # Concatenate is an operator, no need for a proper type
        elif node.fullname in ("typing_extensions.Concatenate", "typing.Concatenate"):
            # We check the return type further up the stack for valid use locations
            return self.apply_concatenate_operator(t)
        else:
            return self.analyze_unbound_type_without_type_info(t, sym, defining_literal)
    else:  # sym is None
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064035.2478">def cannot_resolve_type(self, t: UnboundType) -&gt; None:
    # TODO: Move error message generation to messages.py. We'd first
    #       need access to MessageBuilder here. Also move the similar
    #       message generation logic in semanal.py.
    self.api.fail(f'Cannot resolve name "{t.name}" (possible cyclic definition)', t)
    if not self.options.disable_recursive_aliases and self.api.is_func_scope():
        self.note("Recursive types are not allowed at function scope", t)

</t>
<t tx="ekr.20221004064035.2479">def apply_concatenate_operator(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.api.fail("Concatenate needs type arguments", t, code=codes.VALID_TYPE)
        return AnyType(TypeOfAny.from_error)

    # last argument has to be ParamSpec
    ps = self.anal_type(t.args[-1], allow_param_spec=True)
    if not isinstance(ps, ParamSpecType):
        self.api.fail(
            "The last parameter to Concatenate needs to be a ParamSpec",
            t,
            code=codes.VALID_TYPE,
        )
        return AnyType(TypeOfAny.from_error)

    # TODO: this may not work well with aliases, if those worked.
    #   Those should be special-cased.
    elif ps.prefix.arg_types:
        self.api.fail("Nested Concatenates are invalid", t, code=codes.VALID_TYPE)

    args = self.anal_array(t.args[:-1])
    pre = ps.prefix

    # mypy can't infer this :(
    names: list[str | None] = [None] * len(args)

    pre = Parameters(
        args + pre.arg_types, [ARG_POS] * len(args) + pre.arg_kinds, names + pre.arg_names
    )
    return ps.copy_modified(prefix=pre)

</t>
<t tx="ekr.20221004064035.248">def pretty_callable_or_overload(
    self,
    tp: CallableType | Overloaded,
    context: Context,
    *,
    offset: int = 0,
    add_class_or_static_decorator: bool = False,
    allow_dups: bool = False,
    code: ErrorCode | None = None,
) -&gt; None:
    if isinstance(tp, CallableType):
        if add_class_or_static_decorator:
            decorator = pretty_class_or_static_decorator(tp)
            if decorator is not None:
                self.note(decorator, context, offset=offset, allow_dups=allow_dups, code=code)
        self.note(
            pretty_callable(tp), context, offset=offset, allow_dups=allow_dups, code=code
        )
    elif isinstance(tp, Overloaded):
        self.pretty_overload(
            tp,
            context,
            offset,
            add_class_or_static_decorator=add_class_or_static_decorator,
            allow_dups=allow_dups,
            code=code,
        )

</t>
<t tx="ekr.20221004064035.2480">def try_analyze_special_unbound_type(self, t: UnboundType, fullname: str) -&gt; Type | None:
    """Bind special type that is recognized through magic name such as 'typing.Any'.

    Return the bound type if successful, and return None if the type is a normal type.
    """
    if fullname == "builtins.None":
        return NoneType()
    elif fullname == "typing.Any" or fullname == "builtins.Any":
        return AnyType(TypeOfAny.explicit)
    elif fullname in FINAL_TYPE_NAMES:
        self.fail(
            "Final can be only used as an outermost qualifier in a variable annotation",
            t,
            code=codes.VALID_TYPE,
        )
        return AnyType(TypeOfAny.from_error)
    elif fullname == "typing.Tuple" or (
        fullname == "builtins.tuple"
        and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9))
    ):
        # Tuple is special because it is involved in builtin import cycle
        # and may be not ready when used.
        sym = self.api.lookup_fully_qualified_or_none("builtins.tuple")
        if not sym or isinstance(sym.node, PlaceholderNode):
            if self.api.is_incomplete_namespace("builtins"):
                self.api.record_incomplete_ref()
            else:
                self.fail('Name "tuple" is not defined', t)
            return AnyType(TypeOfAny.special_form)
        if len(t.args) == 0 and not t.empty_tuple_index:
            # Bare 'Tuple' is same as 'tuple'
            any_type = self.get_omitted_any(t)
            return self.named_type("builtins.tuple", [any_type], line=t.line, column=t.column)
        if len(t.args) == 2 and isinstance(t.args[1], EllipsisType):
            # Tuple[T, ...] (uniform, variable-length tuple)
            instance = self.named_type("builtins.tuple", [self.anal_type(t.args[0])])
            instance.line = t.line
            return instance
        return self.tuple_type(self.anal_array(t.args))
    elif fullname == "typing.Union":
        items = self.anal_array(t.args)
        return UnionType.make_union(items)
    elif fullname == "typing.Optional":
        if len(t.args) != 1:
            self.fail(
                "Optional[...] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        item = self.anal_type(t.args[0])
        return make_optional_type(item)
    elif fullname == "typing.Callable":
        return self.analyze_callable_type(t)
    elif fullname == "typing.Type" or (
        fullname == "builtins.type"
        and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9))
    ):
        if len(t.args) == 0:
            if fullname == "typing.Type":
                any_type = self.get_omitted_any(t)
                return TypeType(any_type, line=t.line, column=t.column)
            else:
                # To prevent assignment of 'builtins.type' inferred as 'builtins.object'
                # See https://github.com/python/mypy/issues/9476 for more information
                return None
        if len(t.args) != 1:
            type_str = "Type[...]" if fullname == "typing.Type" else "type[...]"
            self.fail(
                type_str + " must have exactly one type argument", t, code=codes.VALID_TYPE
            )
        item = self.anal_type(t.args[0])
        if bad_type_type_item(item):
            self.fail("Type[...] can't contain another Type[...]", t, code=codes.VALID_TYPE)
            item = AnyType(TypeOfAny.from_error)
        return TypeType.make_normalized(item, line=t.line, column=t.column)
    elif fullname == "typing.ClassVar":
        if self.nesting_level &gt; 0:
            self.fail(
                "Invalid type: ClassVar nested inside other type", t, code=codes.VALID_TYPE
            )
        if len(t.args) == 0:
            return AnyType(TypeOfAny.from_omitted_generics, line=t.line, column=t.column)
        if len(t.args) != 1:
            self.fail(
                "ClassVar[...] must have at most one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in NEVER_NAMES:
        return UninhabitedType(is_noreturn=True)
    elif fullname in LITERAL_TYPE_NAMES:
        return self.analyze_literal_type(t)
    elif fullname in ANNOTATED_TYPE_NAMES:
        if len(t.args) &lt; 2:
            self.fail(
                "Annotated[...] must have exactly one type argument"
                " and at least one annotation",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in ("typing_extensions.Required", "typing.Required"):
        if not self.allow_required:
            self.fail(
                "Required[] can be only used in a TypedDict definition",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail(
                "Required[] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=True)
    elif fullname in ("typing_extensions.NotRequired", "typing.NotRequired"):
        if not self.allow_required:
            self.fail(
                "NotRequired[] can be only used in a TypedDict definition",
                t,
                code=codes.VALID_TYPE,
            )
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail(
                "NotRequired[] must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=False)
    elif self.anal_type_guard_arg(t, fullname) is not None:
        # In most contexts, TypeGuard[...] acts as an alias for bool (ignoring its args)
        return self.named_type("builtins.bool")
    elif fullname in ("typing.Unpack", "typing_extensions.Unpack"):
        # We don't want people to try to use this yet.
        if not self.options.enable_incomplete_features:
            self.fail('"Unpack" is not supported yet, use --enable-incomplete-features', t)
            return AnyType(TypeOfAny.from_error)
        return UnpackType(self.anal_type(t.args[0]), line=t.line, column=t.column)
    return None

</t>
<t tx="ekr.20221004064035.2481">def get_omitted_any(self, typ: Type, fullname: str | None = None) -&gt; AnyType:
    disallow_any = not self.is_typeshed_stub and self.options.disallow_any_generics
    return get_omitted_any(
        disallow_any, self.fail, self.note, typ, self.options.python_version, fullname
    )

</t>
<t tx="ekr.20221004064035.2482">def analyze_type_with_type_info(
    self, info: TypeInfo, args: Sequence[Type], ctx: Context
) -&gt; Type:
    """Bind unbound type when were able to find target TypeInfo.

    This handles simple cases like 'int', 'modname.UserClass[str]', etc.
    """

    if len(args) &gt; 0 and info.fullname == "builtins.tuple":
        fallback = Instance(info, [AnyType(TypeOfAny.special_form)], ctx.line)
        return TupleType(self.anal_array(args), fallback, ctx.line)

    # This is a heuristic: it will be checked later anyways but the error
    # message may be worse.
    with self.set_allow_param_spec_literals(info.has_param_spec_type):
        # Analyze arguments and (usually) construct Instance type. The
        # number of type arguments and their values are
        # checked only later, since we do not always know the
        # valid count at this point. Thus we may construct an
        # Instance with an invalid number of type arguments.
        instance = Instance(
            info, self.anal_array(args, allow_param_spec=True), ctx.line, ctx.column
        )

    # "aesthetic" paramspec literals
    # these do not support mypy_extensions VarArgs, etc. as they were already analyzed
    #   TODO: should these be re-analyzed to get rid of this inconsistency?
    # another inconsistency is with empty type args (Z[] is more possibly an error imo)
    if len(info.type_vars) == 1 and info.has_param_spec_type and len(instance.args) &gt; 0:
        first_arg = get_proper_type(instance.args[0])

        # TODO: can I use tuple syntax to isinstance multiple in 3.6?
        if not (
            len(instance.args) == 1
            and (
                isinstance(first_arg, Parameters)
                or isinstance(first_arg, ParamSpecType)
                or isinstance(first_arg, AnyType)
            )
        ):
            args = instance.args
            instance.args = (Parameters(args, [ARG_POS] * len(args), [None] * len(args)),)

    if info.has_type_var_tuple_type:
        # - 1 to allow for the empty type var tuple case.
        valid_arg_length = len(instance.args) &gt;= len(info.type_vars) - 1
    else:
        valid_arg_length = len(instance.args) == len(info.type_vars)

    # Check type argument count.
    if not valid_arg_length and not self.defining_alias:
        fix_instance(
            instance,
            self.fail,
            self.note,
            disallow_any=self.options.disallow_any_generics and not self.is_typeshed_stub,
            python_version=self.options.python_version,
        )

    tup = info.tuple_type
    if tup is not None:
        # The class has a Tuple[...] base class so it will be
        # represented as a tuple type.
        if info.special_alias:
            return TypeAliasType(info.special_alias, self.anal_array(args))
        return tup.copy_modified(items=self.anal_array(tup.items), fallback=instance)
    td = info.typeddict_type
    if td is not None:
        # The class has a TypedDict[...] base class so it will be
        # represented as a typeddict type.
        if info.special_alias:
            return TypeAliasType(info.special_alias, self.anal_array(args))
        # Create a named TypedDictType
        return td.copy_modified(
            item_types=self.anal_array(list(td.items.values())), fallback=instance
        )

    if info.fullname == "types.NoneType":
        self.fail(
            "NoneType should not be used as a type, please use None instead",
            ctx,
            code=codes.VALID_TYPE,
        )
        return NoneType(ctx.line, ctx.column)

    return instance

</t>
<t tx="ekr.20221004064035.2483">def analyze_unbound_type_without_type_info(
    self, t: UnboundType, sym: SymbolTableNode, defining_literal: bool
) -&gt; Type:
    """Figure out what an unbound type that doesn't refer to a TypeInfo node means.

    This is something unusual. We try our best to find out what it is.
    """
    name = sym.fullname
    if name is None:
        assert sym.node is not None
        name = sym.node.name
    # Option 1:
    # Something with an Any type -- make it an alias for Any in a type
    # context. This is slightly problematic as it allows using the type 'Any'
    # as a base class -- however, this will fail soon at runtime so the problem
    # is pretty minor.
    if isinstance(sym.node, Var):
        typ = get_proper_type(sym.node.type)
        if isinstance(typ, AnyType):
            return AnyType(
                TypeOfAny.from_unimported_type, missing_import_name=typ.missing_import_name
            )
    # Option 2:
    # Unbound type variable. Currently these may be still valid,
    # for example when defining a generic type alias.
    unbound_tvar = (
        isinstance(sym.node, (TypeVarExpr, TypeVarTupleExpr))
        and self.tvar_scope.get_binding(sym) is None
    )
    if self.allow_unbound_tvars and unbound_tvar:
        return t

    # Option 3:
    # Enum value. Note: we only want to return a LiteralType when
    # we're using this enum value specifically within context of
    # a "Literal[...]" type. So, if `defining_literal` is not set,
    # we bail out early with an error.
    #
    # If, in the distant future, we decide to permit things like
    # `def foo(x: Color.RED) -&gt; None: ...`, we can remove that
    # check entirely.
    if isinstance(sym.node, Var) and sym.node.info and sym.node.info.is_enum:
        value = sym.node.name
        base_enum_short_name = sym.node.info.name
        if not defining_literal:
            msg = message_registry.INVALID_TYPE_RAW_ENUM_VALUE.format(
                base_enum_short_name, value
            )
            self.fail(msg.value, t, code=msg.code)
            return AnyType(TypeOfAny.from_error)
        return LiteralType(
            value=value,
            fallback=Instance(sym.node.info, [], line=t.line, column=t.column),
            line=t.line,
            column=t.column,
        )

    # None of the above options worked. We parse the args (if there are any)
    # to make sure there are no remaining semanal-only types, then give up.
    t = t.copy_modified(args=self.anal_array(t.args))
    # TODO: Move this message building logic to messages.py.
    notes: list[str] = []
    if isinstance(sym.node, Var):
        notes.append(
            "See https://mypy.readthedocs.io/en/"
            "stable/common_issues.html#variables-vs-type-aliases"
        )
        message = 'Variable "{}" is not valid as a type'
    elif isinstance(sym.node, (SYMBOL_FUNCBASE_TYPES, Decorator)):
        message = 'Function "{}" is not valid as a type'
        if name == "builtins.any":
            notes.append('Perhaps you meant "typing.Any" instead of "any"?')
        elif name == "builtins.callable":
            notes.append('Perhaps you meant "typing.Callable" instead of "callable"?')
        else:
            notes.append('Perhaps you need "Callable[...]" or a callback protocol?')
    elif isinstance(sym.node, MypyFile):
        # TODO: suggest a protocol when supported.
        message = 'Module "{}" is not valid as a type'
    elif unbound_tvar:
        message = 'Type variable "{}" is unbound'
        short = name.split(".")[-1]
        notes.append(
            (
                '(Hint: Use "Generic[{}]" or "Protocol[{}]" base class'
                ' to bind "{}" inside a class)'
            ).format(short, short, short)
        )
        notes.append(
            '(Hint: Use "{}" in function signature to bind "{}"'
            " inside a function)".format(short, short)
        )
    else:
        message = 'Cannot interpret reference "{}" as a type'
    self.fail(message.format(name), t, code=codes.VALID_TYPE)
    for note in notes:
        self.note(note, t, code=codes.VALID_TYPE)

    # TODO: Would it be better to always return Any instead of UnboundType
    # in case of an error? On one hand, UnboundType has a name so error messages
    # are more detailed, on the other hand, some of them may be bogus,
    # see https://github.com/python/mypy/issues/4987.
    return t

</t>
<t tx="ekr.20221004064035.2484">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2485">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2486">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2487">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    # This type should exist only temporarily during type inference
    assert False, "Internal error: Unexpected erased type"

</t>
<t tx="ekr.20221004064035.2488">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2489">def visit_type_list(self, t: TypeList) -&gt; Type:
    # paramspec literal (Z[[int, str, Whatever]])
    if self.allow_param_spec_literals:
        params = self.analyze_callable_args(t)
        if params:
            ts, kinds, names = params
            # bind these types
            return Parameters(self.anal_array(ts), kinds, names)
        else:
            return AnyType(TypeOfAny.from_error)
    else:
        self.fail(
            'Bracketed expression "[...]" is not valid as a type', t, code=codes.VALID_TYPE
        )
        self.note('Did you mean "List[...]"?', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064035.249">def argument_incompatible_with_supertype(
    self,
    arg_num: int,
    name: str,
    type_name: str | None,
    name_in_supertype: str,
    arg_type_in_supertype: Type,
    supertype: str,
    context: Context,
) -&gt; None:
    target = self.override_target(name, name_in_supertype, supertype)
    arg_type_in_supertype_f = format_type_bare(arg_type_in_supertype)
    self.fail(
        'Argument {} of "{}" is incompatible with {}; '
        'supertype defines the argument type as "{}"'.format(
            arg_num, name, target, arg_type_in_supertype_f
        ),
        context,
        code=codes.OVERRIDE,
    )
    self.note("This violates the Liskov substitution principle", context, code=codes.OVERRIDE)
    self.note(
        "See https://mypy.readthedocs.io/en/stable/common_issues.html#incompatible-overrides",
        context,
        code=codes.OVERRIDE,
    )

    if name == "__eq__" and type_name:
        multiline_msg = self.comparison_method_example_msg(class_name=type_name)
        self.note_multiline(multiline_msg, context, code=codes.OVERRIDE)

</t>
<t tx="ekr.20221004064035.2490">def visit_callable_argument(self, t: CallableArgument) -&gt; Type:
    self.fail("Invalid type", t, code=codes.VALID_TYPE)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064035.2491">def visit_instance(self, t: Instance) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2492">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # TODO: should we do something here?
    return t

</t>
<t tx="ekr.20221004064035.2493">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2494">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2495">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2496">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.2497">def visit_parameters(self, t: Parameters) -&gt; Type:
    raise NotImplementedError("ParamSpec literals cannot have unbound TypeVars")

</t>
<t tx="ekr.20221004064035.2498">def visit_callable_type(self, t: CallableType, nested: bool = True) -&gt; Type:
    # Every Callable can bind its own type variables, if they're not in the outer scope
    with self.tvar_scope_frame():
        if self.defining_alias:
            variables = t.variables
        else:
            variables = self.bind_function_type_variables(t, t)
        special = self.anal_type_guard(t.ret_type)
        arg_kinds = t.arg_kinds
        if len(arg_kinds) &gt;= 2 and arg_kinds[-2] == ARG_STAR and arg_kinds[-1] == ARG_STAR2:
            arg_types = self.anal_array(t.arg_types[:-2], nested=nested) + [
                self.anal_star_arg_type(t.arg_types[-2], ARG_STAR, nested=nested),
                self.anal_star_arg_type(t.arg_types[-1], ARG_STAR2, nested=nested),
            ]
        else:
            arg_types = self.anal_array(t.arg_types, nested=nested)
        ret = t.copy_modified(
            arg_types=arg_types,
            ret_type=self.anal_type(t.ret_type, nested=nested),
            # If the fallback isn't filled in yet,
            # its type will be the falsey FakeInfo
            fallback=(t.fallback if t.fallback.type else self.named_type("builtins.function")),
            variables=self.anal_var_defs(variables),
            type_guard=special,
        )
    return ret

</t>
<t tx="ekr.20221004064035.2499">def anal_type_guard(self, t: Type) -&gt; Type | None:
    if isinstance(t, UnboundType):
        sym = self.lookup_qualified(t.name, t)
        if sym is not None and sym.node is not None:
            return self.anal_type_guard_arg(t, sym.node.fullname)
    # TODO: What if it's an Instance? Then use t.type.fullname?
    return None

</t>
<t tx="ekr.20221004064035.25">class TypeJoinVisitor(TypeVisitor[ProperType]):
    """Implementation of the least upper bound algorithm.

    Attributes:
      s: The other (left) type operand.
    """

    @others
</t>
<t tx="ekr.20221004064035.250">def comparison_method_example_msg(self, class_name: str) -&gt; str:
    return dedent(
        """\
    It is recommended for "__eq__" to work with arbitrary objects, for example:
        def __eq__(self, other: object) -&gt; bool:
            if not isinstance(other, {class_name}):
                return NotImplemented
            return &lt;logic to compare two {class_name} instances&gt;
    """.format(
            class_name=class_name
        )
    )

</t>
<t tx="ekr.20221004064035.2500">def anal_type_guard_arg(self, t: UnboundType, fullname: str) -&gt; Type | None:
    if fullname in ("typing_extensions.TypeGuard", "typing.TypeGuard"):
        if len(t.args) != 1:
            self.fail(
                "TypeGuard must have exactly one type argument", t, code=codes.VALID_TYPE
            )
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    return None

</t>
<t tx="ekr.20221004064035.2501">def anal_star_arg_type(self, t: Type, kind: ArgKind, nested: bool) -&gt; Type:
    """Analyze signature argument type for *args and **kwargs argument."""
    if isinstance(t, UnboundType) and t.name and "." in t.name and not t.args:
        components = t.name.split(".")
        tvar_name = ".".join(components[:-1])
        sym = self.lookup_qualified(tvar_name, t)
        if sym is not None and isinstance(sym.node, ParamSpecExpr):
            tvar_def = self.tvar_scope.get_binding(sym)
            if isinstance(tvar_def, ParamSpecType):
                if kind == ARG_STAR:
                    make_paramspec = paramspec_args
                    if components[-1] != "args":
                        self.fail(
                            f'Use "{tvar_name}.args" for variadic "*" parameter',
                            t,
                            code=codes.VALID_TYPE,
                        )
                elif kind == ARG_STAR2:
                    make_paramspec = paramspec_kwargs
                    if components[-1] != "kwargs":
                        self.fail(
                            f'Use "{tvar_name}.kwargs" for variadic "**" parameter',
                            t,
                            code=codes.VALID_TYPE,
                        )
                else:
                    assert False, kind
                return make_paramspec(
                    tvar_def.name,
                    tvar_def.fullname,
                    tvar_def.id,
                    named_type_func=self.named_type,
                    line=t.line,
                    column=t.column,
                )
    return self.anal_type(t, nested=nested)

</t>
<t tx="ekr.20221004064035.2502">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    # Overloaded types are manually constructed in semanal.py by analyzing the
    # AST and combining together the Callable types this visitor converts.
    #
    # So if we're ever asked to reanalyze an Overloaded type, we know it's
    # fine to just return it as-is.
    return t

</t>
<t tx="ekr.20221004064035.2503">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    # Types such as (t1, t2, ...) only allowed in assignment statements. They'll
    # generate errors elsewhere, and Tuple[t1, t2, ...] must be used instead.
    if t.implicit and not self.allow_tuple_literal:
        self.fail("Syntax error in type annotation", t, code=codes.SYNTAX)
        if len(t.items) == 0:
            self.note(
                "Suggestion: Use Tuple[()] instead of () for an empty tuple, or "
                "None for a function without a return value",
                t,
                code=codes.SYNTAX,
            )
        elif len(t.items) == 1:
            self.note("Suggestion: Is there a spurious trailing comma?", t, code=codes.SYNTAX)
        else:
            self.note(
                "Suggestion: Use Tuple[T1, ..., Tn] instead of (T1, ..., Tn)",
                t,
                code=codes.SYNTAX,
            )
        return AnyType(TypeOfAny.from_error)
    star_count = sum(1 for item in t.items if isinstance(item, StarType))
    if star_count &gt; 1:
        self.fail("At most one star type allowed in a tuple", t)
        if t.implicit:
            return TupleType(
                [AnyType(TypeOfAny.from_error) for _ in t.items],
                self.named_type("builtins.tuple"),
                t.line,
            )
        else:
            return AnyType(TypeOfAny.from_error)
    any_type = AnyType(TypeOfAny.special_form)
    # If the fallback isn't filled in yet, its type will be the falsey FakeInfo
    fallback = (
        t.partial_fallback
        if t.partial_fallback.type
        else self.named_type("builtins.tuple", [any_type])
    )
    return TupleType(self.anal_array(t.items), fallback, t.line)

</t>
<t tx="ekr.20221004064035.2504">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = {
        item_name: self.anal_type(item_type) for (item_name, item_type) in t.items.items()
    }
    return TypedDictType(items, set(t.required_keys), t.fallback)

</t>
<t tx="ekr.20221004064035.2505">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; Type:
    # We should never see a bare Literal. We synthesize these raw literals
    # in the earlier stages of semantic analysis, but those
    # "fake literals" should always be wrapped in an UnboundType
    # corresponding to 'Literal'.
    #
    # Note: if at some point in the distant future, we decide to
    # make signatures like "foo(x: 20) -&gt; None" legal, we can change
    # this method so it generates and returns an actual LiteralType
    # instead.

    if self.report_invalid_types:
        if t.base_type_name in ("builtins.int", "builtins.bool"):
            # The only time it makes sense to use an int or bool is inside of
            # a literal type.
            msg = f"Invalid type: try using Literal[{repr(t.literal_value)}] instead?"
        elif t.base_type_name in ("builtins.float", "builtins.complex"):
            # We special-case warnings for floats and complex numbers.
            msg = f"Invalid type: {t.simple_name()} literals cannot be used as a type"
        else:
            # And in all other cases, we default to a generic error message.
            # Note: the reason why we use a generic error message for strings
            # but not ints or bools is because whenever we see an out-of-place
            # string, it's unclear if the user meant to construct a literal type
            # or just misspelled a regular type. So we avoid guessing.
            msg = "Invalid type comment or annotation"

        self.fail(msg, t, code=codes.VALID_TYPE)
        if t.note is not None:
            self.note(t.note, t, code=codes.VALID_TYPE)

    return AnyType(TypeOfAny.from_error, line=t.line, column=t.column)

</t>
<t tx="ekr.20221004064035.2506">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2507">def visit_star_type(self, t: StarType) -&gt; Type:
    return StarType(self.anal_type(t.type), t.line)

</t>
<t tx="ekr.20221004064035.2508">def visit_union_type(self, t: UnionType) -&gt; Type:
    if (
        t.uses_pep604_syntax is True
        and t.is_evaluated is True
        and not self.always_allow_new_syntax
        and not self.options.python_version &gt;= (3, 10)
    ):
        self.fail("X | Y syntax for unions requires Python 3.10", t, code=codes.SYNTAX)
    return UnionType(self.anal_array(t.items), t.line)

</t>
<t tx="ekr.20221004064035.2509">def visit_partial_type(self, t: PartialType) -&gt; Type:
    assert False, "Internal error: Unexpected partial type"

</t>
<t tx="ekr.20221004064035.251">def return_type_incompatible_with_supertype(
    self,
    name: str,
    name_in_supertype: str,
    supertype: str,
    original: Type,
    override: Type,
    context: Context,
) -&gt; None:
    target = self.override_target(name, name_in_supertype, supertype)
    override_str, original_str = format_type_distinctly(override, original)
    self.fail(
        'Return type {} of "{}" incompatible with return type {} in {}'.format(
            override_str, name, original_str, target
        ),
        context,
        code=codes.OVERRIDE,
    )

</t>
<t tx="ekr.20221004064035.2510">def visit_ellipsis_type(self, t: EllipsisType) -&gt; Type:
    if self.allow_param_spec_literals:
        any_type = AnyType(TypeOfAny.explicit)
        return Parameters(
            [any_type, any_type], [ARG_STAR, ARG_STAR2], [None, None], is_ellipsis_args=True
        )
    else:
        self.fail('Unexpected "..."', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064035.2511">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(self.anal_type(t.item), line=t.line)

</t>
<t tx="ekr.20221004064035.2512">def visit_placeholder_type(self, t: PlaceholderType) -&gt; Type:
    n = None if t.fullname is None else self.api.lookup_fully_qualified(t.fullname)
    if not n or isinstance(n.node, PlaceholderNode):
        self.api.defer()  # Still incomplete
        return t
    else:
        # TODO: Handle non-TypeInfo
        assert isinstance(n.node, TypeInfo)
        return self.analyze_type_with_type_info(n.node, t.args, t)

</t>
<t tx="ekr.20221004064035.2513">def analyze_callable_args_for_paramspec(
    self, callable_args: Type, ret_type: Type, fallback: Instance
) -&gt; CallableType | None:
    """Construct a 'Callable[P, RET]', where P is ParamSpec, return None if we cannot."""
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    tvar_def = self.tvar_scope.get_binding(sym)
    if not isinstance(tvar_def, ParamSpecType):
        return None

    return CallableType(
        [
            paramspec_args(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
            paramspec_kwargs(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
        ],
        [nodes.ARG_STAR, nodes.ARG_STAR2],
        [None, None],
        ret_type=ret_type,
        fallback=fallback,
    )

</t>
<t tx="ekr.20221004064035.2514">def analyze_callable_args_for_concatenate(
    self, callable_args: Type, ret_type: Type, fallback: Instance
) -&gt; CallableType | None:
    """Construct a 'Callable[C, RET]', where C is Concatenate[..., P], returning None if we
    cannot.
    """
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    if sym.node is None:
        return None
    if sym.node.fullname not in ("typing_extensions.Concatenate", "typing.Concatenate"):
        return None

    tvar_def = self.anal_type(callable_args, allow_param_spec=True)
    if not isinstance(tvar_def, ParamSpecType):
        return None

    # ick, CallableType should take ParamSpecType
    prefix = tvar_def.prefix
    # we don't set the prefix here as generic arguments will get updated at some point
    # in the future. CallableType.param_spec() accounts for this.
    return CallableType(
        [
            *prefix.arg_types,
            paramspec_args(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
            paramspec_kwargs(
                tvar_def.name, tvar_def.fullname, tvar_def.id, named_type_func=self.named_type
            ),
        ],
        [*prefix.arg_kinds, nodes.ARG_STAR, nodes.ARG_STAR2],
        [*prefix.arg_names, None, None],
        ret_type=ret_type,
        fallback=fallback,
        from_concatenate=True,
    )

</t>
<t tx="ekr.20221004064035.2515">def analyze_callable_type(self, t: UnboundType) -&gt; Type:
    fallback = self.named_type("builtins.function")
    if len(t.args) == 0:
        # Callable (bare). Treat as Callable[..., Any].
        any_type = self.get_omitted_any(t)
        ret = callable_with_ellipsis(any_type, any_type, fallback)
    elif len(t.args) == 2:
        callable_args = t.args[0]
        ret_type = t.args[1]
        if isinstance(callable_args, TypeList):
            # Callable[[ARG, ...], RET] (ordinary callable type)
            analyzed_args = self.analyze_callable_args(callable_args)
            if analyzed_args is None:
                return AnyType(TypeOfAny.from_error)
            args, kinds, names = analyzed_args
            ret = CallableType(args, kinds, names, ret_type=ret_type, fallback=fallback)
        elif isinstance(callable_args, EllipsisType):
            # Callable[..., RET] (with literal ellipsis; accept arbitrary arguments)
            ret = callable_with_ellipsis(
                AnyType(TypeOfAny.explicit), ret_type=ret_type, fallback=fallback
            )
        else:
            # Callable[P, RET] (where P is ParamSpec)
            maybe_ret = self.analyze_callable_args_for_paramspec(
                callable_args, ret_type, fallback
            ) or self.analyze_callable_args_for_concatenate(callable_args, ret_type, fallback)
            if maybe_ret is None:
                # Callable[?, RET] (where ? is something invalid)
                self.fail(
                    "The first argument to Callable must be a "
                    'list of types, parameter specification, or "..."',
                    t,
                    code=codes.VALID_TYPE,
                )
                self.note(
                    "See https://mypy.readthedocs.io/en/stable/kinds_of_types.html#callable-types-and-lambdas",  # noqa: E501
                    t,
                )
                return AnyType(TypeOfAny.from_error)
            ret = maybe_ret
    else:
        if self.options.disallow_any_generics:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]"', t)
        else:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]" or "Callable"', t)
        return AnyType(TypeOfAny.from_error)
    assert isinstance(ret, CallableType)
    return ret.accept(self)

</t>
<t tx="ekr.20221004064035.2516">def analyze_callable_args(
    self, arglist: TypeList
) -&gt; tuple[list[Type], list[ArgKind], list[str | None]] | None:
    args: list[Type] = []
    kinds: list[ArgKind] = []
    names: list[str | None] = []
    for arg in arglist.items:
        if isinstance(arg, CallableArgument):
            args.append(arg.typ)
            names.append(arg.name)
            if arg.constructor is None:
                return None
            found = self.lookup_qualified(arg.constructor, arg)
            if found is None:
                # Looking it up already put an error message in
                return None
            elif found.fullname not in ARG_KINDS_BY_CONSTRUCTOR:
                self.fail(f'Invalid argument constructor "{found.fullname}"', arg)
                return None
            else:
                assert found.fullname is not None
                kind = ARG_KINDS_BY_CONSTRUCTOR[found.fullname]
                kinds.append(kind)
                if arg.name is not None and kind.is_star():
                    self.fail(f"{arg.constructor} arguments should not have names", arg)
                    return None
        else:
            args.append(arg)
            kinds.append(ARG_POS)
            names.append(None)
    # Note that arglist below is only used for error context.
    check_arg_names(names, [arglist] * len(args), self.fail, "Callable")
    check_arg_kinds(kinds, [arglist] * len(args), self.fail)
    return args, kinds, names

</t>
<t tx="ekr.20221004064035.2517">def analyze_literal_type(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.fail("Literal[...] must have at least one parameter", t, code=codes.VALID_TYPE)
        return AnyType(TypeOfAny.from_error)

    output: list[Type] = []
    for i, arg in enumerate(t.args):
        analyzed_types = self.analyze_literal_param(i + 1, arg, t)
        if analyzed_types is None:
            return AnyType(TypeOfAny.from_error)
        else:
            output.extend(analyzed_types)
    return UnionType.make_union(output, line=t.line)

</t>
<t tx="ekr.20221004064035.2518">def analyze_literal_param(self, idx: int, arg: Type, ctx: Context) -&gt; list[Type] | None:
    # This UnboundType was originally defined as a string.
    if isinstance(arg, UnboundType) and arg.original_str_expr is not None:
        assert arg.original_str_fallback is not None
        return [
            LiteralType(
                value=arg.original_str_expr,
                fallback=self.named_type(arg.original_str_fallback),
                line=arg.line,
                column=arg.column,
            )
        ]

    # If arg is an UnboundType that was *not* originally defined as
    # a string, try expanding it in case it's a type alias or something.
    if isinstance(arg, UnboundType):
        self.nesting_level += 1
        try:
            arg = self.visit_unbound_type(arg, defining_literal=True)
        finally:
            self.nesting_level -= 1

    # Literal[...] cannot contain Any. Give up and add an error message
    # (if we haven't already).
    arg = get_proper_type(arg)
    if isinstance(arg, AnyType):
        # Note: We can encounter Literals containing 'Any' under three circumstances:
        #
        # 1. If the user attempts use an explicit Any as a parameter
        # 2. If the user is trying to use an enum value imported from a module with
        #    no type hints, giving it an implicit type of 'Any'
        # 3. If there's some other underlying problem with the parameter.
        #
        # We report an error in only the first two cases. In the third case, we assume
        # some other region of the code has already reported a more relevant error.
        #
        # TODO: Once we start adding support for enums, make sure we report a custom
        # error for case 2 as well.
        if arg.type_of_any not in (TypeOfAny.from_error, TypeOfAny.special_form):
            self.fail(
                f'Parameter {idx} of Literal[...] cannot be of type "Any"',
                ctx,
                code=codes.VALID_TYPE,
            )
        return None
    elif isinstance(arg, RawExpressionType):
        # A raw literal. Convert it directly into a literal if we can.
        if arg.literal_value is None:
            name = arg.simple_name()
            if name in ("float", "complex"):
                msg = f'Parameter {idx} of Literal[...] cannot be of type "{name}"'
            else:
                msg = "Invalid type: Literal[...] cannot contain arbitrary expressions"
            self.fail(msg, ctx, code=codes.VALID_TYPE)
            # Note: we deliberately ignore arg.note here: the extra info might normally be
            # helpful, but it generally won't make sense in the context of a Literal[...].
            return None

        # Remap bytes and unicode into the appropriate type for the correct Python version
        fallback = self.named_type(arg.base_type_name)
        assert isinstance(fallback, Instance)
        return [LiteralType(arg.literal_value, fallback, line=arg.line, column=arg.column)]
    elif isinstance(arg, (NoneType, LiteralType)):
        # Types that we can just add directly to the literal/potential union of literals.
        return [arg]
    elif isinstance(arg, Instance) and arg.last_known_value is not None:
        # Types generated from declarations like "var: Final = 4".
        return [arg.last_known_value]
    elif isinstance(arg, UnionType):
        out = []
        for union_arg in arg.items:
            union_result = self.analyze_literal_param(idx, union_arg, ctx)
            if union_result is None:
                return None
            out.extend(union_result)
        return out
    else:
        self.fail(f"Parameter {idx} of Literal[...] is invalid", ctx, code=codes.VALID_TYPE)
        return None

</t>
<t tx="ekr.20221004064035.2519">def analyze_type(self, t: Type) -&gt; Type:
    return t.accept(self)

</t>
<t tx="ekr.20221004064035.252">def override_target(self, name: str, name_in_super: str, supertype: str) -&gt; str:
    target = f'supertype "{supertype}"'
    if name_in_super != name:
        target = f'"{name_in_super}" of {target}'
    return target

</t>
<t tx="ekr.20221004064035.2520">def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.fail_func(msg, ctx, code=code)

</t>
<t tx="ekr.20221004064035.2521">def note(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    self.note_func(msg, ctx, code=code)

</t>
<t tx="ekr.20221004064035.2522">@contextmanager
def tvar_scope_frame(self) -&gt; Iterator[None]:
    old_scope = self.tvar_scope
    self.tvar_scope = self.tvar_scope.method_frame()
    yield
    self.tvar_scope = old_scope

</t>
<t tx="ekr.20221004064035.2523">def infer_type_variables(self, type: CallableType) -&gt; list[tuple[str, TypeVarLikeExpr]]:
    """Return list of unique type variables referred to in a callable."""
    names: list[str] = []
    tvars: list[TypeVarLikeExpr] = []
    for arg in type.arg_types:
        for name, tvar_expr in arg.accept(
            TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope)
        ):
            if name not in names:
                names.append(name)
                tvars.append(tvar_expr)
    # When finding type variables in the return type of a function, don't
    # look inside Callable types.  Type variables only appearing in
    # functions in the return type belong to those functions, not the
    # function we're currently analyzing.
    for name, tvar_expr in type.ret_type.accept(
        TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope, include_callables=False)
    ):
        if name not in names:
            names.append(name)
            tvars.append(tvar_expr)
    return list(zip(names, tvars))

</t>
<t tx="ekr.20221004064035.2524">def bind_function_type_variables(
    self, fun_type: CallableType, defn: Context
) -&gt; Sequence[TypeVarLikeType]:
    """Find the type variables of the function type and bind them in our tvar_scope"""
    if fun_type.variables:
        defs = []
        for var in fun_type.variables:
            var_node = self.lookup_qualified(var.name, defn)
            assert var_node, "Binding for function type variable not found within function"
            var_expr = var_node.node
            assert isinstance(var_expr, TypeVarLikeExpr)
            binding = self.tvar_scope.bind_new(var.name, var_expr)
            defs.append(binding)
        return defs
    typevars = self.infer_type_variables(fun_type)
    # Do not define a new type variable if already defined in scope.
    typevars = [
        (name, tvar) for name, tvar in typevars if not self.is_defined_type_var(name, defn)
    ]
    defs = []
    for name, tvar in typevars:
        if not self.tvar_scope.allow_binding(tvar.fullname):
            self.fail(
                f'Type variable "{name}" is bound by an outer class',
                defn,
                code=codes.VALID_TYPE,
            )
        binding = self.tvar_scope.bind_new(name, tvar)
        defs.append(binding)

    return defs

</t>
<t tx="ekr.20221004064035.2525">def is_defined_type_var(self, tvar: str, context: Context) -&gt; bool:
    tvar_node = self.lookup_qualified(tvar, context)
    if not tvar_node:
        return False
    return self.tvar_scope.get_binding(tvar_node) is not None

</t>
<t tx="ekr.20221004064035.2526">def anal_array(
    self, a: Iterable[Type], nested: bool = True, *, allow_param_spec: bool = False
) -&gt; list[Type]:
    res: list[Type] = []
    for t in a:
        res.append(self.anal_type(t, nested, allow_param_spec=allow_param_spec))
    return res

</t>
<t tx="ekr.20221004064035.2527">def anal_type(self, t: Type, nested: bool = True, *, allow_param_spec: bool = False) -&gt; Type:
    if nested:
        self.nesting_level += 1
    old_allow_required = self.allow_required
    self.allow_required = False
    try:
        analyzed = t.accept(self)
    finally:
        if nested:
            self.nesting_level -= 1
        self.allow_required = old_allow_required
    if (
        not allow_param_spec
        and isinstance(analyzed, ParamSpecType)
        and analyzed.flavor == ParamSpecFlavor.BARE
    ):
        if analyzed.prefix.arg_types:
            self.fail("Invalid location for Concatenate", t, code=codes.VALID_TYPE)
            self.note("You can use Concatenate as the first argument to Callable", t)
        else:
            self.fail(
                f'Invalid location for ParamSpec "{analyzed.name}"', t, code=codes.VALID_TYPE
            )
            self.note(
                "You can use ParamSpec as the first argument to Callable, e.g., "
                "'Callable[{}, int]'".format(analyzed.name),
                t,
            )
    return analyzed

</t>
<t tx="ekr.20221004064035.2528">def anal_var_def(self, var_def: TypeVarLikeType) -&gt; TypeVarLikeType:
    if isinstance(var_def, TypeVarType):
        return TypeVarType(
            var_def.name,
            var_def.fullname,
            var_def.id.raw_id,
            self.anal_array(var_def.values),
            var_def.upper_bound.accept(self),
            var_def.variance,
            var_def.line,
        )
    else:
        return var_def

</t>
<t tx="ekr.20221004064035.2529">def anal_var_defs(self, var_defs: Sequence[TypeVarLikeType]) -&gt; list[TypeVarLikeType]:
    return [self.anal_var_def(vd) for vd in var_defs]

</t>
<t tx="ekr.20221004064035.253">def incompatible_type_application(
    self, expected_arg_count: int, actual_arg_count: int, context: Context
) -&gt; None:
    if expected_arg_count == 0:
        self.fail("Type application targets a non-generic function or class", context)
    elif actual_arg_count &gt; expected_arg_count:
        self.fail(
            f"Type application has too many types ({expected_arg_count} expected)", context
        )
    else:
        self.fail(
            f"Type application has too few types ({expected_arg_count} expected)", context
        )

</t>
<t tx="ekr.20221004064035.2530">def named_type(
    self,
    fully_qualified_name: str,
    args: list[Type] | None = None,
    line: int = -1,
    column: int = -1,
) -&gt; Instance:
    node = self.lookup_fqn_func(fully_qualified_name)
    assert isinstance(node.node, TypeInfo)
    any_type = AnyType(TypeOfAny.special_form)
    return Instance(
        node.node, args or [any_type] * len(node.node.defn.type_vars), line=line, column=column
    )

</t>
<t tx="ekr.20221004064035.2531">def tuple_type(self, items: list[Type]) -&gt; TupleType:
    any_type = AnyType(TypeOfAny.special_form)
    return TupleType(items, fallback=self.named_type("builtins.tuple", [any_type]))

</t>
<t tx="ekr.20221004064035.2532">@contextmanager
def set_allow_param_spec_literals(self, to: bool) -&gt; Iterator[None]:
    old = self.allow_param_spec_literals
    try:
        self.allow_param_spec_literals = to
        yield
    finally:
        self.allow_param_spec_literals = old


</t>
<t tx="ekr.20221004064035.2533">TypeVarLikeList = List[Tuple[str, TypeVarLikeExpr]]


</t>
<t tx="ekr.20221004064035.2534">class MsgCallback(Protocol):
    @others
</t>
<t tx="ekr.20221004064035.2535">def __call__(self, __msg: str, __ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    ...


</t>
<t tx="ekr.20221004064035.2536">def get_omitted_any(
    disallow_any: bool,
    fail: MsgCallback,
    note: MsgCallback,
    orig_type: Type,
    python_version: tuple[int, int],
    fullname: str | None = None,
    unexpanded_type: Type | None = None,
) -&gt; AnyType:
    if disallow_any:
        nongen_builtins = get_nongen_builtins(python_version)
        if fullname in nongen_builtins:
            typ = orig_type
            # We use a dedicated error message for builtin generics (as the most common case).
            alternative = nongen_builtins[fullname]
            fail(
                message_registry.IMPLICIT_GENERIC_ANY_BUILTIN.format(alternative),
                typ,
                code=codes.TYPE_ARG,
            )
        else:
            typ = unexpanded_type or orig_type
            type_str = typ.name if isinstance(typ, UnboundType) else format_type_bare(typ)

            fail(
                message_registry.BARE_GENERIC.format(quote_type_string(type_str)),
                typ,
                code=codes.TYPE_ARG,
            )
            base_type = get_proper_type(orig_type)
            base_fullname = (
                base_type.type.fullname if isinstance(base_type, Instance) else fullname
            )
            # Ideally, we'd check whether the type is quoted or `from __future__ annotations`
            # is set before issuing this note
            if python_version &lt; (3, 9) and base_fullname in GENERIC_STUB_NOT_AT_RUNTIME_TYPES:
                # Recommend `from __future__ import annotations` or to put type in quotes
                # (string literal escaping) for classes not generic at runtime
                note(
                    "Subscripting classes that are not generic at runtime may require "
                    "escaping, see https://mypy.readthedocs.io/en/stable/runtime_troubles.html"
                    "#not-generic-runtime",
                    typ,
                    code=codes.TYPE_ARG,
                )

        any_type = AnyType(TypeOfAny.from_error, line=typ.line, column=typ.column)
    else:
        any_type = AnyType(
            TypeOfAny.from_omitted_generics, line=orig_type.line, column=orig_type.column
        )
    return any_type


</t>
<t tx="ekr.20221004064035.2537">def fix_instance(
    t: Instance,
    fail: MsgCallback,
    note: MsgCallback,
    disallow_any: bool,
    python_version: tuple[int, int],
    use_generic_error: bool = False,
    unexpanded_type: Type | None = None,
) -&gt; None:
    """Fix a malformed instance by replacing all type arguments with Any.

    Also emit a suitable error if this is not due to implicit Any's.
    """
    if len(t.args) == 0:
        if use_generic_error:
            fullname: str | None = None
        else:
            fullname = t.type.fullname
        any_type = get_omitted_any(
            disallow_any, fail, note, t, python_version, fullname, unexpanded_type
        )
        t.args = (any_type,) * len(t.type.type_vars)
        return
    # Invalid number of type parameters.
    n = len(t.type.type_vars)
    s = f"{n} type arguments"
    if n == 0:
        s = "no type arguments"
    elif n == 1:
        s = "1 type argument"
    act = str(len(t.args))
    if act == "0":
        act = "none"
    fail(f'"{t.type.name}" expects {s}, but {act} given', t, code=codes.TYPE_ARG)
    # Construct the correct number of type arguments, as
    # otherwise the type checker may crash as it expects
    # things to be right.
    t.args = tuple(AnyType(TypeOfAny.from_error) for _ in t.type.type_vars)
    t.invalid = True


</t>
<t tx="ekr.20221004064035.2538">def expand_type_alias(
    node: TypeAlias,
    args: list[Type],
    fail: MsgCallback,
    no_args: bool,
    ctx: Context,
    *,
    unexpanded_type: Type | None = None,
    disallow_any: bool = False,
) -&gt; Type:
    """Expand a (generic) type alias target following the rules outlined in TypeAlias docstring.

    Here:
        target: original target type (contains unbound type variables)
        alias_tvars: type variable names
        args: types to be substituted in place of type variables
        fail: error reporter callback
        no_args: whether original definition used a bare generic `A = List`
        ctx: context where expansion happens
    """
    exp_len = len(node.alias_tvars)
    act_len = len(args)
    if exp_len &gt; 0 and act_len == 0:
        # Interpret bare Alias same as normal generic, i.e., Alias[Any, Any, ...]
        return set_any_tvars(
            node,
            ctx.line,
            ctx.column,
            disallow_any=disallow_any,
            fail=fail,
            unexpanded_type=unexpanded_type,
        )
    if exp_len == 0 and act_len == 0:
        if no_args:
            assert isinstance(node.target, Instance)  # type: ignore[misc]
            # Note: this is the only case where we use an eager expansion. See more info about
            # no_args aliases like L = List in the docstring for TypeAlias class.
            return Instance(node.target.type, [], line=ctx.line, column=ctx.column)
        return TypeAliasType(node, [], line=ctx.line, column=ctx.column)
    if (
        exp_len == 0
        and act_len &gt; 0
        and isinstance(node.target, Instance)  # type: ignore[misc]
        and no_args
    ):
        tp = Instance(node.target.type, args)
        tp.line = ctx.line
        tp.column = ctx.column
        return tp
    if act_len != exp_len:
        fail(f"Bad number of arguments for type alias, expected: {exp_len}, given: {act_len}", ctx)
        return set_any_tvars(node, ctx.line, ctx.column, from_error=True)
    typ = TypeAliasType(node, args, ctx.line, ctx.column)
    assert typ.alias is not None
    # HACK: Implement FlexibleAlias[T, typ] by expanding it to typ here.
    if (
        isinstance(typ.alias.target, Instance)  # type: ignore[misc]
        and typ.alias.target.type.fullname == "mypy_extensions.FlexibleAlias"
    ):
        exp = get_proper_type(typ)
        assert isinstance(exp, Instance)
        return exp.args[-1]
    return typ


</t>
<t tx="ekr.20221004064035.2539">def set_any_tvars(
    node: TypeAlias,
    newline: int,
    newcolumn: int,
    *,
    from_error: bool = False,
    disallow_any: bool = False,
    fail: MsgCallback | None = None,
    unexpanded_type: Type | None = None,
) -&gt; Type:
    if from_error or disallow_any:
        type_of_any = TypeOfAny.from_error
    else:
        type_of_any = TypeOfAny.from_omitted_generics
    if disallow_any and node.alias_tvars:
        assert fail is not None
        if unexpanded_type:
            type_str = (
                unexpanded_type.name
                if isinstance(unexpanded_type, UnboundType)
                else format_type_bare(unexpanded_type)
            )
        else:
            type_str = node.name

        fail(
            message_registry.BARE_GENERIC.format(quote_type_string(type_str)),
            Context(newline, newcolumn),
            code=codes.TYPE_ARG,
        )
    any_type = AnyType(type_of_any, line=newline, column=newcolumn)
    return TypeAliasType(node, [any_type] * len(node.alias_tvars), newline, newcolumn)


</t>
<t tx="ekr.20221004064035.254">def could_not_infer_type_arguments(
    self, callee_type: CallableType, n: int, context: Context
) -&gt; None:
    callee_name = callable_name(callee_type)
    if callee_name is not None and n &gt; 0:
        self.fail(f"Cannot infer type argument {n} of {callee_name}", context)
    else:
        self.fail("Cannot infer function type argument", context)

</t>
<t tx="ekr.20221004064035.2540">def remove_dups(tvars: Iterable[T]) -&gt; list[T]:
    # Get unique elements in order of appearance
    all_tvars: set[T] = set()
    new_tvars: list[T] = []
    for t in tvars:
        if t not in all_tvars:
            new_tvars.append(t)
            all_tvars.add(t)
    return new_tvars


</t>
<t tx="ekr.20221004064035.2541">def flatten_tvars(ll: Iterable[list[T]]) -&gt; list[T]:
    return remove_dups(chain.from_iterable(ll))


</t>
<t tx="ekr.20221004064035.2542">class TypeVarLikeQuery(TypeQuery[TypeVarLikeList]):
    """Find TypeVar and ParamSpec references in an unbound type."""

    @others
</t>
<t tx="ekr.20221004064035.2543">def __init__(
    self,
    lookup: Callable[[str, Context], SymbolTableNode | None],
    scope: TypeVarLikeScope,
    *,
    include_callables: bool = True,
    include_bound_tvars: bool = False,
) -&gt; None:
    self.include_callables = include_callables
    self.lookup = lookup
    self.scope = scope
    self.include_bound_tvars = include_bound_tvars
    super().__init__(flatten_tvars)
    # Only include type variables in type aliases args. This would be anyway
    # that case if we expand (as target variables would be overridden with args)
    # and it may cause infinite recursion on invalid (diverging) recursive aliases.
    self.skip_alias_target = True

</t>
<t tx="ekr.20221004064035.2544">def _seems_like_callable(self, type: UnboundType) -&gt; bool:
    if not type.args:
        return False
    if isinstance(type.args[0], (EllipsisType, TypeList, ParamSpecType)):
        return True
    return False

</t>
<t tx="ekr.20221004064035.2545">def visit_unbound_type(self, t: UnboundType) -&gt; TypeVarLikeList:
    name = t.name
    node = None
    # Special case P.args and P.kwargs for ParamSpecs only.
    if name.endswith("args"):
        if name.endswith(".args") or name.endswith(".kwargs"):
            base = ".".join(name.split(".")[:-1])
            n = self.lookup(base, t)
            if n is not None and isinstance(n.node, ParamSpecExpr):
                node = n
                name = base
    if node is None:
        node = self.lookup(name, t)
    if (
        node
        and isinstance(node.node, TypeVarLikeExpr)
        and (self.include_bound_tvars or self.scope.get_binding(node) is None)
    ):
        assert isinstance(node.node, TypeVarLikeExpr)
        return [(name, node.node)]
    elif not self.include_callables and self._seems_like_callable(t):
        return []
    elif node and node.fullname in LITERAL_TYPE_NAMES:
        return []
    elif node and node.fullname in ANNOTATED_TYPE_NAMES and t.args:
        # Don't query the second argument to Annotated for TypeVars
        return self.query_types([t.args[0]])
    else:
        return super().visit_unbound_type(t)

</t>
<t tx="ekr.20221004064035.2546">def visit_callable_type(self, t: CallableType) -&gt; TypeVarLikeList:
    if self.include_callables:
        return super().visit_callable_type(t)
    else:
        return []


</t>
<t tx="ekr.20221004064035.2547">class DivergingAliasDetector(TrivialSyntheticTypeTranslator):
    """See docstring of detect_diverging_alias() for details."""

    @others
</t>
<t tx="ekr.20221004064035.2548"># TODO: this doesn't really need to be a translator, but we don't have a trivial visitor.
def __init__(
    self,
    seen_nodes: set[TypeAlias],
    lookup: Callable[[str, Context], SymbolTableNode | None],
    scope: TypeVarLikeScope,
) -&gt; None:
    self.seen_nodes = seen_nodes
    self.lookup = lookup
    self.scope = scope
    self.diverging = False

</t>
<t tx="ekr.20221004064035.2549">def is_alias_tvar(self, t: Type) -&gt; bool:
    # Generic type aliases use unbound type variables.
    if not isinstance(t, UnboundType) or t.args:
        return False
    node = self.lookup(t.name, t)
    if (
        node
        and isinstance(node.node, TypeVarLikeExpr)
        and self.scope.get_binding(node) is None
    ):
        return True
    return False

</t>
<t tx="ekr.20221004064035.255">def invalid_var_arg(self, typ: Type, context: Context) -&gt; None:
    self.fail("List or tuple expected as variadic arguments", context)

</t>
<t tx="ekr.20221004064035.2550">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    assert t.alias is not None, f"Unfixed type alias {t.type_ref}"
    if t.alias in self.seen_nodes:
        for arg in t.args:
            if not self.is_alias_tvar(arg) and bool(
                arg.accept(TypeVarLikeQuery(self.lookup, self.scope))
            ):
                self.diverging = True
                return t
        # All clear for this expansion chain.
        return t
    new_nodes = self.seen_nodes | {t.alias}
    visitor = DivergingAliasDetector(new_nodes, self.lookup, self.scope)
    _ = get_proper_type(t).accept(visitor)
    if visitor.diverging:
        self.diverging = True
    return t


</t>
<t tx="ekr.20221004064035.2551">def detect_diverging_alias(
    node: TypeAlias,
    target: Type,
    lookup: Callable[[str, Context], SymbolTableNode | None],
    scope: TypeVarLikeScope,
) -&gt; bool:
    """This detects type aliases that will diverge during type checking.

    For example F = Something[..., F[List[T]]]. At each expansion step this will produce
    *new* type aliases: e.g. F[List[int]], F[List[List[int]]], etc. So we can't detect
    recursion. It is a known problem in the literature, recursive aliases and generic types
    don't always go well together. It looks like there is no known systematic solution yet.

    # TODO: should we handle such aliases using type_recursion counter and some large limit?
    They may be handy in rare cases, e.g. to express a union of non-mixed nested lists:
    Nested = Union[T, Nested[List[T]]] ~&gt; Union[T, List[T], List[List[T]], ...]
    """
    visitor = DivergingAliasDetector({node}, lookup, scope)
    _ = target.accept(visitor)
    return visitor.diverging


</t>
<t tx="ekr.20221004064035.2552">def check_for_explicit_any(
    typ: Type | None,
    options: Options,
    is_typeshed_stub: bool,
    msg: MessageBuilder,
    context: Context,
) -&gt; None:
    if options.disallow_any_explicit and not is_typeshed_stub and typ and has_explicit_any(typ):
        msg.explicit_any(context)


</t>
<t tx="ekr.20221004064035.2553">def has_explicit_any(t: Type) -&gt; bool:
    """
    Whether this type is or type it contains is an Any coming from explicit type annotation
    """
    return t.accept(HasExplicitAny())


</t>
<t tx="ekr.20221004064035.2554">class HasExplicitAny(TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20221004064035.2555">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20221004064035.2556">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any == TypeOfAny.explicit

</t>
<t tx="ekr.20221004064035.2557">def visit_typeddict_type(self, t: TypedDictType) -&gt; bool:
    # typeddict is checked during TypedDict declaration, so don't typecheck it here.
    return False


</t>
<t tx="ekr.20221004064035.2558">def has_any_from_unimported_type(t: Type) -&gt; bool:
    """Return true if this type is Any because an import was not followed.

    If type t is such Any type or has type arguments that contain such Any type
    this function will return true.
    """
    return t.accept(HasAnyFromUnimportedType())


</t>
<t tx="ekr.20221004064035.2559">class HasAnyFromUnimportedType(TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20221004064035.256">def invalid_keyword_var_arg(self, typ: Type, is_mapping: bool, context: Context) -&gt; None:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and is_mapping:
        self.fail("Keywords must be strings", context)
    else:
        self.fail(
            f"Argument after ** must be a mapping, not {format_type(typ)}",
            context,
            code=codes.ARG_TYPE,
        )

</t>
<t tx="ekr.20221004064035.2560">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20221004064035.2561">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any == TypeOfAny.from_unimported_type

</t>
<t tx="ekr.20221004064035.2562">def visit_typeddict_type(self, t: TypedDictType) -&gt; bool:
    # typeddict is checked during TypedDict declaration, so don't typecheck it here
    return False


</t>
<t tx="ekr.20221004064035.2563">def collect_all_inner_types(t: Type) -&gt; list[Type]:
    """
    Return all types that `t` contains
    """
    return t.accept(CollectAllInnerTypesQuery())


</t>
<t tx="ekr.20221004064035.2564">class CollectAllInnerTypesQuery(TypeQuery[List[Type]]):
    @others
</t>
<t tx="ekr.20221004064035.2565">def __init__(self) -&gt; None:
    super().__init__(self.combine_lists_strategy)

</t>
<t tx="ekr.20221004064035.2566">def query_types(self, types: Iterable[Type]) -&gt; list[Type]:
    return self.strategy([t.accept(self) for t in types]) + list(types)

</t>
<t tx="ekr.20221004064035.2567">@classmethod
def combine_lists_strategy(cls, it: Iterable[list[Type]]) -&gt; list[Type]:
    return list(itertools.chain.from_iterable(it))


</t>
<t tx="ekr.20221004064035.2568">def make_optional_type(t: Type) -&gt; Type:
    """Return the type corresponding to Optional[t].

    Note that we can't use normal union simplification, since this function
    is called during semantic analysis and simplification only works during
    type checking.
    """
    p_t = get_proper_type(t)
    if isinstance(p_t, NoneType):
        return t
    elif isinstance(p_t, UnionType):
        # Eagerly expanding aliases is not safe during semantic analysis.
        items = [
            item
            for item in flatten_nested_unions(p_t.items, handle_type_alias_type=False)
            if not isinstance(get_proper_type(item), NoneType)
        ]
        return UnionType(items + [NoneType()], t.line, t.column)
    else:
        return UnionType([t, NoneType()], t.line, t.column)


</t>
<t tx="ekr.20221004064035.2569">def fix_instance_types(
    t: Type, fail: MsgCallback, note: MsgCallback, python_version: tuple[int, int]
) -&gt; None:
    """Recursively fix all instance types (type argument count) in a given type.

    For example 'Union[Dict, List[str, int]]' will be transformed into
    'Union[Dict[Any, Any], List[Any]]' in place.
    """
    t.accept(InstanceFixer(fail, note, python_version))


</t>
<t tx="ekr.20221004064035.257">def undefined_in_superclass(self, member: str, context: Context) -&gt; None:
    self.fail(f'"{member}" undefined in superclass', context)

</t>
<t tx="ekr.20221004064035.2570">class InstanceFixer(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.2571">def __init__(
    self, fail: MsgCallback, note: MsgCallback, python_version: tuple[int, int]
) -&gt; None:
    self.fail = fail
    self.note = note
    self.python_version = python_version

</t>
<t tx="ekr.20221004064035.2572">def visit_instance(self, typ: Instance) -&gt; None:
    super().visit_instance(typ)
    if len(typ.args) != len(typ.type.type_vars):
        fix_instance(
            typ,
            self.fail,
            self.note,
            disallow_any=False,
            python_version=self.python_version,
            use_generic_error=True,
        )
</t>
<t tx="ekr.20221004064035.2573">@path C:/Repos/ekr-mypy2/mypy/
"""Miscellaneous type operations and helpers for use during type checking.

NOTE: These must not be accessed from mypy.nodes or mypy.types to avoid import
      cycles. These must not be called from the semantic analysis main pass
      since these may assume that MROs are ready.
"""

from __future__ import annotations

import itertools
from typing import Any, Iterable, List, Sequence, TypeVar, cast

from mypy.copytype import copy_type
from mypy.expandtype import expand_type, expand_type_by_instance
from mypy.maptype import map_instance_to_supertype
from mypy.nodes import (
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    SYMBOL_FUNCBASE_TYPES,
    Decorator,
    Expression,
    FuncBase,
    FuncDef,
    FuncItem,
    OverloadedFuncDef,
    StrExpr,
    TypeInfo,
    Var,
)
from mypy.state import state
from mypy.types import (
    ENUM_REMOVED_PROPS,
    AnyType,
    CallableType,
    FormalArgument,
    FunctionLike,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    ProperType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeOfAny,
    TypeQuery,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UninhabitedType,
    UnionType,
    UnpackType,
    flatten_nested_unions,
    get_proper_type,
    get_proper_types,
)
from mypy.typevars import fill_typevars


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2574">def is_recursive_pair(s: Type, t: Type) -&gt; bool:
    """Is this a pair of recursive types?

    There may be more cases, and we may be forced to use e.g. has_recursive_types()
    here, but this function is called in very hot code, so we try to keep it simple
    and return True only in cases we know may have problems.
    """
    if isinstance(s, TypeAliasType) and s.is_recursive:
        return (
            isinstance(get_proper_type(t), Instance)
            or isinstance(t, TypeAliasType)
            and t.is_recursive
        )
    if isinstance(t, TypeAliasType) and t.is_recursive:
        return (
            isinstance(get_proper_type(s), Instance)
            or isinstance(s, TypeAliasType)
            and s.is_recursive
        )
    return False


</t>
<t tx="ekr.20221004064035.2575">def tuple_fallback(typ: TupleType) -&gt; Instance:
    """Return fallback type for a tuple."""
    from mypy.join import join_type_list

    info = typ.partial_fallback.type
    if info.fullname != "builtins.tuple":
        return typ.partial_fallback
    items = []
    for item in typ.items:
        if isinstance(item, UnpackType):
            unpacked_type = get_proper_type(item.type)
            if isinstance(unpacked_type, TypeVarTupleType):
                items.append(unpacked_type.upper_bound)
            elif isinstance(unpacked_type, TupleType):
                # TODO: might make sense to do recursion here to support nested unpacks
                # of tuple constants
                items.extend(unpacked_type.items)
            else:
                raise NotImplementedError
        else:
            items.append(item)
    return Instance(info, [join_type_list(items)], extra_attrs=typ.partial_fallback.extra_attrs)


</t>
<t tx="ekr.20221004064035.2576">def get_self_type(func: CallableType, default_self: Instance | TupleType) -&gt; Type | None:
    if isinstance(get_proper_type(func.ret_type), UninhabitedType):
        return func.ret_type
    elif func.arg_types and func.arg_types[0] != default_self and func.arg_kinds[0] == ARG_POS:
        return func.arg_types[0]
    else:
        return None


</t>
<t tx="ekr.20221004064035.2577">def type_object_type_from_function(
    signature: FunctionLike, info: TypeInfo, def_info: TypeInfo, fallback: Instance, is_new: bool
) -&gt; FunctionLike:
    # We first need to record all non-trivial (explicit) self types in __init__,
    # since they will not be available after we bind them. Note, we use explicit
    # self-types only in the defining class, similar to __new__ (but not exactly the same,
    # see comment in class_callable below). This is mostly useful for annotating library
    # classes such as subprocess.Popen.
    default_self = fill_typevars(info)
    if not is_new and not info.is_newtype:
        orig_self_types = [get_self_type(it, default_self) for it in signature.items]
    else:
        orig_self_types = [None] * len(signature.items)

    # The __init__ method might come from a generic superclass 'def_info'
    # with type variables that do not map identically to the type variables of
    # the class 'info' being constructed. For example:
    #
    #   class A(Generic[T]):
    #       def __init__(self, x: T) -&gt; None: ...
    #   class B(A[List[T]]):
    #      ...
    #
    # We need to map B's __init__ to the type (List[T]) -&gt; None.
    signature = bind_self(signature, original_type=default_self, is_classmethod=is_new)
    signature = cast(FunctionLike, map_type_from_supertype(signature, info, def_info))

    special_sig: str | None = None
    if def_info.fullname == "builtins.dict":
        # Special signature!
        special_sig = "dict"

    if isinstance(signature, CallableType):
        return class_callable(signature, info, fallback, special_sig, is_new, orig_self_types[0])
    else:
        # Overloaded __init__/__new__.
        assert isinstance(signature, Overloaded)
        items: list[CallableType] = []
        for item, orig_self in zip(signature.items, orig_self_types):
            items.append(class_callable(item, info, fallback, special_sig, is_new, orig_self))
        return Overloaded(items)


</t>
<t tx="ekr.20221004064035.2578">def class_callable(
    init_type: CallableType,
    info: TypeInfo,
    type_type: Instance,
    special_sig: str | None,
    is_new: bool,
    orig_self_type: Type | None = None,
) -&gt; CallableType:
    """Create a type object type based on the signature of __init__."""
    variables: list[TypeVarLikeType] = []
    variables.extend(info.defn.type_vars)
    variables.extend(init_type.variables)

    from mypy.subtypes import is_subtype

    init_ret_type = get_proper_type(init_type.ret_type)
    orig_self_type = get_proper_type(orig_self_type)
    default_ret_type = fill_typevars(info)
    explicit_type = init_ret_type if is_new else orig_self_type
    if (
        isinstance(explicit_type, (Instance, TupleType, UninhabitedType))
        # We have to skip protocols, because it can be a subtype of a return type
        # by accident. Like `Hashable` is a subtype of `object`. See #11799
        and isinstance(default_ret_type, Instance)
        and not default_ret_type.type.is_protocol
        # Only use the declared return type from __new__ or declared self in __init__
        # if it is actually returning a subtype of what we would return otherwise.
        and is_subtype(explicit_type, default_ret_type, ignore_type_params=True)
    ):
        ret_type: Type = explicit_type
    else:
        ret_type = default_ret_type

    callable_type = init_type.copy_modified(
        ret_type=ret_type,
        fallback=type_type,
        name=None,
        variables=variables,
        special_sig=special_sig,
    )
    c = callable_type.with_name(info.name)
    return c


</t>
<t tx="ekr.20221004064035.2579">def map_type_from_supertype(typ: Type, sub_info: TypeInfo, super_info: TypeInfo) -&gt; Type:
    """Map type variables in a type defined in a supertype context to be valid
    in the subtype context. Assume that the result is unique; if more than
    one type is possible, return one of the alternatives.

    For example, assume

      class D(Generic[S]): ...
      class C(D[E[T]], Generic[T]): ...

    Now S in the context of D would be mapped to E[T] in the context of C.
    """
    # Create the type of self in subtype, of form t[a1, ...].
    inst_type = fill_typevars(sub_info)
    if isinstance(inst_type, TupleType):
        inst_type = tuple_fallback(inst_type)
    # Map the type of self to supertype. This gets us a description of the
    # supertype type variables in terms of subtype variables, i.e. t[t1, ...]
    # so that any type variables in tN are to be interpreted in subtype
    # context.
    inst_type = map_instance_to_supertype(inst_type, super_info)
    # Finally expand the type variables in type with those in the previously
    # constructed type. Note that both type and inst_type may have type
    # variables, but in type they are interpreted in supertype context while
    # in inst_type they are interpreted in subtype context. This works even if
    # the names of type variables in supertype and subtype overlap.
    return expand_type_by_instance(typ, inst_type)


</t>
<t tx="ekr.20221004064035.258">def variable_may_be_undefined(self, name: str, context: Context) -&gt; None:
    self.fail(f'Name "{name}" may be undefined', context, code=codes.PARTIALLY_DEFINED)

</t>
<t tx="ekr.20221004064035.2580">def supported_self_type(typ: ProperType) -&gt; bool:
    """Is this a supported kind of explicit self-types?

    Currently, this means a X or Type[X], where X is an instance or
    a type variable with an instance upper bound.
    """
    if isinstance(typ, TypeType):
        return supported_self_type(typ.item)
    return isinstance(typ, TypeVarType) or (
        isinstance(typ, Instance) and typ != fill_typevars(typ.type)
    )


</t>
<t tx="ekr.20221004064035.2581">F = TypeVar("F", bound=FunctionLike)


</t>
<t tx="ekr.20221004064035.2582">def bind_self(method: F, original_type: Type | None = None, is_classmethod: bool = False) -&gt; F:
    """Return a copy of `method`, with the type of its first parameter (usually
    self or cls) bound to original_type.

    If the type of `self` is a generic type (T, or Type[T] for classmethods),
    instantiate every occurrence of type with original_type in the rest of the
    signature and in the return type.

    original_type is the type of E in the expression E.copy(). It is None in
    compatibility checks. In this case we treat it as the erasure of the
    declared type of self.

    This way we can express "the type of self". For example:

    T = TypeVar('T', bound='A')
    class A:
        def copy(self: T) -&gt; T: ...

    class B(A): pass

    b = B().copy()  # type: B

    """
    if isinstance(method, Overloaded):
        return cast(
            F, Overloaded([bind_self(c, original_type, is_classmethod) for c in method.items])
        )
    assert isinstance(method, CallableType)
    func = method
    if not func.arg_types:
        # Invalid method, return something.
        return cast(F, func)
    if func.arg_kinds[0] == ARG_STAR:
        # The signature is of the form 'def foo(*args, ...)'.
        # In this case we shouldn't drop the first arg,
        # since func will be absorbed by the *args.

        # TODO: infer bounds on the type of *args?
        return cast(F, func)
    self_param_type = get_proper_type(func.arg_types[0])

    variables: Sequence[TypeVarLikeType] = []
    if func.variables and supported_self_type(self_param_type):
        from mypy.infer import infer_type_arguments

        if original_type is None:
            # TODO: type check method override (see #7861).
            original_type = erase_to_bound(self_param_type)
        original_type = get_proper_type(original_type)

        all_ids = func.type_var_ids()
        typeargs = infer_type_arguments(all_ids, self_param_type, original_type, is_supertype=True)
        if (
            is_classmethod
            # TODO: why do we need the extra guards here?
            and any(isinstance(get_proper_type(t), UninhabitedType) for t in typeargs)
            and isinstance(original_type, (Instance, TypeVarType, TupleType))
        ):
            # In case we call a classmethod through an instance x, fallback to type(x)
            typeargs = infer_type_arguments(
                all_ids, self_param_type, TypeType(original_type), is_supertype=True
            )

        ids = [tid for tid in all_ids if any(tid == t.id for t in get_type_vars(self_param_type))]

        # Technically, some constrains might be unsolvable, make them &lt;nothing&gt;.
        to_apply = [t if t is not None else UninhabitedType() for t in typeargs]

        def expand(target: Type) -&gt; Type:
            return expand_type(target, {id: to_apply[all_ids.index(id)] for id in ids})

        arg_types = [expand(x) for x in func.arg_types[1:]]
        ret_type = expand(func.ret_type)
        variables = [v for v in func.variables if v.id not in ids]
    else:
        arg_types = func.arg_types[1:]
        ret_type = func.ret_type
        variables = func.variables

    original_type = get_proper_type(original_type)
    if isinstance(original_type, CallableType) and original_type.is_type_obj():
        original_type = TypeType.make_normalized(original_type.ret_type)
    res = func.copy_modified(
        arg_types=arg_types,
        arg_kinds=func.arg_kinds[1:],
        arg_names=func.arg_names[1:],
        variables=variables,
        ret_type=ret_type,
        bound_args=[original_type],
    )
    return cast(F, res)


</t>
<t tx="ekr.20221004064035.2583">def erase_to_bound(t: Type) -&gt; Type:
    # TODO: use value restrictions to produce a union?
    t = get_proper_type(t)
    if isinstance(t, TypeVarType):
        return t.upper_bound
    if isinstance(t, TypeType):
        if isinstance(t.item, TypeVarType):
            return TypeType.make_normalized(t.item.upper_bound)
    return t


</t>
<t tx="ekr.20221004064035.2584">def callable_corresponding_argument(
    typ: CallableType | Parameters, model: FormalArgument
) -&gt; FormalArgument | None:
    """Return the argument a function that corresponds to `model`"""

    by_name = typ.argument_by_name(model.name)
    by_pos = typ.argument_by_position(model.pos)
    if by_name is None and by_pos is None:
        return None
    if by_name is not None and by_pos is not None:
        if by_name == by_pos:
            return by_name
        # If we're dealing with an optional pos-only and an optional
        # name-only arg, merge them.  This is the case for all functions
        # taking both *args and **args, or a pair of functions like so:

        # def right(a: int = ...) -&gt; None: ...
        # def left(__a: int = ..., *, a: int = ...) -&gt; None: ...
        from mypy.subtypes import is_equivalent

        if (
            not (by_name.required or by_pos.required)
            and by_pos.name is None
            and by_name.pos is None
            and is_equivalent(by_name.typ, by_pos.typ)
        ):
            return FormalArgument(by_name.name, by_pos.pos, by_name.typ, False)
    return by_name if by_name is not None else by_pos


</t>
<t tx="ekr.20221004064035.2585">def simple_literal_value_key(t: ProperType) -&gt; tuple[str, ...] | None:
    """Return a hashable description of simple literal type.

    Return None if not a simple literal type.

    The return value can be used to simplify away duplicate types in
    unions by comparing keys for equality. For now enum, string or
    Instance with string last_known_value are supported.
    """
    if isinstance(t, LiteralType):
        if t.fallback.type.is_enum or t.fallback.type.fullname == "builtins.str":
            assert isinstance(t.value, str)
            return "literal", t.value, t.fallback.type.fullname
    if isinstance(t, Instance):
        if t.last_known_value is not None and isinstance(t.last_known_value.value, str):
            return "instance", t.last_known_value.value, t.type.fullname
    return None


</t>
<t tx="ekr.20221004064035.2586">def simple_literal_type(t: ProperType | None) -&gt; Instance | None:
    """Extract the underlying fallback Instance type for a simple Literal"""
    if isinstance(t, Instance) and t.last_known_value is not None:
        t = t.last_known_value
    if isinstance(t, LiteralType):
        return t.fallback
    return None


</t>
<t tx="ekr.20221004064035.2587">def is_simple_literal(t: ProperType) -&gt; bool:
    """Fast way to check if simple_literal_value_key() would return a non-None value."""
    if isinstance(t, LiteralType):
        return t.fallback.type.is_enum or t.fallback.type.fullname == "builtins.str"
    if isinstance(t, Instance):
        return t.last_known_value is not None and isinstance(t.last_known_value.value, str)
    return False


</t>
<t tx="ekr.20221004064035.2588">def make_simplified_union(
    items: Sequence[Type],
    line: int = -1,
    column: int = -1,
    *,
    keep_erased: bool = False,
    contract_literals: bool = True,
) -&gt; ProperType:
    """Build union type with redundant union items removed.

    If only a single item remains, this may return a non-union type.

    Examples:

    * [int, str] -&gt; Union[int, str]
    * [int, object] -&gt; object
    * [int, int] -&gt; int
    * [int, Any] -&gt; Union[int, Any] (Any types are not simplified away!)
    * [Any, Any] -&gt; Any

    Note: This must NOT be used during semantic analysis, since TypeInfos may not
          be fully initialized.

    The keep_erased flag is used for type inference against union types
    containing type variables. If set to True, keep all ErasedType items.

    The contract_literals flag indicates whether we need to contract literal types
    back into a sum type. Set it to False when called by try_expanding_sum_type_
    to_union().
    """
    # Step 1: expand all nested unions
    items = flatten_nested_unions(items)

    # Step 2: remove redundant unions
    simplified_set: Sequence[Type] = _remove_redundant_union_items(items, keep_erased)

    # Step 3: If more than one literal exists in the union, try to simplify
    if (
        contract_literals
        and sum(isinstance(get_proper_type(item), LiteralType) for item in simplified_set) &gt; 1
    ):
        simplified_set = try_contracting_literals_in_union(simplified_set)

    result = get_proper_type(UnionType.make_union(simplified_set, line, column))

    # Step 4: At last, we erase any (inconsistent) extra attributes on instances.
    extra_attrs_set = set()
    for item in items:
        instance = try_getting_instance_fallback(item)
        if instance and instance.extra_attrs:
            extra_attrs_set.add(instance.extra_attrs)

    fallback = try_getting_instance_fallback(result)
    if len(extra_attrs_set) &gt; 1 and fallback:
        fallback.extra_attrs = None

    return result


</t>
<t tx="ekr.20221004064035.2589">def _remove_redundant_union_items(items: list[Type], keep_erased: bool) -&gt; list[Type]:
    from mypy.subtypes import is_proper_subtype

    removed: set[int] = set()
    seen: set[tuple[str, ...]] = set()

    # NB: having a separate fast path for Union of Literal and slow path for other things
    # would arguably be cleaner, however it breaks down when simplifying the Union of two
    # different enum types as try_expanding_sum_type_to_union works recursively and will
    # trigger intermediate simplifications that would render the fast path useless
    for i, item in enumerate(items):
        proper_item = get_proper_type(item)
        if i in removed:
            continue
        # Avoid slow nested for loop for Union of Literal of strings/enums (issue #9169)
        k = simple_literal_value_key(proper_item)
        if k is not None:
            if k in seen:
                removed.add(i)
                continue

            # NB: one would naively expect that it would be safe to skip the slow path
            # always for literals. One would be sorely mistaken. Indeed, some simplifications
            # such as that of None/Optional when strict optional is false, do require that we
            # proceed with the slow path. Thankfully, all literals will have the same subtype
            # relationship to non-literal types, so we only need to do that walk for the first
            # literal, which keeps the fast path fast even in the presence of a mixture of
            # literals and other types.
            safe_skip = len(seen) &gt; 0
            seen.add(k)
            if safe_skip:
                continue

        # Keep track of the truthiness info for deleted subtypes which can be relevant
        cbt = cbf = False
        for j, tj in enumerate(items):
            proper_tj = get_proper_type(tj)
            if (
                i == j
                # avoid further checks if this item was already marked redundant.
                or j in removed
                # if the current item is a simple literal then this simplification loop can
                # safely skip all other simple literals as two literals will only ever be
                # subtypes of each other if they are equal, which is already handled above.
                # However, if the current item is not a literal, it might plausibly be a
                # supertype of other literals in the union, so we must check them again.
                # This is an important optimization as is_proper_subtype is pretty expensive.
                or (k is not None and is_simple_literal(proper_tj))
            ):
                continue
            # actual redundancy checks (XXX?)
            if is_redundant_literal_instance(proper_item, proper_tj) and is_proper_subtype(
                tj, item, keep_erased_types=keep_erased, ignore_promotions=True
            ):
                # We found a redundant item in the union.
                removed.add(j)
                cbt = cbt or tj.can_be_true
                cbf = cbf or tj.can_be_false
        # if deleted subtypes had more general truthiness, use that
        if not item.can_be_true and cbt:
            items[i] = true_or_false(item)
        elif not item.can_be_false and cbf:
            items[i] = true_or_false(item)

    return [items[i] for i in range(len(items)) if i not in removed]


</t>
<t tx="ekr.20221004064035.259">def first_argument_for_super_must_be_type(self, actual: Type, context: Context) -&gt; None:
    actual = get_proper_type(actual)
    if isinstance(actual, Instance):
        # Don't include type of instance, because it can look confusingly like a type
        # object.
        type_str = "a non-type instance"
    else:
        type_str = format_type(actual)
    self.fail(
        f'Argument 1 for "super" must be a type object; got {type_str}',
        context,
        code=codes.ARG_TYPE,
    )

</t>
<t tx="ekr.20221004064035.2590">def _get_type_special_method_bool_ret_type(t: Type) -&gt; Type | None:
    t = get_proper_type(t)

    if isinstance(t, Instance):
        bool_method = t.type.get("__bool__")
        if bool_method:
            callee = get_proper_type(bool_method.type)
            if isinstance(callee, CallableType):
                return callee.ret_type

    return None


</t>
<t tx="ekr.20221004064035.2591">def true_only(t: Type) -&gt; ProperType:
    """
    Restricted version of t with only True-ish values
    """
    t = get_proper_type(t)

    if not t.can_be_true:
        # All values of t are False-ish, so there are no true values in it
        return UninhabitedType(line=t.line, column=t.column)
    elif not t.can_be_false:
        # All values of t are already True-ish, so true_only is idempotent in this case
        return t
    elif isinstance(t, UnionType):
        # The true version of a union type is the union of the true versions of its components
        new_items = [true_only(item) for item in t.items]
        can_be_true_items = [item for item in new_items if item.can_be_true]
        return make_simplified_union(can_be_true_items, line=t.line, column=t.column)
    else:
        ret_type = _get_type_special_method_bool_ret_type(t)

        if ret_type and ret_type.can_be_false and not ret_type.can_be_true:
            new_t = copy_type(t)
            new_t.can_be_true = False
            return new_t

        new_t = copy_type(t)
        new_t.can_be_false = False
        return new_t


</t>
<t tx="ekr.20221004064035.2592">def false_only(t: Type) -&gt; ProperType:
    """
    Restricted version of t with only False-ish values
    """
    t = get_proper_type(t)

    if not t.can_be_false:
        if state.strict_optional:
            # All values of t are True-ish, so there are no false values in it
            return UninhabitedType(line=t.line)
        else:
            # When strict optional checking is disabled, everything can be
            # False-ish since anything can be None
            return NoneType(line=t.line)
    elif not t.can_be_true:
        # All values of t are already False-ish, so false_only is idempotent in this case
        return t
    elif isinstance(t, UnionType):
        # The false version of a union type is the union of the false versions of its components
        new_items = [false_only(item) for item in t.items]
        can_be_false_items = [item for item in new_items if item.can_be_false]
        return make_simplified_union(can_be_false_items, line=t.line, column=t.column)
    else:
        ret_type = _get_type_special_method_bool_ret_type(t)

        if ret_type and ret_type.can_be_true and not ret_type.can_be_false:
            new_t = copy_type(t)
            new_t.can_be_false = False
            return new_t

        new_t = copy_type(t)
        new_t.can_be_true = False
        return new_t


</t>
<t tx="ekr.20221004064035.2593">def true_or_false(t: Type) -&gt; ProperType:
    """
    Unrestricted version of t with both True-ish and False-ish values
    """
    t = get_proper_type(t)

    if isinstance(t, UnionType):
        new_items = [true_or_false(item) for item in t.items]
        return make_simplified_union(new_items, line=t.line, column=t.column)

    new_t = copy_type(t)
    new_t.can_be_true = new_t.can_be_true_default()
    new_t.can_be_false = new_t.can_be_false_default()
    return new_t


</t>
<t tx="ekr.20221004064035.2594">def erase_def_to_union_or_bound(tdef: TypeVarLikeType) -&gt; Type:
    # TODO(PEP612): fix for ParamSpecType
    if isinstance(tdef, ParamSpecType):
        return AnyType(TypeOfAny.from_error)
    assert isinstance(tdef, TypeVarType)
    if tdef.values:
        return make_simplified_union(tdef.values)
    else:
        return tdef.upper_bound


</t>
<t tx="ekr.20221004064035.2595">def erase_to_union_or_bound(typ: TypeVarType) -&gt; ProperType:
    if typ.values:
        return make_simplified_union(typ.values)
    else:
        return get_proper_type(typ.upper_bound)


</t>
<t tx="ekr.20221004064035.2596">def function_type(func: FuncBase, fallback: Instance) -&gt; FunctionLike:
    if func.type:
        assert isinstance(func.type, FunctionLike)
        return func.type
    else:
        # Implicit type signature with dynamic types.
        if isinstance(func, FuncItem):
            return callable_type(func, fallback)
        else:
            # Broken overloads can have self.type set to None.
            # TODO: should we instead always set the type in semantic analyzer?
            assert isinstance(func, OverloadedFuncDef)
            any_type = AnyType(TypeOfAny.from_error)
            dummy = CallableType(
                [any_type, any_type],
                [ARG_STAR, ARG_STAR2],
                [None, None],
                any_type,
                fallback,
                line=func.line,
                is_ellipsis_args=True,
            )
            # Return an Overloaded, because some callers may expect that
            # an OverloadedFuncDef has an Overloaded type.
            return Overloaded([dummy])


</t>
<t tx="ekr.20221004064035.2597">def callable_type(
    fdef: FuncItem, fallback: Instance, ret_type: Type | None = None
) -&gt; CallableType:
    # TODO: somewhat unfortunate duplication with prepare_method_signature in semanal
    if fdef.info and not fdef.is_static and fdef.arg_names:
        self_type: Type = fill_typevars(fdef.info)
        if fdef.is_class or fdef.name == "__new__":
            self_type = TypeType.make_normalized(self_type)
        args = [self_type] + [AnyType(TypeOfAny.unannotated)] * (len(fdef.arg_names) - 1)
    else:
        args = [AnyType(TypeOfAny.unannotated)] * len(fdef.arg_names)

    return CallableType(
        args,
        fdef.arg_kinds,
        fdef.arg_names,
        ret_type or AnyType(TypeOfAny.unannotated),
        fallback,
        name=fdef.name,
        line=fdef.line,
        column=fdef.column,
        implicit=True,
        # We need this for better error messages, like missing `self` note:
        definition=fdef if isinstance(fdef, FuncDef) else None,
    )


</t>
<t tx="ekr.20221004064035.2598">def try_getting_str_literals(expr: Expression, typ: Type) -&gt; list[str] | None:
    """If the given expression or type corresponds to a string literal
    or a union of string literals, returns a list of the underlying strings.
    Otherwise, returns None.

    Specifically, this function is guaranteed to return a list with
    one or more strings if one of the following is true:

    1. 'expr' is a StrExpr
    2. 'typ' is a LiteralType containing a string
    3. 'typ' is a UnionType containing only LiteralType of strings
    """
    if isinstance(expr, StrExpr):
        return [expr.value]

    # TODO: See if we can eliminate this function and call the below one directly
    return try_getting_str_literals_from_type(typ)


</t>
<t tx="ekr.20221004064035.2599">def try_getting_str_literals_from_type(typ: Type) -&gt; list[str] | None:
    """If the given expression or type corresponds to a string Literal
    or a union of string Literals, returns a list of the underlying strings.
    Otherwise, returns None.

    For example, if we had the type 'Literal["foo", "bar"]' as input, this function
    would return a list of strings ["foo", "bar"].
    """
    return try_getting_literals_from_type(typ, str, "builtins.str")


</t>
<t tx="ekr.20221004064035.26">def __init__(self, s: ProperType, instance_joiner: InstanceJoiner | None = None) -&gt; None:
    self.s = s
    self.instance_joiner = instance_joiner

</t>
<t tx="ekr.20221004064035.260">def unsafe_super(self, method: str, cls: str, ctx: Context) -&gt; None:
    self.fail(
        'Call to abstract method "{}" of "{}" with trivial body'
        " via super() is unsafe".format(method, cls),
        ctx,
        code=codes.SAFE_SUPER,
    )

</t>
<t tx="ekr.20221004064035.2600">def try_getting_int_literals_from_type(typ: Type) -&gt; list[int] | None:
    """If the given expression or type corresponds to an int Literal
    or a union of int Literals, returns a list of the underlying ints.
    Otherwise, returns None.

    For example, if we had the type 'Literal[1, 2, 3]' as input, this function
    would return a list of ints [1, 2, 3].
    """
    return try_getting_literals_from_type(typ, int, "builtins.int")


</t>
<t tx="ekr.20221004064035.2601">T = TypeVar("T")


</t>
<t tx="ekr.20221004064035.2602">def try_getting_literals_from_type(
    typ: Type, target_literal_type: type[T], target_fullname: str
) -&gt; list[T] | None:
    """If the given expression or type corresponds to a Literal or
    union of Literals where the underlying values correspond to the given
    target type, returns a list of those underlying values. Otherwise,
    returns None.
    """
    typ = get_proper_type(typ)

    if isinstance(typ, Instance) and typ.last_known_value is not None:
        possible_literals: list[Type] = [typ.last_known_value]
    elif isinstance(typ, UnionType):
        possible_literals = list(typ.items)
    else:
        possible_literals = [typ]

    literals: list[T] = []
    for lit in get_proper_types(possible_literals):
        if isinstance(lit, LiteralType) and lit.fallback.type.fullname == target_fullname:
            val = lit.value
            if isinstance(val, target_literal_type):
                literals.append(val)
            else:
                return None
        else:
            return None
    return literals


</t>
<t tx="ekr.20221004064035.2603">def is_literal_type_like(t: Type | None) -&gt; bool:
    """Returns 'true' if the given type context is potentially either a LiteralType,
    a Union of LiteralType, or something similar.
    """
    t = get_proper_type(t)
    if t is None:
        return False
    elif isinstance(t, LiteralType):
        return True
    elif isinstance(t, UnionType):
        return any(is_literal_type_like(item) for item in t.items)
    elif isinstance(t, TypeVarType):
        return is_literal_type_like(t.upper_bound) or any(
            is_literal_type_like(item) for item in t.values
        )
    else:
        return False


</t>
<t tx="ekr.20221004064035.2604">def is_singleton_type(typ: Type) -&gt; bool:
    """Returns 'true' if this type is a "singleton type" -- if there exists
    exactly only one runtime value associated with this type.

    That is, given two values 'a' and 'b' that have the same type 't',
    'is_singleton_type(t)' returns True if and only if the expression 'a is b' is
    always true.

    Currently, this returns True when given NoneTypes, enum LiteralTypes,
    enum types with a single value and ... (Ellipses).

    Note that other kinds of LiteralTypes cannot count as singleton types. For
    example, suppose we do 'a = 100000 + 1' and 'b = 100001'. It is not guaranteed
    that 'a is b' will always be true -- some implementations of Python will end up
    constructing two distinct instances of 100001.
    """
    typ = get_proper_type(typ)
    return typ.is_singleton_type()


</t>
<t tx="ekr.20221004064035.2605">def try_expanding_sum_type_to_union(typ: Type, target_fullname: str) -&gt; ProperType:
    """Attempts to recursively expand any enum Instances with the given target_fullname
    into a Union of all of its component LiteralTypes.

    For example, if we have:

        class Color(Enum):
            RED = 1
            BLUE = 2
            YELLOW = 3

        class Status(Enum):
            SUCCESS = 1
            FAILURE = 2
            UNKNOWN = 3

    ...and if we call `try_expanding_enum_to_union(Union[Color, Status], 'module.Color')`,
    this function will return Literal[Color.RED, Color.BLUE, Color.YELLOW, Status].
    """
    typ = get_proper_type(typ)

    if isinstance(typ, UnionType):
        items = [
            try_expanding_sum_type_to_union(item, target_fullname) for item in typ.relevant_items()
        ]
        return make_simplified_union(items, contract_literals=False)
    elif isinstance(typ, Instance) and typ.type.fullname == target_fullname:
        if typ.type.is_enum:
            new_items = []
            for name, symbol in typ.type.names.items():
                if not isinstance(symbol.node, Var):
                    continue
                # Skip these since Enum will remove it
                if name in ENUM_REMOVED_PROPS:
                    continue
                new_items.append(LiteralType(name, typ))
            return make_simplified_union(new_items, contract_literals=False)
        elif typ.type.fullname == "builtins.bool":
            return make_simplified_union(
                [LiteralType(True, typ), LiteralType(False, typ)], contract_literals=False
            )

    return typ


</t>
<t tx="ekr.20221004064035.2606">def try_contracting_literals_in_union(types: Sequence[Type]) -&gt; list[ProperType]:
    """Contracts any literal types back into a sum type if possible.

    Will replace the first instance of the literal with the sum type and
    remove all others.

    If we call `try_contracting_union(Literal[Color.RED, Color.BLUE, Color.YELLOW])`,
    this function will return Color.

    We also treat `Literal[True, False]` as `bool`.
    """
    proper_types = [get_proper_type(typ) for typ in types]
    sum_types: dict[str, tuple[set[Any], list[int]]] = {}
    marked_for_deletion = set()
    for idx, typ in enumerate(proper_types):
        if isinstance(typ, LiteralType):
            fullname = typ.fallback.type.fullname
            if typ.fallback.type.is_enum or isinstance(typ.value, bool):
                if fullname not in sum_types:
                    sum_types[fullname] = (
                        set(typ.fallback.get_enum_values())
                        if typ.fallback.type.is_enum
                        else {True, False},
                        [],
                    )
                literals, indexes = sum_types[fullname]
                literals.discard(typ.value)
                indexes.append(idx)
                if not literals:
                    first, *rest = indexes
                    proper_types[first] = typ.fallback
                    marked_for_deletion |= set(rest)
    return list(
        itertools.compress(
            proper_types, [(i not in marked_for_deletion) for i in range(len(proper_types))]
        )
    )


</t>
<t tx="ekr.20221004064035.2607">def coerce_to_literal(typ: Type) -&gt; Type:
    """Recursively converts any Instances that have a last_known_value or are
    instances of enum types with a single value into the corresponding LiteralType.
    """
    original_type = typ
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        new_items = [coerce_to_literal(item) for item in typ.items]
        return UnionType.make_union(new_items)
    elif isinstance(typ, Instance):
        if typ.last_known_value:
            return typ.last_known_value
        elif typ.type.is_enum:
            enum_values = typ.get_enum_values()
            if len(enum_values) == 1:
                return LiteralType(value=enum_values[0], fallback=typ)
    return original_type


</t>
<t tx="ekr.20221004064035.2608">def get_type_vars(tp: Type) -&gt; list[TypeVarType]:
    return tp.accept(TypeVarExtractor())


</t>
<t tx="ekr.20221004064035.2609">class TypeVarExtractor(TypeQuery[List[TypeVarType]]):
    @others
</t>
<t tx="ekr.20221004064035.261">def too_few_string_formatting_arguments(self, context: Context) -&gt; None:
    self.fail("Not enough arguments for format string", context, code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20221004064035.2610">def __init__(self) -&gt; None:
    super().__init__(self._merge)

</t>
<t tx="ekr.20221004064035.2611">def _merge(self, iter: Iterable[list[TypeVarType]]) -&gt; list[TypeVarType]:
    out = []
    for item in iter:
        out.extend(item)
    return out

</t>
<t tx="ekr.20221004064035.2612">def visit_type_var(self, t: TypeVarType) -&gt; list[TypeVarType]:
    return [t]


</t>
<t tx="ekr.20221004064035.2613">def custom_special_method(typ: Type, name: str, check_all: bool = False) -&gt; bool:
    """Does this type have a custom special method such as __format__() or __eq__()?

    If check_all is True ensure all items of a union have a custom method, not just some.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        method = typ.type.get(name)
        if method and isinstance(method.node, (SYMBOL_FUNCBASE_TYPES, Decorator, Var)):
            if method.node.info:
                return not method.node.info.fullname.startswith("builtins.")
        return False
    if isinstance(typ, UnionType):
        if check_all:
            return all(custom_special_method(t, name, check_all) for t in typ.items)
        return any(custom_special_method(t, name) for t in typ.items)
    if isinstance(typ, TupleType):
        return custom_special_method(tuple_fallback(typ), name, check_all)
    if isinstance(typ, CallableType) and typ.is_type_obj():
        # Look up __method__ on the metaclass for class objects.
        return custom_special_method(typ.fallback, name, check_all)
    if isinstance(typ, AnyType):
        # Avoid false positives in uncertain cases.
        return True
    # TODO: support other types (see ExpressionChecker.has_member())?
    return False


</t>
<t tx="ekr.20221004064035.2614">def is_redundant_literal_instance(general: ProperType, specific: ProperType) -&gt; bool:
    if not isinstance(general, Instance) or general.last_known_value is None:
        return True
    if isinstance(specific, Instance) and specific.last_known_value == general.last_known_value:
        return True
    if isinstance(specific, UninhabitedType):
        return True

    return False


</t>
<t tx="ekr.20221004064035.2615">def separate_union_literals(t: UnionType) -&gt; tuple[Sequence[LiteralType], Sequence[Type]]:
    """Separate literals from other members in a union type."""
    literal_items = []
    union_items = []

    for item in t.items:
        proper = get_proper_type(item)
        if isinstance(proper, LiteralType):
            literal_items.append(proper)
        else:
            union_items.append(item)

    return literal_items, union_items


</t>
<t tx="ekr.20221004064035.2616">def try_getting_instance_fallback(typ: Type) -&gt; Instance | None:
    """Returns the Instance fallback for this type if one exists or None."""
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return typ
    elif isinstance(typ, TupleType):
        return typ.partial_fallback
    elif isinstance(typ, TypedDictType):
        return typ.fallback
    elif isinstance(typ, FunctionLike):
        return typ.fallback
    elif isinstance(typ, LiteralType):
        return typ.fallback
    elif isinstance(typ, TypeVarType):
        return try_getting_instance_fallback(typ.upper_bound)
    return None
</t>
<t tx="ekr.20221004064035.2617">@path C:/Repos/ekr-mypy2/mypy/
"""Classes for representing mypy types."""

from __future__ import annotations

import sys
from abc import abstractmethod
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Dict,
    Iterable,
    NamedTuple,
    NewType,
    Sequence,
    TypeVar,
    Union,
    cast,
)
from typing_extensions import Final, TypeAlias as _TypeAlias, TypeGuard, overload

import mypy.nodes
from mypy.bogus_type import Bogus
from mypy.nodes import (
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    INVARIANT,
    ArgKind,
    FakeInfo,
    FuncDef,
    SymbolNode,
)
from mypy.state import state
from mypy.util import IdMapper

T = TypeVar("T")

JsonDict: _TypeAlias = Dict[str, Any]

# The set of all valid expressions that can currently be contained
# inside of a Literal[...].
#
# Literals can contain bytes and enum-values: we special-case both of these
# and store the value as a string. We rely on the fallback type that's also
# stored with the Literal to determine how a string is being used.
#
# TODO: confirm that we're happy with representing enums (and the
# other types) in the manner described above.
#
# Note: if we change the set of types included below, we must also
# make sure to audit the following methods:
#
# 1. types.LiteralType's serialize and deserialize methods: this method
#    needs to make sure it can convert the below types into JSON and back.
#
# 2. types.LiteralType's 'alue_repr` method: this method is ultimately used
#    by TypeStrVisitor's visit_literal_type to generate a reasonable
#    repr-able output.
#
# 3. server.astdiff.SnapshotTypeVisitor's visit_literal_type_method: this
#    method assumes that the following types supports equality checks and
#    hashability.
#
# Note: Although "Literal[None]" is a valid type, we internally always convert
# such a type directly into "None". So, "None" is not a valid parameter of
# LiteralType and is omitted from this list.
LiteralValue: _TypeAlias = Union[int, str, bool]


# If we only import type_visitor in the middle of the file, mypy
# breaks, and if we do it at the top, it breaks at runtime because of
# import cycle issues, so we do it at the top while typechecking and
# then again in the middle at runtime.
# We should be able to remove this once we are switched to the new
# semantic analyzer!
if TYPE_CHECKING:
    from mypy.type_visitor import (
        SyntheticTypeVisitor as SyntheticTypeVisitor,
        TypeVisitor as TypeVisitor,
    )

TYPED_NAMEDTUPLE_NAMES: Final = ("typing.NamedTuple", "typing_extensions.NamedTuple")

# Supported names of TypedDict type constructors.
TPDICT_NAMES: Final = (
    "typing.TypedDict",
    "typing_extensions.TypedDict",
    "mypy_extensions.TypedDict",
)

# Supported fallback instance type names for TypedDict types.
TPDICT_FB_NAMES: Final = (
    "typing._TypedDict",
    "typing_extensions._TypedDict",
    "mypy_extensions._TypedDict",
)

# Supported names of Protocol base class.
PROTOCOL_NAMES: Final = ("typing.Protocol", "typing_extensions.Protocol")

# Supported TypeAlias names.
TYPE_ALIAS_NAMES: Final = ("typing.TypeAlias", "typing_extensions.TypeAlias")

# Supported Final type names.
FINAL_TYPE_NAMES: Final = ("typing.Final", "typing_extensions.Final")

# Supported @final decorator names.
FINAL_DECORATOR_NAMES: Final = ("typing.final", "typing_extensions.final")

# Supported Literal type names.
LITERAL_TYPE_NAMES: Final = ("typing.Literal", "typing_extensions.Literal")

# Supported Annotated type names.
ANNOTATED_TYPE_NAMES: Final = ("typing.Annotated", "typing_extensions.Annotated")

# We use this constant in various places when checking `tuple` subtyping:
TUPLE_LIKE_INSTANCE_NAMES: Final = (
    "builtins.tuple",
    "typing.Iterable",
    "typing.Container",
    "typing.Sequence",
    "typing.Reversible",
)

REVEAL_TYPE_NAMES: Final = (
    "builtins.reveal_type",
    "typing.reveal_type",
    "typing_extensions.reveal_type",
)

ASSERT_TYPE_NAMES: Final = ("typing.assert_type", "typing_extensions.assert_type")

OVERLOAD_NAMES: Final = ("typing.overload", "typing_extensions.overload")

# Attributes that can optionally be defined in the body of a subclass of
# enum.Enum but are removed from the class __dict__ by EnumMeta.
ENUM_REMOVED_PROPS: Final = ("_ignore_", "_order_", "__order__")

NEVER_NAMES: Final = (
    "typing.NoReturn",
    "typing_extensions.NoReturn",
    "mypy_extensions.NoReturn",
    "typing.Never",
    "typing_extensions.Never",
)

# A placeholder used for Bogus[...] parameters
_dummy: Final[Any] = object()


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2618">class TypeOfAny:
    """
    This class describes different types of Any. Each 'Any' can be of only one type at a time.
    """

    __slots__ = ()

    # Was this Any type inferred without a type annotation?
    unannotated: Final = 1
    # Does this Any come from an explicit type annotation?
    explicit: Final = 2
    # Does this come from an unfollowed import? See --disallow-any-unimported option
    from_unimported_type: Final = 3
    # Does this Any type come from omitted generics?
    from_omitted_generics: Final = 4
    # Does this Any come from an error?
    from_error: Final = 5
    # Is this a type that can't be represented in mypy's type system? For instance, type of
    # call to NewType...). Even though these types aren't real Anys, we treat them as such.
    # Also used for variables named '_'.
    special_form: Final = 6
    # Does this Any come from interaction with another Any?
    from_another_any: Final = 7
    # Does this Any come from an implementation limitation/bug?
    implementation_artifact: Final = 8
    # Does this Any come from use in the suggestion engine?  This is
    # used to ignore Anys inserted by the suggestion engine when
    # generating constraints.
    suggestion_engine: Final = 9


</t>
<t tx="ekr.20221004064035.2619">def deserialize_type(data: JsonDict | str) -&gt; Type:
    if isinstance(data, str):
        return Instance.deserialize(data)
    classname = data[".class"]
    method = deserialize_map.get(classname)
    if method is not None:
        return method(data)
    raise NotImplementedError(f"unexpected .class {classname}")


</t>
<t tx="ekr.20221004064035.262">def too_many_string_formatting_arguments(self, context: Context) -&gt; None:
    self.fail(
        "Not all arguments converted during string formatting",
        context,
        code=codes.STRING_FORMATTING,
    )

</t>
<t tx="ekr.20221004064035.2620">class Type(mypy.nodes.Context):
    """Abstract base class for all types."""

    __slots__ = ("can_be_true", "can_be_false")
    # 'can_be_true' and 'can_be_false' mean whether the value of the
    # expression can be true or false in a boolean context. They are useful
    # when inferring the type of logic expressions like `x and y`.
    #
    # For example:
    #   * the literal `False` can't be true while `True` can.
    #   * a value with type `bool` can be true or false.
    #   * `None` can't be true
    #   * ...

    @others
</t>
<t tx="ekr.20221004064035.2621">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.can_be_true = self.can_be_true_default()
    self.can_be_false = self.can_be_false_default()

</t>
<t tx="ekr.20221004064035.2622">def can_be_true_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20221004064035.2623">def can_be_false_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20221004064035.2624">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")

</t>
<t tx="ekr.20221004064035.2625">def __repr__(self) -&gt; str:
    return self.accept(TypeStrVisitor())

</t>
<t tx="ekr.20221004064035.2626">def serialize(self) -&gt; JsonDict | str:
    raise NotImplementedError(f"Cannot serialize {self.__class__.__name__} instance")

</t>
<t tx="ekr.20221004064035.2627">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Type:
    raise NotImplementedError(f"Cannot deserialize {cls.__name__} instance")

</t>
<t tx="ekr.20221004064035.2628">def is_singleton_type(self) -&gt; bool:
    return False


</t>
<t tx="ekr.20221004064035.2629">class TypeAliasType(Type):
    """A type alias to another type.

    To support recursive type aliases we don't immediately expand a type alias
    during semantic analysis, but create an instance of this type that records the target alias
    definition node (mypy.nodes.TypeAlias) and type arguments (for generic aliases).

    This is very similar to how TypeInfo vs Instance interact, where a recursive class-based
    structure like
        class Node:
            value: int
            children: List[Node]
    can be represented in a tree-like manner.
    """

    __slots__ = ("alias", "args", "type_ref")

    @others
</t>
<t tx="ekr.20221004064035.263">def unsupported_placeholder(self, placeholder: str, context: Context) -&gt; None:
    self.fail(
        f'Unsupported format character "{placeholder}"', context, code=codes.STRING_FORMATTING
    )

</t>
<t tx="ekr.20221004064035.2630">def __init__(
    self,
    alias: mypy.nodes.TypeAlias | None,
    args: list[Type],
    line: int = -1,
    column: int = -1,
) -&gt; None:
    self.alias = alias
    self.args = args
    self.type_ref: str | None = None
    super().__init__(line, column)

</t>
<t tx="ekr.20221004064035.2631">def _expand_once(self) -&gt; Type:
    """Expand to the target type exactly once.

    This doesn't do full expansion, i.e. the result can contain another
    (or even this same) type alias. Use this internal helper only when really needed,
    its public wrapper mypy.types.get_proper_type() is preferred.
    """
    assert self.alias is not None
    if self.alias.no_args:
        # We know that no_args=True aliases like L = List must have an instance
        # as their target.
        assert isinstance(self.alias.target, Instance)  # type: ignore[misc]
        return self.alias.target.copy_modified(args=self.args)
    return replace_alias_tvars(
        self.alias.target, self.alias.alias_tvars, self.args, self.line, self.column
    )

</t>
<t tx="ekr.20221004064035.2632">def _partial_expansion(self) -&gt; tuple[ProperType, bool]:
    # Private method mostly for debugging and testing.
    unroller = UnrollAliasVisitor(set())
    unrolled = self.accept(unroller)
    assert isinstance(unrolled, ProperType)
    return unrolled, unroller.recursed

</t>
<t tx="ekr.20221004064035.2633">def expand_all_if_possible(self) -&gt; ProperType | None:
    """Attempt a full expansion of the type alias (including nested aliases).

    If the expansion is not possible, i.e. the alias is (mutually-)recursive,
    return None.
    """
    unrolled, recursed = self._partial_expansion()
    if recursed:
        return None
    return unrolled

</t>
<t tx="ekr.20221004064035.2634">@property
def is_recursive(self) -&gt; bool:
    assert self.alias is not None, "Unfixed type alias"
    is_recursive = self.alias._is_recursive
    if is_recursive is None:
        is_recursive = self.expand_all_if_possible() is None
        # We cache the value on the underlying TypeAlias node as an optimization,
        # since the value is the same for all instances of the same alias.
        self.alias._is_recursive = is_recursive
    return is_recursive

</t>
<t tx="ekr.20221004064035.2635">def can_be_true_default(self) -&gt; bool:
    if self.alias is not None:
        return self.alias.target.can_be_true
    return super().can_be_true_default()

</t>
<t tx="ekr.20221004064035.2636">def can_be_false_default(self) -&gt; bool:
    if self.alias is not None:
        return self.alias.target.can_be_false
    return super().can_be_false_default()

</t>
<t tx="ekr.20221004064035.2637">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_alias_type(self)

</t>
<t tx="ekr.20221004064035.2638">def __hash__(self) -&gt; int:
    return hash((self.alias, tuple(self.args)))

</t>
<t tx="ekr.20221004064035.2639">def __eq__(self, other: object) -&gt; bool:
    # Note: never use this to determine subtype relationships, use is_subtype().
    if not isinstance(other, TypeAliasType):
        return NotImplemented
    return self.alias == other.alias and self.args == other.args

</t>
<t tx="ekr.20221004064035.264">def string_interpolation_with_star_and_key(self, context: Context) -&gt; None:
    self.fail(
        "String interpolation contains both stars and mapping keys",
        context,
        code=codes.STRING_FORMATTING,
    )

</t>
<t tx="ekr.20221004064035.2640">def serialize(self) -&gt; JsonDict:
    assert self.alias is not None
    data: JsonDict = {
        ".class": "TypeAliasType",
        "type_ref": self.alias.fullname,
        "args": [arg.serialize() for arg in self.args],
    }
    return data

</t>
<t tx="ekr.20221004064035.2641">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeAliasType:
    assert data[".class"] == "TypeAliasType"
    args: list[Type] = []
    if "args" in data:
        args_list = data["args"]
        assert isinstance(args_list, list)
        args = [deserialize_type(arg) for arg in args_list]
    alias = TypeAliasType(None, args)
    alias.type_ref = data["type_ref"]
    return alias

</t>
<t tx="ekr.20221004064035.2642">def copy_modified(self, *, args: list[Type] | None = None) -&gt; TypeAliasType:
    return TypeAliasType(
        self.alias, args if args is not None else self.args.copy(), self.line, self.column
    )


</t>
<t tx="ekr.20221004064035.2643">class TypeGuardedType(Type):
    """Only used by find_isinstance_check() etc."""

    __slots__ = ("type_guard",)

    @others
</t>
<t tx="ekr.20221004064035.2644">def __init__(self, type_guard: Type):
    super().__init__(line=type_guard.line, column=type_guard.column)
    self.type_guard = type_guard

</t>
<t tx="ekr.20221004064035.2645">def __repr__(self) -&gt; str:
    return f"TypeGuard({self.type_guard})"


</t>
<t tx="ekr.20221004064035.2646">class RequiredType(Type):
    """Required[T] or NotRequired[T]. Only usable at top-level of a TypedDict definition."""

    @others
</t>
<t tx="ekr.20221004064035.2647">def __init__(self, item: Type, *, required: bool) -&gt; None:
    super().__init__(line=item.line, column=item.column)
    self.item = item
    self.required = required

</t>
<t tx="ekr.20221004064035.2648">def __repr__(self) -&gt; str:
    if self.required:
        return f"Required[{self.item}]"
    else:
        return f"NotRequired[{self.item}]"

</t>
<t tx="ekr.20221004064035.2649">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return self.item.accept(visitor)


</t>
<t tx="ekr.20221004064035.265">def requires_int_or_single_byte(self, context: Context, format_call: bool = False) -&gt; None:
    self.fail(
        '"{}c" requires an integer in range(256) or a single byte'.format(
            ":" if format_call else "%"
        ),
        context,
        code=codes.STRING_FORMATTING,
    )

</t>
<t tx="ekr.20221004064035.2650">class ProperType(Type):
    """Not a type alias.

    Every type except TypeAliasType must inherit from this type.
    """

    __slots__ = ()


</t>
<t tx="ekr.20221004064035.2651">class TypeVarId:
    # A type variable is uniquely identified by its raw id and meta level.

    # For plain variables (type parameters of generic classes and
    # functions) raw ids are allocated by semantic analysis, using
    # positive ids 1, 2, ... for generic class parameters and negative
    # ids -1, ... for generic function type arguments. This convention
    # is only used to keep type variable ids distinct when allocating
    # them; the type checker makes no distinction between class and
    # function type variables.

    # Metavariables are allocated unique ids starting from 1.
    raw_id: int = 0

    # Level of the variable in type inference. Currently either 0 for
    # declared types, or 1 for type inference metavariables.
    meta_level: int = 0

    # Class variable used for allocating fresh ids for metavariables.
    next_raw_id: ClassVar[int] = 1

    # Fullname of class (or potentially function in the future) which
    # declares this type variable (not the fullname of the TypeVar
    # definition!), or ''
    namespace: str

    @others
</t>
<t tx="ekr.20221004064035.2652">def __init__(self, raw_id: int, meta_level: int = 0, *, namespace: str = "") -&gt; None:
    self.raw_id = raw_id
    self.meta_level = meta_level
    self.namespace = namespace

</t>
<t tx="ekr.20221004064035.2653">@staticmethod
def new(meta_level: int) -&gt; TypeVarId:
    raw_id = TypeVarId.next_raw_id
    TypeVarId.next_raw_id += 1
    return TypeVarId(raw_id, meta_level)

</t>
<t tx="ekr.20221004064035.2654">def __repr__(self) -&gt; str:
    return self.raw_id.__repr__()

</t>
<t tx="ekr.20221004064035.2655">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, TypeVarId):
        return (
            self.raw_id == other.raw_id
            and self.meta_level == other.meta_level
            and self.namespace == other.namespace
        )
    else:
        return False

</t>
<t tx="ekr.20221004064035.2656">def __ne__(self, other: object) -&gt; bool:
    return not (self == other)

</t>
<t tx="ekr.20221004064035.2657">def __hash__(self) -&gt; int:
    return hash((self.raw_id, self.meta_level, self.namespace))

</t>
<t tx="ekr.20221004064035.2658">def is_meta_var(self) -&gt; bool:
    return self.meta_level &gt; 0


</t>
<t tx="ekr.20221004064035.2659">class TypeVarLikeType(ProperType):

    __slots__ = ("name", "fullname", "id", "upper_bound")

    name: str  # Name (may be qualified)
    fullname: str  # Fully qualified name
    id: TypeVarId
    upper_bound: Type

    @others
</t>
<t tx="ekr.20221004064035.266">def requires_int_or_char(self, context: Context, format_call: bool = False) -&gt; None:
    self.fail(
        '"{}c" requires int or char'.format(":" if format_call else "%"),
        context,
        code=codes.STRING_FORMATTING,
    )

</t>
<t tx="ekr.20221004064035.2660">def __init__(
    self,
    name: str,
    fullname: str,
    id: TypeVarId | int,
    upper_bound: Type,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.name = name
    self.fullname = fullname
    if isinstance(id, int):
        id = TypeVarId(id)
    self.id = id
    self.upper_bound = upper_bound

</t>
<t tx="ekr.20221004064035.2661">def serialize(self) -&gt; JsonDict:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.2662">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarLikeType:
    raise NotImplementedError


</t>
<t tx="ekr.20221004064035.2663">class TypeVarType(TypeVarLikeType):
    """Type that refers to a type variable."""

    __slots__ = ("values", "variance")

    values: list[Type]  # Value restriction, empty list if no restriction
    variance: int

    @others
</t>
<t tx="ekr.20221004064035.2664">def __init__(
    self,
    name: str,
    fullname: str,
    id: TypeVarId | int,
    values: list[Type],
    upper_bound: Type,
    variance: int = INVARIANT,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(name, fullname, id, upper_bound, line, column)
    assert values is not None, "No restrictions must be represented by empty list"
    self.values = values
    self.variance = variance

</t>
<t tx="ekr.20221004064035.2665">@staticmethod
def new_unification_variable(old: TypeVarType) -&gt; TypeVarType:
    new_id = TypeVarId.new(meta_level=1)
    return old.copy_modified(id=new_id)

</t>
<t tx="ekr.20221004064035.2666">def copy_modified(
    self,
    values: Bogus[list[Type]] = _dummy,
    upper_bound: Bogus[Type] = _dummy,
    id: Bogus[TypeVarId | int] = _dummy,
) -&gt; TypeVarType:
    return TypeVarType(
        self.name,
        self.fullname,
        self.id if id is _dummy else id,
        self.values if values is _dummy else values,
        self.upper_bound if upper_bound is _dummy else upper_bound,
        self.variance,
        self.line,
        self.column,
    )

</t>
<t tx="ekr.20221004064035.2667">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_var(self)

</t>
<t tx="ekr.20221004064035.2668">def __hash__(self) -&gt; int:
    return hash((self.id, self.upper_bound))

</t>
<t tx="ekr.20221004064035.2669">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeVarType):
        return NotImplemented
    return self.id == other.id and self.upper_bound == other.upper_bound

</t>
<t tx="ekr.20221004064035.267">def key_not_in_mapping(self, key: str, context: Context) -&gt; None:
    self.fail(f'Key "{key}" not found in mapping', context, code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20221004064035.2670">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {
        ".class": "TypeVarType",
        "name": self.name,
        "fullname": self.fullname,
        "id": self.id.raw_id,
        "namespace": self.id.namespace,
        "values": [v.serialize() for v in self.values],
        "upper_bound": self.upper_bound.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20221004064035.2671">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarType:
    assert data[".class"] == "TypeVarType"
    return TypeVarType(
        data["name"],
        data["fullname"],
        TypeVarId(data["id"], namespace=data["namespace"]),
        [deserialize_type(v) for v in data["values"]],
        deserialize_type(data["upper_bound"]),
        data["variance"],
    )


</t>
<t tx="ekr.20221004064035.2672">class ParamSpecFlavor:
    # Simple ParamSpec reference such as "P"
    BARE: Final = 0
    # P.args
    ARGS: Final = 1
    # P.kwargs
    KWARGS: Final = 2


</t>
<t tx="ekr.20221004064035.2673">class ParamSpecType(TypeVarLikeType):
    """Type that refers to a ParamSpec.

    A ParamSpec is a type variable that represents the parameter
    types, names and kinds of a callable (i.e., the signature without
    the return type).

    This can be one of these forms
     * P (ParamSpecFlavor.BARE)
     * P.args (ParamSpecFlavor.ARGS)
     * P.kwargs (ParamSpecFLavor.KWARGS)

    The upper_bound is really used as a fallback type -- it's shared
    with TypeVarType for simplicity. It can't be specified by the user
    and the value is directly derived from the flavor (currently
    always just 'object').
    """

    __slots__ = ("flavor", "prefix")

    flavor: int
    prefix: Parameters

    @others
</t>
<t tx="ekr.20221004064035.2674">def __init__(
    self,
    name: str,
    fullname: str,
    id: TypeVarId | int,
    flavor: int,
    upper_bound: Type,
    *,
    line: int = -1,
    column: int = -1,
    prefix: Parameters | None = None,
) -&gt; None:
    super().__init__(name, fullname, id, upper_bound, line=line, column=column)
    self.flavor = flavor
    self.prefix = prefix or Parameters([], [], [])

</t>
<t tx="ekr.20221004064035.2675">@staticmethod
def new_unification_variable(old: ParamSpecType) -&gt; ParamSpecType:
    new_id = TypeVarId.new(meta_level=1)
    return old.copy_modified(id=new_id)

</t>
<t tx="ekr.20221004064035.2676">def with_flavor(self, flavor: int) -&gt; ParamSpecType:
    return ParamSpecType(
        self.name,
        self.fullname,
        self.id,
        flavor,
        upper_bound=self.upper_bound,
        prefix=self.prefix,
    )

</t>
<t tx="ekr.20221004064035.2677">def copy_modified(
    self,
    *,
    id: Bogus[TypeVarId | int] = _dummy,
    flavor: Bogus[int] = _dummy,
    prefix: Bogus[Parameters] = _dummy,
) -&gt; ParamSpecType:
    return ParamSpecType(
        self.name,
        self.fullname,
        id if id is not _dummy else self.id,
        flavor if flavor is not _dummy else self.flavor,
        self.upper_bound,
        line=self.line,
        column=self.column,
        prefix=prefix if prefix is not _dummy else self.prefix,
    )

</t>
<t tx="ekr.20221004064035.2678">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_param_spec(self)

</t>
<t tx="ekr.20221004064035.2679">def name_with_suffix(self) -&gt; str:
    n = self.name
    if self.flavor == ParamSpecFlavor.ARGS:
        return f"{n}.args"
    elif self.flavor == ParamSpecFlavor.KWARGS:
        return f"{n}.kwargs"
    return n

</t>
<t tx="ekr.20221004064035.268">def string_interpolation_mixing_key_and_non_keys(self, context: Context) -&gt; None:
    self.fail(
        "String interpolation mixes specifier with and without mapping keys",
        context,
        code=codes.STRING_FORMATTING,
    )

</t>
<t tx="ekr.20221004064035.2680">def __hash__(self) -&gt; int:
    return hash((self.id, self.flavor))

</t>
<t tx="ekr.20221004064035.2681">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, ParamSpecType):
        return NotImplemented
    # Upper bound can be ignored, since it's determined by flavor.
    return self.id == other.id and self.flavor == other.flavor

</t>
<t tx="ekr.20221004064035.2682">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {
        ".class": "ParamSpecType",
        "name": self.name,
        "fullname": self.fullname,
        "id": self.id.raw_id,
        "flavor": self.flavor,
        "upper_bound": self.upper_bound.serialize(),
        "prefix": self.prefix.serialize(),
    }

</t>
<t tx="ekr.20221004064035.2683">@classmethod
def deserialize(cls, data: JsonDict) -&gt; ParamSpecType:
    assert data[".class"] == "ParamSpecType"
    return ParamSpecType(
        data["name"],
        data["fullname"],
        data["id"],
        data["flavor"],
        deserialize_type(data["upper_bound"]),
        prefix=Parameters.deserialize(data["prefix"]),
    )


</t>
<t tx="ekr.20221004064035.2684">class TypeVarTupleType(TypeVarLikeType):
    """Type that refers to a TypeVarTuple.

    See PEP646 for more information.
    """

    @others
</t>
<t tx="ekr.20221004064035.2685">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {
        ".class": "TypeVarTupleType",
        "name": self.name,
        "fullname": self.fullname,
        "id": self.id.raw_id,
        "upper_bound": self.upper_bound.serialize(),
    }

</t>
<t tx="ekr.20221004064035.2686">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarTupleType:
    assert data[".class"] == "TypeVarTupleType"
    return TypeVarTupleType(
        data["name"], data["fullname"], data["id"], deserialize_type(data["upper_bound"])
    )

</t>
<t tx="ekr.20221004064035.2687">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_var_tuple(self)

</t>
<t tx="ekr.20221004064035.2688">def __hash__(self) -&gt; int:
    return hash(self.id)

</t>
<t tx="ekr.20221004064035.2689">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeVarTupleType):
        return NotImplemented
    return self.id == other.id

</t>
<t tx="ekr.20221004064035.269">def cannot_determine_type(self, name: str, context: Context) -&gt; None:
    self.fail(f'Cannot determine type of "{name}"', context, code=codes.HAS_TYPE)

</t>
<t tx="ekr.20221004064035.2690">@staticmethod
def new_unification_variable(old: TypeVarTupleType) -&gt; TypeVarTupleType:
    new_id = TypeVarId.new(meta_level=1)
    return old.copy_modified(id=new_id)

</t>
<t tx="ekr.20221004064035.2691">def copy_modified(self, id: Bogus[TypeVarId | int] = _dummy) -&gt; TypeVarTupleType:
    return TypeVarTupleType(
        self.name,
        self.fullname,
        self.id if id is _dummy else id,
        self.upper_bound,
        line=self.line,
        column=self.column,
    )


</t>
<t tx="ekr.20221004064035.2692">class UnboundType(ProperType):
    """Instance type that has not been bound during semantic analysis."""

    __slots__ = (
        "name",
        "args",
        "optional",
        "empty_tuple_index",
        "original_str_expr",
        "original_str_fallback",
    )

    @others
</t>
<t tx="ekr.20221004064035.2693">def __init__(
    self,
    name: str | None,
    args: Sequence[Type] | None = None,
    line: int = -1,
    column: int = -1,
    optional: bool = False,
    empty_tuple_index: bool = False,
    original_str_expr: str | None = None,
    original_str_fallback: str | None = None,
) -&gt; None:
    super().__init__(line, column)
    if not args:
        args = []
    assert name is not None
    self.name = name
    self.args = tuple(args)
    # Should this type be wrapped in an Optional?
    self.optional = optional
    # Special case for X[()]
    self.empty_tuple_index = empty_tuple_index
    # If this UnboundType was originally defined as a str or bytes, keep track of
    # the original contents of that string-like thing. This way, if this UnboundExpr
    # ever shows up inside of a LiteralType, we can determine whether that
    # Literal[...] is valid or not. E.g. Literal[foo] is most likely invalid
    # (unless 'foo' is an alias for another literal or something) and
    # Literal["foo"] most likely is.
    #
    # We keep track of the entire string instead of just using a boolean flag
    # so we can distinguish between things like Literal["foo"] vs
    # Literal["    foo   "].
    #
    # We also keep track of what the original base fallback type was supposed to be
    # so we don't have to try and recompute it later
    self.original_str_expr = original_str_expr
    self.original_str_fallback = original_str_fallback

</t>
<t tx="ekr.20221004064035.2694">def copy_modified(self, args: Bogus[Sequence[Type] | None] = _dummy) -&gt; UnboundType:
    if args is _dummy:
        args = self.args
    return UnboundType(
        name=self.name,
        args=args,
        line=self.line,
        column=self.column,
        optional=self.optional,
        empty_tuple_index=self.empty_tuple_index,
        original_str_expr=self.original_str_expr,
        original_str_fallback=self.original_str_fallback,
    )

</t>
<t tx="ekr.20221004064035.2695">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_unbound_type(self)

</t>
<t tx="ekr.20221004064035.2696">def __hash__(self) -&gt; int:
    return hash((self.name, self.optional, tuple(self.args), self.original_str_expr))

</t>
<t tx="ekr.20221004064035.2697">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, UnboundType):
        return NotImplemented
    return (
        self.name == other.name
        and self.optional == other.optional
        and self.args == other.args
        and self.original_str_expr == other.original_str_expr
        and self.original_str_fallback == other.original_str_fallback
    )

</t>
<t tx="ekr.20221004064035.2698">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "UnboundType",
        "name": self.name,
        "args": [a.serialize() for a in self.args],
        "expr": self.original_str_expr,
        "expr_fallback": self.original_str_fallback,
    }

</t>
<t tx="ekr.20221004064035.2699">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UnboundType:
    assert data[".class"] == "UnboundType"
    return UnboundType(
        data["name"],
        [deserialize_type(a) for a in data["args"]],
        original_str_expr=data["expr"],
        original_str_fallback=data["expr_fallback"],
    )


</t>
<t tx="ekr.20221004064035.27">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20221004064035.270">def cannot_determine_type_in_base(self, name: str, base: str, context: Context) -&gt; None:
    self.fail(f'Cannot determine type of "{name}" in base class "{base}"', context)

</t>
<t tx="ekr.20221004064035.2700">class CallableArgument(ProperType):
    """Represents a Arg(type, 'name') inside a Callable's type list.

    Note that this is a synthetic type for helping parse ASTs, not a real type.
    """

    __slots__ = ("typ", "name", "constructor")

    typ: Type
    name: str | None
    constructor: str | None

    @others
</t>
<t tx="ekr.20221004064035.2701">def __init__(
    self,
    typ: Type,
    name: str | None,
    constructor: str | None,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.typ = typ
    self.name = name
    self.constructor = constructor

</t>
<t tx="ekr.20221004064035.2702">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return cast(T, visitor.visit_callable_argument(self))

</t>
<t tx="ekr.20221004064035.2703">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20221004064035.2704">class TypeList(ProperType):
    """Information about argument types and names [...].

    This is used for the arguments of a Callable type, i.e. for
    [arg, ...] in Callable[[arg, ...], ret]. This is not a real type
    but a syntactic AST construct. UnboundTypes can also have TypeList
    types before they are processed into Callable types.
    """

    __slots__ = ("items",)

    items: list[Type]

    @others
</t>
<t tx="ekr.20221004064035.2705">def __init__(self, items: list[Type], line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.items = items

</t>
<t tx="ekr.20221004064035.2706">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return cast(T, visitor.visit_type_list(self))

</t>
<t tx="ekr.20221004064035.2707">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"

</t>
<t tx="ekr.20221004064035.2708">def __hash__(self) -&gt; int:
    return hash(tuple(self.items))

</t>
<t tx="ekr.20221004064035.2709">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeList):
        return False
    return self.items == other.items


</t>
<t tx="ekr.20221004064035.271">def no_formal_self(self, name: str, item: CallableType, context: Context) -&gt; None:
    self.fail(
        'Attribute function "%s" with type %s does not accept self argument'
        % (name, format_type(item)),
        context,
    )

</t>
<t tx="ekr.20221004064035.2710">class UnpackType(ProperType):
    """Type operator Unpack from PEP646. Can be either with Unpack[]
    or unpacking * syntax.

    The inner type should be either a TypeVarTuple, a constant size
    tuple, or a variable length tuple, or a union of one of those.
    """

    __slots__ = ["type"]

    @others
</t>
<t tx="ekr.20221004064035.2711">def __init__(self, typ: Type, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.type = typ

</t>
<t tx="ekr.20221004064035.2712">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_unpack_type(self)

</t>
<t tx="ekr.20221004064035.2713">def serialize(self) -&gt; JsonDict:
    return {".class": "UnpackType", "type": self.type.serialize()}

</t>
<t tx="ekr.20221004064035.2714">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UnpackType:
    assert data[".class"] == "UnpackType"
    typ = data["type"]
    return UnpackType(deserialize_type(typ))


</t>
<t tx="ekr.20221004064035.2715">class AnyType(ProperType):
    """The type 'Any'."""

    __slots__ = ("type_of_any", "source_any", "missing_import_name")

    @others
</t>
<t tx="ekr.20221004064035.2716">def __init__(
    self,
    type_of_any: int,
    source_any: AnyType | None = None,
    missing_import_name: str | None = None,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.type_of_any = type_of_any
    # If this Any was created as a result of interacting with another 'Any', record the source
    # and use it in reports.
    self.source_any = source_any
    if source_any and source_any.source_any:
        self.source_any = source_any.source_any

    if source_any is None:
        self.missing_import_name = missing_import_name
    else:
        self.missing_import_name = source_any.missing_import_name

    # Only unimported type anys and anys from other anys should have an import name
    assert missing_import_name is None or type_of_any in (
        TypeOfAny.from_unimported_type,
        TypeOfAny.from_another_any,
    )
    # Only Anys that come from another Any can have source_any.
    assert type_of_any != TypeOfAny.from_another_any or source_any is not None
    # We should not have chains of Anys.
    assert not self.source_any or self.source_any.type_of_any != TypeOfAny.from_another_any

</t>
<t tx="ekr.20221004064035.2717">@property
def is_from_error(self) -&gt; bool:
    return self.type_of_any == TypeOfAny.from_error

</t>
<t tx="ekr.20221004064035.2718">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_any(self)

</t>
<t tx="ekr.20221004064035.2719">def copy_modified(
    self,
    # Mark with Bogus because _dummy is just an object (with type Any)
    type_of_any: Bogus[int] = _dummy,
    original_any: Bogus[AnyType | None] = _dummy,
) -&gt; AnyType:
    if type_of_any is _dummy:
        type_of_any = self.type_of_any
    if original_any is _dummy:
        original_any = self.source_any
    return AnyType(
        type_of_any=type_of_any,
        source_any=original_any,
        missing_import_name=self.missing_import_name,
        line=self.line,
        column=self.column,
    )

</t>
<t tx="ekr.20221004064035.272">def incompatible_self_argument(
    self, name: str, arg: Type, sig: CallableType, is_classmethod: bool, context: Context
) -&gt; None:
    kind = "class attribute function" if is_classmethod else "attribute function"
    self.fail(
        'Invalid self argument %s to %s "%s" with type %s'
        % (format_type(arg), kind, name, format_type(sig)),
        context,
    )

</t>
<t tx="ekr.20221004064035.2720">def __hash__(self) -&gt; int:
    return hash(AnyType)

</t>
<t tx="ekr.20221004064035.2721">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, AnyType)

</t>
<t tx="ekr.20221004064035.2722">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "AnyType",
        "type_of_any": self.type_of_any,
        "source_any": self.source_any.serialize() if self.source_any is not None else None,
        "missing_import_name": self.missing_import_name,
    }

</t>
<t tx="ekr.20221004064035.2723">@classmethod
def deserialize(cls, data: JsonDict) -&gt; AnyType:
    assert data[".class"] == "AnyType"
    source = data["source_any"]
    return AnyType(
        data["type_of_any"],
        AnyType.deserialize(source) if source is not None else None,
        data["missing_import_name"],
    )


</t>
<t tx="ekr.20221004064035.2724">class UninhabitedType(ProperType):
    """This type has no members.

    This type is the bottom type.
    With strict Optional checking, it is the only common subtype between all
    other types, which allows `meet` to be well defined.  Without strict
    Optional checking, NoneType fills this role.

    In general, for any type T:
        join(UninhabitedType, T) = T
        meet(UninhabitedType, T) = UninhabitedType
        is_subtype(UninhabitedType, T) = True
    """

    __slots__ = ("ambiguous", "is_noreturn")

    is_noreturn: bool  # Does this come from a NoReturn?  Purely for error messages.
    # It is important to track whether this is an actual NoReturn type, or just a result
    # of ambiguous type inference, in the latter case we don't want to mark a branch as
    # unreachable in binder.
    ambiguous: bool  # Is this a result of inference for a variable without constraints?

    @others
</t>
<t tx="ekr.20221004064035.2725">def __init__(self, is_noreturn: bool = False, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.is_noreturn = is_noreturn
    self.ambiguous = False

</t>
<t tx="ekr.20221004064035.2726">def can_be_true_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20221004064035.2727">def can_be_false_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20221004064035.2728">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_uninhabited_type(self)

</t>
<t tx="ekr.20221004064035.2729">def __hash__(self) -&gt; int:
    return hash(UninhabitedType)

</t>
<t tx="ekr.20221004064035.273">def incompatible_conditional_function_def(
    self, defn: FuncDef, old_type: FunctionLike, new_type: FunctionLike
) -&gt; None:
    self.fail("All conditional function variants must have identical signatures", defn)
    if isinstance(old_type, (CallableType, Overloaded)) and isinstance(
        new_type, (CallableType, Overloaded)
    ):
        self.note("Original:", defn)
        self.pretty_callable_or_overload(old_type, defn, offset=4)
        self.note("Redefinition:", defn)
        self.pretty_callable_or_overload(new_type, defn, offset=4)

</t>
<t tx="ekr.20221004064035.2730">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, UninhabitedType)

</t>
<t tx="ekr.20221004064035.2731">def serialize(self) -&gt; JsonDict:
    return {".class": "UninhabitedType", "is_noreturn": self.is_noreturn}

</t>
<t tx="ekr.20221004064035.2732">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UninhabitedType:
    assert data[".class"] == "UninhabitedType"
    return UninhabitedType(is_noreturn=data["is_noreturn"])


</t>
<t tx="ekr.20221004064035.2733">class NoneType(ProperType):
    """The type of 'None'.

    This type can be written by users as 'None'.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.2734">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)

</t>
<t tx="ekr.20221004064035.2735">def can_be_true_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20221004064035.2736">def __hash__(self) -&gt; int:
    return hash(NoneType)

</t>
<t tx="ekr.20221004064035.2737">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, NoneType)

</t>
<t tx="ekr.20221004064035.2738">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_none_type(self)

</t>
<t tx="ekr.20221004064035.2739">def serialize(self) -&gt; JsonDict:
    return {".class": "NoneType"}

</t>
<t tx="ekr.20221004064035.274">def cannot_instantiate_abstract_class(
    self, class_name: str, abstract_attributes: dict[str, bool], context: Context
) -&gt; None:
    attrs = format_string_list([f'"{a}"' for a in abstract_attributes])
    self.fail(
        'Cannot instantiate abstract class "%s" with abstract '
        "attribute%s %s" % (class_name, plural_s(abstract_attributes), attrs),
        context,
        code=codes.ABSTRACT,
    )
    attrs_with_none = [
        f'"{a}"'
        for a, implicit_and_can_return_none in abstract_attributes.items()
        if implicit_and_can_return_none
    ]
    if not attrs_with_none:
        return
    if len(attrs_with_none) == 1:
        note = (
            f"{attrs_with_none[0]} is implicitly abstract because it has an empty function "
            "body. If it is not meant to be abstract, explicitly `return` or `return None`."
        )
    else:
        note = (
            "The following methods were marked implicitly abstract because they have empty "
            f"function bodies: {format_string_list(attrs_with_none)}. "
            "If they are not meant to be abstract, explicitly `return` or `return None`."
        )
    self.note(note, context, code=codes.ABSTRACT)

</t>
<t tx="ekr.20221004064035.2740">@classmethod
def deserialize(cls, data: JsonDict) -&gt; NoneType:
    assert data[".class"] == "NoneType"
    return NoneType()

</t>
<t tx="ekr.20221004064035.2741">def is_singleton_type(self) -&gt; bool:
    return True


</t>
<t tx="ekr.20221004064035.2742"># NoneType used to be called NoneTyp so to avoid needlessly breaking
# external plugins we keep that alias here.
NoneTyp = NoneType


</t>
<t tx="ekr.20221004064035.2743">class ErasedType(ProperType):
    """Placeholder for an erased type.

    This is used during type inference. This has the special property that
    it is ignored during type inference.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.2744">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_erased_type(self)


</t>
<t tx="ekr.20221004064035.2745">class DeletedType(ProperType):
    """Type of deleted variables.

    These can be used as lvalues but not rvalues.
    """

    __slots__ = ("source",)

    source: str | None  # May be None; name that generated this value

    @others
</t>
<t tx="ekr.20221004064035.2746">def __init__(self, source: str | None = None, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.source = source

</t>
<t tx="ekr.20221004064035.2747">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_deleted_type(self)

</t>
<t tx="ekr.20221004064035.2748">def serialize(self) -&gt; JsonDict:
    return {".class": "DeletedType", "source": self.source}

</t>
<t tx="ekr.20221004064035.2749">@classmethod
def deserialize(cls, data: JsonDict) -&gt; DeletedType:
    assert data[".class"] == "DeletedType"
    return DeletedType(data["source"])


</t>
<t tx="ekr.20221004064035.275">def base_class_definitions_incompatible(
    self, name: str, base1: TypeInfo, base2: TypeInfo, context: Context
) -&gt; None:
    self.fail(
        'Definition of "{}" in base class "{}" is incompatible '
        'with definition in base class "{}"'.format(name, base1.name, base2.name),
        context,
    )

</t>
<t tx="ekr.20221004064035.2750"># Fake TypeInfo to be used as a placeholder during Instance de-serialization.
NOT_READY: Final = mypy.nodes.FakeInfo("De-serialization failure: TypeInfo not fixed")


</t>
<t tx="ekr.20221004064035.2751">class ExtraAttrs:
    """Summary of module attributes and types.

    This is used for instances of types.ModuleType, because they can have different
    attributes per instance, and for type narrowing with hasattr() checks.
    """

    @others
</t>
<t tx="ekr.20221004064035.2752">def __init__(
    self,
    attrs: dict[str, Type],
    immutable: set[str] | None = None,
    mod_name: str | None = None,
) -&gt; None:
    self.attrs = attrs
    if immutable is None:
        immutable = set()
    self.immutable = immutable
    self.mod_name = mod_name

</t>
<t tx="ekr.20221004064035.2753">def __hash__(self) -&gt; int:
    return hash((tuple(self.attrs.items()), tuple(sorted(self.immutable))))

</t>
<t tx="ekr.20221004064035.2754">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, ExtraAttrs):
        return NotImplemented
    return self.attrs == other.attrs and self.immutable == other.immutable

</t>
<t tx="ekr.20221004064035.2755">def copy(self) -&gt; ExtraAttrs:
    return ExtraAttrs(self.attrs.copy(), self.immutable.copy(), self.mod_name)

</t>
<t tx="ekr.20221004064035.2756">def __repr__(self) -&gt; str:
    return f"ExtraAttrs({self.attrs!r}, {self.immutable!r}, {self.mod_name!r})"


</t>
<t tx="ekr.20221004064035.2757">class Instance(ProperType):
    """An instance type of form C[T1, ..., Tn].

    The list of type variables may be empty.

    Several types have fallbacks to `Instance`, because in Python everything is an object
    and this concept is impossible to express without intersection types. We therefore use
    fallbacks for all "non-special" (like UninhabitedType, ErasedType etc) types.
    """

    __slots__ = ("type", "args", "invalid", "type_ref", "last_known_value", "_hash", "extra_attrs")

    @others
</t>
<t tx="ekr.20221004064035.2758">def __init__(
    self,
    typ: mypy.nodes.TypeInfo,
    args: Sequence[Type],
    line: int = -1,
    column: int = -1,
    *,
    last_known_value: LiteralType | None = None,
    extra_attrs: ExtraAttrs | None = None,
) -&gt; None:
    super().__init__(line, column)
    self.type = typ
    self.args = tuple(args)
    self.type_ref: str | None = None

    # True if recovered after incorrect number of type arguments error
    self.invalid = False

    # This field keeps track of the underlying Literal[...] value associated with
    # this instance, if one is known.
    #
    # This field is set whenever possible within expressions, but is erased upon
    # variable assignment (see erasetype.remove_instance_last_known_values) unless
    # the variable is declared to be final.
    #
    # For example, consider the following program:
    #
    #     a = 1
    #     b: Final[int] = 2
    #     c: Final = 3
    #     print(a + b + c + 4)
    #
    # The 'Instance' objects associated with the expressions '1', '2', '3', and '4' will
    # have last_known_values of type Literal[1], Literal[2], Literal[3], and Literal[4]
    # respectively. However, the Instance object assigned to 'a' and 'b' will have their
    # last_known_value erased: variable 'a' is mutable; variable 'b' was declared to be
    # specifically an int.
    #
    # Or more broadly, this field lets this Instance "remember" its original declaration
    # when applicable. We want this behavior because we want implicit Final declarations
    # to act pretty much identically with constants: we should be able to replace any
    # places where we use some Final variable with the original value and get the same
    # type-checking behavior. For example, we want this program:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    var: Final = 3
    #    expects_literal(var)
    #
    # ...to type-check in the exact same way as if we had written the program like this:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    expects_literal(3)
    #
    # In order to make this work (especially with literal types), we need var's type
    # (an Instance) to remember the "original" value.
    #
    # Preserving this value within expressions is useful for similar reasons.
    #
    # Currently most of mypy will ignore this field and will continue to treat this type like
    # a regular Instance. We end up using this field only when we are explicitly within a
    # Literal context.
    self.last_known_value = last_known_value

    # Cached hash value
    self._hash = -1

    # Additional attributes defined per instance of this type. For example modules
    # have different attributes per instance of types.ModuleType. This is intended
    # to be "short-lived", we don't serialize it, and even don't store as variable type.
    self.extra_attrs = extra_attrs

</t>
<t tx="ekr.20221004064035.2759">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_instance(self)

</t>
<t tx="ekr.20221004064035.276">def cant_assign_to_method(self, context: Context) -&gt; None:
    self.fail(message_registry.CANNOT_ASSIGN_TO_METHOD, context, code=codes.ASSIGNMENT)

</t>
<t tx="ekr.20221004064035.2760">def __hash__(self) -&gt; int:
    if self._hash == -1:
        self._hash = hash((self.type, self.args, self.last_known_value, self.extra_attrs))
    return self._hash

</t>
<t tx="ekr.20221004064035.2761">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Instance):
        return NotImplemented
    return (
        self.type == other.type
        and self.args == other.args
        and self.last_known_value == other.last_known_value
        and self.extra_attrs == other.extra_attrs
    )

</t>
<t tx="ekr.20221004064035.2762">def serialize(self) -&gt; JsonDict | str:
    assert self.type is not None
    type_ref = self.type.fullname
    if not self.args and not self.last_known_value:
        return type_ref
    data: JsonDict = {".class": "Instance"}
    data["type_ref"] = type_ref
    data["args"] = [arg.serialize() for arg in self.args]
    if self.last_known_value is not None:
        data["last_known_value"] = self.last_known_value.serialize()
    return data

</t>
<t tx="ekr.20221004064035.2763">@classmethod
def deserialize(cls, data: JsonDict | str) -&gt; Instance:
    if isinstance(data, str):
        inst = Instance(NOT_READY, [])
        inst.type_ref = data
        return inst
    assert data[".class"] == "Instance"
    args: list[Type] = []
    if "args" in data:
        args_list = data["args"]
        assert isinstance(args_list, list)
        args = [deserialize_type(arg) for arg in args_list]
    inst = Instance(NOT_READY, args)
    inst.type_ref = data["type_ref"]  # Will be fixed up by fixup.py later.
    if "last_known_value" in data:
        inst.last_known_value = LiteralType.deserialize(data["last_known_value"])
    return inst

</t>
<t tx="ekr.20221004064035.2764">def copy_modified(
    self,
    *,
    args: Bogus[list[Type]] = _dummy,
    last_known_value: Bogus[LiteralType | None] = _dummy,
) -&gt; Instance:
    new = Instance(
        self.type,
        args if args is not _dummy else self.args,
        self.line,
        self.column,
        last_known_value=last_known_value
        if last_known_value is not _dummy
        else self.last_known_value,
    )
    # We intentionally don't copy the extra_attrs here, so they will be erased.
    new.can_be_true = self.can_be_true
    new.can_be_false = self.can_be_false
    return new

</t>
<t tx="ekr.20221004064035.2765">def copy_with_extra_attr(self, name: str, typ: Type) -&gt; Instance:
    if self.extra_attrs:
        existing_attrs = self.extra_attrs.copy()
    else:
        existing_attrs = ExtraAttrs({}, set(), None)
    existing_attrs.attrs[name] = typ
    new = self.copy_modified()
    new.extra_attrs = existing_attrs
    return new

</t>
<t tx="ekr.20221004064035.2766">def has_readable_member(self, name: str) -&gt; bool:
    return self.type.has_readable_member(name)

</t>
<t tx="ekr.20221004064035.2767">def is_singleton_type(self) -&gt; bool:
    # TODO:
    # Also make this return True if the type corresponds to NotImplemented?
    return (
        self.type.is_enum
        and len(self.get_enum_values()) == 1
        or self.type.fullname == "builtins.ellipsis"
    )

</t>
<t tx="ekr.20221004064035.2768">def get_enum_values(self) -&gt; list[str]:
    """Return the list of values for an Enum."""
    return [
        name for name, sym in self.type.names.items() if isinstance(sym.node, mypy.nodes.Var)
    ]


</t>
<t tx="ekr.20221004064035.2769">class FunctionLike(ProperType):
    """Abstract base class for function types."""

    __slots__ = ("fallback",)

    fallback: Instance

    @others
</t>
<t tx="ekr.20221004064035.277">def cant_assign_to_classvar(self, name: str, context: Context) -&gt; None:
    self.fail(f'Cannot assign to class variable "{name}" via instance', context)

</t>
<t tx="ekr.20221004064035.2770">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.can_be_false = False

</t>
<t tx="ekr.20221004064035.2771">@abstractmethod
def is_type_obj(self) -&gt; bool:
    pass

</t>
<t tx="ekr.20221004064035.2772">@abstractmethod
def type_object(self) -&gt; mypy.nodes.TypeInfo:
    pass

</t>
<t tx="ekr.20221004064035.2773">@property
@abstractmethod
def items(self) -&gt; list[CallableType]:
    pass

</t>
<t tx="ekr.20221004064035.2774">@abstractmethod
def with_name(self, name: str) -&gt; FunctionLike:
    pass

</t>
<t tx="ekr.20221004064035.2775">@abstractmethod
def get_name(self) -&gt; str | None:
    pass


</t>
<t tx="ekr.20221004064035.2776">class FormalArgument(NamedTuple):
    name: str | None
    pos: int | None
    typ: Type
    required: bool


</t>
<t tx="ekr.20221004064035.2777"># TODO: should this take bound typevars too? what would this take?
#   ex: class Z(Generic[P, T]): ...; Z[[V], V]
# What does a typevar even mean in this context?
class Parameters(ProperType):
    """Type that represents the parameters to a function.

    Used for ParamSpec analysis."""

    __slots__ = (
        "arg_types",
        "arg_kinds",
        "arg_names",
        "min_args",
        "is_ellipsis_args",
        "variables",
    )

    @others
CT = TypeVar("CT", bound="CallableType")


</t>
<t tx="ekr.20221004064035.2778">def __init__(
    self,
    arg_types: Sequence[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None],
    *,
    variables: Sequence[TypeVarLikeType] | None = None,
    is_ellipsis_args: bool = False,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.arg_types = list(arg_types)
    self.arg_kinds = arg_kinds
    self.arg_names = list(arg_names)
    assert len(arg_types) == len(arg_kinds) == len(arg_names)
    self.min_args = arg_kinds.count(ARG_POS)
    self.is_ellipsis_args = is_ellipsis_args
    self.variables = variables or []

</t>
<t tx="ekr.20221004064035.2779">def copy_modified(
    self,
    arg_types: Bogus[Sequence[Type]] = _dummy,
    arg_kinds: Bogus[list[ArgKind]] = _dummy,
    arg_names: Bogus[Sequence[str | None]] = _dummy,
    *,
    variables: Bogus[Sequence[TypeVarLikeType]] = _dummy,
    is_ellipsis_args: Bogus[bool] = _dummy,
) -&gt; Parameters:
    return Parameters(
        arg_types=arg_types if arg_types is not _dummy else self.arg_types,
        arg_kinds=arg_kinds if arg_kinds is not _dummy else self.arg_kinds,
        arg_names=arg_names if arg_names is not _dummy else self.arg_names,
        is_ellipsis_args=(
            is_ellipsis_args if is_ellipsis_args is not _dummy else self.is_ellipsis_args
        ),
        variables=variables if variables is not _dummy else self.variables,
    )

</t>
<t tx="ekr.20221004064035.278">def final_cant_override_writable(self, name: str, ctx: Context) -&gt; None:
    self.fail(f'Cannot override writable attribute "{name}" with a final one', ctx)

</t>
<t tx="ekr.20221004064035.2780"># the following are copied from CallableType. Is there a way to decrease code duplication?
def var_arg(self) -&gt; FormalArgument | None:
    """The formal argument for *args."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20221004064035.2781">def kw_arg(self) -&gt; FormalArgument | None:
    """The formal argument for **kwargs."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR2:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20221004064035.2782">def formal_arguments(self, include_star_args: bool = False) -&gt; list[FormalArgument]:
    """Yields the formal arguments corresponding to this callable, ignoring *arg and **kwargs.

    To handle *args and **kwargs, use the 'callable.var_args' and 'callable.kw_args' fields,
    if they are not None.

    If you really want to include star args in the yielded output, set the
    'include_star_args' parameter to 'True'."""
    args = []
    done_with_positional = False
    for i in range(len(self.arg_types)):
        kind = self.arg_kinds[i]
        if kind.is_named() or kind.is_star():
            done_with_positional = True
        if not include_star_args and kind.is_star():
            continue

        required = kind.is_required()
        pos = None if done_with_positional else i
        arg = FormalArgument(self.arg_names[i], pos, self.arg_types[i], required)
        args.append(arg)
    return args

</t>
<t tx="ekr.20221004064035.2783">def argument_by_name(self, name: str | None) -&gt; FormalArgument | None:
    if name is None:
        return None
    seen_star = False
    for i, (arg_name, kind, typ) in enumerate(
        zip(self.arg_names, self.arg_kinds, self.arg_types)
    ):
        # No more positional arguments after these.
        if kind.is_named() or kind.is_star():
            seen_star = True
        if kind.is_star():
            continue
        if arg_name == name:
            position = None if seen_star else i
            return FormalArgument(name, position, typ, kind.is_required())
    return self.try_synthesizing_arg_from_kwarg(name)

</t>
<t tx="ekr.20221004064035.2784">def argument_by_position(self, position: int | None) -&gt; FormalArgument | None:
    if position is None:
        return None
    if position &gt;= len(self.arg_names):
        return self.try_synthesizing_arg_from_vararg(position)
    name, kind, typ = (
        self.arg_names[position],
        self.arg_kinds[position],
        self.arg_types[position],
    )
    if kind.is_positional():
        return FormalArgument(name, position, typ, kind == ARG_POS)
    else:
        return self.try_synthesizing_arg_from_vararg(position)

</t>
<t tx="ekr.20221004064035.2785">def try_synthesizing_arg_from_kwarg(self, name: str | None) -&gt; FormalArgument | None:
    kw_arg = self.kw_arg()
    if kw_arg is not None:
        return FormalArgument(name, None, kw_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20221004064035.2786">def try_synthesizing_arg_from_vararg(self, position: int | None) -&gt; FormalArgument | None:
    var_arg = self.var_arg()
    if var_arg is not None:
        return FormalArgument(None, position, var_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20221004064035.2787">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_parameters(self)

</t>
<t tx="ekr.20221004064035.2788">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "Parameters",
        "arg_types": [t.serialize() for t in self.arg_types],
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "arg_names": self.arg_names,
        "variables": [tv.serialize() for tv in self.variables],
    }

</t>
<t tx="ekr.20221004064035.2789">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Parameters:
    assert data[".class"] == "Parameters"
    return Parameters(
        [deserialize_type(t) for t in data["arg_types"]],
        [ArgKind(x) for x in data["arg_kinds"]],
        data["arg_names"],
        variables=[cast(TypeVarLikeType, deserialize_type(v)) for v in data["variables"]],
    )

</t>
<t tx="ekr.20221004064035.279">def cant_override_final(self, name: str, base_name: str, ctx: Context) -&gt; None:
    self.fail(
        'Cannot override final attribute "{}"'
        ' (previously declared in base class "{}")'.format(name, base_name),
        ctx,
    )

</t>
<t tx="ekr.20221004064035.2790">def __hash__(self) -&gt; int:
    return hash(
        (
            self.is_ellipsis_args,
            tuple(self.arg_types),
            tuple(self.arg_names),
            tuple(self.arg_kinds),
        )
    )

</t>
<t tx="ekr.20221004064035.2791">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, Parameters) or isinstance(other, CallableType):
        return (
            self.arg_types == other.arg_types
            and self.arg_names == other.arg_names
            and self.arg_kinds == other.arg_kinds
            and self.is_ellipsis_args == other.is_ellipsis_args
        )
    else:
        return NotImplemented


</t>
<t tx="ekr.20221004064035.2792">class CallableType(FunctionLike):
    """Type of a non-overloaded callable object (such as function)."""

    __slots__ = (
        "arg_types",  # Types of function arguments
        "arg_kinds",  # ARG_ constants
        "arg_names",  # Argument names; None if not a keyword argument
        "min_args",  # Minimum number of arguments; derived from arg_kinds
        "ret_type",  # Return value type
        "name",  # Name (may be None; for error messages and plugins)
        "definition",  # For error messages.  May be None.
        "variables",  # Type variables for a generic function
        "is_ellipsis_args",  # Is this Callable[..., t] (with literal '...')?
        "is_classmethod_class",  # Is this callable constructed for the benefit
        # of a classmethod's 'cls' argument?
        "implicit",  # Was this type implicitly generated instead of explicitly
        # specified by the user?
        "special_sig",  # Non-None for signatures that require special handling
        # (currently only value is 'dict' for a signature similar to
        # 'dict')
        "from_type_type",  # Was this callable generated by analyzing Type[...]
        # instantiation?
        "bound_args",  # Bound type args, mostly unused but may be useful for
        # tools that consume mypy ASTs
        "def_extras",  # Information about original definition we want to serialize.
        # This is used for more detailed error messages.
        "type_guard",  # T, if -&gt; TypeGuard[T] (ret_type is bool in this case).
        "from_concatenate",  # whether this callable is from a concatenate object
        # (this is used for error messages)
        "unpack_kwargs",  # Was an Unpack[...] with **kwargs used to define this callable?
    )

    @others
# This is a little safety net to prevent reckless special-casing of callables
# that can potentially break Unpack[...] with **kwargs.
# TODO: use this in more places in checkexpr.py etc?
NormalizedCallableType = NewType("NormalizedCallableType", CallableType)


</t>
<t tx="ekr.20221004064035.2793">def __init__(
    self,
    # maybe this should be refactored to take a Parameters object
    arg_types: Sequence[Type],
    arg_kinds: list[ArgKind],
    arg_names: Sequence[str | None],
    ret_type: Type,
    fallback: Instance,
    name: str | None = None,
    definition: SymbolNode | None = None,
    variables: Sequence[TypeVarLikeType] | None = None,
    line: int = -1,
    column: int = -1,
    is_ellipsis_args: bool = False,
    implicit: bool = False,
    special_sig: str | None = None,
    from_type_type: bool = False,
    bound_args: Sequence[Type | None] = (),
    def_extras: dict[str, Any] | None = None,
    type_guard: Type | None = None,
    from_concatenate: bool = False,
    unpack_kwargs: bool = False,
) -&gt; None:
    super().__init__(line, column)
    assert len(arg_types) == len(arg_kinds) == len(arg_names)
    if variables is None:
        variables = []
    self.arg_types = list(arg_types)
    self.arg_kinds = arg_kinds
    self.arg_names = list(arg_names)
    self.min_args = arg_kinds.count(ARG_POS)
    self.ret_type = ret_type
    self.fallback = fallback
    assert not name or "&lt;bound method" not in name
    self.name = name
    self.definition = definition
    self.variables = variables
    self.is_ellipsis_args = is_ellipsis_args
    self.implicit = implicit
    self.special_sig = special_sig
    self.from_type_type = from_type_type
    self.from_concatenate = from_concatenate
    if not bound_args:
        bound_args = ()
    self.bound_args = bound_args
    if def_extras:
        self.def_extras = def_extras
    elif isinstance(definition, FuncDef):
        # This information would be lost if we don't have definition
        # after serialization, but it is useful in error messages.
        # TODO: decide how to add more info here (file, line, column)
        # without changing interface hash.
        first_arg: str | None = None
        if definition.arg_names and definition.info and not definition.is_static:
            if getattr(definition, "arguments", None):
                first_arg = definition.arguments[0].variable.name
            else:
                first_arg = definition.arg_names[0]
        self.def_extras = {"first_arg": first_arg}
    else:
        self.def_extras = {}
    self.type_guard = type_guard
    self.unpack_kwargs = unpack_kwargs

</t>
<t tx="ekr.20221004064035.2794">def copy_modified(
    self: CT,
    arg_types: Bogus[Sequence[Type]] = _dummy,
    arg_kinds: Bogus[list[ArgKind]] = _dummy,
    arg_names: Bogus[list[str | None]] = _dummy,
    ret_type: Bogus[Type] = _dummy,
    fallback: Bogus[Instance] = _dummy,
    name: Bogus[str | None] = _dummy,
    definition: Bogus[SymbolNode] = _dummy,
    variables: Bogus[Sequence[TypeVarLikeType]] = _dummy,
    line: Bogus[int] = _dummy,
    column: Bogus[int] = _dummy,
    is_ellipsis_args: Bogus[bool] = _dummy,
    implicit: Bogus[bool] = _dummy,
    special_sig: Bogus[str | None] = _dummy,
    from_type_type: Bogus[bool] = _dummy,
    bound_args: Bogus[list[Type | None]] = _dummy,
    def_extras: Bogus[dict[str, Any]] = _dummy,
    type_guard: Bogus[Type | None] = _dummy,
    from_concatenate: Bogus[bool] = _dummy,
    unpack_kwargs: Bogus[bool] = _dummy,
) -&gt; CT:
    return type(self)(
        arg_types=arg_types if arg_types is not _dummy else self.arg_types,
        arg_kinds=arg_kinds if arg_kinds is not _dummy else self.arg_kinds,
        arg_names=arg_names if arg_names is not _dummy else self.arg_names,
        ret_type=ret_type if ret_type is not _dummy else self.ret_type,
        fallback=fallback if fallback is not _dummy else self.fallback,
        name=name if name is not _dummy else self.name,
        definition=definition if definition is not _dummy else self.definition,
        variables=variables if variables is not _dummy else self.variables,
        line=line if line is not _dummy else self.line,
        column=column if column is not _dummy else self.column,
        is_ellipsis_args=(
            is_ellipsis_args if is_ellipsis_args is not _dummy else self.is_ellipsis_args
        ),
        implicit=implicit if implicit is not _dummy else self.implicit,
        special_sig=special_sig if special_sig is not _dummy else self.special_sig,
        from_type_type=from_type_type if from_type_type is not _dummy else self.from_type_type,
        bound_args=bound_args if bound_args is not _dummy else self.bound_args,
        def_extras=def_extras if def_extras is not _dummy else dict(self.def_extras),
        type_guard=type_guard if type_guard is not _dummy else self.type_guard,
        from_concatenate=(
            from_concatenate if from_concatenate is not _dummy else self.from_concatenate
        ),
        unpack_kwargs=unpack_kwargs if unpack_kwargs is not _dummy else self.unpack_kwargs,
    )

</t>
<t tx="ekr.20221004064035.2795">def var_arg(self) -&gt; FormalArgument | None:
    """The formal argument for *args."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20221004064035.2796">def kw_arg(self) -&gt; FormalArgument | None:
    """The formal argument for **kwargs."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR2:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20221004064035.2797">@property
def is_var_arg(self) -&gt; bool:
    """Does this callable have a *args argument?"""
    return ARG_STAR in self.arg_kinds

</t>
<t tx="ekr.20221004064035.2798">@property
def is_kw_arg(self) -&gt; bool:
    """Does this callable have a **kwargs argument?"""
    return ARG_STAR2 in self.arg_kinds

</t>
<t tx="ekr.20221004064035.2799">def is_type_obj(self) -&gt; bool:
    return self.fallback.type.is_metaclass() and not isinstance(
        get_proper_type(self.ret_type), UninhabitedType
    )

</t>
<t tx="ekr.20221004064035.28">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    if is_proper_subtype(self.s, t):
        return t
    else:
        return mypy.typeops.make_simplified_union([self.s, t])

</t>
<t tx="ekr.20221004064035.280">def cant_assign_to_final(self, name: str, attr_assign: bool, ctx: Context) -&gt; None:
    """Warn about a prohibited assignment to a final attribute.

    Pass `attr_assign=True` if the assignment assigns to an attribute.
    """
    kind = "attribute" if attr_assign else "name"
    self.fail(f'Cannot assign to final {kind} "{unmangle(name)}"', ctx)

</t>
<t tx="ekr.20221004064035.2800">def type_object(self) -&gt; mypy.nodes.TypeInfo:
    assert self.is_type_obj()
    ret = get_proper_type(self.ret_type)
    if isinstance(ret, TypeVarType):
        ret = get_proper_type(ret.upper_bound)
    if isinstance(ret, TupleType):
        ret = ret.partial_fallback
    if isinstance(ret, TypedDictType):
        ret = ret.fallback
    assert isinstance(ret, Instance)
    return ret.type

</t>
<t tx="ekr.20221004064035.2801">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_callable_type(self)

</t>
<t tx="ekr.20221004064035.2802">def with_name(self, name: str) -&gt; CallableType:
    """Return a copy of this type with the specified name."""
    return self.copy_modified(ret_type=self.ret_type, name=name)

</t>
<t tx="ekr.20221004064035.2803">def get_name(self) -&gt; str | None:
    return self.name

</t>
<t tx="ekr.20221004064035.2804">def max_possible_positional_args(self) -&gt; int:
    """Returns maximum number of positional arguments this method could possibly accept.

    This takes into account *arg and **kwargs but excludes keyword-only args."""
    if self.is_var_arg or self.is_kw_arg:
        return sys.maxsize
    return sum(kind.is_positional() for kind in self.arg_kinds)

</t>
<t tx="ekr.20221004064035.2805">def formal_arguments(self, include_star_args: bool = False) -&gt; list[FormalArgument]:
    """Return a list of the formal arguments of this callable, ignoring *arg and **kwargs.

    To handle *args and **kwargs, use the 'callable.var_args' and 'callable.kw_args' fields,
    if they are not None.

    If you really want to include star args in the yielded output, set the
    'include_star_args' parameter to 'True'."""
    args = []
    done_with_positional = False
    for i in range(len(self.arg_types)):
        kind = self.arg_kinds[i]
        if kind.is_named() or kind.is_star():
            done_with_positional = True
        if not include_star_args and kind.is_star():
            continue

        required = kind.is_required()
        pos = None if done_with_positional else i
        arg = FormalArgument(self.arg_names[i], pos, self.arg_types[i], required)
        args.append(arg)
    return args

</t>
<t tx="ekr.20221004064035.2806">def argument_by_name(self, name: str | None) -&gt; FormalArgument | None:
    if name is None:
        return None
    seen_star = False
    for i, (arg_name, kind, typ) in enumerate(
        zip(self.arg_names, self.arg_kinds, self.arg_types)
    ):
        # No more positional arguments after these.
        if kind.is_named() or kind.is_star():
            seen_star = True
        if kind.is_star():
            continue
        if arg_name == name:
            position = None if seen_star else i
            return FormalArgument(name, position, typ, kind.is_required())
    return self.try_synthesizing_arg_from_kwarg(name)

</t>
<t tx="ekr.20221004064035.2807">def argument_by_position(self, position: int | None) -&gt; FormalArgument | None:
    if position is None:
        return None
    if position &gt;= len(self.arg_names):
        return self.try_synthesizing_arg_from_vararg(position)
    name, kind, typ = (
        self.arg_names[position],
        self.arg_kinds[position],
        self.arg_types[position],
    )
    if kind.is_positional():
        return FormalArgument(name, position, typ, kind == ARG_POS)
    else:
        return self.try_synthesizing_arg_from_vararg(position)

</t>
<t tx="ekr.20221004064035.2808">def try_synthesizing_arg_from_kwarg(self, name: str | None) -&gt; FormalArgument | None:
    kw_arg = self.kw_arg()
    if kw_arg is not None:
        return FormalArgument(name, None, kw_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20221004064035.2809">def try_synthesizing_arg_from_vararg(self, position: int | None) -&gt; FormalArgument | None:
    var_arg = self.var_arg()
    if var_arg is not None:
        return FormalArgument(None, position, var_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20221004064035.281">def protocol_members_cant_be_final(self, ctx: Context) -&gt; None:
    self.fail("Protocol member cannot be final", ctx)

</t>
<t tx="ekr.20221004064035.2810">@property
def items(self) -&gt; list[CallableType]:
    return [self]

</t>
<t tx="ekr.20221004064035.2811">def is_generic(self) -&gt; bool:
    return bool(self.variables)

</t>
<t tx="ekr.20221004064035.2812">def type_var_ids(self) -&gt; list[TypeVarId]:
    a: list[TypeVarId] = []
    for tv in self.variables:
        a.append(tv.id)
    return a

</t>
<t tx="ekr.20221004064035.2813">def param_spec(self) -&gt; ParamSpecType | None:
    """Return ParamSpec if callable can be called with one.

    A Callable accepting ParamSpec P args (*args, **kwargs) must have the
    two final parameters like this: *args: P.args, **kwargs: P.kwargs.
    """
    if len(self.arg_types) &lt; 2:
        return None
    if self.arg_kinds[-2] != ARG_STAR or self.arg_kinds[-1] != ARG_STAR2:
        return None
    arg_type = self.arg_types[-2]
    if not isinstance(arg_type, ParamSpecType):
        return None
    # sometimes paramspectypes are analyzed in from mysterious places,
    # e.g. def f(prefix..., *args: P.args, **kwargs: P.kwargs) -&gt; ...: ...
    prefix = arg_type.prefix
    if not prefix.arg_types:
        # TODO: confirm that all arg kinds are positional
        prefix = Parameters(self.arg_types[:-2], self.arg_kinds[:-2], self.arg_names[:-2])
    return ParamSpecType(
        arg_type.name,
        arg_type.fullname,
        arg_type.id,
        ParamSpecFlavor.BARE,
        arg_type.upper_bound,
        prefix=prefix,
    )

</t>
<t tx="ekr.20221004064035.2814">def expand_param_spec(
    self, c: CallableType | Parameters, no_prefix: bool = False
) -&gt; CallableType:
    variables = c.variables

    if no_prefix:
        return self.copy_modified(
            arg_types=c.arg_types,
            arg_kinds=c.arg_kinds,
            arg_names=c.arg_names,
            is_ellipsis_args=c.is_ellipsis_args,
            variables=[*variables, *self.variables],
        )
    else:
        return self.copy_modified(
            arg_types=self.arg_types[:-2] + c.arg_types,
            arg_kinds=self.arg_kinds[:-2] + c.arg_kinds,
            arg_names=self.arg_names[:-2] + c.arg_names,
            is_ellipsis_args=c.is_ellipsis_args,
            variables=[*variables, *self.variables],
        )

</t>
<t tx="ekr.20221004064035.2815">def with_unpacked_kwargs(self) -&gt; NormalizedCallableType:
    if not self.unpack_kwargs:
        return NormalizedCallableType(self.copy_modified())
    last_type = get_proper_type(self.arg_types[-1])
    assert isinstance(last_type, TypedDictType)
    extra_kinds = [
        ArgKind.ARG_NAMED if name in last_type.required_keys else ArgKind.ARG_NAMED_OPT
        for name in last_type.items
    ]
    new_arg_kinds = self.arg_kinds[:-1] + extra_kinds
    new_arg_names = self.arg_names[:-1] + list(last_type.items)
    new_arg_types = self.arg_types[:-1] + list(last_type.items.values())
    return NormalizedCallableType(
        self.copy_modified(
            arg_kinds=new_arg_kinds,
            arg_names=new_arg_names,
            arg_types=new_arg_types,
            unpack_kwargs=False,
        )
    )

</t>
<t tx="ekr.20221004064035.2816">def __hash__(self) -&gt; int:
    # self.is_type_obj() will fail if self.fallback.type is a FakeInfo
    if isinstance(self.fallback.type, FakeInfo):
        is_type_obj = 2
    else:
        is_type_obj = self.is_type_obj()
    return hash(
        (
            self.ret_type,
            is_type_obj,
            self.is_ellipsis_args,
            self.name,
            tuple(self.arg_types),
            tuple(self.arg_names),
            tuple(self.arg_kinds),
            self.fallback,
        )
    )

</t>
<t tx="ekr.20221004064035.2817">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, CallableType):
        return (
            self.ret_type == other.ret_type
            and self.arg_types == other.arg_types
            and self.arg_names == other.arg_names
            and self.arg_kinds == other.arg_kinds
            and self.name == other.name
            and self.is_type_obj() == other.is_type_obj()
            and self.is_ellipsis_args == other.is_ellipsis_args
            and self.fallback == other.fallback
        )
    else:
        return NotImplemented

</t>
<t tx="ekr.20221004064035.2818">def serialize(self) -&gt; JsonDict:
    # TODO: As an optimization, leave out everything related to
    # generic functions for non-generic functions.
    return {
        ".class": "CallableType",
        "arg_types": [t.serialize() for t in self.arg_types],
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "arg_names": self.arg_names,
        "ret_type": self.ret_type.serialize(),
        "fallback": self.fallback.serialize(),
        "name": self.name,
        # We don't serialize the definition (only used for error messages).
        "variables": [v.serialize() for v in self.variables],
        "is_ellipsis_args": self.is_ellipsis_args,
        "implicit": self.implicit,
        "bound_args": [(None if t is None else t.serialize()) for t in self.bound_args],
        "def_extras": dict(self.def_extras),
        "type_guard": self.type_guard.serialize() if self.type_guard is not None else None,
        "from_concatenate": self.from_concatenate,
        "unpack_kwargs": self.unpack_kwargs,
    }

</t>
<t tx="ekr.20221004064035.2819">@classmethod
def deserialize(cls, data: JsonDict) -&gt; CallableType:
    assert data[".class"] == "CallableType"
    # TODO: Set definition to the containing SymbolNode?
    return CallableType(
        [deserialize_type(t) for t in data["arg_types"]],
        [ArgKind(x) for x in data["arg_kinds"]],
        data["arg_names"],
        deserialize_type(data["ret_type"]),
        Instance.deserialize(data["fallback"]),
        name=data["name"],
        variables=[cast(TypeVarLikeType, deserialize_type(v)) for v in data["variables"]],
        is_ellipsis_args=data["is_ellipsis_args"],
        implicit=data["implicit"],
        bound_args=[(None if t is None else deserialize_type(t)) for t in data["bound_args"]],
        def_extras=data["def_extras"],
        type_guard=(
            deserialize_type(data["type_guard"]) if data["type_guard"] is not None else None
        ),
        from_concatenate=data["from_concatenate"],
        unpack_kwargs=data["unpack_kwargs"],
    )


</t>
<t tx="ekr.20221004064035.282">def final_without_value(self, ctx: Context) -&gt; None:
    self.fail("Final name must be initialized with a value", ctx)

</t>
<t tx="ekr.20221004064035.2820">class Overloaded(FunctionLike):
    """Overloaded function type T1, ... Tn, where each Ti is CallableType.

    The variant to call is chosen based on static argument
    types. Overloaded function types can only be defined in stub
    files, and thus there is no explicit runtime dispatch
    implementation.
    """

    __slots__ = ("_items",)

    _items: list[CallableType]  # Must not be empty

    @others
</t>
<t tx="ekr.20221004064035.2821">def __init__(self, items: list[CallableType]) -&gt; None:
    super().__init__(items[0].line, items[0].column)
    self._items = items
    self.fallback = items[0].fallback

</t>
<t tx="ekr.20221004064035.2822">@property
def items(self) -&gt; list[CallableType]:
    return self._items

</t>
<t tx="ekr.20221004064035.2823">def name(self) -&gt; str | None:
    return self.get_name()

</t>
<t tx="ekr.20221004064035.2824">def is_type_obj(self) -&gt; bool:
    # All the items must have the same type object status, so it's
    # sufficient to query only (any) one of them.
    return self._items[0].is_type_obj()

</t>
<t tx="ekr.20221004064035.2825">def type_object(self) -&gt; mypy.nodes.TypeInfo:
    # All the items must have the same type object, so it's sufficient to
    # query only (any) one of them.
    return self._items[0].type_object()

</t>
<t tx="ekr.20221004064035.2826">def with_name(self, name: str) -&gt; Overloaded:
    ni: list[CallableType] = []
    for it in self._items:
        ni.append(it.with_name(name))
    return Overloaded(ni)

</t>
<t tx="ekr.20221004064035.2827">def get_name(self) -&gt; str | None:
    return self._items[0].name

</t>
<t tx="ekr.20221004064035.2828">def with_unpacked_kwargs(self) -&gt; Overloaded:
    return Overloaded([i.with_unpacked_kwargs() for i in self.items])

</t>
<t tx="ekr.20221004064035.2829">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_overloaded(self)

</t>
<t tx="ekr.20221004064035.283">def read_only_property(self, name: str, type: TypeInfo, context: Context) -&gt; None:
    self.fail(f'Property "{name}" defined in "{type.name}" is read-only', context)

</t>
<t tx="ekr.20221004064035.2830">def __hash__(self) -&gt; int:
    return hash(tuple(self.items))

</t>
<t tx="ekr.20221004064035.2831">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Overloaded):
        return NotImplemented
    return self.items == other.items

</t>
<t tx="ekr.20221004064035.2832">def serialize(self) -&gt; JsonDict:
    return {".class": "Overloaded", "items": [t.serialize() for t in self.items]}

</t>
<t tx="ekr.20221004064035.2833">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Overloaded:
    assert data[".class"] == "Overloaded"
    return Overloaded([CallableType.deserialize(t) for t in data["items"]])


</t>
<t tx="ekr.20221004064035.2834">class TupleType(ProperType):
    """The tuple type Tuple[T1, ..., Tn] (at least one type argument).

    Instance variables:
        items: Tuple item types
        partial_fallback: The (imprecise) underlying instance type that is used
            for non-tuple methods. This is generally builtins.tuple[Any, ...] for
            regular tuples, but it's different for named tuples and classes with
            a tuple base class. Use mypy.typeops.tuple_fallback to calculate the
            precise fallback type derived from item types.
        implicit: If True, derived from a tuple expression (t,....) instead of Tuple[t, ...]
    """

    __slots__ = ("items", "partial_fallback", "implicit")

    items: list[Type]
    partial_fallback: Instance
    implicit: bool

    @others
</t>
<t tx="ekr.20221004064035.2835">def __init__(
    self,
    items: list[Type],
    fallback: Instance,
    line: int = -1,
    column: int = -1,
    implicit: bool = False,
) -&gt; None:
    self.partial_fallback = fallback
    self.items = items
    self.implicit = implicit
    super().__init__(line, column)

</t>
<t tx="ekr.20221004064035.2836">def can_be_true_default(self) -&gt; bool:
    if self.can_be_any_bool():
        # Corner case: it is a `NamedTuple` with `__bool__` method defined.
        # It can be anything: both `True` and `False`.
        return True
    return self.length() &gt; 0

</t>
<t tx="ekr.20221004064035.2837">def can_be_false_default(self) -&gt; bool:
    if self.can_be_any_bool():
        # Corner case: it is a `NamedTuple` with `__bool__` method defined.
        # It can be anything: both `True` and `False`.
        return True
    return self.length() == 0

</t>
<t tx="ekr.20221004064035.2838">def can_be_any_bool(self) -&gt; bool:
    return bool(
        self.partial_fallback.type
        and self.partial_fallback.type.fullname != "builtins.tuple"
        and self.partial_fallback.type.names.get("__bool__")
    )

</t>
<t tx="ekr.20221004064035.2839">def length(self) -&gt; int:
    return len(self.items)

</t>
<t tx="ekr.20221004064035.284">def incompatible_typevar_value(
    self, callee: CallableType, typ: Type, typevar_name: str, context: Context
) -&gt; None:
    self.fail(
        message_registry.INCOMPATIBLE_TYPEVAR_VALUE.format(
            typevar_name, callable_name(callee) or "function", format_type(typ)
        ),
        context,
        code=codes.TYPE_VAR,
    )

</t>
<t tx="ekr.20221004064035.2840">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_tuple_type(self)

</t>
<t tx="ekr.20221004064035.2841">def __hash__(self) -&gt; int:
    return hash((tuple(self.items), self.partial_fallback))

</t>
<t tx="ekr.20221004064035.2842">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TupleType):
        return NotImplemented
    return self.items == other.items and self.partial_fallback == other.partial_fallback

</t>
<t tx="ekr.20221004064035.2843">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TupleType",
        "items": [t.serialize() for t in self.items],
        "partial_fallback": self.partial_fallback.serialize(),
        "implicit": self.implicit,
    }

</t>
<t tx="ekr.20221004064035.2844">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TupleType:
    assert data[".class"] == "TupleType"
    return TupleType(
        [deserialize_type(t) for t in data["items"]],
        Instance.deserialize(data["partial_fallback"]),
        implicit=data["implicit"],
    )

</t>
<t tx="ekr.20221004064035.2845">def copy_modified(
    self, *, fallback: Instance | None = None, items: list[Type] | None = None
) -&gt; TupleType:
    if fallback is None:
        fallback = self.partial_fallback
    if items is None:
        items = self.items
    return TupleType(items, fallback, self.line, self.column)

</t>
<t tx="ekr.20221004064035.2846">def slice(self, begin: int | None, end: int | None, stride: int | None) -&gt; TupleType:
    return TupleType(
        self.items[begin:end:stride],
        self.partial_fallback,
        self.line,
        self.column,
        self.implicit,
    )


</t>
<t tx="ekr.20221004064035.2847">class TypedDictType(ProperType):
    """Type of TypedDict object {'k1': v1, ..., 'kn': vn}.

    A TypedDict object is a dictionary with specific string (literal) keys. Each
    key has a value with a distinct type that depends on the key. TypedDict objects
    are normal dict objects at runtime.

    A TypedDictType can be either named or anonymous. If it's anonymous, its
    fallback will be typing_extensions._TypedDict (Instance). _TypedDict is a subclass
    of Mapping[str, object] and defines all non-mapping dict methods that TypedDict
    supports. Some dict methods are unsafe and not supported. _TypedDict isn't defined
    at runtime.

    If a TypedDict is named, its fallback will be an Instance of the named type
    (ex: "Point") whose TypeInfo has a typeddict_type that is anonymous. This
    is similar to how named tuples work.

    TODO: The fallback structure is perhaps overly complicated.
    """

    __slots__ = ("items", "required_keys", "fallback")

    items: dict[str, Type]  # item_name -&gt; item_type
    required_keys: set[str]
    fallback: Instance

    @others
</t>
<t tx="ekr.20221004064035.2848">def __init__(
    self,
    items: dict[str, Type],
    required_keys: set[str],
    fallback: Instance,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    super().__init__(line, column)
    self.items = items
    self.required_keys = required_keys
    self.fallback = fallback
    self.can_be_true = len(self.items) &gt; 0
    self.can_be_false = len(self.required_keys) == 0

</t>
<t tx="ekr.20221004064035.2849">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_typeddict_type(self)

</t>
<t tx="ekr.20221004064035.285">def dangerous_comparison(self, left: Type, right: Type, kind: str, ctx: Context) -&gt; None:
    left_str = "element" if kind == "container" else "left operand"
    right_str = "container item" if kind == "container" else "right operand"
    message = "Non-overlapping {} check ({} type: {}, {} type: {})"
    left_typ, right_typ = format_type_distinctly(left, right)
    self.fail(
        message.format(kind, left_str, left_typ, right_str, right_typ),
        ctx,
        code=codes.COMPARISON_OVERLAP,
    )

</t>
<t tx="ekr.20221004064035.2850">def __hash__(self) -&gt; int:
    return hash((frozenset(self.items.items()), self.fallback, frozenset(self.required_keys)))

</t>
<t tx="ekr.20221004064035.2851">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, TypedDictType):
        if frozenset(self.items.keys()) != frozenset(other.items.keys()):
            return False
        for (_, left_item_type, right_item_type) in self.zip(other):
            if not left_item_type == right_item_type:
                return False
        return self.fallback == other.fallback and self.required_keys == other.required_keys
    else:
        return NotImplemented

</t>
<t tx="ekr.20221004064035.2852">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TypedDictType",
        "items": [[n, t.serialize()] for (n, t) in self.items.items()],
        "required_keys": sorted(self.required_keys),
        "fallback": self.fallback.serialize(),
    }

</t>
<t tx="ekr.20221004064035.2853">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypedDictType:
    assert data[".class"] == "TypedDictType"
    return TypedDictType(
        {n: deserialize_type(t) for (n, t) in data["items"]},
        set(data["required_keys"]),
        Instance.deserialize(data["fallback"]),
    )

</t>
<t tx="ekr.20221004064035.2854">def is_anonymous(self) -&gt; bool:
    return self.fallback.type.fullname in TPDICT_FB_NAMES

</t>
<t tx="ekr.20221004064035.2855">def as_anonymous(self) -&gt; TypedDictType:
    if self.is_anonymous():
        return self
    assert self.fallback.type.typeddict_type is not None
    return self.fallback.type.typeddict_type.as_anonymous()

</t>
<t tx="ekr.20221004064035.2856">def copy_modified(
    self,
    *,
    fallback: Instance | None = None,
    item_types: list[Type] | None = None,
    required_keys: set[str] | None = None,
) -&gt; TypedDictType:
    if fallback is None:
        fallback = self.fallback
    if item_types is None:
        items = self.items
    else:
        items = dict(zip(self.items, item_types))
    if required_keys is None:
        required_keys = self.required_keys
    return TypedDictType(items, required_keys, fallback, self.line, self.column)

</t>
<t tx="ekr.20221004064035.2857">def create_anonymous_fallback(self) -&gt; Instance:
    anonymous = self.as_anonymous()
    return anonymous.fallback

</t>
<t tx="ekr.20221004064035.2858">def names_are_wider_than(self, other: TypedDictType) -&gt; bool:
    return len(other.items.keys() - self.items.keys()) == 0

</t>
<t tx="ekr.20221004064035.2859">def zip(self, right: TypedDictType) -&gt; Iterable[tuple[str, Type, Type]]:
    left = self
    for (item_name, left_item_type) in left.items.items():
        right_item_type = right.items.get(item_name)
        if right_item_type is not None:
            yield (item_name, left_item_type, right_item_type)

</t>
<t tx="ekr.20221004064035.286">def overload_inconsistently_applies_decorator(self, decorator: str, context: Context) -&gt; None:
    self.fail(
        f'Overload does not consistently use the "@{decorator}" '
        + "decorator on all function signatures.",
        context,
    )

</t>
<t tx="ekr.20221004064035.2860">def zipall(self, right: TypedDictType) -&gt; Iterable[tuple[str, Type | None, Type | None]]:
    left = self
    for (item_name, left_item_type) in left.items.items():
        right_item_type = right.items.get(item_name)
        yield (item_name, left_item_type, right_item_type)
    for (item_name, right_item_type) in right.items.items():
        if item_name in left.items:
            continue
        yield (item_name, None, right_item_type)


</t>
<t tx="ekr.20221004064035.2861">class RawExpressionType(ProperType):
    """A synthetic type representing some arbitrary expression that does not cleanly
    translate into a type.

    This synthetic type is only used at the beginning stages of semantic analysis
    and should be completely removing during the process for mapping UnboundTypes to
    actual types: we either turn it into a LiteralType or an AnyType.

    For example, suppose `Foo[1]` is initially represented as the following:

        UnboundType(
            name='Foo',
            args=[
                RawExpressionType(value=1, base_type_name='builtins.int'),
            ],
        )

    As we perform semantic analysis, this type will transform into one of two
    possible forms.

    If 'Foo' was an alias for 'Literal' all along, this type is transformed into:

        LiteralType(value=1, fallback=int_instance_here)

    Alternatively, if 'Foo' is an unrelated class, we report an error and instead
    produce something like this:

        Instance(type=typeinfo_for_foo, args=[AnyType(TypeOfAny.from_error))

    If the "note" field is not None, the provided note will be reported alongside the
    error at this point.

    Note: if "literal_value" is None, that means this object is representing some
    expression that cannot possibly be a parameter of Literal[...]. For example,
    "Foo[3j]" would be represented as:

        UnboundType(
            name='Foo',
            args=[
                RawExpressionType(value=None, base_type_name='builtins.complex'),
            ],
        )
    """

    __slots__ = ("literal_value", "base_type_name", "note")

    @others
</t>
<t tx="ekr.20221004064035.2862">def __init__(
    self,
    literal_value: LiteralValue | None,
    base_type_name: str,
    line: int = -1,
    column: int = -1,
    note: str | None = None,
) -&gt; None:
    super().__init__(line, column)
    self.literal_value = literal_value
    self.base_type_name = base_type_name
    self.note = note

</t>
<t tx="ekr.20221004064035.2863">def simple_name(self) -&gt; str:
    return self.base_type_name.replace("builtins.", "")

</t>
<t tx="ekr.20221004064035.2864">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return cast(T, visitor.visit_raw_expression_type(self))

</t>
<t tx="ekr.20221004064035.2865">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"

</t>
<t tx="ekr.20221004064035.2866">def __hash__(self) -&gt; int:
    return hash((self.literal_value, self.base_type_name))

</t>
<t tx="ekr.20221004064035.2867">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, RawExpressionType):
        return (
            self.base_type_name == other.base_type_name
            and self.literal_value == other.literal_value
        )
    else:
        return NotImplemented


</t>
<t tx="ekr.20221004064035.2868">class LiteralType(ProperType):
    """The type of a Literal instance. Literal[Value]

    A Literal always consists of:

    1. A native Python object corresponding to the contained inner value
    2. A fallback for this Literal. The fallback also corresponds to the
       parent type this Literal subtypes.

    For example, 'Literal[42]' is represented as
    'LiteralType(value=42, fallback=instance_of_int)'

    As another example, `Literal[Color.RED]` (where Color is an enum) is
    represented as `LiteralType(value="RED", fallback=instance_of_color)'.
    """

    __slots__ = ("value", "fallback", "_hash")

    @others
</t>
<t tx="ekr.20221004064035.2869">def __init__(
    self, value: LiteralValue, fallback: Instance, line: int = -1, column: int = -1
) -&gt; None:
    self.value = value
    super().__init__(line, column)
    self.fallback = fallback
    self._hash = -1  # Cached hash value

</t>
<t tx="ekr.20221004064035.287">def overloaded_signatures_overlap(self, index1: int, index2: int, context: Context) -&gt; None:
    self.fail(
        "Overloaded function signatures {} and {} overlap with "
        "incompatible return types".format(index1, index2),
        context,
    )

</t>
<t tx="ekr.20221004064035.2870">def can_be_false_default(self) -&gt; bool:
    return not self.value

</t>
<t tx="ekr.20221004064035.2871">def can_be_true_default(self) -&gt; bool:
    return bool(self.value)

</t>
<t tx="ekr.20221004064035.2872">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_literal_type(self)

</t>
<t tx="ekr.20221004064035.2873">def __hash__(self) -&gt; int:
    if self._hash == -1:
        self._hash = hash((self.value, self.fallback))
    return self._hash

</t>
<t tx="ekr.20221004064035.2874">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, LiteralType):
        return self.fallback == other.fallback and self.value == other.value
    else:
        return NotImplemented

</t>
<t tx="ekr.20221004064035.2875">def is_enum_literal(self) -&gt; bool:
    return self.fallback.type.is_enum

</t>
<t tx="ekr.20221004064035.2876">def value_repr(self) -&gt; str:
    """Returns the string representation of the underlying type.

    This function is almost equivalent to running `repr(self.value)`,
    except it includes some additional logic to correctly handle cases
    where the value is a string, byte string, a unicode string, or an enum.
    """
    raw = repr(self.value)
    fallback_name = self.fallback.type.fullname

    # If this is backed by an enum,
    if self.is_enum_literal():
        return f"{fallback_name}.{self.value}"

    if fallback_name == "builtins.bytes":
        # Note: 'builtins.bytes' only appears in Python 3, so we want to
        # explicitly prefix with a "b"
        return "b" + raw
    else:
        # 'builtins.str' could mean either depending on context, but either way
        # we don't prefix: it's the "native" string. And of course, if value is
        # some other type, we just return that string repr directly.
        return raw

</t>
<t tx="ekr.20221004064035.2877">def serialize(self) -&gt; JsonDict | str:
    return {
        ".class": "LiteralType",
        "value": self.value,
        "fallback": self.fallback.serialize(),
    }

</t>
<t tx="ekr.20221004064035.2878">@classmethod
def deserialize(cls, data: JsonDict) -&gt; LiteralType:
    assert data[".class"] == "LiteralType"
    return LiteralType(value=data["value"], fallback=Instance.deserialize(data["fallback"]))

</t>
<t tx="ekr.20221004064035.2879">def is_singleton_type(self) -&gt; bool:
    return self.is_enum_literal() or isinstance(self.value, bool)


</t>
<t tx="ekr.20221004064035.288">def overloaded_signature_will_never_match(
    self, index1: int, index2: int, context: Context
) -&gt; None:
    self.fail(
        "Overloaded function signature {index2} will never be matched: "
        "signature {index1}'s parameter type(s) are the same or broader".format(
            index1=index1, index2=index2
        ),
        context,
    )

</t>
<t tx="ekr.20221004064035.2880">class StarType(ProperType):
    """The star type *type_parameter.

    This is not a real type but a syntactic AST construct.
    """

    __slots__ = ("type",)

    type: Type

    @others
</t>
<t tx="ekr.20221004064035.2881">def __init__(self, type: Type, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.type = type

</t>
<t tx="ekr.20221004064035.2882">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return cast(T, visitor.visit_star_type(self))

</t>
<t tx="ekr.20221004064035.2883">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20221004064035.2884">class UnionType(ProperType):
    """The union type Union[T1, ..., Tn] (at least one type argument)."""

    __slots__ = ("items", "is_evaluated", "uses_pep604_syntax")

    @others
</t>
<t tx="ekr.20221004064035.2885">def __init__(
    self,
    items: Sequence[Type],
    line: int = -1,
    column: int = -1,
    is_evaluated: bool = True,
    uses_pep604_syntax: bool = False,
) -&gt; None:
    super().__init__(line, column)
    # We must keep this false to avoid crashes during semantic analysis.
    # TODO: maybe switch this to True during type-checking pass?
    self.items = flatten_nested_unions(items, handle_type_alias_type=False)
    self.can_be_true = any(item.can_be_true for item in items)
    self.can_be_false = any(item.can_be_false for item in items)
    # is_evaluated should be set to false for type comments and string literals
    self.is_evaluated = is_evaluated
    # uses_pep604_syntax is True if Union uses OR syntax (X | Y)
    self.uses_pep604_syntax = uses_pep604_syntax

</t>
<t tx="ekr.20221004064035.2886">def __hash__(self) -&gt; int:
    return hash(frozenset(self.items))

</t>
<t tx="ekr.20221004064035.2887">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, UnionType):
        return NotImplemented
    return frozenset(self.items) == frozenset(other.items)

</t>
<t tx="ekr.20221004064035.2888">@overload
@staticmethod
def make_union(items: Sequence[ProperType], line: int = -1, column: int = -1) -&gt; ProperType:
    ...

</t>
<t tx="ekr.20221004064035.2889">@overload
@staticmethod
def make_union(items: Sequence[Type], line: int = -1, column: int = -1) -&gt; Type:
    ...

</t>
<t tx="ekr.20221004064035.289">def overloaded_signatures_typevar_specific(self, index: int, context: Context) -&gt; None:
    self.fail(
        f"Overloaded function implementation cannot satisfy signature {index} "
        + "due to inconsistencies in how they use type variables",
        context,
    )

</t>
<t tx="ekr.20221004064035.2890">@staticmethod
def make_union(items: Sequence[Type], line: int = -1, column: int = -1) -&gt; Type:
    if len(items) &gt; 1:
        return UnionType(items, line, column)
    elif len(items) == 1:
        return items[0]
    else:
        return UninhabitedType()

</t>
<t tx="ekr.20221004064035.2891">def length(self) -&gt; int:
    return len(self.items)

</t>
<t tx="ekr.20221004064035.2892">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_union_type(self)

</t>
<t tx="ekr.20221004064035.2893">def has_readable_member(self, name: str) -&gt; bool:
    """For a tree of unions of instances, check whether all instances have a given member.

    TODO: Deal with attributes of TupleType etc.
    TODO: This should probably be refactored to go elsewhere.
    """
    return all(
        (isinstance(x, UnionType) and x.has_readable_member(name))
        or (isinstance(x, Instance) and x.type.has_readable_member(name))
        for x in get_proper_types(self.relevant_items())
    )

</t>
<t tx="ekr.20221004064035.2894">def relevant_items(self) -&gt; list[Type]:
    """Removes NoneTypes from Unions when strict Optional checking is off."""
    if state.strict_optional:
        return self.items
    else:
        return [i for i in self.items if not isinstance(get_proper_type(i), NoneType)]

</t>
<t tx="ekr.20221004064035.2895">def serialize(self) -&gt; JsonDict:
    return {".class": "UnionType", "items": [t.serialize() for t in self.items]}

</t>
<t tx="ekr.20221004064035.2896">@classmethod
def deserialize(cls, data: JsonDict) -&gt; UnionType:
    assert data[".class"] == "UnionType"
    return UnionType([deserialize_type(t) for t in data["items"]])


</t>
<t tx="ekr.20221004064035.2897">class PartialType(ProperType):
    """Type such as List[?] where type arguments are unknown, or partial None type.

    These are used for inferring types in multiphase initialization such as this:

      x = []       # x gets a partial type List[?], as item type is unknown
      x.append(1)  # partial type gets replaced with normal type List[int]

    Or with None:

      x = None  # x gets a partial type None
      if c:
          x = 1  # Infer actual type int for x
    """

    __slots__ = ("type", "var", "value_type")

    # None for the 'None' partial type; otherwise a generic class
    type: mypy.nodes.TypeInfo | None
    var: mypy.nodes.Var
    # For partial defaultdict[K, V], the type V (K is unknown). If V is generic,
    # the type argument is Any and will be replaced later.
    value_type: Instance | None

    @others
</t>
<t tx="ekr.20221004064035.2898">def __init__(
    self,
    type: mypy.nodes.TypeInfo | None,
    var: mypy.nodes.Var,
    value_type: Instance | None = None,
) -&gt; None:
    super().__init__()
    self.type = type
    self.var = var
    self.value_type = value_type

</t>
<t tx="ekr.20221004064035.2899">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_partial_type(self)


</t>
<t tx="ekr.20221004064035.29">def visit_any(self, t: AnyType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20221004064035.290">def overloaded_signatures_arg_specific(self, index: int, context: Context) -&gt; None:
    self.fail(
        "Overloaded function implementation does not accept all possible arguments "
        "of signature {}".format(index),
        context,
    )

</t>
<t tx="ekr.20221004064035.2900">class EllipsisType(ProperType):
    """The type ... (ellipsis).

    This is not a real type but a syntactic AST construct, used in Callable[..., T], for example.

    A semantically analyzed type will never have ellipsis types.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.2901">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return cast(T, visitor.visit_ellipsis_type(self))

</t>
<t tx="ekr.20221004064035.2902">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20221004064035.2903">class TypeType(ProperType):
    """For types like Type[User].

    This annotates variables that are class objects, constrained by
    the type argument.  See PEP 484 for more details.

    We may encounter expressions whose values are specific classes;
    those are represented as callables (possibly overloaded)
    corresponding to the class's constructor's signature and returning
    an instance of that class.  The difference with Type[C] is that
    those callables always represent the exact class given as the
    return type; Type[C] represents any class that's a subclass of C,
    and C may also be a type variable or a union (or Any).

    Many questions around subtype relationships between Type[C1] and
    def(...) -&gt; C2 are answered by looking at the subtype
    relationships between C1 and C2, since Type[] is considered
    covariant.

    There's an unsolved problem with constructor signatures (also
    unsolved in PEP 484): calling a variable whose type is Type[C]
    assumes the constructor signature for C, even though a subclass of
    C might completely change the constructor signature.  For now we
    just assume that users of Type[C] are careful not to do that (in
    the future we might detect when they are violating that
    assumption).
    """

    __slots__ = ("item",)

    # This can't be everything, but it can be a class reference,
    # a generic class instance, a union, Any, a type variable...
    item: ProperType

    @others
</t>
<t tx="ekr.20221004064035.2904">def __init__(
    self,
    item: Bogus[Instance | AnyType | TypeVarType | TupleType | NoneType | CallableType],
    *,
    line: int = -1,
    column: int = -1,
) -&gt; None:
    """To ensure Type[Union[A, B]] is always represented as Union[Type[A], Type[B]], item of
    type UnionType must be handled through make_normalized static method.
    """
    super().__init__(line, column)
    self.item = item

</t>
<t tx="ekr.20221004064035.2905">@staticmethod
def make_normalized(item: Type, *, line: int = -1, column: int = -1) -&gt; ProperType:
    item = get_proper_type(item)
    if isinstance(item, UnionType):
        return UnionType.make_union(
            [TypeType.make_normalized(union_item) for union_item in item.items],
            line=line,
            column=column,
        )
    return TypeType(item, line=line, column=column)  # type: ignore[arg-type]

</t>
<t tx="ekr.20221004064035.2906">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    return visitor.visit_type_type(self)

</t>
<t tx="ekr.20221004064035.2907">def __hash__(self) -&gt; int:
    return hash(self.item)

</t>
<t tx="ekr.20221004064035.2908">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeType):
        return NotImplemented
    return self.item == other.item

</t>
<t tx="ekr.20221004064035.2909">def serialize(self) -&gt; JsonDict:
    return {".class": "TypeType", "item": self.item.serialize()}

</t>
<t tx="ekr.20221004064035.291">def overloaded_signatures_ret_specific(self, index: int, context: Context) -&gt; None:
    self.fail(
        "Overloaded function implementation cannot produce return type "
        "of signature {}".format(index),
        context,
    )

</t>
<t tx="ekr.20221004064035.2910">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Type:
    assert data[".class"] == "TypeType"
    return TypeType.make_normalized(deserialize_type(data["item"]))


</t>
<t tx="ekr.20221004064035.2911">class PlaceholderType(ProperType):
    """Temporary, yet-unknown type during semantic analysis.

    This is needed when there's a reference to a type before the real symbol
    table entry of the target type is available (specifically, we use a
    temporary PlaceholderNode symbol node). Consider this example:

      class str(Sequence[str]): ...

    We use a PlaceholderType for the 'str' in 'Sequence[str]' since we can't create
    a TypeInfo for 'str' until all base classes have been resolved. We'll soon
    perform another analysis iteration which replaces the base class with a complete
    type without any placeholders. After semantic analysis, no placeholder types must
    exist.
    """

    __slots__ = ("fullname", "args")

    @others
</t>
<t tx="ekr.20221004064035.2912">def __init__(self, fullname: str | None, args: list[Type], line: int) -&gt; None:
    super().__init__(line)
    self.fullname = fullname  # Must be a valid full name of an actual node (or None).
    self.args = args

</t>
<t tx="ekr.20221004064035.2913">def accept(self, visitor: TypeVisitor[T]) -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return cast(T, visitor.visit_placeholder_type(self))

</t>
<t tx="ekr.20221004064035.2914">def serialize(self) -&gt; str:
    # We should never get here since all placeholders should be replaced
    # during semantic analysis.
    assert False, f"Internal error: unresolved placeholder type {self.fullname}"


</t>
<t tx="ekr.20221004064035.2915">@overload
def get_proper_type(typ: None) -&gt; None:
    ...


</t>
<t tx="ekr.20221004064035.2916">@overload
def get_proper_type(typ: Type) -&gt; ProperType:
    ...


</t>
<t tx="ekr.20221004064035.2917">def get_proper_type(typ: Type | None) -&gt; ProperType | None:
    """Get the expansion of a type alias type.

    If the type is already a proper type, this is a no-op. Use this function
    wherever a decision is made on a call like e.g. 'if isinstance(typ, UnionType): ...',
    because 'typ' in this case may be an alias to union. Note: if after making the decision
    on the isinstance() call you pass on the original type (and not one of its components)
    it is recommended to *always* pass on the unexpanded alias.
    """
    if typ is None:
        return None
    if isinstance(typ, TypeGuardedType):  # type: ignore[misc]
        typ = typ.type_guard
    while isinstance(typ, TypeAliasType):
        typ = typ._expand_once()
    assert isinstance(typ, ProperType), typ
    # TODO: store the name of original type alias on this type, so we can show it in errors.
    return typ


</t>
<t tx="ekr.20221004064035.2918">@overload
def get_proper_types(it: Iterable[Type]) -&gt; list[ProperType]:  # type: ignore[misc]
    ...


</t>
<t tx="ekr.20221004064035.2919">@overload
def get_proper_types(it: Iterable[Type | None]) -&gt; list[ProperType | None]:
    ...


</t>
<t tx="ekr.20221004064035.292">def warn_both_operands_are_from_unions(self, context: Context) -&gt; None:
    self.note("Both left and right operands are unions", context, code=codes.OPERATOR)

</t>
<t tx="ekr.20221004064035.2920">def get_proper_types(it: Iterable[Type | None]) -&gt; list[ProperType] | list[ProperType | None]:
    return [get_proper_type(t) for t in it]


</t>
<t tx="ekr.20221004064035.2921"># We split off the type visitor base classes to another module
# to make it easier to gradually get modules working with mypyc.
# Import them here, after the types are defined.
# This is intended as a re-export also.
from mypy.type_visitor import (  # noqa: F811
    SyntheticTypeVisitor as SyntheticTypeVisitor,
    TypeQuery as TypeQuery,
    TypeTranslator as TypeTranslator,
    TypeVisitor as TypeVisitor,
)
from mypy.typetraverser import TypeTraverserVisitor


</t>
<t tx="ekr.20221004064035.2922">class TypeStrVisitor(SyntheticTypeVisitor[str]):
    """Visitor for pretty-printing types into strings.

    This is mostly for debugging/testing.

    Do not preserve original formatting.

    Notes:
     - Represent unbound types as Foo? or Foo?[...].
     - Represent the NoneType type as None.
    """

    @others
</t>
<t tx="ekr.20221004064035.2923">def __init__(self, id_mapper: IdMapper | None = None) -&gt; None:
    self.id_mapper = id_mapper
    self.any_as_dots = False

</t>
<t tx="ekr.20221004064035.2924">def visit_unbound_type(self, t: UnboundType) -&gt; str:
    s = t.name + "?"
    if t.args:
        s += f"[{self.list_str(t.args)}]"
    return s

</t>
<t tx="ekr.20221004064035.2925">def visit_type_list(self, t: TypeList) -&gt; str:
    return f"&lt;TypeList {self.list_str(t.items)}&gt;"

</t>
<t tx="ekr.20221004064035.2926">def visit_callable_argument(self, t: CallableArgument) -&gt; str:
    typ = t.typ.accept(self)
    if t.name is None:
        return f"{t.constructor}({typ})"
    else:
        return f"{t.constructor}({typ}, {t.name})"

</t>
<t tx="ekr.20221004064035.2927">def visit_any(self, t: AnyType) -&gt; str:
    if self.any_as_dots and t.type_of_any == TypeOfAny.special_form:
        return "..."
    return "Any"

</t>
<t tx="ekr.20221004064035.2928">def visit_none_type(self, t: NoneType) -&gt; str:
    return "None"

</t>
<t tx="ekr.20221004064035.2929">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; str:
    return "&lt;nothing&gt;"

</t>
<t tx="ekr.20221004064035.293">def warn_operand_was_from_union(self, side: str, original: Type, context: Context) -&gt; None:
    self.note(
        f"{side} operand is of type {format_type(original)}", context, code=codes.OPERATOR
    )

</t>
<t tx="ekr.20221004064035.2930">def visit_erased_type(self, t: ErasedType) -&gt; str:
    return "&lt;Erased&gt;"

</t>
<t tx="ekr.20221004064035.2931">def visit_deleted_type(self, t: DeletedType) -&gt; str:
    if t.source is None:
        return "&lt;Deleted&gt;"
    else:
        return f"&lt;Deleted '{t.source}'&gt;"

</t>
<t tx="ekr.20221004064035.2932">def visit_instance(self, t: Instance) -&gt; str:
    if t.last_known_value and not t.args:
        # Instances with a literal fallback should never be generic. If they are,
        # something went wrong so we fall back to showing the full Instance repr.
        s = f"{t.last_known_value}?"
    else:
        s = t.type.fullname or t.type.name or "&lt;???&gt;"

    if t.args:
        if t.type.fullname == "builtins.tuple":
            assert len(t.args) == 1
            s += f"[{self.list_str(t.args)}, ...]"
        else:
            s += f"[{self.list_str(t.args)}]"
    if self.id_mapper:
        s += f"&lt;{self.id_mapper.id(t.type)}&gt;"
    return s

</t>
<t tx="ekr.20221004064035.2933">def visit_type_var(self, t: TypeVarType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f"`{t.id}"
    else:
        # Named type variable type.
        s = f"{t.name}`{t.id}"
    if self.id_mapper and t.upper_bound:
        s += f"(upper_bound={t.upper_bound.accept(self)})"
    return s

</t>
<t tx="ekr.20221004064035.2934">def visit_param_spec(self, t: ParamSpecType) -&gt; str:
    # prefixes are displayed as Concatenate
    s = ""
    if t.prefix.arg_types:
        s += f"[{self.list_str(t.prefix.arg_types)}, **"
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s += f"`{t.id}"
    else:
        # Named type variable type.
        s += f"{t.name_with_suffix()}`{t.id}"
    if t.prefix.arg_types:
        s += "]"
    return s

</t>
<t tx="ekr.20221004064035.2935">def visit_parameters(self, t: Parameters) -&gt; str:
    # This is copied from visit_callable -- is there a way to decrease duplication?
    if t.is_ellipsis_args:
        return "..."

    s = ""
    bare_asterisk = False
    for i in range(len(t.arg_types)):
        if s != "":
            s += ", "
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += "*, "
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += "*"
        if t.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = t.arg_names[i]
        if name:
            s += f"{name}: "
        r = t.arg_types[i].accept(self)

        s += r

        if t.arg_kinds[i].is_optional():
            s += " ="

    return f"[{s}]"

</t>
<t tx="ekr.20221004064035.2936">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f"`{t.id}"
    else:
        # Named type variable type.
        s = f"{t.name}`{t.id}"
    return s

</t>
<t tx="ekr.20221004064035.2937">def visit_callable_type(self, t: CallableType) -&gt; str:
    param_spec = t.param_spec()
    if param_spec is not None:
        num_skip = 2
    else:
        num_skip = 0

    s = ""
    bare_asterisk = False
    for i in range(len(t.arg_types) - num_skip):
        if s != "":
            s += ", "
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += "*, "
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += "*"
        if t.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = t.arg_names[i]
        if name:
            s += name + ": "
        type_str = t.arg_types[i].accept(self)
        if t.arg_kinds[i] == ARG_STAR2 and t.unpack_kwargs:
            type_str = f"Unpack[{type_str}]"
        s += type_str
        if t.arg_kinds[i].is_optional():
            s += " ="

    if param_spec is not None:
        n = param_spec.name
        if s:
            s += ", "
        s += f"*{n}.args, **{n}.kwargs"

    s = f"({s})"

    if not isinstance(get_proper_type(t.ret_type), NoneType):
        if t.type_guard is not None:
            s += f" -&gt; TypeGuard[{t.type_guard.accept(self)}]"
        else:
            s += f" -&gt; {t.ret_type.accept(self)}"

    if t.variables:
        vs = []
        for var in t.variables:
            if isinstance(var, TypeVarType):
                # We reimplement TypeVarType.__repr__ here in order to support id_mapper.
                if var.values:
                    vals = f"({', '.join(val.accept(self) for val in var.values)})"
                    vs.append(f"{var.name} in {vals}")
                elif not is_named_instance(var.upper_bound, "builtins.object"):
                    vs.append(f"{var.name} &lt;: {var.upper_bound.accept(self)}")
                else:
                    vs.append(var.name)
            else:
                # For other TypeVarLikeTypes, just use the name
                vs.append(var.name)
        s = f"[{', '.join(vs)}] {s}"

    return f"def {s}"

</t>
<t tx="ekr.20221004064035.2938">def visit_overloaded(self, t: Overloaded) -&gt; str:
    a = []
    for i in t.items:
        a.append(i.accept(self))
    return f"Overload({', '.join(a)})"

</t>
<t tx="ekr.20221004064035.2939">def visit_tuple_type(self, t: TupleType) -&gt; str:
    s = self.list_str(t.items)
    if t.partial_fallback and t.partial_fallback.type:
        fallback_name = t.partial_fallback.type.fullname
        if fallback_name != "builtins.tuple":
            return f"Tuple[{s}, fallback={t.partial_fallback.accept(self)}]"
    return f"Tuple[{s}]"

</t>
<t tx="ekr.20221004064035.294">def operator_method_signatures_overlap(
    self,
    reverse_class: TypeInfo,
    reverse_method: str,
    forward_class: Type,
    forward_method: str,
    context: Context,
) -&gt; None:
    self.fail(
        'Signatures of "{}" of "{}" and "{}" of {} '
        "are unsafely overlapping".format(
            reverse_method, reverse_class.name, forward_method, format_type(forward_class)
        ),
        context,
    )

</t>
<t tx="ekr.20221004064035.2940">def visit_typeddict_type(self, t: TypedDictType) -&gt; str:
    def item_str(name: str, typ: str) -&gt; str:
        if name in t.required_keys:
            return f"{name!r}: {typ}"
        else:
            return f"{name!r}?: {typ}"

    s = (
        "{"
        + ", ".join(item_str(name, typ.accept(self)) for name, typ in t.items.items())
        + "}"
    )
    prefix = ""
    if t.fallback and t.fallback.type:
        if t.fallback.type.fullname not in TPDICT_FB_NAMES:
            prefix = repr(t.fallback.type.fullname) + ", "
    return f"TypedDict({prefix}{s})"

</t>
<t tx="ekr.20221004064035.2941">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; str:
    return repr(t.literal_value)

</t>
<t tx="ekr.20221004064035.2942">def visit_literal_type(self, t: LiteralType) -&gt; str:
    return f"Literal[{t.value_repr()}]"

</t>
<t tx="ekr.20221004064035.2943">def visit_star_type(self, t: StarType) -&gt; str:
    s = t.type.accept(self)
    return f"*{s}"

</t>
<t tx="ekr.20221004064035.2944">def visit_union_type(self, t: UnionType) -&gt; str:
    s = self.list_str(t.items)
    return f"Union[{s}]"

</t>
<t tx="ekr.20221004064035.2945">def visit_partial_type(self, t: PartialType) -&gt; str:
    if t.type is None:
        return "&lt;partial None&gt;"
    else:
        return "&lt;partial {}[{}]&gt;".format(t.type.name, ", ".join(["?"] * len(t.type.type_vars)))

</t>
<t tx="ekr.20221004064035.2946">def visit_ellipsis_type(self, t: EllipsisType) -&gt; str:
    return "..."

</t>
<t tx="ekr.20221004064035.2947">def visit_type_type(self, t: TypeType) -&gt; str:
    return f"Type[{t.item.accept(self)}]"

</t>
<t tx="ekr.20221004064035.2948">def visit_placeholder_type(self, t: PlaceholderType) -&gt; str:
    return f"&lt;placeholder {t.fullname}&gt;"

</t>
<t tx="ekr.20221004064035.2949">def visit_type_alias_type(self, t: TypeAliasType) -&gt; str:
    if t.alias is not None:
        unrolled, recursed = t._partial_expansion()
        self.any_as_dots = recursed
        type_str = unrolled.accept(self)
        self.any_as_dots = False
        return type_str
    return "&lt;alias (unfixed)&gt;"

</t>
<t tx="ekr.20221004064035.295">def forward_operator_not_callable(self, forward_method: str, context: Context) -&gt; None:
    self.fail(f'Forward operator "{forward_method}" is not callable', context)

</t>
<t tx="ekr.20221004064035.2950">def visit_unpack_type(self, t: UnpackType) -&gt; str:
    return f"Unpack[{t.type.accept(self)}]"

</t>
<t tx="ekr.20221004064035.2951">def list_str(self, a: Iterable[Type]) -&gt; str:
    """Convert items of an array to strings (pretty-print types)
    and join the results with commas.
    """
    res = []
    for t in a:
        res.append(t.accept(self))
    return ", ".join(res)


</t>
<t tx="ekr.20221004064035.2952">class TrivialSyntheticTypeTranslator(TypeTranslator, SyntheticTypeVisitor[Type]):
    """A base class for type translators that need to be run during semantic analysis."""

    @others
</t>
<t tx="ekr.20221004064035.2953">def visit_placeholder_type(self, t: PlaceholderType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2954">def visit_callable_argument(self, t: CallableArgument) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2955">def visit_ellipsis_type(self, t: EllipsisType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2956">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2957">def visit_star_type(self, t: StarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.2958">def visit_type_list(self, t: TypeList) -&gt; Type:
    return t


</t>
<t tx="ekr.20221004064035.2959">class UnrollAliasVisitor(TrivialSyntheticTypeTranslator):
    def __init__(self, initial_aliases: set[TypeAliasType]) -&gt; None:
        self.recursed = False
        self.initial_aliases = initial_aliases

    def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
        if t in self.initial_aliases:
            self.recursed = True
            return AnyType(TypeOfAny.special_form)
        # Create a new visitor on encountering a new type alias, so that an alias like
        #     A = Tuple[B, B]
        #     B = int
        # will not be detected as recursive on the second encounter of B.
        subvisitor = UnrollAliasVisitor(self.initial_aliases | {t})
        result = get_proper_type(t).accept(subvisitor)
        if subvisitor.recursed:
            self.recursed = True
        return result


</t>
<t tx="ekr.20221004064035.296">def signatures_incompatible(self, method: str, other_method: str, context: Context) -&gt; None:
    self.fail(f'Signatures of "{method}" and "{other_method}" are incompatible', context)

</t>
<t tx="ekr.20221004064035.2962">def strip_type(typ: Type) -&gt; Type:
    """Make a copy of type without 'debugging info' (function name)."""
    orig_typ = typ
    typ = get_proper_type(typ)
    if isinstance(typ, CallableType):
        return typ.copy_modified(name=None)
    elif isinstance(typ, Overloaded):
        return Overloaded([cast(CallableType, strip_type(item)) for item in typ.items])
    else:
        return orig_typ


</t>
<t tx="ekr.20221004064035.2963">def is_named_instance(t: Type, fullnames: str | tuple[str, ...]) -&gt; TypeGuard[Instance]:
    if not isinstance(fullnames, tuple):
        fullnames = (fullnames,)

    t = get_proper_type(t)
    return isinstance(t, Instance) and t.type.fullname in fullnames


</t>
<t tx="ekr.20221004064035.2964">class InstantiateAliasVisitor(TrivialSyntheticTypeTranslator):
    @others
</t>
<t tx="ekr.20221004064035.2965">def __init__(self, vars: list[str], subs: list[Type]) -&gt; None:
    self.replacements = {v: s for (v, s) in zip(vars, subs)}

</t>
<t tx="ekr.20221004064035.2966">def visit_type_alias_type(self, typ: TypeAliasType) -&gt; Type:
    return typ.copy_modified(args=[t.accept(self) for t in typ.args])

</t>
<t tx="ekr.20221004064035.2967">def visit_unbound_type(self, typ: UnboundType) -&gt; Type:
    # TODO: stop using unbound type variables for type aliases.
    # Now that type aliases are very similar to TypeInfos we should
    # make type variable tracking similar as well. Maybe we can even support
    # upper bounds etc. for generic type aliases.
    if typ.name in self.replacements:
        return self.replacements[typ.name]
    return typ

</t>
<t tx="ekr.20221004064035.2968">def visit_type_var(self, typ: TypeVarType) -&gt; Type:
    if typ.name in self.replacements:
        return self.replacements[typ.name]
    return typ


</t>
<t tx="ekr.20221004064035.2969">class LocationSetter(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.297">def yield_from_invalid_operand_type(self, expr: Type, context: Context) -&gt; Type:
    text = format_type(expr) if format_type(expr) != "object" else expr
    self.fail(f'"yield from" can\'t be applied to {text}', context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20221004064035.2970"># TODO: Should we update locations of other Type subclasses?
def __init__(self, line: int, column: int) -&gt; None:
    self.line = line
    self.column = column

</t>
<t tx="ekr.20221004064035.2971">def visit_instance(self, typ: Instance) -&gt; None:
    typ.line = self.line
    typ.column = self.column
    super().visit_instance(typ)


</t>
<t tx="ekr.20221004064035.2972">def replace_alias_tvars(
    tp: Type, vars: list[str], subs: list[Type], newline: int, newcolumn: int
) -&gt; Type:
    """Replace type variables in a generic type alias tp with substitutions subs
    resetting context. Length of subs should be already checked.
    """
    replacer = InstantiateAliasVisitor(vars, subs)
    new_tp = tp.accept(replacer)
    new_tp.accept(LocationSetter(newline, newcolumn))
    new_tp.line = newline
    new_tp.column = newcolumn
    return new_tp


</t>
<t tx="ekr.20221004064035.2973">class HasTypeVars(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_type_var(self, t: TypeVarType) -&gt; bool:
        return True


</t>
<t tx="ekr.20221004064035.2974">def has_type_vars(typ: Type) -&gt; bool:
    """Check if a type contains any type variables (recursively)."""
    return typ.accept(HasTypeVars())


</t>
<t tx="ekr.20221004064035.2975">class HasRecursiveType(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_type_alias_type(self, t: TypeAliasType) -&gt; bool:
        return t.is_recursive


</t>
<t tx="ekr.20221004064035.2976">def has_recursive_types(typ: Type) -&gt; bool:
    """Check if a type contains any recursive aliases (recursively)."""
    return typ.accept(HasRecursiveType())


</t>
<t tx="ekr.20221004064035.2977">def flatten_nested_unions(
    types: Iterable[Type], handle_type_alias_type: bool = True
) -&gt; list[Type]:
    """Flatten nested unions in a type list."""
    flat_items: list[Type] = []
    # TODO: avoid duplicate types in unions (e.g. using hash)
    for t in types:
        tp = get_proper_type(t) if handle_type_alias_type else t
        if isinstance(tp, ProperType) and isinstance(tp, UnionType):
            flat_items.extend(
                flatten_nested_unions(tp.items, handle_type_alias_type=handle_type_alias_type)
            )
        else:
            # Must preserve original aliases when possible.
            flat_items.append(t)
    return flat_items


</t>
<t tx="ekr.20221004064035.2978">def invalid_recursive_alias(seen_nodes: set[mypy.nodes.TypeAlias], target: Type) -&gt; bool:
    """Flag aliases like A = Union[int, A] (and similar mutual aliases).

    Such aliases don't make much sense, and cause problems in later phases.
    """
    if isinstance(target, TypeAliasType):
        if target.alias in seen_nodes:
            return True
        assert target.alias, f"Unfixed type alias {target.type_ref}"
        return invalid_recursive_alias(seen_nodes | {target.alias}, get_proper_type(target))
    assert isinstance(target, ProperType)
    if not isinstance(target, UnionType):
        return False
    return any(invalid_recursive_alias(seen_nodes, item) for item in target.items)


</t>
<t tx="ekr.20221004064035.2979">def bad_type_type_item(item: Type) -&gt; bool:
    """Prohibit types like Type[Type[...]].

    Such types are explicitly prohibited by PEP 484. Also they cause problems
    with recursive types like T = Type[T], because internal representation of
    TypeType item is normalized (i.e. always a proper type).
    """
    item = get_proper_type(item)
    if isinstance(item, TypeType):
        return True
    if isinstance(item, UnionType):
        return any(
            isinstance(get_proper_type(i), TypeType) for i in flatten_nested_unions(item.items)
        )
    return False


</t>
<t tx="ekr.20221004064035.298">def invalid_signature(self, func_type: Type, context: Context) -&gt; None:
    self.fail(f"Invalid signature {format_type(func_type)}", context)

</t>
<t tx="ekr.20221004064035.2980">def is_union_with_any(tp: Type) -&gt; bool:
    """Is this a union with Any or a plain Any type?"""
    tp = get_proper_type(tp)
    if isinstance(tp, AnyType):
        return True
    if not isinstance(tp, UnionType):
        return False
    return any(is_union_with_any(t) for t in get_proper_types(tp.items))


</t>
<t tx="ekr.20221004064035.2981">def is_generic_instance(tp: Type) -&gt; bool:
    tp = get_proper_type(tp)
    return isinstance(tp, Instance) and bool(tp.args)


</t>
<t tx="ekr.20221004064035.2982">def is_optional(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, UnionType) and any(
        isinstance(get_proper_type(e), NoneType) for e in t.items
    )


</t>
<t tx="ekr.20221004064035.2983">def remove_optional(typ: Type) -&gt; Type:
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        return UnionType.make_union(
            [t for t in typ.items if not isinstance(get_proper_type(t), NoneType)]
        )
    else:
        return typ


</t>
<t tx="ekr.20221004064035.2984">def is_literal_type(typ: ProperType, fallback_fullname: str, value: LiteralValue) -&gt; bool:
    """Check if this type is a LiteralType with the given fallback type and value."""
    if isinstance(typ, Instance) and typ.last_known_value:
        typ = typ.last_known_value
    if not isinstance(typ, LiteralType):
        return False
    if typ.fallback.type.fullname != fallback_fullname:
        return False
    return typ.value == value


</t>
<t tx="ekr.20221004064035.2985">def is_self_type_like(typ: Type, *, is_classmethod: bool) -&gt; bool:
    """Does this look like a self-type annotation?"""
    typ = get_proper_type(typ)
    if not is_classmethod:
        return isinstance(typ, TypeVarType)
    if not isinstance(typ, TypeType):
        return False
    return isinstance(typ.item, TypeVarType)


</t>
<t tx="ekr.20221004064035.2986">names: Final = globals().copy()
names.pop("NOT_READY", None)
deserialize_map: Final = {
    key: obj.deserialize
    for key, obj in names.items()
    if isinstance(obj, type) and issubclass(obj, Type) and obj is not Type
}


</t>
<t tx="ekr.20221004064035.2987">def callable_with_ellipsis(any_type: AnyType, ret_type: Type, fallback: Instance) -&gt; CallableType:
    """Construct type Callable[..., ret_type]."""
    return CallableType(
        [any_type, any_type],
        [ARG_STAR, ARG_STAR2],
        [None, None],
        ret_type=ret_type,
        fallback=fallback,
        is_ellipsis_args=True,
    )
</t>
<t tx="ekr.20221004064035.2988">@path C:/Repos/ekr-mypy2/mypy/
"""
A shared state for all TypeInfos that holds global cache and dependency information,
and potentially other mutable TypeInfo state. This module contains mutable global state.
"""

from __future__ import annotations

from typing import ClassVar, Dict, Set, Tuple
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy.nodes import TypeInfo
from mypy.server.trigger import make_trigger
from mypy.types import Instance, Type, get_proper_type

# Represents that the 'left' instance is a subtype of the 'right' instance
SubtypeRelationship: _TypeAlias = Tuple[Instance, Instance]

# A tuple encoding the specific conditions under which we performed the subtype check.
# (e.g. did we want a proper subtype? A regular subtype while ignoring variance?)
SubtypeKind: _TypeAlias = Tuple[bool, ...]

# A cache that keeps track of whether the given TypeInfo is a part of a particular
# subtype relationship
SubtypeCache: _TypeAlias = Dict[TypeInfo, Dict[SubtypeKind, Set[SubtypeRelationship]]]


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.2989">class TypeState:
    """This class provides subtype caching to improve performance of subtype checks.
    It also holds protocol fine grained dependencies.

    Note: to avoid leaking global state, 'reset_all_subtype_caches()' should be called
    after a build has finished and after a daemon shutdown. This subtype cache only exists for
    performance reasons, resetting subtype caches for a class has no semantic effect.
    The protocol dependencies however are only stored here, and shouldn't be deleted unless
    not needed any more (e.g. during daemon shutdown).
    """

    # '_subtype_caches' keeps track of (subtype, supertype) pairs where supertypes are
    # instances of the given TypeInfo. The cache also keeps track of whether the check
    # was done in strict optional mode and of the specific *kind* of subtyping relationship,
    # which we represent as an arbitrary hashable tuple.
    # We need the caches, since subtype checks for structural types are very slow.
    _subtype_caches: Final[SubtypeCache] = {}

    # This contains protocol dependencies generated after running a full build,
    # or after an update. These dependencies are special because:
    #   * They are a global property of the program; i.e. some dependencies for imported
    #     classes can be generated in the importing modules.
    #   * Because of the above, they are serialized separately, after a full run,
    #     or a full update.
    # `proto_deps` can be None if after deserialization it turns out that they are
    # inconsistent with the other cache files (or an error occurred during deserialization).
    # A blocking error will be generated in this case, since we can't proceed safely.
    # For the description of kinds of protocol dependencies and corresponding examples,
    # see _snapshot_protocol_deps.
    proto_deps: ClassVar[dict[str, set[str]] | None] = {}

    # Protocols (full names) a given class attempted to implement.
    # Used to calculate fine grained protocol dependencies and optimize protocol
    # subtype cache invalidation in fine grained mode. For example, if we pass a value
    # of type a.A to a function expecting something compatible with protocol p.P,
    # we'd have 'a.A' -&gt; {'p.P', ...} in the map. This map is flushed after every incremental
    # update.
    _attempted_protocols: Final[dict[str, set[str]]] = {}
    # We also snapshot protocol members of the above protocols. For example, if we pass
    # a value of type a.A to a function expecting something compatible with Iterable, we'd have
    # 'a.A' -&gt; {'__iter__', ...} in the map. This map is also flushed after every incremental
    # update. This map is needed to only generate dependencies like &lt;a.A.__iter__&gt; -&gt; &lt;a.A&gt;
    # instead of a wildcard to avoid unnecessarily invalidating classes.
    _checked_against_members: Final[dict[str, set[str]]] = {}
    # TypeInfos that appeared as a left type (subtype) in a subtype check since latest
    # dependency snapshot update. This is an optimisation for fine grained mode; during a full
    # run we only take a dependency snapshot at the very end, so this set will contain all
    # subtype-checked TypeInfos. After a fine grained update however, we can gather only new
    # dependencies generated from (typically) few TypeInfos that were subtype-checked
    # (i.e. appeared as r.h.s. in an assignment or an argument in a function call in
    # a re-checked target) during the update.
    _rechecked_types: Final[set[TypeInfo]] = set()

    # The two attributes below are assumption stacks for subtyping relationships between
    # recursive type aliases. Normally, one would pass type assumptions as an additional
    # arguments to is_subtype(), but this would mean updating dozens of related functions
    # threading this through all callsites (see also comment for TypeInfo.assuming).
    _assuming: Final[list[tuple[Type, Type]]] = []
    _assuming_proper: Final[list[tuple[Type, Type]]] = []
    # Ditto for inference of generic constraints against recursive type aliases.
    inferring: Final[list[tuple[Type, Type]]] = []
    # Whether to use joins or unions when solving constraints, see checkexpr.py for details.
    infer_unions: ClassVar = False

    # N.B: We do all of the accesses to these properties through
    # TypeState, instead of making these classmethods and accessing
    # via the cls parameter, since mypyc can optimize accesses to
    # Final attributes of a directly referenced type.

    @others
</t>
<t tx="ekr.20221004064035.299">def invalid_signature_for_special_method(
    self, func_type: Type, context: Context, method_name: str
) -&gt; None:
    self.fail(f'Invalid signature {format_type(func_type)} for "{method_name}"', context)

</t>
<t tx="ekr.20221004064035.2990">@staticmethod
def is_assumed_subtype(left: Type, right: Type) -&gt; bool:
    for (l, r) in reversed(TypeState._assuming):
        if get_proper_type(l) == get_proper_type(left) and get_proper_type(
            r
        ) == get_proper_type(right):
            return True
    return False

</t>
<t tx="ekr.20221004064035.2991">@staticmethod
def is_assumed_proper_subtype(left: Type, right: Type) -&gt; bool:
    for (l, r) in reversed(TypeState._assuming_proper):
        if get_proper_type(l) == get_proper_type(left) and get_proper_type(
            r
        ) == get_proper_type(right):
            return True
    return False

</t>
<t tx="ekr.20221004064035.2992">@staticmethod
def get_assumptions(is_proper: bool) -&gt; list[tuple[Type, Type]]:
    if is_proper:
        return TypeState._assuming_proper
    return TypeState._assuming

</t>
<t tx="ekr.20221004064035.2993">@staticmethod
def reset_all_subtype_caches() -&gt; None:
    """Completely reset all known subtype caches."""
    TypeState._subtype_caches.clear()

</t>
<t tx="ekr.20221004064035.2994">@staticmethod
def reset_subtype_caches_for(info: TypeInfo) -&gt; None:
    """Reset subtype caches (if any) for a given supertype TypeInfo."""
    if info in TypeState._subtype_caches:
        TypeState._subtype_caches[info].clear()

</t>
<t tx="ekr.20221004064035.2995">@staticmethod
def reset_all_subtype_caches_for(info: TypeInfo) -&gt; None:
    """Reset subtype caches (if any) for a given supertype TypeInfo and its MRO."""
    for item in info.mro:
        TypeState.reset_subtype_caches_for(item)

</t>
<t tx="ekr.20221004064035.2996">@staticmethod
def is_cached_subtype_check(kind: SubtypeKind, left: Instance, right: Instance) -&gt; bool:
    if left.last_known_value is not None or right.last_known_value is not None:
        # If there is a literal last known value, give up. There
        # will be an unbounded number of potential types to cache,
        # making caching less effective.
        return False
    info = right.type
    cache = TypeState._subtype_caches.get(info)
    if cache is None:
        return False
    subcache = cache.get(kind)
    if subcache is None:
        return False
    return (left, right) in subcache

</t>
<t tx="ekr.20221004064035.2997">@staticmethod
def record_subtype_cache_entry(kind: SubtypeKind, left: Instance, right: Instance) -&gt; None:
    if left.last_known_value is not None or right.last_known_value is not None:
        # These are unlikely to match, due to the large space of
        # possible values.  Avoid uselessly increasing cache sizes.
        return
    cache = TypeState._subtype_caches.setdefault(right.type, dict())
    cache.setdefault(kind, set()).add((left, right))

</t>
<t tx="ekr.20221004064035.2998">@staticmethod
def reset_protocol_deps() -&gt; None:
    """Reset dependencies after a full run or before a daemon shutdown."""
    TypeState.proto_deps = {}
    TypeState._attempted_protocols.clear()
    TypeState._checked_against_members.clear()
    TypeState._rechecked_types.clear()

</t>
<t tx="ekr.20221004064035.2999">@staticmethod
def record_protocol_subtype_check(left_type: TypeInfo, right_type: TypeInfo) -&gt; None:
    assert right_type.is_protocol
    TypeState._rechecked_types.add(left_type)
    TypeState._attempted_protocols.setdefault(left_type.fullname, set()).add(
        right_type.fullname
    )
    TypeState._checked_against_members.setdefault(left_type.fullname, set()).update(
        right_type.protocol_members
    )

</t>
<t tx="ekr.20221004064035.3">def __init__(self, name: str, timeout: float | None) -&gt; None:
    self.name = name
    self.timeout = timeout

</t>
<t tx="ekr.20221004064035.30">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    if state.strict_optional:
        if isinstance(self.s, (NoneType, UninhabitedType)):
            return t
        elif isinstance(self.s, UnboundType):
            return AnyType(TypeOfAny.special_form)
        else:
            return mypy.typeops.make_simplified_union([self.s, t])
    else:
        return self.s

</t>
<t tx="ekr.20221004064035.300">def reveal_type(self, typ: Type, context: Context) -&gt; None:
    self.note(f'Revealed type is "{typ}"', context)

</t>
<t tx="ekr.20221004064035.3000">@staticmethod
def _snapshot_protocol_deps() -&gt; dict[str, set[str]]:
    """Collect protocol attribute dependencies found so far from registered subtype checks.

    There are three kinds of protocol dependencies. For example, after a subtype check:

        x: Proto = C()

    the following dependencies will be generated:
        1. ..., &lt;SuperProto[wildcard]&gt;, &lt;Proto[wildcard]&gt; -&gt; &lt;Proto&gt;
        2. ..., &lt;B.attr&gt;, &lt;C.attr&gt; -&gt; &lt;C&gt; [for every attr in Proto members]
        3. &lt;C&gt; -&gt; Proto  # this one to invalidate the subtype cache

    The first kind is generated immediately per-module in deps.py (see also an example there
    for motivation why it is needed). While two other kinds are generated here after all
    modules are type checked and we have recorded all the subtype checks. To understand these
    two kinds, consider a simple example:

        class A:
            def __iter__(self) -&gt; Iterator[int]:
                ...

        it: Iterable[int] = A()

    We add &lt;a.A.__iter__&gt; -&gt; &lt;a.A&gt; to invalidate the assignment (module target in this case),
    whenever the signature of a.A.__iter__ changes. We also add &lt;a.A&gt; -&gt; typing.Iterable,
    to invalidate the subtype caches of the latter. (Note that the same logic applies to
    proper subtype checks, and calculating meets and joins, if this involves calling
    'subtypes.is_protocol_implementation').
    """
    deps: dict[str, set[str]] = {}
    for info in TypeState._rechecked_types:
        for attr in TypeState._checked_against_members[info.fullname]:
            # The need for full MRO here is subtle, during an update, base classes of
            # a concrete class may not be reprocessed, so not all &lt;B.x&gt; -&gt; &lt;C.x&gt; deps
            # are added.
            for base_info in info.mro[:-1]:
                trigger = make_trigger(f"{base_info.fullname}.{attr}")
                if "typing" in trigger or "builtins" in trigger:
                    # TODO: avoid everything from typeshed
                    continue
                deps.setdefault(trigger, set()).add(make_trigger(info.fullname))
        for proto in TypeState._attempted_protocols[info.fullname]:
            trigger = make_trigger(info.fullname)
            if "typing" in trigger or "builtins" in trigger:
                continue
            # If any class that was checked against a protocol changes,
            # we need to reset the subtype cache for the protocol.
            #
            # Note: strictly speaking, the protocol doesn't need to be
            # re-checked, we only need to reset the cache, and its uses
            # elsewhere are still valid (unless invalidated by other deps).
            deps.setdefault(trigger, set()).add(proto)
    return deps

</t>
<t tx="ekr.20221004064035.3001">@staticmethod
def update_protocol_deps(second_map: dict[str, set[str]] | None = None) -&gt; None:
    """Update global protocol dependency map.

    We update the global map incrementally, using a snapshot only from recently
    type checked types. If second_map is given, update it as well. This is currently used
    by FineGrainedBuildManager that maintains normal (non-protocol) dependencies.
    """
    assert (
        TypeState.proto_deps is not None
    ), "This should not be called after failed cache load"
    new_deps = TypeState._snapshot_protocol_deps()
    for trigger, targets in new_deps.items():
        TypeState.proto_deps.setdefault(trigger, set()).update(targets)
    if second_map is not None:
        for trigger, targets in new_deps.items():
            second_map.setdefault(trigger, set()).update(targets)
    TypeState._rechecked_types.clear()
    TypeState._attempted_protocols.clear()
    TypeState._checked_against_members.clear()

</t>
<t tx="ekr.20221004064035.3002">@staticmethod
def add_all_protocol_deps(deps: dict[str, set[str]]) -&gt; None:
    """Add all known protocol dependencies to deps.

    This is used by tests and debug output, and also when collecting
    all collected or loaded dependencies as part of build.
    """
    TypeState.update_protocol_deps()  # just in case
    if TypeState.proto_deps is not None:
        for trigger, targets in TypeState.proto_deps.items():
            deps.setdefault(trigger, set()).update(targets)


</t>
<t tx="ekr.20221004064035.3003">def reset_global_state() -&gt; None:
    """Reset most existing global state.

    Currently most of it is in this module. Few exceptions are strict optional status and
    and functools.lru_cache.
    """
    TypeState.reset_all_subtype_caches()
    TypeState.reset_protocol_deps()
</t>
<t tx="ekr.20221004064035.3004">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Iterable

from mypy_extensions import trait

from mypy.types import (
    AnyType,
    CallableArgument,
    CallableType,
    DeletedType,
    EllipsisType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    PlaceholderType,
    RawExpressionType,
    StarType,
    SyntheticTypeVisitor,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.3005">@trait
class TypeTraverserVisitor(SyntheticTypeVisitor[None]):
    """Visitor that traverses all components of a type"""

    # Atomic types

    @others
</t>
<t tx="ekr.20221004064035.3006">def visit_any(self, t: AnyType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3007">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3008">def visit_none_type(self, t: NoneType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3009">def visit_erased_type(self, t: ErasedType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.301">def reveal_locals(self, type_map: dict[str, Type | None], context: Context) -&gt; None:
    # To ensure that the output is predictable on Python &lt; 3.6,
    # use an ordered dictionary sorted by variable name
    sorted_locals = dict(sorted(type_map.items(), key=lambda t: t[0]))
    if sorted_locals:
        self.note("Revealed local types are:", context)
        for k, v in sorted_locals.items():
            self.note(f"    {k}: {v}", context)
    else:
        self.note("There are no locals to reveal", context)

</t>
<t tx="ekr.20221004064035.3010">def visit_deleted_type(self, t: DeletedType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3011">def visit_type_var(self, t: TypeVarType) -&gt; None:
    # Note that type variable values and upper bound aren't treated as
    # components, since they are components of the type variable
    # definition. We want to traverse everything just once.
    pass

</t>
<t tx="ekr.20221004064035.3012">def visit_param_spec(self, t: ParamSpecType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3013">def visit_parameters(self, t: Parameters) -&gt; None:
    self.traverse_types(t.arg_types)

</t>
<t tx="ekr.20221004064035.3014">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3015">def visit_literal_type(self, t: LiteralType) -&gt; None:
    t.fallback.accept(self)

</t>
<t tx="ekr.20221004064035.3016"># Composite types

</t>
<t tx="ekr.20221004064035.3017">def visit_instance(self, t: Instance) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20221004064035.3018">def visit_callable_type(self, t: CallableType) -&gt; None:
    # FIX generics
    self.traverse_types(t.arg_types)
    t.ret_type.accept(self)
    t.fallback.accept(self)

</t>
<t tx="ekr.20221004064035.3019">def visit_tuple_type(self, t: TupleType) -&gt; None:
    self.traverse_types(t.items)
    t.partial_fallback.accept(self)

</t>
<t tx="ekr.20221004064035.302">def unsupported_type_type(self, item: Type, context: Context) -&gt; None:
    self.fail(f'Cannot instantiate type "Type[{format_type_bare(item)}]"', context)

</t>
<t tx="ekr.20221004064035.3020">def visit_typeddict_type(self, t: TypedDictType) -&gt; None:
    self.traverse_types(t.items.values())
    t.fallback.accept(self)

</t>
<t tx="ekr.20221004064035.3021">def visit_union_type(self, t: UnionType) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20221004064035.3022">def visit_overloaded(self, t: Overloaded) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20221004064035.3023">def visit_type_type(self, t: TypeType) -&gt; None:
    t.item.accept(self)

</t>
<t tx="ekr.20221004064035.3024"># Special types (not real types)

</t>
<t tx="ekr.20221004064035.3025">def visit_callable_argument(self, t: CallableArgument) -&gt; None:
    t.typ.accept(self)

</t>
<t tx="ekr.20221004064035.3026">def visit_unbound_type(self, t: UnboundType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20221004064035.3027">def visit_type_list(self, t: TypeList) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20221004064035.3028">def visit_star_type(self, t: StarType) -&gt; None:
    t.type.accept(self)

</t>
<t tx="ekr.20221004064035.3029">def visit_ellipsis_type(self, t: EllipsisType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.303">def redundant_cast(self, typ: Type, context: Context) -&gt; None:
    self.fail(f"Redundant cast to {format_type(typ)}", context, code=codes.REDUNDANT_CAST)

</t>
<t tx="ekr.20221004064035.3030">def visit_placeholder_type(self, t: PlaceholderType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20221004064035.3031">def visit_partial_type(self, t: PartialType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3032">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.3033">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20221004064035.3034">def visit_unpack_type(self, t: UnpackType) -&gt; None:
    t.type.accept(self)

</t>
<t tx="ekr.20221004064035.3035"># Helpers

</t>
<t tx="ekr.20221004064035.3036">def traverse_types(self, types: Iterable[Type]) -&gt; None:
    for typ in types:
        typ.accept(self)
</t>
<t tx="ekr.20221004064035.3037">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from mypy.erasetype import erase_typevars
from mypy.nodes import TypeInfo
from mypy.types import (
    AnyType,
    Instance,
    ParamSpecType,
    TupleType,
    Type,
    TypeOfAny,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnpackType,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.3038">def fill_typevars(typ: TypeInfo) -&gt; Instance | TupleType:
    """For a non-generic type, return instance type representing the type.

    For a generic G type with parameters T1, .., Tn, return G[T1, ..., Tn].
    """
    tvs: list[Type] = []
    # TODO: why do we need to keep both typ.type_vars and typ.defn.type_vars?
    for i in range(len(typ.defn.type_vars)):
        tv: TypeVarLikeType | UnpackType = typ.defn.type_vars[i]
        # Change the line number
        if isinstance(tv, TypeVarType):
            tv = TypeVarType(
                tv.name,
                tv.fullname,
                tv.id,
                tv.values,
                tv.upper_bound,
                tv.variance,
                line=-1,
                column=-1,
            )
        elif isinstance(tv, TypeVarTupleType):
            tv = UnpackType(
                TypeVarTupleType(tv.name, tv.fullname, tv.id, tv.upper_bound, line=-1, column=-1)
            )
        else:
            assert isinstance(tv, ParamSpecType)
            tv = ParamSpecType(
                tv.name, tv.fullname, tv.id, tv.flavor, tv.upper_bound, line=-1, column=-1
            )
        tvs.append(tv)
    inst = Instance(typ, tvs)
    if typ.tuple_type is None:
        return inst
    return typ.tuple_type.copy_modified(fallback=inst)


</t>
<t tx="ekr.20221004064035.3039">def fill_typevars_with_any(typ: TypeInfo) -&gt; Instance | TupleType:
    """Apply a correct number of Any's as type arguments to a type."""
    inst = Instance(typ, [AnyType(TypeOfAny.special_form)] * len(typ.defn.type_vars))
    if typ.tuple_type is None:
        return inst
    return typ.tuple_type.copy_modified(fallback=inst)


</t>
<t tx="ekr.20221004064035.304">def assert_type_fail(self, source_type: Type, target_type: Type, context: Context) -&gt; None:
    self.fail(
        f"Expression is of type {format_type(source_type)}, "
        f"not {format_type(target_type)}",
        context,
        code=codes.ASSERT_TYPE,
    )

</t>
<t tx="ekr.20221004064035.3040">def has_no_typevars(typ: Type) -&gt; bool:
    # We test if a type contains type variables by erasing all type variables
    # and comparing the result to the original type. We use comparison by equality that
    # in turn uses `__eq__` defined for types. Note: we can't use `is_same_type` because
    # it is not safe with unresolved forward references, while this function may be called
    # before forward references resolution patch pass. Note also that it is not safe to use
    # `is` comparison because `erase_typevars` doesn't preserve type identity.
    return typ == erase_typevars(typ)
</t>
<t tx="ekr.20221004064035.3041">@path C:/Repos/ekr-mypy2/mypy/
"""Helpers for interacting with type var tuples."""

from __future__ import annotations

from typing import Sequence, TypeVar

from mypy.types import Instance, ProperType, Type, UnpackType, get_proper_type


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.3042">def find_unpack_in_list(items: Sequence[Type]) -&gt; int | None:
    unpack_index: int | None = None
    for i, item in enumerate(items):
        proper_item = get_proper_type(item)
        if isinstance(proper_item, UnpackType):
            # We cannot fail here, so we must check this in an earlier
            # semanal phase.
            # Funky code here avoids mypyc narrowing the type of unpack_index.
            old_index = unpack_index
            assert old_index is None
            # Don't return so that we can also sanity check there is only one.
            unpack_index = i
    return unpack_index


</t>
<t tx="ekr.20221004064035.3043">T = TypeVar("T")


</t>
<t tx="ekr.20221004064035.3044">def split_with_prefix_and_suffix(
    types: tuple[T, ...], prefix: int, suffix: int
) -&gt; tuple[tuple[T, ...], tuple[T, ...], tuple[T, ...]]:
    if suffix:
        return (types[:prefix], types[prefix:-suffix], types[-suffix:])
    else:
        return (types[:prefix], types[prefix:], ())


</t>
<t tx="ekr.20221004064035.3045">def split_with_instance(
    typ: Instance,
) -&gt; tuple[tuple[Type, ...], tuple[Type, ...], tuple[Type, ...]]:
    assert typ.type.type_var_tuple_prefix is not None
    assert typ.type.type_var_tuple_suffix is not None
    return split_with_prefix_and_suffix(
        typ.args, typ.type.type_var_tuple_prefix, typ.type.type_var_tuple_suffix
    )


</t>
<t tx="ekr.20221004064035.3046">def split_with_mapped_and_template(
    mapped: Instance, template: Instance
) -&gt; tuple[
    tuple[Type, ...],
    tuple[Type, ...],
    tuple[Type, ...],
    tuple[Type, ...],
    tuple[Type, ...],
    tuple[Type, ...],
]:
    mapped_prefix, mapped_middle, mapped_suffix = split_with_instance(mapped)
    template_prefix, template_middle, template_suffix = split_with_instance(template)

    unpack_prefix = find_unpack_in_list(template_middle)
    assert unpack_prefix is not None
    unpack_suffix = len(template_middle) - unpack_prefix - 1

    (
        mapped_middle_prefix,
        mapped_middle_middle,
        mapped_middle_suffix,
    ) = split_with_prefix_and_suffix(mapped_middle, unpack_prefix, unpack_suffix)
    (
        template_middle_prefix,
        template_middle_middle,
        template_middle_suffix,
    ) = split_with_prefix_and_suffix(template_middle, unpack_prefix, unpack_suffix)

    return (
        mapped_prefix + mapped_middle_prefix,
        mapped_middle_middle,
        mapped_middle_suffix + mapped_suffix,
        template_prefix + template_middle_prefix,
        template_middle_middle,
        template_middle_suffix + template_suffix,
    )


</t>
<t tx="ekr.20221004064035.3047">def extract_unpack(types: Sequence[Type]) -&gt; ProperType | None:
    """Given a list of types, extracts either a single type from an unpack, or returns None."""
    if len(types) == 1:
        proper_type = get_proper_type(types[0])
        if isinstance(proper_type, UnpackType):
            return get_proper_type(proper_type.type)
    return None
</t>
<t tx="ekr.20221004064035.3048">@path C:/Repos/ekr-mypy2/mypy/
"""Type visitor classes.

This module defines the type visitors that are intended to be
subclassed by other code.  They have been separated out into their own
module to ease converting mypy to run under mypyc, since currently
mypyc-extension classes can extend interpreted classes but not the
other way around. Separating them out, then, allows us to compile
types before we can compile everything that uses a TypeVisitor.

The visitors are all re-exported from mypy.types and that is how
other modules refer to them.
"""

from __future__ import annotations

from abc import abstractmethod
from typing import Any, Callable, Generic, Iterable, Sequence, TypeVar, cast

from mypy_extensions import mypyc_attr, trait

from mypy.types import (
    AnyType,
    CallableArgument,
    CallableType,
    DeletedType,
    EllipsisType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecType,
    PartialType,
    PlaceholderType,
    RawExpressionType,
    StarType,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    get_proper_type,
)

T = TypeVar("T")


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.3049">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class TypeVisitor(Generic[T]):
    """Visitor class for types (Type subclasses).

    The parameter T is the return type of the visit methods.
    """

    @others
</t>
<t tx="ekr.20221004064035.305">def unimported_type_becomes_any(self, prefix: str, typ: Type, ctx: Context) -&gt; None:
    self.fail(
        f"{prefix} becomes {format_type(typ)} due to an unfollowed import",
        ctx,
        code=codes.NO_ANY_UNIMPORTED,
    )

</t>
<t tx="ekr.20221004064035.3050">@abstractmethod
def visit_unbound_type(self, t: UnboundType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3051">@abstractmethod
def visit_any(self, t: AnyType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3052">@abstractmethod
def visit_none_type(self, t: NoneType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3053">@abstractmethod
def visit_uninhabited_type(self, t: UninhabitedType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3054">@abstractmethod
def visit_erased_type(self, t: ErasedType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3055">@abstractmethod
def visit_deleted_type(self, t: DeletedType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3056">@abstractmethod
def visit_type_var(self, t: TypeVarType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3057">@abstractmethod
def visit_param_spec(self, t: ParamSpecType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3058">@abstractmethod
def visit_parameters(self, t: Parameters) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3059">@abstractmethod
def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.306">def need_annotation_for_var(
    self, node: SymbolNode, context: Context, python_version: tuple[int, int] | None = None
) -&gt; None:
    hint = ""
    has_variable_annotations = not python_version or python_version &gt;= (3, 6)
    pep604_supported = not python_version or python_version &gt;= (3, 10)
    # type to recommend the user adds
    recommended_type = None
    # Only gives hint if it's a variable declaration and the partial type is a builtin type
    if python_version and isinstance(node, Var) and isinstance(node.type, PartialType):
        type_dec = "&lt;type&gt;"
        if not node.type.type:
            # partial None
            if pep604_supported:
                recommended_type = f"{type_dec} | None"
            else:
                recommended_type = f"Optional[{type_dec}]"
        elif node.type.type.fullname in reverse_builtin_aliases:
            # partial types other than partial None
            alias = reverse_builtin_aliases[node.type.type.fullname]
            alias = alias.split(".")[-1]
            if alias == "Dict":
                type_dec = f"{type_dec}, {type_dec}"
            recommended_type = f"{alias}[{type_dec}]"
    if recommended_type is not None:
        if has_variable_annotations:
            hint = f' (hint: "{node.name}: {recommended_type} = ...")'
        else:
            hint = f' (hint: "{node.name} = ...  # type: {recommended_type}")'

    if has_variable_annotations:
        needed = "annotation"
    else:
        needed = "comment"

    self.fail(
        f'Need type {needed} for "{unmangle(node.name)}"{hint}',
        context,
        code=codes.VAR_ANNOTATED,
    )

</t>
<t tx="ekr.20221004064035.3060">@abstractmethod
def visit_instance(self, t: Instance) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3061">@abstractmethod
def visit_callable_type(self, t: CallableType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3062">@abstractmethod
def visit_overloaded(self, t: Overloaded) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3063">@abstractmethod
def visit_tuple_type(self, t: TupleType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3064">@abstractmethod
def visit_typeddict_type(self, t: TypedDictType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3065">@abstractmethod
def visit_literal_type(self, t: LiteralType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3066">@abstractmethod
def visit_union_type(self, t: UnionType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3067">@abstractmethod
def visit_partial_type(self, t: PartialType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3068">@abstractmethod
def visit_type_type(self, t: TypeType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3069">@abstractmethod
def visit_type_alias_type(self, t: TypeAliasType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.307">def explicit_any(self, ctx: Context) -&gt; None:
    self.fail('Explicit "Any" is not allowed', ctx)

</t>
<t tx="ekr.20221004064035.3070">@abstractmethod
def visit_unpack_type(self, t: UnpackType) -&gt; T:
    pass


</t>
<t tx="ekr.20221004064035.3071">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class SyntheticTypeVisitor(TypeVisitor[T]):
    """A TypeVisitor that also knows how to visit synthetic AST constructs.

    Not just real types."""

    @others
</t>
<t tx="ekr.20221004064035.3072">@abstractmethod
def visit_star_type(self, t: StarType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3073">@abstractmethod
def visit_type_list(self, t: TypeList) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3074">@abstractmethod
def visit_callable_argument(self, t: CallableArgument) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3075">@abstractmethod
def visit_ellipsis_type(self, t: EllipsisType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3076">@abstractmethod
def visit_raw_expression_type(self, t: RawExpressionType) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064035.3077">@abstractmethod
def visit_placeholder_type(self, t: PlaceholderType) -&gt; T:
    pass


</t>
<t tx="ekr.20221004064035.3078">@mypyc_attr(allow_interpreted_subclasses=True)
class TypeTranslator(TypeVisitor[Type]):
    """Identity type transformation.

    Subclass this and override some methods to implement a non-trivial
    transformation.
    """

    @others
</t>
<t tx="ekr.20221004064035.3079">def visit_unbound_type(self, t: UnboundType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.308">def unexpected_typeddict_keys(
    self,
    typ: TypedDictType,
    expected_keys: list[str],
    actual_keys: list[str],
    context: Context,
) -&gt; None:
    actual_set = set(actual_keys)
    expected_set = set(expected_keys)
    if not typ.is_anonymous():
        # Generate simpler messages for some common special cases.
        if actual_set &lt; expected_set:
            # Use list comprehension instead of set operations to preserve order.
            missing = [key for key in expected_keys if key not in actual_set]
            self.fail(
                "Missing {} for TypedDict {}".format(
                    format_key_list(missing, short=True), format_type(typ)
                ),
                context,
                code=codes.TYPEDDICT_ITEM,
            )
            return
        else:
            extra = [key for key in actual_keys if key not in expected_set]
            if extra:
                # If there are both extra and missing keys, only report extra ones for
                # simplicity.
                self.fail(
                    "Extra {} for TypedDict {}".format(
                        format_key_list(extra, short=True), format_type(typ)
                    ),
                    context,
                    code=codes.TYPEDDICT_ITEM,
                )
                return
    found = format_key_list(actual_keys, short=True)
    if not expected_keys:
        self.fail(f"Unexpected TypedDict {found}", context)
        return
    expected = format_key_list(expected_keys)
    if actual_keys and actual_set &lt; expected_set:
        found = f"only {found}"
    self.fail(f"Expected {expected} but found {found}", context, code=codes.TYPEDDICT_ITEM)

</t>
<t tx="ekr.20221004064035.3080">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3081">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3082">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3083">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3084">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3085">def visit_instance(self, t: Instance) -&gt; Type:
    last_known_value: LiteralType | None = None
    if t.last_known_value is not None:
        raw_last_known_value = t.last_known_value.accept(self)
        assert isinstance(raw_last_known_value, LiteralType)  # type: ignore[misc]
        last_known_value = raw_last_known_value
    return Instance(
        typ=t.type,
        args=self.translate_types(t.args),
        line=t.line,
        column=t.column,
        last_known_value=last_known_value,
    )

</t>
<t tx="ekr.20221004064035.3086">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3087">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3088">def visit_parameters(self, t: Parameters) -&gt; Type:
    return t.copy_modified(arg_types=self.translate_types(t.arg_types))

</t>
<t tx="ekr.20221004064035.3089">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.309">def typeddict_key_must_be_string_literal(self, typ: TypedDictType, context: Context) -&gt; None:
    self.fail(
        "TypedDict key must be a string literal; expected one of {}".format(
            format_item_name_list(typ.items.keys())
        ),
        context,
        code=codes.LITERAL_REQ,
    )

</t>
<t tx="ekr.20221004064035.3090">def visit_partial_type(self, t: PartialType) -&gt; Type:
    return t

</t>
<t tx="ekr.20221004064035.3091">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    return UnpackType(t.type.accept(self))

</t>
<t tx="ekr.20221004064035.3092">def visit_callable_type(self, t: CallableType) -&gt; Type:
    return t.copy_modified(
        arg_types=self.translate_types(t.arg_types),
        ret_type=t.ret_type.accept(self),
        variables=self.translate_variables(t.variables),
    )

</t>
<t tx="ekr.20221004064035.3093">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    return TupleType(
        self.translate_types(t.items),
        # TODO: This appears to be unsafe.
        cast(Any, t.partial_fallback.accept(self)),
        t.line,
        t.column,
    )

</t>
<t tx="ekr.20221004064035.3094">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = {item_name: item_type.accept(self) for (item_name, item_type) in t.items.items()}
    return TypedDictType(
        items,
        t.required_keys,
        # TODO: This appears to be unsafe.
        cast(Any, t.fallback.accept(self)),
        t.line,
        t.column,
    )

</t>
<t tx="ekr.20221004064035.3095">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    fallback = t.fallback.accept(self)
    assert isinstance(fallback, Instance)  # type: ignore[misc]
    return LiteralType(value=t.value, fallback=fallback, line=t.line, column=t.column)

</t>
<t tx="ekr.20221004064035.3096">def visit_union_type(self, t: UnionType) -&gt; Type:
    return UnionType(self.translate_types(t.items), t.line, t.column)

</t>
<t tx="ekr.20221004064035.3097">def translate_types(self, types: Iterable[Type]) -&gt; list[Type]:
    return [t.accept(self) for t in types]

</t>
<t tx="ekr.20221004064035.3098">def translate_variables(
    self, variables: Sequence[TypeVarLikeType]
) -&gt; Sequence[TypeVarLikeType]:
    return variables

</t>
<t tx="ekr.20221004064035.3099">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    items: list[CallableType] = []
    for item in t.items:
        new = item.accept(self)
        assert isinstance(new, CallableType)  # type: ignore[misc]
        items.append(new)
    return Overloaded(items=items)

</t>
<t tx="ekr.20221004064035.31">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20221004064035.310">def typeddict_key_not_found(
    self, typ: TypedDictType, item_name: str, context: Context
) -&gt; None:
    if typ.is_anonymous():
        self.fail(
            '"{}" is not a valid TypedDict key; expected one of {}'.format(
                item_name, format_item_name_list(typ.items.keys())
            ),
            context,
        )
    else:
        self.fail(
            f'TypedDict {format_type(typ)} has no key "{item_name}"',
            context,
            code=codes.TYPEDDICT_ITEM,
        )
        matches = best_matches(item_name, typ.items.keys())
        if matches:
            self.note(
                "Did you mean {}?".format(pretty_seq(matches[:3], "or")),
                context,
                code=codes.TYPEDDICT_ITEM,
            )

</t>
<t tx="ekr.20221004064035.3100">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(t.item.accept(self), line=t.line, column=t.column)

</t>
<t tx="ekr.20221004064035.3101">@abstractmethod
def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # This method doesn't have a default implementation for type translators,
    # because type aliases are special: some information is contained in the
    # TypeAlias node, and we normally don't generate new nodes. Every subclass
    # must implement this depending on its semantics.
    pass


</t>
<t tx="ekr.20221004064035.3102">@mypyc_attr(allow_interpreted_subclasses=True)
class TypeQuery(SyntheticTypeVisitor[T]):
    """Visitor for performing queries of types.

    strategy is used to combine results for a series of types,
    common use cases involve a boolean query using `any` or `all`.

    Note: this visitor keeps an internal state (tracks type aliases to avoid
    recursion), so it should *never* be re-used for querying different types,
    create a new visitor instance instead.

    # TODO: check that we don't have existing violations of this rule.
    """

    @others
</t>
<t tx="ekr.20221004064035.3103">def __init__(self, strategy: Callable[[Iterable[T]], T]) -&gt; None:
    self.strategy = strategy
    # Keep track of the type aliases already visited. This is needed to avoid
    # infinite recursion on types like A = Union[int, List[A]].
    self.seen_aliases: set[TypeAliasType] = set()
    # By default, we eagerly expand type aliases, and query also types in the
    # alias target. In most cases this is a desired behavior, but we may want
    # to skip targets in some cases (e.g. when collecting type variables).
    self.skip_alias_target = False

</t>
<t tx="ekr.20221004064035.3104">def visit_unbound_type(self, t: UnboundType) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20221004064035.3105">def visit_type_list(self, t: TypeList) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20221004064035.3106">def visit_callable_argument(self, t: CallableArgument) -&gt; T:
    return t.typ.accept(self)

</t>
<t tx="ekr.20221004064035.3107">def visit_any(self, t: AnyType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3108">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3109">def visit_none_type(self, t: NoneType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.311">def typeddict_context_ambiguous(self, types: list[TypedDictType], context: Context) -&gt; None:
    formatted_types = ", ".join(list(format_type_distinctly(*types)))
    self.fail(f"Type of TypedDict is ambiguous, could be any of ({formatted_types})", context)

</t>
<t tx="ekr.20221004064035.3110">def visit_erased_type(self, t: ErasedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3111">def visit_deleted_type(self, t: DeletedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3112">def visit_type_var(self, t: TypeVarType) -&gt; T:
    return self.query_types([t.upper_bound] + t.values)

</t>
<t tx="ekr.20221004064035.3113">def visit_param_spec(self, t: ParamSpecType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3114">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3115">def visit_unpack_type(self, t: UnpackType) -&gt; T:
    return self.query_types([t.type])

</t>
<t tx="ekr.20221004064035.3116">def visit_parameters(self, t: Parameters) -&gt; T:
    return self.query_types(t.arg_types)

</t>
<t tx="ekr.20221004064035.3117">def visit_partial_type(self, t: PartialType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3118">def visit_instance(self, t: Instance) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20221004064035.3119">def visit_callable_type(self, t: CallableType) -&gt; T:
    # FIX generics
    return self.query_types(t.arg_types + [t.ret_type])

</t>
<t tx="ekr.20221004064035.312">def typeddict_key_cannot_be_deleted(
    self, typ: TypedDictType, item_name: str, context: Context
) -&gt; None:
    if typ.is_anonymous():
        self.fail(f'TypedDict key "{item_name}" cannot be deleted', context)
    else:
        self.fail(
            f'Key "{item_name}" of TypedDict {format_type(typ)} cannot be deleted', context
        )

</t>
<t tx="ekr.20221004064035.3120">def visit_tuple_type(self, t: TupleType) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20221004064035.3121">def visit_typeddict_type(self, t: TypedDictType) -&gt; T:
    return self.query_types(t.items.values())

</t>
<t tx="ekr.20221004064035.3122">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.3123">def visit_literal_type(self, t: LiteralType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064035.313">def typeddict_setdefault_arguments_inconsistent(
    self, default: Type, expected: Type, context: Context
) -&gt; None:
    msg = 'Argument 2 to "setdefault" of "TypedDict" has incompatible type {}; expected {}'
    self.fail(
        msg.format(format_type(default), format_type(expected)),
        context,
        code=codes.TYPEDDICT_ITEM,
    )

</t>
<t tx="ekr.20221004064035.314">def type_arguments_not_allowed(self, context: Context) -&gt; None:
    self.fail("Parameterized generics cannot be used with class or instance checks", context)

</t>
<t tx="ekr.20221004064035.315">def disallowed_any_type(self, typ: Type, context: Context) -&gt; None:
    typ = get_proper_type(typ)
    if isinstance(typ, AnyType):
        message = 'Expression has type "Any"'
    else:
        message = f'Expression type contains "Any" (has type {format_type(typ)})'
    self.fail(message, context)

</t>
<t tx="ekr.20221004064035.316">def incorrectly_returning_any(self, typ: Type, context: Context) -&gt; None:
    message = f"Returning Any from function declared to return {format_type(typ)}"
    self.fail(message, context, code=codes.NO_ANY_RETURN)

</t>
<t tx="ekr.20221004064035.317">def incorrect__exit__return(self, context: Context) -&gt; None:
    self.fail(
        '"bool" is invalid as return type for "__exit__" that always returns False',
        context,
        code=codes.EXIT_RETURN,
    )
    self.note(
        'Use "typing_extensions.Literal[False]" as the return type or change it to "None"',
        context,
        code=codes.EXIT_RETURN,
    )
    self.note(
        'If return type of "__exit__" implies that it may return True, '
        "the context manager may swallow exceptions",
        context,
        code=codes.EXIT_RETURN,
    )

</t>
<t tx="ekr.20221004064035.318">def untyped_decorated_function(self, typ: Type, context: Context) -&gt; None:
    typ = get_proper_type(typ)
    if isinstance(typ, AnyType):
        self.fail("Function is untyped after decorator transformation", context)
    else:
        self.fail(
            f'Type of decorated function contains type "Any" ({format_type(typ)})', context
        )

</t>
<t tx="ekr.20221004064035.319">def typed_function_untyped_decorator(self, func_name: str, context: Context) -&gt; None:
    self.fail(f'Untyped decorator makes function "{func_name}" untyped', context)

</t>
<t tx="ekr.20221004064035.32">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20221004064035.320">def bad_proto_variance(
    self, actual: int, tvar_name: str, expected: int, context: Context
) -&gt; None:
    msg = capitalize(
        '{} type variable "{}" used in protocol where'
        " {} one is expected".format(
            variance_string(actual), tvar_name, variance_string(expected)
        )
    )
    self.fail(msg, context)

</t>
<t tx="ekr.20221004064035.321">def concrete_only_assign(self, typ: Type, context: Context) -&gt; None:
    self.fail(
        f"Can only assign concrete classes to a variable of type {format_type(typ)}", context
    )

</t>
<t tx="ekr.20221004064035.322">def concrete_only_call(self, typ: Type, context: Context) -&gt; None:
    self.fail(
        f"Only concrete class can be given where {format_type(typ)} is expected",
        context,
        code=codes.TYPE_ABSTRACT,
    )

</t>
<t tx="ekr.20221004064035.323">def cannot_use_function_with_type(
    self, method_name: str, type_name: str, context: Context
) -&gt; None:
    self.fail(f"Cannot use {method_name}() with {type_name} type", context)

</t>
<t tx="ekr.20221004064035.324">def report_non_method_protocol(
    self, tp: TypeInfo, members: list[str], context: Context
) -&gt; None:
    self.fail(
        "Only protocols that don't have non-method members can be used with issubclass()",
        context,
    )
    if len(members) &lt; 3:
        attrs = ", ".join(members)
        self.note(f'Protocol "{tp.name}" has non-method member(s): {attrs}', context)

</t>
<t tx="ekr.20221004064035.325">def note_call(
    self, subtype: Type, call: Type, context: Context, *, code: ErrorCode | None
) -&gt; None:
    self.note(
        '"{}.__call__" has type {}'.format(
            format_type_bare(subtype), format_type(call, verbosity=1)
        ),
        context,
        code=code,
    )

</t>
<t tx="ekr.20221004064035.326">def unreachable_statement(self, context: Context) -&gt; None:
    self.fail("Statement is unreachable", context, code=codes.UNREACHABLE)

</t>
<t tx="ekr.20221004064035.327">def redundant_left_operand(self, op_name: str, context: Context) -&gt; None:
    """Indicates that the left operand of a boolean expression is redundant:
    it does not change the truth value of the entire condition as a whole.
    'op_name' should either be the string "and" or the string "or".
    """
    self.redundant_expr(f'Left operand of "{op_name}"', op_name == "and", context)

</t>
<t tx="ekr.20221004064035.328">def unreachable_right_operand(self, op_name: str, context: Context) -&gt; None:
    """Indicates that the right operand of a boolean expression is redundant:
    it does not change the truth value of the entire condition as a whole.
    'op_name' should either be the string "and" or the string "or".
    """
    self.fail(
        f'Right operand of "{op_name}" is never evaluated', context, code=codes.UNREACHABLE
    )

</t>
<t tx="ekr.20221004064035.329">def redundant_condition_in_comprehension(self, truthiness: bool, context: Context) -&gt; None:
    self.redundant_expr("If condition in comprehension", truthiness, context)

</t>
<t tx="ekr.20221004064035.33">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20221004064035.330">def redundant_condition_in_if(self, truthiness: bool, context: Context) -&gt; None:
    self.redundant_expr("If condition", truthiness, context)

</t>
<t tx="ekr.20221004064035.331">def redundant_expr(self, description: str, truthiness: bool, context: Context) -&gt; None:
    self.fail(
        f"{description} is always {str(truthiness).lower()}",
        context,
        code=codes.REDUNDANT_EXPR,
    )

</t>
<t tx="ekr.20221004064035.332">def impossible_intersection(
    self, formatted_base_class_list: str, reason: str, context: Context
) -&gt; None:
    template = "Subclass of {} cannot exist: would have {}"
    self.fail(
        template.format(formatted_base_class_list, reason), context, code=codes.UNREACHABLE
    )

</t>
<t tx="ekr.20221004064035.333">def report_protocol_problems(
    self,
    subtype: Instance | TupleType | TypedDictType | TypeType | CallableType,
    supertype: Instance,
    context: Context,
    *,
    code: ErrorCode | None,
) -&gt; None:
    """Report possible protocol conflicts between 'subtype' and 'supertype'.

    This includes missing members, incompatible types, and incompatible
    attribute flags, such as settable vs read-only or class variable vs
    instance variable.
    """
    OFFSET = 4  # Four spaces, so that notes will look like this:
    # note: 'Cls' is missing following 'Proto' members:
    # note:     method, attr
    MAX_ITEMS = 2  # Maximum number of conflicts, missing members, and overloads shown
    # List of special situations where we don't want to report additional problems
    exclusions: dict[type, list[str]] = {
        TypedDictType: ["typing.Mapping"],
        TupleType: ["typing.Iterable", "typing.Sequence"],
    }
    if supertype.type.fullname in exclusions.get(type(subtype), []):
        return
    if any(isinstance(tp, UninhabitedType) for tp in get_proper_types(supertype.args)):
        # We don't want to add notes for failed inference (e.g. Iterable[&lt;nothing&gt;]).
        # This will be only confusing a user even more.
        return

    class_obj = False
    is_module = False
    if isinstance(subtype, TupleType):
        if not isinstance(subtype.partial_fallback, Instance):
            return
        subtype = subtype.partial_fallback
    elif isinstance(subtype, TypedDictType):
        if not isinstance(subtype.fallback, Instance):
            return
        subtype = subtype.fallback
    elif isinstance(subtype, TypeType):
        if not isinstance(subtype.item, Instance):
            return
        class_obj = True
        subtype = subtype.item
    elif isinstance(subtype, CallableType):
        if not subtype.is_type_obj():
            return
        ret_type = get_proper_type(subtype.ret_type)
        if isinstance(ret_type, TupleType):
            ret_type = ret_type.partial_fallback
        if not isinstance(ret_type, Instance):
            return
        class_obj = True
        subtype = ret_type
    if subtype.extra_attrs and subtype.extra_attrs.mod_name:
        is_module = True

    # Report missing members
    missing = get_missing_protocol_members(subtype, supertype)
    if (
        missing
        and len(missing) &lt; len(supertype.type.protocol_members)
        and len(missing) &lt;= MAX_ITEMS
    ):
        if missing == ["__call__"] and class_obj:
            self.note(
                '"{}" has constructor incompatible with "__call__" of "{}"'.format(
                    subtype.type.name, supertype.type.name
                ),
                context,
                code=code,
            )
        else:
            self.note(
                '"{}" is missing following "{}" protocol member{}:'.format(
                    subtype.type.name, supertype.type.name, plural_s(missing)
                ),
                context,
                code=code,
            )
            self.note(", ".join(missing), context, offset=OFFSET, code=code)
    elif len(missing) &gt; MAX_ITEMS or len(missing) == len(supertype.type.protocol_members):
        # This is an obviously wrong type: too many missing members
        return

    # Report member type conflicts
    conflict_types = get_conflict_protocol_types(subtype, supertype, class_obj=class_obj)
    if conflict_types and (
        not is_subtype(subtype, erase_type(supertype))
        or not subtype.type.defn.type_vars
        or not supertype.type.defn.type_vars
    ):
        type_name = format_type(subtype, module_names=True)
        self.note(f"Following member(s) of {type_name} have conflicts:", context, code=code)
        for name, got, exp in conflict_types[:MAX_ITEMS]:
            exp = get_proper_type(exp)
            got = get_proper_type(got)
            if not isinstance(exp, (CallableType, Overloaded)) or not isinstance(
                got, (CallableType, Overloaded)
            ):
                self.note(
                    "{}: expected {}, got {}".format(name, *format_type_distinctly(exp, got)),
                    context,
                    offset=OFFSET,
                    code=code,
                )
            else:
                self.note("Expected:", context, offset=OFFSET, code=code)
                if isinstance(exp, CallableType):
                    self.note(
                        pretty_callable(exp, skip_self=class_obj or is_module),
                        context,
                        offset=2 * OFFSET,
                        code=code,
                    )
                else:
                    assert isinstance(exp, Overloaded)
                    self.pretty_overload(
                        exp, context, 2 * OFFSET, code=code, skip_self=class_obj or is_module
                    )
                self.note("Got:", context, offset=OFFSET, code=code)
                if isinstance(got, CallableType):
                    self.note(
                        pretty_callable(got, skip_self=class_obj or is_module),
                        context,
                        offset=2 * OFFSET,
                        code=code,
                    )
                else:
                    assert isinstance(got, Overloaded)
                    self.pretty_overload(
                        got, context, 2 * OFFSET, code=code, skip_self=class_obj or is_module
                    )
        self.print_more(conflict_types, context, OFFSET, MAX_ITEMS, code=code)

    # Report flag conflicts (i.e. settable vs read-only etc.)
    conflict_flags = get_bad_protocol_flags(subtype, supertype, class_obj=class_obj)
    for name, subflags, superflags in conflict_flags[:MAX_ITEMS]:
        if not class_obj and IS_CLASSVAR in subflags and IS_CLASSVAR not in superflags:
            self.note(
                "Protocol member {}.{} expected instance variable,"
                " got class variable".format(supertype.type.name, name),
                context,
                code=code,
            )
        if not class_obj and IS_CLASSVAR in superflags and IS_CLASSVAR not in subflags:
            self.note(
                "Protocol member {}.{} expected class variable,"
                " got instance variable".format(supertype.type.name, name),
                context,
                code=code,
            )
        if IS_SETTABLE in superflags and IS_SETTABLE not in subflags:
            self.note(
                "Protocol member {}.{} expected settable variable,"
                " got read-only attribute".format(supertype.type.name, name),
                context,
                code=code,
            )
        if IS_CLASS_OR_STATIC in superflags and IS_CLASS_OR_STATIC not in subflags:
            self.note(
                "Protocol member {}.{} expected class or static method".format(
                    supertype.type.name, name
                ),
                context,
                code=code,
            )
        if (
            class_obj
            and IS_VAR in superflags
            and (IS_VAR in subflags and IS_CLASSVAR not in subflags)
        ):
            self.note(
                "Only class variables allowed for class object access on protocols,"
                ' {} is an instance variable of "{}"'.format(name, subtype.type.name),
                context,
                code=code,
            )
        if class_obj and IS_CLASSVAR in superflags:
            self.note(
                "ClassVar protocol member {}.{} can never be matched by a class object".format(
                    supertype.type.name, name
                ),
                context,
                code=code,
            )
    self.print_more(conflict_flags, context, OFFSET, MAX_ITEMS, code=code)

</t>
<t tx="ekr.20221004064035.334">def pretty_overload(
    self,
    tp: Overloaded,
    context: Context,
    offset: int,
    *,
    add_class_or_static_decorator: bool = False,
    allow_dups: bool = False,
    code: ErrorCode | None = None,
    skip_self: bool = False,
) -&gt; None:
    for item in tp.items:
        self.note("@overload", context, offset=offset, allow_dups=allow_dups, code=code)

        if add_class_or_static_decorator:
            decorator = pretty_class_or_static_decorator(item)
            if decorator is not None:
                self.note(decorator, context, offset=offset, allow_dups=allow_dups, code=code)

        self.note(
            pretty_callable(item, skip_self=skip_self),
            context,
            offset=offset,
            allow_dups=allow_dups,
            code=code,
        )

</t>
<t tx="ekr.20221004064035.335">def print_more(
    self,
    conflicts: Sequence[Any],
    context: Context,
    offset: int,
    max_items: int,
    *,
    code: ErrorCode | None = None,
) -&gt; None:
    if len(conflicts) &gt; max_items:
        self.note(
            f"&lt;{len(conflicts) - max_items} more conflict(s) not shown&gt;",
            context,
            offset=offset,
            code=code,
        )

</t>
<t tx="ekr.20221004064035.336">def try_report_long_tuple_assignment_error(
    self,
    subtype: ProperType,
    supertype: ProperType,
    context: Context,
    msg: message_registry.ErrorMessage,
    subtype_label: str | None = None,
    supertype_label: str | None = None,
) -&gt; bool:
    """Try to generate meaningful error message for very long tuple assignment

    Returns a bool: True when generating long tuple assignment error,
    False when no such error reported
    """
    if isinstance(subtype, TupleType):
        if (
            len(subtype.items) &gt; 10
            and isinstance(supertype, Instance)
            and supertype.type.fullname == "builtins.tuple"
        ):
            lhs_type = supertype.args[0]
            lhs_types = [lhs_type] * len(subtype.items)
            self.generate_incompatible_tuple_error(lhs_types, subtype.items, context, msg)
            return True
        elif isinstance(supertype, TupleType) and (
            len(subtype.items) &gt; 10 or len(supertype.items) &gt; 10
        ):
            if len(subtype.items) != len(supertype.items):
                if supertype_label is not None and subtype_label is not None:
                    msg = msg.with_additional_msg(
                        " ({} {}, {} {})".format(
                            subtype_label,
                            self.format_long_tuple_type(subtype),
                            supertype_label,
                            self.format_long_tuple_type(supertype),
                        )
                    )
                    self.fail(msg.value, context, code=msg.code)
                    return True
            self.generate_incompatible_tuple_error(
                supertype.items, subtype.items, context, msg
            )
            return True
    return False

</t>
<t tx="ekr.20221004064035.337">def format_long_tuple_type(self, typ: TupleType) -&gt; str:
    """Format very long tuple type using an ellipsis notation"""
    item_cnt = len(typ.items)
    if item_cnt &gt; 10:
        return "Tuple[{}, {}, ... &lt;{} more items&gt;]".format(
            format_type_bare(typ.items[0]), format_type_bare(typ.items[1]), str(item_cnt - 2)
        )
    else:
        return format_type_bare(typ)

</t>
<t tx="ekr.20221004064035.338">def generate_incompatible_tuple_error(
    self,
    lhs_types: list[Type],
    rhs_types: list[Type],
    context: Context,
    msg: message_registry.ErrorMessage,
) -&gt; None:
    """Generate error message for individual incompatible tuple pairs"""
    error_cnt = 0
    notes = []  # List[str]
    for i, (lhs_t, rhs_t) in enumerate(zip(lhs_types, rhs_types)):
        if not is_subtype(lhs_t, rhs_t):
            if error_cnt &lt; 3:
                notes.append(
                    "Expression tuple item {} has type {}; {} expected; ".format(
                        str(i), format_type(rhs_t), format_type(lhs_t)
                    )
                )
            error_cnt += 1

    info = f" ({str(error_cnt)} tuple items are incompatible"
    if error_cnt - 3 &gt; 0:
        info += f"; {str(error_cnt - 3)} items are omitted)"
    else:
        info += ")"
    msg = msg.with_additional_msg(info)
    self.fail(msg.value, context, code=msg.code)
    for note in notes:
        self.note(note, context, code=msg.code)

</t>
<t tx="ekr.20221004064035.339">def add_fixture_note(self, fullname: str, ctx: Context) -&gt; None:
    self.note(f'Maybe your test fixture does not define "{fullname}"?', ctx)
    if fullname in SUGGESTED_TEST_FIXTURES:
        self.note(
            "Consider adding [builtins fixtures/{}] to your test description".format(
                SUGGESTED_TEST_FIXTURES[fullname]
            ),
            ctx,
        )


</t>
<t tx="ekr.20221004064035.34">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    if isinstance(self.s, TypeVarType) and self.s.id == t.id:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.340">def quote_type_string(type_string: str) -&gt; str:
    """Quotes a type representation for use in messages."""
    no_quote_regex = r"^&lt;(tuple|union): \d+ items&gt;$"
    if (
        type_string in ["Module", "overloaded function", "&lt;nothing&gt;", "&lt;deleted&gt;"]
        or re.match(no_quote_regex, type_string) is not None
        or type_string.endswith("?")
    ):
        # Messages are easier to read if these aren't quoted.  We use a
        # regex to match strings with variable contents.
        return type_string
    return f'"{type_string}"'


</t>
<t tx="ekr.20221004064035.341">def format_callable_args(
    arg_types: list[Type],
    arg_kinds: list[ArgKind],
    arg_names: list[str | None],
    format: Callable[[Type], str],
    verbosity: int,
) -&gt; str:
    """Format a bunch of Callable arguments into a string"""
    arg_strings = []
    for arg_name, arg_type, arg_kind in zip(arg_names, arg_types, arg_kinds):
        if arg_kind == ARG_POS and arg_name is None or verbosity == 0 and arg_kind.is_positional():

            arg_strings.append(format(arg_type))
        else:
            constructor = ARG_CONSTRUCTOR_NAMES[arg_kind]
            if arg_kind.is_star() or arg_name is None:
                arg_strings.append(f"{constructor}({format(arg_type)})")
            else:
                arg_strings.append(f"{constructor}({format(arg_type)}, {repr(arg_name)})")

    return ", ".join(arg_strings)


</t>
<t tx="ekr.20221004064035.342">def format_type_inner(
    typ: Type, verbosity: int, fullnames: set[str] | None, module_names: bool = False
) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    Args:
      verbosity: a coarse grained control on the verbosity of the type
      fullnames: a set of names that should be printed in full
    """

    @others
    if isinstance(typ, TypeAliasType) and typ.is_recursive:
        # TODO: find balance here, str(typ) doesn't support custom verbosity, and may be
        # too verbose for user messages, OTOH it nicely shows structure of recursive types.
        if verbosity &lt; 2:
            type_str = typ.alias.name if typ.alias else "&lt;alias (unfixed)&gt;"
            if typ.args:
                type_str += f"[{format_list(typ.args)}]"
            return type_str
        return str(typ)

    # TODO: always mention type alias names in errors.
    typ = get_proper_type(typ)

    if isinstance(typ, Instance):
        itype = typ
        # Get the short name of the type.
        if itype.type.fullname in ("types.ModuleType", "_importlib_modulespec.ModuleType"):
            # Make some common error messages simpler and tidier.
            base_str = "Module"
            if itype.extra_attrs and itype.extra_attrs.mod_name and module_names:
                return f"{base_str} {itype.extra_attrs.mod_name}"
            return base_str
        if verbosity &gt;= 2 or (fullnames and itype.type.fullname in fullnames):
            base_str = itype.type.fullname
        else:
            base_str = itype.type.name
        if not itype.args:
            # No type arguments, just return the type name
            return base_str
        elif itype.type.fullname == "builtins.tuple":
            item_type_str = format(itype.args[0])
            return f"Tuple[{item_type_str}, ...]"
        elif itype.type.fullname in reverse_builtin_aliases:
            alias = reverse_builtin_aliases[itype.type.fullname]
            alias = alias.split(".")[-1]
            return f"{alias}[{format_list(itype.args)}]"
        else:
            # There are type arguments. Convert the arguments to strings.
            return f"{base_str}[{format_list(itype.args)}]"
    elif isinstance(typ, TypeVarType):
        # This is similar to non-generic instance types.
        return typ.name
    elif isinstance(typ, ParamSpecType):
        # Concatenate[..., P]
        if typ.prefix.arg_types:
            args = format_callable_args(
                typ.prefix.arg_types, typ.prefix.arg_kinds, typ.prefix.arg_names, format, verbosity
            )

            return f"[{args}, **{typ.name_with_suffix()}]"
        else:
            return typ.name_with_suffix()
    elif isinstance(typ, TupleType):
        # Prefer the name of the fallback class (if not tuple), as it's more informative.
        if typ.partial_fallback.type.fullname != "builtins.tuple":
            return format(typ.partial_fallback)
        s = f"Tuple[{format_list(typ.items)}]"
        return s
    elif isinstance(typ, TypedDictType):
        # If the TypedDictType is named, return the name
        if not typ.is_anonymous():
            return format(typ.fallback)
        items = []
        for (item_name, item_type) in typ.items.items():
            modifier = "" if item_name in typ.required_keys else "?"
            items.append(f"{item_name!r}{modifier}: {format(item_type)}")
        s = f"TypedDict({{{', '.join(items)}}})"
        return s
    elif isinstance(typ, LiteralType):
        return f"Literal[{format_literal_value(typ)}]"
    elif isinstance(typ, UnionType):
        literal_items, union_items = separate_union_literals(typ)

        # Coalesce multiple Literal[] members. This also changes output order.
        # If there's just one Literal item, retain the original ordering.
        if len(literal_items) &gt; 1:
            literal_str = "Literal[{}]".format(
                ", ".join(format_literal_value(t) for t in literal_items)
            )

            if len(union_items) == 1 and isinstance(get_proper_type(union_items[0]), NoneType):
                return f"Optional[{literal_str}]"
            elif union_items:
                return f"Union[{format_list(union_items)}, {literal_str}]"
            else:
                return literal_str
        else:
            # Only print Union as Optional if the Optional wouldn't have to contain another Union
            print_as_optional = (
                len(typ.items) - sum(isinstance(get_proper_type(t), NoneType) for t in typ.items)
                == 1
            )
            if print_as_optional:
                rest = [t for t in typ.items if not isinstance(get_proper_type(t), NoneType)]
                return f"Optional[{format(rest[0])}]"
            else:
                s = f"Union[{format_list(typ.items)}]"

            return s
    elif isinstance(typ, NoneType):
        return "None"
    elif isinstance(typ, AnyType):
        return "Any"
    elif isinstance(typ, DeletedType):
        return "&lt;deleted&gt;"
    elif isinstance(typ, UninhabitedType):
        if typ.is_noreturn:
            return "NoReturn"
        else:
            return "&lt;nothing&gt;"
    elif isinstance(typ, TypeType):
        return f"Type[{format(typ.item)}]"
    elif isinstance(typ, FunctionLike):
        func = typ
        if func.is_type_obj():
            # The type of a type object type can be derived from the
            # return type (this always works).
            return format(TypeType.make_normalized(erase_type(func.items[0].ret_type)))
        elif isinstance(func, CallableType):
            if func.type_guard is not None:
                return_type = f"TypeGuard[{format(func.type_guard)}]"
            else:
                return_type = format(func.ret_type)
            if func.is_ellipsis_args:
                return f"Callable[..., {return_type}]"
            param_spec = func.param_spec()
            if param_spec is not None:
                return f"Callable[{format(param_spec)}, {return_type}]"
            args = format_callable_args(
                func.arg_types, func.arg_kinds, func.arg_names, format, verbosity
            )
            return f"Callable[[{args}], {return_type}]"
        else:
            # Use a simple representation for function types; proper
            # function types may result in long and difficult-to-read
            # error messages.
            return "overloaded function"
    elif isinstance(typ, UnboundType):
        return str(typ)
    elif isinstance(typ, Parameters):
        args = format_callable_args(typ.arg_types, typ.arg_kinds, typ.arg_names, format, verbosity)
        return f"[{args}]"
    elif typ is None:
        raise RuntimeError("Type is None")
    else:
        # Default case; we simply have to return something meaningful here.
        return "object"


</t>
<t tx="ekr.20221004064035.343">def format(typ: Type) -&gt; str:
    return format_type_inner(typ, verbosity, fullnames)

</t>
<t tx="ekr.20221004064035.344">def format_list(types: Sequence[Type]) -&gt; str:
    return ", ".join(format(typ) for typ in types)

</t>
<t tx="ekr.20221004064035.345">def format_literal_value(typ: LiteralType) -&gt; str:
    if typ.is_enum_literal():
        underlying_type = format(typ.fallback)
        return f"{underlying_type}.{typ.value}"
    else:
        return typ.value_repr()

</t>
<t tx="ekr.20221004064035.346">def collect_all_instances(t: Type) -&gt; list[Instance]:
    """Return all instances that `t` contains (including `t`).

    This is similar to collect_all_inner_types from typeanal but only
    returns instances and will recurse into fallbacks.
    """
    visitor = CollectAllInstancesQuery()
    t.accept(visitor)
    return visitor.instances


</t>
<t tx="ekr.20221004064035.347">class CollectAllInstancesQuery(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20221004064035.348">def __init__(self) -&gt; None:
    self.instances: list[Instance] = []

</t>
<t tx="ekr.20221004064035.349">def visit_instance(self, t: Instance) -&gt; None:
    self.instances.append(t)
    super().visit_instance(t)

</t>
<t tx="ekr.20221004064035.35">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    if self.s == t:
        return t
    return self.default(self.s)

</t>
<t tx="ekr.20221004064035.350">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    if t.alias and not t.is_recursive:
        t.alias.target.accept(self)
    super().visit_type_alias_type(t)


</t>
<t tx="ekr.20221004064035.351">def find_type_overlaps(*types: Type) -&gt; set[str]:
    """Return a set of fullnames that share a short name and appear in either type.

    This is used to ensure that distinct types with the same short name are printed
    with their fullname.
    """
    d: dict[str, set[str]] = {}
    for type in types:
        for inst in collect_all_instances(type):
            d.setdefault(inst.type.name, set()).add(inst.type.fullname)
    for shortname in d.keys():
        if f"typing.{shortname}" in TYPES_FOR_UNIMPORTED_HINTS:
            d[shortname].add(f"typing.{shortname}")

    overlaps: set[str] = set()
    for fullnames in d.values():
        if len(fullnames) &gt; 1:
            overlaps.update(fullnames)
    return overlaps


</t>
<t tx="ekr.20221004064035.352">def format_type(typ: Type, verbosity: int = 0, module_names: bool = False) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    `verbosity` is a coarse grained control on the verbosity of the type

    This function returns a string appropriate for unmodified use in error
    messages; this means that it will be quoted in most cases.  If
    modification of the formatted string is required, callers should use
    format_type_bare.
    """
    return quote_type_string(format_type_bare(typ, verbosity, module_names))


</t>
<t tx="ekr.20221004064035.353">def format_type_bare(typ: Type, verbosity: int = 0, module_names: bool = False) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    `verbosity` is a coarse grained control on the verbosity of the type
    `fullnames` specifies a set of names that should be printed in full

    This function will return an unquoted string.  If a caller doesn't need to
    perform post-processing on the string output, format_type should be used
    instead.  (The caller may want to use quote_type_string after
    processing has happened, to maintain consistent quoting in messages.)
    """
    return format_type_inner(typ, verbosity, find_type_overlaps(typ), module_names)


</t>
<t tx="ekr.20221004064035.354">def format_type_distinctly(*types: Type, bare: bool = False) -&gt; tuple[str, ...]:
    """Jointly format types to distinct strings.

    Increase the verbosity of the type strings until they become distinct
    while also requiring that distinct types with the same short name are
    formatted distinctly.

    By default, the returned strings are created using format_type() and will be
    quoted accordingly. If ``bare`` is True, the returned strings will not
    be quoted; callers who need to do post-processing of the strings before
    quoting them (such as prepending * or **) should use this.
    """
    overlapping = find_type_overlaps(*types)
    for verbosity in range(2):
        strs = [
            format_type_inner(type, verbosity=verbosity, fullnames=overlapping) for type in types
        ]
        if len(set(strs)) == len(strs):
            break
    if bare:
        return tuple(strs)
    else:
        return tuple(quote_type_string(s) for s in strs)


</t>
<t tx="ekr.20221004064035.355">def pretty_class_or_static_decorator(tp: CallableType) -&gt; str | None:
    """Return @classmethod or @staticmethod, if any, for the given callable type."""
    if tp.definition is not None and isinstance(tp.definition, SYMBOL_FUNCBASE_TYPES):
        if tp.definition.is_class:
            return "@classmethod"
        if tp.definition.is_static:
            return "@staticmethod"
    return None


</t>
<t tx="ekr.20221004064035.356">def pretty_callable(tp: CallableType, skip_self: bool = False) -&gt; str:
    """Return a nice easily-readable representation of a callable type.
    For example:
        def [T &lt;: int] f(self, x: int, y: T) -&gt; None

    If skip_self is True, print an actual callable type, as it would appear
    when bound on an instance/class, rather than how it would appear in the
    defining statement.
    """
    s = ""
    asterisk = False
    slash = False
    for i in range(len(tp.arg_types)):
        if s:
            s += ", "
        if tp.arg_kinds[i].is_named() and not asterisk:
            s += "*, "
            asterisk = True
        if tp.arg_kinds[i] == ARG_STAR:
            s += "*"
            asterisk = True
        if tp.arg_kinds[i] == ARG_STAR2:
            s += "**"
        name = tp.arg_names[i]
        if name:
            s += name + ": "
        type_str = format_type_bare(tp.arg_types[i])
        if tp.arg_kinds[i] == ARG_STAR2 and tp.unpack_kwargs:
            type_str = f"Unpack[{type_str}]"
        s += type_str
        if tp.arg_kinds[i].is_optional():
            s += " = ..."
        if (
            not slash
            and tp.arg_kinds[i].is_positional()
            and name is None
            and (
                i == len(tp.arg_types) - 1
                or (tp.arg_names[i + 1] is not None or not tp.arg_kinds[i + 1].is_positional())
            )
        ):
            s += ", /"
            slash = True

    # If we got a "special arg" (i.e: self, cls, etc...), prepend it to the arg list
    if isinstance(tp.definition, FuncDef) and hasattr(tp.definition, "arguments"):
        definition_arg_names = [arg.variable.name for arg in tp.definition.arguments]
        if (
            len(definition_arg_names) &gt; len(tp.arg_names)
            and definition_arg_names[0]
            and not skip_self
        ):
            if s:
                s = ", " + s
            s = definition_arg_names[0] + s
        s = f"{tp.definition.name}({s})"
    elif tp.name:
        first_arg = tp.def_extras.get("first_arg")
        if first_arg:
            if s:
                s = ", " + s
            s = first_arg + s
        s = f"{tp.name.split()[0]}({s})"  # skip "of Class" part
    else:
        s = f"({s})"

    s += " -&gt; "
    if tp.type_guard is not None:
        s += f"TypeGuard[{format_type_bare(tp.type_guard)}]"
    else:
        s += format_type_bare(tp.ret_type)

    if tp.variables:
        tvars = []
        for tvar in tp.variables:
            if isinstance(tvar, TypeVarType):
                upper_bound = get_proper_type(tvar.upper_bound)
                if (
                    isinstance(upper_bound, Instance)
                    and upper_bound.type.fullname != "builtins.object"
                ):
                    tvars.append(f"{tvar.name} &lt;: {format_type_bare(upper_bound)}")
                elif tvar.values:
                    tvars.append(
                        "{} in ({})".format(
                            tvar.name, ", ".join([format_type_bare(tp) for tp in tvar.values])
                        )
                    )
                else:
                    tvars.append(tvar.name)
            else:
                # For other TypeVarLikeTypes, just use the repr
                tvars.append(repr(tvar))
        s = f"[{', '.join(tvars)}] {s}"
    return f"def {s}"


</t>
<t tx="ekr.20221004064035.357">def variance_string(variance: int) -&gt; str:
    if variance == COVARIANT:
        return "covariant"
    elif variance == CONTRAVARIANT:
        return "contravariant"
    else:
        return "invariant"


</t>
<t tx="ekr.20221004064035.358">def get_missing_protocol_members(left: Instance, right: Instance) -&gt; list[str]:
    """Find all protocol members of 'right' that are not implemented
    (i.e. completely missing) in 'left'.
    """
    assert right.type.is_protocol
    missing: list[str] = []
    for member in right.type.protocol_members:
        if not find_member(member, left, left):
            missing.append(member)
    return missing


</t>
<t tx="ekr.20221004064035.359">def get_conflict_protocol_types(
    left: Instance, right: Instance, class_obj: bool = False
) -&gt; list[tuple[str, Type, Type]]:
    """Find members that are defined in 'left' but have incompatible types.
    Return them as a list of ('member', 'got', 'expected').
    """
    assert right.type.is_protocol
    conflicts: list[tuple[str, Type, Type]] = []
    for member in right.type.protocol_members:
        if member in ("__init__", "__new__"):
            continue
        supertype = find_member(member, right, left)
        assert supertype is not None
        subtype = find_member(member, left, left, class_obj=class_obj)
        if not subtype:
            continue
        is_compat = is_subtype(subtype, supertype, ignore_pos_arg_names=True)
        if IS_SETTABLE in get_member_flags(member, right):
            is_compat = is_compat and is_subtype(supertype, subtype)
        if not is_compat:
            conflicts.append((member, subtype, supertype))
    return conflicts


</t>
<t tx="ekr.20221004064035.36">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    if self.s == t:
        return t
    return self.default(self.s)

</t>
<t tx="ekr.20221004064035.360">def get_bad_protocol_flags(
    left: Instance, right: Instance, class_obj: bool = False
) -&gt; list[tuple[str, set[int], set[int]]]:
    """Return all incompatible attribute flags for members that are present in both
    'left' and 'right'.
    """
    assert right.type.is_protocol
    all_flags: list[tuple[str, set[int], set[int]]] = []
    for member in right.type.protocol_members:
        if find_member(member, left, left):
            item = (member, get_member_flags(member, left), get_member_flags(member, right))
            all_flags.append(item)
    bad_flags = []
    for name, subflags, superflags in all_flags:
        if (
            IS_CLASSVAR in subflags
            and IS_CLASSVAR not in superflags
            or IS_CLASSVAR in superflags
            and IS_CLASSVAR not in subflags
            or IS_SETTABLE in superflags
            and IS_SETTABLE not in subflags
            or IS_CLASS_OR_STATIC in superflags
            and IS_CLASS_OR_STATIC not in subflags
            or class_obj
            and IS_VAR in superflags
            and IS_CLASSVAR not in subflags
            or class_obj
            and IS_CLASSVAR in superflags
        ):
            bad_flags.append((name, subflags, superflags))
    return bad_flags


</t>
<t tx="ekr.20221004064035.361">def capitalize(s: str) -&gt; str:
    """Capitalize the first character of a string."""
    if s == "":
        return ""
    else:
        return s[0].upper() + s[1:]


</t>
<t tx="ekr.20221004064035.362">def extract_type(name: str) -&gt; str:
    """If the argument is the name of a method (of form C.m), return
    the type portion in quotes (e.g. "y"). Otherwise, return the string
    unmodified.
    """
    name = re.sub('^"[a-zA-Z0-9_]+" of ', "", name)
    return name


</t>
<t tx="ekr.20221004064035.363">def strip_quotes(s: str) -&gt; str:
    """Strip a double quote at the beginning and end of the string, if any."""
    s = re.sub('^"', "", s)
    s = re.sub('"$', "", s)
    return s


</t>
<t tx="ekr.20221004064035.364">def format_string_list(lst: list[str]) -&gt; str:
    assert len(lst) &gt; 0
    if len(lst) == 1:
        return lst[0]
    elif len(lst) &lt;= 5:
        return f"{', '.join(lst[:-1])} and {lst[-1]}"
    else:
        return "%s, ... and %s (%i methods suppressed)" % (
            ", ".join(lst[:2]),
            lst[-1],
            len(lst) - 3,
        )


</t>
<t tx="ekr.20221004064035.365">def format_item_name_list(s: Iterable[str]) -&gt; str:
    lst = list(s)
    if len(lst) &lt;= 5:
        return "(" + ", ".join([f'"{name}"' for name in lst]) + ")"
    else:
        return "(" + ", ".join([f'"{name}"' for name in lst[:5]]) + ", ...)"


</t>
<t tx="ekr.20221004064035.366">def callable_name(type: FunctionLike) -&gt; str | None:
    name = type.get_name()
    if name is not None and name[0] != "&lt;":
        return f'"{name}"'.replace(" of ", '" of "')
    return name


</t>
<t tx="ekr.20221004064035.367">def for_function(callee: CallableType) -&gt; str:
    name = callable_name(callee)
    if name is not None:
        return f" for {name}"
    return ""


</t>
<t tx="ekr.20221004064035.368">def find_defining_module(modules: dict[str, MypyFile], typ: CallableType) -&gt; MypyFile | None:
    if not typ.definition:
        return None
    fullname = typ.definition.fullname
    if "." in fullname:
        for i in range(fullname.count(".")):
            module_name = fullname.rsplit(".", i + 1)[0]
            try:
                return modules[module_name]
            except KeyError:
                pass
        assert False, "Couldn't determine module from CallableType"
    return None


</t>
<t tx="ekr.20221004064035.369"># For hard-coding suggested missing member alternatives.
COMMON_MISTAKES: Final[dict[str, Sequence[str]]] = {"add": ("append", "extend")}


</t>
<t tx="ekr.20221004064035.37">def visit_unpack_type(self, t: UnpackType) -&gt; UnpackType:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.370">def best_matches(current: str, options: Iterable[str]) -&gt; list[str]:
    ratios = {v: difflib.SequenceMatcher(a=current, b=v).ratio() for v in options}
    return sorted(
        (o for o in options if ratios[o] &gt; 0.75), reverse=True, key=lambda v: (ratios[v], v)
    )


</t>
<t tx="ekr.20221004064035.371">def pretty_seq(args: Sequence[str], conjunction: str) -&gt; str:
    quoted = ['"' + a + '"' for a in args]
    if len(quoted) == 1:
        return quoted[0]
    if len(quoted) == 2:
        return f"{quoted[0]} {conjunction} {quoted[1]}"
    last_sep = ", " + conjunction + " "
    return ", ".join(quoted[:-1]) + last_sep + quoted[-1]


</t>
<t tx="ekr.20221004064035.372">def append_invariance_notes(
    notes: list[str], arg_type: Instance, expected_type: Instance
) -&gt; list[str]:
    """Explain that the type is invariant and give notes for how to solve the issue."""
    invariant_type = ""
    covariant_suggestion = ""
    if (
        arg_type.type.fullname == "builtins.list"
        and expected_type.type.fullname == "builtins.list"
        and is_subtype(arg_type.args[0], expected_type.args[0])
    ):
        invariant_type = "List"
        covariant_suggestion = 'Consider using "Sequence" instead, which is covariant'
    elif (
        arg_type.type.fullname == "builtins.dict"
        and expected_type.type.fullname == "builtins.dict"
        and is_same_type(arg_type.args[0], expected_type.args[0])
        and is_subtype(arg_type.args[1], expected_type.args[1])
    ):
        invariant_type = "Dict"
        covariant_suggestion = (
            'Consider using "Mapping" instead, ' "which is covariant in the value type"
        )
    if invariant_type and covariant_suggestion:
        notes.append(
            f'"{invariant_type}" is invariant -- see '
            + "https://mypy.readthedocs.io/en/stable/common_issues.html#variance"
        )
        notes.append(covariant_suggestion)
    return notes


</t>
<t tx="ekr.20221004064035.373">def make_inferred_type_note(
    context: Context, subtype: Type, supertype: Type, supertype_str: str
) -&gt; str:
    """Explain that the user may have forgotten to type a variable.

    The user does not expect an error if the inferred container type is the same as the return
    type of a function and the argument type(s) are a subtype of the argument type(s) of the
    return type. This note suggests that they add a type annotation with the return type instead
    of relying on the inferred type.
    """
    subtype = get_proper_type(subtype)
    supertype = get_proper_type(supertype)
    if (
        isinstance(subtype, Instance)
        and isinstance(supertype, Instance)
        and subtype.type.fullname == supertype.type.fullname
        and subtype.args
        and supertype.args
        and isinstance(context, ReturnStmt)
        and isinstance(context.expr, NameExpr)
        and isinstance(context.expr.node, Var)
        and context.expr.node.is_inferred
    ):
        for subtype_arg, supertype_arg in zip(subtype.args, supertype.args):
            if not is_subtype(subtype_arg, supertype_arg):
                return ""
        var_name = context.expr.name
        return 'Perhaps you need a type annotation for "{}"? Suggestion: {}'.format(
            var_name, supertype_str
        )
    return ""


</t>
<t tx="ekr.20221004064035.374">def format_key_list(keys: list[str], *, short: bool = False) -&gt; str:
    formatted_keys = [f'"{key}"' for key in keys]
    td = "" if short else "TypedDict "
    if len(keys) == 0:
        return f"no {td}keys"
    elif len(keys) == 1:
        return f"{td}key {formatted_keys[0]}"
    else:
        return f"{td}keys ({', '.join(formatted_keys)})"
</t>
<t tx="ekr.20221004064035.375">@path C:/Repos/ekr-mypy2/mypy/
"""Message constants for generating error messages during type checking.

Literal messages should be defined as constants in this module so they won't get out of sync
if used in more than one place, and so that they can be easily introspected. These messages are
ultimately consumed by messages.MessageBuilder.fail(). For more non-trivial message generation,
add a method to MessageBuilder and call this instead.
"""

from __future__ import annotations

from typing import NamedTuple
from typing_extensions import Final

from mypy import errorcodes as codes


@others
# Invalid types
INVALID_TYPE_RAW_ENUM_VALUE: Final = ErrorMessage(
    "Invalid type: try using Literal[{}.{}] instead?", codes.VALID_TYPE
)

# Type checker error message constants
NO_RETURN_VALUE_EXPECTED: Final = ErrorMessage("No return value expected", codes.RETURN_VALUE)
MISSING_RETURN_STATEMENT: Final = ErrorMessage("Missing return statement", codes.RETURN)
EMPTY_BODY_ABSTRACT: Final = ErrorMessage(
    "If the method is meant to be abstract, use @abc.abstractmethod", codes.EMPTY_BODY
)
INVALID_IMPLICIT_RETURN: Final = ErrorMessage("Implicit return in function which does not return")
INCOMPATIBLE_RETURN_VALUE_TYPE: Final = ErrorMessage(
    "Incompatible return value type", codes.RETURN_VALUE
)
RETURN_VALUE_EXPECTED: Final = ErrorMessage("Return value expected", codes.RETURN_VALUE)
NO_RETURN_EXPECTED: Final = ErrorMessage("Return statement in function which does not return")
INVALID_EXCEPTION: Final = ErrorMessage("Exception must be derived from BaseException")
INVALID_EXCEPTION_TYPE: Final = ErrorMessage("Exception type must be derived from BaseException")
RETURN_IN_ASYNC_GENERATOR: Final = ErrorMessage(
    '"return" with value in async generator is not allowed'
)
INVALID_RETURN_TYPE_FOR_GENERATOR: Final = ErrorMessage(
    'The return type of a generator function should be "Generator"' " or one of its supertypes"
)
INVALID_RETURN_TYPE_FOR_ASYNC_GENERATOR: Final = ErrorMessage(
    'The return type of an async generator function should be "AsyncGenerator" or one of its '
    "supertypes"
)
YIELD_VALUE_EXPECTED: Final = ErrorMessage("Yield value expected")
INCOMPATIBLE_TYPES: Final = ErrorMessage("Incompatible types")
INCOMPATIBLE_TYPES_IN_ASSIGNMENT: Final = ErrorMessage(
    "Incompatible types in assignment", code=codes.ASSIGNMENT
)
INCOMPATIBLE_TYPES_IN_AWAIT: Final = ErrorMessage('Incompatible types in "await"')
INCOMPATIBLE_REDEFINITION: Final = ErrorMessage("Incompatible redefinition")
INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AENTER: Final = (
    'Incompatible types in "async with" for "__aenter__"'
)
INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AEXIT: Final = (
    'Incompatible types in "async with" for "__aexit__"'
)
INCOMPATIBLE_TYPES_IN_ASYNC_FOR: Final = 'Incompatible types in "async for"'
INVALID_TYPE_FOR_SLOTS: Final = 'Invalid type for "__slots__"'

ASYNC_FOR_OUTSIDE_COROUTINE: Final = '"async for" outside async function'
ASYNC_WITH_OUTSIDE_COROUTINE: Final = '"async with" outside async function'

INCOMPATIBLE_TYPES_IN_YIELD: Final = ErrorMessage('Incompatible types in "yield"')
INCOMPATIBLE_TYPES_IN_YIELD_FROM: Final = ErrorMessage('Incompatible types in "yield from"')
INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION: Final = "Incompatible types in string interpolation"
INCOMPATIBLE_TYPES_IN_CAPTURE: Final = ErrorMessage("Incompatible types in capture pattern")
MUST_HAVE_NONE_RETURN_TYPE: Final = ErrorMessage('The return type of "{}" must be None')
TUPLE_INDEX_OUT_OF_RANGE: Final = ErrorMessage("Tuple index out of range")
INVALID_SLICE_INDEX: Final = ErrorMessage("Slice index must be an integer or None")
CANNOT_INFER_LAMBDA_TYPE: Final = ErrorMessage("Cannot infer type of lambda")
CANNOT_ACCESS_INIT: Final = (
    'Accessing "__init__" on an instance is unsound, since instance.__init__ could be from'
    " an incompatible subclass"
)
NON_INSTANCE_NEW_TYPE: Final = ErrorMessage('"__new__" must return a class instance (got {})')
INVALID_NEW_TYPE: Final = ErrorMessage('Incompatible return type for "__new__"')
BAD_CONSTRUCTOR_TYPE: Final = ErrorMessage("Unsupported decorated constructor type")
CANNOT_ASSIGN_TO_METHOD: Final = "Cannot assign to a method"
CANNOT_ASSIGN_TO_TYPE: Final = "Cannot assign to a type"
INCONSISTENT_ABSTRACT_OVERLOAD: Final = ErrorMessage(
    "Overloaded method has both abstract and non-abstract variants"
)
MULTIPLE_OVERLOADS_REQUIRED: Final = ErrorMessage("Single overload definition, multiple required")
READ_ONLY_PROPERTY_OVERRIDES_READ_WRITE: Final = ErrorMessage(
    "Read-only property cannot override read-write property"
)
FORMAT_REQUIRES_MAPPING: Final = "Format requires a mapping"
RETURN_TYPE_CANNOT_BE_CONTRAVARIANT: Final = ErrorMessage(
    "Cannot use a contravariant type variable as return type"
)
FUNCTION_PARAMETER_CANNOT_BE_COVARIANT: Final = ErrorMessage(
    "Cannot use a covariant type variable as a parameter"
)
INCOMPATIBLE_IMPORT_OF: Final = ErrorMessage('Incompatible import of "{}"', code=codes.ASSIGNMENT)
FUNCTION_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a type annotation", codes.NO_UNTYPED_DEF
)
ONLY_CLASS_APPLICATION: Final = ErrorMessage(
    "Type application is only supported for generic classes"
)
RETURN_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a return type annotation", codes.NO_UNTYPED_DEF
)
ARGUMENT_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a type annotation for one or more arguments", codes.NO_UNTYPED_DEF
)
KEYWORD_ARGUMENT_REQUIRES_STR_KEY_TYPE: Final = ErrorMessage(
    'Keyword argument only valid with "str" key type in call to "dict"'
)
ALL_MUST_BE_SEQ_STR: Final = ErrorMessage("Type of __all__ must be {}, not {}")
INVALID_TYPEDDICT_ARGS: Final = ErrorMessage(
    "Expected keyword arguments, {...}, or dict(...) in TypedDict constructor"
)
TYPEDDICT_KEY_MUST_BE_STRING_LITERAL: Final = ErrorMessage(
    "Expected TypedDict key to be string literal"
)
MALFORMED_ASSERT: Final = ErrorMessage("Assertion is always true, perhaps remove parentheses?")
DUPLICATE_TYPE_SIGNATURES: Final = "Function has duplicate type signatures"
DESCRIPTOR_SET_NOT_CALLABLE: Final = ErrorMessage("{}.__set__ is not callable")
DESCRIPTOR_GET_NOT_CALLABLE: Final = "{}.__get__ is not callable"
MODULE_LEVEL_GETATTRIBUTE: Final = ErrorMessage(
    "__getattribute__ is not valid at the module level"
)
NAME_NOT_IN_SLOTS: Final = ErrorMessage(
    'Trying to assign name "{}" that is not in "__slots__" of type "{}"'
)
TYPE_ALWAYS_TRUE: Final = ErrorMessage(
    "{} which does not implement __bool__ or __len__ "
    "so it could always be true in boolean context",
    code=codes.TRUTHY_BOOL,
)
TYPE_ALWAYS_TRUE_UNIONTYPE: Final = ErrorMessage(
    "{} of which no members implement __bool__ or __len__ "
    "so it could always be true in boolean context",
    code=codes.TRUTHY_BOOL,
)
FUNCTION_ALWAYS_TRUE: Final = ErrorMessage(
    "Function {} could always be true in boolean context", code=codes.TRUTHY_BOOL
)
NOT_CALLABLE: Final = "{} not callable"
TYPE_MUST_BE_USED: Final = "Value of type {} must be used"

# Generic
GENERIC_INSTANCE_VAR_CLASS_ACCESS: Final = (
    "Access to generic instance variables via class is ambiguous"
)
GENERIC_CLASS_VAR_ACCESS: Final = "Access to generic class variables is ambiguous"
BARE_GENERIC: Final = "Missing type parameters for generic type {}"
IMPLICIT_GENERIC_ANY_BUILTIN: Final = (
    'Implicit generic "Any". Use "{}" and specify generic parameters'
)
INVALID_UNPACK = "{} cannot be unpacked (must be tuple or TypeVarTuple)"

# TypeVar
INCOMPATIBLE_TYPEVAR_VALUE: Final = 'Value of type variable "{}" of {} cannot be {}'
CANNOT_USE_TYPEVAR_AS_EXPRESSION: Final = 'Type variable "{}.{}" cannot be used as an expression'
INVALID_TYPEVAR_AS_TYPEARG: Final = 'Type variable "{}" not valid as type argument value for "{}"'
INVALID_TYPEVAR_ARG_BOUND: Final = 'Type argument {} of "{}" must be a subtype of {}'
INVALID_TYPEVAR_ARG_VALUE: Final = 'Invalid type argument value for "{}"'
TYPEVAR_VARIANCE_DEF: Final = 'TypeVar "{}" may only be a literal bool'
TYPEVAR_BOUND_MUST_BE_TYPE: Final = 'TypeVar "bound" must be a type'
TYPEVAR_UNEXPECTED_ARGUMENT: Final = 'Unexpected argument to "TypeVar()"'
UNBOUND_TYPEVAR: Final = (
    "A function returning TypeVar should receive at least "
    "one argument containing the same TypeVar"
)

# Super
TOO_MANY_ARGS_FOR_SUPER: Final = ErrorMessage('Too many arguments for "super"')
SUPER_WITH_SINGLE_ARG_NOT_SUPPORTED: Final = ErrorMessage(
    '"super" with a single argument not supported'
)
UNSUPPORTED_ARG_1_FOR_SUPER: Final = ErrorMessage('Unsupported argument 1 for "super"')
UNSUPPORTED_ARG_2_FOR_SUPER: Final = ErrorMessage('Unsupported argument 2 for "super"')
SUPER_VARARGS_NOT_SUPPORTED: Final = ErrorMessage('Varargs not supported with "super"')
SUPER_POSITIONAL_ARGS_REQUIRED: Final = ErrorMessage('"super" only accepts positional arguments')
SUPER_ARG_2_NOT_INSTANCE_OF_ARG_1: Final = ErrorMessage(
    'Argument 2 for "super" not an instance of argument 1'
)
TARGET_CLASS_HAS_NO_BASE_CLASS: Final = ErrorMessage("Target class has no base class")
SUPER_OUTSIDE_OF_METHOD_NOT_SUPPORTED: Final = ErrorMessage(
    "super() outside of a method is not supported"
)
SUPER_ENCLOSING_POSITIONAL_ARGS_REQUIRED: Final = ErrorMessage(
    "super() requires one or more positional arguments in enclosing function"
)

# Self-type
MISSING_OR_INVALID_SELF_TYPE: Final = ErrorMessage(
    "Self argument missing for a non-static method (or an invalid type for self)"
)
ERASED_SELF_TYPE_NOT_SUPERTYPE: Final = ErrorMessage(
    'The erased type of self "{}" is not a supertype of its class "{}"'
)

# Final
CANNOT_INHERIT_FROM_FINAL: Final = ErrorMessage('Cannot inherit from final class "{}"')
DEPENDENT_FINAL_IN_CLASS_BODY: Final = ErrorMessage(
    "Final name declared in class body cannot depend on type variables"
)
CANNOT_ACCESS_FINAL_INSTANCE_ATTR: Final = (
    'Cannot access final instance attribute "{}" on class object'
)
CANNOT_MAKE_DELETABLE_FINAL: Final = ErrorMessage("Deletable attribute cannot be final")

# Enum
ENUM_MEMBERS_ATTR_WILL_BE_OVERRIDEN: Final = ErrorMessage(
    'Assigned "__members__" will be overridden by "Enum" internally'
)

# ClassVar
CANNOT_OVERRIDE_INSTANCE_VAR: Final = ErrorMessage(
    'Cannot override instance variable (previously declared on base class "{}") with class '
    "variable"
)
CANNOT_OVERRIDE_CLASS_VAR: Final = ErrorMessage(
    'Cannot override class variable (previously declared on base class "{}") with instance '
    "variable"
)
CLASS_VAR_WITH_TYPEVARS: Final = "ClassVar cannot contain type variables"
CLASS_VAR_OUTSIDE_OF_CLASS: Final = "ClassVar can only be used for assignments in class body"

# Protocol
RUNTIME_PROTOCOL_EXPECTED: Final = ErrorMessage(
    "Only @runtime_checkable protocols can be used with instance and class checks"
)
CANNOT_INSTANTIATE_PROTOCOL: Final = ErrorMessage('Cannot instantiate protocol class "{}"')
TOO_MANY_UNION_COMBINATIONS: Final = ErrorMessage(
    "Not all union combinations were tried because there are too many unions"
)

CONTIGUOUS_ITERABLE_EXPECTED: Final = ErrorMessage("Contiguous iterable with same type expected")
ITERABLE_TYPE_EXPECTED: Final = ErrorMessage("Invalid type '{}' for *expr (iterable expected)")
TYPE_GUARD_POS_ARG_REQUIRED: Final = ErrorMessage("Type guard requires positional argument")

# Match Statement
MISSING_MATCH_ARGS: Final = 'Class "{}" doesn\'t define "__match_args__"'
OR_PATTERN_ALTERNATIVE_NAMES: Final = "Alternative patterns bind different names"
CLASS_PATTERN_GENERIC_TYPE_ALIAS: Final = (
    "Class pattern class must not be a type alias with type parameters"
)
CLASS_PATTERN_TYPE_REQUIRED: Final = 'Expected type in class pattern; found "{}"'
CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS: Final = "Too many positional patterns for class pattern"
CLASS_PATTERN_KEYWORD_MATCHES_POSITIONAL: Final = (
    'Keyword "{}" already matches a positional pattern'
)
CLASS_PATTERN_DUPLICATE_KEYWORD_PATTERN: Final = 'Duplicate keyword pattern "{}"'
CLASS_PATTERN_UNKNOWN_KEYWORD: Final = 'Class "{}" has no attribute "{}"'
MULTIPLE_ASSIGNMENTS_IN_PATTERN: Final = 'Multiple assignments to name "{}" in pattern'
CANNOT_MODIFY_MATCH_ARGS: Final = 'Cannot assign to "__match_args__"'
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.376">class ErrorMessage(NamedTuple):
    value: str
    code: codes.ErrorCode | None = None

    @others
</t>
<t tx="ekr.20221004064035.377">def format(self, *args: object, **kwargs: object) -&gt; ErrorMessage:
    return ErrorMessage(self.value.format(*args, **kwargs), code=self.code)

</t>
<t tx="ekr.20221004064035.378">def with_additional_msg(self, info: str) -&gt; ErrorMessage:
    return ErrorMessage(self.value + info, code=self.code)


</t>
<t tx="ekr.20221004064035.379">@path C:/Repos/ekr-mypy2/mypy/
"""Interfaces for accessing metadata.

We provide two implementations.
 * The "classic" file system implementation, which uses a directory
   structure of files.
 * A hokey sqlite backed implementation, which basically simulates
   the file system in an effort to work around poor file system performance
   on OS X.
"""

from __future__ import annotations

import binascii
import os
import time
from abc import abstractmethod
from typing import TYPE_CHECKING, Any, Iterable

if TYPE_CHECKING:
    # We avoid importing sqlite3 unless we are using it so we can mostly work
    # on semi-broken pythons that are missing it.
    import sqlite3


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.38">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    if self.s == t:
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.380">class MetadataStore:
    """Generic interface for metadata storage."""

    @others
</t>
<t tx="ekr.20221004064035.381">@abstractmethod
def getmtime(self, name: str) -&gt; float:
    """Read the mtime of a metadata entry..

    Raises FileNotFound if the entry does not exist.
    """

</t>
<t tx="ekr.20221004064035.382">@abstractmethod
def read(self, name: str) -&gt; str:
    """Read the contents of a metadata entry.

    Raises FileNotFound if the entry does not exist.
    """

</t>
<t tx="ekr.20221004064035.383">@abstractmethod
def write(self, name: str, data: str, mtime: float | None = None) -&gt; bool:
    """Write a metadata entry.

    If mtime is specified, set it as the mtime of the entry. Otherwise,
    the current time is used.

    Returns True if the entry is successfully written, False otherwise.
    """

</t>
<t tx="ekr.20221004064035.384">@abstractmethod
def remove(self, name: str) -&gt; None:
    """Delete a metadata entry"""

</t>
<t tx="ekr.20221004064035.385">@abstractmethod
def commit(self) -&gt; None:
    """If the backing store requires a commit, do it.

    But N.B. that this is not *guaranteed* to do anything, and
    there is no guarantee that changes are not made until it is
    called.
    """

</t>
<t tx="ekr.20221004064035.386">@abstractmethod
def list_all(self) -&gt; Iterable[str]:
    ...


</t>
<t tx="ekr.20221004064035.387">def random_string() -&gt; str:
    return binascii.hexlify(os.urandom(8)).decode("ascii")


</t>
<t tx="ekr.20221004064035.388">class FilesystemMetadataStore(MetadataStore):
    @others
</t>
<t tx="ekr.20221004064035.389">def __init__(self, cache_dir_prefix: str) -&gt; None:
    # We check startswith instead of equality because the version
    # will have already been appended by the time the cache dir is
    # passed here.
    if cache_dir_prefix.startswith(os.devnull):
        self.cache_dir_prefix = None
    else:
        self.cache_dir_prefix = cache_dir_prefix

</t>
<t tx="ekr.20221004064035.39">def visit_instance(self, t: Instance) -&gt; ProperType:
    if isinstance(self.s, Instance):
        if self.instance_joiner is None:
            self.instance_joiner = InstanceJoiner()
        nominal = self.instance_joiner.join_instances(t, self.s)
        structural: Instance | None = None
        if t.type.is_protocol and is_protocol_implementation(self.s, t):
            structural = t
        elif self.s.type.is_protocol and is_protocol_implementation(t, self.s):
            structural = self.s
        # Structural join is preferred in the case where we have found both
        # structural and nominal and they have same MRO length (see two comments
        # in join_instances_via_supertype). Otherwise, just return the nominal join.
        if not structural or is_better(nominal, structural):
            return nominal
        return structural
    elif isinstance(self.s, FunctionLike):
        if t.type.is_protocol:
            call = unpack_callback_protocol(t)
            if call:
                return join_types(call, self.s)
        return join_types(t, self.s.fallback)
    elif isinstance(self.s, TypeType):
        return join_types(t, self.s)
    elif isinstance(self.s, TypedDictType):
        return join_types(t, self.s)
    elif isinstance(self.s, TupleType):
        return join_types(t, self.s)
    elif isinstance(self.s, LiteralType):
        return join_types(t, self.s)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.390">def getmtime(self, name: str) -&gt; float:
    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    return int(os.path.getmtime(os.path.join(self.cache_dir_prefix, name)))

</t>
<t tx="ekr.20221004064035.391">def read(self, name: str) -&gt; str:
    assert os.path.normpath(name) != os.path.abspath(name), "Don't use absolute paths!"

    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    with open(os.path.join(self.cache_dir_prefix, name)) as f:
        return f.read()

</t>
<t tx="ekr.20221004064035.392">def write(self, name: str, data: str, mtime: float | None = None) -&gt; bool:
    assert os.path.normpath(name) != os.path.abspath(name), "Don't use absolute paths!"

    if not self.cache_dir_prefix:
        return False

    path = os.path.join(self.cache_dir_prefix, name)
    tmp_filename = path + "." + random_string()
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(tmp_filename, "w") as f:
            f.write(data)
        os.replace(tmp_filename, path)
        if mtime is not None:
            os.utime(path, times=(mtime, mtime))

    except os.error:
        return False
    return True

</t>
<t tx="ekr.20221004064035.393">def remove(self, name: str) -&gt; None:
    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    os.remove(os.path.join(self.cache_dir_prefix, name))

</t>
<t tx="ekr.20221004064035.394">def commit(self) -&gt; None:
    pass

</t>
<t tx="ekr.20221004064035.395">def list_all(self) -&gt; Iterable[str]:
    if not self.cache_dir_prefix:
        return

    for dir, _, files in os.walk(self.cache_dir_prefix):
        dir = os.path.relpath(dir, self.cache_dir_prefix)
        for file in files:
            yield os.path.join(dir, file)


</t>
<t tx="ekr.20221004064035.396">SCHEMA = """
CREATE TABLE IF NOT EXISTS files (
    path TEXT UNIQUE NOT NULL,
    mtime REAL,
    data TEXT
);
CREATE INDEX IF NOT EXISTS path_idx on files(path);
"""
# No migrations yet
MIGRATIONS: list[str] = []


</t>
<t tx="ekr.20221004064035.397">def connect_db(db_file: str) -&gt; sqlite3.Connection:
    import sqlite3.dbapi2

    db = sqlite3.dbapi2.connect(db_file)
    db.executescript(SCHEMA)
    for migr in MIGRATIONS:
        try:
            db.executescript(migr)
        except sqlite3.OperationalError:
            pass
    return db


</t>
<t tx="ekr.20221004064035.398">class SqliteMetadataStore(MetadataStore):
    @others
</t>
<t tx="ekr.20221004064035.399">def __init__(self, cache_dir_prefix: str) -&gt; None:
    # We check startswith instead of equality because the version
    # will have already been appended by the time the cache dir is
    # passed here.
    if cache_dir_prefix.startswith(os.devnull):
        self.db = None
        return

    os.makedirs(cache_dir_prefix, exist_ok=True)
    self.db = connect_db(os.path.join(cache_dir_prefix, "cache.db"))

</t>
<t tx="ekr.20221004064035.4">def read(self, size: int = 100000) -&gt; bytes:
    """Read bytes from an IPC connection until its empty."""
    bdata = bytearray()
    if sys.platform == "win32":
        while True:
            ov, err = _winapi.ReadFile(self.connection, size, overlapped=True)
            try:
                if err == _winapi.ERROR_IO_PENDING:
                    timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                    res = _winapi.WaitForSingleObject(ov.event, timeout)
                    if res != _winapi.WAIT_OBJECT_0:
                        raise IPCException(f"Bad result from I/O wait: {res}")
            except BaseException:
                ov.cancel()
                raise
            _, err = ov.GetOverlappedResult(True)
            more = ov.getbuffer()
            if more:
                bdata.extend(more)
            if err == 0:
                # we are done!
                break
            elif err == _winapi.ERROR_MORE_DATA:
                # read again
                continue
            elif err == _winapi.ERROR_OPERATION_ABORTED:
                raise IPCException("ReadFile operation aborted.")
    else:
        while True:
            more = self.connection.recv(size)
            if not more:
                break
            bdata.extend(more)
    return bytes(bdata)

</t>
<t tx="ekr.20221004064035.40">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    if isinstance(self.s, CallableType) and is_similar_callables(t, self.s):
        if is_equivalent(t, self.s):
            return combine_similar_callables(t, self.s)
        result = join_similar_callables(t, self.s)
        # We set the from_type_type flag to suppress error when a collection of
        # concrete class objects gets inferred as their common abstract superclass.
        if not (
            (t.is_type_obj() and t.type_object().is_abstract)
            or (self.s.is_type_obj() and self.s.type_object().is_abstract)
        ):
            result.from_type_type = True
        if any(
            isinstance(tp, (NoneType, UninhabitedType))
            for tp in get_proper_types(result.arg_types)
        ):
            # We don't want to return unusable Callable, attempt fallback instead.
            return join_types(t.fallback, self.s)
        return result
    elif isinstance(self.s, Overloaded):
        # Switch the order of arguments to that we'll get to visit_overloaded.
        return join_types(t, self.s)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = unpack_callback_protocol(self.s)
        if call:
            return join_types(t, call)
    return join_types(t.fallback, self.s)

</t>
<t tx="ekr.20221004064035.400">def _query(self, name: str, field: str) -&gt; Any:
    # Raises FileNotFound for consistency with the file system version
    if not self.db:
        raise FileNotFoundError()

    cur = self.db.execute(f"SELECT {field} FROM files WHERE path = ?", (name,))
    results = cur.fetchall()
    if not results:
        raise FileNotFoundError()
    assert len(results) == 1
    return results[0][0]

</t>
<t tx="ekr.20221004064035.401">def getmtime(self, name: str) -&gt; float:
    mtime = self._query(name, "mtime")
    assert isinstance(mtime, float)
    return mtime

</t>
<t tx="ekr.20221004064035.402">def read(self, name: str) -&gt; str:
    data = self._query(name, "data")
    assert isinstance(data, str)
    return data

</t>
<t tx="ekr.20221004064035.403">def write(self, name: str, data: str, mtime: float | None = None) -&gt; bool:
    import sqlite3

    if not self.db:
        return False
    try:
        if mtime is None:
            mtime = time.time()
        self.db.execute(
            "INSERT OR REPLACE INTO files(path, mtime, data) VALUES(?, ?, ?)",
            (name, mtime, data),
        )
    except sqlite3.OperationalError:
        return False
    return True

</t>
<t tx="ekr.20221004064035.404">def remove(self, name: str) -&gt; None:
    if not self.db:
        raise FileNotFoundError()

    self.db.execute("DELETE FROM files WHERE path = ?", (name,))

</t>
<t tx="ekr.20221004064035.405">def commit(self) -&gt; None:
    if self.db:
        self.db.commit()

</t>
<t tx="ekr.20221004064035.406">def list_all(self) -&gt; Iterable[str]:
    if self.db:
        for row in self.db.execute("SELECT path FROM files"):
            yield row[0]
</t>
<t tx="ekr.20221004064035.407">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from mypy.nodes import (
    AssertTypeExpr,
    AssignmentStmt,
    CastExpr,
    ClassDef,
    ForStmt,
    FuncItem,
    NamedTupleExpr,
    NewTypeExpr,
    PromoteExpr,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    Var,
    WithStmt,
)
from mypy.traverser import TraverserVisitor
from mypy.types import Type
from mypy.typetraverser import TypeTraverserVisitor


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.408">class MixedTraverserVisitor(TraverserVisitor, TypeTraverserVisitor):
    """Recursive traversal of both Node and Type objects."""

    # Symbol nodes

    @others
</t>
<t tx="ekr.20221004064035.409">def visit_var(self, var: Var) -&gt; None:
    self.visit_optional_type(var.type)

</t>
<t tx="ekr.20221004064035.41">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    # This is more complex than most other cases. Here are some
    # examples that illustrate how this works.
    #
    # First let's define a concise notation:
    #  - Cn are callable types (for n in 1, 2, ...)
    #  - Ov(C1, C2, ...) is an overloaded type with items C1, C2, ...
    #  - Callable[[T, ...], S] is written as [T, ...] -&gt; S.
    #
    # We want some basic properties to hold (assume Cn are all
    # unrelated via Any-similarity):
    #
    #   join(Ov(C1, C2), C1) == C1
    #   join(Ov(C1, C2), Ov(C1, C2)) == Ov(C1, C2)
    #   join(Ov(C1, C2), Ov(C1, C3)) == C1
    #   join(Ov(C2, C2), C3) == join of fallback types
    #
    # The presence of Any types makes things more interesting. The join is the
    # most general type we can get with respect to Any:
    #
    #   join(Ov([int] -&gt; int, [str] -&gt; str), [Any] -&gt; str) == Any -&gt; str
    #
    # We could use a simplification step that removes redundancies, but that's not
    # implemented right now. Consider this example, where we get a redundancy:
    #
    #   join(Ov([int, Any] -&gt; Any, [str, Any] -&gt; Any), [Any, int] -&gt; Any) ==
    #       Ov([Any, int] -&gt; Any, [Any, int] -&gt; Any)
    #
    # TODO: Consider more cases of callable subtyping.
    result: list[CallableType] = []
    s = self.s
    if isinstance(s, FunctionLike):
        # The interesting case where both types are function types.
        for t_item in t.items:
            for s_item in s.items:
                if is_similar_callables(t_item, s_item):
                    if is_equivalent(t_item, s_item):
                        result.append(combine_similar_callables(t_item, s_item))
                    elif is_subtype(t_item, s_item):
                        result.append(s_item)
        if result:
            # TODO: Simplify redundancies from the result.
            if len(result) == 1:
                return result[0]
            else:
                return Overloaded(result)
        return join_types(t.fallback, s.fallback)
    elif isinstance(s, Instance) and s.type.is_protocol:
        call = unpack_callback_protocol(s)
        if call:
            return join_types(t, call)
    return join_types(t.fallback, s)

</t>
<t tx="ekr.20221004064035.410">def visit_func(self, o: FuncItem) -&gt; None:
    super().visit_func(o)
    self.visit_optional_type(o.type)

</t>
<t tx="ekr.20221004064035.411">def visit_class_def(self, o: ClassDef) -&gt; None:
    # TODO: Should we visit generated methods/variables as well, either here or in
    #       TraverserVisitor?
    super().visit_class_def(o)
    info = o.info
    if info:
        for base in info.bases:
            base.accept(self)

</t>
<t tx="ekr.20221004064035.412">def visit_type_alias_expr(self, o: TypeAliasExpr) -&gt; None:
    super().visit_type_alias_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20221004064035.413">def visit_type_var_expr(self, o: TypeVarExpr) -&gt; None:
    super().visit_type_var_expr(o)
    o.upper_bound.accept(self)
    for value in o.values:
        value.accept(self)

</t>
<t tx="ekr.20221004064035.414">def visit_typeddict_expr(self, o: TypedDictExpr) -&gt; None:
    super().visit_typeddict_expr(o)
    self.visit_optional_type(o.info.typeddict_type)

</t>
<t tx="ekr.20221004064035.415">def visit_namedtuple_expr(self, o: NamedTupleExpr) -&gt; None:
    super().visit_namedtuple_expr(o)
    assert o.info.tuple_type
    o.info.tuple_type.accept(self)

</t>
<t tx="ekr.20221004064035.416">def visit__promote_expr(self, o: PromoteExpr) -&gt; None:
    super().visit__promote_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20221004064035.417">def visit_newtype_expr(self, o: NewTypeExpr) -&gt; None:
    super().visit_newtype_expr(o)
    self.visit_optional_type(o.old_type)

</t>
<t tx="ekr.20221004064035.418"># Statements

</t>
<t tx="ekr.20221004064035.419">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    super().visit_assignment_stmt(o)
    self.visit_optional_type(o.type)

</t>
<t tx="ekr.20221004064035.42">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    # When given two fixed-length tuples:
    # * If they have the same length, join their subtypes item-wise:
    #   Tuple[int, bool] + Tuple[bool, bool] becomes Tuple[int, bool]
    # * If lengths do not match, return a variadic tuple:
    #   Tuple[bool, int] + Tuple[bool] becomes Tuple[int, ...]
    #
    # Otherwise, `t` is a fixed-length tuple but `self.s` is NOT:
    # * Joining with a variadic tuple returns variadic tuple:
    #   Tuple[int, bool] + Tuple[bool, ...] becomes Tuple[int, ...]
    # * Joining with any Sequence also returns a Sequence:
    #   Tuple[int, bool] + List[bool] becomes Sequence[int]
    if isinstance(self.s, TupleType) and self.s.length() == t.length():
        if self.instance_joiner is None:
            self.instance_joiner = InstanceJoiner()
        fallback = self.instance_joiner.join_instances(
            mypy.typeops.tuple_fallback(self.s), mypy.typeops.tuple_fallback(t)
        )
        assert isinstance(fallback, Instance)
        if self.s.length() == t.length():
            items: list[Type] = []
            for i in range(t.length()):
                items.append(self.join(t.items[i], self.s.items[i]))
            return TupleType(items, fallback)
        else:
            return fallback
    else:
        return join_types(self.s, mypy.typeops.tuple_fallback(t))

</t>
<t tx="ekr.20221004064035.420">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    super().visit_for_stmt(o)
    self.visit_optional_type(o.index_type)

</t>
<t tx="ekr.20221004064035.421">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    super().visit_with_stmt(o)
    for typ in o.analyzed_types:
        typ.accept(self)

</t>
<t tx="ekr.20221004064035.422"># Expressions

</t>
<t tx="ekr.20221004064035.423">def visit_cast_expr(self, o: CastExpr) -&gt; None:
    super().visit_cast_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20221004064035.424">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    super().visit_assert_type_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20221004064035.425">def visit_type_application(self, o: TypeApplication) -&gt; None:
    super().visit_type_application(o)
    for t in o.types:
        t.accept(self)

</t>
<t tx="ekr.20221004064035.426"># Helpers

</t>
<t tx="ekr.20221004064035.427">def visit_optional_type(self, t: Type | None) -&gt; None:
    if t:
        t.accept(self)
</t>
<t tx="ekr.20221004064035.428">@path C:/Repos/ekr-mypy2/mypy/
"""Low-level infrastructure to find modules.

This builds on fscache.py; find_sources.py builds on top of this.
"""

from __future__ import annotations

import ast
import collections
import functools
import os
import re
import subprocess
import sys
from enum import Enum, unique

from mypy.errors import CompileError

if sys.version_info &gt;= (3, 11):
    import tomllib
else:
    import tomli as tomllib

from typing import Dict, List, NamedTuple, Optional, Tuple, Union
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy import pyinfo
from mypy.fscache import FileSystemCache
from mypy.nodes import MypyFile
from mypy.options import Options
from mypy.stubinfo import approved_stub_package_exists


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.429"># Paths to be searched in find_module().
class SearchPaths(NamedTuple):
    python_path: tuple[str, ...]  # where user code is found
    mypy_path: tuple[str, ...]  # from $MYPYPATH or config variable
    package_path: tuple[str, ...]  # from get_site_packages_dirs()
    typeshed_path: tuple[str, ...]  # paths in typeshed


</t>
<t tx="ekr.20221004064035.43">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    if isinstance(self.s, TypedDictType):
        items = {
            item_name: s_item_type
            for (item_name, s_item_type, t_item_type) in self.s.zip(t)
            if (
                is_equivalent(s_item_type, t_item_type)
                and (item_name in t.required_keys) == (item_name in self.s.required_keys)
            )
        }
        fallback = self.s.create_anonymous_fallback()
        # We need to filter by items.keys() since some required keys present in both t and
        # self.s might be missing from the join if the types are incompatible.
        required_keys = set(items.keys()) &amp; t.required_keys &amp; self.s.required_keys
        return TypedDictType(items, required_keys, fallback)
    elif isinstance(self.s, Instance):
        return join_types(self.s, t.fallback)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.430"># Package dirs are a two-tuple of path to search and whether to verify the module
OnePackageDir = Tuple[str, bool]
PackageDirs = List[OnePackageDir]

# Minimum and maximum Python versions for modules in stdlib as (major, minor)
StdlibVersions: _TypeAlias = Dict[str, Tuple[Tuple[int, int], Optional[Tuple[int, int]]]]

PYTHON_EXTENSIONS: Final = [".pyi", ".py"]


</t>
<t tx="ekr.20221004064035.431"># TODO: Consider adding more reasons here?
# E.g. if we deduce a module would likely be found if the user were
# to set the --namespace-packages flag.
@unique
class ModuleNotFoundReason(Enum):
    # The module was not found: we found neither stubs nor a plausible code
    # implementation (with or without a py.typed file).
    NOT_FOUND = 0

    # The implementation for this module plausibly exists (e.g. we
    # found a matching folder or *.py file), but either the parent package
    # did not contain a py.typed file or we were unable to find a
    # corresponding *-stubs package.
    FOUND_WITHOUT_TYPE_HINTS = 1

    # The module was not found in the current working directory, but
    # was able to be found in the parent directory.
    WRONG_WORKING_DIRECTORY = 2

    # Stub PyPI package (typically types-pkgname) known to exist but not installed.
    APPROVED_STUBS_NOT_INSTALLED = 3

    @others
</t>
<t tx="ekr.20221004064035.432">def error_message_templates(self, daemon: bool) -&gt; tuple[str, list[str]]:
    doc_link = "See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports"
    if self is ModuleNotFoundReason.NOT_FOUND:
        msg = 'Cannot find implementation or library stub for module named "{module}"'
        notes = [doc_link]
    elif self is ModuleNotFoundReason.WRONG_WORKING_DIRECTORY:
        msg = 'Cannot find implementation or library stub for module named "{module}"'
        notes = [
            "You may be running mypy in a subpackage, "
            "mypy should be run on the package root"
        ]
    elif self is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
        msg = (
            'Skipping analyzing "{module}": module is installed, but missing library stubs '
            "or py.typed marker"
        )
        notes = [doc_link]
    elif self is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
        msg = 'Library stubs not installed for "{module}"'
        notes = ['Hint: "python3 -m pip install {stub_dist}"']
        if not daemon:
            notes.append(
                '(or run "mypy --install-types" to install all missing stub packages)'
            )
        notes.append(doc_link)
    else:
        assert False
    return msg, notes


</t>
<t tx="ekr.20221004064035.433"># If we found the module, returns the path to the module as a str.
# Otherwise, returns the reason why the module wasn't found.
ModuleSearchResult = Union[str, ModuleNotFoundReason]


</t>
<t tx="ekr.20221004064035.434">class BuildSource:
    """A single source file."""

    @others
</t>
<t tx="ekr.20221004064035.435">def __init__(
    self,
    path: str | None,
    module: str | None,
    text: str | None = None,
    base_dir: str | None = None,
) -&gt; None:
    self.path = path  # File where it's found (e.g. 'xxx/yyy/foo/bar.py')
    self.module = module or "__main__"  # Module name (e.g. 'foo.bar')
    self.text = text  # Source code, if initially supplied, else None
    self.base_dir = base_dir  # Directory where the package is rooted (e.g. 'xxx/yyy')

</t>
<t tx="ekr.20221004064035.436">def __repr__(self) -&gt; str:
    return "BuildSource(path={!r}, module={!r}, has_text={}, base_dir={!r})".format(
        self.path, self.module, self.text is not None, self.base_dir
    )


</t>
<t tx="ekr.20221004064035.437">class BuildSourceSet:
    """Helper to efficiently test a file's membership in a set of build sources."""

    @others
</t>
<t tx="ekr.20221004064035.438">def __init__(self, sources: list[BuildSource]) -&gt; None:
    self.source_text_present = False
    self.source_modules: dict[str, str] = {}
    self.source_paths: set[str] = set()

    for source in sources:
        if source.text is not None:
            self.source_text_present = True
        if source.path:
            self.source_paths.add(source.path)
        if source.module:
            self.source_modules[source.module] = source.path or ""

</t>
<t tx="ekr.20221004064035.439">def is_source(self, file: MypyFile) -&gt; bool:
    if file.path and file.path in self.source_paths:
        return True
    elif file._fullname in self.source_modules:
        return True
    elif self.source_text_present:
        return True
    else:
        return False


</t>
<t tx="ekr.20221004064035.44">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    if isinstance(self.s, LiteralType):
        if t == self.s:
            return t
        if self.s.fallback.type.is_enum and t.fallback.type.is_enum:
            return mypy.typeops.make_simplified_union([self.s, t])
        return join_types(self.s.fallback, t.fallback)
    else:
        return join_types(self.s, t.fallback)

</t>
<t tx="ekr.20221004064035.440">class FindModuleCache:
    """Module finder with integrated cache.

    Module locations and some intermediate results are cached internally
    and can be cleared with the clear() method.

    All file system accesses are performed through a FileSystemCache,
    which is not ever cleared by this class. If necessary it must be
    cleared by client code.
    """

    @others
</t>
<t tx="ekr.20221004064035.441">def __init__(
    self,
    search_paths: SearchPaths,
    fscache: FileSystemCache | None,
    options: Options | None,
    stdlib_py_versions: StdlibVersions | None = None,
    source_set: BuildSourceSet | None = None,
) -&gt; None:
    self.search_paths = search_paths
    self.source_set = source_set
    self.fscache = fscache or FileSystemCache()
    # Cache for get_toplevel_possibilities:
    # search_paths -&gt; (toplevel_id -&gt; list(package_dirs))
    self.initial_components: dict[tuple[str, ...], dict[str, list[str]]] = {}
    # Cache find_module: id -&gt; result
    self.results: dict[str, ModuleSearchResult] = {}
    self.ns_ancestors: dict[str, str] = {}
    self.options = options
    custom_typeshed_dir = None
    if options:
        custom_typeshed_dir = options.custom_typeshed_dir
    self.stdlib_py_versions = stdlib_py_versions or load_stdlib_py_versions(
        custom_typeshed_dir
    )

</t>
<t tx="ekr.20221004064035.442">def clear(self) -&gt; None:
    self.results.clear()
    self.initial_components.clear()
    self.ns_ancestors.clear()

</t>
<t tx="ekr.20221004064035.443">def find_module_via_source_set(self, id: str) -&gt; ModuleSearchResult | None:
    """Fast path to find modules by looking through the input sources

    This is only used when --fast-module-lookup is passed on the command line."""
    if not self.source_set:
        return None

    p = self.source_set.source_modules.get(id, None)
    if p and self.fscache.isfile(p):
        # We need to make sure we still have __init__.py all the way up
        # otherwise we might have false positives compared to slow path
        # in case of deletion of init files, which is covered by some tests.
        # TODO: are there some combination of flags in which this check should be skipped?
        d = os.path.dirname(p)
        for _ in range(id.count(".")):
            if not any(
                self.fscache.isfile(os.path.join(d, "__init__" + x)) for x in PYTHON_EXTENSIONS
            ):
                return None
            d = os.path.dirname(d)
        return p

    idx = id.rfind(".")
    if idx != -1:
        # When we're looking for foo.bar.baz and can't find a matching module
        # in the source set, look up for a foo.bar module.
        parent = self.find_module_via_source_set(id[:idx])
        if parent is None or not isinstance(parent, str):
            return None

        basename, ext = os.path.splitext(parent)
        if not any(parent.endswith("__init__" + x) for x in PYTHON_EXTENSIONS) and (
            ext in PYTHON_EXTENSIONS and not self.fscache.isdir(basename)
        ):
            # If we do find such a *module* (and crucially, we don't want a package,
            # hence the filtering out of __init__ files, and checking for the presence
            # of a folder with a matching name), then we can be pretty confident that
            # 'baz' will either be a top-level variable in foo.bar, or will not exist.
            #
            # Either way, spelunking in other search paths for another 'foo.bar.baz'
            # module should be avoided because:
            #  1. in the unlikely event that one were found, it's highly likely that
            #     it would be unrelated to the source being typechecked and therefore
            #     more likely to lead to erroneous results
            #  2. as described in _find_module, in some cases the search itself could
            #  potentially waste significant amounts of time
            return ModuleNotFoundReason.NOT_FOUND
    return None

</t>
<t tx="ekr.20221004064035.444">def find_lib_path_dirs(self, id: str, lib_path: tuple[str, ...]) -&gt; PackageDirs:
    """Find which elements of a lib_path have the directory a module needs to exist.

    This is run for the python_path, mypy_path, and typeshed_path search paths.
    """
    components = id.split(".")
    dir_chain = os.sep.join(components[:-1])  # e.g., 'foo/bar'

    dirs = []
    for pathitem in self.get_toplevel_possibilities(lib_path, components[0]):
        # e.g., '/usr/lib/python3.4/foo/bar'
        dir = os.path.normpath(os.path.join(pathitem, dir_chain))
        if self.fscache.isdir(dir):
            dirs.append((dir, True))
    return dirs

</t>
<t tx="ekr.20221004064035.445">def get_toplevel_possibilities(self, lib_path: tuple[str, ...], id: str) -&gt; list[str]:
    """Find which elements of lib_path could contain a particular top-level module.

    In practice, almost all modules can be routed to the correct entry in
    lib_path by looking at just the first component of the module name.

    We take advantage of this by enumerating the contents of all of the
    directories on the lib_path and building a map of which entries in
    the lib_path could contain each potential top-level module that appears.
    """

    if lib_path in self.initial_components:
        return self.initial_components[lib_path].get(id, [])

    # Enumerate all the files in the directories on lib_path and produce the map
    components: dict[str, list[str]] = {}
    for dir in lib_path:
        try:
            contents = self.fscache.listdir(dir)
        except OSError:
            contents = []
        # False positives are fine for correctness here, since we will check
        # precisely later, so we only look at the root of every filename without
        # any concern for the exact details.
        for name in contents:
            name = os.path.splitext(name)[0]
            components.setdefault(name, []).append(dir)

    self.initial_components[lib_path] = components
    return components.get(id, [])

</t>
<t tx="ekr.20221004064035.446">def find_module(self, id: str, *, fast_path: bool = False) -&gt; ModuleSearchResult:
    """Return the path of the module source file or why it wasn't found.

    If fast_path is True, prioritize performance over generating detailed
    error descriptions.
    """
    if id not in self.results:
        top_level = id.partition(".")[0]
        use_typeshed = True
        if id in self.stdlib_py_versions:
            use_typeshed = self._typeshed_has_version(id)
        elif top_level in self.stdlib_py_versions:
            use_typeshed = self._typeshed_has_version(top_level)
        self.results[id] = self._find_module(id, use_typeshed)
        if (
            not (fast_path or (self.options is not None and self.options.fast_module_lookup))
            and self.results[id] is ModuleNotFoundReason.NOT_FOUND
            and self._can_find_module_in_parent_dir(id)
        ):
            self.results[id] = ModuleNotFoundReason.WRONG_WORKING_DIRECTORY
    return self.results[id]

</t>
<t tx="ekr.20221004064035.447">def _typeshed_has_version(self, module: str) -&gt; bool:
    if not self.options:
        return True
    version = typeshed_py_version(self.options)
    min_version, max_version = self.stdlib_py_versions[module]
    return version &gt;= min_version and (max_version is None or version &lt;= max_version)

</t>
<t tx="ekr.20221004064035.448">def _find_module_non_stub_helper(
    self, components: list[str], pkg_dir: str
) -&gt; OnePackageDir | ModuleNotFoundReason:
    plausible_match = False
    dir_path = pkg_dir
    for index, component in enumerate(components):
        dir_path = os.path.join(dir_path, component)
        if self.fscache.isfile(os.path.join(dir_path, "py.typed")):
            return os.path.join(pkg_dir, *components[:-1]), index == 0
        elif not plausible_match and (
            self.fscache.isdir(dir_path) or self.fscache.isfile(dir_path + ".py")
        ):
            plausible_match = True
        # If this is not a directory then we can't traverse further into it
        if not self.fscache.isdir(dir_path):
            break
    if approved_stub_package_exists(components[0]):
        if len(components) == 1 or (
            self.find_module(components[0])
            is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
        ):
            return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    if approved_stub_package_exists(".".join(components[:2])):
        return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    if plausible_match:
        return ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS
    else:
        return ModuleNotFoundReason.NOT_FOUND

</t>
<t tx="ekr.20221004064035.449">def _update_ns_ancestors(self, components: list[str], match: tuple[str, bool]) -&gt; None:
    path, verify = match
    for i in range(1, len(components)):
        pkg_id = ".".join(components[:-i])
        if pkg_id not in self.ns_ancestors and self.fscache.isdir(path):
            self.ns_ancestors[pkg_id] = path
        path = os.path.dirname(path)

</t>
<t tx="ekr.20221004064035.45">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # We only have partial information so we can't decide the join result. We should
    # never get here.
    assert False, "Internal error"

</t>
<t tx="ekr.20221004064035.450">def _can_find_module_in_parent_dir(self, id: str) -&gt; bool:
    """Test if a module can be found by checking the parent directories
    of the current working directory.
    """
    working_dir = os.getcwd()
    parent_search = FindModuleCache(
        SearchPaths((), (), (), ()),
        self.fscache,
        self.options,
        stdlib_py_versions=self.stdlib_py_versions,
    )
    while any(is_init_file(file) for file in os.listdir(working_dir)):
        working_dir = os.path.dirname(working_dir)
        parent_search.search_paths = SearchPaths((working_dir,), (), (), ())
        if not isinstance(parent_search._find_module(id, False), ModuleNotFoundReason):
            return True
    return False

</t>
<t tx="ekr.20221004064035.451">def _find_module(self, id: str, use_typeshed: bool) -&gt; ModuleSearchResult:
    fscache = self.fscache

    # Fast path for any modules in the current source set.
    # This is particularly important when there are a large number of search
    # paths which share the first (few) component(s) due to the use of namespace
    # packages, for instance:
    # foo/
    #    company/
    #        __init__.py
    #        foo/
    # bar/
    #    company/
    #        __init__.py
    #        bar/
    # baz/
    #    company/
    #        __init__.py
    #        baz/
    #
    # mypy gets [foo/company/foo, bar/company/bar, baz/company/baz, ...] as input
    # and computes [foo, bar, baz, ...] as the module search path.
    #
    # This would result in O(n) search for every import of company.*, leading to
    # O(n**2) behavior in load_graph as such imports are unsurprisingly present
    # at least once, and usually many more times than that, in each and every file
    # being parsed.
    #
    # Thankfully, such cases are efficiently handled by looking up the module path
    # via BuildSourceSet.
    p = (
        self.find_module_via_source_set(id)
        if (self.options is not None and self.options.fast_module_lookup)
        else None
    )
    if p:
        return p

    # If we're looking for a module like 'foo.bar.baz', it's likely that most of the
    # many elements of lib_path don't even have a subdirectory 'foo/bar'.  Discover
    # that only once and cache it for when we look for modules like 'foo.bar.blah'
    # that will require the same subdirectory.
    components = id.split(".")
    dir_chain = os.sep.join(components[:-1])  # e.g., 'foo/bar'

    # We have two sets of folders so that we collect *all* stubs folders and
    # put them in the front of the search path
    third_party_inline_dirs: PackageDirs = []
    third_party_stubs_dirs: PackageDirs = []
    found_possible_third_party_missing_type_hints = False
    need_installed_stubs = False
    # Third-party stub/typed packages
    for pkg_dir in self.search_paths.package_path:
        stub_name = components[0] + "-stubs"
        stub_dir = os.path.join(pkg_dir, stub_name)
        if fscache.isdir(stub_dir) and self._is_compatible_stub_package(stub_dir):
            stub_typed_file = os.path.join(stub_dir, "py.typed")
            stub_components = [stub_name] + components[1:]
            path = os.path.join(pkg_dir, *stub_components[:-1])
            if fscache.isdir(path):
                if fscache.isfile(stub_typed_file):
                    # Stub packages can have a py.typed file, which must include
                    # 'partial\n' to make the package partial
                    # Partial here means that mypy should look at the runtime
                    # package if installed.
                    if fscache.read(stub_typed_file).decode().strip() == "partial":
                        runtime_path = os.path.join(pkg_dir, dir_chain)
                        third_party_inline_dirs.append((runtime_path, True))
                        # if the package is partial, we don't verify the module, as
                        # the partial stub package may not have a __init__.pyi
                        third_party_stubs_dirs.append((path, False))
                    else:
                        # handle the edge case where people put a py.typed file
                        # in a stub package, but it isn't partial
                        third_party_stubs_dirs.append((path, True))
                else:
                    third_party_stubs_dirs.append((path, True))
        non_stub_match = self._find_module_non_stub_helper(components, pkg_dir)
        if isinstance(non_stub_match, ModuleNotFoundReason):
            if non_stub_match is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
                found_possible_third_party_missing_type_hints = True
            elif non_stub_match is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
                need_installed_stubs = True
        else:
            third_party_inline_dirs.append(non_stub_match)
            self._update_ns_ancestors(components, non_stub_match)
    if self.options and self.options.use_builtins_fixtures:
        # Everything should be in fixtures.
        third_party_inline_dirs.clear()
        third_party_stubs_dirs.clear()
        found_possible_third_party_missing_type_hints = False
    python_mypy_path = self.search_paths.mypy_path + self.search_paths.python_path
    candidate_base_dirs = self.find_lib_path_dirs(id, python_mypy_path)
    if use_typeshed:
        # Search for stdlib stubs in typeshed before installed
        # stubs to avoid picking up backports (dataclasses, for
        # example) when the library is included in stdlib.
        candidate_base_dirs += self.find_lib_path_dirs(id, self.search_paths.typeshed_path)
    candidate_base_dirs += third_party_stubs_dirs + third_party_inline_dirs

    # If we're looking for a module like 'foo.bar.baz', then candidate_base_dirs now
    # contains just the subdirectories 'foo/bar' that actually exist under the
    # elements of lib_path.  This is probably much shorter than lib_path itself.
    # Now just look for 'baz.pyi', 'baz/__init__.py', etc., inside those directories.
    seplast = os.sep + components[-1]  # so e.g. '/baz'
    sepinit = os.sep + "__init__"
    near_misses = []  # Collect near misses for namespace mode (see below).
    for base_dir, verify in candidate_base_dirs:
        base_path = base_dir + seplast  # so e.g. '/usr/lib/python3.4/foo/bar/baz'
        has_init = False
        dir_prefix = base_dir
        for _ in range(len(components) - 1):
            dir_prefix = os.path.dirname(dir_prefix)
        # Prefer package over module, i.e. baz/__init__.py* over baz.py*.
        for extension in PYTHON_EXTENSIONS:
            path = base_path + sepinit + extension
            path_stubs = base_path + "-stubs" + sepinit + extension
            if fscache.isfile_case(path, dir_prefix):
                has_init = True
                if verify and not verify_module(fscache, id, path, dir_prefix):
                    near_misses.append((path, dir_prefix))
                    continue
                return path
            elif fscache.isfile_case(path_stubs, dir_prefix):
                if verify and not verify_module(fscache, id, path_stubs, dir_prefix):
                    near_misses.append((path_stubs, dir_prefix))
                    continue
                return path_stubs

        # In namespace mode, register a potential namespace package
        if self.options and self.options.namespace_packages:
            if fscache.exists_case(base_path, dir_prefix) and not has_init:
                near_misses.append((base_path, dir_prefix))

        # No package, look for module.
        for extension in PYTHON_EXTENSIONS:
            path = base_path + extension
            if fscache.isfile_case(path, dir_prefix):
                if verify and not verify_module(fscache, id, path, dir_prefix):
                    near_misses.append((path, dir_prefix))
                    continue
                return path

    # In namespace mode, re-check those entries that had 'verify'.
    # Assume search path entries xxx, yyy and zzz, and we're
    # looking for foo.bar.baz.  Suppose near_misses has:
    #
    # - xxx/foo/bar/baz.py
    # - yyy/foo/bar/baz/__init__.py
    # - zzz/foo/bar/baz.pyi
    #
    # If any of the foo directories has __init__.py[i], it wins.
    # Else, we look for foo/bar/__init__.py[i], etc.  If there are
    # none, the first hit wins.  Note that this does not take into
    # account whether the lowest-level module is a file (baz.py),
    # a package (baz/__init__.py), or a stub file (baz.pyi) -- for
    # these the first one encountered along the search path wins.
    #
    # The helper function highest_init_level() returns an int that
    # indicates the highest level at which a __init__.py[i] file
    # is found; if no __init__ was found it returns 0, if we find
    # only foo/bar/__init__.py it returns 1, and if we have
    # foo/__init__.py it returns 2 (regardless of what's in
    # foo/bar).  It doesn't look higher than that.
    if self.options and self.options.namespace_packages and near_misses:
        levels = [
            highest_init_level(fscache, id, path, dir_prefix)
            for path, dir_prefix in near_misses
        ]
        index = levels.index(max(levels))
        return near_misses[index][0]

    # Finally, we may be asked to produce an ancestor for an
    # installed package with a py.typed marker that is a
    # subpackage of a namespace package.  We only fess up to these
    # if we would otherwise return "not found".
    ancestor = self.ns_ancestors.get(id)
    if ancestor is not None:
        return ancestor

    if need_installed_stubs:
        return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    elif found_possible_third_party_missing_type_hints:
        return ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS
    else:
        return ModuleNotFoundReason.NOT_FOUND

</t>
<t tx="ekr.20221004064035.452">def _is_compatible_stub_package(self, stub_dir: str) -&gt; bool:
    """Does a stub package support the target Python version?

    Stub packages may contain a metadata file which specifies
    whether the stubs are compatible with Python 2 and 3.
    """
    metadata_fnam = os.path.join(stub_dir, "METADATA.toml")
    if os.path.isfile(metadata_fnam):
        with open(metadata_fnam, "rb") as f:
            metadata = tomllib.load(f)
        return bool(metadata.get("python3", True))
    return True

</t>
<t tx="ekr.20221004064035.453">def find_modules_recursive(self, module: str) -&gt; list[BuildSource]:
    module_path = self.find_module(module)
    if isinstance(module_path, ModuleNotFoundReason):
        return []
    sources = [BuildSource(module_path, module, None)]

    package_path = None
    if is_init_file(module_path):
        package_path = os.path.dirname(module_path)
    elif self.fscache.isdir(module_path):
        package_path = module_path
    if package_path is None:
        return sources

    # This logic closely mirrors that in find_sources. One small but important difference is
    # that we do not sort names with keyfunc. The recursive call to find_modules_recursive
    # calls find_module, which will handle the preference between packages, pyi and py.
    # Another difference is it doesn't handle nested search paths / package roots.

    seen: set[str] = set()
    names = sorted(self.fscache.listdir(package_path))
    for name in names:
        # Skip certain names altogether
        if name in ("__pycache__", "site-packages", "node_modules") or name.startswith("."):
            continue
        subpath = os.path.join(package_path, name)

        if self.options and matches_exclude(
            subpath, self.options.exclude, self.fscache, self.options.verbosity &gt;= 2
        ):
            continue

        if self.fscache.isdir(subpath):
            # Only recurse into packages
            if (self.options and self.options.namespace_packages) or (
                self.fscache.isfile(os.path.join(subpath, "__init__.py"))
                or self.fscache.isfile(os.path.join(subpath, "__init__.pyi"))
            ):
                seen.add(name)
                sources.extend(self.find_modules_recursive(module + "." + name))
        else:
            stem, suffix = os.path.splitext(name)
            if stem == "__init__":
                continue
            if stem not in seen and "." not in stem and suffix in PYTHON_EXTENSIONS:
                # (If we sorted names by keyfunc) we could probably just make the BuildSource
                # ourselves, but this ensures compatibility with find_module / the cache
                seen.add(stem)
                sources.extend(self.find_modules_recursive(module + "." + stem))
    return sources


</t>
<t tx="ekr.20221004064035.454">def matches_exclude(
    subpath: str, excludes: list[str], fscache: FileSystemCache, verbose: bool
) -&gt; bool:
    if not excludes:
        return False
    subpath_str = os.path.relpath(subpath).replace(os.sep, "/")
    if fscache.isdir(subpath):
        subpath_str += "/"
    for exclude in excludes:
        if re.search(exclude, subpath_str):
            if verbose:
                print(
                    f"TRACE: Excluding {subpath_str} (matches pattern {exclude})", file=sys.stderr
                )
            return True
    return False


</t>
<t tx="ekr.20221004064035.455">def is_init_file(path: str) -&gt; bool:
    return os.path.basename(path) in ("__init__.py", "__init__.pyi")


</t>
<t tx="ekr.20221004064035.456">def verify_module(fscache: FileSystemCache, id: str, path: str, prefix: str) -&gt; bool:
    """Check that all packages containing id have a __init__ file."""
    if is_init_file(path):
        path = os.path.dirname(path)
    for i in range(id.count(".")):
        path = os.path.dirname(path)
        if not any(
            fscache.isfile_case(os.path.join(path, f"__init__{extension}"), prefix)
            for extension in PYTHON_EXTENSIONS
        ):
            return False
    return True


</t>
<t tx="ekr.20221004064035.457">def highest_init_level(fscache: FileSystemCache, id: str, path: str, prefix: str) -&gt; int:
    """Compute the highest level where an __init__ file is found."""
    if is_init_file(path):
        path = os.path.dirname(path)
    level = 0
    for i in range(id.count(".")):
        path = os.path.dirname(path)
        if any(
            fscache.isfile_case(os.path.join(path, f"__init__{extension}"), prefix)
            for extension in PYTHON_EXTENSIONS
        ):
            level = i + 1
    return level


</t>
<t tx="ekr.20221004064035.458">def mypy_path() -&gt; list[str]:
    path_env = os.getenv("MYPYPATH")
    if not path_env:
        return []
    return path_env.split(os.pathsep)


</t>
<t tx="ekr.20221004064035.459">def default_lib_path(
    data_dir: str, pyversion: tuple[int, int], custom_typeshed_dir: str | None
) -&gt; list[str]:
    """Return default standard library search paths."""
    path: list[str] = []

    if custom_typeshed_dir:
        typeshed_dir = os.path.join(custom_typeshed_dir, "stdlib")
        mypy_extensions_dir = os.path.join(custom_typeshed_dir, "stubs", "mypy-extensions")
        versions_file = os.path.join(typeshed_dir, "VERSIONS")
        if not os.path.isdir(typeshed_dir) or not os.path.isfile(versions_file):
            print(
                "error: --custom-typeshed-dir does not point to a valid typeshed ({})".format(
                    custom_typeshed_dir
                )
            )
            sys.exit(2)
    else:
        auto = os.path.join(data_dir, "stubs-auto")
        if os.path.isdir(auto):
            data_dir = auto
        typeshed_dir = os.path.join(data_dir, "typeshed", "stdlib")
        mypy_extensions_dir = os.path.join(data_dir, "typeshed", "stubs", "mypy-extensions")
    path.append(typeshed_dir)

    # Get mypy-extensions stubs from typeshed, since we treat it as an
    # "internal" library, similar to typing and typing-extensions.
    path.append(mypy_extensions_dir)

    # Add fallback path that can be used if we have a broken installation.
    if sys.platform != "win32":
        path.append("/usr/local/lib/mypy")
    if not path:
        print(
            "Could not resolve typeshed subdirectories. Your mypy install is broken.\n"
            "Python executable is located at {}.\nMypy located at {}".format(
                sys.executable, data_dir
            ),
            file=sys.stderr,
        )
        sys.exit(1)
    return path


</t>
<t tx="ekr.20221004064035.46">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    if isinstance(self.s, TypeType):
        return TypeType.make_normalized(self.join(t.item, self.s.item), line=t.line)
    elif isinstance(self.s, Instance) and self.s.type.fullname == "builtins.type":
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20221004064035.460">@functools.lru_cache(maxsize=None)
def get_search_dirs(python_executable: str | None) -&gt; tuple[list[str], list[str]]:
    """Find package directories for given python.

    This runs a subprocess call, which generates a list of the directories in sys.path.
    To avoid repeatedly calling a subprocess (which can be slow!) we
    lru_cache the results.
    """

    if python_executable is None:
        return ([], [])
    elif python_executable == sys.executable:
        # Use running Python's package dirs
        sys_path, site_packages = pyinfo.getsearchdirs()
    else:
        # Use subprocess to get the package directory of given Python
        # executable
        try:
            sys_path, site_packages = ast.literal_eval(
                subprocess.check_output(
                    [python_executable, pyinfo.__file__, "getsearchdirs"], stderr=subprocess.PIPE
                ).decode()
            )
        except OSError as err:
            reason = os.strerror(err.errno)
            raise CompileError(
                [f"mypy: Invalid python executable '{python_executable}': {reason}"]
            ) from err
    return sys_path, site_packages


</t>
<t tx="ekr.20221004064035.461">def compute_search_paths(
    sources: list[BuildSource], options: Options, data_dir: str, alt_lib_path: str | None = None
) -&gt; SearchPaths:
    """Compute the search paths as specified in PEP 561.

    There are the following 4 members created:
    - User code (from `sources`)
    - MYPYPATH (set either via config or environment variable)
    - installed package directories (which will later be split into stub-only and inline)
    - typeshed
    """
    # Determine the default module search path.
    lib_path = collections.deque(
        default_lib_path(
            data_dir, options.python_version, custom_typeshed_dir=options.custom_typeshed_dir
        )
    )

    if options.use_builtins_fixtures:
        # Use stub builtins (to speed up test cases and to make them easier to
        # debug).  This is a test-only feature, so assume our files are laid out
        # as in the source tree.
        # We also need to allow overriding where to look for it. Argh.
        root_dir = os.getenv("MYPY_TEST_PREFIX", None)
        if not root_dir:
            root_dir = os.path.dirname(os.path.dirname(__file__))
        lib_path.appendleft(os.path.join(root_dir, "test-data", "unit", "lib-stub"))
    # alt_lib_path is used by some tests to bypass the normal lib_path mechanics.
    # If we don't have one, grab directories of source files.
    python_path: list[str] = []
    if not alt_lib_path:
        for source in sources:
            # Include directory of the program file in the module search path.
            if source.base_dir:
                dir = source.base_dir
                if dir not in python_path:
                    python_path.append(dir)

        # Do this even if running as a file, for sanity (mainly because with
        # multiple builds, there could be a mix of files/modules, so its easier
        # to just define the semantics that we always add the current director
        # to the lib_path
        # TODO: Don't do this in some cases; for motivation see see
        # https://github.com/python/mypy/issues/4195#issuecomment-341915031
        if options.bazel:
            dir = "."
        else:
            dir = os.getcwd()
        if dir not in lib_path:
            python_path.insert(0, dir)

    # Start with a MYPYPATH environment variable at the front of the mypy_path, if defined.
    mypypath = mypy_path()

    # Add a config-defined mypy path.
    mypypath.extend(options.mypy_path)

    # If provided, insert the caller-supplied extra module path to the
    # beginning (highest priority) of the search path.
    if alt_lib_path:
        mypypath.insert(0, alt_lib_path)

    sys_path, site_packages = get_search_dirs(options.python_executable)
    # We only use site packages for this check
    for site in site_packages:
        assert site not in lib_path
        if (
            site in mypypath
            or any(p.startswith(site + os.path.sep) for p in mypypath)
            or (os.path.altsep and any(p.startswith(site + os.path.altsep) for p in mypypath))
        ):
            print(f"{site} is in the MYPYPATH. Please remove it.", file=sys.stderr)
            print(
                "See https://mypy.readthedocs.io/en/stable/running_mypy.html"
                "#how-mypy-handles-imports for more info",
                file=sys.stderr,
            )
            sys.exit(1)

    return SearchPaths(
        python_path=tuple(reversed(python_path)),
        mypy_path=tuple(mypypath),
        package_path=tuple(sys_path + site_packages),
        typeshed_path=tuple(lib_path),
    )


</t>
<t tx="ekr.20221004064035.462">def load_stdlib_py_versions(custom_typeshed_dir: str | None) -&gt; StdlibVersions:
    """Return dict with minimum and maximum Python versions of stdlib modules.

    The contents look like
    {..., 'secrets': ((3, 6), None), 'symbol': ((2, 7), (3, 9)), ...}

    None means there is no maximum version.
    """
    typeshed_dir = custom_typeshed_dir or os.path.join(os.path.dirname(__file__), "typeshed")
    stdlib_dir = os.path.join(typeshed_dir, "stdlib")
    result = {}

    versions_path = os.path.join(stdlib_dir, "VERSIONS")
    assert os.path.isfile(versions_path), (custom_typeshed_dir, versions_path, __file__)
    with open(versions_path) as f:
        for line in f:
            line = line.split("#")[0].strip()
            if line == "":
                continue
            module, version_range = line.split(":")
            versions = version_range.split("-")
            min_version = parse_version(versions[0])
            max_version = (
                parse_version(versions[1]) if len(versions) &gt;= 2 and versions[1].strip() else None
            )
            result[module] = min_version, max_version
    return result


</t>
<t tx="ekr.20221004064035.463">def parse_version(version: str) -&gt; tuple[int, int]:
    major, minor = version.strip().split(".")
    return int(major), int(minor)


</t>
<t tx="ekr.20221004064035.464">def typeshed_py_version(options: Options) -&gt; tuple[int, int]:
    """Return Python version used for checking whether module supports typeshed."""
    # Typeshed no longer covers Python 3.x versions before 3.7, so 3.7 is
    # the earliest we can support.
    return max(options.python_version, (3, 7))
</t>
<t tx="ekr.20221004064035.465">@path C:/Repos/ekr-mypy2/mypy/
"""Basic introspection of modules."""

from __future__ import annotations

import importlib
import inspect
import os
import pkgutil
import queue
import sys
from multiprocessing import Process, Queue
from types import ModuleType


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.466">class ModuleProperties:
    @others
</t>
<t tx="ekr.20221004064035.467"># Note that all __init__ args must have default values
def __init__(
    self,
    name: str = "",
    file: str | None = None,
    path: list[str] | None = None,
    all: list[str] | None = None,
    is_c_module: bool = False,
    subpackages: list[str] | None = None,
) -&gt; None:
    self.name = name  # __name__ attribute
    self.file = file  # __file__ attribute
    self.path = path  # __path__ attribute
    self.all = all  # __all__ attribute
    self.is_c_module = is_c_module
    self.subpackages = subpackages or []


</t>
<t tx="ekr.20221004064035.468">def is_c_module(module: ModuleType) -&gt; bool:
    if module.__dict__.get("__file__") is None:
        # Could be a namespace package. These must be handled through
        # introspection, since there is no source file.
        return True
    return os.path.splitext(module.__dict__["__file__"])[-1] in [".so", ".pyd"]


</t>
<t tx="ekr.20221004064035.469">class InspectError(Exception):
    pass


</t>
<t tx="ekr.20221004064035.47">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, f"This should be never called, got {t}"

</t>
<t tx="ekr.20221004064035.470">def get_package_properties(package_id: str) -&gt; ModuleProperties:
    """Use runtime introspection to get information about a module/package."""
    try:
        package = importlib.import_module(package_id)
    except BaseException as e:
        raise InspectError(str(e)) from e
    name = getattr(package, "__name__", package_id)
    file = getattr(package, "__file__", None)
    path: list[str] | None = getattr(package, "__path__", None)
    if not isinstance(path, list):
        path = None
    pkg_all = getattr(package, "__all__", None)
    if pkg_all is not None:
        try:
            pkg_all = list(pkg_all)
        except Exception:
            pkg_all = None
    is_c = is_c_module(package)

    if path is None:
        # Object has no path; this means it's either a module inside a package
        # (and thus no sub-packages), or it could be a C extension package.
        if is_c:
            # This is a C extension module, now get the list of all sub-packages
            # using the inspect module
            subpackages = [
                package.__name__ + "." + name
                for name, val in inspect.getmembers(package)
                if inspect.ismodule(val) and val.__name__ == package.__name__ + "." + name
            ]
        else:
            # It's a module inside a package.  There's nothing else to walk/yield.
            subpackages = []
    else:
        all_packages = pkgutil.walk_packages(
            path, prefix=package.__name__ + ".", onerror=lambda r: None
        )
        subpackages = [qualified_name for importer, qualified_name, ispkg in all_packages]
    return ModuleProperties(
        name=name, file=file, path=path, all=pkg_all, is_c_module=is_c, subpackages=subpackages
    )


</t>
<t tx="ekr.20221004064035.471">def worker(tasks: Queue[str], results: Queue[str | ModuleProperties], sys_path: list[str]) -&gt; None:
    """The main loop of a worker introspection process."""
    sys.path = sys_path
    while True:
        mod = tasks.get()
        try:
            prop = get_package_properties(mod)
        except InspectError as e:
            results.put(str(e))
            continue
        results.put(prop)


</t>
<t tx="ekr.20221004064035.472">class ModuleInspect:
    """Perform runtime introspection of modules in a separate process.

    Reuse the process for multiple modules for efficiency. However, if there is an
    error, retry using a fresh process to avoid cross-contamination of state between
    modules.

    We use a separate process to isolate us from many side effects. For example, the
    import of a module may kill the current process, and we want to recover from that.

    Always use in a with statement for proper clean-up:

      with ModuleInspect() as m:
          p = m.get_package_properties('urllib.parse')
    """

    @others
</t>
<t tx="ekr.20221004064035.473">def __init__(self) -&gt; None:
    self._start()

</t>
<t tx="ekr.20221004064035.474">def _start(self) -&gt; None:
    self.tasks: Queue[str] = Queue()
    self.results: Queue[ModuleProperties | str] = Queue()
    self.proc = Process(target=worker, args=(self.tasks, self.results, sys.path))
    self.proc.start()
    self.counter = 0  # Number of successful roundtrips

</t>
<t tx="ekr.20221004064035.475">def close(self) -&gt; None:
    """Free any resources used."""
    self.proc.terminate()

</t>
<t tx="ekr.20221004064035.476">def get_package_properties(self, package_id: str) -&gt; ModuleProperties:
    """Return some properties of a module/package using runtime introspection.

    Raise InspectError if the target couldn't be imported.
    """
    self.tasks.put(package_id)
    res = self._get_from_queue()
    if res is None:
        # The process died; recover and report error.
        self._start()
        raise InspectError(f"Process died when importing {package_id!r}")
    if isinstance(res, str):
        # Error importing module
        if self.counter &gt; 0:
            # Also try with a fresh process. Maybe one of the previous imports has
            # corrupted some global state.
            self.close()
            self._start()
            return self.get_package_properties(package_id)
        raise InspectError(res)
    self.counter += 1
    return res

</t>
<t tx="ekr.20221004064035.477">def _get_from_queue(self) -&gt; ModuleProperties | str | None:
    """Get value from the queue.

    Return the value read from the queue, or None if the process unexpectedly died.
    """
    max_iter = 600
    n = 0
    while True:
        if n == max_iter:
            raise RuntimeError("Timeout waiting for subprocess")
        try:
            return self.results.get(timeout=0.05)
        except queue.Empty:
            if not self.proc.is_alive():
                return None
        n += 1

</t>
<t tx="ekr.20221004064035.478">def __enter__(self) -&gt; ModuleInspect:
    return self

</t>
<t tx="ekr.20221004064035.479">def __exit__(self, *args: object) -&gt; None:
    self.close()
</t>
<t tx="ekr.20221004064035.48">def join(self, s: Type, t: Type) -&gt; ProperType:
    return join_types(s, t)

</t>
<t tx="ekr.20221004064035.480">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Callable

from mypy.nodes import TypeInfo
from mypy.types import Instance
from mypy.typestate import TypeState


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.481">def calculate_mro(info: TypeInfo, obj_type: Callable[[], Instance] | None = None) -&gt; None:
    """Calculate and set mro (method resolution order).

    Raise MroError if cannot determine mro.
    """
    mro = linearize_hierarchy(info, obj_type)
    assert mro, f"Could not produce a MRO at all for {info}"
    info.mro = mro
    # The property of falling back to Any is inherited.
    info.fallback_to_any = any(baseinfo.fallback_to_any for baseinfo in info.mro)
    TypeState.reset_all_subtype_caches_for(info)


</t>
<t tx="ekr.20221004064035.482">class MroError(Exception):
    """Raised if a consistent mro cannot be determined for a class."""


</t>
<t tx="ekr.20221004064035.483">def linearize_hierarchy(
    info: TypeInfo, obj_type: Callable[[], Instance] | None = None
) -&gt; list[TypeInfo]:
    # TODO describe
    if info.mro:
        return info.mro
    bases = info.direct_base_classes()
    if not bases and info.fullname != "builtins.object" and obj_type is not None:
        # Probably an error, add a dummy `object` base class,
        # otherwise MRO calculation may spuriously fail.
        bases = [obj_type().type]
    lin_bases = []
    for base in bases:
        assert base is not None, f"Cannot linearize bases for {info.fullname} {bases}"
        lin_bases.append(linearize_hierarchy(base, obj_type))
    lin_bases.append(bases)
    return [info] + merge(lin_bases)


</t>
<t tx="ekr.20221004064035.484">def merge(seqs: list[list[TypeInfo]]) -&gt; list[TypeInfo]:
    seqs = [s[:] for s in seqs]
    result: list[TypeInfo] = []
    while True:
        seqs = [s for s in seqs if s]
        if not seqs:
            return result
        for seq in seqs:
            head = seq[0]
            if not [s for s in seqs if head in s[1:]]:
                break
        else:
            raise MroError()
        result.append(head)
        for s in seqs:
            if s[0] is head:
                del s[0]
</t>
<t tx="ekr.20221004064035.485">@path C:/Repos/ekr-mypy2/mypy/
"""Abstract syntax tree node classes (i.e. parse tree)."""

from __future__ import annotations

import os
from abc import abstractmethod
from collections import defaultdict
from enum import Enum, unique
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Iterator,
    Optional,
    Sequence,
    Tuple,
    TypeVar,
    Union,
    cast,
)
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy_extensions import trait

import mypy.strconv
from mypy.bogus_type import Bogus
from mypy.util import short_type
from mypy.visitor import ExpressionVisitor, NodeVisitor, StatementVisitor

if TYPE_CHECKING:
    from mypy.patterns import Pattern


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.486">class Context:
    """Base type for objects that are valid as error message locations."""

    __slots__ = ("line", "column", "end_line", "end_column")

    @others
</t>
<t tx="ekr.20221004064035.487">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    self.line = line
    self.column = column
    self.end_line: int | None = None
    self.end_column: int | None = None

</t>
<t tx="ekr.20221004064035.488">def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    """If target is a node, pull line (and column) information
    into this node. If column is specified, this will override any column
    information coming from a node.
    """
    if isinstance(target, int):
        self.line = target
    else:
        self.line = target.line
        self.column = target.column
        self.end_line = target.end_line
        self.end_column = target.end_column

    if column is not None:
        self.column = column

    if end_line is not None:
        self.end_line = end_line

    if end_column is not None:
        self.end_column = end_column

</t>
<t tx="ekr.20221004064035.489">def get_line(self) -&gt; int:
    """Don't use. Use x.line."""
    return self.line

</t>
<t tx="ekr.20221004064035.49">def default(self, typ: Type) -&gt; ProperType:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return object_from_instance(typ)
    elif isinstance(typ, UnboundType):
        return AnyType(TypeOfAny.special_form)
    elif isinstance(typ, TupleType):
        return self.default(mypy.typeops.tuple_fallback(typ))
    elif isinstance(typ, TypedDictType):
        return self.default(typ.fallback)
    elif isinstance(typ, FunctionLike):
        return self.default(typ.fallback)
    elif isinstance(typ, TypeVarType):
        return self.default(typ.upper_bound)
    elif isinstance(typ, ParamSpecType):
        return self.default(typ.upper_bound)
    else:
        return AnyType(TypeOfAny.special_form)


</t>
<t tx="ekr.20221004064035.490">def get_column(self) -&gt; int:
    """Don't use. Use x.column."""
    return self.column


</t>
<t tx="ekr.20221004064035.491">if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.types


T = TypeVar("T")

JsonDict: _TypeAlias = Dict[str, Any]


# Symbol table node kinds
#
# TODO rename to use more descriptive names

LDEF: Final = 0
GDEF: Final = 1
MDEF: Final = 2

# Placeholder for a name imported via 'from ... import'. Second phase of
# semantic will replace this the actual imported reference. This is
# needed so that we can detect whether a name has been imported during
# XXX what?
UNBOUND_IMPORTED: Final = 3

# RevealExpr node kinds
REVEAL_TYPE: Final = 0
REVEAL_LOCALS: Final = 1

LITERAL_YES: Final = 2
LITERAL_TYPE: Final = 1
LITERAL_NO: Final = 0

node_kinds: Final = {LDEF: "Ldef", GDEF: "Gdef", MDEF: "Mdef", UNBOUND_IMPORTED: "UnboundImported"}
inverse_node_kinds: Final = {_kind: _name for _name, _kind in node_kinds.items()}


implicit_module_attrs: Final = {
    "__name__": "__builtins__.str",
    "__doc__": None,  # depends on Python version, see semanal.py
    "__path__": None,  # depends on if the module is a package
    "__file__": "__builtins__.str",
    "__package__": "__builtins__.str",
    "__annotations__": None,  # dict[str, Any] bounded in add_implicit_module_attrs()
}


# These aliases exist because built-in class objects are not subscriptable.
# For example `list[int]` fails at runtime. Instead List[int] should be used.
type_aliases: Final = {
    "typing.List": "builtins.list",
    "typing.Dict": "builtins.dict",
    "typing.Set": "builtins.set",
    "typing.FrozenSet": "builtins.frozenset",
    "typing.ChainMap": "collections.ChainMap",
    "typing.Counter": "collections.Counter",
    "typing.DefaultDict": "collections.defaultdict",
    "typing.Deque": "collections.deque",
    "typing.OrderedDict": "collections.OrderedDict",
    # HACK: a lie in lieu of actual support for PEP 675
    "typing.LiteralString": "builtins.str",
}

# This keeps track of the oldest supported Python version where the corresponding
# alias source is available.
type_aliases_source_versions: Final = {
    "typing.List": (2, 7),
    "typing.Dict": (2, 7),
    "typing.Set": (2, 7),
    "typing.FrozenSet": (2, 7),
    "typing.ChainMap": (3, 3),
    "typing.Counter": (2, 7),
    "typing.DefaultDict": (2, 7),
    "typing.Deque": (2, 7),
    "typing.OrderedDict": (3, 7),
    "typing.LiteralString": (3, 11),
}

# This keeps track of aliases in `typing_extensions`, which we treat specially.
typing_extensions_aliases: Final = {
    # See: https://github.com/python/mypy/issues/11528
    "typing_extensions.OrderedDict": "collections.OrderedDict",
    # HACK: a lie in lieu of actual support for PEP 675
    "typing_extensions.LiteralString": "builtins.str",
}

reverse_builtin_aliases: Final = {
    "builtins.list": "typing.List",
    "builtins.dict": "typing.Dict",
    "builtins.set": "typing.Set",
    "builtins.frozenset": "typing.FrozenSet",
}

_nongen_builtins: Final = {"builtins.tuple": "typing.Tuple", "builtins.enumerate": ""}
_nongen_builtins.update((name, alias) for alias, name in type_aliases.items())
# Drop OrderedDict from this for backward compatibility
del _nongen_builtins["collections.OrderedDict"]
# HACK: consequence of hackily treating LiteralString as an alias for str
del _nongen_builtins["builtins.str"]


</t>
<t tx="ekr.20221004064035.492">def get_nongen_builtins(python_version: tuple[int, int]) -&gt; dict[str, str]:
    # After 3.9 with pep585 generic builtins are allowed.
    return _nongen_builtins if python_version &lt; (3, 9) else {}


</t>
<t tx="ekr.20221004064035.493">RUNTIME_PROTOCOL_DECOS: Final = (
    "typing.runtime_checkable",
    "typing_extensions.runtime",
    "typing_extensions.runtime_checkable",
)


</t>
<t tx="ekr.20221004064035.494">class Node(Context):
    """Common base class for all non-type parse tree nodes."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.495">def __str__(self) -&gt; str:
    ans = self.accept(mypy.strconv.StrConv())
    if ans is None:
        return repr(self)
    return ans

</t>
<t tx="ekr.20221004064035.496">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")


</t>
<t tx="ekr.20221004064035.497">@trait
class Statement(Node):
    """A statement node."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.498">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")


</t>
<t tx="ekr.20221004064035.499">@trait
class Expression(Node):
    """An expression node."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.5">def write(self, data: bytes) -&gt; None:
    """Write bytes to an IPC connection."""
    if sys.platform == "win32":
        try:
            ov, err = _winapi.WriteFile(self.connection, data, overlapped=True)
            # TODO: remove once typeshed supports Literal types
            assert isinstance(ov, _winapi.Overlapped)
            assert isinstance(err, int)
            try:
                if err == _winapi.ERROR_IO_PENDING:
                    timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                    res = _winapi.WaitForSingleObject(ov.event, timeout)
                    if res != _winapi.WAIT_OBJECT_0:
                        raise IPCException(f"Bad result from I/O wait: {res}")
                elif err != 0:
                    raise IPCException(f"Failed writing to pipe with error: {err}")
            except BaseException:
                ov.cancel()
                raise
            bytes_written, err = ov.GetOverlappedResult(True)
            assert err == 0, err
            assert bytes_written == len(data)
        except OSError as e:
            raise IPCException(f"Failed to write with error: {e.winerror}") from e
    else:
        self.connection.sendall(data)
        self.connection.shutdown(socket.SHUT_WR)

</t>
<t tx="ekr.20221004064035.50">def is_better(t: Type, s: Type) -&gt; bool:
    # Given two possible results from join_instances_via_supertype(),
    # indicate whether t is the better one.
    t = get_proper_type(t)
    s = get_proper_type(s)

    if isinstance(t, Instance):
        if not isinstance(s, Instance):
            return True
        # Use len(mro) as a proxy for the better choice.
        if len(t.type.mro) &gt; len(s.type.mro):
            return True
    return False


</t>
<t tx="ekr.20221004064035.500">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")


</t>
<t tx="ekr.20221004064035.501">class FakeExpression(Expression):
    """A dummy expression.

    We need a dummy expression in one place, and can't instantiate Expression
    because it is a trait and mypyc barfs.
    """

    __slots__ = ()


</t>
<t tx="ekr.20221004064035.502"># TODO:
# Lvalue = Union['NameExpr', 'MemberExpr', 'IndexExpr', 'SuperExpr', 'StarExpr'
#                'TupleExpr']; see #1783.
Lvalue: _TypeAlias = Expression


</t>
<t tx="ekr.20221004064035.503">@trait
class SymbolNode(Node):
    """Nodes that can be stored in a symbol table."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.504">@property
@abstractmethod
def name(self) -&gt; str:
    pass

</t>
<t tx="ekr.20221004064035.505"># fullname can often be None even though the type system
# disagrees. We mark this with Bogus to let mypyc know not to
# worry about it.
@property
@abstractmethod
def fullname(self) -&gt; Bogus[str]:
    pass

</t>
<t tx="ekr.20221004064035.506">@abstractmethod
def serialize(self) -&gt; JsonDict:
    pass

</t>
<t tx="ekr.20221004064035.507">@classmethod
def deserialize(cls, data: JsonDict) -&gt; SymbolNode:
    classname = data[".class"]
    method = deserialize_map.get(classname)
    if method is not None:
        return method(data)
    raise NotImplementedError(f"unexpected .class {classname}")


</t>
<t tx="ekr.20221004064035.508"># Items: fullname, related symbol table node, surrounding type (if any)
Definition: _TypeAlias = Tuple[str, "SymbolTableNode", Optional["TypeInfo"]]


</t>
<t tx="ekr.20221004064035.509">class MypyFile(SymbolNode):
    """The abstract syntax tree of a single source file."""

    __slots__ = (
        "_fullname",
        "path",
        "defs",
        "alias_deps",
        "is_bom",
        "names",
        "imports",
        "ignored_lines",
        "is_stub",
        "is_cache_skeleton",
        "is_partial_stub_package",
        "plugin_deps",
        "future_import_flags",
    )

    # Fully qualified module name
    _fullname: Bogus[str]
    # Path to the file (empty string if not known)
    path: str
    # Top-level definitions and statements
    defs: list[Statement]
    # Type alias dependencies as mapping from target to set of alias full names
    alias_deps: defaultdict[str, set[str]]
    # Is there a UTF-8 BOM at the start?
    is_bom: bool
    names: SymbolTable
    # All import nodes within the file (also ones within functions etc.)
    imports: list[ImportBase]
    # Lines on which to ignore certain errors when checking.
    # If the value is empty, ignore all errors; otherwise, the list contains all
    # error codes to ignore.
    ignored_lines: dict[int, list[str]]
    # Is this file represented by a stub file (.pyi)?
    is_stub: bool
    # Is this loaded from the cache and thus missing the actual body of the file?
    is_cache_skeleton: bool
    # Does this represent an __init__.pyi stub with a module __getattr__
    # (i.e. a partial stub package), for such packages we suppress any missing
    # module errors in addition to missing attribute errors.
    is_partial_stub_package: bool
    # Plugin-created dependencies
    plugin_deps: dict[str, set[str]]
    # Future imports defined in this file. Populated during semantic analysis.
    future_import_flags: set[str]

    @others
</t>
<t tx="ekr.20221004064035.51">def normalize_callables(s: ProperType, t: ProperType) -&gt; tuple[ProperType, ProperType]:
    if isinstance(s, (CallableType, Overloaded)):
        s = s.with_unpacked_kwargs()
    if isinstance(t, (CallableType, Overloaded)):
        t = t.with_unpacked_kwargs()
    return s, t


</t>
<t tx="ekr.20221004064035.510">def __init__(
    self,
    defs: list[Statement],
    imports: list[ImportBase],
    is_bom: bool = False,
    ignored_lines: dict[int, list[str]] | None = None,
) -&gt; None:
    super().__init__()
    self.defs = defs
    self.line = 1  # Dummy line number
    self.column = 0  # Dummy column
    self.imports = imports
    self.is_bom = is_bom
    self.alias_deps = defaultdict(set)
    self.plugin_deps = {}
    if ignored_lines:
        self.ignored_lines = ignored_lines
    else:
        self.ignored_lines = {}

    self.path = ""
    self.is_stub = False
    self.is_cache_skeleton = False
    self.is_partial_stub_package = False
    self.future_import_flags = set()

</t>
<t tx="ekr.20221004064035.511">def local_definitions(self) -&gt; Iterator[Definition]:
    """Return all definitions within the module (including nested).

    This doesn't include imported definitions.
    """
    return local_definitions(self.names, self.fullname)

</t>
<t tx="ekr.20221004064035.512">@property
def name(self) -&gt; str:
    return "" if not self._fullname else self._fullname.split(".")[-1]

</t>
<t tx="ekr.20221004064035.513">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20221004064035.514">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_mypy_file(self)

</t>
<t tx="ekr.20221004064035.515">def is_package_init_file(self) -&gt; bool:
    return len(self.path) != 0 and os.path.basename(self.path).startswith("__init__.")

</t>
<t tx="ekr.20221004064035.516">def is_future_flag_set(self, flag: str) -&gt; bool:
    return flag in self.future_import_flags

</t>
<t tx="ekr.20221004064035.517">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "MypyFile",
        "_fullname": self._fullname,
        "names": self.names.serialize(self._fullname),
        "is_stub": self.is_stub,
        "path": self.path,
        "is_partial_stub_package": self.is_partial_stub_package,
        "future_import_flags": list(self.future_import_flags),
    }

</t>
<t tx="ekr.20221004064035.518">@classmethod
def deserialize(cls, data: JsonDict) -&gt; MypyFile:
    assert data[".class"] == "MypyFile", data
    tree = MypyFile([], [])
    tree._fullname = data["_fullname"]
    tree.names = SymbolTable.deserialize(data["names"])
    tree.is_stub = data["is_stub"]
    tree.path = data["path"]
    tree.is_partial_stub_package = data["is_partial_stub_package"]
    tree.is_cache_skeleton = True
    tree.future_import_flags = set(data["future_import_flags"])
    return tree


</t>
<t tx="ekr.20221004064035.519">class ImportBase(Statement):
    """Base class for all import statements."""

    __slots__ = ("is_unreachable", "is_top_level", "is_mypy_only", "assignments")

    is_unreachable: bool  # Set by semanal.SemanticAnalyzerPass1 if inside `if False` etc.
    is_top_level: bool  # Ditto if outside any class or def
    is_mypy_only: bool  # Ditto if inside `if TYPE_CHECKING` or `if MYPY`

    # If an import replaces existing definitions, we construct dummy assignment
    # statements that assign the imported names to the names in the current scope,
    # for type checking purposes. Example:
    #
    #     x = 1
    #     from m import x   &lt;-- add assignment representing "x = m.x"
    assignments: list[AssignmentStmt]

    @others
</t>
<t tx="ekr.20221004064035.52">def is_similar_callables(t: CallableType, s: CallableType) -&gt; bool:
    """Return True if t and s have identical numbers of
    arguments, default arguments and varargs.
    """
    return (
        len(t.arg_types) == len(s.arg_types)
        and t.min_args == s.min_args
        and t.is_var_arg == s.is_var_arg
    )


</t>
<t tx="ekr.20221004064035.520">def __init__(self) -&gt; None:
    super().__init__()
    self.assignments = []
    self.is_unreachable = False
    self.is_top_level = False
    self.is_mypy_only = False


</t>
<t tx="ekr.20221004064035.521">class Import(ImportBase):
    """import m [as n]"""

    __slots__ = ("ids",)

    ids: list[tuple[str, str | None]]  # (module id, as id)

    @others
</t>
<t tx="ekr.20221004064035.522">def __init__(self, ids: list[tuple[str, str | None]]) -&gt; None:
    super().__init__()
    self.ids = ids

</t>
<t tx="ekr.20221004064035.523">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import(self)


</t>
<t tx="ekr.20221004064035.524">class ImportFrom(ImportBase):
    """from m import x [as y], ..."""

    __slots__ = ("id", "names", "relative")

    id: str
    relative: int
    names: list[tuple[str, str | None]]  # Tuples (name, as name)

    @others
</t>
<t tx="ekr.20221004064035.525">def __init__(self, id: str, relative: int, names: list[tuple[str, str | None]]) -&gt; None:
    super().__init__()
    self.id = id
    self.names = names
    self.relative = relative

</t>
<t tx="ekr.20221004064035.526">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import_from(self)


</t>
<t tx="ekr.20221004064035.527">class ImportAll(ImportBase):
    """from m import *"""

    __slots__ = ("id", "relative", "imported_names")

    id: str
    relative: int
    # NOTE: Only filled and used by old semantic analyzer.
    imported_names: list[str]

    @others
</t>
<t tx="ekr.20221004064035.528">def __init__(self, id: str, relative: int) -&gt; None:
    super().__init__()
    self.id = id
    self.relative = relative
    self.imported_names = []

</t>
<t tx="ekr.20221004064035.529">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import_all(self)


</t>
<t tx="ekr.20221004064035.53">def join_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    from mypy.meet import meet_types

    arg_types: list[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(meet_types(t.arg_types[i], s.arg_types[i]))
    # TODO in combine_similar_callables also applies here (names and kinds; user metaclasses)
    # The fallback type can be either 'function', 'type', or some user-provided metaclass.
    # The result should always use 'function' as a fallback if either operands are using it.
    if t.fallback.type.fullname == "builtins.function":
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(
        arg_types=arg_types,
        arg_names=combine_arg_names(t, s),
        ret_type=join_types(t.ret_type, s.ret_type),
        fallback=fallback,
        name=None,
    )


</t>
<t tx="ekr.20221004064035.530">class ImportedName(SymbolNode):
    """Indirect reference to a fullname stored in symbol table.

    This node is not present in the original program as such. This is
    just a temporary artifact in binding imported names. After semantic
    analysis pass 2, these references should be replaced with direct
    reference to a real AST node.

    Note that this is neither a Statement nor an Expression so this
    can't be visited.
    """

    __slots__ = ("target_fullname",)

    @others
</t>
<t tx="ekr.20221004064035.531">def __init__(self, target_fullname: str) -&gt; None:
    super().__init__()
    self.target_fullname = target_fullname

</t>
<t tx="ekr.20221004064035.532">@property
def name(self) -&gt; str:
    return self.target_fullname.split(".")[-1]

</t>
<t tx="ekr.20221004064035.533">@property
def fullname(self) -&gt; str:
    return self.target_fullname

</t>
<t tx="ekr.20221004064035.534">def serialize(self) -&gt; JsonDict:
    assert False, "ImportedName leaked from semantic analysis"

</t>
<t tx="ekr.20221004064035.535">@classmethod
def deserialize(cls, data: JsonDict) -&gt; ImportedName:
    assert False, "ImportedName should never be serialized"

</t>
<t tx="ekr.20221004064035.536">def __str__(self) -&gt; str:
    return f"ImportedName({self.target_fullname})"


</t>
<t tx="ekr.20221004064035.537">FUNCBASE_FLAGS: Final = ["is_property", "is_class", "is_static", "is_final"]


</t>
<t tx="ekr.20221004064035.538">class FuncBase(Node):
    """Abstract base class for function-like nodes.

    N.B: Although this has SymbolNode subclasses (FuncDef,
    OverloadedFuncDef), avoid calling isinstance(..., FuncBase) on
    something that is typed as SymbolNode.  This is to work around
    mypy bug #3603, in which mypy doesn't understand multiple
    inheritance very well, and will assume that a SymbolNode
    cannot be a FuncBase.

    Instead, test against SYMBOL_FUNCBASE_TYPES, which enumerates
    SymbolNode subclasses that are also FuncBase subclasses.
    """

    __slots__ = (
        "type",
        "unanalyzed_type",
        "info",
        "is_property",
        "is_class",  # Uses "@classmethod" (explicit or implicit)
        "is_static",  # Uses "@staticmethod"
        "is_final",  # Uses "@final"
        "_fullname",
    )

    @others
</t>
<t tx="ekr.20221004064035.539">def __init__(self) -&gt; None:
    super().__init__()
    # Type signature. This is usually CallableType or Overloaded, but it can be
    # something else for decorated functions.
    self.type: mypy.types.ProperType | None = None
    # Original, not semantically analyzed type (used for reprocessing)
    self.unanalyzed_type: mypy.types.ProperType | None = None
    # If method, reference to TypeInfo
    # TODO: Type should be Optional[TypeInfo]
    self.info = FUNC_NO_INFO
    self.is_property = False
    self.is_class = False
    self.is_static = False
    self.is_final = False
    # Name with module prefix
    # TODO: Type should be Optional[str]
    self._fullname = cast(Bogus[str], None)

</t>
<t tx="ekr.20221004064035.54">def combine_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    arg_types: list[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(join_types(t.arg_types[i], s.arg_types[i]))
    # TODO kinds and argument names
    # TODO what should happen if one fallback is 'type' and the other is a user-provided metaclass?
    # The fallback type can be either 'function', 'type', or some user-provided metaclass.
    # The result should always use 'function' as a fallback if either operands are using it.
    if t.fallback.type.fullname == "builtins.function":
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(
        arg_types=arg_types,
        arg_names=combine_arg_names(t, s),
        ret_type=join_types(t.ret_type, s.ret_type),
        fallback=fallback,
        name=None,
    )


</t>
<t tx="ekr.20221004064035.540">@property
@abstractmethod
def name(self) -&gt; str:
    pass

</t>
<t tx="ekr.20221004064035.541">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname


</t>
<t tx="ekr.20221004064035.542">OverloadPart: _TypeAlias = Union["FuncDef", "Decorator"]


</t>
<t tx="ekr.20221004064035.543">class OverloadedFuncDef(FuncBase, SymbolNode, Statement):
    """A logical node representing all the variants of a multi-declaration function.

    A multi-declaration function is often an @overload, but can also be a
    @property with a setter and a/or a deleter.

    This node has no explicit representation in the source program.
    Overloaded variants must be consecutive in the source file.
    """

    __slots__ = ("items", "unanalyzed_items", "impl")

    items: list[OverloadPart]
    unanalyzed_items: list[OverloadPart]
    impl: OverloadPart | None

    @others
</t>
<t tx="ekr.20221004064035.544">def __init__(self, items: list[OverloadPart]) -&gt; None:
    super().__init__()
    self.items = items
    self.unanalyzed_items = items.copy()
    self.impl = None
    if len(items) &gt; 0:
        # TODO: figure out how to reliably set end position (we don't know the impl here).
        self.set_line(items[0].line, items[0].column)
    self.is_final = False

</t>
<t tx="ekr.20221004064035.545">@property
def name(self) -&gt; str:
    if self.items:
        return self.items[0].name
    else:
        # This may happen for malformed overload
        assert self.impl is not None
        return self.impl.name

</t>
<t tx="ekr.20221004064035.546">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_overloaded_func_def(self)

</t>
<t tx="ekr.20221004064035.547">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "OverloadedFuncDef",
        "items": [i.serialize() for i in self.items],
        "type": None if self.type is None else self.type.serialize(),
        "fullname": self._fullname,
        "impl": None if self.impl is None else self.impl.serialize(),
        "flags": get_flags(self, FUNCBASE_FLAGS),
    }

</t>
<t tx="ekr.20221004064035.548">@classmethod
def deserialize(cls, data: JsonDict) -&gt; OverloadedFuncDef:
    assert data[".class"] == "OverloadedFuncDef"
    res = OverloadedFuncDef(
        [cast(OverloadPart, SymbolNode.deserialize(d)) for d in data["items"]]
    )
    if data.get("impl") is not None:
        res.impl = cast(OverloadPart, SymbolNode.deserialize(data["impl"]))
        # set line for empty overload items, as not set in __init__
        if len(res.items) &gt; 0:
            res.set_line(res.impl.line)
    if data.get("type") is not None:
        typ = mypy.types.deserialize_type(data["type"])
        assert isinstance(typ, mypy.types.ProperType)
        res.type = typ
    res._fullname = data["fullname"]
    set_flags(res, data["flags"])
    # NOTE: res.info will be set in the fixup phase.
    return res


</t>
<t tx="ekr.20221004064035.549">class Argument(Node):
    """A single argument in a FuncItem."""

    __slots__ = ("variable", "type_annotation", "initializer", "kind", "pos_only")

    @others
</t>
<t tx="ekr.20221004064035.55">def combine_arg_names(t: CallableType, s: CallableType) -&gt; list[str | None]:
    """Produces a list of argument names compatible with both callables.

    For example, suppose 't' and 's' have the following signatures:

    - t: (a: int, b: str, X: str) -&gt; None
    - s: (a: int, b: str, Y: str) -&gt; None

    This function would return ["a", "b", None]. This information
    is then used above to compute the join of t and s, which results
    in a signature of (a: int, b: str, str) -&gt; None.

    Note that the third argument's name is omitted and 't' and 's'
    are both valid subtypes of this inferred signature.

    Precondition: is_similar_types(t, s) is true.
    """
    num_args = len(t.arg_types)
    new_names = []
    for i in range(num_args):
        t_name = t.arg_names[i]
        s_name = s.arg_names[i]
        if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
            new_names.append(t_name)
        else:
            new_names.append(None)
    return new_names


</t>
<t tx="ekr.20221004064035.550">def __init__(
    self,
    variable: Var,
    type_annotation: mypy.types.Type | None,
    initializer: Expression | None,
    kind: ArgKind,
    pos_only: bool = False,
) -&gt; None:
    super().__init__()
    self.variable = variable
    self.type_annotation = type_annotation
    self.initializer = initializer
    self.kind = kind  # must be an ARG_* constant
    self.pos_only = pos_only

</t>
<t tx="ekr.20221004064035.551">def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    super().set_line(target, column, end_line, end_column)

    if self.initializer and self.initializer.line &lt; 0:
        self.initializer.set_line(self.line, self.column, self.end_line, self.end_column)

    self.variable.set_line(self.line, self.column, self.end_line, self.end_column)


</t>
<t tx="ekr.20221004064035.552">FUNCITEM_FLAGS: Final = FUNCBASE_FLAGS + [
    "is_overload",
    "is_generator",
    "is_coroutine",
    "is_async_generator",
    "is_awaitable_coroutine",
]


</t>
<t tx="ekr.20221004064035.553">class FuncItem(FuncBase):
    """Base class for nodes usable as overloaded function items."""

    __slots__ = (
        "arguments",  # Note that can be unset if deserialized (type is a lie!)
        "arg_names",  # Names of arguments
        "arg_kinds",  # Kinds of arguments
        "min_args",  # Minimum number of arguments
        "max_pos",  # Maximum number of positional arguments, -1 if no explicit
        # limit (*args not included)
        "body",  # Body of the function
        "is_overload",  # Is this an overload variant of function with more than
        # one overload variant?
        "is_generator",  # Contains a yield statement?
        "is_coroutine",  # Defined using 'async def' syntax?
        "is_async_generator",  # Is an async def generator?
        "is_awaitable_coroutine",  # Decorated with '@{typing,asyncio}.coroutine'?
        "expanded",  # Variants of function with type variables with values expanded
    )

    __deletable__ = ("arguments", "max_pos", "min_args")

    @others
</t>
<t tx="ekr.20221004064035.554">def __init__(
    self,
    arguments: list[Argument] | None = None,
    body: Block | None = None,
    typ: mypy.types.FunctionLike | None = None,
) -&gt; None:
    super().__init__()
    self.arguments = arguments or []
    self.arg_names = [None if arg.pos_only else arg.variable.name for arg in self.arguments]
    self.arg_kinds: list[ArgKind] = [arg.kind for arg in self.arguments]
    self.max_pos: int = self.arg_kinds.count(ARG_POS) + self.arg_kinds.count(ARG_OPT)
    self.body: Block = body or Block([])
    self.type = typ
    self.unanalyzed_type = typ
    self.is_overload: bool = False
    self.is_generator: bool = False
    self.is_coroutine: bool = False
    self.is_async_generator: bool = False
    self.is_awaitable_coroutine: bool = False
    self.expanded: list[FuncItem] = []

    self.min_args = 0
    for i in range(len(self.arguments)):
        if self.arguments[i] is None and i &lt; self.max_fixed_argc():
            self.min_args = i + 1

</t>
<t tx="ekr.20221004064035.555">def max_fixed_argc(self) -&gt; int:
    return self.max_pos

</t>
<t tx="ekr.20221004064035.556">def set_line(
    self,
    target: Context | int,
    column: int | None = None,
    end_line: int | None = None,
    end_column: int | None = None,
) -&gt; None:
    super().set_line(target, column, end_line, end_column)
    for arg in self.arguments:
        # TODO: set arguments line/column to their precise locations.
        arg.set_line(self.line, self.column, self.end_line, end_column)

</t>
<t tx="ekr.20221004064035.557">def is_dynamic(self) -&gt; bool:
    return self.type is None


</t>
<t tx="ekr.20221004064035.558">FUNCDEF_FLAGS: Final = FUNCITEM_FLAGS + [
    "is_decorated",
    "is_conditional",
    "is_trivial_body",
    "is_mypy_only",
]

# Abstract status of a function
NOT_ABSTRACT: Final = 0
# Explicitly abstract (with @abstractmethod or overload without implementation)
IS_ABSTRACT: Final = 1
# Implicitly abstract: used for functions with trivial bodies defined in Protocols
IMPLICITLY_ABSTRACT: Final = 2


</t>
<t tx="ekr.20221004064035.559">class FuncDef(FuncItem, SymbolNode, Statement):
    """Function definition.

    This is a non-lambda function defined using 'def'.
    """

    __slots__ = (
        "_name",
        "is_decorated",
        "is_conditional",
        "abstract_status",
        "original_def",
        "deco_line",
        "is_trivial_body",
        "is_mypy_only",
    )

    @others
</t>
<t tx="ekr.20221004064035.56">def object_from_instance(instance: Instance) -&gt; Instance:
    """Construct the type 'builtins.object' from an instance type."""
    # Use the fact that 'object' is always the last class in the mro.
    res = Instance(instance.type.mro[-1], [])
    return res


</t>
<t tx="ekr.20221004064035.560"># Note that all __init__ args must have default values
def __init__(
    self,
    name: str = "",  # Function name
    arguments: list[Argument] | None = None,
    body: Block | None = None,
    typ: mypy.types.FunctionLike | None = None,
) -&gt; None:
    super().__init__(arguments, body, typ)
    self._name = name
    self.is_decorated = False
    self.is_conditional = False  # Defined conditionally (within block)?
    self.abstract_status = NOT_ABSTRACT
    # Is this an abstract method with trivial body?
    # Such methods can't be called via super().
    self.is_trivial_body = False
    self.is_final = False
    # Original conditional definition
    self.original_def: None | FuncDef | Var | Decorator = None
    # Used for error reporting (to keep backward compatibility with pre-3.8)
    self.deco_line: int | None = None
    # Definitions that appear in if TYPE_CHECKING are marked with this flag.
    self.is_mypy_only = False

</t>
<t tx="ekr.20221004064035.561">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20221004064035.562">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_func_def(self)

</t>
<t tx="ekr.20221004064035.563">def serialize(self) -&gt; JsonDict:
    # We're deliberating omitting arguments and storing only arg_names and
    # arg_kinds for space-saving reasons (arguments is not used in later
    # stages of mypy).
    # TODO: After a FuncDef is deserialized, the only time we use `arg_names`
    # and `arg_kinds` is when `type` is None and we need to infer a type. Can
    # we store the inferred type ahead of time?
    return {
        ".class": "FuncDef",
        "name": self._name,
        "fullname": self._fullname,
        "arg_names": self.arg_names,
        "arg_kinds": [int(x.value) for x in self.arg_kinds],
        "type": None if self.type is None else self.type.serialize(),
        "flags": get_flags(self, FUNCDEF_FLAGS),
        "abstract_status": self.abstract_status,
        # TODO: Do we need expanded, original_def?
    }

</t>
<t tx="ekr.20221004064035.564">@classmethod
def deserialize(cls, data: JsonDict) -&gt; FuncDef:
    assert data[".class"] == "FuncDef"
    body = Block([])
    ret = FuncDef(
        data["name"],
        [],
        body,
        (
            None
            if data["type"] is None
            else cast(mypy.types.FunctionLike, mypy.types.deserialize_type(data["type"]))
        ),
    )
    ret._fullname = data["fullname"]
    set_flags(ret, data["flags"])
    # NOTE: ret.info is set in the fixup phase.
    ret.arg_names = data["arg_names"]
    ret.arg_kinds = [ArgKind(x) for x in data["arg_kinds"]]
    ret.abstract_status = data["abstract_status"]
    # Leave these uninitialized so that future uses will trigger an error
    del ret.arguments
    del ret.max_pos
    del ret.min_args
    return ret


</t>
<t tx="ekr.20221004064035.565"># All types that are both SymbolNodes and FuncBases. See the FuncBase
# docstring for the rationale.
SYMBOL_FUNCBASE_TYPES = (OverloadedFuncDef, FuncDef)


</t>
<t tx="ekr.20221004064035.566">class Decorator(SymbolNode, Statement):
    """A decorated function.

    A single Decorator object can include any number of function decorators.
    """

    __slots__ = ("func", "decorators", "original_decorators", "var", "is_overload")

    func: FuncDef  # Decorated function
    decorators: list[Expression]  # Decorators (may be empty)
    # Some decorators are removed by semanal, keep the original here.
    original_decorators: list[Expression]
    # TODO: This is mostly used for the type; consider replacing with a 'type' attribute
    var: Var  # Represents the decorated function obj
    is_overload: bool

    @others
</t>
<t tx="ekr.20221004064035.567">def __init__(self, func: FuncDef, decorators: list[Expression], var: Var) -&gt; None:
    super().__init__()
    self.func = func
    self.decorators = decorators
    self.original_decorators = decorators.copy()
    self.var = var
    self.is_overload = False

</t>
<t tx="ekr.20221004064035.568">@property
def name(self) -&gt; str:
    return self.func.name

</t>
<t tx="ekr.20221004064035.569">@property
def fullname(self) -&gt; Bogus[str]:
    return self.func.fullname

</t>
<t tx="ekr.20221004064035.57">def object_or_any_from_type(typ: ProperType) -&gt; ProperType:
    # Similar to object_from_instance() but tries hard for all types.
    # TODO: find a better way to get object, or make this more reliable.
    if isinstance(typ, Instance):
        return object_from_instance(typ)
    elif isinstance(typ, (CallableType, TypedDictType, LiteralType)):
        return object_from_instance(typ.fallback)
    elif isinstance(typ, TupleType):
        return object_from_instance(typ.partial_fallback)
    elif isinstance(typ, TypeType):
        return object_or_any_from_type(typ.item)
    elif isinstance(typ, TypeVarType) and isinstance(typ.upper_bound, ProperType):
        return object_or_any_from_type(typ.upper_bound)
    elif isinstance(typ, UnionType):
        for item in typ.items:
            if isinstance(item, ProperType):
                candidate = object_or_any_from_type(item)
                if isinstance(candidate, Instance):
                    return candidate
    return AnyType(TypeOfAny.implementation_artifact)


</t>
<t tx="ekr.20221004064035.570">@property
def is_final(self) -&gt; bool:
    return self.func.is_final

</t>
<t tx="ekr.20221004064035.571">@property
def info(self) -&gt; TypeInfo:
    return self.func.info

</t>
<t tx="ekr.20221004064035.572">@property
def type(self) -&gt; mypy.types.Type | None:
    return self.var.type

</t>
<t tx="ekr.20221004064035.573">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_decorator(self)

</t>
<t tx="ekr.20221004064035.574">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "Decorator",
        "func": self.func.serialize(),
        "var": self.var.serialize(),
        "is_overload": self.is_overload,
    }

</t>
<t tx="ekr.20221004064035.575">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Decorator:
    assert data[".class"] == "Decorator"
    dec = Decorator(FuncDef.deserialize(data["func"]), [], Var.deserialize(data["var"]))
    dec.is_overload = data["is_overload"]
    return dec


</t>
<t tx="ekr.20221004064035.576">VAR_FLAGS: Final = [
    "is_self",
    "is_initialized_in_class",
    "is_staticmethod",
    "is_classmethod",
    "is_property",
    "is_settable_property",
    "is_suppressed_import",
    "is_classvar",
    "is_abstract_var",
    "is_final",
    "final_unset_in_class",
    "final_set_in_init",
    "explicit_self_type",
    "is_ready",
    "is_inferred",
    "invalid_partial_type",
    "from_module_getattr",
    "has_explicit_value",
    "allow_incompatible_override",
]


</t>
<t tx="ekr.20221004064035.577">class Var(SymbolNode):
    """A variable.

    It can refer to global/local variable or a data attribute.
    """

    __slots__ = (
        "_name",
        "_fullname",
        "info",
        "type",
        "final_value",
        "is_self",
        "is_ready",
        "is_inferred",
        "is_initialized_in_class",
        "is_staticmethod",
        "is_classmethod",
        "is_property",
        "is_settable_property",
        "is_classvar",
        "is_abstract_var",
        "is_final",
        "final_unset_in_class",
        "final_set_in_init",
        "is_suppressed_import",
        "explicit_self_type",
        "from_module_getattr",
        "has_explicit_value",
        "allow_incompatible_override",
        "invalid_partial_type",
    )

    @others
</t>
<t tx="ekr.20221004064035.578">def __init__(self, name: str, type: mypy.types.Type | None = None) -&gt; None:
    super().__init__()
    self._name = name  # Name without module prefix
    # TODO: Should be Optional[str]
    self._fullname = cast("Bogus[str]", None)  # Name with module prefix
    # TODO: Should be Optional[TypeInfo]
    self.info = VAR_NO_INFO
    self.type: mypy.types.Type | None = type  # Declared or inferred type, or None
    # Is this the first argument to an ordinary method (usually "self")?
    self.is_self = False
    self.is_ready = True  # If inferred, is the inferred type available?
    self.is_inferred = self.type is None
    # Is this initialized explicitly to a non-None value in class body?
    self.is_initialized_in_class = False
    self.is_staticmethod = False
    self.is_classmethod = False
    self.is_property = False
    self.is_settable_property = False
    self.is_classvar = False
    self.is_abstract_var = False
    # Set to true when this variable refers to a module we were unable to
    # parse for some reason (eg a silenced module)
    self.is_suppressed_import = False
    # Was this "variable" (rather a constant) defined as Final[...]?
    self.is_final = False
    # If constant value is a simple literal,
    # store the literal value (unboxed) for the benefit of
    # tools like mypyc.
    self.final_value: int | float | bool | str | None = None
    # Where the value was set (only for class attributes)
    self.final_unset_in_class = False
    self.final_set_in_init = False
    # This is True for a variable that was declared on self with an explicit type:
    #     class C:
    #         def __init__(self) -&gt; None:
    #             self.x: int
    # This case is important because this defines a new Var, even if there is one
    # present in a superclass (without explicit type this doesn't create a new Var).
    # See SemanticAnalyzer.analyze_member_lvalue() for details.
    self.explicit_self_type = False
    # If True, this is an implicit Var created due to module-level __getattr__.
    self.from_module_getattr = False
    # Var can be created with an explicit value `a = 1` or without one `a: int`,
    # we need a way to tell which one is which.
    self.has_explicit_value = False
    # If True, subclasses can override this with an incompatible type.
    self.allow_incompatible_override = False
    # If True, this means we didn't manage to infer full type and fall back to
    # something like list[Any]. We may decide to not use such types as context.
    self.invalid_partial_type = False

</t>
<t tx="ekr.20221004064035.579">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20221004064035.58">def join_type_list(types: list[Type]) -&gt; ProperType:
    if not types:
        # This is a little arbitrary but reasonable. Any empty tuple should be compatible
        # with all variable length tuples, and this makes it possible.
        return UninhabitedType()
    joined = get_proper_type(types[0])
    for t in types[1:]:
        joined = join_types(joined, t)
    return joined


</t>
<t tx="ekr.20221004064035.580">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20221004064035.581">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_var(self)

</t>
<t tx="ekr.20221004064035.582">def serialize(self) -&gt; JsonDict:
    # TODO: Leave default values out?
    # NOTE: Sometimes self.is_ready is False here, but we don't care.
    data: JsonDict = {
        ".class": "Var",
        "name": self._name,
        "fullname": self._fullname,
        "type": None if self.type is None else self.type.serialize(),
        "flags": get_flags(self, VAR_FLAGS),
    }
    if self.final_value is not None:
        data["final_value"] = self.final_value
    return data

</t>
<t tx="ekr.20221004064035.583">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Var:
    assert data[".class"] == "Var"
    name = data["name"]
    type = None if data["type"] is None else mypy.types.deserialize_type(data["type"])
    v = Var(name, type)
    v.is_ready = False  # Override True default set in __init__
    v._fullname = data["fullname"]
    set_flags(v, data["flags"])
    v.final_value = data.get("final_value")
    return v


</t>
<t tx="ekr.20221004064035.584">class ClassDef(Statement):
    """Class definition"""

    __slots__ = (
        "name",
        "fullname",
        "defs",
        "type_vars",
        "base_type_exprs",
        "removed_base_type_exprs",
        "info",
        "metaclass",
        "decorators",
        "keywords",
        "analyzed",
        "has_incompatible_baseclass",
        "deco_line",
    )

    name: str  # Name of the class without module prefix
    fullname: Bogus[str]  # Fully qualified name of the class
    defs: Block
    type_vars: list[mypy.types.TypeVarLikeType]
    # Base class expressions (not semantically analyzed -- can be arbitrary expressions)
    base_type_exprs: list[Expression]
    # Special base classes like Generic[...] get moved here during semantic analysis
    removed_base_type_exprs: list[Expression]
    info: TypeInfo  # Related TypeInfo
    metaclass: Expression | None
    decorators: list[Expression]
    keywords: dict[str, Expression]
    analyzed: Expression | None
    has_incompatible_baseclass: bool

    @others
</t>
<t tx="ekr.20221004064035.585">def __init__(
    self,
    name: str,
    defs: Block,
    type_vars: list[mypy.types.TypeVarLikeType] | None = None,
    base_type_exprs: list[Expression] | None = None,
    metaclass: Expression | None = None,
    keywords: list[tuple[str, Expression]] | None = None,
) -&gt; None:
    super().__init__()
    self.name = name
    self.fullname = None  # type: ignore[assignment]
    self.defs = defs
    self.type_vars = type_vars or []
    self.base_type_exprs = base_type_exprs or []
    self.removed_base_type_exprs = []
    self.info = CLASSDEF_NO_INFO
    self.metaclass = metaclass
    self.decorators = []
    self.keywords = dict(keywords) if keywords else {}
    self.analyzed = None
    self.has_incompatible_baseclass = False
    # Used for error reporting (to keep backwad compatibility with pre-3.8)
    self.deco_line: int | None = None

</t>
<t tx="ekr.20221004064035.586">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_class_def(self)

</t>
<t tx="ekr.20221004064035.587">def is_generic(self) -&gt; bool:
    return self.info.is_generic()

</t>
<t tx="ekr.20221004064035.588">def serialize(self) -&gt; JsonDict:
    # Not serialized: defs, base_type_exprs, metaclass, decorators,
    # analyzed (for named tuples etc.)
    return {
        ".class": "ClassDef",
        "name": self.name,
        "fullname": self.fullname,
        "type_vars": [v.serialize() for v in self.type_vars],
    }

</t>
<t tx="ekr.20221004064035.589">@classmethod
def deserialize(self, data: JsonDict) -&gt; ClassDef:
    assert data[".class"] == "ClassDef"
    res = ClassDef(
        data["name"],
        Block([]),
        # https://github.com/python/mypy/issues/12257
        [
            cast(mypy.types.TypeVarLikeType, mypy.types.deserialize_type(v))
            for v in data["type_vars"]
        ],
    )
    res.fullname = data["fullname"]
    return res


</t>
<t tx="ekr.20221004064035.59">def unpack_callback_protocol(t: Instance) -&gt; Type | None:
    assert t.type.is_protocol
    if t.type.protocol_members == ["__call__"]:
        return find_member("__call__", t, t, is_operator=True)
    return None
</t>
<t tx="ekr.20221004064035.590">class GlobalDecl(Statement):
    """Declaration global x, y, ..."""

    __slots__ = ("names",)

    names: list[str]

    @others
</t>
<t tx="ekr.20221004064035.591">def __init__(self, names: list[str]) -&gt; None:
    super().__init__()
    self.names = names

</t>
<t tx="ekr.20221004064035.592">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_global_decl(self)


</t>
<t tx="ekr.20221004064035.593">class NonlocalDecl(Statement):
    """Declaration nonlocal x, y, ..."""

    __slots__ = ("names",)

    names: list[str]

    @others
</t>
<t tx="ekr.20221004064035.594">def __init__(self, names: list[str]) -&gt; None:
    super().__init__()
    self.names = names

</t>
<t tx="ekr.20221004064035.595">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_nonlocal_decl(self)


</t>
<t tx="ekr.20221004064035.596">class Block(Statement):
    __slots__ = ("body", "is_unreachable")

    @others
</t>
<t tx="ekr.20221004064035.597">def __init__(self, body: list[Statement]) -&gt; None:
    super().__init__()
    self.body = body
    # True if we can determine that this block is not executed during semantic
    # analysis. For example, this applies to blocks that are protected by
    # something like "if PY3:" when using Python 2. However, some code is
    # only considered unreachable during type checking and this is not true
    # in those cases.
    self.is_unreachable = False

</t>
<t tx="ekr.20221004064035.598">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_block(self)


</t>
<t tx="ekr.20221004064035.599"># Statements


</t>
<t tx="ekr.20221004064035.6">def close(self) -&gt; None:
    if sys.platform == "win32":
        if self.connection != _winapi.NULL:
            _winapi.CloseHandle(self.connection)
    else:
        self.connection.close()


</t>
<t tx="ekr.20221004064035.60">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from typing import Any, Iterable, Optional, Tuple
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy.nodes import (
    LITERAL_NO,
    LITERAL_TYPE,
    LITERAL_YES,
    AssertTypeExpr,
    AssignmentExpr,
    AwaitExpr,
    BytesExpr,
    CallExpr,
    CastExpr,
    ComparisonExpr,
    ComplexExpr,
    ConditionalExpr,
    DictExpr,
    DictionaryComprehension,
    EllipsisExpr,
    EnumCallExpr,
    Expression,
    FloatExpr,
    GeneratorExpr,
    IndexExpr,
    IntExpr,
    LambdaExpr,
    ListComprehension,
    ListExpr,
    MemberExpr,
    NamedTupleExpr,
    NameExpr,
    NewTypeExpr,
    OpExpr,
    ParamSpecExpr,
    PromoteExpr,
    RevealExpr,
    SetComprehension,
    SetExpr,
    SliceExpr,
    StarExpr,
    StrExpr,
    SuperExpr,
    TempNode,
    TupleExpr,
    TypeAliasExpr,
    TypeApplication,
    TypedDictExpr,
    TypeVarExpr,
    TypeVarTupleExpr,
    UnaryExpr,
    Var,
    YieldExpr,
    YieldFromExpr,
)
from mypy.visitor import ExpressionVisitor

# [Note Literals and literal_hash]
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Mypy uses the term "literal" to refer to any expression built out of
# the following:
#
# * Plain literal expressions, like `1` (integer, float, string, etc.)
#
# * Compound literal expressions, like `(lit1, lit2)` (list, dict,
#   set, or tuple)
#
# * Operator expressions, like `lit1 + lit2`
#
# * Variable references, like `x`
#
# * Member references, like `lit.m`
#
# * Index expressions, like `lit[0]`
#
# A typical "literal" looks like `x[(i,j+1)].m`.
#
# An expression that is a literal has a `literal_hash`, with the
# following properties.
#
# * `literal_hash` is a Key: a tuple containing basic data types and
#   possibly other Keys. So it can be used as a key in a dictionary
#   that will be compared by value (as opposed to the Node itself,
#   which is compared by identity).
#
# * Two expressions have equal `literal_hash`es if and only if they
#   are syntactically equal expressions. (NB: Actually, we also
#   identify as equal expressions like `3` and `3.0`; is this a good
#   idea?)
#
# * The elements of `literal_hash` that are tuples are exactly the
#   subexpressions of the original expression (e.g. the base and index
#   of an index expression, or the operands of an operator expression).


@others
_hasher: Final = _Hasher()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.600">class ExpressionStmt(Statement):
    """An expression as a statement, such as print(s)."""

    __slots__ = ("expr",)

    expr: Expression

    @others
</t>
<t tx="ekr.20221004064035.601">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20221004064035.602">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_expression_stmt(self)


</t>
<t tx="ekr.20221004064035.603">class AssignmentStmt(Statement):
    """Assignment statement.

    The same node class is used for single assignment, multiple assignment
    (e.g. x, y = z) and chained assignment (e.g. x = y = z), assignments
    that define new names, and assignments with explicit types ("# type: t"
    or "x: t [= ...]").

    An lvalue can be NameExpr, TupleExpr, ListExpr, MemberExpr, or IndexExpr.
    """

    __slots__ = (
        "lvalues",
        "rvalue",
        "type",
        "unanalyzed_type",
        "new_syntax",
        "is_alias_def",
        "is_final_def",
        "invalid_recursive_alias",
    )

    lvalues: list[Lvalue]
    # This is a TempNode if and only if no rvalue (x: t).
    rvalue: Expression
    # Declared type in a comment, may be None.
    type: mypy.types.Type | None
    # Original, not semantically analyzed type in annotation (used for reprocessing)
    unanalyzed_type: mypy.types.Type | None
    # This indicates usage of PEP 526 type annotation syntax in assignment.
    new_syntax: bool
    # Does this assignment define a type alias?
    is_alias_def: bool
    # Is this a final definition?
    # Final attributes can't be re-assigned once set, and can't be overridden
    # in a subclass. This flag is not set if an attempted declaration was found to
    # be invalid during semantic analysis. It is still set to `True` if
    # a final declaration overrides another final declaration (this is checked
    # during type checking when MROs are known).
    is_final_def: bool
    # Stop further processing of this assignment, to prevent flipping back and forth
    # during semantic analysis passes.
    invalid_recursive_alias: bool

    @others
</t>
<t tx="ekr.20221004064035.604">def __init__(
    self,
    lvalues: list[Lvalue],
    rvalue: Expression,
    type: mypy.types.Type | None = None,
    new_syntax: bool = False,
) -&gt; None:
    super().__init__()
    self.lvalues = lvalues
    self.rvalue = rvalue
    self.type = type
    self.unanalyzed_type = type
    self.new_syntax = new_syntax
    self.is_alias_def = False
    self.is_final_def = False
    self.invalid_recursive_alias = False

</t>
<t tx="ekr.20221004064035.605">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_assignment_stmt(self)


</t>
<t tx="ekr.20221004064035.606">class OperatorAssignmentStmt(Statement):
    """Operator assignment statement such as x += 1"""

    __slots__ = ("op", "lvalue", "rvalue")

    op: str  # TODO: Enum?
    lvalue: Lvalue
    rvalue: Expression

    @others
</t>
<t tx="ekr.20221004064035.607">def __init__(self, op: str, lvalue: Lvalue, rvalue: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.lvalue = lvalue
    self.rvalue = rvalue

</t>
<t tx="ekr.20221004064035.608">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_operator_assignment_stmt(self)


</t>
<t tx="ekr.20221004064035.609">class WhileStmt(Statement):
    __slots__ = ("expr", "body", "else_body")

    expr: Expression
    body: Block
    else_body: Block | None

    @others
</t>
<t tx="ekr.20221004064035.61">def literal(e: Expression) -&gt; int:
    if isinstance(e, ComparisonExpr):
        return min(literal(o) for o in e.operands)

    elif isinstance(e, OpExpr):
        return min(literal(e.left), literal(e.right))

    elif isinstance(e, (MemberExpr, UnaryExpr, StarExpr)):
        return literal(e.expr)

    elif isinstance(e, AssignmentExpr):
        return literal(e.target)

    elif isinstance(e, IndexExpr):
        if literal(e.index) == LITERAL_YES:
            return literal(e.base)
        else:
            return LITERAL_NO

    elif isinstance(e, NameExpr):
        if isinstance(e.node, Var) and e.node.is_final and e.node.final_value is not None:
            return LITERAL_YES
        return LITERAL_TYPE

    if isinstance(e, (IntExpr, FloatExpr, ComplexExpr, StrExpr, BytesExpr)):
        return LITERAL_YES

    if literal_hash(e):
        return LITERAL_YES

    return LITERAL_NO


</t>
<t tx="ekr.20221004064035.610">def __init__(self, expr: Expression, body: Block, else_body: Block | None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.body = body
    self.else_body = else_body

</t>
<t tx="ekr.20221004064035.611">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_while_stmt(self)


</t>
<t tx="ekr.20221004064035.612">class ForStmt(Statement):
    __slots__ = (
        "index",
        "index_type",
        "unanalyzed_index_type",
        "inferred_item_type",
        "inferred_iterator_type",
        "expr",
        "body",
        "else_body",
        "is_async",
    )

    # Index variables
    index: Lvalue
    # Type given by type comments for index, can be None
    index_type: mypy.types.Type | None
    # Original, not semantically analyzed type in annotation (used for reprocessing)
    unanalyzed_index_type: mypy.types.Type | None
    # Inferred iterable item type
    inferred_item_type: mypy.types.Type | None
    # Inferred iterator type
    inferred_iterator_type: mypy.types.Type | None
    # Expression to iterate
    expr: Expression
    body: Block
    else_body: Block | None
    is_async: bool  # True if `async for ...` (PEP 492, Python 3.5)

    @others
</t>
<t tx="ekr.20221004064035.613">def __init__(
    self,
    index: Lvalue,
    expr: Expression,
    body: Block,
    else_body: Block | None,
    index_type: mypy.types.Type | None = None,
) -&gt; None:
    super().__init__()
    self.index = index
    self.index_type = index_type
    self.unanalyzed_index_type = index_type
    self.inferred_item_type = None
    self.inferred_iterator_type = None
    self.expr = expr
    self.body = body
    self.else_body = else_body
    self.is_async = False

</t>
<t tx="ekr.20221004064035.614">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_for_stmt(self)


</t>
<t tx="ekr.20221004064035.615">class ReturnStmt(Statement):
    __slots__ = ("expr",)

    expr: Expression | None

    @others
</t>
<t tx="ekr.20221004064035.616">def __init__(self, expr: Expression | None) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20221004064035.617">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_return_stmt(self)


</t>
<t tx="ekr.20221004064035.618">class AssertStmt(Statement):
    __slots__ = ("expr", "msg")

    expr: Expression
    msg: Expression | None

    @others
</t>
<t tx="ekr.20221004064035.619">def __init__(self, expr: Expression, msg: Expression | None = None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.msg = msg

</t>
<t tx="ekr.20221004064035.62">Key: _TypeAlias = Tuple[Any, ...]


</t>
<t tx="ekr.20221004064035.620">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_assert_stmt(self)


</t>
<t tx="ekr.20221004064035.621">class DelStmt(Statement):
    __slots__ = ("expr",)

    expr: Lvalue

    @others
</t>
<t tx="ekr.20221004064035.622">def __init__(self, expr: Lvalue) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20221004064035.623">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_del_stmt(self)


</t>
<t tx="ekr.20221004064035.624">class BreakStmt(Statement):
    __slots__ = ()

    def accept(self, visitor: StatementVisitor[T]) -&gt; T:
        return visitor.visit_break_stmt(self)


</t>
<t tx="ekr.20221004064035.625">class ContinueStmt(Statement):
    __slots__ = ()

    def accept(self, visitor: StatementVisitor[T]) -&gt; T:
        return visitor.visit_continue_stmt(self)


</t>
<t tx="ekr.20221004064035.626">class PassStmt(Statement):
    __slots__ = ()

    def accept(self, visitor: StatementVisitor[T]) -&gt; T:
        return visitor.visit_pass_stmt(self)


</t>
<t tx="ekr.20221004064035.627">class IfStmt(Statement):
    __slots__ = ("expr", "body", "else_body")

    expr: list[Expression]
    body: list[Block]
    else_body: Block | None

    @others
</t>
<t tx="ekr.20221004064035.628">def __init__(self, expr: list[Expression], body: list[Block], else_body: Block | None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.body = body
    self.else_body = else_body

</t>
<t tx="ekr.20221004064035.629">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_if_stmt(self)


</t>
<t tx="ekr.20221004064035.63">def subkeys(key: Key) -&gt; Iterable[Key]:
    return [elt for elt in key if isinstance(elt, tuple)]


</t>
<t tx="ekr.20221004064035.630">class RaiseStmt(Statement):
    __slots__ = ("expr", "from_expr")

    # Plain 'raise' is a valid statement.
    expr: Expression | None
    from_expr: Expression | None

    @others
</t>
<t tx="ekr.20221004064035.631">def __init__(self, expr: Expression | None, from_expr: Expression | None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.from_expr = from_expr

</t>
<t tx="ekr.20221004064035.632">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_raise_stmt(self)


</t>
<t tx="ekr.20221004064035.633">class TryStmt(Statement):
    __slots__ = ("body", "types", "vars", "handlers", "else_body", "finally_body")

    body: Block  # Try body
    # Plain 'except:' also possible
    types: list[Expression | None]  # Except type expressions
    vars: list[NameExpr | None]  # Except variable names
    handlers: list[Block]  # Except bodies
    else_body: Block | None
    finally_body: Block | None

    @others
</t>
<t tx="ekr.20221004064035.634">def __init__(
    self,
    body: Block,
    vars: list[NameExpr | None],
    types: list[Expression | None],
    handlers: list[Block],
    else_body: Block | None,
    finally_body: Block | None,
) -&gt; None:
    super().__init__()
    self.body = body
    self.vars = vars
    self.types = types
    self.handlers = handlers
    self.else_body = else_body
    self.finally_body = finally_body

</t>
<t tx="ekr.20221004064035.635">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_try_stmt(self)


</t>
<t tx="ekr.20221004064035.636">class WithStmt(Statement):
    __slots__ = ("expr", "target", "unanalyzed_type", "analyzed_types", "body", "is_async")

    expr: list[Expression]
    target: list[Lvalue | None]
    # Type given by type comments for target, can be None
    unanalyzed_type: mypy.types.Type | None
    # Semantically analyzed types from type comment (TypeList type expanded)
    analyzed_types: list[mypy.types.Type]
    body: Block
    is_async: bool  # True if `async with ...` (PEP 492, Python 3.5)

    @others
</t>
<t tx="ekr.20221004064035.637">def __init__(
    self,
    expr: list[Expression],
    target: list[Lvalue | None],
    body: Block,
    target_type: mypy.types.Type | None = None,
) -&gt; None:
    super().__init__()
    self.expr = expr
    self.target = target
    self.unanalyzed_type = target_type
    self.analyzed_types = []
    self.body = body
    self.is_async = False

</t>
<t tx="ekr.20221004064035.638">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_with_stmt(self)


</t>
<t tx="ekr.20221004064035.639">class MatchStmt(Statement):
    subject: Expression
    patterns: list[Pattern]
    guards: list[Expression | None]
    bodies: list[Block]

    @others
</t>
<t tx="ekr.20221004064035.64">def literal_hash(e: Expression) -&gt; Key | None:
    return e.accept(_hasher)


</t>
<t tx="ekr.20221004064035.640">def __init__(
    self,
    subject: Expression,
    patterns: list[Pattern],
    guards: list[Expression | None],
    bodies: list[Block],
) -&gt; None:
    super().__init__()
    assert len(patterns) == len(guards) == len(bodies)
    self.subject = subject
    self.patterns = patterns
    self.guards = guards
    self.bodies = bodies

</t>
<t tx="ekr.20221004064035.641">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_match_stmt(self)


</t>
<t tx="ekr.20221004064035.642"># Expressions


</t>
<t tx="ekr.20221004064035.643">class IntExpr(Expression):
    """Integer literal"""

    __slots__ = ("value",)

    value: int  # 0 by default

    @others
</t>
<t tx="ekr.20221004064035.644">def __init__(self, value: int) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20221004064035.645">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_int_expr(self)


</t>
<t tx="ekr.20221004064035.646"># How mypy uses StrExpr and BytesExpr:
#
# b'x' -&gt; BytesExpr
# 'x', u'x' -&gt; StrExpr


</t>
<t tx="ekr.20221004064035.647">class StrExpr(Expression):
    """String literal"""

    __slots__ = ("value",)

    value: str  # '' by default

    @others
</t>
<t tx="ekr.20221004064035.648">def __init__(self, value: str) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20221004064035.649">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_str_expr(self)


</t>
<t tx="ekr.20221004064035.65">class _Hasher(ExpressionVisitor[Optional[Key]]):
    @others
</t>
<t tx="ekr.20221004064035.650">class BytesExpr(Expression):
    """Bytes literal"""

    __slots__ = ("value",)

    # Note: we deliberately do NOT use bytes here because it ends up
    # unnecessarily complicating a lot of the result logic. For example,
    # we'd have to worry about converting the bytes into a format we can
    # easily serialize/deserialize to and from JSON, would have to worry
    # about turning the bytes into a human-readable representation in
    # error messages...
    #
    # It's more convenient to just store the human-readable representation
    # from the very start.
    value: str

    @others
</t>
<t tx="ekr.20221004064035.651">def __init__(self, value: str) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20221004064035.652">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_bytes_expr(self)


</t>
<t tx="ekr.20221004064035.653">class FloatExpr(Expression):
    """Float literal"""

    __slots__ = ("value",)

    value: float  # 0.0 by default

    @others
</t>
<t tx="ekr.20221004064035.654">def __init__(self, value: float) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20221004064035.655">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_float_expr(self)


</t>
<t tx="ekr.20221004064035.656">class ComplexExpr(Expression):
    """Complex literal"""

    __slots__ = ("value",)

    value: complex

    @others
</t>
<t tx="ekr.20221004064035.657">def __init__(self, value: complex) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20221004064035.658">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_complex_expr(self)


</t>
<t tx="ekr.20221004064035.659">class EllipsisExpr(Expression):
    """Ellipsis (...)"""

    __slots__ = ()

    def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
        return visitor.visit_ellipsis(self)


</t>
<t tx="ekr.20221004064035.66">def visit_int_expr(self, e: IntExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20221004064035.660">class StarExpr(Expression):
    """Star expression"""

    __slots__ = ("expr", "valid")

    expr: Expression
    valid: bool

    @others
</t>
<t tx="ekr.20221004064035.661">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

    # Whether this starred expression is used in a tuple/list and as lvalue
    self.valid = False

</t>
<t tx="ekr.20221004064035.662">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_star_expr(self)


</t>
<t tx="ekr.20221004064035.663">class RefExpr(Expression):
    """Abstract base class for name-like constructs"""

    __slots__ = (
        "kind",
        "node",
        "fullname",
        "is_new_def",
        "is_inferred_def",
        "is_alias_rvalue",
        "type_guard",
    )

    @others
</t>
<t tx="ekr.20221004064035.664">def __init__(self) -&gt; None:
    super().__init__()
    # LDEF/GDEF/MDEF/... (None if not available)
    self.kind: int | None = None
    # Var, FuncDef or TypeInfo that describes this
    self.node: SymbolNode | None = None
    # Fully qualified name (or name if not global)
    self.fullname: str | None = None
    # Does this define a new name?
    self.is_new_def = False
    # Does this define a new name with inferred type?
    #
    # For members, after semantic analysis, this does not take base
    # classes into consideration at all; the type checker deals with these.
    self.is_inferred_def = False
    # Is this expression appears as an rvalue of a valid type alias definition?
    self.is_alias_rvalue = False
    # Cache type guard from callable_type.type_guard
    self.type_guard: mypy.types.Type | None = None


</t>
<t tx="ekr.20221004064035.665">class NameExpr(RefExpr):
    """Name expression

    This refers to a local name, global name or a module.
    """

    __slots__ = ("name", "is_special_form")

    @others
</t>
<t tx="ekr.20221004064035.666">def __init__(self, name: str) -&gt; None:
    super().__init__()
    self.name = name  # Name referred to (may be qualified)
    # Is this a l.h.s. of a special form assignment like typed dict or type variable?
    self.is_special_form = False

</t>
<t tx="ekr.20221004064035.667">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_name_expr(self)

</t>
<t tx="ekr.20221004064035.668">def serialize(self) -&gt; JsonDict:
    assert False, f"Serializing NameExpr: {self}"


</t>
<t tx="ekr.20221004064035.669">class MemberExpr(RefExpr):
    """Member access expression x.y"""

    __slots__ = ("expr", "name", "def_var")

    @others
</t>
<t tx="ekr.20221004064035.67">def visit_str_expr(self, e: StrExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20221004064035.670">def __init__(self, expr: Expression, name: str) -&gt; None:
    super().__init__()
    self.expr = expr
    self.name = name
    # The variable node related to a definition through 'self.x = &lt;initializer&gt;'.
    # The nodes of other kinds of member expressions are resolved during type checking.
    self.def_var: Var | None = None

</t>
<t tx="ekr.20221004064035.671">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_member_expr(self)


</t>
<t tx="ekr.20221004064035.672"># Kinds of arguments
@unique
class ArgKind(Enum):
    # Positional argument
    ARG_POS = 0
    # Positional, optional argument (functions only, not calls)
    ARG_OPT = 1
    # *arg argument
    ARG_STAR = 2
    # Keyword argument x=y in call, or keyword-only function arg
    ARG_NAMED = 3
    # **arg argument
    ARG_STAR2 = 4
    # In an argument list, keyword-only and also optional
    ARG_NAMED_OPT = 5

    @others
</t>
<t tx="ekr.20221004064035.673">def is_positional(self, star: bool = False) -&gt; bool:
    return self == ARG_POS or self == ARG_OPT or (star and self == ARG_STAR)

</t>
<t tx="ekr.20221004064035.674">def is_named(self, star: bool = False) -&gt; bool:
    return self == ARG_NAMED or self == ARG_NAMED_OPT or (star and self == ARG_STAR2)

</t>
<t tx="ekr.20221004064035.675">def is_required(self) -&gt; bool:
    return self == ARG_POS or self == ARG_NAMED

</t>
<t tx="ekr.20221004064035.676">def is_optional(self) -&gt; bool:
    return self == ARG_OPT or self == ARG_NAMED_OPT

</t>
<t tx="ekr.20221004064035.677">def is_star(self) -&gt; bool:
    return self == ARG_STAR or self == ARG_STAR2


</t>
<t tx="ekr.20221004064035.678">ARG_POS: Final = ArgKind.ARG_POS
ARG_OPT: Final = ArgKind.ARG_OPT
ARG_STAR: Final = ArgKind.ARG_STAR
ARG_NAMED: Final = ArgKind.ARG_NAMED
ARG_STAR2: Final = ArgKind.ARG_STAR2
ARG_NAMED_OPT: Final = ArgKind.ARG_NAMED_OPT


</t>
<t tx="ekr.20221004064035.679">class CallExpr(Expression):
    """Call expression.

    This can also represent several special forms that are syntactically calls
    such as cast(...) and None  # type: ....
    """

    __slots__ = ("callee", "args", "arg_kinds", "arg_names", "analyzed")

    @others
</t>
<t tx="ekr.20221004064035.68">def visit_bytes_expr(self, e: BytesExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20221004064035.680">def __init__(
    self,
    callee: Expression,
    args: list[Expression],
    arg_kinds: list[ArgKind],
    arg_names: list[str | None],
    analyzed: Expression | None = None,
) -&gt; None:
    super().__init__()
    if not arg_names:
        arg_names = [None] * len(args)

    self.callee = callee
    self.args = args
    self.arg_kinds = arg_kinds  # ARG_ constants
    # Each name can be None if not a keyword argument.
    self.arg_names: list[str | None] = arg_names
    # If not None, the node that represents the meaning of the CallExpr. For
    # cast(...) this is a CastExpr.
    self.analyzed = analyzed

</t>
<t tx="ekr.20221004064035.681">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_call_expr(self)


</t>
<t tx="ekr.20221004064035.682">class YieldFromExpr(Expression):
    __slots__ = ("expr",)

    expr: Expression

    @others
</t>
<t tx="ekr.20221004064035.683">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20221004064035.684">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_yield_from_expr(self)


</t>
<t tx="ekr.20221004064035.685">class YieldExpr(Expression):
    __slots__ = ("expr",)

    expr: Expression | None

    @others
</t>
<t tx="ekr.20221004064035.686">def __init__(self, expr: Expression | None) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20221004064035.687">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_yield_expr(self)


</t>
<t tx="ekr.20221004064035.688">class IndexExpr(Expression):
    """Index expression x[y].

    Also wraps type application such as List[int] as a special form.
    """

    __slots__ = ("base", "index", "method_type", "analyzed")

    base: Expression
    index: Expression
    # Inferred __getitem__ method type
    method_type: mypy.types.Type | None
    # If not None, this is actually semantically a type application
    # Class[type, ...] or a type alias initializer.
    analyzed: TypeApplication | TypeAliasExpr | None

    @others
</t>
<t tx="ekr.20221004064035.689">def __init__(self, base: Expression, index: Expression) -&gt; None:
    super().__init__()
    self.base = base
    self.index = index
    self.method_type = None
    self.analyzed = None

</t>
<t tx="ekr.20221004064035.69">def visit_float_expr(self, e: FloatExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20221004064035.690">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_index_expr(self)


</t>
<t tx="ekr.20221004064035.691">class UnaryExpr(Expression):
    """Unary operation"""

    __slots__ = ("op", "expr", "method_type")

    op: str  # TODO: Enum?
    expr: Expression
    # Inferred operator method type
    method_type: mypy.types.Type | None

    @others
</t>
<t tx="ekr.20221004064035.692">def __init__(self, op: str, expr: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.expr = expr
    self.method_type = None

</t>
<t tx="ekr.20221004064035.693">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_unary_expr(self)


</t>
<t tx="ekr.20221004064035.694">class AssignmentExpr(Expression):
    """Assignment expressions in Python 3.8+, like "a := 2"."""

    __slots__ = ("target", "value")

    @others
</t>
<t tx="ekr.20221004064035.695">def __init__(self, target: Expression, value: Expression) -&gt; None:
    super().__init__()
    self.target = target
    self.value = value

</t>
<t tx="ekr.20221004064035.696">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_assignment_expr(self)


</t>
<t tx="ekr.20221004064035.697">class OpExpr(Expression):
    """Binary operation (other than . or [] or comparison operators,
    which have specific nodes)."""

    __slots__ = ("op", "left", "right", "method_type", "right_always", "right_unreachable")

    op: str  # TODO: Enum?
    left: Expression
    right: Expression
    # Inferred type for the operator method type (when relevant).
    method_type: mypy.types.Type | None
    # Per static analysis only: Is the right side going to be evaluated every time?
    right_always: bool
    # Per static analysis only: Is the right side unreachable?
    right_unreachable: bool

    @others
</t>
<t tx="ekr.20221004064035.698">def __init__(self, op: str, left: Expression, right: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.left = left
    self.right = right
    self.method_type = None
    self.right_always = False
    self.right_unreachable = False

</t>
<t tx="ekr.20221004064035.699">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_op_expr(self)


</t>
<t tx="ekr.20221004064035.7">class IPCClient(IPCBase):
    """The client side of an IPC connection."""

    @others
</t>
<t tx="ekr.20221004064035.70">def visit_complex_expr(self, e: ComplexExpr) -&gt; Key:
    return ("Literal", e.value)

</t>
<t tx="ekr.20221004064035.700">class ComparisonExpr(Expression):
    """Comparison expression (e.g. a &lt; b &gt; c &lt; d)."""

    __slots__ = ("operators", "operands", "method_types")

    operators: list[str]
    operands: list[Expression]
    # Inferred type for the operator methods (when relevant; None for 'is').
    method_types: list[mypy.types.Type | None]

    @others
</t>
<t tx="ekr.20221004064035.701">def __init__(self, operators: list[str], operands: list[Expression]) -&gt; None:
    super().__init__()
    self.operators = operators
    self.operands = operands
    self.method_types = []

</t>
<t tx="ekr.20221004064035.702">def pairwise(self) -&gt; Iterator[tuple[str, Expression, Expression]]:
    """If this comparison expr is "a &lt; b is c == d", yields the sequence
    ("&lt;", a, b), ("is", b, c), ("==", c, d)
    """
    for i, operator in enumerate(self.operators):
        yield operator, self.operands[i], self.operands[i + 1]

</t>
<t tx="ekr.20221004064035.703">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_comparison_expr(self)


</t>
<t tx="ekr.20221004064035.704">class SliceExpr(Expression):
    """Slice expression (e.g. 'x:y', 'x:', '::2' or ':').

    This is only valid as index in index expressions.
    """

    __slots__ = ("begin_index", "end_index", "stride")

    begin_index: Expression | None
    end_index: Expression | None
    stride: Expression | None

    @others
</t>
<t tx="ekr.20221004064035.705">def __init__(
    self,
    begin_index: Expression | None,
    end_index: Expression | None,
    stride: Expression | None,
) -&gt; None:
    super().__init__()
    self.begin_index = begin_index
    self.end_index = end_index
    self.stride = stride

</t>
<t tx="ekr.20221004064035.706">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_slice_expr(self)


</t>
<t tx="ekr.20221004064035.707">class CastExpr(Expression):
    """Cast expression cast(type, expr)."""

    __slots__ = ("expr", "type")

    expr: Expression
    type: mypy.types.Type

    @others
</t>
<t tx="ekr.20221004064035.708">def __init__(self, expr: Expression, typ: mypy.types.Type) -&gt; None:
    super().__init__()
    self.expr = expr
    self.type = typ

</t>
<t tx="ekr.20221004064035.709">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_cast_expr(self)


</t>
<t tx="ekr.20221004064035.71">def visit_star_expr(self, e: StarExpr) -&gt; Key:
    return ("Star", literal_hash(e.expr))

</t>
<t tx="ekr.20221004064035.710">class AssertTypeExpr(Expression):
    """Represents a typing.assert_type(expr, type) call."""

    __slots__ = ("expr", "type")

    expr: Expression
    type: mypy.types.Type

    @others
</t>
<t tx="ekr.20221004064035.711">def __init__(self, expr: Expression, typ: mypy.types.Type) -&gt; None:
    super().__init__()
    self.expr = expr
    self.type = typ

</t>
<t tx="ekr.20221004064035.712">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_assert_type_expr(self)


</t>
<t tx="ekr.20221004064035.713">class RevealExpr(Expression):
    """Reveal type expression reveal_type(expr) or reveal_locals() expression."""

    __slots__ = ("expr", "kind", "local_nodes")

    expr: Expression | None
    kind: int
    local_nodes: list[Var] | None

    @others
</t>
<t tx="ekr.20221004064035.714">def __init__(
    self, kind: int, expr: Expression | None = None, local_nodes: list[Var] | None = None
) -&gt; None:
    super().__init__()
    self.expr = expr
    self.kind = kind
    self.local_nodes = local_nodes

</t>
<t tx="ekr.20221004064035.715">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_reveal_expr(self)


</t>
<t tx="ekr.20221004064035.716">class SuperExpr(Expression):
    """Expression super().name"""

    __slots__ = ("name", "info", "call")

    name: str
    info: TypeInfo | None  # Type that contains this super expression
    call: CallExpr  # The expression super(...)

    @others
</t>
<t tx="ekr.20221004064035.717">def __init__(self, name: str, call: CallExpr) -&gt; None:
    super().__init__()
    self.name = name
    self.call = call
    self.info = None

</t>
<t tx="ekr.20221004064035.718">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_super_expr(self)


</t>
<t tx="ekr.20221004064035.719">class LambdaExpr(FuncItem, Expression):
    """Lambda expression"""

    @others
</t>
<t tx="ekr.20221004064035.72">def visit_name_expr(self, e: NameExpr) -&gt; Key:
    if isinstance(e.node, Var) and e.node.is_final and e.node.final_value is not None:
        return ("Literal", e.node.final_value)
    # N.B: We use the node itself as the key, and not the name,
    # because using the name causes issues when there is shadowing
    # (for example, in list comprehensions).
    return ("Var", e.node)

</t>
<t tx="ekr.20221004064035.720">@property
def name(self) -&gt; str:
    return "&lt;lambda&gt;"

</t>
<t tx="ekr.20221004064035.721">def expr(self) -&gt; Expression:
    """Return the expression (the body) of the lambda."""
    ret = cast(ReturnStmt, self.body.body[-1])
    expr = ret.expr
    assert expr is not None  # lambda can't have empty body
    return expr

</t>
<t tx="ekr.20221004064035.722">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_lambda_expr(self)

</t>
<t tx="ekr.20221004064035.723">def is_dynamic(self) -&gt; bool:
    return False


</t>
<t tx="ekr.20221004064035.724">class ListExpr(Expression):
    """List literal expression [...]."""

    __slots__ = ("items",)

    items: list[Expression]

    @others
</t>
<t tx="ekr.20221004064035.725">def __init__(self, items: list[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20221004064035.726">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_list_expr(self)


</t>
<t tx="ekr.20221004064035.727">class DictExpr(Expression):
    """Dictionary literal expression {key: value, ...}."""

    __slots__ = ("items",)

    items: list[tuple[Expression | None, Expression]]

    @others
</t>
<t tx="ekr.20221004064035.728">def __init__(self, items: list[tuple[Expression | None, Expression]]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20221004064035.729">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_dict_expr(self)


</t>
<t tx="ekr.20221004064035.73">def visit_member_expr(self, e: MemberExpr) -&gt; Key:
    return ("Member", literal_hash(e.expr), e.name)

</t>
<t tx="ekr.20221004064035.730">class TupleExpr(Expression):
    """Tuple literal expression (..., ...)

    Also lvalue sequences (..., ...) and [..., ...]"""

    __slots__ = ("items",)

    items: list[Expression]

    @others
</t>
<t tx="ekr.20221004064035.731">def __init__(self, items: list[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20221004064035.732">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_tuple_expr(self)


</t>
<t tx="ekr.20221004064035.733">class SetExpr(Expression):
    """Set literal expression {value, ...}."""

    __slots__ = ("items",)

    items: list[Expression]

    @others
</t>
<t tx="ekr.20221004064035.734">def __init__(self, items: list[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20221004064035.735">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_set_expr(self)


</t>
<t tx="ekr.20221004064035.736">class GeneratorExpr(Expression):
    """Generator expression ... for ... in ... [ for ...  in ... ] [ if ... ]."""

    __slots__ = ("left_expr", "sequences", "condlists", "is_async", "indices")

    left_expr: Expression
    sequences: list[Expression]
    condlists: list[list[Expression]]
    is_async: list[bool]
    indices: list[Lvalue]

    @others
</t>
<t tx="ekr.20221004064035.737">def __init__(
    self,
    left_expr: Expression,
    indices: list[Lvalue],
    sequences: list[Expression],
    condlists: list[list[Expression]],
    is_async: list[bool],
) -&gt; None:
    super().__init__()
    self.left_expr = left_expr
    self.sequences = sequences
    self.condlists = condlists
    self.indices = indices
    self.is_async = is_async

</t>
<t tx="ekr.20221004064035.738">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_generator_expr(self)


</t>
<t tx="ekr.20221004064035.739">class ListComprehension(Expression):
    """List comprehension (e.g. [x + 1 for x in a])"""

    __slots__ = ("generator",)

    generator: GeneratorExpr

    @others
</t>
<t tx="ekr.20221004064035.74">def visit_op_expr(self, e: OpExpr) -&gt; Key:
    return ("Binary", e.op, literal_hash(e.left), literal_hash(e.right))

</t>
<t tx="ekr.20221004064035.740">def __init__(self, generator: GeneratorExpr) -&gt; None:
    super().__init__()
    self.generator = generator

</t>
<t tx="ekr.20221004064035.741">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_list_comprehension(self)


</t>
<t tx="ekr.20221004064035.742">class SetComprehension(Expression):
    """Set comprehension (e.g. {x + 1 for x in a})"""

    __slots__ = ("generator",)

    generator: GeneratorExpr

    @others
</t>
<t tx="ekr.20221004064035.743">def __init__(self, generator: GeneratorExpr) -&gt; None:
    super().__init__()
    self.generator = generator

</t>
<t tx="ekr.20221004064035.744">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_set_comprehension(self)


</t>
<t tx="ekr.20221004064035.745">class DictionaryComprehension(Expression):
    """Dictionary comprehension (e.g. {k: v for k, v in a}"""

    __slots__ = ("key", "value", "sequences", "condlists", "is_async", "indices")

    key: Expression
    value: Expression
    sequences: list[Expression]
    condlists: list[list[Expression]]
    is_async: list[bool]
    indices: list[Lvalue]

    @others
</t>
<t tx="ekr.20221004064035.746">def __init__(
    self,
    key: Expression,
    value: Expression,
    indices: list[Lvalue],
    sequences: list[Expression],
    condlists: list[list[Expression]],
    is_async: list[bool],
) -&gt; None:
    super().__init__()
    self.key = key
    self.value = value
    self.sequences = sequences
    self.condlists = condlists
    self.indices = indices
    self.is_async = is_async

</t>
<t tx="ekr.20221004064035.747">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_dictionary_comprehension(self)


</t>
<t tx="ekr.20221004064035.748">class ConditionalExpr(Expression):
    """Conditional expression (e.g. x if y else z)"""

    __slots__ = ("cond", "if_expr", "else_expr")

    cond: Expression
    if_expr: Expression
    else_expr: Expression

    @others
</t>
<t tx="ekr.20221004064035.749">def __init__(self, cond: Expression, if_expr: Expression, else_expr: Expression) -&gt; None:
    super().__init__()
    self.cond = cond
    self.if_expr = if_expr
    self.else_expr = else_expr

</t>
<t tx="ekr.20221004064035.75">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; Key:
    rest: tuple[str | Key | None, ...] = tuple(e.operators)
    rest += tuple(literal_hash(o) for o in e.operands)
    return ("Comparison",) + rest

</t>
<t tx="ekr.20221004064035.750">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_conditional_expr(self)


</t>
<t tx="ekr.20221004064035.751">class TypeApplication(Expression):
    """Type application expr[type, ...]"""

    __slots__ = ("expr", "types")

    expr: Expression
    types: list[mypy.types.Type]

    @others
</t>
<t tx="ekr.20221004064035.752">def __init__(self, expr: Expression, types: list[mypy.types.Type]) -&gt; None:
    super().__init__()
    self.expr = expr
    self.types = types

</t>
<t tx="ekr.20221004064035.753">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_application(self)


</t>
<t tx="ekr.20221004064035.754"># Variance of a type variable. For example, T in the definition of
# List[T] is invariant, so List[int] is not a subtype of List[object],
# and also List[object] is not a subtype of List[int].
#
# The T in Iterable[T] is covariant, so Iterable[int] is a subtype of
# Iterable[object], but not vice versa.
#
# If T is contravariant in Foo[T], Foo[object] is a subtype of
# Foo[int], but not vice versa.
INVARIANT: Final = 0
COVARIANT: Final = 1
CONTRAVARIANT: Final = 2


</t>
<t tx="ekr.20221004064035.755">class TypeVarLikeExpr(SymbolNode, Expression):
    """Base class for TypeVarExpr, ParamSpecExpr and TypeVarTupleExpr.

    Note that they are constructed by the semantic analyzer.
    """

    __slots__ = ("_name", "_fullname", "upper_bound", "variance")

    _name: str
    _fullname: str
    # Upper bound: only subtypes of upper_bound are valid as values. By default
    # this is 'object', meaning no restriction.
    upper_bound: mypy.types.Type
    # Variance of the type variable. Invariant is the default.
    # TypeVar(..., covariant=True) defines a covariant type variable.
    # TypeVar(..., contravariant=True) defines a contravariant type
    # variable.
    variance: int

    @others
</t>
<t tx="ekr.20221004064035.756">def __init__(
    self, name: str, fullname: str, upper_bound: mypy.types.Type, variance: int = INVARIANT
) -&gt; None:
    super().__init__()
    self._name = name
    self._fullname = fullname
    self.upper_bound = upper_bound
    self.variance = variance

</t>
<t tx="ekr.20221004064035.757">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20221004064035.758">@property
def fullname(self) -&gt; str:
    return self._fullname


</t>
<t tx="ekr.20221004064035.759">class TypeVarExpr(TypeVarLikeExpr):
    """Type variable expression TypeVar(...).

    This is also used to represent type variables in symbol tables.

    A type variable is not valid as a type unless bound in a TypeVarLikeScope.
    That happens within:

     1. a generic class that uses the type variable as a type argument or
     2. a generic function that refers to the type variable in its signature.
    """

    __slots__ = ("values",)

    # Value restriction: only types in the list are valid as values. If the
    # list is empty, there is no restriction.
    values: list[mypy.types.Type]

    @others
</t>
<t tx="ekr.20221004064035.76">def visit_unary_expr(self, e: UnaryExpr) -&gt; Key:
    return ("Unary", e.op, literal_hash(e.expr))

</t>
<t tx="ekr.20221004064035.760">def __init__(
    self,
    name: str,
    fullname: str,
    values: list[mypy.types.Type],
    upper_bound: mypy.types.Type,
    variance: int = INVARIANT,
) -&gt; None:
    super().__init__(name, fullname, upper_bound, variance)
    self.values = values

</t>
<t tx="ekr.20221004064035.761">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_var_expr(self)

</t>
<t tx="ekr.20221004064035.762">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TypeVarExpr",
        "name": self._name,
        "fullname": self._fullname,
        "values": [t.serialize() for t in self.values],
        "upper_bound": self.upper_bound.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20221004064035.763">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarExpr:
    assert data[".class"] == "TypeVarExpr"
    return TypeVarExpr(
        data["name"],
        data["fullname"],
        [mypy.types.deserialize_type(v) for v in data["values"]],
        mypy.types.deserialize_type(data["upper_bound"]),
        data["variance"],
    )


</t>
<t tx="ekr.20221004064035.764">class ParamSpecExpr(TypeVarLikeExpr):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.765">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_paramspec_expr(self)

</t>
<t tx="ekr.20221004064035.766">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "ParamSpecExpr",
        "name": self._name,
        "fullname": self._fullname,
        "upper_bound": self.upper_bound.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20221004064035.767">@classmethod
def deserialize(cls, data: JsonDict) -&gt; ParamSpecExpr:
    assert data[".class"] == "ParamSpecExpr"
    return ParamSpecExpr(
        data["name"],
        data["fullname"],
        mypy.types.deserialize_type(data["upper_bound"]),
        data["variance"],
    )


</t>
<t tx="ekr.20221004064035.768">class TypeVarTupleExpr(TypeVarLikeExpr):
    """Type variable tuple expression TypeVarTuple(...)."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.769">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_var_tuple_expr(self)

</t>
<t tx="ekr.20221004064035.77">def seq_expr(self, e: ListExpr | TupleExpr | SetExpr, name: str) -&gt; Key | None:
    if all(literal(x) == LITERAL_YES for x in e.items):
        rest: tuple[Key | None, ...] = tuple(literal_hash(x) for x in e.items)
        return (name,) + rest
    return None

</t>
<t tx="ekr.20221004064035.770">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "TypeVarTupleExpr",
        "name": self._name,
        "fullname": self._fullname,
        "upper_bound": self.upper_bound.serialize(),
        "variance": self.variance,
    }

</t>
<t tx="ekr.20221004064035.771">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeVarTupleExpr:
    assert data[".class"] == "TypeVarTupleExpr"
    return TypeVarTupleExpr(
        data["name"],
        data["fullname"],
        mypy.types.deserialize_type(data["upper_bound"]),
        data["variance"],
    )


</t>
<t tx="ekr.20221004064035.772">class TypeAliasExpr(Expression):
    """Type alias expression (rvalue)."""

    __slots__ = ("type", "tvars", "no_args", "node")

    # The target type.
    type: mypy.types.Type
    # Names of unbound type variables used to define the alias
    tvars: list[str]
    # Whether this alias was defined in bare form. Used to distinguish
    # between
    #     A = List
    # and
    #     A = List[Any]
    no_args: bool
    node: TypeAlias

    @others
</t>
<t tx="ekr.20221004064035.773">def __init__(self, node: TypeAlias) -&gt; None:
    super().__init__()
    self.type = node.target
    self.tvars = node.alias_tvars
    self.no_args = node.no_args
    self.node = node

</t>
<t tx="ekr.20221004064035.774">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_alias_expr(self)


</t>
<t tx="ekr.20221004064035.775">class NamedTupleExpr(Expression):
    """Named tuple expression namedtuple(...) or NamedTuple(...)."""

    __slots__ = ("info", "is_typed")

    # The class representation of this named tuple (its tuple_type attribute contains
    # the tuple item types)
    info: TypeInfo
    is_typed: bool  # whether this class was created with typing(_extensions).NamedTuple

    @others
</t>
<t tx="ekr.20221004064035.776">def __init__(self, info: TypeInfo, is_typed: bool = False) -&gt; None:
    super().__init__()
    self.info = info
    self.is_typed = is_typed

</t>
<t tx="ekr.20221004064035.777">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_namedtuple_expr(self)


</t>
<t tx="ekr.20221004064035.778">class TypedDictExpr(Expression):
    """Typed dict expression TypedDict(...)."""

    __slots__ = ("info",)

    # The class representation of this typed dict
    info: TypeInfo

    @others
</t>
<t tx="ekr.20221004064035.779">def __init__(self, info: TypeInfo) -&gt; None:
    super().__init__()
    self.info = info

</t>
<t tx="ekr.20221004064035.78">def visit_list_expr(self, e: ListExpr) -&gt; Key | None:
    return self.seq_expr(e, "List")

</t>
<t tx="ekr.20221004064035.780">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_typeddict_expr(self)


</t>
<t tx="ekr.20221004064035.781">class EnumCallExpr(Expression):
    """Named tuple expression Enum('name', 'val1 val2 ...')."""

    __slots__ = ("info", "items", "values")

    # The class representation of this enumerated type
    info: TypeInfo
    # The item names (for debugging)
    items: list[str]
    values: list[Expression | None]

    @others
</t>
<t tx="ekr.20221004064035.782">def __init__(self, info: TypeInfo, items: list[str], values: list[Expression | None]) -&gt; None:
    super().__init__()
    self.info = info
    self.items = items
    self.values = values

</t>
<t tx="ekr.20221004064035.783">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_enum_call_expr(self)


</t>
<t tx="ekr.20221004064035.784">class PromoteExpr(Expression):
    """Ducktype class decorator expression _promote(...)."""

    __slots__ = ("type",)

    type: mypy.types.Type

    @others
</t>
<t tx="ekr.20221004064035.785">def __init__(self, type: mypy.types.Type) -&gt; None:
    super().__init__()
    self.type = type

</t>
<t tx="ekr.20221004064035.786">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit__promote_expr(self)


</t>
<t tx="ekr.20221004064035.787">class NewTypeExpr(Expression):
    """NewType expression NewType(...)."""

    __slots__ = ("name", "old_type", "info")

    name: str
    # The base type (the second argument to NewType)
    old_type: mypy.types.Type | None
    # The synthesized class representing the new type (inherits old_type)
    info: TypeInfo | None

    @others
</t>
<t tx="ekr.20221004064035.788">def __init__(
    self, name: str, old_type: mypy.types.Type | None, line: int, column: int
) -&gt; None:
    super().__init__(line=line, column=column)
    self.name = name
    self.old_type = old_type
    self.info = None

</t>
<t tx="ekr.20221004064035.789">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_newtype_expr(self)


</t>
<t tx="ekr.20221004064035.79">def visit_dict_expr(self, e: DictExpr) -&gt; Key | None:
    if all(a and literal(a) == literal(b) == LITERAL_YES for a, b in e.items):
        rest: tuple[Key | None, ...] = tuple(
            (literal_hash(a) if a else None, literal_hash(b)) for a, b in e.items
        )
        return ("Dict",) + rest
    return None

</t>
<t tx="ekr.20221004064035.790">class AwaitExpr(Expression):
    """Await expression (await ...)."""

    __slots__ = ("expr",)

    expr: Expression

    @others
</t>
<t tx="ekr.20221004064035.791">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20221004064035.792">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_await_expr(self)


</t>
<t tx="ekr.20221004064035.793"># Constants


</t>
<t tx="ekr.20221004064035.794">class TempNode(Expression):
    """Temporary dummy node used during type checking.

    This node is not present in the original program; it is just an artifact
    of the type checker implementation. It only represents an opaque node with
    some fixed type.
    """

    __slots__ = ("type", "no_rhs")

    type: mypy.types.Type
    # Is this TempNode used to indicate absence of a right hand side in an annotated assignment?
    # (e.g. for 'x: int' the rvalue is TempNode(AnyType(TypeOfAny.special_form), no_rhs=True))
    no_rhs: bool

    @others
</t>
<t tx="ekr.20221004064035.795">def __init__(
    self, typ: mypy.types.Type, no_rhs: bool = False, *, context: Context | None = None
) -&gt; None:
    """Construct a dummy node; optionally borrow line/column from context object."""
    super().__init__()
    self.type = typ
    self.no_rhs = no_rhs
    if context is not None:
        self.line = context.line
        self.column = context.column

</t>
<t tx="ekr.20221004064035.796">def __repr__(self) -&gt; str:
    return "TempNode:%d(%s)" % (self.line, str(self.type))

</t>
<t tx="ekr.20221004064035.797">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_temp_node(self)


</t>
<t tx="ekr.20221004064035.798">class TypeInfo(SymbolNode):
    """The type structure of a single class.

    Each TypeInfo corresponds one-to-one to a ClassDef, which
    represents the AST of the class.

    In type-theory terms, this is a "type constructor", and if the
    class is generic then it will be a type constructor of higher kind.
    Where the class is used in an actual type, it's in the form of an
    Instance, which amounts to a type application of the tycon to
    the appropriate number of arguments.
    """

    __slots__ = (
        "_fullname",
        "module_name",
        "defn",
        "mro",
        "_mro_refs",
        "bad_mro",
        "is_final",
        "declared_metaclass",
        "metaclass_type",
        "names",
        "is_abstract",
        "is_protocol",
        "runtime_protocol",
        "abstract_attributes",
        "deletable_attributes",
        "slots",
        "assuming",
        "assuming_proper",
        "inferring",
        "is_enum",
        "fallback_to_any",
        "type_vars",
        "has_param_spec_type",
        "bases",
        "_promote",
        "tuple_type",
        "special_alias",
        "is_named_tuple",
        "typeddict_type",
        "is_newtype",
        "is_intersection",
        "metadata",
        "alt_promote",
        "has_type_var_tuple_type",
        "type_var_tuple_prefix",
        "type_var_tuple_suffix",
    )

    _fullname: Bogus[str]  # Fully qualified name
    # Fully qualified name for the module this type was defined in. This
    # information is also in the fullname, but is harder to extract in the
    # case of nested class definitions.
    module_name: str
    defn: ClassDef  # Corresponding ClassDef
    # Method Resolution Order: the order of looking up attributes. The first
    # value always to refers to this class.
    mro: list[TypeInfo]
    # Used to stash the names of the mro classes temporarily between
    # deserialization and fixup. See deserialize() for why.
    _mro_refs: list[str] | None
    bad_mro: bool  # Could not construct full MRO
    is_final: bool

    declared_metaclass: mypy.types.Instance | None
    metaclass_type: mypy.types.Instance | None

    names: SymbolTable  # Names defined directly in this type
    is_abstract: bool  # Does the class have any abstract attributes?
    is_protocol: bool  # Is this a protocol class?
    runtime_protocol: bool  # Does this protocol support isinstance checks?
    # List of names of abstract attributes together with their abstract status.
    # The abstract status must be one of `NOT_ABSTRACT`, `IS_ABSTRACT`, `IMPLICITLY_ABSTRACT`.
    abstract_attributes: list[tuple[str, int]]
    deletable_attributes: list[str]  # Used by mypyc only
    # Does this type have concrete `__slots__` defined?
    # If class does not have `__slots__` defined then it is `None`,
    # if it has empty `__slots__` then it is an empty set.
    slots: set[str] | None

    # The attributes 'assuming' and 'assuming_proper' represent structural subtype matrices.
    #
    # In languages with structural subtyping, one can keep a global subtype matrix like this:
    #   . A B C .
    #   A 1 0 0
    #   B 1 1 1
    #   C 1 0 1
    #   .
    # where 1 indicates that the type in corresponding row is a subtype of the type
    # in corresponding column. This matrix typically starts filled with all 1's and
    # a typechecker tries to "disprove" every subtyping relation using atomic (or nominal) types.
    # However, we don't want to keep this huge global state. Instead, we keep the subtype
    # information in the form of list of pairs (subtype, supertype) shared by all Instances
    # with given supertype's TypeInfo. When we enter a subtype check we push a pair in this list
    # thus assuming that we started with 1 in corresponding matrix element. Such algorithm allows
    # to treat recursive and mutually recursive protocols and other kinds of complex situations.
    #
    # If concurrent/parallel type checking will be added in future,
    # then there should be one matrix per thread/process to avoid false negatives
    # during the type checking phase.
    assuming: list[tuple[mypy.types.Instance, mypy.types.Instance]]
    assuming_proper: list[tuple[mypy.types.Instance, mypy.types.Instance]]
    # Ditto for temporary 'inferring' stack of recursive constraint inference.
    # It contains Instances of protocol types that appeared as an argument to
    # constraints.infer_constraints(). We need 'inferring' to avoid infinite recursion for
    # recursive and mutually recursive protocols.
    #
    # We make 'assuming' and 'inferring' attributes here instead of passing they as kwargs,
    # since this would require to pass them in many dozens of calls. In particular,
    # there is a dependency infer_constraint -&gt; is_subtype -&gt; is_callable_subtype -&gt;
    # -&gt; infer_constraints.
    inferring: list[mypy.types.Instance]
    # 'inferring' and 'assuming' can't be made sets, since we need to use
    # is_same_type to correctly treat unions.

    # Classes inheriting from Enum shadow their true members with a __getattr__, so we
    # have to treat them as a special case.
    is_enum: bool
    # If true, any unknown attributes should have type 'Any' instead
    # of generating a type error.  This would be true if there is a
    # base class with type 'Any', but other use cases may be
    # possible. This is similar to having __getattr__ that returns Any
    # (and __setattr__), but without the __getattr__ method.
    fallback_to_any: bool

    # Information related to type annotations.

    # Generic type variable names (full names)
    type_vars: list[str]

    # Whether this class has a ParamSpec type variable
    has_param_spec_type: bool

    # Direct base classes.
    bases: list[mypy.types.Instance]

    # Another type which this type will be treated as a subtype of,
    # even though it's not a subclass in Python.  The non-standard
    # `@_promote` decorator introduces this, and there are also
    # several builtin examples, in particular `int` -&gt; `float`.
    _promote: list[mypy.types.Type]

    # This is used for promoting native integer types such as 'i64' to
    # 'int'. (_promote is used for the other direction.) This only
    # supports one-step promotions (e.g., i64 -&gt; int, not
    # i64 -&gt; int -&gt; float, and this isn't used to promote in joins.
    #
    # This results in some unintuitive results, such as that even
    # though i64 is compatible with int and int is compatible with
    # float, i64 is *not* compatible with float.
    alt_promote: TypeInfo | None

    # Representation of a Tuple[...] base class, if the class has any
    # (e.g., for named tuples). If this is not None, the actual Type
    # object used for this class is not an Instance but a TupleType;
    # the corresponding Instance is set as the fallback type of the
    # tuple type.
    tuple_type: mypy.types.TupleType | None

    # Is this a named tuple type?
    is_named_tuple: bool

    # If this class is defined by the TypedDict type constructor,
    # then this is not None.
    typeddict_type: mypy.types.TypedDictType | None

    # Is this a newtype type?
    is_newtype: bool

    # Is this a synthesized intersection type?
    is_intersection: bool

    # This is a dictionary that will be serialized and un-serialized as is.
    # It is useful for plugins to add their data to save in the cache.
    metadata: dict[str, JsonDict]

    # Store type alias representing this type (for named tuples and TypedDicts).
    # Although definitions of these types are stored in symbol tables as TypeInfo,
    # when a type analyzer will find them, it should construct a TupleType, or
    # a TypedDict type. However, we can't use the plain types, since if the definition
    # is recursive, this will create an actual recursive structure of types (i.e. as
    # internal Python objects) causing infinite recursions everywhere during type checking.
    # To overcome this, we create a TypeAlias node, that will point to these types.
    # We store this node in the `special_alias` attribute, because it must be the same node
    # in case we are doing multiple semantic analysis passes.
    special_alias: TypeAlias | None

    FLAGS: Final = [
        "is_abstract",
        "is_enum",
        "fallback_to_any",
        "is_named_tuple",
        "is_newtype",
        "is_protocol",
        "runtime_protocol",
        "is_final",
        "is_intersection",
    ]

    @others
</t>
<t tx="ekr.20221004064035.799">def __init__(self, names: SymbolTable, defn: ClassDef, module_name: str) -&gt; None:
    """Initialize a TypeInfo."""
    super().__init__()
    self._fullname = defn.fullname
    self.names = names
    self.defn = defn
    self.module_name = module_name
    self.type_vars = []
    self.has_param_spec_type = False
    self.has_type_var_tuple_type = False
    self.bases = []
    self.mro = []
    self._mro_refs = None
    self.bad_mro = False
    self.declared_metaclass = None
    self.metaclass_type = None
    self.is_abstract = False
    self.abstract_attributes = []
    self.deletable_attributes = []
    self.slots = None
    self.assuming = []
    self.assuming_proper = []
    self.inferring = []
    self.is_protocol = False
    self.runtime_protocol = False
    self.type_var_tuple_prefix: int | None = None
    self.type_var_tuple_suffix: int | None = None
    self.add_type_vars()
    self.is_final = False
    self.is_enum = False
    self.fallback_to_any = False
    self._promote = []
    self.alt_promote = None
    self.tuple_type = None
    self.special_alias = None
    self.is_named_tuple = False
    self.typeddict_type = None
    self.is_newtype = False
    self.is_intersection = False
    self.metadata = {}

</t>
<t tx="ekr.20221004064035.8">def __init__(self, name: str, timeout: float | None) -&gt; None:
    super().__init__(name, timeout)
    if sys.platform == "win32":
        timeout = int(self.timeout * 1000) if self.timeout else _winapi.NMPWAIT_WAIT_FOREVER
        try:
            _winapi.WaitNamedPipe(self.name, timeout)
        except FileNotFoundError as e:
            raise IPCException(f"The NamedPipe at {self.name} was not found.") from e
        except OSError as e:
            if e.winerror == _winapi.ERROR_SEM_TIMEOUT:
                raise IPCException("Timed out waiting for connection.") from e
            else:
                raise
        try:
            self.connection = _winapi.CreateFile(
                self.name,
                _winapi.GENERIC_READ | _winapi.GENERIC_WRITE,
                0,
                _winapi.NULL,
                _winapi.OPEN_EXISTING,
                _winapi.FILE_FLAG_OVERLAPPED,
                _winapi.NULL,
            )
        except OSError as e:
            if e.winerror == _winapi.ERROR_PIPE_BUSY:
                raise IPCException("The connection is busy.") from e
            else:
                raise
        _winapi.SetNamedPipeHandleState(
            self.connection, _winapi.PIPE_READMODE_MESSAGE, None, None
        )
    else:
        self.connection = socket.socket(socket.AF_UNIX)
        self.connection.settimeout(timeout)
        self.connection.connect(name)

</t>
<t tx="ekr.20221004064035.80">def visit_tuple_expr(self, e: TupleExpr) -&gt; Key | None:
    return self.seq_expr(e, "Tuple")

</t>
<t tx="ekr.20221004064035.800">def add_type_vars(self) -&gt; None:
    self.has_type_var_tuple_type = False
    if self.defn.type_vars:
        for i, vd in enumerate(self.defn.type_vars):
            if isinstance(vd, mypy.types.ParamSpecType):
                self.has_param_spec_type = True
            if isinstance(vd, mypy.types.TypeVarTupleType):
                assert not self.has_type_var_tuple_type
                self.has_type_var_tuple_type = True
                self.type_var_tuple_prefix = i
                self.type_var_tuple_suffix = len(self.defn.type_vars) - i - 1
            self.type_vars.append(vd.name)
    assert not (
        self.has_param_spec_type and self.has_type_var_tuple_type
    ), "Mixing type var tuples and param specs not supported yet"

</t>
<t tx="ekr.20221004064035.801">@property
def name(self) -&gt; str:
    """Short name."""
    return self.defn.name

</t>
<t tx="ekr.20221004064035.802">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20221004064035.803">def is_generic(self) -&gt; bool:
    """Is the type generic (i.e. does it have type variables)?"""
    return len(self.type_vars) &gt; 0

</t>
<t tx="ekr.20221004064035.804">def get(self, name: str) -&gt; SymbolTableNode | None:
    for cls in self.mro:
        n = cls.names.get(name)
        if n:
            return n
    return None

</t>
<t tx="ekr.20221004064035.805">def get_containing_type_info(self, name: str) -&gt; TypeInfo | None:
    for cls in self.mro:
        if name in cls.names:
            return cls
    return None

</t>
<t tx="ekr.20221004064035.806">@property
def protocol_members(self) -&gt; list[str]:
    # Protocol members are names of all attributes/methods defined in a protocol
    # and in all its supertypes (except for 'object').
    members: set[str] = set()
    assert self.mro, "This property can be only accessed after MRO is (re-)calculated"
    for base in self.mro[:-1]:  # we skip "object" since everyone implements it
        if base.is_protocol:
            for name, node in base.names.items():
                if isinstance(node.node, (TypeAlias, TypeVarExpr)):
                    # These are auxiliary definitions (and type aliases are prohibited).
                    continue
                members.add(name)
    return sorted(list(members))

</t>
<t tx="ekr.20221004064035.807">def __getitem__(self, name: str) -&gt; SymbolTableNode:
    n = self.get(name)
    if n:
        return n
    else:
        raise KeyError(name)

</t>
<t tx="ekr.20221004064035.808">def __repr__(self) -&gt; str:
    return f"&lt;TypeInfo {self.fullname}&gt;"

</t>
<t tx="ekr.20221004064035.809">def __bool__(self) -&gt; bool:
    # We defined this here instead of just overriding it in
    # FakeInfo so that mypyc can generate a direct call instead of
    # using the generic bool handling.
    return not isinstance(self, FakeInfo)

</t>
<t tx="ekr.20221004064035.81">def visit_set_expr(self, e: SetExpr) -&gt; Key | None:
    return self.seq_expr(e, "Set")

</t>
<t tx="ekr.20221004064035.810">def has_readable_member(self, name: str) -&gt; bool:
    return self.get(name) is not None

</t>
<t tx="ekr.20221004064035.811">def get_method(self, name: str) -&gt; FuncBase | Decorator | None:
    for cls in self.mro:
        if name in cls.names:
            node = cls.names[name].node
            if isinstance(node, FuncBase):
                return node
            elif isinstance(node, Decorator):  # Two `if`s make `mypyc` happy
                return node
            else:
                return None
    return None

</t>
<t tx="ekr.20221004064035.812">def calculate_metaclass_type(self) -&gt; mypy.types.Instance | None:
    declared = self.declared_metaclass
    if declared is not None and not declared.type.has_base("builtins.type"):
        return declared
    if self._fullname == "builtins.type":
        return mypy.types.Instance(self, [])
    candidates = [
        s.declared_metaclass
        for s in self.mro
        if s.declared_metaclass is not None and s.declared_metaclass.type is not None
    ]
    for c in candidates:
        if all(other.type in c.type.mro for other in candidates):
            return c
    return None

</t>
<t tx="ekr.20221004064035.813">def is_metaclass(self) -&gt; bool:
    return (
        self.has_base("builtins.type")
        or self.fullname == "abc.ABCMeta"
        or self.fallback_to_any
    )

</t>
<t tx="ekr.20221004064035.814">def has_base(self, fullname: str) -&gt; bool:
    """Return True if type has a base type with the specified name.

    This can be either via extension or via implementation.
    """
    for cls in self.mro:
        if cls.fullname == fullname:
            return True
    return False

</t>
<t tx="ekr.20221004064035.815">def direct_base_classes(self) -&gt; list[TypeInfo]:
    """Return a direct base classes.

    Omit base classes of other base classes.
    """
    return [base.type for base in self.bases]

</t>
<t tx="ekr.20221004064035.816">def update_tuple_type(self, typ: mypy.types.TupleType) -&gt; None:
    """Update tuple_type and special_alias as needed."""
    self.tuple_type = typ
    alias = TypeAlias.from_tuple_type(self)
    if not self.special_alias:
        self.special_alias = alias
    else:
        self.special_alias.target = alias.target

</t>
<t tx="ekr.20221004064035.817">def update_typeddict_type(self, typ: mypy.types.TypedDictType) -&gt; None:
    """Update typeddict_type and special_alias as needed."""
    self.typeddict_type = typ
    alias = TypeAlias.from_typeddict_type(self)
    if not self.special_alias:
        self.special_alias = alias
    else:
        self.special_alias.target = alias.target

</t>
<t tx="ekr.20221004064035.818">def __str__(self) -&gt; str:
    """Return a string representation of the type.

    This includes the most important information about the type.
    """
    return self.dump()

</t>
<t tx="ekr.20221004064035.819">def dump(
    self,
    str_conv: mypy.strconv.StrConv | None = None,
    type_str_conv: mypy.types.TypeStrVisitor | None = None,
) -&gt; str:
    """Return a string dump of the contents of the TypeInfo."""
    if not str_conv:
        str_conv = mypy.strconv.StrConv()
    base: str = ""

    def type_str(typ: mypy.types.Type) -&gt; str:
        if type_str_conv:
            return typ.accept(type_str_conv)
        return str(typ)

    head = "TypeInfo" + str_conv.format_id(self)
    if self.bases:
        base = f"Bases({', '.join(type_str(base) for base in self.bases)})"
    mro = "Mro({})".format(
        ", ".join(item.fullname + str_conv.format_id(item) for item in self.mro)
    )
    names = []
    for name in sorted(self.names):
        description = name + str_conv.format_id(self.names[name].node)
        node = self.names[name].node
        if isinstance(node, Var) and node.type:
            description += f" ({type_str(node.type)})"
        names.append(description)
    items = [f"Name({self.fullname})", base, mro, ("Names", names)]
    if self.declared_metaclass:
        items.append(f"DeclaredMetaclass({type_str(self.declared_metaclass)})")
    if self.metaclass_type:
        items.append(f"MetaclassType({type_str(self.metaclass_type)})")
    return mypy.strconv.dump_tagged(items, head, str_conv=str_conv)

</t>
<t tx="ekr.20221004064035.82">def visit_index_expr(self, e: IndexExpr) -&gt; Key | None:
    if literal(e.index) == LITERAL_YES:
        return ("Index", literal_hash(e.base), literal_hash(e.index))
    return None

</t>
<t tx="ekr.20221004064035.820">def serialize(self) -&gt; JsonDict:
    # NOTE: This is where all ClassDefs originate, so there shouldn't be duplicates.
    data = {
        ".class": "TypeInfo",
        "module_name": self.module_name,
        "fullname": self.fullname,
        "names": self.names.serialize(self.fullname),
        "defn": self.defn.serialize(),
        "abstract_attributes": self.abstract_attributes,
        "type_vars": self.type_vars,
        "has_param_spec_type": self.has_param_spec_type,
        "bases": [b.serialize() for b in self.bases],
        "mro": [c.fullname for c in self.mro],
        "_promote": [p.serialize() for p in self._promote],
        "declared_metaclass": (
            None if self.declared_metaclass is None else self.declared_metaclass.serialize()
        ),
        "metaclass_type": None
        if self.metaclass_type is None
        else self.metaclass_type.serialize(),
        "tuple_type": None if self.tuple_type is None else self.tuple_type.serialize(),
        "typeddict_type": None
        if self.typeddict_type is None
        else self.typeddict_type.serialize(),
        "flags": get_flags(self, TypeInfo.FLAGS),
        "metadata": self.metadata,
        "slots": list(sorted(self.slots)) if self.slots is not None else None,
        "deletable_attributes": self.deletable_attributes,
    }
    return data

</t>
<t tx="ekr.20221004064035.821">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeInfo:
    names = SymbolTable.deserialize(data["names"])
    defn = ClassDef.deserialize(data["defn"])
    module_name = data["module_name"]
    ti = TypeInfo(names, defn, module_name)
    ti._fullname = data["fullname"]
    # TODO: Is there a reason to reconstruct ti.subtypes?
    ti.abstract_attributes = [(attr[0], attr[1]) for attr in data["abstract_attributes"]]
    ti.type_vars = data["type_vars"]
    ti.has_param_spec_type = data["has_param_spec_type"]
    ti.bases = [mypy.types.Instance.deserialize(b) for b in data["bases"]]
    ti._promote = [mypy.types.deserialize_type(p) for p in data["_promote"]]
    ti.declared_metaclass = (
        None
        if data["declared_metaclass"] is None
        else mypy.types.Instance.deserialize(data["declared_metaclass"])
    )
    ti.metaclass_type = (
        None
        if data["metaclass_type"] is None
        else mypy.types.Instance.deserialize(data["metaclass_type"])
    )
    # NOTE: ti.mro will be set in the fixup phase based on these
    # names.  The reason we need to store the mro instead of just
    # recomputing it from base classes has to do with a subtle
    # point about fine-grained incremental: the cache files might
    # not be loaded until after a class in the mro has changed its
    # bases, which causes the mro to change. If we recomputed our
    # mro, we would compute the *new* mro, which leaves us with no
    # way to detect that the mro has changed! Thus we need to make
    # sure to load the original mro so that once the class is
    # rechecked, it can tell that the mro has changed.
    ti._mro_refs = data["mro"]
    ti.tuple_type = (
        None
        if data["tuple_type"] is None
        else mypy.types.TupleType.deserialize(data["tuple_type"])
    )
    ti.typeddict_type = (
        None
        if data["typeddict_type"] is None
        else mypy.types.TypedDictType.deserialize(data["typeddict_type"])
    )
    ti.metadata = data["metadata"]
    ti.slots = set(data["slots"]) if data["slots"] is not None else None
    ti.deletable_attributes = data["deletable_attributes"]
    set_flags(ti, data["flags"])
    return ti


</t>
<t tx="ekr.20221004064035.822">class FakeInfo(TypeInfo):

    __slots__ = ("msg",)

    @others
</t>
<t tx="ekr.20221004064035.823"># types.py defines a single instance of this class, called types.NOT_READY.
# This instance is used as a temporary placeholder in the process of de-serialization
# of 'Instance' types. The de-serialization happens in two steps: In the first step,
# Instance.type is set to NOT_READY. In the second step (in fixup.py) it is replaced by
# an actual TypeInfo. If you see the assertion error below, then most probably something
# went wrong during the second step and an 'Instance' that raised this error was not fixed.
# Note:
# 'None' is not used as a dummy value for two reasons:
# 1. This will require around 80-100 asserts to make 'mypy --strict-optional mypy'
#    pass cleanly.
# 2. If NOT_READY value is accidentally used somewhere, it will be obvious where the value
#    is from, whereas a 'None' value could come from anywhere.
#
# Additionally, this serves as a more general-purpose placeholder
# for missing TypeInfos in a number of places where the excuses
# for not being Optional are a little weaker.
#
# TypeInfo defines a __bool__ method that returns False for FakeInfo
# so that it can be conveniently tested against in the same way that it
# would be if things were properly optional.
def __init__(self, msg: str) -&gt; None:
    self.msg = msg

</t>
<t tx="ekr.20221004064035.824">def __getattribute__(self, attr: str) -&gt; type:
    # Handle __class__ so that isinstance still works...
    if attr == "__class__":
        return object.__getattribute__(self, attr)  # type: ignore[no-any-return]
    raise AssertionError(object.__getattribute__(self, "msg"))


</t>
<t tx="ekr.20221004064035.825">VAR_NO_INFO: Final[TypeInfo] = FakeInfo("Var is lacking info")
CLASSDEF_NO_INFO: Final[TypeInfo] = FakeInfo("ClassDef is lacking info")
FUNC_NO_INFO: Final[TypeInfo] = FakeInfo("FuncBase for non-methods lack info")


</t>
<t tx="ekr.20221004064035.826">class TypeAlias(SymbolNode):
    """
    A symbol node representing a type alias.

    Type alias is a static concept, in contrast to variables with types
    like Type[...]. Namely:
        * type aliases
            - can be used in type context (annotations)
            - cannot be re-assigned
        * variables with type Type[...]
            - cannot be used in type context
            - but can be re-assigned

    An alias can be defined only by an assignment to a name (not any other lvalues).

    Such assignment defines an alias by default. To define a variable,
    an explicit Type[...] annotation is required. As an exception,
    at non-global scope non-subscripted rvalue creates a variable even without
    an annotation. This exception exists to accommodate the common use case of
    class-valued attributes. See SemanticAnalyzerPass2.check_and_set_up_type_alias
    for details.

    Aliases can be generic. Currently, mypy uses unbound type variables for
    generic aliases and identifies them by name. Essentially, type aliases
    work as macros that expand textually. The definition and expansion rules are
    following:

        1. An alias targeting a generic class without explicit variables act as
        the given class (this doesn't apply to TypedDict, Tuple and Callable, which
        are not proper classes but special type constructors):

            A = List
            AA = List[Any]

            x: A  # same as List[Any]
            x: A[int]  # same as List[int]

            x: AA  # same as List[Any]
            x: AA[int]  # Error!

            C = Callable  # Same as Callable[..., Any]
            T = Tuple  # Same as Tuple[Any, ...]

        2. An alias using explicit type variables in its rvalue expects
        replacements (type arguments) for these variables. If missing, they
        are treated as Any, like for other generics:

            B = List[Tuple[T, T]]

            x: B  # same as List[Tuple[Any, Any]]
            x: B[int]  # same as List[Tuple[int, int]]

            def f(x: B[T]) -&gt; T: ...  # without T, Any would be used here

        3. An alias can be defined using another aliases. In the definition
        rvalue the Any substitution doesn't happen for top level unsubscripted
        generic classes:

            A = List
            B = A  # here A is expanded to List, _not_ List[Any],
                   # to match the Python runtime behaviour
            x: B[int]  # same as List[int]
            C = List[A]  # this expands to List[List[Any]]

            AA = List[T]
            D = AA  # here AA expands to List[Any]
            x: D[int]  # Error!

    Note: the fact that we support aliases like `A = List` means that the target
    type will be initially an instance type with wrong number of type arguments.
    Such instances are all fixed either during or after main semantic analysis passes.
    We therefore store the difference between `List` and `List[Any]` rvalues (targets)
    using the `no_args` flag. See also TypeAliasExpr.no_args.

    Meaning of other fields:

    target: The target type. For generic aliases contains unbound type variables
        as nested types.
    _fullname: Qualified name of this type alias. This is used in particular
        to track fine grained dependencies from aliases.
    alias_tvars: Names of unbound type variables used to define this alias.
    normalized: Used to distinguish between `A = List`, and `A = list`. Both
        are internally stored using `builtins.list` (because `typing.List` is
        itself an alias), while the second cannot be subscripted because of
        Python runtime limitation.
    line and column: Line and column on the original alias definition.
    eager: If True, immediately expand alias when referred to (useful for aliases
        within functions that can't be looked up from the symbol table)
    """

    __slots__ = (
        "target",
        "_fullname",
        "alias_tvars",
        "no_args",
        "normalized",
        "_is_recursive",
        "eager",
    )

    @others
</t>
<t tx="ekr.20221004064035.827">def __init__(
    self,
    target: mypy.types.Type,
    fullname: str,
    line: int,
    column: int,
    *,
    alias_tvars: list[str] | None = None,
    no_args: bool = False,
    normalized: bool = False,
    eager: bool = False,
) -&gt; None:
    self._fullname = fullname
    self.target = target
    if alias_tvars is None:
        alias_tvars = []
    self.alias_tvars = alias_tvars
    self.no_args = no_args
    self.normalized = normalized
    # This attribute is manipulated by TypeAliasType. If non-None,
    # it is the cached value.
    self._is_recursive: bool | None = None
    self.eager = eager
    super().__init__(line, column)

</t>
<t tx="ekr.20221004064035.828">@classmethod
def from_tuple_type(cls, info: TypeInfo) -&gt; TypeAlias:
    """Generate an alias to the tuple type described by a given TypeInfo."""
    assert info.tuple_type
    return TypeAlias(
        info.tuple_type.copy_modified(fallback=mypy.types.Instance(info, info.defn.type_vars)),
        info.fullname,
        info.line,
        info.column,
    )

</t>
<t tx="ekr.20221004064035.829">@classmethod
def from_typeddict_type(cls, info: TypeInfo) -&gt; TypeAlias:
    """Generate an alias to the TypedDict type described by a given TypeInfo."""
    assert info.typeddict_type
    return TypeAlias(
        info.typeddict_type.copy_modified(
            fallback=mypy.types.Instance(info, info.defn.type_vars)
        ),
        info.fullname,
        info.line,
        info.column,
    )

</t>
<t tx="ekr.20221004064035.83">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; Key | None:
    return literal_hash(e.target)

</t>
<t tx="ekr.20221004064035.830">@property
def name(self) -&gt; str:
    return self._fullname.split(".")[-1]

</t>
<t tx="ekr.20221004064035.831">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20221004064035.832">def serialize(self) -&gt; JsonDict:
    data: JsonDict = {
        ".class": "TypeAlias",
        "fullname": self._fullname,
        "target": self.target.serialize(),
        "alias_tvars": self.alias_tvars,
        "no_args": self.no_args,
        "normalized": self.normalized,
        "line": self.line,
        "column": self.column,
    }
    return data

</t>
<t tx="ekr.20221004064035.833">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_type_alias(self)

</t>
<t tx="ekr.20221004064035.834">@classmethod
def deserialize(cls, data: JsonDict) -&gt; TypeAlias:
    assert data[".class"] == "TypeAlias"
    fullname = data["fullname"]
    alias_tvars = data["alias_tvars"]
    target = mypy.types.deserialize_type(data["target"])
    no_args = data["no_args"]
    normalized = data["normalized"]
    line = data["line"]
    column = data["column"]
    return cls(
        target,
        fullname,
        line,
        column,
        alias_tvars=alias_tvars,
        no_args=no_args,
        normalized=normalized,
    )


</t>
<t tx="ekr.20221004064035.835">class PlaceholderNode(SymbolNode):
    """Temporary symbol node that will later become a real SymbolNode.

    These are only present during semantic analysis when using the new
    semantic analyzer. These are created if some essential dependencies
    of a definition are not yet complete.

    A typical use is for names imported from a module which is still
    incomplete (within an import cycle):

      from m import f  # Initially may create PlaceholderNode

    This is particularly important if the imported shadows a name from
    an enclosing scope or builtins:

      from m import int  # Placeholder avoids mixups with builtins.int

    Another case where this is useful is when there is another definition
    or assignment:

      from m import f
      def f() -&gt; None: ...

    In the above example, the presence of PlaceholderNode allows us to
    handle the second definition as a redefinition.

    They are also used to create PlaceholderType instances for types
    that refer to incomplete types. Example:

      class C(Sequence[C]): ...

    We create a PlaceholderNode (with becomes_typeinfo=True) for C so
    that the type C in Sequence[C] can be bound.

    Attributes:

      fullname: Full name of the PlaceholderNode.
      node: AST node that contains the definition that caused this to
          be created. This is useful for tracking order of incomplete definitions
          and for debugging.
      becomes_typeinfo: If True, this refers something that could later
          become a TypeInfo. It can't be used with type variables, in
          particular, as this would cause issues with class type variable
          detection.

    The long-term purpose of placeholder nodes/types is to evolve into
    something that can support general recursive types.
    """

    __slots__ = ("_fullname", "node", "becomes_typeinfo")

    @others
</t>
<t tx="ekr.20221004064035.836">def __init__(
    self, fullname: str, node: Node, line: int, *, becomes_typeinfo: bool = False
) -&gt; None:
    self._fullname = fullname
    self.node = node
    self.becomes_typeinfo = becomes_typeinfo
    self.line = line

</t>
<t tx="ekr.20221004064035.837">@property
def name(self) -&gt; str:
    return self._fullname.split(".")[-1]

</t>
<t tx="ekr.20221004064035.838">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20221004064035.839">def serialize(self) -&gt; JsonDict:
    assert False, "PlaceholderNode can't be serialized"

</t>
<t tx="ekr.20221004064035.84">def visit_call_expr(self, e: CallExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.840">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_placeholder_node(self)


</t>
<t tx="ekr.20221004064035.841">class SymbolTableNode:
    """Description of a name binding in a symbol table.

    These are only used as values in module (global), function (local)
    and class symbol tables (see SymbolTable). The name that is bound is
    the key in SymbolTable.

    Symbol tables don't contain direct references to AST nodes primarily
    because there can be multiple symbol table references to a single
    AST node (due to imports and aliases), and different references can
    behave differently. This class describes the unique properties of
    each reference.

    The most fundamental attribute is 'node', which is the AST node that
    the name refers to.

    The kind is usually one of LDEF, GDEF or MDEF, depending on the scope
    of the definition. These three kinds can usually be used
    interchangeably and the difference between local, global and class
    scopes is mostly descriptive, with no semantic significance.
    However, some tools that consume mypy ASTs may care about these so
    they should be correct.

    Attributes:
        node: AST node of definition. Among others, this can be one of
            FuncDef, Var, TypeInfo, TypeVarExpr or MypyFile -- or None
            for cross_ref that hasn't been fixed up yet.
        kind: Kind of node. Possible values:
               - LDEF: local definition
               - GDEF: global (module-level) definition
               - MDEF: class member definition
               - UNBOUND_IMPORTED: temporary kind for imported names (we
                 don't know the final kind yet)
        module_public: If False, this name won't be imported via
            'from &lt;module&gt; import *'. This has no effect on names within
            classes.
        module_hidden: If True, the name will be never exported (needed for
            stub files)
        cross_ref: For deserialized MypyFile nodes, the referenced module
            name; for other nodes, optionally the name of the referenced object.
        implicit: Was this defined by assignment to self attribute?
        plugin_generated: Was this symbol generated by a plugin?
            (And therefore needs to be removed in aststrip.)
        no_serialize: Do not serialize this node if True. This is used to prevent
            keys in the cache that refer to modules on which this file does not
            depend. Currently this can happen if there is a module not in build
            used e.g. like this:
                import a.b.c # type: ignore
            This will add a submodule symbol to parent module `a` symbol table,
            but `a.b` is _not_ added as its dependency. Therefore, we should
            not serialize these symbols as they may not be found during fixup
            phase, instead they will be re-added during subsequent patch parents
            phase.
            TODO: Refactor build.py to make dependency tracking more transparent
            and/or refactor look-up functions to not require parent patching.

    NOTE: No other attributes should be added to this class unless they
    are shared by all node kinds.
    """

    __slots__ = (
        "kind",
        "node",
        "module_public",
        "module_hidden",
        "cross_ref",
        "implicit",
        "plugin_generated",
        "no_serialize",
    )

    @others
</t>
<t tx="ekr.20221004064035.842">def __init__(
    self,
    kind: int,
    node: SymbolNode | None,
    module_public: bool = True,
    implicit: bool = False,
    module_hidden: bool = False,
    *,
    plugin_generated: bool = False,
    no_serialize: bool = False,
) -&gt; None:
    self.kind = kind
    self.node = node
    self.module_public = module_public
    self.implicit = implicit
    self.module_hidden = module_hidden
    self.cross_ref: str | None = None
    self.plugin_generated = plugin_generated
    self.no_serialize = no_serialize

</t>
<t tx="ekr.20221004064035.843">@property
def fullname(self) -&gt; str | None:
    if self.node is not None:
        return self.node.fullname
    else:
        return None

</t>
<t tx="ekr.20221004064035.844">@property
def type(self) -&gt; mypy.types.Type | None:
    node = self.node
    if isinstance(node, (Var, SYMBOL_FUNCBASE_TYPES)) and node.type is not None:
        return node.type
    elif isinstance(node, Decorator):
        return node.var.type
    else:
        return None

</t>
<t tx="ekr.20221004064035.845">def copy(self) -&gt; SymbolTableNode:
    new = SymbolTableNode(
        self.kind, self.node, self.module_public, self.implicit, self.module_hidden
    )
    new.cross_ref = self.cross_ref
    return new

</t>
<t tx="ekr.20221004064035.846">def __str__(self) -&gt; str:
    s = f"{node_kinds[self.kind]}/{short_type(self.node)}"
    if isinstance(self.node, SymbolNode):
        s += f" ({self.node.fullname})"
    # Include declared type of variables and functions.
    if self.type is not None:
        s += f" : {self.type}"
    return s

</t>
<t tx="ekr.20221004064035.847">def serialize(self, prefix: str, name: str) -&gt; JsonDict:
    """Serialize a SymbolTableNode.

    Args:
      prefix: full name of the containing module or class; or None
      name: name of this object relative to the containing object
    """
    data: JsonDict = {".class": "SymbolTableNode", "kind": node_kinds[self.kind]}
    if self.module_hidden:
        data["module_hidden"] = True
    if not self.module_public:
        data["module_public"] = False
    if self.implicit:
        data["implicit"] = True
    if self.plugin_generated:
        data["plugin_generated"] = True
    if isinstance(self.node, MypyFile):
        data["cross_ref"] = self.node.fullname
    else:
        assert self.node is not None, f"{prefix}:{name}"
        if prefix is not None:
            fullname = self.node.fullname
            if (
                # See the comment above SymbolNode.fullname -- fullname can often be None,
                # but for complex reasons it's annotated as being `Bogus[str]` instead of `str | None`,
                # meaning mypy erroneously thinks the `fullname is not None` check here is redundant
                fullname is not None  # type: ignore[redundant-expr]
                and "." in fullname
                and fullname != prefix + "." + name
                and not (isinstance(self.node, Var) and self.node.from_module_getattr)
            ):
                assert not isinstance(
                    self.node, PlaceholderNode
                ), f"Definition of {fullname} is unexpectedly incomplete"
                data["cross_ref"] = fullname
                return data
        data["node"] = self.node.serialize()
    return data

</t>
<t tx="ekr.20221004064035.848">@classmethod
def deserialize(cls, data: JsonDict) -&gt; SymbolTableNode:
    assert data[".class"] == "SymbolTableNode"
    kind = inverse_node_kinds[data["kind"]]
    if "cross_ref" in data:
        # This will be fixed up later.
        stnode = SymbolTableNode(kind, None)
        stnode.cross_ref = data["cross_ref"]
    else:
        assert "node" in data, data
        node = SymbolNode.deserialize(data["node"])
        stnode = SymbolTableNode(kind, node)
    if "module_hidden" in data:
        stnode.module_hidden = data["module_hidden"]
    if "module_public" in data:
        stnode.module_public = data["module_public"]
    if "implicit" in data:
        stnode.implicit = data["implicit"]
    if "plugin_generated" in data:
        stnode.plugin_generated = data["plugin_generated"]
    return stnode


</t>
<t tx="ekr.20221004064035.849">class SymbolTable(Dict[str, SymbolTableNode]):
    """Static representation of a namespace dictionary.

    This is used for module, class and function namespaces.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.85">def visit_slice_expr(self, e: SliceExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.850">def __str__(self) -&gt; str:
    a: list[str] = []
    for key, value in self.items():
        # Filter out the implicit import of builtins.
        if isinstance(value, SymbolTableNode):
            if (
                value.fullname != "builtins"
                and (value.fullname or "").split(".")[-1] not in implicit_module_attrs
            ):
                a.append("  " + str(key) + " : " + str(value))
        else:
            a.append("  &lt;invalid item&gt;")
    a = sorted(a)
    a.insert(0, "SymbolTable(")
    a[-1] += ")"
    return "\n".join(a)

</t>
<t tx="ekr.20221004064035.851">def copy(self) -&gt; SymbolTable:
    return SymbolTable([(key, node.copy()) for key, node in self.items()])

</t>
<t tx="ekr.20221004064035.852">def serialize(self, fullname: str) -&gt; JsonDict:
    data: JsonDict = {".class": "SymbolTable"}
    for key, value in self.items():
        # Skip __builtins__: it's a reference to the builtins
        # module that gets added to every module by
        # SemanticAnalyzerPass2.visit_file(), but it shouldn't be
        # accessed by users of the module.
        if key == "__builtins__" or value.no_serialize:
            continue
        data[key] = value.serialize(fullname, key)
    return data

</t>
<t tx="ekr.20221004064035.853">@classmethod
def deserialize(cls, data: JsonDict) -&gt; SymbolTable:
    assert data[".class"] == "SymbolTable"
    st = SymbolTable()
    for key, value in data.items():
        if key != ".class":
            st[key] = SymbolTableNode.deserialize(value)
    return st


</t>
<t tx="ekr.20221004064035.854">def get_flags(node: Node, names: list[str]) -&gt; list[str]:
    return [name for name in names if getattr(node, name)]


</t>
<t tx="ekr.20221004064035.855">def set_flags(node: Node, flags: list[str]) -&gt; None:
    for name in flags:
        setattr(node, name, True)


</t>
<t tx="ekr.20221004064035.856">def get_member_expr_fullname(expr: MemberExpr) -&gt; str | None:
    """Return the qualified name representation of a member expression.

    Return a string of form foo.bar, foo.bar.baz, or similar, or None if the
    argument cannot be represented in this form.
    """
    initial: str | None = None
    if isinstance(expr.expr, NameExpr):
        initial = expr.expr.name
    elif isinstance(expr.expr, MemberExpr):
        initial = get_member_expr_fullname(expr.expr)
    else:
        return None
    return f"{initial}.{expr.name}"


</t>
<t tx="ekr.20221004064035.857">deserialize_map: Final = {
    key: obj.deserialize
    for key, obj in globals().items()
    if type(obj) is not FakeInfo
    and isinstance(obj, type)
    and issubclass(obj, SymbolNode)
    and obj is not SymbolNode
}


</t>
<t tx="ekr.20221004064035.858">def check_arg_kinds(
    arg_kinds: list[ArgKind], nodes: list[T], fail: Callable[[str, T], None]
) -&gt; None:
    is_var_arg = False
    is_kw_arg = False
    seen_named = False
    seen_opt = False
    for kind, node in zip(arg_kinds, nodes):
        if kind == ARG_POS:
            if is_var_arg or is_kw_arg or seen_named or seen_opt:
                fail(
                    "Required positional args may not appear after default, named or var args",
                    node,
                )
                break
        elif kind == ARG_OPT:
            if is_var_arg or is_kw_arg or seen_named:
                fail("Positional default args may not appear after named or var args", node)
                break
            seen_opt = True
        elif kind == ARG_STAR:
            if is_var_arg or is_kw_arg or seen_named:
                fail("Var args may not appear after named or var args", node)
                break
            is_var_arg = True
        elif kind == ARG_NAMED or kind == ARG_NAMED_OPT:
            seen_named = True
            if is_kw_arg:
                fail("A **kwargs argument must be the last argument", node)
                break
        elif kind == ARG_STAR2:
            if is_kw_arg:
                fail("You may only have one **kwargs argument", node)
                break
            is_kw_arg = True


</t>
<t tx="ekr.20221004064035.859">def check_arg_names(
    names: Sequence[str | None],
    nodes: list[T],
    fail: Callable[[str, T], None],
    description: str = "function definition",
) -&gt; None:
    seen_names: set[str | None] = set()
    for name, node in zip(names, nodes):
        if name is not None and name in seen_names:
            fail(f'Duplicate argument "{name}" in {description}', node)
            break
        seen_names.add(name)


</t>
<t tx="ekr.20221004064035.86">def visit_cast_expr(self, e: CastExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.860">def is_class_var(expr: NameExpr) -&gt; bool:
    """Return whether the expression is ClassVar[...]"""
    if isinstance(expr.node, Var):
        return expr.node.is_classvar
    return False


</t>
<t tx="ekr.20221004064035.861">def is_final_node(node: SymbolNode | None) -&gt; bool:
    """Check whether `node` corresponds to a final attribute."""
    return isinstance(node, (Var, FuncDef, OverloadedFuncDef, Decorator)) and node.is_final


</t>
<t tx="ekr.20221004064035.862">def local_definitions(
    names: SymbolTable, name_prefix: str, info: TypeInfo | None = None
) -&gt; Iterator[Definition]:
    """Iterate over local definitions (not imported) in a symbol table.

    Recursively iterate over class members and nested classes.
    """
    # TODO: What should the name be? Or maybe remove it?
    for name, symnode in names.items():
        shortname = name
        if "-redef" in name:
            # Restore original name from mangled name of multiply defined function
            shortname = name.split("-redef")[0]
        fullname = name_prefix + "." + shortname
        node = symnode.node
        if node and node.fullname == fullname:
            yield fullname, symnode, info
            if isinstance(node, TypeInfo):
                yield from local_definitions(node.names, fullname, node)
</t>
<t tx="ekr.20221004064035.863">@path C:/Repos/ekr-mypy2/mypy/
"""Information about Python operators"""

from __future__ import annotations

from typing_extensions import Final

# Map from binary operator id to related method name (in Python 3).
op_methods: Final = {
    "+": "__add__",
    "-": "__sub__",
    "*": "__mul__",
    "/": "__truediv__",
    "%": "__mod__",
    "divmod": "__divmod__",
    "//": "__floordiv__",
    "**": "__pow__",
    "@": "__matmul__",
    "&amp;": "__and__",
    "|": "__or__",
    "^": "__xor__",
    "&lt;&lt;": "__lshift__",
    "&gt;&gt;": "__rshift__",
    "==": "__eq__",
    "!=": "__ne__",
    "&lt;": "__lt__",
    "&gt;=": "__ge__",
    "&gt;": "__gt__",
    "&lt;=": "__le__",
    "in": "__contains__",
}

op_methods_to_symbols: Final = {v: k for (k, v) in op_methods.items()}

ops_falling_back_to_cmp: Final = {"__ne__", "__eq__", "__lt__", "__le__", "__gt__", "__ge__"}


ops_with_inplace_method: Final = {
    "+",
    "-",
    "*",
    "/",
    "%",
    "//",
    "**",
    "@",
    "&amp;",
    "|",
    "^",
    "&lt;&lt;",
    "&gt;&gt;",
}

inplace_operator_methods: Final = {"__i" + op_methods[op][2:] for op in ops_with_inplace_method}

reverse_op_methods: Final = {
    "__add__": "__radd__",
    "__sub__": "__rsub__",
    "__mul__": "__rmul__",
    "__truediv__": "__rtruediv__",
    "__mod__": "__rmod__",
    "__divmod__": "__rdivmod__",
    "__floordiv__": "__rfloordiv__",
    "__pow__": "__rpow__",
    "__matmul__": "__rmatmul__",
    "__and__": "__rand__",
    "__or__": "__ror__",
    "__xor__": "__rxor__",
    "__lshift__": "__rlshift__",
    "__rshift__": "__rrshift__",
    "__eq__": "__eq__",
    "__ne__": "__ne__",
    "__lt__": "__gt__",
    "__ge__": "__le__",
    "__gt__": "__lt__",
    "__le__": "__ge__",
}

reverse_op_method_names: Final = set(reverse_op_methods.values())

# Suppose we have some class A. When we do A() + A(), Python will only check
# the output of A().__add__(A()) and skip calling the __radd__ method entirely.
# This shortcut is used only for the following methods:
op_methods_that_shortcut: Final = {
    "__add__",
    "__sub__",
    "__mul__",
    "__truediv__",
    "__mod__",
    "__divmod__",
    "__floordiv__",
    "__pow__",
    "__matmul__",
    "__and__",
    "__or__",
    "__xor__",
    "__lshift__",
    "__rshift__",
}

normal_from_reverse_op: Final = {m: n for n, m in reverse_op_methods.items()}
reverse_op_method_set: Final = set(reverse_op_methods.values())

unary_op_methods: Final = {"-": "__neg__", "+": "__pos__", "~": "__invert__"}
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.864">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import pprint
import re
import sys
from typing import Any, Callable, Dict, Mapping, Pattern
from typing_extensions import Final

from mypy import defaults
from mypy.errorcodes import ErrorCode, error_codes
from mypy.util import get_class_descriptors, replace_object_state


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.865">class BuildType:
    STANDARD: Final = 0
    MODULE: Final = 1
    PROGRAM_TEXT: Final = 2


</t>
<t tx="ekr.20221004064035.866">PER_MODULE_OPTIONS: Final = {
    # Please keep this list sorted
    "allow_redefinition",
    "allow_untyped_globals",
    "always_false",
    "always_true",
    "check_untyped_defs",
    "debug_cache",
    "disable_error_code",
    "disabled_error_codes",
    "disallow_any_decorated",
    "disallow_any_explicit",
    "disallow_any_expr",
    "disallow_any_generics",
    "disallow_any_unimported",
    "disallow_incomplete_defs",
    "disallow_subclassing_any",
    "disallow_untyped_calls",
    "disallow_untyped_decorators",
    "disallow_untyped_defs",
    "enable_error_code",
    "enabled_error_codes",
    "follow_imports_for_stubs",
    "follow_imports",
    "ignore_errors",
    "ignore_missing_imports",
    "implicit_optional",
    "implicit_reexport",
    "local_partial_types",
    "mypyc",
    "strict_concatenate",
    "strict_equality",
    "strict_optional",
    "warn_no_return",
    "warn_return_any",
    "warn_unreachable",
    "warn_unused_ignores",
}

OPTIONS_AFFECTING_CACHE: Final = (PER_MODULE_OPTIONS | {"platform", "bazel", "plugins"}) - {
    "debug_cache"
}


</t>
<t tx="ekr.20221004064035.867">class Options:
    """Options collected from flags."""

    @others
</t>
<t tx="ekr.20221004064035.868">def __init__(self) -&gt; None:
    # Cache for clone_for_module()
    self._per_module_cache: dict[str, Options] | None = None

    # -- build options --
    self.build_type = BuildType.STANDARD
    self.python_version: tuple[int, int] = sys.version_info[:2]
    # The executable used to search for PEP 561 packages. If this is None,
    # then mypy does not search for PEP 561 packages.
    self.python_executable: str | None = sys.executable
    self.platform = sys.platform
    self.custom_typing_module: str | None = None
    self.custom_typeshed_dir: str | None = None
    # The abspath() version of the above, we compute it once as an optimization.
    self.abs_custom_typeshed_dir: str | None = None
    self.mypy_path: list[str] = []
    self.report_dirs: dict[str, str] = {}
    # Show errors in PEP 561 packages/site-packages modules
    self.no_silence_site_packages = False
    self.no_site_packages = False
    self.ignore_missing_imports = False
    # Is ignore_missing_imports set in a per-module section
    self.ignore_missing_imports_per_module = False
    self.follow_imports = "normal"  # normal|silent|skip|error
    # Whether to respect the follow_imports setting even for stub files.
    # Intended to be used for disabling specific stubs.
    self.follow_imports_for_stubs = False
    # PEP 420 namespace packages
    # This allows definitions of packages without __init__.py and allows packages to span
    # multiple directories. This flag affects both import discovery and the association of
    # input files/modules/packages to the relevant file and fully qualified module name.
    self.namespace_packages = True
    # Use current directory and MYPYPATH to determine fully qualified module names of files
    # passed by automatically considering their subdirectories as packages. This is only
    # relevant if namespace packages are enabled, since otherwise examining __init__.py's is
    # sufficient to determine module names for files. As a possible alternative, add a single
    # top-level __init__.py to your packages.
    self.explicit_package_bases = False
    # File names, directory names or subpaths to avoid checking
    self.exclude: list[str] = []

    # disallow_any options
    self.disallow_any_generics = False
    self.disallow_any_unimported = False
    self.disallow_any_expr = False
    self.disallow_any_decorated = False
    self.disallow_any_explicit = False

    # Disallow calling untyped functions from typed ones
    self.disallow_untyped_calls = False

    # Disallow defining untyped (or incompletely typed) functions
    self.disallow_untyped_defs = False

    # Disallow defining incompletely typed functions
    self.disallow_incomplete_defs = False

    # Type check unannotated functions
    self.check_untyped_defs = False

    # Disallow decorating typed functions with untyped decorators
    self.disallow_untyped_decorators = False

    # Disallow subclassing values of type 'Any'
    self.disallow_subclassing_any = False

    # Also check typeshed for missing annotations
    self.warn_incomplete_stub = False

    # Warn about casting an expression to its inferred type
    self.warn_redundant_casts = False

    # Warn about falling off the end of a function returning non-None
    self.warn_no_return = True

    # Warn about returning objects of type Any when the function is
    # declared with a precise type
    self.warn_return_any = False

    # Warn about unused '# type: ignore' comments
    self.warn_unused_ignores = False

    # Warn about unused '[mypy-&lt;pattern&gt;]'  or '[[tool.mypy.overrides]]' config sections
    self.warn_unused_configs = False

    # Files in which to ignore all non-fatal errors
    self.ignore_errors = False

    # Apply strict None checking
    self.strict_optional = True

    # Show "note: In function "foo":" messages.
    self.show_error_context = False

    # Use nicer output (when possible).
    self.color_output = True
    self.error_summary = True

    # Assume arguments with default values of None are Optional
    self.implicit_optional = False

    # Don't re-export names unless they are imported with `from ... as ...`
    self.implicit_reexport = True

    # Suppress toplevel errors caused by missing annotations
    self.allow_untyped_globals = False

    # Allow variable to be redefined with an arbitrary type in the same block
    # and the same nesting level as the initialization
    self.allow_redefinition = False

    # Prohibit equality, identity, and container checks for non-overlapping types.
    # This makes 1 == '1', 1 in ['1'], and 1 is '1' errors.
    self.strict_equality = False

    # Make arguments prepended via Concatenate be truly positional-only.
    self.strict_concatenate = False

    # Report an error for any branches inferred to be unreachable as a result of
    # type analysis.
    self.warn_unreachable = False

    # Variable names considered True
    self.always_true: list[str] = []

    # Variable names considered False
    self.always_false: list[str] = []

    # Error codes to disable
    self.disable_error_code: list[str] = []
    self.disabled_error_codes: set[ErrorCode] = set()

    # Error codes to enable
    self.enable_error_code: list[str] = []
    self.enabled_error_codes: set[ErrorCode] = set()

    # Use script name instead of __main__
    self.scripts_are_modules = False

    # Config file name
    self.config_file: str | None = None

    # A filename containing a JSON mapping from filenames to
    # mtime/size/hash arrays, used to avoid having to recalculate
    # source hashes as often.
    self.quickstart_file: str | None = None

    # A comma-separated list of files/directories for mypy to type check;
    # supports globbing
    self.files: list[str] | None = None

    # Write junit.xml to given file
    self.junit_xml: str | None = None

    # Caching and incremental checking options
    self.incremental = True
    self.cache_dir = defaults.CACHE_DIR
    self.sqlite_cache = False
    self.debug_cache = False
    self.skip_version_check = False
    self.skip_cache_mtime_checks = False
    self.fine_grained_incremental = False
    # Include fine-grained dependencies in written cache files
    self.cache_fine_grained = False
    # Read cache files in fine-grained incremental mode (cache must include dependencies)
    self.use_fine_grained_cache = False

    # Tune certain behaviors when being used as a front-end to mypyc. Set per-module
    # in modules being compiled. Not in the config file or command line.
    self.mypyc = False

    # An internal flag to modify some type-checking logic while
    # running inspections (e.g. don't expand function definitions).
    # Not in the config file or command line.
    self.inspections = False

    # Disable the memory optimization of freeing ASTs when
    # possible. This isn't exposed as a command line option
    # because it is intended for software integrating with
    # mypy. (Like mypyc.)
    self.preserve_asts = False

    # Paths of user plugins
    self.plugins: list[str] = []

    # Per-module options (raw)
    self.per_module_options: dict[str, dict[str, object]] = {}
    self._glob_options: list[tuple[str, Pattern[str]]] = []
    self.unused_configs: set[str] = set()

    # -- development options --
    self.verbosity = 0  # More verbose messages (for troubleshooting)
    self.pdb = False
    self.show_traceback = False
    self.raise_exceptions = False
    self.dump_type_stats = False
    self.dump_inference_stats = False
    self.dump_build_stats = False
    self.enable_incomplete_features = False
    self.timing_stats: str | None = None

    # -- test options --
    # Stop after the semantic analysis phase
    self.semantic_analysis_only = False

    # Use stub builtins fixtures to speed up tests
    self.use_builtins_fixtures = False

    # -- experimental options --
    self.shadow_file: list[list[str]] | None = None
    self.show_column_numbers: bool = False
    self.show_error_end: bool = False
    self.hide_error_codes = False
    # Use soft word wrap and show trimmed source snippets with error location markers.
    self.pretty = False
    self.dump_graph = False
    self.dump_deps = False
    self.logical_deps = False
    # If True, partial types can't span a module top level and a function
    self.local_partial_types = False
    # Some behaviors are changed when using Bazel (https://bazel.build).
    self.bazel = False
    # If True, export inferred types for all expressions as BuildResult.types
    self.export_types = False
    # List of package roots -- directories under these are packages even
    # if they don't have __init__.py.
    self.package_root: list[str] = []
    self.cache_map: dict[str, tuple[str, str]] = {}
    # Don't properly free objects on exit, just kill the current process.
    self.fast_exit = True
    # fast path for finding modules from source set
    self.fast_module_lookup = False
    # Allow empty function bodies even if it is not safe, used for testing only.
    self.allow_empty_bodies = False
    # Used to transform source code before parsing if not None
    # TODO: Make the type precise (AnyStr -&gt; AnyStr)
    self.transform_source: Callable[[Any], Any] | None = None
    # Print full path to each file in the report.
    self.show_absolute_path: bool = False
    # Install missing stub packages if True
    self.install_types = False
    # Install missing stub packages in non-interactive mode (don't prompt for
    # confirmation, and don't show any errors)
    self.non_interactive = False
    # When we encounter errors that may cause many additional errors,
    # skip most errors after this many messages have been reported.
    # -1 means unlimited.
    self.many_errors_threshold = defaults.MANY_ERRORS_THRESHOLD
    # Disable recursive type aliases (currently experimental)
    self.disable_recursive_aliases = False

</t>
<t tx="ekr.20221004064035.869"># To avoid breaking plugin compatibility, keep providing new_semantic_analyzer
@property
def new_semantic_analyzer(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20221004064035.87">def visit_assert_type_expr(self, e: AssertTypeExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.870">def snapshot(self) -&gt; object:
    """Produce a comparable snapshot of this Option"""
    # Under mypyc, we don't have a __dict__, so we need to do worse things.
    d = dict(getattr(self, "__dict__", ()))
    for k in get_class_descriptors(Options):
        if hasattr(self, k) and k != "new_semantic_analyzer":
            d[k] = getattr(self, k)
    # Remove private attributes from snapshot
    d = {k: v for k, v in d.items() if not k.startswith("_")}
    return d

</t>
<t tx="ekr.20221004064035.871">def __repr__(self) -&gt; str:
    return f"Options({pprint.pformat(self.snapshot())})"

</t>
<t tx="ekr.20221004064035.872">def apply_changes(self, changes: dict[str, object]) -&gt; Options:
    new_options = Options()
    # Under mypyc, we don't have a __dict__, so we need to do worse things.
    replace_object_state(new_options, self, copy_dict=True)
    for key, value in changes.items():
        setattr(new_options, key, value)
    if changes.get("ignore_missing_imports"):
        # This is the only option for which a per-module and a global
        # option sometimes beheave differently.
        new_options.ignore_missing_imports_per_module = True

    # These two act as overrides, so apply them when cloning.
    # Similar to global codes enabling overrides disabling, so we start from latter.
    new_options.disabled_error_codes = self.disabled_error_codes.copy()
    new_options.enabled_error_codes = self.enabled_error_codes.copy()
    for code_str in new_options.disable_error_code:
        code = error_codes[code_str]
        new_options.disabled_error_codes.add(code)
        new_options.enabled_error_codes.discard(code)
    for code_str in new_options.enable_error_code:
        code = error_codes[code_str]
        new_options.enabled_error_codes.add(code)
        new_options.disabled_error_codes.discard(code)

    return new_options

</t>
<t tx="ekr.20221004064035.873">def build_per_module_cache(self) -&gt; None:
    self._per_module_cache = {}

    # Config precedence is as follows:
    #  1. Concrete section names: foo.bar.baz
    #  2. "Unstructured" glob patterns: foo.*.baz, in the order
    #     they appear in the file (last wins)
    #  3. "Well-structured" wildcard patterns: foo.bar.*, in specificity order.

    # Since structured configs inherit from structured configs above them in the hierarchy,
    # we need to process per-module configs in a careful order.
    # We have to process foo.* before foo.bar.* before foo.bar,
    # and we need to apply *.bar to foo.bar but not to foo.bar.*.
    # To do this, process all well-structured glob configs before non-glob configs and
    # exploit the fact that foo.* sorts earlier ASCIIbetically (unicodebetically?)
    # than foo.bar.*.
    # (A section being "processed last" results in its config "winning".)
    # Unstructured glob configs are stored and are all checked for each module.
    unstructured_glob_keys = [k for k in self.per_module_options.keys() if "*" in k[:-1]]
    structured_keys = [k for k in self.per_module_options.keys() if "*" not in k[:-1]]
    wildcards = sorted(k for k in structured_keys if k.endswith(".*"))
    concrete = [k for k in structured_keys if not k.endswith(".*")]

    for glob in unstructured_glob_keys:
        self._glob_options.append((glob, self.compile_glob(glob)))

    # We (for ease of implementation) treat unstructured glob
    # sections as used if any real modules use them or if any
    # concrete config sections use them. This means we need to
    # track which get used while constructing.
    self.unused_configs = set(unstructured_glob_keys)

    for key in wildcards + concrete:
        # Find what the options for this key would be, just based
        # on inheriting from parent configs.
        options = self.clone_for_module(key)
        # And then update it with its per-module options.
        self._per_module_cache[key] = options.apply_changes(self.per_module_options[key])

    # Add the more structured sections into unused configs, since
    # they only count as used if actually used by a real module.
    self.unused_configs.update(structured_keys)

</t>
<t tx="ekr.20221004064035.874">def clone_for_module(self, module: str) -&gt; Options:
    """Create an Options object that incorporates per-module options.

    NOTE: Once this method is called all Options objects should be
    considered read-only, else the caching might be incorrect.
    """
    if self._per_module_cache is None:
        self.build_per_module_cache()
    assert self._per_module_cache is not None

    # If the module just directly has a config entry, use it.
    if module in self._per_module_cache:
        self.unused_configs.discard(module)
        return self._per_module_cache[module]

    # If not, search for glob paths at all the parents. So if we are looking for
    # options for foo.bar.baz, we search foo.bar.baz.*, foo.bar.*, foo.*,
    # in that order, looking for an entry.
    # This is technically quadratic in the length of the path, but module paths
    # don't actually get all that long.
    options = self
    path = module.split(".")
    for i in range(len(path), 0, -1):
        key = ".".join(path[:i] + ["*"])
        if key in self._per_module_cache:
            self.unused_configs.discard(key)
            options = self._per_module_cache[key]
            break

    # OK and *now* we need to look for unstructured glob matches.
    # We only do this for concrete modules, not structured wildcards.
    if not module.endswith(".*"):
        for key, pattern in self._glob_options:
            if pattern.match(module):
                self.unused_configs.discard(key)
                options = options.apply_changes(self.per_module_options[key])

    # We could update the cache to directly point to modules once
    # they have been looked up, but in testing this made things
    # slower and not faster, so we don't bother.

    return options

</t>
<t tx="ekr.20221004064035.875">def compile_glob(self, s: str) -&gt; Pattern[str]:
    # Compile one of the glob patterns to a regex so that '.*' can
    # match *zero or more* module sections. This means we compile
    # '.*' into '(\..*)?'.
    parts = s.split(".")
    expr = re.escape(parts[0]) if parts[0] != "*" else ".*"
    for part in parts[1:]:
        expr += re.escape("." + part) if part != "*" else r"(\..*)?"
    return re.compile(expr + "\\Z")

</t>
<t tx="ekr.20221004064035.876">def select_options_affecting_cache(self) -&gt; Mapping[str, object]:
    result: Dict[str, object] = {}
    for opt in OPTIONS_AFFECTING_CACHE:
        val = getattr(self, opt)
        if opt in ("disabled_error_codes", "enabled_error_codes"):
            val = sorted([code.code for code in val])
        result[opt] = val
    return result
</t>
<t tx="ekr.20221004064035.877">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from mypy.errors import Errors
from mypy.nodes import MypyFile
from mypy.options import Options


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.878">def parse(
    source: str | bytes, fnam: str, module: str | None, errors: Errors | None, options: Options
) -&gt; MypyFile:
    """Parse a source file, without doing any semantic analysis.

    Return the parse tree. If errors is not provided, raise ParseError
    on failure. Otherwise, use the errors object to report parse errors.

    The python_version (major, minor) option determines the Python syntax variant.
    """
    if options.transform_source is not None:
        source = options.transform_source(source)
    import mypy.fastparse

    return mypy.fastparse.parse(source, fnam=fnam, module=module, errors=errors, options=options)
</t>
<t tx="ekr.20221004064035.879">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

from mypy import checker
from mypy.messages import MessageBuilder
from mypy.nodes import (
    AssertStmt,
    AssignmentExpr,
    AssignmentStmt,
    BreakStmt,
    ContinueStmt,
    DictionaryComprehension,
    Expression,
    ExpressionStmt,
    ForStmt,
    FuncDef,
    FuncItem,
    GeneratorExpr,
    IfStmt,
    ListExpr,
    Lvalue,
    NameExpr,
    RaiseStmt,
    ReturnStmt,
    TupleExpr,
    WhileStmt,
    WithStmt,
)
from mypy.traverser import ExtendedTraverserVisitor
from mypy.types import Type, UninhabitedType


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.88">def visit_conditional_expr(self, e: ConditionalExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.880">class BranchState:
    """BranchState contains information about variable definition at the end of a branching statement.
    `if` and `match` are examples of branching statements.

    `may_be_defined` contains variables that were defined in only some branches.
    `must_be_defined` contains variables that were defined in all branches.
    """

    @others
</t>
<t tx="ekr.20221004064035.881">def __init__(
    self,
    must_be_defined: set[str] | None = None,
    may_be_defined: set[str] | None = None,
    skipped: bool = False,
) -&gt; None:
    if may_be_defined is None:
        may_be_defined = set()
    if must_be_defined is None:
        must_be_defined = set()

    self.may_be_defined = set(may_be_defined)
    self.must_be_defined = set(must_be_defined)
    self.skipped = skipped


</t>
<t tx="ekr.20221004064035.882">class BranchStatement:
    @others
</t>
<t tx="ekr.20221004064035.883">def __init__(self, initial_state: BranchState) -&gt; None:
    self.initial_state = initial_state
    self.branches: list[BranchState] = [
        BranchState(must_be_defined=self.initial_state.must_be_defined)
    ]

</t>
<t tx="ekr.20221004064035.884">def next_branch(self) -&gt; None:
    self.branches.append(BranchState(must_be_defined=self.initial_state.must_be_defined))

</t>
<t tx="ekr.20221004064035.885">def record_definition(self, name: str) -&gt; None:
    assert len(self.branches) &gt; 0
    self.branches[-1].must_be_defined.add(name)
    self.branches[-1].may_be_defined.discard(name)

</t>
<t tx="ekr.20221004064035.886">def record_nested_branch(self, state: BranchState) -&gt; None:
    assert len(self.branches) &gt; 0
    current_branch = self.branches[-1]
    if state.skipped:
        current_branch.skipped = True
        return
    current_branch.must_be_defined.update(state.must_be_defined)
    current_branch.may_be_defined.update(state.may_be_defined)
    current_branch.may_be_defined.difference_update(current_branch.must_be_defined)

</t>
<t tx="ekr.20221004064035.887">def skip_branch(self) -&gt; None:
    assert len(self.branches) &gt; 0
    self.branches[-1].skipped = True

</t>
<t tx="ekr.20221004064035.888">def is_possibly_undefined(self, name: str) -&gt; bool:
    assert len(self.branches) &gt; 0
    return name in self.branches[-1].may_be_defined

</t>
<t tx="ekr.20221004064035.889">def done(self) -&gt; BranchState:
    branches = [b for b in self.branches if not b.skipped]
    if len(branches) == 0:
        return BranchState(skipped=True)
    if len(branches) == 1:
        return branches[0]

    # must_be_defined is a union of must_be_defined of all branches.
    must_be_defined = set(branches[0].must_be_defined)
    for b in branches[1:]:
        must_be_defined.intersection_update(b.must_be_defined)
    # may_be_defined are all variables that are not must be defined.
    all_vars = set()
    for b in branches:
        all_vars.update(b.may_be_defined)
        all_vars.update(b.must_be_defined)
    may_be_defined = all_vars.difference(must_be_defined)
    return BranchState(may_be_defined=may_be_defined, must_be_defined=must_be_defined)


</t>
<t tx="ekr.20221004064035.89">def visit_ellipsis(self, e: EllipsisExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.890">class DefinedVariableTracker:
    """DefinedVariableTracker manages the state and scope for the UndefinedVariablesVisitor."""

    @others
</t>
<t tx="ekr.20221004064035.891">def __init__(self) -&gt; None:
    # There's always at least one scope. Within each scope, there's at least one "global" BranchingStatement.
    self.scopes: list[list[BranchStatement]] = [[BranchStatement(BranchState())]]

</t>
<t tx="ekr.20221004064035.892">def _scope(self) -&gt; list[BranchStatement]:
    assert len(self.scopes) &gt; 0
    return self.scopes[-1]

</t>
<t tx="ekr.20221004064035.893">def enter_scope(self) -&gt; None:
    assert len(self._scope()) &gt; 0
    self.scopes.append([BranchStatement(self._scope()[-1].branches[-1])])

</t>
<t tx="ekr.20221004064035.894">def exit_scope(self) -&gt; None:
    self.scopes.pop()

</t>
<t tx="ekr.20221004064035.895">def start_branch_statement(self) -&gt; None:
    assert len(self._scope()) &gt; 0
    self._scope().append(BranchStatement(self._scope()[-1].branches[-1]))

</t>
<t tx="ekr.20221004064035.896">def next_branch(self) -&gt; None:
    assert len(self._scope()) &gt; 1
    self._scope()[-1].next_branch()

</t>
<t tx="ekr.20221004064035.897">def end_branch_statement(self) -&gt; None:
    assert len(self._scope()) &gt; 1
    result = self._scope().pop().done()
    self._scope()[-1].record_nested_branch(result)

</t>
<t tx="ekr.20221004064035.898">def skip_branch(self) -&gt; None:
    # Only skip branch if we're outside of "root" branch statement.
    if len(self._scope()) &gt; 1:
        self._scope()[-1].skip_branch()

</t>
<t tx="ekr.20221004064035.899">def record_declaration(self, name: str) -&gt; None:
    assert len(self.scopes) &gt; 0
    assert len(self.scopes[-1]) &gt; 0
    self._scope()[-1].record_definition(name)

</t>
<t tx="ekr.20221004064035.9">def __enter__(self) -&gt; IPCClient:
    return self

</t>
<t tx="ekr.20221004064035.90">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.900">def is_possibly_undefined(self, name: str) -&gt; bool:
    assert len(self._scope()) &gt; 0
    # A variable is undefined if it's in a set of `may_be_defined` but not in `must_be_defined`.
    # Cases where a variable is not defined altogether are handled by semantic analyzer.
    return self._scope()[-1].is_possibly_undefined(name)


</t>
<t tx="ekr.20221004064035.901">class PartiallyDefinedVariableVisitor(ExtendedTraverserVisitor):
    """Detect variables that are defined only part of the time.

    This visitor detects the following case:
    if foo():
        x = 1
    print(x)  # Error: "x" may be undefined.

    Note that this code does not detect variables not defined in any of the branches -- that is
    handled by the semantic analyzer.
    """

    @others
</t>
<t tx="ekr.20221004064035.902">def __init__(self, msg: MessageBuilder, type_map: dict[Expression, Type]) -&gt; None:
    self.msg = msg
    self.type_map = type_map
    self.tracker = DefinedVariableTracker()

</t>
<t tx="ekr.20221004064035.903">def process_lvalue(self, lvalue: Lvalue | None) -&gt; None:
    if isinstance(lvalue, NameExpr):
        self.tracker.record_declaration(lvalue.name)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.process_lvalue(item)

</t>
<t tx="ekr.20221004064035.904">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    for lvalue in o.lvalues:
        self.process_lvalue(lvalue)
    super().visit_assignment_stmt(o)

</t>
<t tx="ekr.20221004064035.905">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    o.value.accept(self)
    self.process_lvalue(o.target)

</t>
<t tx="ekr.20221004064035.906">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    for e in o.expr:
        e.accept(self)
    self.tracker.start_branch_statement()
    for b in o.body:
        b.accept(self)
        self.tracker.next_branch()
    if o.else_body:
        o.else_body.accept(self)
    self.tracker.end_branch_statement()

</t>
<t tx="ekr.20221004064035.907">def visit_func_def(self, o: FuncDef) -&gt; None:
    self.tracker.enter_scope()
    super().visit_func_def(o)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20221004064035.908">def visit_func(self, o: FuncItem) -&gt; None:
    if o.arguments is not None:
        for arg in o.arguments:
            self.tracker.record_declaration(arg.variable.name)
    super().visit_func(o)

</t>
<t tx="ekr.20221004064035.909">def visit_generator_expr(self, o: GeneratorExpr) -&gt; None:
    self.tracker.enter_scope()
    for idx in o.indices:
        self.process_lvalue(idx)
    super().visit_generator_expr(o)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20221004064035.91">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.910">def visit_dictionary_comprehension(self, o: DictionaryComprehension) -&gt; None:
    self.tracker.enter_scope()
    for idx in o.indices:
        self.process_lvalue(idx)
    super().visit_dictionary_comprehension(o)
    self.tracker.exit_scope()

</t>
<t tx="ekr.20221004064035.911">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    o.expr.accept(self)
    self.process_lvalue(o.index)
    o.index.accept(self)
    self.tracker.start_branch_statement()
    o.body.accept(self)
    self.tracker.next_branch()
    if o.else_body:
        o.else_body.accept(self)
    self.tracker.end_branch_statement()

</t>
<t tx="ekr.20221004064035.912">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    super().visit_return_stmt(o)
    self.tracker.skip_branch()

</t>
<t tx="ekr.20221004064035.913">def visit_assert_stmt(self, o: AssertStmt) -&gt; None:
    super().visit_assert_stmt(o)
    if checker.is_false_literal(o.expr):
        self.tracker.skip_branch()

</t>
<t tx="ekr.20221004064035.914">def visit_raise_stmt(self, o: RaiseStmt) -&gt; None:
    super().visit_raise_stmt(o)
    self.tracker.skip_branch()

</t>
<t tx="ekr.20221004064035.915">def visit_continue_stmt(self, o: ContinueStmt) -&gt; None:
    super().visit_continue_stmt(o)
    self.tracker.skip_branch()

</t>
<t tx="ekr.20221004064035.916">def visit_break_stmt(self, o: BreakStmt) -&gt; None:
    super().visit_break_stmt(o)
    self.tracker.skip_branch()

</t>
<t tx="ekr.20221004064035.917">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    if isinstance(self.type_map.get(o.expr, None), UninhabitedType):
        self.tracker.skip_branch()
    super().visit_expression_stmt(o)

</t>
<t tx="ekr.20221004064035.918">def visit_while_stmt(self, o: WhileStmt) -&gt; None:
    o.expr.accept(self)
    self.tracker.start_branch_statement()
    o.body.accept(self)
    if not checker.is_true_literal(o.expr):
        self.tracker.next_branch()
        if o.else_body:
            o.else_body.accept(self)
    self.tracker.end_branch_statement()

</t>
<t tx="ekr.20221004064035.919">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if self.tracker.is_possibly_undefined(o.name):
        self.msg.variable_may_be_undefined(o.name, o)
        # We don't want to report the error on the same variable multiple times.
        self.tracker.record_declaration(o.name)
    super().visit_name_expr(o)

</t>
<t tx="ekr.20221004064035.92">def visit_reveal_expr(self, e: RevealExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.920">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    for expr, idx in zip(o.expr, o.target):
        expr.accept(self)
        self.process_lvalue(idx)
    o.body.accept(self)
</t>
<t tx="ekr.20221004064035.921">@path C:/Repos/ekr-mypy2/mypy/
"""Classes for representing match statement patterns."""

from __future__ import annotations

from typing import TypeVar

from mypy_extensions import trait

from mypy.nodes import Expression, NameExpr, Node, RefExpr
from mypy.visitor import PatternVisitor

T = TypeVar("T")


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.922">@trait
class Pattern(Node):
    """A pattern node."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20221004064035.923">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    raise RuntimeError("Not implemented")


</t>
<t tx="ekr.20221004064035.924">class AsPattern(Pattern):
    """The pattern &lt;pattern&gt; as &lt;name&gt;"""

    # The python ast, and therefore also our ast merges capture, wildcard and as patterns into one
    # for easier handling.
    # If pattern is None this is a capture pattern. If name and pattern are both none this is a
    # wildcard pattern.
    # Only name being None should not happen but also won't break anything.
    pattern: Pattern | None
    name: NameExpr | None

    @others
</t>
<t tx="ekr.20221004064035.925">def __init__(self, pattern: Pattern | None, name: NameExpr | None) -&gt; None:
    super().__init__()
    self.pattern = pattern
    self.name = name

</t>
<t tx="ekr.20221004064035.926">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_as_pattern(self)


</t>
<t tx="ekr.20221004064035.927">class OrPattern(Pattern):
    """The pattern &lt;pattern&gt; | &lt;pattern&gt; | ..."""

    patterns: list[Pattern]

    @others
</t>
<t tx="ekr.20221004064035.928">def __init__(self, patterns: list[Pattern]) -&gt; None:
    super().__init__()
    self.patterns = patterns

</t>
<t tx="ekr.20221004064035.929">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_or_pattern(self)


</t>
<t tx="ekr.20221004064035.93">def visit_super_expr(self, e: SuperExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.930">class ValuePattern(Pattern):
    """The pattern x.y (or x.y.z, ...)"""

    expr: Expression

    @others
</t>
<t tx="ekr.20221004064035.931">def __init__(self, expr: Expression):
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20221004064035.932">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_value_pattern(self)


</t>
<t tx="ekr.20221004064035.933">class SingletonPattern(Pattern):
    # This can be exactly True, False or None
    value: bool | None

    @others
</t>
<t tx="ekr.20221004064035.934">def __init__(self, value: bool | None):
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20221004064035.935">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_singleton_pattern(self)


</t>
<t tx="ekr.20221004064035.936">class SequencePattern(Pattern):
    """The pattern [&lt;pattern&gt;, ...]"""

    patterns: list[Pattern]

    @others
</t>
<t tx="ekr.20221004064035.937">def __init__(self, patterns: list[Pattern]):
    super().__init__()
    self.patterns = patterns

</t>
<t tx="ekr.20221004064035.938">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_sequence_pattern(self)


</t>
<t tx="ekr.20221004064035.939">class StarredPattern(Pattern):
    # None corresponds to *_ in a list pattern. It will match multiple items but won't bind them to
    # a name.
    capture: NameExpr | None

    @others
</t>
<t tx="ekr.20221004064035.94">def visit_type_application(self, e: TypeApplication) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.940">def __init__(self, capture: NameExpr | None):
    super().__init__()
    self.capture = capture

</t>
<t tx="ekr.20221004064035.941">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_starred_pattern(self)


</t>
<t tx="ekr.20221004064035.942">class MappingPattern(Pattern):
    keys: list[Expression]
    values: list[Pattern]
    rest: NameExpr | None

    @others
</t>
<t tx="ekr.20221004064035.943">def __init__(self, keys: list[Expression], values: list[Pattern], rest: NameExpr | None):
    super().__init__()
    assert len(keys) == len(values)
    self.keys = keys
    self.values = values
    self.rest = rest

</t>
<t tx="ekr.20221004064035.944">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_mapping_pattern(self)


</t>
<t tx="ekr.20221004064035.945">class ClassPattern(Pattern):
    """The pattern Cls(...)"""

    class_ref: RefExpr
    positionals: list[Pattern]
    keyword_keys: list[str]
    keyword_values: list[Pattern]

    @others
</t>
<t tx="ekr.20221004064035.946">def __init__(
    self,
    class_ref: RefExpr,
    positionals: list[Pattern],
    keyword_keys: list[str],
    keyword_values: list[Pattern],
):
    super().__init__()
    assert len(keyword_keys) == len(keyword_values)
    self.class_ref = class_ref
    self.positionals = positionals
    self.keyword_keys = keyword_keys
    self.keyword_values = keyword_values

</t>
<t tx="ekr.20221004064035.947">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_class_pattern(self)
</t>
<t tx="ekr.20221004064035.948">@path C:/Repos/ekr-mypy2/mypy/
"""Plugin system for extending mypy.

At large scale the plugin system works as following:

* Plugins are collected from the corresponding mypy config file option
  (either via paths to Python files, or installed Python modules)
  and imported using importlib.

* Every module should get an entry point function (called 'plugin' by default,
  but may be overridden in the config file) that should accept a single string
  argument that is a full mypy version (includes git commit hash for dev
  versions) and return a subclass of mypy.plugins.Plugin.

* All plugin class constructors should match the signature of mypy.plugin.Plugin
  (i.e. should accept an mypy.options.Options object), and *must* call
  super().__init__().

* At several steps during semantic analysis and type checking mypy calls
  special `get_xxx` methods on user plugins with a single string argument that
  is a fully qualified name (full name) of a relevant definition
  (see mypy.plugin.Plugin method docstrings for details).

* The plugins are called in the order they are passed in the config option.
  Every plugin must decide whether to act on a given full name. The first
  plugin that returns non-None object will be used.

* The above decision should be made using the limited common API specified by
  mypy.plugin.CommonPluginApi.

* The callback returned by the plugin will be called with a larger context that
  includes relevant current state (e.g. a default return type, or a default
  attribute type) and a wider relevant API provider (e.g.
  SemanticAnalyzerPluginInterface or CheckerPluginInterface).

* The result of this is used for further processing. See various `XxxContext`
  named tuples for details about which information is given to each hook.

Plugin developers should ensure that their plugins work well in incremental and
daemon modes. In particular, plugins should not hold global state, and should
always call add_plugin_dependency() in plugin hooks called during semantic
analysis. See the method docstring for more details.

There is no dedicated cache storage for plugins, but plugins can store
per-TypeInfo data in a special .metadata attribute that is serialized to the
mypy caches between incremental runs. To avoid collisions between plugins, they
are encouraged to store their state under a dedicated key coinciding with
plugin name in the metadata dictionary. Every value stored there must be
JSON-serializable.

## Notes about the semantic analyzer

Mypy 0.710 introduced a new semantic analyzer that changed how plugins are
expected to work in several notable ways (from mypy 0.730 the old semantic
analyzer is no longer available):

1. The order of processing AST nodes in modules is different. The old semantic
   analyzer processed modules in textual order, one module at a time. The new
   semantic analyzer first processes the module top levels, including bodies of
   any top-level classes and classes nested within classes. ("Top-level" here
   means "not nested within a function/method".) Functions and methods are
   processed only after module top levels have been finished. If there is an
   import cycle, all module top levels in the cycle are processed before
   processing any functions or methods. Each unit of processing (a module top
   level or a function/method) is called a *target*.

   This also means that function signatures in the same module have not been
   analyzed yet when analyzing the module top level. If you need access to
   a function signature, you'll need to explicitly analyze the signature first
   using `anal_type()`.

2. Each target can be processed multiple times. This may happen if some forward
   references are not ready yet, for example. This means that semantic analyzer
   related plugin hooks can be called multiple times for the same full name.
   These plugin methods must thus be idempotent.

3. The `anal_type` API function returns None if some part of the type is not
   available yet. If this happens, the current target being analyzed will be
   *deferred*, which means that it will be processed again soon, in the hope
   that additional dependencies will be available. This may happen if there are
   forward references to types or inter-module references to types within an
   import cycle.

   Note that if there is a circular definition, mypy may decide to stop
   processing to avoid an infinite number of iterations. When this happens,
   `anal_type` will generate an error and return an `AnyType` type object
   during the final iteration (instead of None).

4. There is a new API method `defer()`. This can be used to explicitly request
   the current target to be reprocessed one more time. You don't need this
   to call this if `anal_type` returns None, however.

5. There is a new API property `final_iteration`, which is true once mypy
   detected no progress during the previous iteration or if the maximum
   semantic analysis iteration count has been reached. You must never
   defer during the final iteration, as it will cause a crash.

6. The `node` attribute of SymbolTableNode objects may contain a reference to
   a PlaceholderNode object. This object means that this definition has not
   been fully processed yet. If you encounter a PlaceholderNode, you should
   defer unless it's the final iteration. If it's the final iteration, you
   should generate an error message. It usually means that there's a cyclic
   definition that cannot be resolved by mypy. PlaceholderNodes can only refer
   to references inside an import cycle. If you are looking up things from
   another module, such as the builtins, that is outside the current module or
   import cycle, you can safely assume that you won't receive a placeholder.

When testing your plugin, you should have a test case that forces a module top
level to be processed multiple times. The easiest way to do this is to include
a forward reference to a class in a top-level annotation. Example:

    c: C  # Forward reference causes second analysis pass
    class C: pass

Note that a forward reference in a function signature won't trigger another
pass, since all functions are processed only after the top level has been fully
analyzed.

You can use `api.options.new_semantic_analyzer` to check whether the new
semantic analyzer is enabled (it's always true in mypy 0.730 and later).
"""

from __future__ import annotations

from abc import abstractmethod
from typing import Any, Callable, NamedTuple, TypeVar

from mypy_extensions import mypyc_attr, trait

from mypy.errorcodes import ErrorCode
from mypy.lookup import lookup_fully_qualified
from mypy.message_registry import ErrorMessage
from mypy.messages import MessageBuilder
from mypy.nodes import (
    ArgKind,
    CallExpr,
    ClassDef,
    Context,
    Expression,
    MypyFile,
    SymbolTableNode,
    TypeInfo,
)
from mypy.options import Options
from mypy.tvar_scope import TypeVarLikeScope
from mypy.types import (
    CallableType,
    FunctionLike,
    Instance,
    ProperType,
    Type,
    TypeList,
    UnboundType,
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064035.949">@trait
class TypeAnalyzerPluginInterface:
    """Interface for accessing semantic analyzer functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in typeanal.py for more details.
    """

    # An options object. Note: these are the cloned options for the current file.
    # This might be different from Plugin.options (that contains default/global options)
    # if there are per-file options in the config. This applies to all other interfaces
    # in this file.
    options: Options

    @others
</t>
<t tx="ekr.20221004064035.95">def visit_lambda_expr(self, e: LambdaExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.950">@abstractmethod
def fail(self, msg: str, ctx: Context, *, code: ErrorCode | None = None) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.951">@abstractmethod
def named_type(self, name: str, args: list[Type]) -&gt; Instance:
    """Construct an instance of a builtin type with given name."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.952">@abstractmethod
def analyze_type(self, typ: Type) -&gt; Type:
    """Analyze an unbound type using the default mypy logic."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.953">@abstractmethod
def analyze_callable_args(
    self, arglist: TypeList
) -&gt; tuple[list[Type], list[ArgKind], list[str | None]] | None:
    """Find types, kinds, and names of arguments from extended callable syntax."""
    raise NotImplementedError


</t>
<t tx="ekr.20221004064035.954"># A context for a hook that semantically analyzes an unbound type.
class AnalyzeTypeContext(NamedTuple):
    type: UnboundType  # Type to analyze
    context: Context  # Relevant location context (e.g. for error messages)
    api: TypeAnalyzerPluginInterface


</t>
<t tx="ekr.20221004064035.955">@mypyc_attr(allow_interpreted_subclasses=True)
class CommonPluginApi:
    """
    A common plugin API (shared between semantic analysis and type checking phases)
    that all plugin hooks get independently of the context.
    """

    # Global mypy options.
    # Per-file options can be only accessed on various
    # XxxPluginInterface classes.
    options: Options

    @others
</t>
<t tx="ekr.20221004064035.956">@abstractmethod
def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode | None:
    """Lookup a symbol by its full name (including module).

    This lookup function available for all plugins. Return None if a name
    is not found. This function doesn't support lookup from current scope.
    Use SemanticAnalyzerPluginInterface.lookup_qualified() for this."""
    raise NotImplementedError


</t>
<t tx="ekr.20221004064035.957">@trait
class CheckerPluginInterface:
    """Interface for accessing type checker functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in checker.py for more details.
    """

    msg: MessageBuilder
    options: Options
    path: str

    @others
</t>
<t tx="ekr.20221004064035.958"># Type context for type inference
@property
@abstractmethod
def type_context(self) -&gt; list[Type | None]:
    """Return the type context of the plugin"""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.959">@abstractmethod
def fail(
    self, msg: str | ErrorMessage, ctx: Context, *, code: ErrorCode | None = None
) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.96">def visit_list_comprehension(self, e: ListComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.960">@abstractmethod
def named_generic_type(self, name: str, args: list[Type]) -&gt; Instance:
    """Construct an instance of a builtin type with given type arguments."""
    raise NotImplementedError


</t>
<t tx="ekr.20221004064035.961">@trait
class SemanticAnalyzerPluginInterface:
    """Interface for accessing semantic analyzer functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in semanal.py for more details.

    # TODO: clean-up lookup functions.
    """

    modules: dict[str, MypyFile]
    # Options for current file.
    options: Options
    cur_mod_id: str
    msg: MessageBuilder

    @others
</t>
<t tx="ekr.20221004064035.962">@abstractmethod
def named_type(self, fullname: str, args: list[Type] | None = None) -&gt; Instance:
    """Construct an instance of a builtin type with given type arguments."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.963">@abstractmethod
def builtin_type(self, fully_qualified_name: str) -&gt; Instance:
    """Legacy function -- use named_type() instead."""
    # NOTE: Do not delete this since many plugins may still use it.
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.964">@abstractmethod
def named_type_or_none(self, fullname: str, args: list[Type] | None = None) -&gt; Instance | None:
    """Construct an instance of a type with given type arguments.

    Return None if a type could not be constructed for the qualified
    type name. This is possible when the qualified name includes a
    module name and the module has not been imported.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.965">@abstractmethod
def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.966">@abstractmethod
def parse_bool(self, expr: Expression) -&gt; bool | None:
    """Parse True/False literals."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.967">@abstractmethod
def fail(
    self,
    msg: str,
    ctx: Context,
    serious: bool = False,
    *,
    blocker: bool = False,
    code: ErrorCode | None = None,
) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.968">@abstractmethod
def anal_type(
    self,
    t: Type,
    *,
    tvar_scope: TypeVarLikeScope | None = None,
    allow_tuple_literal: bool = False,
    allow_unbound_tvars: bool = False,
    report_invalid_types: bool = True,
    third_pass: bool = False,
) -&gt; Type | None:
    """Analyze an unbound type.

    Return None if some part of the type is not ready yet. In this
    case the current target being analyzed will be deferred and
    analyzed again.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.969">@abstractmethod
def class_type(self, self_type: Type) -&gt; Type:
    """Generate type of first argument of class methods from type of self."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.97">def visit_set_comprehension(self, e: SetComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.970">@abstractmethod
def lookup_fully_qualified(self, name: str) -&gt; SymbolTableNode:
    """Lookup a symbol by its fully qualified name.

    Raise an error if not found.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.971">@abstractmethod
def lookup_fully_qualified_or_none(self, name: str) -&gt; SymbolTableNode | None:
    """Lookup a symbol by its fully qualified name.

    Return None if not found.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.972">@abstractmethod
def lookup_qualified(
    self, name: str, ctx: Context, suppress_errors: bool = False
) -&gt; SymbolTableNode | None:
    """Lookup symbol using a name in current scope.

    This follows Python local-&gt;non-local-&gt;global-&gt;builtins rules.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.973">@abstractmethod
def add_plugin_dependency(self, trigger: str, target: str | None = None) -&gt; None:
    """Specify semantic dependencies for generated methods/variables.

    If the symbol with full name given by trigger is found to be stale by mypy,
    then the body of node with full name given by target will be re-checked.
    By default, this is the node that is currently analyzed.

    For example, the dataclass plugin adds a generated __init__ method with
    a signature that depends on types of attributes in ancestor classes. If any
    attribute in an ancestor class gets stale (modified), we need to reprocess
    the subclasses (and thus regenerate __init__ methods).

    This is used by fine-grained incremental mode (mypy daemon). See mypy/server/deps.py
    for more details.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.974">@abstractmethod
def add_symbol_table_node(self, name: str, stnode: SymbolTableNode) -&gt; Any:
    """Add node to global symbol table (or to nearest class if there is one)."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.975">@abstractmethod
def qualified_name(self, n: str) -&gt; str:
    """Make qualified name using current module and enclosing class (if any)."""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.976">@abstractmethod
def defer(self) -&gt; None:
    """Call this to defer the processing of the current node.

    This will request an additional iteration of semantic analysis.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.977">@property
@abstractmethod
def final_iteration(self) -&gt; bool:
    """Is this the final iteration of semantic analysis?"""
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.978">@property
@abstractmethod
def is_stub_file(self) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20221004064035.979">@abstractmethod
def analyze_simple_literal_type(self, rvalue: Expression, is_final: bool) -&gt; Type | None:
    raise NotImplementedError


</t>
<t tx="ekr.20221004064035.98">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.980"># A context for querying for configuration data about a module for
# cache invalidation purposes.
class ReportConfigContext(NamedTuple):
    id: str  # Module name
    path: str  # Module file path
    is_check: bool  # Is this invocation for checking whether the config matches


</t>
<t tx="ekr.20221004064035.981"># A context for a function signature hook that infers a better signature for a
# function.  Note that argument types aren't available yet.  If you need them,
# you have to use a method hook instead.
class FunctionSigContext(NamedTuple):
    args: list[list[Expression]]  # Actual expressions for each formal argument
    default_signature: CallableType  # Original signature of the method
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20221004064035.982"># A context for a function hook that infers the return type of a function with
# a special signature.
#
# A no-op callback would just return the inferred return type, but a useful
# callback at least sometimes can infer a more precise type.
class FunctionContext(NamedTuple):
    arg_types: list[list[Type]]  # List of actual caller types for each formal argument
    arg_kinds: list[list[ArgKind]]  # Ditto for argument kinds, see nodes.ARG_* constants
    # Names of formal parameters from the callee definition,
    # these will be sufficient in most cases.
    callee_arg_names: list[str | None]
    # Names of actual arguments in the call expression. For example,
    # in a situation like this:
    #     def func(**kwargs) -&gt; None:
    #         pass
    #     func(kw1=1, kw2=2)
    # callee_arg_names will be ['kwargs'] and arg_names will be [['kw1', 'kw2']].
    arg_names: list[list[str | None]]
    default_return_type: Type  # Return type inferred from signature
    args: list[list[Expression]]  # Actual expressions for each formal argument
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20221004064035.983"># A context for a method signature hook that infers a better signature for a
# method.  Note that argument types aren't available yet.  If you need them,
# you have to use a method hook instead.
# TODO: document ProperType in the plugin changelog/update issue.
class MethodSigContext(NamedTuple):
    type: ProperType  # Base object type for method call
    args: list[list[Expression]]  # Actual expressions for each formal argument
    default_signature: CallableType  # Original signature of the method
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20221004064035.984"># A context for a method hook that infers the return type of a method with a
# special signature.
#
# This is very similar to FunctionContext (only differences are documented).
class MethodContext(NamedTuple):
    type: ProperType  # Base object type for method call
    arg_types: list[list[Type]]  # List of actual caller types for each formal argument
    # see FunctionContext for details about names and kinds
    arg_kinds: list[list[ArgKind]]
    callee_arg_names: list[str | None]
    arg_names: list[list[str | None]]
    default_return_type: Type  # Return type inferred by mypy
    args: list[list[Expression]]  # Lists of actual expressions for every formal argument
    context: Context
    api: CheckerPluginInterface


</t>
<t tx="ekr.20221004064035.985"># A context for an attribute type hook that infers the type of an attribute.
class AttributeContext(NamedTuple):
    type: ProperType  # Type of object with attribute
    default_attr_type: Type  # Original attribute type
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20221004064035.986"># A context for a class hook that modifies the class definition.
class ClassDefContext(NamedTuple):
    cls: ClassDef  # The class definition
    reason: Expression  # The expression being applied (decorator, metaclass, base class)
    api: SemanticAnalyzerPluginInterface


</t>
<t tx="ekr.20221004064035.987"># A context for dynamic class definitions like
# Base = declarative_base()
class DynamicClassDefContext(NamedTuple):
    call: CallExpr  # The r.h.s. of dynamic class definition
    name: str  # The name this class is being assigned to
    api: SemanticAnalyzerPluginInterface


</t>
<t tx="ekr.20221004064035.988">@mypyc_attr(allow_interpreted_subclasses=True)
class Plugin(CommonPluginApi):
    """Base class of all type checker plugins.

    This defines a no-op plugin.  Subclasses can override some methods to
    provide some actual functionality.

    All get_ methods are treated as pure functions (you should assume that
    results might be cached). A plugin should return None from a get_ method
    to give way to other plugins.

    Look at the comments of various *Context objects for additional information on
    various hooks.
    """

    @others
</t>
<t tx="ekr.20221004064035.989">def __init__(self, options: Options) -&gt; None:
    self.options = options
    self.python_version = options.python_version
    # This can't be set in __init__ because it is executed too soon in build.py.
    # Therefore, build.py *must* set it later before graph processing starts
    # by calling set_modules().
    self._modules: dict[str, MypyFile] | None = None

</t>
<t tx="ekr.20221004064035.99">def visit_generator_expr(self, e: GeneratorExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20221004064035.990">def set_modules(self, modules: dict[str, MypyFile]) -&gt; None:
    self._modules = modules

</t>
<t tx="ekr.20221004064035.991">def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode | None:
    assert self._modules is not None
    return lookup_fully_qualified(fullname, self._modules)

</t>
<t tx="ekr.20221004064035.992">def report_config_data(self, ctx: ReportConfigContext) -&gt; Any:
    """Get representation of configuration data for a module.

    The data must be encodable as JSON and will be stored in the
    cache metadata for the module. A mismatch between the cached
    values and the returned will result in that module's cache
    being invalidated and the module being rechecked.

    This can be called twice for each module, once after loading
    the cache to check if it is valid and once while writing new
    cache information.

    If is_check in the context is true, then the return of this
    call will be checked against the cached version. Otherwise the
    call is being made to determine what to put in the cache. This
    can be used to allow consulting extra cache files in certain
    complex situations.

    This can be used to incorporate external configuration information
    that might require changes to typechecking.
    """
    return None

</t>
<t tx="ekr.20221004064035.993">def get_additional_deps(self, file: MypyFile) -&gt; list[tuple[int, str, int]]:
    """Customize dependencies for a module.

    This hook allows adding in new dependencies for a module. It
    is called after parsing a file but before analysis. This can
    be useful if a library has dependencies that are dynamic based
    on configuration information, for example.

    Returns a list of (priority, module name, line number) tuples.

    The line number can be -1 when there is not a known real line number.

    Priorities are defined in mypy.build (but maybe shouldn't be).
    10 is a good choice for priority.
    """
    return []

</t>
<t tx="ekr.20221004064035.994">def get_type_analyze_hook(self, fullname: str) -&gt; Callable[[AnalyzeTypeContext], Type] | None:
    """Customize behaviour of the type analyzer for given full names.

    This method is called during the semantic analysis pass whenever mypy sees an
    unbound type. For example, while analysing this code:

        from lib import Special, Other

        var: Special
        def func(x: Other[int]) -&gt; None:
            ...

    this method will be called with 'lib.Special', and then with 'lib.Other'.
    The callback returned by plugin must return an analyzed type,
    i.e. an instance of `mypy.types.Type`.
    """
    return None

</t>
<t tx="ekr.20221004064035.995">def get_function_signature_hook(
    self, fullname: str
) -&gt; Callable[[FunctionSigContext], FunctionLike] | None:
    """Adjust the signature of a function.

    This method is called before type checking a function call. Plugin
    may infer a better type for the function.

        from lib import Class, do_stuff

        do_stuff(42)
        Class()

    This method will be called with 'lib.do_stuff' and then with 'lib.Class'.
    """
    return None

</t>
<t tx="ekr.20221004064035.996">def get_function_hook(self, fullname: str) -&gt; Callable[[FunctionContext], Type] | None:
    """Adjust the return type of a function call.

    This method is called after type checking a call. Plugin may adjust the return
    type inferred by mypy, and/or emit some error messages. Note, this hook is also
    called for class instantiation calls, so that in this example:

        from lib import Class, do_stuff

        do_stuff(42)
        Class()

    This method will be called with 'lib.do_stuff' and then with 'lib.Class'.
    """
    return None

</t>
<t tx="ekr.20221004064035.997">def get_method_signature_hook(
    self, fullname: str
) -&gt; Callable[[MethodSigContext], FunctionLike] | None:
    """Adjust the signature of a method.

    This method is called before type checking a method call. Plugin
    may infer a better type for the method. The hook is also called for special
    Python dunder methods except __init__ and __new__ (use get_function_hook to customize
    class instantiation). This function is called with the method full name using
    the class where it was _defined_. For example, in this code:

        from lib import Special

        class Base:
            def method(self, arg: Any) -&gt; Any:
                ...
        class Derived(Base):
            ...

        var: Derived
        var.method(42)

        x: Special
        y = x[0]

    this method is called with '__main__.Base.method', and then with
    'lib.Special.__getitem__'.
    """
    return None

</t>
<t tx="ekr.20221004064035.998">def get_method_hook(self, fullname: str) -&gt; Callable[[MethodContext], Type] | None:
    """Adjust return type of a method call.

    This is the same as get_function_hook(), but is called with the
    method full name (again, using the class where the method is defined).
    """
    return None

</t>
<t tx="ekr.20221004064035.999">def get_attribute_hook(self, fullname: str) -&gt; Callable[[AttributeContext], Type] | None:
    """Adjust type of an instance attribute.

    This method is called with attribute full name using the class of the instance where
    the attribute was defined (or Var.info.fullname for generated attributes).

    For classes without __getattr__ or __getattribute__, this hook is only called for
    names of fields/properties (but not methods) that exist in the instance MRO.

    For classes that implement __getattr__ or __getattribute__, this hook is called
    for all fields/properties, including nonexistent ones (but still not methods).

    For example:

        class Base:
            x: Any
            def __getattr__(self, attr: str) -&gt; Any: ...

        class Derived(Base):
            ...

        var: Derived
        var.x
        var.y

    get_attribute_hook is called with '__main__.Base.x' and '__main__.Base.y'.
    However, if we had not implemented __getattr__ on Base, you would only get
    the callback for 'var.x'; 'var.y' would produce an error without calling the hook.
    """
    return None

</t>
<t tx="ekr.20221004064036.1">def visit_star_type(self, t: StarType) -&gt; T:
    return t.type.accept(self)

</t>
<t tx="ekr.20221004064036.10">def is_dunder(name: str, exclude_special: bool = False) -&gt; bool:
    """Returns whether name is a dunder name.

    Args:
        exclude_special: Whether to return False for a couple special dunder methods.

    """
    if exclude_special and name in SPECIAL_DUNDERS:
        return False
    return name.startswith("__") and name.endswith("__")


</t>
<t tx="ekr.20221004064036.100">@abstractmethod
def visit_paramspec_expr(self, o: mypy.nodes.ParamSpecExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.101">@abstractmethod
def visit_type_var_tuple_expr(self, o: mypy.nodes.TypeVarTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.102">@abstractmethod
def visit_type_alias_expr(self, o: mypy.nodes.TypeAliasExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.103">@abstractmethod
def visit_namedtuple_expr(self, o: mypy.nodes.NamedTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.104">@abstractmethod
def visit_enum_call_expr(self, o: mypy.nodes.EnumCallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.105">@abstractmethod
def visit_typeddict_expr(self, o: mypy.nodes.TypedDictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.106">@abstractmethod
def visit_newtype_expr(self, o: mypy.nodes.NewTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.107">@abstractmethod
def visit__promote_expr(self, o: mypy.nodes.PromoteExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.108">@abstractmethod
def visit_await_expr(self, o: mypy.nodes.AwaitExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.109">@abstractmethod
def visit_temp_node(self, o: mypy.nodes.TempNode) -&gt; T:
    pass


</t>
<t tx="ekr.20221004064036.11">def is_sunder(name: str) -&gt; bool:
    return not is_dunder(name) and name.startswith("_") and name.endswith("_")


</t>
<t tx="ekr.20221004064036.110">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class StatementVisitor(Generic[T]):
    # Definitions

    @others
</t>
<t tx="ekr.20221004064036.111">@abstractmethod
def visit_assignment_stmt(self, o: mypy.nodes.AssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.112">@abstractmethod
def visit_for_stmt(self, o: mypy.nodes.ForStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.113">@abstractmethod
def visit_with_stmt(self, o: mypy.nodes.WithStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.114">@abstractmethod
def visit_del_stmt(self, o: mypy.nodes.DelStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.115">@abstractmethod
def visit_func_def(self, o: mypy.nodes.FuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.116">@abstractmethod
def visit_overloaded_func_def(self, o: mypy.nodes.OverloadedFuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.117">@abstractmethod
def visit_class_def(self, o: mypy.nodes.ClassDef) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.118">@abstractmethod
def visit_global_decl(self, o: mypy.nodes.GlobalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.119">@abstractmethod
def visit_nonlocal_decl(self, o: mypy.nodes.NonlocalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.12">def split_module_names(mod_name: str) -&gt; list[str]:
    """Return the module and all parent module names.

    So, if `mod_name` is 'a.b.c', this function will return
    ['a.b.c', 'a.b', and 'a'].
    """
    out = [mod_name]
    while "." in mod_name:
        mod_name = mod_name.rsplit(".", 1)[0]
        out.append(mod_name)
    return out


</t>
<t tx="ekr.20221004064036.120">@abstractmethod
def visit_decorator(self, o: mypy.nodes.Decorator) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.121"># Module structure

</t>
<t tx="ekr.20221004064036.122">@abstractmethod
def visit_import(self, o: mypy.nodes.Import) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.123">@abstractmethod
def visit_import_from(self, o: mypy.nodes.ImportFrom) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.124">@abstractmethod
def visit_import_all(self, o: mypy.nodes.ImportAll) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.125"># Statements

</t>
<t tx="ekr.20221004064036.126">@abstractmethod
def visit_block(self, o: mypy.nodes.Block) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.127">@abstractmethod
def visit_expression_stmt(self, o: mypy.nodes.ExpressionStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.128">@abstractmethod
def visit_operator_assignment_stmt(self, o: mypy.nodes.OperatorAssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.129">@abstractmethod
def visit_while_stmt(self, o: mypy.nodes.WhileStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.13">def module_prefix(modules: Iterable[str], target: str) -&gt; str | None:
    result = split_target(modules, target)
    if result is None:
        return None
    return result[0]


</t>
<t tx="ekr.20221004064036.130">@abstractmethod
def visit_return_stmt(self, o: mypy.nodes.ReturnStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.131">@abstractmethod
def visit_assert_stmt(self, o: mypy.nodes.AssertStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.132">@abstractmethod
def visit_if_stmt(self, o: mypy.nodes.IfStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.133">@abstractmethod
def visit_break_stmt(self, o: mypy.nodes.BreakStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.134">@abstractmethod
def visit_continue_stmt(self, o: mypy.nodes.ContinueStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.135">@abstractmethod
def visit_pass_stmt(self, o: mypy.nodes.PassStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.136">@abstractmethod
def visit_raise_stmt(self, o: mypy.nodes.RaiseStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.137">@abstractmethod
def visit_try_stmt(self, o: mypy.nodes.TryStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.138">@abstractmethod
def visit_match_stmt(self, o: mypy.nodes.MatchStmt) -&gt; T:
    pass


</t>
<t tx="ekr.20221004064036.139">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class PatternVisitor(Generic[T]):
    @others
</t>
<t tx="ekr.20221004064036.14">def split_target(modules: Iterable[str], target: str) -&gt; tuple[str, str] | None:
    remaining: list[str] = []
    while True:
        if target in modules:
            return target, ".".join(remaining)
        components = target.rsplit(".", 1)
        if len(components) == 1:
            return None
        target = components[0]
        remaining.insert(0, components[1])


</t>
<t tx="ekr.20221004064036.140">@abstractmethod
def visit_as_pattern(self, o: mypy.patterns.AsPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.141">@abstractmethod
def visit_or_pattern(self, o: mypy.patterns.OrPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.142">@abstractmethod
def visit_value_pattern(self, o: mypy.patterns.ValuePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.143">@abstractmethod
def visit_singleton_pattern(self, o: mypy.patterns.SingletonPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.144">@abstractmethod
def visit_sequence_pattern(self, o: mypy.patterns.SequencePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.145">@abstractmethod
def visit_starred_pattern(self, o: mypy.patterns.StarredPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.146">@abstractmethod
def visit_mapping_pattern(self, o: mypy.patterns.MappingPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.147">@abstractmethod
def visit_class_pattern(self, o: mypy.patterns.ClassPattern) -&gt; T:
    pass


</t>
<t tx="ekr.20221004064036.148">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class NodeVisitor(Generic[T], ExpressionVisitor[T], StatementVisitor[T], PatternVisitor[T]):
    """Empty base class for parse tree node visitors.

    The T type argument specifies the return type of the visit
    methods. As all methods defined here return None by default,
    subclasses do not always need to override all the methods.

    TODO: make the default return value explicit, then turn on
          empty body checking in mypy_self_check.ini.
    """

    # Not in superclasses:

    @others
</t>
<t tx="ekr.20221004064036.149">def visit_mypy_file(self, o: mypy.nodes.MypyFile) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.15">def short_type(obj: object) -&gt; str:
    """Return the last component of the type name of an object.

    If obj is None, return 'nil'. For example, if obj is 1, return 'int'.
    """
    if obj is None:
        return "nil"
    t = str(type(obj))
    return t.split(".")[-1].rstrip("'&gt;")


</t>
<t tx="ekr.20221004064036.150"># TODO: We have a visit_var method, but no visit_typeinfo or any
# other non-Statement SymbolNode (accepting those will raise a
# runtime error). Maybe this should be resolved in some direction.
def visit_var(self, o: mypy.nodes.Var) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.151"># Module structure

</t>
<t tx="ekr.20221004064036.152">def visit_import(self, o: mypy.nodes.Import) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.153">def visit_import_from(self, o: mypy.nodes.ImportFrom) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.154">def visit_import_all(self, o: mypy.nodes.ImportAll) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.155"># Definitions

</t>
<t tx="ekr.20221004064036.156">def visit_func_def(self, o: mypy.nodes.FuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.157">def visit_overloaded_func_def(self, o: mypy.nodes.OverloadedFuncDef) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.158">def visit_class_def(self, o: mypy.nodes.ClassDef) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.159">def visit_global_decl(self, o: mypy.nodes.GlobalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.16">def find_python_encoding(text: bytes) -&gt; tuple[str, int]:
    """PEP-263 for detecting Python file encoding"""
    result = ENCODING_RE.match(text)
    if result:
        line = 2 if result.group(1) else 1
        encoding = result.group(3).decode("ascii")
        # Handle some aliases that Python is happy to accept and that are used in the wild.
        if encoding.startswith(("iso-latin-1-", "latin-1-")) or encoding == "iso-latin-1":
            encoding = "latin-1"
        return encoding, line
    else:
        default_encoding = "utf8"
        return default_encoding, -1


</t>
<t tx="ekr.20221004064036.160">def visit_nonlocal_decl(self, o: mypy.nodes.NonlocalDecl) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.161">def visit_decorator(self, o: mypy.nodes.Decorator) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.162">def visit_type_alias(self, o: mypy.nodes.TypeAlias) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.163">def visit_placeholder_node(self, o: mypy.nodes.PlaceholderNode) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.164"># Statements

</t>
<t tx="ekr.20221004064036.165">def visit_block(self, o: mypy.nodes.Block) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.166">def visit_expression_stmt(self, o: mypy.nodes.ExpressionStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.167">def visit_assignment_stmt(self, o: mypy.nodes.AssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.168">def visit_operator_assignment_stmt(self, o: mypy.nodes.OperatorAssignmentStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.169">def visit_while_stmt(self, o: mypy.nodes.WhileStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.17">def bytes_to_human_readable_repr(b: bytes) -&gt; str:
    """Converts bytes into some human-readable representation. Unprintable
    bytes such as the nul byte are escaped. For example:

        &gt;&gt;&gt; b = bytes([102, 111, 111, 10, 0])
        &gt;&gt;&gt; s = bytes_to_human_readable_repr(b)
        &gt;&gt;&gt; print(s)
        foo\n\x00
        &gt;&gt;&gt; print(repr(s))
        'foo\\n\\x00'
    """
    return repr(b)[2:-1]


</t>
<t tx="ekr.20221004064036.170">def visit_for_stmt(self, o: mypy.nodes.ForStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.171">def visit_return_stmt(self, o: mypy.nodes.ReturnStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.172">def visit_assert_stmt(self, o: mypy.nodes.AssertStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.173">def visit_del_stmt(self, o: mypy.nodes.DelStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.174">def visit_if_stmt(self, o: mypy.nodes.IfStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.175">def visit_break_stmt(self, o: mypy.nodes.BreakStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.176">def visit_continue_stmt(self, o: mypy.nodes.ContinueStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.177">def visit_pass_stmt(self, o: mypy.nodes.PassStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.178">def visit_raise_stmt(self, o: mypy.nodes.RaiseStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.179">def visit_try_stmt(self, o: mypy.nodes.TryStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.18">class DecodeError(Exception):
    """Exception raised when a file cannot be decoded due to an unknown encoding type.

    Essentially a wrapper for the LookupError raised by `bytearray.decode`
    """


</t>
<t tx="ekr.20221004064036.180">def visit_with_stmt(self, o: mypy.nodes.WithStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.181">def visit_match_stmt(self, o: mypy.nodes.MatchStmt) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.182"># Expressions (default no-op implementation)

</t>
<t tx="ekr.20221004064036.183">def visit_int_expr(self, o: mypy.nodes.IntExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.184">def visit_str_expr(self, o: mypy.nodes.StrExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.185">def visit_bytes_expr(self, o: mypy.nodes.BytesExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.186">def visit_float_expr(self, o: mypy.nodes.FloatExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.187">def visit_complex_expr(self, o: mypy.nodes.ComplexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.188">def visit_ellipsis(self, o: mypy.nodes.EllipsisExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.189">def visit_star_expr(self, o: mypy.nodes.StarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.19">def decode_python_encoding(source: bytes) -&gt; str:
    """Read the Python file with while obeying PEP-263 encoding detection.

    Returns the source as a string.
    """
    # check for BOM UTF-8 encoding and strip it out if present
    if source.startswith(b"\xef\xbb\xbf"):
        encoding = "utf8"
        source = source[3:]
    else:
        # look at first two lines and check if PEP-263 coding is present
        encoding, _ = find_python_encoding(source)

    try:
        source_text = source.decode(encoding)
    except LookupError as lookuperr:
        raise DecodeError(str(lookuperr)) from lookuperr
    return source_text


</t>
<t tx="ekr.20221004064036.190">def visit_name_expr(self, o: mypy.nodes.NameExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.191">def visit_member_expr(self, o: mypy.nodes.MemberExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.192">def visit_yield_from_expr(self, o: mypy.nodes.YieldFromExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.193">def visit_yield_expr(self, o: mypy.nodes.YieldExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.194">def visit_call_expr(self, o: mypy.nodes.CallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.195">def visit_op_expr(self, o: mypy.nodes.OpExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.196">def visit_comparison_expr(self, o: mypy.nodes.ComparisonExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.197">def visit_cast_expr(self, o: mypy.nodes.CastExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.198">def visit_assert_type_expr(self, o: mypy.nodes.AssertTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.199">def visit_reveal_expr(self, o: mypy.nodes.RevealExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.2">def visit_union_type(self, t: UnionType) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20221004064036.20">def read_py_file(path: str, read: Callable[[str], bytes]) -&gt; list[str] | None:
    """Try reading a Python file as list of source lines.

    Return None if something goes wrong.
    """
    try:
        source = read(path)
    except OSError:
        return None
    else:
        try:
            source_lines = decode_python_encoding(source).splitlines()
        except DecodeError:
            return None
        return source_lines


</t>
<t tx="ekr.20221004064036.200">def visit_super_expr(self, o: mypy.nodes.SuperExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.201">def visit_assignment_expr(self, o: mypy.nodes.AssignmentExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.202">def visit_unary_expr(self, o: mypy.nodes.UnaryExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.203">def visit_list_expr(self, o: mypy.nodes.ListExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.204">def visit_dict_expr(self, o: mypy.nodes.DictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.205">def visit_tuple_expr(self, o: mypy.nodes.TupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.206">def visit_set_expr(self, o: mypy.nodes.SetExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.207">def visit_index_expr(self, o: mypy.nodes.IndexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.208">def visit_type_application(self, o: mypy.nodes.TypeApplication) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.209">def visit_lambda_expr(self, o: mypy.nodes.LambdaExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.21">def trim_source_line(line: str, max_len: int, col: int, min_width: int) -&gt; tuple[str, int]:
    """Trim a line of source code to fit into max_len.

    Show 'min_width' characters on each side of 'col' (an error location). If either
    start or end is trimmed, this is indicated by adding '...' there.
    A typical result looks like this:
        ...some_variable = function_to_call(one_arg, other_arg) or...

    Return the trimmed string and the column offset to to adjust error location.
    """
    if max_len &lt; 2 * min_width + 1:
        # In case the window is too tiny it is better to still show something.
        max_len = 2 * min_width + 1

    # Trivial case: line already fits in.
    if len(line) &lt;= max_len:
        return line, 0

    # If column is not too large so that there is still min_width after it,
    # the line doesn't need to be trimmed at the start.
    if col + min_width &lt; max_len:
        return line[:max_len] + "...", 0

    # Otherwise, if the column is not too close to the end, trim both sides.
    if col &lt; len(line) - min_width - 1:
        offset = col - max_len + min_width + 1
        return "..." + line[offset : col + min_width + 1] + "...", offset - 3

    # Finally, if the column is near the end, just trim the start.
    return "..." + line[-max_len:], len(line) - max_len - 3


</t>
<t tx="ekr.20221004064036.210">def visit_list_comprehension(self, o: mypy.nodes.ListComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.211">def visit_set_comprehension(self, o: mypy.nodes.SetComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.212">def visit_dictionary_comprehension(self, o: mypy.nodes.DictionaryComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.213">def visit_generator_expr(self, o: mypy.nodes.GeneratorExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.214">def visit_slice_expr(self, o: mypy.nodes.SliceExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.215">def visit_conditional_expr(self, o: mypy.nodes.ConditionalExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.216">def visit_type_var_expr(self, o: mypy.nodes.TypeVarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.217">def visit_paramspec_expr(self, o: mypy.nodes.ParamSpecExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.218">def visit_type_var_tuple_expr(self, o: mypy.nodes.TypeVarTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.219">def visit_type_alias_expr(self, o: mypy.nodes.TypeAliasExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.22">def get_mypy_comments(source: str) -&gt; list[tuple[int, str]]:
    PREFIX = "# mypy: "
    # Don't bother splitting up the lines unless we know it is useful
    if PREFIX not in source:
        return []
    lines = source.split("\n")
    results = []
    for i, line in enumerate(lines):
        if line.startswith(PREFIX):
            results.append((i + 1, line[len(PREFIX) :]))

    return results


</t>
<t tx="ekr.20221004064036.220">def visit_namedtuple_expr(self, o: mypy.nodes.NamedTupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.221">def visit_enum_call_expr(self, o: mypy.nodes.EnumCallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.222">def visit_typeddict_expr(self, o: mypy.nodes.TypedDictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.223">def visit_newtype_expr(self, o: mypy.nodes.NewTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.224">def visit__promote_expr(self, o: mypy.nodes.PromoteExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.225">def visit_await_expr(self, o: mypy.nodes.AwaitExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.226">def visit_temp_node(self, o: mypy.nodes.TempNode) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.227"># Patterns

</t>
<t tx="ekr.20221004064036.228">def visit_as_pattern(self, o: mypy.patterns.AsPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.229">def visit_or_pattern(self, o: mypy.patterns.OrPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.23">PASS_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="0" failures="0" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""

FAIL_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="0" failures="1" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
    &lt;failure message="mypy produced messages"&gt;{text}&lt;/failure&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""

ERROR_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="1" failures="0" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
    &lt;error message="mypy produced errors"&gt;{text}&lt;/error&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""


</t>
<t tx="ekr.20221004064036.230">def visit_value_pattern(self, o: mypy.patterns.ValuePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.231">def visit_singleton_pattern(self, o: mypy.patterns.SingletonPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.232">def visit_sequence_pattern(self, o: mypy.patterns.SequencePattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.233">def visit_starred_pattern(self, o: mypy.patterns.StarredPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.234">def visit_mapping_pattern(self, o: mypy.patterns.MappingPattern) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.235">def visit_class_pattern(self, o: mypy.patterns.ClassPattern) -&gt; T:
    pass
</t>
<t tx="ekr.20221004064036.237">@path C:/Repos/ekr-mypy2/mypy/
"""Mypy type checker command line tool."""

from __future__ import annotations

import os
import sys
import traceback

from mypy.main import main, process_options
from mypy.util import FancyFormatter


@others
if __name__ == "__main__":
    console_entry()
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064036.238">def console_entry() -&gt; None:
    try:
        main()
        sys.stdout.flush()
        sys.stderr.flush()
    except BrokenPipeError:
        # Python flushes standard streams on exit; redirect remaining output
        # to devnull to avoid another BrokenPipeError at shutdown
        devnull = os.open(os.devnull, os.O_WRONLY)
        os.dup2(devnull, sys.stdout.fileno())
        sys.exit(2)
    except KeyboardInterrupt:
        _, options = process_options(args=sys.argv[1:])
        if options.show_traceback:
            sys.stdout.write(traceback.format_exc())
        formatter = FancyFormatter(sys.stdout, sys.stderr, False)
        msg = "Interrupted\n"
        sys.stdout.write(formatter.style(msg, color="red", bold=True))
        sys.stdout.flush()
        sys.stderr.flush()
        sys.exit(2)


</t>
<t tx="ekr.20221004064036.24">def write_junit_xml(
    dt: float, serious: bool, messages: list[str], path: str, version: str, platform: str
) -&gt; None:
    from xml.sax.saxutils import escape

    if not messages and not serious:
        xml = PASS_TEMPLATE.format(time=dt, ver=version, platform=platform)
    elif not serious:
        xml = FAIL_TEMPLATE.format(
            text=escape("\n".join(messages)), time=dt, ver=version, platform=platform
        )
    else:
        xml = ERROR_TEMPLATE.format(
            text=escape("\n".join(messages)), time=dt, ver=version, platform=platform
        )

    # checks for a directory structure in path and creates folders if needed
    xml_dirs = os.path.dirname(os.path.abspath(path))
    if not os.path.isdir(xml_dirs):
        os.makedirs(xml_dirs)

    with open(path, "wb") as f:
        f.write(xml.encode("utf-8"))


</t>
<t tx="ekr.20221004064036.25">class IdMapper:
    """Generate integer ids for objects.

    Unlike id(), these start from 0 and increment by 1, and ids won't
    get reused across the life-time of IdMapper.

    Assume objects don't redefine __eq__ or __hash__.
    """

    @others
</t>
<t tx="ekr.20221004064036.26">def __init__(self) -&gt; None:
    self.id_map: dict[object, int] = {}
    self.next_id = 0

</t>
<t tx="ekr.20221004064036.27">def id(self, o: object) -&gt; int:
    if o not in self.id_map:
        self.id_map[o] = self.next_id
        self.next_id += 1
    return self.id_map[o]


</t>
<t tx="ekr.20221004064036.28">def get_prefix(fullname: str) -&gt; str:
    """Drop the final component of a qualified name (e.g. ('x.y' -&gt; 'x')."""
    return fullname.rsplit(".", 1)[0]


</t>
<t tx="ekr.20221004064036.29">def get_top_two_prefixes(fullname: str) -&gt; tuple[str, str]:
    """Return one and two component prefixes of a fully qualified name.

    Given 'a.b.c.d', return ('a', 'a.b').

    If fullname has only one component, return (fullname, fullname).
    """
    components = fullname.split(".", 3)
    return components[0], ".".join(components[:2])


</t>
<t tx="ekr.20221004064036.3">def visit_overloaded(self, t: Overloaded) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20221004064036.30">def correct_relative_import(
    cur_mod_id: str, relative: int, target: str, is_cur_package_init_file: bool
) -&gt; tuple[str, bool]:
    if relative == 0:
        return target, True
    parts = cur_mod_id.split(".")
    rel = relative
    if is_cur_package_init_file:
        rel -= 1
    ok = len(parts) &gt;= rel
    if rel != 0:
        cur_mod_id = ".".join(parts[:-rel])
    return cur_mod_id + (("." + target) if target else ""), ok


</t>
<t tx="ekr.20221004064036.31">fields_cache: Final[dict[type[object], list[str]]] = {}


</t>
<t tx="ekr.20221004064036.32">def get_class_descriptors(cls: type[object]) -&gt; Sequence[str]:
    import inspect  # Lazy import for minor startup speed win

    # Maintain a cache of type -&gt; attributes defined by descriptors in the class
    # (that is, attributes from __slots__ and C extension classes)
    if cls not in fields_cache:
        members = inspect.getmembers(
            cls, lambda o: inspect.isgetsetdescriptor(o) or inspect.ismemberdescriptor(o)
        )
        fields_cache[cls] = [x for x, y in members if x != "__weakref__" and x != "__dict__"]
    return fields_cache[cls]


</t>
<t tx="ekr.20221004064036.33">def replace_object_state(
    new: object, old: object, copy_dict: bool = False, skip_slots: tuple[str, ...] = ()
) -&gt; None:
    """Copy state of old node to the new node.

    This handles cases where there is __dict__ and/or attribute descriptors
    (either from slots or because the type is defined in a C extension module).

    Assume that both objects have the same __class__.
    """
    if hasattr(old, "__dict__"):
        if copy_dict:
            new.__dict__ = dict(old.__dict__)
        else:
            new.__dict__ = old.__dict__

    for attr in get_class_descriptors(old.__class__):
        if attr in skip_slots:
            continue
        try:
            if hasattr(old, attr):
                setattr(new, attr, getattr(old, attr))
            elif hasattr(new, attr):
                delattr(new, attr)
        # There is no way to distinguish getsetdescriptors that allow
        # writes from ones that don't (I think?), so we just ignore
        # AttributeErrors if we need to.
        # TODO: What about getsetdescriptors that act like properties???
        except AttributeError:
            pass


</t>
<t tx="ekr.20221004064036.34">def is_sub_path(path1: str, path2: str) -&gt; bool:
    """Given two paths, return if path1 is a sub-path of path2."""
    return pathlib.Path(path2) in pathlib.Path(path1).parents


</t>
<t tx="ekr.20221004064036.35">def hard_exit(status: int = 0) -&gt; None:
    """Kill the current process without fully cleaning up.

    This can be quite a bit faster than a normal exit() since objects are not freed.
    """
    sys.stdout.flush()
    sys.stderr.flush()
    os._exit(status)


</t>
<t tx="ekr.20221004064036.36">def unmangle(name: str) -&gt; str:
    """Remove internal suffixes from a short name."""
    return name.rstrip("'")


</t>
<t tx="ekr.20221004064036.37">def get_unique_redefinition_name(name: str, existing: Container[str]) -&gt; str:
    """Get a simple redefinition name not present among existing.

    For example, for name 'foo' we try 'foo-redefinition', 'foo-redefinition2',
    'foo-redefinition3', etc. until we find one that is not in existing.
    """
    r_name = name + "-redefinition"
    if r_name not in existing:
        return r_name

    i = 2
    while r_name + str(i) in existing:
        i += 1
    return r_name + str(i)


</t>
<t tx="ekr.20221004064036.38">def check_python_version(program: str) -&gt; None:
    """Report issues with the Python used to run mypy, dmypy, or stubgen"""
    # Check for known bad Python versions.
    if sys.version_info[:2] &lt; (3, 7):
        sys.exit(
            "Running {name} with Python 3.6 or lower is not supported; "
            "please upgrade to 3.7 or newer".format(name=program)
        )


</t>
<t tx="ekr.20221004064036.39">def count_stats(messages: list[str]) -&gt; tuple[int, int, int]:
    """Count total number of errors, notes and error_files in message list."""
    errors = [e for e in messages if ": error:" in e]
    error_files = {e.split(":")[0] for e in errors}
    notes = [e for e in messages if ": note:" in e]
    return len(errors), len(notes), len(error_files)


</t>
<t tx="ekr.20221004064036.4">def visit_type_type(self, t: TypeType) -&gt; T:
    return t.item.accept(self)

</t>
<t tx="ekr.20221004064036.40">def split_words(msg: str) -&gt; list[str]:
    """Split line of text into words (but not within quoted groups)."""
    next_word = ""
    res: list[str] = []
    allow_break = True
    for c in msg:
        if c == " " and allow_break:
            res.append(next_word)
            next_word = ""
            continue
        if c == '"':
            allow_break = not allow_break
        next_word += c
    res.append(next_word)
    return res


</t>
<t tx="ekr.20221004064036.41">def get_terminal_width() -&gt; int:
    """Get current terminal width if possible, otherwise return the default one."""
    return (
        int(os.getenv("MYPY_FORCE_TERMINAL_WIDTH", "0"))
        or shutil.get_terminal_size().columns
        or DEFAULT_COLUMNS
    )


</t>
<t tx="ekr.20221004064036.42">def soft_wrap(msg: str, max_len: int, first_offset: int, num_indent: int = 0) -&gt; str:
    """Wrap a long error message into few lines.

    Breaks will only happen between words, and never inside a quoted group
    (to avoid breaking types such as "Union[int, str]"). The 'first_offset' is
    the width before the start of first line.

    Pad every next line with 'num_indent' spaces. Every line will be at most 'max_len'
    characters, except if it is a single word or quoted group.

    For example:
               first_offset
        ------------------------
        path/to/file: error: 58: Some very long error message
            that needs to be split in separate lines.
            "Long[Type, Names]" are never split.
        ^^^^--------------------------------------------------
        num_indent           max_len
    """
    words = split_words(msg)
    next_line = words.pop(0)
    lines: list[str] = []
    while words:
        next_word = words.pop(0)
        max_line_len = max_len - num_indent if lines else max_len - first_offset
        # Add 1 to account for space between words.
        if len(next_line) + len(next_word) + 1 &lt;= max_line_len:
            next_line += " " + next_word
        else:
            lines.append(next_line)
            next_line = next_word
    lines.append(next_line)
    padding = "\n" + " " * num_indent
    return padding.join(lines)


</t>
<t tx="ekr.20221004064036.43">def hash_digest(data: bytes) -&gt; str:
    """Compute a hash digest of some data.

    We use a cryptographic hash because we want a low probability of
    accidental collision, but we don't really care about any of the
    cryptographic properties.
    """
    # Once we drop Python 3.5 support, we should consider using
    # blake2b, which is faster.
    return hashlib.sha256(data).hexdigest()


</t>
<t tx="ekr.20221004064036.44">def parse_gray_color(cup: bytes) -&gt; str:
    """Reproduce a gray color in ANSI escape sequence"""
    if sys.platform == "win32":
        assert False, "curses is not available on Windows"
    set_color = "".join([cup[:-1].decode(), "m"])
    gray = curses.tparm(set_color.encode("utf-8"), 1, 9).decode()
    return gray


</t>
<t tx="ekr.20221004064036.45">class FancyFormatter:
    """Apply color and bold font to terminal output.

    This currently only works on Linux and Mac.
    """

    @others
</t>
<t tx="ekr.20221004064036.46">def __init__(self, f_out: IO[str], f_err: IO[str], hide_error_codes: bool) -&gt; None:
    self.hide_error_codes = hide_error_codes
    # Check if we are in a human-facing terminal on a supported platform.
    if sys.platform not in ("linux", "darwin", "win32", "emscripten"):
        self.dummy_term = True
        return
    force_color = int(os.getenv("MYPY_FORCE_COLOR", "0"))
    if not force_color and (not f_out.isatty() or not f_err.isatty()):
        self.dummy_term = True
        return
    if sys.platform == "win32":
        self.dummy_term = not self.initialize_win_colors()
    elif sys.platform == "emscripten":
        self.dummy_term = not self.initialize_vt100_colors()
    else:
        self.dummy_term = not self.initialize_unix_colors()
    if not self.dummy_term:
        self.colors = {
            "red": self.RED,
            "green": self.GREEN,
            "blue": self.BLUE,
            "yellow": self.YELLOW,
            "none": "",
        }

</t>
<t tx="ekr.20221004064036.47">def initialize_vt100_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    # Windows and Emscripten can both use ANSI/VT100 escape sequences for color
    assert sys.platform in ("win32", "emscripten")
    self.BOLD = "\033[1m"
    self.UNDER = "\033[4m"
    self.BLUE = "\033[94m"
    self.GREEN = "\033[92m"
    self.RED = "\033[91m"
    self.YELLOW = "\033[93m"
    self.NORMAL = "\033[0m"
    self.DIM = "\033[2m"
    return True

</t>
<t tx="ekr.20221004064036.48">def initialize_win_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    # Windows ANSI escape sequences are only supported on Threshold 2 and above.
    # we check with an assert at runtime and an if check for mypy, as asserts do not
    # yet narrow platform
    assert sys.platform == "win32"
    if sys.platform == "win32":
        winver = sys.getwindowsversion()
        if (
            winver.major &lt; MINIMUM_WINDOWS_MAJOR_VT100
            or winver.build &lt; MINIMUM_WINDOWS_BUILD_VT100
        ):
            return False
        import ctypes

        kernel32 = ctypes.windll.kernel32
        ENABLE_PROCESSED_OUTPUT = 0x1
        ENABLE_WRAP_AT_EOL_OUTPUT = 0x2
        ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x4
        STD_OUTPUT_HANDLE = -11
        kernel32.SetConsoleMode(
            kernel32.GetStdHandle(STD_OUTPUT_HANDLE),
            ENABLE_PROCESSED_OUTPUT
            | ENABLE_WRAP_AT_EOL_OUTPUT
            | ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        )
        self.initialize_vt100_colors()
        return True
    return False

</t>
<t tx="ekr.20221004064036.49">def initialize_unix_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    if sys.platform == "win32" or not CURSES_ENABLED:
        return False
    try:
        # setupterm wants a fd to potentially write an "initialization sequence".
        # We override sys.stdout for the daemon API so if stdout doesn't have an fd,
        # just give it /dev/null.
        try:
            fd = sys.stdout.fileno()
        except io.UnsupportedOperation:
            with open("/dev/null", "rb") as f:
                curses.setupterm(fd=f.fileno())
        else:
            curses.setupterm(fd=fd)
    except curses.error:
        # Most likely terminfo not found.
        return False
    bold = curses.tigetstr("bold")
    under = curses.tigetstr("smul")
    set_color = curses.tigetstr("setaf")
    set_eseq = curses.tigetstr("cup")
    normal = curses.tigetstr("sgr0")

    if not (bold and under and set_color and set_eseq and normal):
        return False

    self.NORMAL = normal.decode()
    self.BOLD = bold.decode()
    self.UNDER = under.decode()
    self.DIM = parse_gray_color(set_eseq)
    self.BLUE = curses.tparm(set_color, curses.COLOR_BLUE).decode()
    self.GREEN = curses.tparm(set_color, curses.COLOR_GREEN).decode()
    self.RED = curses.tparm(set_color, curses.COLOR_RED).decode()
    self.YELLOW = curses.tparm(set_color, curses.COLOR_YELLOW).decode()
    return True

</t>
<t tx="ekr.20221004064036.5">def visit_ellipsis_type(self, t: EllipsisType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20221004064036.50">def style(
    self,
    text: str,
    color: Literal["red", "green", "blue", "yellow", "none"],
    bold: bool = False,
    underline: bool = False,
    dim: bool = False,
) -&gt; str:
    """Apply simple color and style (underlined or bold)."""
    if self.dummy_term:
        return text
    if bold:
        start = self.BOLD
    else:
        start = ""
    if underline:
        start += self.UNDER
    if dim:
        start += self.DIM
    return start + self.colors[color] + text + self.NORMAL

</t>
<t tx="ekr.20221004064036.51">def fit_in_terminal(
    self, messages: list[str], fixed_terminal_width: int | None = None
) -&gt; list[str]:
    """Improve readability by wrapping error messages and trimming source code."""
    width = fixed_terminal_width or get_terminal_width()
    new_messages = messages.copy()
    for i, error in enumerate(messages):
        if ": error:" in error:
            loc, msg = error.split("error:", maxsplit=1)
            msg = soft_wrap(msg, width, first_offset=len(loc) + len("error: "))
            new_messages[i] = loc + "error:" + msg
        if error.startswith(" " * DEFAULT_SOURCE_OFFSET) and "^" not in error:
            # TODO: detecting source code highlights through an indent can be surprising.
            # Restore original error message and error location.
            error = error[DEFAULT_SOURCE_OFFSET:]
            marker_line = messages[i + 1]
            marker_column = marker_line.index("^")
            column = marker_column - DEFAULT_SOURCE_OFFSET
            if "~" not in marker_line:
                marker = "^"
            else:
                # +1 because both ends are included
                marker = marker_line[marker_column : marker_line.rindex("~") + 1]

            # Let source have some space also on the right side, plus 6
            # to accommodate ... on each side.
            max_len = width - DEFAULT_SOURCE_OFFSET - 6
            source_line, offset = trim_source_line(error, max_len, column, MINIMUM_WIDTH)

            new_messages[i] = " " * DEFAULT_SOURCE_OFFSET + source_line
            # Also adjust the error marker position and trim error marker is needed.
            new_marker_line = " " * (DEFAULT_SOURCE_OFFSET + column - offset) + marker
            if len(new_marker_line) &gt; len(new_messages[i]) and len(marker) &gt; 3:
                new_marker_line = new_marker_line[: len(new_messages[i]) - 3] + "..."
            new_messages[i + 1] = new_marker_line
    return new_messages

</t>
<t tx="ekr.20221004064036.52">def colorize(self, error: str) -&gt; str:
    """Colorize an output line by highlighting the status and error code."""
    if ": error:" in error:
        loc, msg = error.split("error:", maxsplit=1)
        if self.hide_error_codes:
            return (
                loc + self.style("error:", "red", bold=True) + self.highlight_quote_groups(msg)
            )
        codepos = msg.rfind("[")
        if codepos != -1:
            code = msg[codepos:]
            msg = msg[:codepos]
        else:
            code = ""  # no error code specified
        return (
            loc
            + self.style("error:", "red", bold=True)
            + self.highlight_quote_groups(msg)
            + self.style(code, "yellow")
        )
    elif ": note:" in error:
        loc, msg = error.split("note:", maxsplit=1)
        formatted = self.highlight_quote_groups(self.underline_link(msg))
        return loc + self.style("note:", "blue") + formatted
    elif error.startswith(" " * DEFAULT_SOURCE_OFFSET):
        # TODO: detecting source code highlights through an indent can be surprising.
        if "^" not in error:
            return self.style(error, "none", dim=True)
        return self.style(error, "red")
    else:
        return error

</t>
<t tx="ekr.20221004064036.53">def highlight_quote_groups(self, msg: str) -&gt; str:
    """Make groups quoted with double quotes bold (including quotes).

    This is used to highlight types, attribute names etc.
    """
    if msg.count('"') % 2:
        # Broken error message, don't do any formatting.
        return msg
    parts = msg.split('"')
    out = ""
    for i, part in enumerate(parts):
        if i % 2 == 0:
            out += self.style(part, "none")
        else:
            out += self.style('"' + part + '"', "none", bold=True)
    return out

</t>
<t tx="ekr.20221004064036.54">def underline_link(self, note: str) -&gt; str:
    """Underline a link in a note message (if any).

    This assumes there is at most one link in the message.
    """
    match = re.search(r"https?://\S*", note)
    if not match:
        return note
    start = match.start()
    end = match.end()
    return note[:start] + self.style(note[start:end], "none", underline=True) + note[end:]

</t>
<t tx="ekr.20221004064036.55">def format_success(self, n_sources: int, use_color: bool = True) -&gt; str:
    """Format short summary in case of success.

    n_sources is total number of files passed directly on command line,
    i.e. excluding stubs and followed imports.
    """
    msg = f"Success: no issues found in {n_sources} source file{plural_s(n_sources)}"
    if not use_color:
        return msg
    return self.style(msg, "green", bold=True)

</t>
<t tx="ekr.20221004064036.56">def format_error(
    self,
    n_errors: int,
    n_files: int,
    n_sources: int,
    *,
    blockers: bool = False,
    use_color: bool = True,
) -&gt; str:
    """Format a short summary in case of errors."""
    msg = f"Found {n_errors} error{plural_s(n_errors)} in {n_files} file{plural_s(n_files)}"
    if blockers:
        msg += " (errors prevented further checking)"
    else:
        msg += f" (checked {n_sources} source file{plural_s(n_sources)})"
    if not use_color:
        return msg
    return self.style(msg, "red", bold=True)


</t>
<t tx="ekr.20221004064036.57">def is_typeshed_file(typeshed_dir: str | None, file: str) -&gt; bool:
    typeshed_dir = typeshed_dir if typeshed_dir is not None else TYPESHED_DIR
    try:
        return os.path.commonpath((typeshed_dir, os.path.abspath(file))) == typeshed_dir
    except ValueError:  # Different drives on Windows
        return False


</t>
<t tx="ekr.20221004064036.58">def is_stub_package_file(file: str) -&gt; bool:
    # Use hacky heuristics to check whether file is part of a PEP 561 stub package.
    if not file.endswith(".pyi"):
        return False
    return any(component.endswith("-stubs") for component in os.path.split(os.path.abspath(file)))


</t>
<t tx="ekr.20221004064036.59">def unnamed_function(name: str | None) -&gt; bool:
    return name is not None and name == "_"


</t>
<t tx="ekr.20221004064036.6">def visit_placeholder_type(self, t: PlaceholderType) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20221004064036.60"># TODO: replace with uses of perf_counter_ns when support for py3.6 is dropped
# (or when mypy properly handles alternate definitions based on python version check
time_ref = time.perf_counter


</t>
<t tx="ekr.20221004064036.61">def time_spent_us(t0: float) -&gt; int:
    return int((time.perf_counter() - t0) * 1e6)


</t>
<t tx="ekr.20221004064036.62">def plural_s(s: int | Sized) -&gt; str:
    count = s if isinstance(s, int) else len(s)
    if count &gt; 1:
        return "s"
    else:
        return ""
</t>
<t tx="ekr.20221004064036.63">@path C:/Repos/ekr-mypy2/mypy/
from __future__ import annotations

import os

from mypy import git

# Base version.
# - Release versions have the form "0.NNN".
# - Dev versions have the form "0.NNN+dev" (PLUS sign to conform to PEP 440).
# - For 1.0 we'll switch back to 1.2.3 form.
__version__ = "0.990+dev"
base_version = __version__

mypy_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
if __version__.endswith("+dev") and git.is_git_repo(mypy_dir) and git.have_git():
    __version__ += "." + git.git_revision(mypy_dir).decode("utf-8")
    if git.is_dirty(mypy_dir):
        __version__ += ".dirty"
del mypy_dir
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064036.64">@path C:/Repos/ekr-mypy2/mypy/
"""Generic abstract syntax tree node visitor"""

from __future__ import annotations

from abc import abstractmethod
from typing import TYPE_CHECKING, Generic, TypeVar

from mypy_extensions import mypyc_attr, trait

if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.nodes
    import mypy.patterns


T = TypeVar("T")


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064036.65">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class ExpressionVisitor(Generic[T]):
    @others
</t>
<t tx="ekr.20221004064036.66">@abstractmethod
def visit_int_expr(self, o: mypy.nodes.IntExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.67">@abstractmethod
def visit_str_expr(self, o: mypy.nodes.StrExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.68">@abstractmethod
def visit_bytes_expr(self, o: mypy.nodes.BytesExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.69">@abstractmethod
def visit_float_expr(self, o: mypy.nodes.FloatExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.7">def visit_type_alias_type(self, t: TypeAliasType) -&gt; T:
    if self.skip_alias_target:
        return self.query_types(t.args)
    return get_proper_type(t).accept(self)

</t>
<t tx="ekr.20221004064036.70">@abstractmethod
def visit_complex_expr(self, o: mypy.nodes.ComplexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.71">@abstractmethod
def visit_ellipsis(self, o: mypy.nodes.EllipsisExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.72">@abstractmethod
def visit_star_expr(self, o: mypy.nodes.StarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.73">@abstractmethod
def visit_name_expr(self, o: mypy.nodes.NameExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.74">@abstractmethod
def visit_member_expr(self, o: mypy.nodes.MemberExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.75">@abstractmethod
def visit_yield_from_expr(self, o: mypy.nodes.YieldFromExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.76">@abstractmethod
def visit_yield_expr(self, o: mypy.nodes.YieldExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.77">@abstractmethod
def visit_call_expr(self, o: mypy.nodes.CallExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.78">@abstractmethod
def visit_op_expr(self, o: mypy.nodes.OpExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.79">@abstractmethod
def visit_comparison_expr(self, o: mypy.nodes.ComparisonExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.791">@path C:/Repos/ekr-mypy2/mypy/test/
"""Type checker test cases"""

from __future__ import annotations

import os
import re
import sys

from mypy import build
from mypy.build import Graph
from mypy.errors import CompileError
from mypy.modulefinder import BuildSource, FindModuleCache, SearchPaths
from mypy.semanal_main import core_modules
from mypy.test.config import test_data_prefix, test_temp_dir
from mypy.test.data import DataDrivenTestCase, DataSuite, FileOperation, module_from_path
from mypy.test.helpers import (
    assert_module_equivalence,
    assert_string_arrays_equal,
    assert_target_equivalence,
    check_test_output_files,
    find_test_files,
    normalize_error_messages,
    parse_options,
    perform_file_operations,
    update_testcase_output,
)

try:
    import lxml  # type: ignore[import]
except ImportError:
    lxml = None

import pytest

# List of files that contain test case descriptions.
# Includes all check-* files with the .test extension in the test-data/unit directory
typecheck_files = find_test_files(pattern="check-*.test")

# Tests that use Python 3.8-only AST features (like expression-scoped ignores):
if sys.version_info &lt; (3, 8):
    typecheck_files.remove("check-python38.test")
if sys.version_info &lt; (3, 9):
    typecheck_files.remove("check-python39.test")
if sys.version_info &lt; (3, 10):
    typecheck_files.remove("check-python310.test")

# Special tests for platforms with case-insensitive filesystems.
if sys.platform not in ("darwin", "win32"):
    typecheck_files.remove("check-modules-case.test")


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064036.792">class TypeCheckSuite(DataSuite):
    files = typecheck_files

    @others
</t>
<t tx="ekr.20221004064036.793">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    if lxml is None and os.path.basename(testcase.file) == "check-reports.test":
        pytest.skip("Cannot import lxml. Is it installed?")
    incremental = (
        "incremental" in testcase.name.lower()
        or "incremental" in testcase.file
        or "serialize" in testcase.file
    )
    if incremental:
        # Incremental tests are run once with a cold cache, once with a warm cache.
        # Expect success on first run, errors from testcase.output (if any) on second run.
        num_steps = max([2] + list(testcase.output2.keys()))
        # Check that there are no file changes beyond the last run (they would be ignored).
        for dn, dirs, files in os.walk(os.curdir):
            for file in files:
                m = re.search(r"\.([2-9])$", file)
                if m and int(m.group(1)) &gt; num_steps:
                    raise ValueError(
                        "Output file {} exists though test case only has {} runs".format(
                            file, num_steps
                        )
                    )
        steps = testcase.find_steps()
        for step in range(1, num_steps + 1):
            idx = step - 2
            ops = steps[idx] if idx &lt; len(steps) and idx &gt;= 0 else []
            self.run_case_once(testcase, ops, step)
    else:
        self.run_case_once(testcase)

</t>
<t tx="ekr.20221004064036.794">def run_case_once(
    self,
    testcase: DataDrivenTestCase,
    operations: list[FileOperation] = [],
    incremental_step: int = 0,
) -&gt; None:
    original_program_text = "\n".join(testcase.input)
    module_data = self.parse_module(original_program_text, incremental_step)

    # Unload already loaded plugins, they may be updated.
    for file, _ in testcase.files:
        module = module_from_path(file)
        if module.endswith("_plugin") and module in sys.modules:
            del sys.modules[module]
    if incremental_step == 0 or incremental_step == 1:
        # In run 1, copy program text to program file.
        for module_name, program_path, program_text in module_data:
            if module_name == "__main__":
                with open(program_path, "w", encoding="utf8") as f:
                    f.write(program_text)
                break
    elif incremental_step &gt; 1:
        # In runs 2+, copy *.[num] files to * files.
        perform_file_operations(operations)

    # Parse options after moving files (in case mypy.ini is being moved).
    options = parse_options(original_program_text, testcase, incremental_step)
    options.use_builtins_fixtures = True
    options.enable_incomplete_features = True
    options.show_traceback = True

    # Enable some options automatically based on test file name.
    if "optional" in testcase.file:
        options.strict_optional = True
    if "columns" in testcase.file:
        options.show_column_numbers = True
    if "errorcodes" in testcase.file:
        options.hide_error_codes = False
    if "abstract" not in testcase.file:
        options.allow_empty_bodies = not testcase.name.endswith("_no_empty")

    if incremental_step and options.incremental:
        # Don't overwrite # flags: --no-incremental in incremental test cases
        options.incremental = True
    else:
        options.incremental = False
        # Don't waste time writing cache unless we are specifically looking for it
        if not testcase.writescache:
            options.cache_dir = os.devnull

    sources = []
    for module_name, program_path, program_text in module_data:
        # Always set to none so we're forced to reread the module in incremental mode
        sources.append(
            BuildSource(program_path, module_name, None if incremental_step else program_text)
        )

    plugin_dir = os.path.join(test_data_prefix, "plugins")
    sys.path.insert(0, plugin_dir)

    res = None
    try:
        ### import pdb ; pdb.set_trace()  ###
        res = build.build(sources=sources, options=options, alt_lib_path=test_temp_dir)
        a = res.errors
    except CompileError as e:
        a = e.messages
    finally:
        assert sys.path[0] == plugin_dir
        del sys.path[0]

    if testcase.normalize_output:
        a = normalize_error_messages(a)

    # Make sure error messages match
    if incremental_step == 0:
        # Not incremental
        msg = "Unexpected type checker output ({}, line {})"
        output = testcase.output
    elif incremental_step == 1:
        msg = "Unexpected type checker output in incremental, run 1 ({}, line {})"
        output = testcase.output
    elif incremental_step &gt; 1:
        msg = (
            f"Unexpected type checker output in incremental, run {incremental_step}"
            + " ({}, line {})"
        )
        output = testcase.output2.get(incremental_step, [])
    else:
        raise AssertionError()

    if output != a and testcase.config.getoption("--update-data", False):
        update_testcase_output(testcase, a)
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))

    if res:
        if options.cache_dir != os.devnull:
            self.verify_cache(module_data, res.errors, res.manager, res.graph)

        name = "targets"
        if incremental_step:
            name += str(incremental_step + 1)
        expected = testcase.expected_fine_grained_targets.get(incremental_step + 1)
        actual = res.manager.processed_targets
        # Skip the initial builtin cycle.
        actual = [
            t
            for t in actual
            if not any(t.startswith(mod) for mod in core_modules + ["mypy_extensions"])
        ]
        if expected is not None:
            assert_target_equivalence(name, expected, actual)
        if incremental_step &gt; 1:
            suffix = "" if incremental_step == 2 else str(incremental_step - 1)
            expected_rechecked = testcase.expected_rechecked_modules.get(incremental_step - 1)
            if expected_rechecked is not None:
                assert_module_equivalence(
                    "rechecked" + suffix, expected_rechecked, res.manager.rechecked_modules
                )
            expected_stale = testcase.expected_stale_modules.get(incremental_step - 1)
            if expected_stale is not None:
                assert_module_equivalence(
                    "stale" + suffix, expected_stale, res.manager.stale_modules
                )

    if testcase.output_files:
        check_test_output_files(testcase, incremental_step, strip_prefix="tmp/")

</t>
<t tx="ekr.20221004064036.795">def verify_cache(
    self,
    module_data: list[tuple[str, str, str]],
    a: list[str],
    manager: build.BuildManager,
    graph: Graph,
) -&gt; None:
    # There should be valid cache metadata for each module except
    # for those that had an error in themselves or one of their
    # dependencies.
    error_paths = self.find_error_message_paths(a)
    busted_paths = {m.path for id, m in manager.modules.items() if graph[id].transitive_error}
    modules = self.find_module_files(manager)
    modules.update({module_name: path for module_name, path, text in module_data})
    missing_paths = self.find_missing_cache_files(modules, manager)
    # We would like to assert error_paths.issubset(busted_paths)
    # but this runs into trouble because while some 'notes' are
    # really errors that cause an error to be marked, many are
    # just notes attached to other errors.
    assert error_paths or not busted_paths, "Some modules reported error despite no errors"
    if not missing_paths == busted_paths:
        raise AssertionError(f"cache data discrepancy {missing_paths} != {busted_paths}")
    assert os.path.isfile(os.path.join(manager.options.cache_dir, ".gitignore"))
    cachedir_tag = os.path.join(manager.options.cache_dir, "CACHEDIR.TAG")
    assert os.path.isfile(cachedir_tag)
    with open(cachedir_tag) as f:
        assert f.read().startswith("Signature: 8a477f597d28d172789f06886806bc55")

</t>
<t tx="ekr.20221004064036.796">def find_error_message_paths(self, a: list[str]) -&gt; set[str]:
    hits = set()
    for line in a:
        m = re.match(r"([^\s:]+):(\d+:)?(\d+:)? (error|warning|note):", line)
        if m:
            p = m.group(1)
            hits.add(p)
    return hits

</t>
<t tx="ekr.20221004064036.797">def find_module_files(self, manager: build.BuildManager) -&gt; dict[str, str]:
    return {id: module.path for id, module in manager.modules.items()}

</t>
<t tx="ekr.20221004064036.798">def find_missing_cache_files(
    self, modules: dict[str, str], manager: build.BuildManager
) -&gt; set[str]:
    ignore_errors = True
    missing = {}
    for id, path in modules.items():
        meta = build.find_cache_meta(id, path, manager)
        if not build.validate_meta(meta, id, path, ignore_errors, manager):
            missing[id] = path
    return set(missing.values())

</t>
<t tx="ekr.20221004064036.799">def parse_module(
    self, program_text: str, incremental_step: int = 0
) -&gt; list[tuple[str, str, str]]:
    """Return the module and program names for a test case.

    Normally, the unit tests will parse the default ('__main__')
    module and follow all the imports listed there. You can override
    this behavior and instruct the tests to check multiple modules
    by using a comment like this in the test case input:

      # cmd: mypy -m foo.bar foo.baz

    You can also use `# cmdN:` to have a different cmd for incremental
    step N (2, 3, ...).

    Return a list of tuples (module name, file name, program text).
    """
    m = re.search("# cmd: mypy -m ([a-zA-Z0-9_. ]+)$", program_text, flags=re.MULTILINE)
    if incremental_step &gt; 1:
        alt_regex = f"# cmd{incremental_step}: mypy -m ([a-zA-Z0-9_. ]+)$"
        alt_m = re.search(alt_regex, program_text, flags=re.MULTILINE)
        if alt_m is not None:
            # Optionally return a different command if in a later step
            # of incremental mode, otherwise default to reusing the
            # original cmd.
            m = alt_m

    if m:
        # The test case wants to use a non-default main
        # module. Look up the module and give it as the thing to
        # analyze.
        module_names = m.group(1)
        out = []
        search_paths = SearchPaths((test_temp_dir,), (), (), ())
        cache = FindModuleCache(search_paths, fscache=None, options=None)
        for module_name in module_names.split(" "):
            path = cache.find_module(module_name)
            assert isinstance(path, str), f"Can't find ad hoc case file: {module_name}"
            with open(path, encoding="utf8") as f:
                program_text = f.read()
            out.append((module_name, path, program_text))
        return out
    else:
        return [("__main__", "main", program_text)]
</t>
<t tx="ekr.20221004064036.8">def query_types(self, types: Iterable[Type]) -&gt; T:
    """Perform a query for a list of types.

    Use the strategy to combine the results.
    Skip type aliases already visited types to avoid infinite recursion.
    """
    res: list[T] = []
    for t in types:
        if isinstance(t, TypeAliasType):
            # Avoid infinite recursion for recursive type aliases.
            # TODO: Ideally we should fire subvisitors here (or use caching) if we care
            #       about duplicates.
            if t in self.seen_aliases:
                continue
            self.seen_aliases.add(t)
        res.append(t.accept(self))
    return self.strategy(res)
</t>
<t tx="ekr.20221004064036.80">@abstractmethod
def visit_cast_expr(self, o: mypy.nodes.CastExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.81">@abstractmethod
def visit_assert_type_expr(self, o: mypy.nodes.AssertTypeExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.82">@abstractmethod
def visit_reveal_expr(self, o: mypy.nodes.RevealExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.83">@abstractmethod
def visit_super_expr(self, o: mypy.nodes.SuperExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.84">@abstractmethod
def visit_unary_expr(self, o: mypy.nodes.UnaryExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.85">@abstractmethod
def visit_assignment_expr(self, o: mypy.nodes.AssignmentExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.86">@abstractmethod
def visit_list_expr(self, o: mypy.nodes.ListExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.87">@abstractmethod
def visit_dict_expr(self, o: mypy.nodes.DictExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.88">@abstractmethod
def visit_tuple_expr(self, o: mypy.nodes.TupleExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.89">@abstractmethod
def visit_set_expr(self, o: mypy.nodes.SetExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.9">@path C:/Repos/ekr-mypy2/mypy/
"""Utility functions with no non-trivial dependencies."""

from __future__ import annotations

import hashlib
import io
import os
import pathlib
import re
import shutil
import sys
import time
from importlib import resources as importlib_resources
from typing import IO, Callable, Container, Iterable, Sequence, Sized, TypeVar
from typing_extensions import Final, Literal

try:
    import curses

    import _curses  # noqa: F401

    CURSES_ENABLED = True
except ImportError:
    CURSES_ENABLED = False

T = TypeVar("T")

if sys.version_info &gt;= (3, 9):
    TYPESHED_DIR: Final = str(importlib_resources.files("mypy") / "typeshed")
else:
    with importlib_resources.path(
        "mypy",  # mypy-c doesn't support __package__
        "py.typed",  # a marker file for type information, we assume typeshed to live in the same dir
    ) as _resource:
        TYPESHED_DIR = str(_resource.parent / "typeshed")


ENCODING_RE: Final = re.compile(rb"([ \t\v]*#.*(\r\n?|\n))??[ \t\v]*#.*coding[:=][ \t]*([-\w.]+)")

DEFAULT_SOURCE_OFFSET: Final = 4
DEFAULT_COLUMNS: Final = 80

# At least this number of columns will be shown on each side of
# error location when printing source code snippet.
MINIMUM_WIDTH: Final = 20

# VT100 color code processing was added in Windows 10, but only the second major update,
# Threshold 2. Fortunately, everyone (even on LTSB, Long Term Support Branch) should
# have a version of Windows 10 newer than this. Note that Windows 8 and below are not
# supported, but are either going out of support, or make up only a few % of the market.
MINIMUM_WINDOWS_MAJOR_VT100: Final = 10
MINIMUM_WINDOWS_BUILD_VT100: Final = 10586

SPECIAL_DUNDERS: Final = frozenset(
    ("__init__", "__new__", "__call__", "__init_subclass__", "__class_getitem__")
)


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20221004064036.90">@abstractmethod
def visit_index_expr(self, o: mypy.nodes.IndexExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.91">@abstractmethod
def visit_type_application(self, o: mypy.nodes.TypeApplication) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.92">@abstractmethod
def visit_lambda_expr(self, o: mypy.nodes.LambdaExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.93">@abstractmethod
def visit_list_comprehension(self, o: mypy.nodes.ListComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.94">@abstractmethod
def visit_set_comprehension(self, o: mypy.nodes.SetComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.95">@abstractmethod
def visit_dictionary_comprehension(self, o: mypy.nodes.DictionaryComprehension) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.96">@abstractmethod
def visit_generator_expr(self, o: mypy.nodes.GeneratorExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.97">@abstractmethod
def visit_slice_expr(self, o: mypy.nodes.SliceExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.98">@abstractmethod
def visit_conditional_expr(self, o: mypy.nodes.ConditionalExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221004064036.99">@abstractmethod
def visit_type_var_expr(self, o: mypy.nodes.TypeVarExpr) -&gt; T:
    pass

</t>
<t tx="ekr.20221005053958.1">
from __future__ import annotations

import itertools
from contextlib import contextmanager
from itertools import chain
from typing import Callable, Iterable, Iterator, List, Sequence, Tuple, TypeVar
from typing_extensions import Final, Protocol

from mypy import errorcodes as codes, message_registry, nodes
from mypy.errorcodes import ErrorCode
from mypy.exprtotype import TypeTranslationError, expr_to_unanalyzed_type
from mypy.messages import MessageBuilder, format_type_bare, quote_type_string
from mypy.nodes import (
    ARG_NAMED,
    ARG_NAMED_OPT,
    ARG_OPT,
    ARG_POS,
    ARG_STAR,
    ARG_STAR2,
    SYMBOL_FUNCBASE_TYPES,
    ArgKind,
    Context,
    Decorator,
    Expression,
    MypyFile,
    ParamSpecExpr,
    PlaceholderNode,
    SymbolTableNode,
    TypeAlias,
    TypeInfo,
    TypeVarExpr,
    TypeVarLikeExpr,
    TypeVarTupleExpr,
    Var,
    check_arg_kinds,
    check_arg_names,
    get_nongen_builtins,
)
from mypy.options import Options
from mypy.plugin import AnalyzeTypeContext, Plugin, TypeAnalyzerPluginInterface
from mypy.semanal_shared import SemanticAnalyzerCoreInterface, paramspec_args, paramspec_kwargs
from mypy.tvar_scope import TypeVarLikeScope
from mypy.types import (
    ANNOTATED_TYPE_NAMES,
    FINAL_TYPE_NAMES,
    LITERAL_TYPE_NAMES,
    NEVER_NAMES,
    TYPE_ALIAS_NAMES,
    AnyType,
    CallableArgument,
    CallableType,
    DeletedType,
    EllipsisType,
    ErasedType,
    Instance,
    LiteralType,
    NoneType,
    Overloaded,
    Parameters,
    ParamSpecFlavor,
    ParamSpecType,
    PartialType,
    PlaceholderType,
    RawExpressionType,
    RequiredType,
    StarType,
    SyntheticTypeVisitor,
    TrivialSyntheticTypeTranslator,
    TupleType,
    Type,
    TypeAliasType,
    TypedDictType,
    TypeList,
    TypeOfAny,
    TypeQuery,
    TypeType,
    TypeVarLikeType,
    TypeVarTupleType,
    TypeVarType,
    UnboundType,
    UninhabitedType,
    UnionType,
    UnpackType,
    bad_type_type_item,
    callable_with_ellipsis,
    flatten_nested_unions,
    get_proper_type,
)
from mypy.typetraverser import TypeTraverserVisitor
</t>
<t tx="ekr.20221005054114.1">
T = TypeVar("T")

type_constructors: Final = {
    "typing.Callable",
    "typing.Optional",
    "typing.Tuple",
    "typing.Type",
    "typing.Union",
    *LITERAL_TYPE_NAMES,
    *ANNOTATED_TYPE_NAMES,
}

ARG_KINDS_BY_CONSTRUCTOR: Final = {
    "mypy_extensions.Arg": ARG_POS,
    "mypy_extensions.DefaultArg": ARG_OPT,
    "mypy_extensions.NamedArg": ARG_NAMED,
    "mypy_extensions.DefaultNamedArg": ARG_NAMED_OPT,
    "mypy_extensions.VarArg": ARG_STAR,
    "mypy_extensions.KwArg": ARG_STAR2,
}

GENERIC_STUB_NOT_AT_RUNTIME_TYPES: Final = {
    "queue.Queue",
    "builtins._PathLike",
    "asyncio.futures.Future",
}


</t>
<t tx="ekr.20221005074708.1"></t>
<t tx="ekr.20221005080738.1"></t>
<t tx="ekr.20221005080808.1"></t>
<t tx="ekr.20221005080846.1"></t>
<t tx="ekr.20221005080920.1"></t>
<t tx="ekr.20221005081102.1"></t>
<t tx="ekr.20221005081130.1"></t>
<t tx="ekr.20221005081317.1"></t>
<t tx="ekr.20221005081400.1"></t>
<t tx="ekr.20221005081520.1"></t>
<t tx="ekr.20221005081733.1"></t>
<t tx="ekr.20221005082302.1"></t>
<t tx="ekr.20221025071515.1">@path C:\Users\Dev
@language md

Private (for now) mypy notes.

[Markdown best practices](https://www.markdownguide.org/basic-syntax/)

@others
</t>
<t tx="ekr.20221025071935.1">#### Ahas

- Static analysis should usually suffice!
  That's the *goal* of well-designed classes.
  
- Use (mypy) unit tests as a means of running code.
  We don't usually have to know how the tests are run!
</t>
<t tx="ekr.20221025072341.1">#### Running unit tests

- Must use pytest, not unittest.

- sitecustomize.py (Python 3.10) adds ekr-mypy2 and leo-editor to sys.path.

- tm.cmd runs mypy unit tests:

        pytest -n0 -s --mypy-verbose --verbose -k testForcedAssignment

- Use -s to suppress redirection of stdout and stderr.
  This option shows mypy log messages and allows g.trace to be visible.
  
- mypy/test/data.py contains class DataDrivenTestCase(pytest.Item).

- mypy/test/testcheck.py creates test cases from .test files.
  run_case_once calls build.build.</t>
<t tx="ekr.20221025075836.1">@path c:\scripts
@language batch

echo off

cd c:\Repos\ekr-mypy2

rem -s disable redirection of stdout (so g.trace works).

pytest -n0 -s --mypy-verbose --verbose -k testForcedAssignment</t>
<t tx="ekr.20221025081332.1"># Python 3.10: sitecustomize.py
@path C:\Python\Python3.10\Lib
import sys
paths = (
	r'C:\Repos\ekr-mypy2',  # The path continaing the top-level mypy directory.
	r'C:\Repos\leo-editor',  # So mypy can import leoGlobals.
)
for path in paths:
	if path not in sys.path:
		sys.path.append(path)
if 0:
	print(f"\n***** {__file__}: add {path}\n")
	for z in sys.path:
		print(z)</t>
<t tx="ekr.20221025092126.1"></t>
</tnodes>
</leo_file>
